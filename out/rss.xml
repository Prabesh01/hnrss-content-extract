<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Wed, 24 Sep 2025 16:43:51 +0000</lastBuildDate><item><title>Huntington's disease treated for first time</title><link>https://www.bbc.com/news/articles/cevz13xkxpro</link><description>&lt;doc fingerprint="2e099265b8279da2"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Huntington's disease successfully treated for first time&lt;/head&gt;
    &lt;p&gt;One of the cruellest and most devastating diseases ‚Äì Huntington's ‚Äì has been successfully treated for the first time, say doctors.&lt;/p&gt;
    &lt;p&gt;The disease runs through families, relentlessly kills brain cells and resembles a combination of dementia, Parkinson's and motor neurone disease.&lt;/p&gt;
    &lt;p&gt;An emotional research team became tearful as they described how data shows the disease was slowed by 75% in patients.&lt;/p&gt;
    &lt;p&gt;It means the decline you would normally expect in one year would take four years after treatment, giving patients decades of "good quality life", Prof Sarah Tabrizi told BBC News.&lt;/p&gt;
    &lt;p&gt;The new treatment is a type of gene therapy given during 12 to 18 hours of delicate brain surgery.&lt;/p&gt;
    &lt;p&gt;The first symptoms of Huntington's disease tend to appear in your 30s or 40s and is normally fatal within two decades ‚Äì opening the possibility that earlier treatment could prevent symptoms from ever emerging.&lt;/p&gt;
    &lt;p&gt;Prof Tabrizi, director of the University College London Huntington's Disease Centre, described the results as "spectacular".&lt;/p&gt;
    &lt;p&gt;"We never in our wildest dreams would have expected a 75% slowing of clinical progression," she said.&lt;/p&gt;
    &lt;p&gt;None of the patients who have been treated are being identified, but one was medically retired and has returned to work. Others in the trial are still walking despite being expected to need a wheelchair.&lt;/p&gt;
    &lt;p&gt;Treatment is likely to be very expensive. However, this is a moment of real hope in a disease that hits people in their prime and devastates families.&lt;/p&gt;
    &lt;p&gt;Huntington's runs through Jack May-Davis' family. He has the faulty gene that causes the disease, as did his dad, Fred, and his grandmother, Joyce.&lt;/p&gt;
    &lt;p&gt;Jack said it was "really awful and horrible" watching his dad's inexorable decline.&lt;/p&gt;
    &lt;p&gt;The first symptoms appeared in Fred's late 30s, including changes in behaviour and the way he moved. He eventually needed 24/7 palliative care before he died at the age of 54, in 2016.&lt;/p&gt;
    &lt;p&gt;Jack is 30, a barrister's clerk, newly engaged to Chloe and has taken part in research at UCL to turn his diagnosis into a positive.&lt;/p&gt;
    &lt;p&gt;But he'd always known he was destined to share his father's fate, until today.&lt;/p&gt;
    &lt;p&gt;Now he says the "absolutely incredible" breakthrough has left him "overwhelmed" and able to look to a future that "seems a little bit brighter, it does allow me to think my life could be that much longer".&lt;/p&gt;
    &lt;p&gt;Huntington's disease is caused by an error in part of our DNA called the huntingtin gene.&lt;/p&gt;
    &lt;p&gt;If one of your parents has Huntington's disease, there's a 50% chance that you will inherit the altered gene and will eventually develop Huntington's too.&lt;/p&gt;
    &lt;p&gt;This mutation turns a normal protein needed in the brain ‚Äì called the huntingtin protein ‚Äì into a killer of neurons.&lt;/p&gt;
    &lt;p&gt;The goal of the treatment is to reduce levels of this toxic protein permanently, in a single dose.&lt;/p&gt;
    &lt;p&gt;The therapy uses cutting edge genetic medicine combining gene therapy and gene silencing technologies.&lt;/p&gt;
    &lt;p&gt;It starts with a safe virus that has been altered to contain a specially designed sequence of DNA.&lt;/p&gt;
    &lt;p&gt;This is infused deep into the brain using real-time MRI scanning to guide a microcatheter to two brain regions - the caudate nucleus and the putamen. This takes 12 to 18 hours of neurosurgery.&lt;/p&gt;
    &lt;p&gt;The virus then acts like a microscopic postman ‚Äì delivering the new piece of DNA inside brain cells, where it becomes active.&lt;/p&gt;
    &lt;p&gt;This turns the neurons into a factory for making the therapy to avert their own death.&lt;/p&gt;
    &lt;p&gt;The cells produce a small fragment of genetic material (called microRNA) that is designed to intercept and disable the instructions (called messenger RNA) being sent from the cells' DNA for building mutant huntingtin.&lt;/p&gt;
    &lt;p&gt;This results in lower levels of mutant huntingtin in the brain.&lt;/p&gt;
    &lt;p&gt;Results from the trial - which involved 29 patients - have been released in a statement by the company uniQure, but have not yet been published in full for review by other specialists.&lt;/p&gt;
    &lt;p&gt;The data showed that three years after surgery there was an average 75% slowing of the disease based on a measure which combines cognition, motor function and the ability to manage in daily life.&lt;/p&gt;
    &lt;p&gt;The data also shows the treatment is saving brain cells. Levels of neurofilaments in spinal fluid ‚Äì a clear sign of brain cells dying ‚Äì should have increased by a third if the disease continued to progress, but was actually lower than at the start of the trial.&lt;/p&gt;
    &lt;p&gt;"This is the result we've been waiting for," said Prof Ed Wild, consultant neurologist at the National Hospital for Neurology and Neurosurgery at UCLH.&lt;/p&gt;
    &lt;p&gt;"There was every chance that we would never see a result like this, so to be living in a world where we know this is not only possible, but the actual magnitude of the effect is breathtaking, it's very difficult to fully encapsulate the emotion."&lt;/p&gt;
    &lt;p&gt;He said he was "a bit teary" thinking about the impact it could have on families.&lt;/p&gt;
    &lt;p&gt;The treatment was considered safe, although some patients did develop inflammation from the virus that caused headaches and confusion that either resolved or needed steroid treatment.&lt;/p&gt;
    &lt;p&gt;Prof Wild anticipates the therapy "should last for life" because brain cells are not replaced by the body in the same manner as blood, bone and skin are constantly renewed.&lt;/p&gt;
    &lt;p&gt;Approximately 75,000 people have Huntington's disease in the UK, US and Europe with hundreds of thousands carrying the mutation meaning they will develop the disease.&lt;/p&gt;
    &lt;p&gt;UniQure says it will apply for a licence in the US in the first quarter of 2026 with the aim of launching the drug later that year. Conversations with authorities in the UK and Europe will start next year, but the initial focus is on the US.&lt;/p&gt;
    &lt;p&gt;Dr Walid Abi-Saab, the chief medical officer at uniQure, said he was "incredibly excited" about what the results mean for families, and added that the treatment had "the potential to fundamentally transform" Huntington's disease.&lt;/p&gt;
    &lt;p&gt;However, the drug will not be available for everyone due to the highly complex surgery and the anticipated cost.&lt;/p&gt;
    &lt;p&gt;"It will be expensive for sure," says Prof Wild.&lt;/p&gt;
    &lt;p&gt;There isn't an official price for the drug. Gene therapies are often pricey, but their long-term impact means that can still be affordable. In the UK, the NHS does pay for a ¬£2.6m-per-patient gene therapy for haemophilia B.&lt;/p&gt;
    &lt;p&gt;Prof Tabrizi says this gene therapy "is the beginning" and will open the gates for therapies that can reach more people.&lt;/p&gt;
    &lt;p&gt;She paid tribute to the "truly brave" volunteers who took part in the trial, saying she was "overjoyed for the patients and families".&lt;/p&gt;
    &lt;p&gt;She is already working with a group of young people who know they have the gene, but don't yet have symptoms ‚Äì known as stage zero Huntington's ‚Äì and is aiming to do the first prevention trial to see if the disease can be significantly delayed or even stopped completely.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45358940</guid><pubDate>Wed, 24 Sep 2025 11:37:30 +0000</pubDate></item><item><title>Yt-dlp: Upcoming new requirements for YouTube downloads</title><link>https://github.com/yt-dlp/yt-dlp/issues/14404</link><description>&lt;doc fingerprint="806fb82cc43ebc72"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; Notifications &lt;tool-tip&gt;You must be signed in to change notification settings&lt;/tool-tip&gt;&lt;/item&gt;
      &lt;item&gt;Fork 10.2k&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Description&lt;/head&gt;
    &lt;head rend="h3"&gt;Beginning very soon, you'll need to have the JavaScript runtime Deno installed to keep YouTube downloads working as normal.&lt;/head&gt;
    &lt;head rend="h2"&gt;Why?&lt;/head&gt;
    &lt;p&gt;Up until now, yt-dlp has been able to use its built-in JavaScript "interpreter" to solve the JavaScript challenges that are required for YouTube downloads. But due to recent changes on YouTube's end, the built-in JS interpreter will soon be insufficient for this purpose. The changes are so drastic that yt-dlp will need to leverage a proper JavaScript runtime in order to solve the JS challenges.&lt;/p&gt;
    &lt;head rend="h2"&gt;What do I need to do?&lt;/head&gt;
    &lt;head rend="h3"&gt;Everyone will need to install Deno.&lt;/head&gt;
    &lt;p&gt;yt-dlp will also need a few JavaScript components, and this may require additional action from you depending on how you installed yt-dlp:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Official PyInstaller-bundled executable users (e.g.&lt;/p&gt;&lt;code&gt;yt-dlp.exe&lt;/code&gt;,&lt;code&gt;yt-dlp_macos&lt;/code&gt;,&lt;code&gt;yt-dlp_linux&lt;/code&gt;, etc):&lt;list rend="ul"&gt;&lt;item&gt;No additional action required (besides having Deno). All the necessary JavaScript components will be bundled with these executables.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;PyPI package users (e.g. installed with&lt;/p&gt;&lt;code&gt;pip&lt;/code&gt;,&lt;code&gt;pipx&lt;/code&gt;, etc):&lt;list rend="ul"&gt;&lt;item&gt;Install and upgrade yt-dlp with the &lt;code&gt;default&lt;/code&gt;optional dependency group included, e.g.:&lt;code&gt;pip install -U "yt-dlp[default]"&lt;/code&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Install and upgrade yt-dlp with the &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Official zipimport binary users (the&lt;/p&gt;&lt;code&gt;yt-dlp&lt;/code&gt;Unix executable):&lt;list rend="ul"&gt;&lt;item&gt;Run yt-dlp with an additional flag to allow Deno to download &lt;code&gt;npm&lt;/code&gt;dependencies --or-- install yt-dlp's JS solver package in your Python environment. (The flag name and the package name are both still TBD.)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Run yt-dlp with an additional flag to allow Deno to download &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Third-party package users (e.g. installed with&lt;/p&gt;&lt;code&gt;pacman&lt;/code&gt;,&lt;code&gt;brew&lt;/code&gt;, etc):&lt;list rend="ul"&gt;&lt;item&gt;The action required will depend on how your third-party package repository decides to handle this change. But the options available for "official zipimport binary users" should work for you as well.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45358980</guid><pubDate>Wed, 24 Sep 2025 11:41:54 +0000</pubDate></item><item><title>EU age verification app not planning desktop support</title><link>https://github.com/eu-digital-identity-wallet/av-doc-technical-specification/issues/22</link><description>&lt;doc fingerprint="e70383268912763c"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; Notifications &lt;tool-tip&gt;You must be signed in to change notification settings&lt;/tool-tip&gt;&lt;/item&gt;
      &lt;item&gt;Fork 2&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Description&lt;/head&gt;
    &lt;p&gt;Hi I found multiple usability issues with this solution.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;The focus is so strong on the app, that it assumes everyone owns a smartphone. The other day I saw a granny on the bus with a phone that was 2cms thick and predates the famous Nokia 3310. How is she and other users without a smartphone supposed to verify their age online?&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;How will this impact the browsing experience on the web? Every website has GDPR checkboxes these days which somewhat disrupts browsing experience if browsing in for example incognito mode. Imagine if you want to browse the web privately. Websites don't know who you are so you will have to verify your age every single time. This makes the web unusable for anyone who wants to browse the web privately. Especially on a pc. A solution would be to have some sort of browser extension that handles it automatically. Since you at least claim to value privacy that could work. But it wouldn't really look trustworthy. Note this doesn't only apply to incognito but browsing the web in general. Like trying to compare various news sites. Doing this for every website to visit is a major hindrance usability wise.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;What will the cost be of implementing this? My trust in the EU to develop affordable and good technologies has diminished since we created a Peppol access point for our company. The solution was made using technologies only java has proper libraries for. Locking the developer to that language and eco system. Of course not a big issue for a big company. But a small start up won't be able to survive if they have to implement this.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45359074</guid><pubDate>Wed, 24 Sep 2025 11:52:50 +0000</pubDate></item><item><title>WiGLE: Wireless Network Mapping</title><link>https://wigle.net/index</link><description>&lt;doc fingerprint="8e2ed84303c8a523"&gt;
  &lt;main&gt;
    &lt;p&gt;Toggle navigation View Basic Search Advanced Search Map Uploads Info Android App FAQ App FAQ Forums Mastodon Bsky Twitter Stats Tools Downloads API Account CSV Upload Login User Name Password Forgot your password? keep me logged-in New? Register All the networks. Found by Everyone. &amp;lt;&amp;lt; Latitude Longitude SSID BSSID Date Range: 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025 2026 Possible FreeNet Possible Commercial Net No Labels Only Discovered By Me Only Discovered By Others Coloring: density QoS channel View: Greyscale Nightvision Standard Notes: Zoom in to see individual SSIDs. cell tower: blue QoS: Quality of Signal is a metric based on the number of observations and observers Statistics Over Time WiFi Networks Over Time [Full-screen Graph] WiFi Encryption Over Time [Full-screen Graph] [2 Years only Graph] Mouse-over graphs to interact with data. Select a range to zoom in, double click to zoom back out. Modify the number in the corner to smooth over multiple days. Full-screen graphs available! √ó √ó √ó √ó Join WiGLE √ó A Message from WiGLE&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45359201</guid><pubDate>Wed, 24 Sep 2025 12:10:00 +0000</pubDate></item><item><title>Rights groups urge UK PM Starmer to abandon plans for mandatory digital ID</title><link>https://bigbrotherwatch.org.uk/press-releases/rights-groups-urge-starmer-to-abandon-plans-for-mandatory-digital-id/</link><description>&lt;doc fingerprint="8da196ac48835d9b"&gt;
  &lt;main&gt;
    &lt;p&gt;Big Brother Watch and other human rights, civil liberties, digital rights, and racial justice organisations have written to the Prime Minister urging him to abandon plans for a mandatory digital ID.&lt;/p&gt;
    &lt;p&gt;The letter comes just days before an expected statement from Keir Starmer announcing the rollout of a mandatory digital ID scheme aimed at deterring illegal immigration.&lt;/p&gt;
    &lt;p&gt;The leaders of Big Brother Watch, Article 19, Connected by Data, Liberty, Open Rights Group, The Runnymede Trust, Unlock Democracy and medConfidential argue that digital ID would change our relationship with the state, cause irreversible damage to our civil liberties, and fail to deter illegal immigration:&lt;/p&gt;
    &lt;p&gt;‚ÄúMandatory digital ID would fundamentally change the relationship between the population and the state by requiring frequent identity checks as we navigate our daily lives. Although the current digital ID proposals are being considered in the context of immigration, there is no guarantee that a future government would not make digital ID a requirement to access a range of public and private services.‚Äù&lt;/p&gt;
    &lt;p&gt;The joint letter can be found using this link.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Spokespeople are available for interview. Please direct enquiries or requests for interviews to info@bigbrotherwatch.org.uk or 07730439257&lt;/item&gt;
      &lt;item&gt; Read Big Brother Watch‚Äôs recent report ‚ÄúCheckpoint Britain: the dangers of digital ID and why privacy must be protected‚Äù&lt;/item&gt;
      &lt;item&gt; Big Brother Watch will be hosting events on the dangers of digital ID during the Labour and Conservative Party conferences in Liverpool and Manchester. Contact info@bigbrotherwatch.org.uk for details‚Äù&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45359356</guid><pubDate>Wed, 24 Sep 2025 12:28:15 +0000</pubDate></item><item><title>US Airlines Push to Strip Away Travelers' Rights by Rolling Back Key Protections</title><link>https://www.travelandtourworld.com/news/article/american-joins-delta-southwest-united-and-other-us-airlines-push-to-strip-away-travelers-rights-and-add-more-fees-by-rolling-back-key-protections-in-new-deregulation-move/</link><description>&lt;doc fingerprint="567c9a5a01baa55b"&gt;
  &lt;main&gt;
    &lt;p&gt;Home¬ªAIRLINE NEWS¬ª American Joins Delta, Southwest, United and Other US Airlines Push to Strip Away Travelers‚Äô Rights and Add More Fees by Rolling Back Key Protections in New Deregulation Move&lt;/p&gt;
    &lt;p&gt;American Joins Delta, Southwest, United and Other US Airlines Push to Strip Away Travelers‚Äô Rights and Add More Fees by Rolling Back Key Protections in New Deregulation Move&lt;/p&gt;
    &lt;p&gt;American Airlines joins with Delta, Southwest, United, and other US airlines are pushing to remove key protections for passengers and add more fees by rolling back rules, claiming it will lower costs and boost competition, but it may leave travelers with fewer rights and more hidden charges. Under the guise of lowering costs and boosting competition, these changes are likely to result in the erosion of consumer rights ‚Äì the right to cancel a ticket with an automatic refund, transparency of pricing, and the right to sit with your family on the same reservation. Airlines claim that removing these ‚Äòprotections‚Äô will decrease airfare and increase competition on the routes. The prospects for travelers, on the other hand, will likely be more fees, less certainty of receiving the service paid for, and diminished responsibility from the airlines for service failures. If these projections become reality, deregulation will aggravate the air travel experience for the consumer making it more expensive and more opaque.&lt;/p&gt;
    &lt;p&gt;The Airline Industry‚Äôs Deregulatory Push&lt;/p&gt;
    &lt;p&gt;The U.S. airline industry is pushing for a significant rollback of consumer protections, which many see as a major step backward for air travel. Airline lobbyists, representing carriers like American, Delta, Southwest, United, and the Airlines for America (A4A) association, have laid out a detailed agenda that would fundamentally alter the landscape of air travel, making it more difficult for passengers to know what they‚Äôre actually paying for and less likely to receive compensation when things go wrong.&lt;/p&gt;
    &lt;p&gt;This agenda centers on weakening or eliminating four major consumer protections:&lt;/p&gt;
    &lt;p&gt;Automatic Refunds for Cancellations: Airlines want to remove the requirement to provide automatic refunds when flights are cancelled or significantly altered. Passengers may instead receive only vouchers or no compensation at all, leaving them without recourse in the event of a major flight disruption.&lt;/p&gt;
    &lt;p&gt;Transparency of Fees: The airlines also aim to strip away rules that require them to disclose all fees (like baggage, seat assignments, and service charges) upfront. Instead of the clear, itemized pricing system that passengers currently rely on, airlines could hide fees until later in the booking process, making the true cost of a ticket much higher than expected.&lt;/p&gt;
    &lt;p&gt;Family Seating Guarantees: Under current regulations, airlines must ensure that families with young children are seated together without additional charges. This would no longer be guaranteed under the new proposal, meaning families could face extra costs just to sit next to one another.&lt;/p&gt;
    &lt;p&gt;Accessibility Protections for Disabled Passengers: The deregulation proposal also targets protections for disabled passengers, weakening their access to support and assistance during air travel.&lt;/p&gt;
    &lt;p&gt;The Airline Industry‚Äôs Argument: Deregulation as a Path to Lower Prices&lt;/p&gt;
    &lt;p&gt;The airline industry‚Äôs argument for deregulation is grounded in a belief that removing these protections will lead to lower prices, more competition, and better services for consumers. Airline lobbyists argue that deregulation, which began with the Airline Deregulation Act of 1978, has led to increased competition, lower airfares, and more choices for passengers.&lt;/p&gt;
    &lt;p&gt;Advertisement&lt;/p&gt;
    &lt;p&gt;However, while some might agree that competition can drive prices down, there‚Äôs a serious concern that deregulation could lead to more surprise charges and less accountability for airlines. Instead of benefiting consumers, deregulation may open the door for airlines to charge excessive fees for basic services, which are often hidden until later in the booking process. This could leave passengers paying far more than they anticipated and receiving less value for their money.&lt;/p&gt;
    &lt;p&gt;The Airlines‚Äô Detailed Deregulatory Agenda&lt;/p&gt;
    &lt;p&gt;The Airlines for America (A4A), the industry group representing major U.S. airlines, has strongly supported deregulation, arguing that it has benefited both airlines and passengers since the 1970s. In a recent document, A4A outlined their full deregulatory wish list, which includes the following key points:&lt;/p&gt;
    &lt;p&gt;Advertisement&lt;/p&gt;
    &lt;p&gt;Support for Deregulation: A4A strongly advocates for the continuation of deregulation, claiming that it has led to lower prices, increased competition, and better services for consumers. The group argues that removing regulations would allow airlines to better compete in the market and reinvest in improving services for passengers.&lt;/p&gt;
    &lt;p&gt;Criticism of the Biden Administration‚Äôs Regulatory Actions:&lt;/p&gt;
    &lt;p&gt;Ancillary Fee Transparency: A4A opposes the U.S. Department of Transportation‚Äôs (DOT) rules requiring airlines to disclose ancillary fees upfront, arguing that these rules exceed the DOT‚Äôs authority and don‚Äôt provide any clear benefits to consumers.&lt;/p&gt;
    &lt;p&gt;Refund Rules: A4A calls for the repeal or revision of refund rules that, according to them, go beyond what‚Äôs required by law, imposing unnecessary costs on airlines without providing any clear benefit to the public.&lt;/p&gt;
    &lt;p&gt;Flight Delay and Cancellations: The group also criticizes DOT‚Äôs policies on flight delays and cancellations, claiming the rules unfairly penalize airlines, particularly when disruptions are caused by factors beyond their control.&lt;/p&gt;
    &lt;p&gt;Deregulatory Priorities: A4A outlines several changes they would like to see the DOT pursue:&lt;/p&gt;
    &lt;p&gt;Rescinding Unlawful Regulations: A4A seeks the repeal of certain regulations, such as family seating and mobility aid assistance rules, which they argue exceed DOT‚Äôs authority.&lt;/p&gt;
    &lt;p&gt;Limiting Refund Rules: The group wants to limit DOT‚Äôs authority on flight refund rules, particularly for minor operational changes, such as changes to flight numbers or itineraries that don‚Äôt cause harm to passengers.&lt;/p&gt;
    &lt;p&gt;Economic Impact of Deregulation: A4A points to the success of deregulation, citing a rise in low-cost carriers, which have made air travel more affordable. They also highlight the significant decrease in airfare prices, which has directly benefited consumers.&lt;/p&gt;
    &lt;p&gt;Investment in Airline Operations: A4A argues that deregulation has allowed airlines to reinvest in their services, improving customer satisfaction and innovation.&lt;/p&gt;
    &lt;p&gt;Support for Technological Innovation: The airline industry is also backing the use of technology, including artificial intelligence (AI) and biometrics, to improve operational efficiency and the customer experience.&lt;/p&gt;
    &lt;p&gt;Why Deregulation is a Concern for Passengers&lt;/p&gt;
    &lt;p&gt;While the airline industry argues that deregulation will lead to lower prices and more competition, critics are skeptical. Here are the key reasons why deregulation could harm consumers:&lt;/p&gt;
    &lt;p&gt;More Hidden Fees: If airlines are no longer required to disclose fees upfront, passengers may face a barrage of surprise charges, from baggage fees to seat selection costs. The cost of air travel could increase significantly, even if base fares appear lower.&lt;/p&gt;
    &lt;p&gt;No Guarantees for Families: The proposal to eliminate the guarantee that families will be seated together without extra charges could lead to more stress for families travelling with young children. Parents may find themselves paying additional fees just to sit next to their kids.&lt;/p&gt;
    &lt;p&gt;Less Accountability for Cancellations: Airlines would have more power to decide whether or not to refund passengers for flight cancellations. This could lead to more vouchers, rather than cash refunds, leaving passengers at the mercy of airlines‚Äô own policies.&lt;/p&gt;
    &lt;p&gt;Weaker Protections for Disabled Passengers: The weakening of accessibility regulations could make it more difficult for passengers with disabilities to access the services and assistance they need during their travel.&lt;/p&gt;
    &lt;p&gt;Less Competition, Not More: While deregulation advocates claim it will lead to more competition, the reality is that fewer protections for consumers could allow major airlines to exploit passengers without fear of consequences. Smaller carriers may also struggle to compete on an uneven playing field.&lt;/p&gt;
    &lt;p&gt;The Risk of Over-Regulation vs. Consumer Protection&lt;/p&gt;
    &lt;p&gt;The battle between over-regulation and consumer protection is a complex issue. While it‚Äôs true that some regulations may stifle innovation, the protections that are in place help ensure fair treatment and transparency for consumers. The question is not whether airlines should be regulated, but how much regulation is necessary to strike a balance between profitability and protecting passengers.&lt;/p&gt;
    &lt;p&gt;In Europe, stricter regulations have led to fewer delays and cancellations, and the market remains competitive with budget airlines thriving under the current system. The fear is that deregulation in the U.S. could result in a situation where airlines dominate the market, and passengers are left with fewer rights and more fees.&lt;/p&gt;
    &lt;p&gt;What Passengers Can Do&lt;/p&gt;
    &lt;p&gt;As a passenger, it‚Äôs important to stay informed about these changes and advocate for your rights. Here are some steps you can take:&lt;/p&gt;
    &lt;p&gt;Stay Informed: Keep up with the latest news and updates on airline regulations.&lt;/p&gt;
    &lt;p&gt;Contact Your Representatives: Let your senators and congress members know how you feel about the deregulation of the airline industry.&lt;/p&gt;
    &lt;p&gt;Know Your Rights: Understand the protections you currently have and how they might change.&lt;/p&gt;
    &lt;p&gt;American Airlines, Delta, Southwest, United, and other U.S. airlines are pushing to remove key protections for passengers and add more fees by rolling back regulations, arguing that it will lower costs and increase competition, but it could also lead to fewer rights and more hidden charges for travelers.&lt;/p&gt;
    &lt;p&gt;Conclusion&lt;/p&gt;
    &lt;p&gt;The deregulation push by U.S. airlines is a major threat to passenger rights. While airlines argue that deregulation will lead to cheaper fares and more competition, the reality is likely to be more fees, less transparency, and fewer protections for passengers. If successful, this move could turn back the clock to a time when flying was riddled with hidden charges and unfair treatment.&lt;/p&gt;
    &lt;p&gt;The future of air travel depends on consumers, advocacy groups, and lawmakers standing up for passenger rights. The airline industry may be pushing for deregulation, but it‚Äôs up to the public to ensure that the changes made are in the best interest of all passengers, not just the airlines. The battle is not just about cheaper tickets, but about ensuring that air travel remains fair, transparent, and accountable for everyone.&lt;/p&gt;
    &lt;p&gt;We use cookies on our website to give you the most relevant experience by remembering your preferences and repeat visits. By clicking ‚ÄúAccept‚Äù, you consent to the use of ALL the cookies.&lt;/p&gt;
    &lt;p&gt;This website uses cookies to improve your experience while you navigate through the website. Out of these, the cookies that are categorized as necessary are stored on your browser as they are essential for the working of basic functionalities of the website. We also use third-party cookies that help us analyze and understand how you use this website. These cookies will be stored in your browser only with your consent. You also have the option to opt-out of these cookies. But opting out of some of these cookies may affect your browsing experience.&lt;/p&gt;
    &lt;p&gt;Necessary cookies are absolutely essential for the website to function properly. These cookies ensure basic functionalities and security features of the website, anonymously.&lt;/p&gt;
    &lt;p&gt;Cookie&lt;/p&gt;
    &lt;p&gt;Duration&lt;/p&gt;
    &lt;p&gt;Description&lt;/p&gt;
    &lt;p&gt;cookielawinfo-checkbox-analytics&lt;/p&gt;
    &lt;p&gt;11 months&lt;/p&gt;
    &lt;p&gt;This cookie is set by GDPR Cookie Consent plugin. The cookie is used to store the user consent for the cookies in the category "Analytics".&lt;/p&gt;
    &lt;p&gt;cookielawinfo-checkbox-functional&lt;/p&gt;
    &lt;p&gt;11 months&lt;/p&gt;
    &lt;p&gt;The cookie is set by GDPR cookie consent to record the user consent for the cookies in the category "Functional".&lt;/p&gt;
    &lt;p&gt;cookielawinfo-checkbox-necessary&lt;/p&gt;
    &lt;p&gt;11 months&lt;/p&gt;
    &lt;p&gt;This cookie is set by GDPR Cookie Consent plugin. The cookies is used to store the user consent for the cookies in the category "Necessary".&lt;/p&gt;
    &lt;p&gt;cookielawinfo-checkbox-others&lt;/p&gt;
    &lt;p&gt;11 months&lt;/p&gt;
    &lt;p&gt;This cookie is set by GDPR Cookie Consent plugin. The cookie is used to store the user consent for the cookies in the category "Other.&lt;/p&gt;
    &lt;p&gt;cookielawinfo-checkbox-performance&lt;/p&gt;
    &lt;p&gt;11 months&lt;/p&gt;
    &lt;p&gt;This cookie is set by GDPR Cookie Consent plugin. The cookie is used to store the user consent for the cookies in the category "Performance".&lt;/p&gt;
    &lt;p&gt;viewed_cookie_policy&lt;/p&gt;
    &lt;p&gt;11 months&lt;/p&gt;
    &lt;p&gt;The cookie is set by the GDPR Cookie Consent plugin and is used to store whether or not user has consented to the use of cookies. It does not store any personal data.&lt;/p&gt;
    &lt;p&gt;Functional cookies help to perform certain functionalities like sharing the content of the website on social media platforms, collect feedbacks, and other third-party features.&lt;/p&gt;
    &lt;p&gt;Performance cookies are used to understand and analyze the key performance indexes of the website which helps in delivering a better user experience for the visitors.&lt;/p&gt;
    &lt;p&gt;Analytical cookies are used to understand how visitors interact with the website. These cookies help provide information on metrics the number of visitors, bounce rate, traffic source, etc.&lt;/p&gt;
    &lt;p&gt;Advertisement cookies are used to provide visitors with relevant ads and marketing campaigns. These cookies track visitors across websites and collect information to provide customized ads.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45359378</guid><pubDate>Wed, 24 Sep 2025 12:30:59 +0000</pubDate></item><item><title>My Ed(1) Toolbox</title><link>https://aartaka.me/my-ed.html</link><description>&lt;doc fingerprint="f768818853b18a8d"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;My ed(1) Toolbox&lt;/head&gt;By Artyom Bologov&lt;p&gt;Apparently, I‚Äôm a huge ed(1) fan. I keep posting about it and use it as e.g. my Git editor, sudo editing tool, and my static site generator. But am I using it raw and standard as it is? Sometimes yes, but mostly no. This post is a listing of all the ed implementations and scripts I use.&lt;/p&gt;&lt;head rend="h2"&gt;GNU ed + red‚ÄîEternal Classics #&lt;/head&gt;&lt;p&gt;ed(1) is the standard text editor. And it‚Äôs available on most UNIX/POSIX-derived systems. (Some Linux distributions don‚Äôt provide it in default installation anymore, but that‚Äôs on them!) So relying on ed(1) and its powers is a good bet.&lt;/p&gt;&lt;p&gt;That‚Äôs why I always have GNU ed installed on my systems. It‚Äôs battle-tested, intuitive, and easily scriptable.&lt;/p&gt;&lt;p&gt;Bundled with GNU ed (and any POSIX-compliand ed(1) really), red(1) is the ‚Äúrestricted‚Äù version of ed(1). It‚Äôs locked to the directory it‚Äôs called in. And has no ability to pass through to the shell. I find it relatively useless though: I use ed(1) on secure systems and never allow anyone to access my precious ed(1) session. But still, red(1) is nice to have!&lt;/p&gt;&lt;head rend="h2"&gt;oed‚ÄîOpenBSD ed #&lt;/head&gt;&lt;p&gt;Now, GNU ed is not conforming to POSIX in some behaviors:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;It has more CLI flags&lt;/item&gt;&lt;item&gt; It has &lt;code&gt;wq&lt;/code&gt;&lt;/item&gt;&lt;item&gt;It has POSIX extended regular expressions (EREs,) while most other implementations don‚Äôt. Thus making EREs a non-portable extension&lt;/item&gt;&lt;/list&gt;&lt;p&gt;I risk introducing non-portable behavior if I only focus on GNU ed. And I want to have my scripts (including my website build scripts) portable across implementations.&lt;/p&gt;&lt;p&gt; So I installed OpenBSD ed from the repository kindly provided by one of the maintainers. And now I can safely replace &lt;code&gt;ed&lt;/code&gt; with &lt;code&gt;oed&lt;/code&gt; for most of my scripts.
As the least effort shot at portability.
It‚Äôs too convenient to not use it now.

&lt;/p&gt;&lt;head rend="h2"&gt;wed‚Äîed wImproved #&lt;/head&gt;&lt;p&gt; I asked it on GNU ed mailing list whether they might support scripting abilities. Like sed(1) &lt;code&gt;-e&lt;/code&gt; and &lt;code&gt;-f&lt;/code&gt; flags or as a special executable for it.
One of the maintainers (predictably) replied that they (mostly) abide by POSIX and won‚Äôt add it.

&lt;/p&gt;&lt;p&gt;But! there was a person that emailed me personally and recommended slewsys ed as a version of ed(1) supporting scripts (among many other things.) So I installed it and called it wed(1) just to distinguish this re-implementation from The ed(1).&lt;/p&gt;&lt;p&gt;I don‚Äôt really use wed(1)‚ÄîI‚Äôm fine with standard ed(1) (and my scripting wrapper for it). Bust still, it‚Äôs a modern and user-friendly extension. Want to get started with ed(1) but don‚Äôt want to deviate from the tradition?‚Äîthis is the one to start with, probably.&lt;/p&gt;&lt;p&gt;Don‚Äôt get wed to an anime hologram. Get wed(1).&lt;/p&gt;&lt;head rend="h2"&gt;aed‚ÄîBlaphemy Against Minimalism #&lt;/head&gt;&lt;p&gt;I understand the complaints about ed(1) being somewhat hostile to new users. It‚Äôs usually mitigated by the time spend with this magnificent software. But still, ed(1) is not perfect and might need some modernization.&lt;/p&gt;&lt;p&gt;So I made aed(1) as a better and more interactive ed(1). It‚Äôs mostly abusing Readline and shell scripts to deliver a friendlier experience. With syntax highlighting and perfectly inline-editable inputs.&lt;/p&gt;&lt;p&gt;So once you‚Äôre comfortable with basic ed(1). (Or it‚Äôs slightly friendlier wed(1) version.) You might want to speed up your workflows with aed(1)!&lt;/p&gt;&lt;p&gt;I might‚Äôve gone too far though.&lt;/p&gt;&lt;head rend="h2"&gt;xed‚ÄîYou Don‚Äôt Need sed #&lt;/head&gt;&lt;p&gt;I have a user-friendly ed(1) on me now for interactive use. One use-case for ed(1) is not covered yet though‚Äîscripting! Having to do this type of newline-delimited scripts is too verbose compared to sed(1) ones.&lt;/p&gt;&lt;p&gt;What if I told you this is doable with a one-liner using my xed(1) script? Here:&lt;/p&gt;&lt;p&gt;It‚Äôs not the prettiest one, but it‚Äôs fulfilling many sed(1) use-cases. Speaking of the devil...&lt;/p&gt;&lt;head rend="h2"&gt;sed and ex... No. #&lt;/head&gt;&lt;p&gt;You don‚Äôt need ex(1) either, because it‚Äôs too vi(1)-oriented. They promised ed(1) eXtended, but we got ed(1) Fucked Up. Commands are incompatible with ed(1). Configuration is useless in ex(1) mode. Overall, ex(1) is just a poorly integrated back-end for vi(1).&lt;/p&gt;&lt;head rend="h2"&gt;My own ed(1) implementations #&lt;/head&gt;&lt;p&gt;If one likes some piece of software as an idea, they will inevitably try to reproduce it. So I did. I implemented ed(1) in Brainfuck under the aegis of Brainfuck Enterprise Solutions. I also did one in BASIC, pushing the limits of the no-memory BASIC as far as possible. And, finally, I did ed(1) in Modal, a term-rewriting-only system. All of these are useable... to a certain extent. But they don‚Äôt compare to the magnificence and purity of the Standard Text Editor.&lt;/p&gt;&lt;head rend="h2"&gt;Use ed(1) #&lt;/head&gt;&lt;p&gt;Whatever implementation you pick (pick aed(1)!), use it and love it. Because ed(1) deserves your love üòå&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45359388</guid><pubDate>Wed, 24 Sep 2025 12:31:52 +0000</pubDate></item><item><title>Learning Persian with Anki, ChatGPT and YouTube</title><link>https://cjauvin.github.io/posts/learning-persian/</link><description>&lt;doc fingerprint="9290b055cdb636d2"&gt;
  &lt;main&gt;
    &lt;p&gt;I‚Äôve been learning Persian (Farsi) for a while now, and I‚Äôm using a bunch of tools for it. The central one is certainly Anki, a spaced repetition app to train memory. I‚Äôm creating my own never-ending deck of cards, with different types of content, for different purposes. The most frequent type of cards is grammar focused phrases (very rarely single words) coming sometimes from my own daily life, but also very often directly from videos of the Persian Learning YouTube channel, created by Majid, a very talented and nice Persian teacher, in my opinion.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs take an example, suppose there is this slide in one of Majid‚Äôs videos:&lt;/p&gt;
    &lt;p&gt;From this, I will extract three screenshots (with the MacOS screenshot tool). First, to create a card of type ‚Äúbasic‚Äù (one side). I use this type of card to exercise my reading, which is very difficult and remains stubbornly slow, even though I know the 32 letters of the Persian alphabet quite well by now. But the different ways of writing them (which varies by their position in the word) and the fact that the vowels are not present makes it an enduringly challenging task.&lt;/p&gt;
    &lt;p&gt;The next type of card I create with the two remaining screenshots is ‚Äúbasic and reversed‚Äù, which actually creates two cards (one for each direction), one with some romanized phrase, and the other with the English or French translation:&lt;/p&gt;
    &lt;p&gt;When I review these cards in my daily Anki routine, this is where ChatGPT enters into play. First I have set a ‚ÄúPersian‚Äù project with these instructions:&lt;/p&gt;
    &lt;p&gt;With this project, every time I have a doubt or don‚Äôt remember something in Anki, I just take a screenshot and paste it in the project:&lt;/p&gt;
    &lt;p&gt;With this, I have an instant refresher on any notion, in any context. Sometimes I need to do this over and over, before it gels into a deeper, more instant and visceral ‚Äúknowledge‚Äù.&lt;/p&gt;
    &lt;p&gt;The next set of techniques is also based on YouTube. I use a Chrome extension called Dual Subtitles (which only works of course with videos having actual dual sources of subtitles):&lt;/p&gt;
    &lt;p&gt;The dual subtitles serve a couple of purposes: first as a source of new Anki cards (I create the cards directly, again with screenshots in the clipboard).&lt;/p&gt;
    &lt;p&gt;I also use the Tweaks for YouTube extension, which allows me to get extra keyboard shortcuts, to go back and forward only 1 second, instead of the built-in 5 seconds.&lt;/p&gt;
    &lt;p&gt;With these YouTube extensions, I have developed this particular ‚Äútechnique‚Äù to improve my vocal understanding:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;I listen at 75% speed&lt;/item&gt;
      &lt;item&gt;I use the ‚Äúdual subtitles‚Äù browser extension to have both the Farsi and English subtitles at the same time (I set the Farsi one slightly bigger)&lt;/item&gt;
      &lt;item&gt;Every time a new sentence appears, I read it very quickly first in English (I pause if I need to), and then I listen carefully to the voice, to let the meaning and sound of Farsi infuse my mind (this part is very subtle but the most important: you must ‚Äúfeel‚Äù that you understand, and this feeling must cover even the words that you don‚Äôt know; because the meaning of the sentence is currently present and active in your mind, because you just read the English part, I believe that its mapping with the Farsi words that you then hear is particularly efficient, at least that‚Äôs my theory)&lt;/item&gt;
      &lt;item&gt;I also read the Farsi script, to improve my understanding, and disambiguate certain words for which it‚Äôs hard for me to hear what is exactly said&lt;/item&gt;
      &lt;item&gt;I repeat out loud what has been said also, which is quite important&lt;/item&gt;
      &lt;item&gt;Most importantly: I repeat this process (for a single video) over and over, in order to reach a stage where I genuinely understand what is said, in real-time, which is a very powerful and exhilarating feeling.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45359524</guid><pubDate>Wed, 24 Sep 2025 12:45:07 +0000</pubDate></item><item><title>How to Lead in a Room Full of Experts</title><link>https://idiallo.com/blog/how-to-lead-in-a-room-full-of-experts</link><description>&lt;doc fingerprint="f3751d93156404b7"&gt;
  &lt;main&gt;
    &lt;p&gt;Here is a realization I made recently. I'm sitting in a room full of smart people. On one side are developers who understand the ins and outs of our microservice architecture. On the other are the front-end developers who can debug React in their sleep. In front of me is the product team that has memorized every possible user path that exists on our website. And then, there is me. The lead developer. I don't have the deepest expertise on any single technology.&lt;/p&gt;
    &lt;p&gt;So what exactly is my role when I'm surrounded by experts? Well, that's easy. I have all the answers.&lt;/p&gt;
    &lt;head rend="h2"&gt;Technical Leadership&lt;/head&gt;
    &lt;p&gt;OK. Technically, I don't have all the answers. But I know exactly where to find them and connect the pieces together.&lt;/p&gt;
    &lt;p&gt;When the backend team explains why a new authentication service would take three weeks to build, I'm not thinking about the OAuth flows or JWT token validation. Instead, I think about how I can communicate it to the product team who expects it done "sometime this week." When the product team requests a "simple" feature, I'm thinking about the 3 teams that need to be involved to update the necessary microservices.&lt;/p&gt;
    &lt;p&gt;Leadership in technical environments isn't about being the smartest person in the room. It's about being the most effective translator.&lt;/p&gt;
    &lt;head rend="h3"&gt;Leading is a Social Skill&lt;/head&gt;
    &lt;p&gt;I often get "eye rolls" when I say this to developers: You are not going to convince anyone with facts. In a room full of experts, your technical credibility gets you a seat at the table, but your social skills determine whether anything productive happens once you're there.&lt;/p&gt;
    &lt;p&gt;Where ideally you will provide documentation that everyone can read and understand, in reality, you need to talk to get people to understand. People can get animated when it comes to the tools they use. When the database team and the API team are talking past each other about response times, your role isn't to lay down the facts. Instead it's to read the room and find a way to address technical constraints and unclear requirements. It means knowing when to let a heated technical debate continue because it's productive, and when to intervene because it's become personal.&lt;/p&gt;
    &lt;head rend="h3"&gt;Leading is Remembering the Goal&lt;/head&gt;
    &lt;p&gt;When you are an expert in your field, you love to dive deep. It's what makes you experts. But someone needs to keep one eye on the forest while everyone else is examining the trees.&lt;/p&gt;
    &lt;p&gt;I've sat through countless meetings where engineers debated the merits of different caching strategies while the real issue was that we hadn't clearly defined what "fast enough" meant for the user experience. The technical discussion was fascinating, but it wasn't moving us toward shipping.&lt;/p&gt;
    &lt;p&gt;As a leader, your job isn't to have sophisticated technical opinions. It's to ask how this "discussion" can move us closer to solving our actual problem.&lt;/p&gt;
    &lt;p&gt;When you understand a problem, and you have a room full of experts, the solution often emerges from the discussion. But someone needs to clearly articulate what problem we're actually trying to solve.&lt;/p&gt;
    &lt;p&gt;When a product team says customers are reporting the app is too slow, that's not a clear problem. It's a symptom. It might be that users are not noticing when the shopping cart is loaded, or that maybe we have an event that is not being triggered at the right time. Or maybe the app feels sluggish during peak hours. Each of those problems has different solutions, different priorities, and different trade-offs. Each expert might be looking at the problem with their own lense, and may miss the real underlying problem.&lt;/p&gt;
    &lt;p&gt;Your role as a leader is to make sure the problem is translated in a way the team can clearly understand the problem.&lt;/p&gt;
    &lt;head rend="h3"&gt;Leading is Saying "I Don't Know"&lt;/head&gt;
    &lt;p&gt;By definition, leading is knowing the way forward. But in reality, in a room full of experts, pretending to know everything makes you look like an idiot.&lt;/p&gt;
    &lt;p&gt;Instead, "I don't know, but let's figure it out" becomes a superpower. It gives your experts permission to share uncertainty. It models intellectual humility. And it keeps the focus on moving forward rather than defending ego. It's also an opportunity to let your experts shine.&lt;/p&gt;
    &lt;p&gt;Nothing is more annoying than a lead who needs to be the smartest person in every conversation. Your database expert spent years learning how to optimize queries - let them be the hero when performance issues arise. Your security specialist knows threat models better than you, give them the floor when discussing architecture decisions.&lt;/p&gt;
    &lt;p&gt;Make room for some productive discussion. When two experts disagree about implementation approaches, your job isn't to pick the "right" answer. It's to help frame the decision in terms of trade-offs, timeline, and user impact.&lt;/p&gt;
    &lt;p&gt;Your value isn't in having all the expertise. It's in recognizing which expertise is needed when, and creating space for the right people to contribute their best work.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Translation Challenge&lt;/head&gt;
    &lt;p&gt;There was this fun blog post I read recently about how non-developers read tutorials written by developers. What sounds natural to you, can be complete gibberish to someone else. As a lead, you constantly need to think about your audience. You need to learn multiple languages to communicate the same thing:&lt;/p&gt;
    &lt;p&gt;Developer language: "The authentication service has a dependency on the user service, and if we don't implement proper circuit breakers, we'll have cascading failures during high load."&lt;/p&gt;
    &lt;p&gt;Product language: "If our login system goes down, it could take the entire app with it. We need to build in some safeguards, which will add about a week to the timeline but prevent potential outages."&lt;/p&gt;
    &lt;p&gt;Executive language: "We're prioritizing system reliability over feature velocity for this sprint. This reduces risk of user-facing downtime that could impact revenue."&lt;/p&gt;
    &lt;p&gt;All three statements describe the same technical decision, but each is crafted for its audience. Your experts shouldn't have to learn product speak, and your product team shouldn't need to understand circuit breaker patterns. But someone needs to bridge that gap.&lt;/p&gt;
    &lt;head rend="h2"&gt;Beyond "Because, that's why!"&lt;/head&gt;
    &lt;p&gt;"I'm the lead, and we are going to do it this way." That's probably the worst way to make a decision. That might work in the short term, but it erodes trust and kills the collaborative culture that makes expert teams thrive.&lt;/p&gt;
    &lt;p&gt;Instead, treat your teams like adults and communicate the reason behind your decision:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;"We're choosing the more conservative approach because the cost of being wrong is high, and we can iterate later."&lt;/item&gt;
      &lt;item&gt;"I know this feels like extra work, but it aligns with our architectural goals and will save us time on the next three features."&lt;/item&gt;
      &lt;item&gt;"This isn't the most elegant solution, but it's the one we can ship confidently within our timeline."&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The more comfortable you become with not being the expert, the more effective you become as a leader.&lt;/p&gt;
    &lt;p&gt;When you stop trying to out-expert the experts, you can focus on what expert teams actually need:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Clear problem definitions&lt;/item&gt;
      &lt;item&gt;Context for decision-making&lt;/item&gt;
      &lt;item&gt;Translation between different perspectives&lt;/item&gt;
      &lt;item&gt;Protection from unnecessary complexity&lt;/item&gt;
      &lt;item&gt;Space to do their best work&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Your role isn't to have all the answers. It's to make sure the right questions get asked, the right people get heard, and the right decisions get made for the right reasons.&lt;/p&gt;
    &lt;p&gt;Technical leadership in expert environments is less about command and control, and more about connection and context. You're not the conductor trying to play every instrument. You're the one helping the orchestra understand what song they're playing together.&lt;/p&gt;
    &lt;p&gt;That's a much more interesting challenge than trying to be the smartest person in the room.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45359604</guid><pubDate>Wed, 24 Sep 2025 12:52:52 +0000</pubDate></item><item><title>Just Let Me Select Text</title><link>https://aartaka.me/select-text.html</link><description>&lt;doc fingerprint="cb024a5731ccf78e"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Just Let Me Select Text&lt;/head&gt;By Artyom Bologov&lt;head rend="h2"&gt;Untranslatable Bios #&lt;/head&gt;&lt;p&gt;I‚Äôm lonely. Like everyone-ish else. Naturally, I‚Äôm on Bumble. (Because Tinder is a rape-friendly lure trap.) When work calls get boring I inevitably start swiping (mostly left üò¢)&lt;/p&gt;&lt;p&gt;There are lots of tourists in Armenia in the summer. From all over the world really. Speaking a stupefying range of languages. With bios and prompt answers in these numerous languages. Not necessarily discernible to me due to my language learning stagnation.&lt;/p&gt;&lt;p&gt;So there‚Äôs this profile of a pretty German girl. With bio and prompts in (an undeniably beautiful) German. Speaking English, she made the decision to use her mother tongue for the bio. A totally valid choice.&lt;/p&gt;&lt;p&gt;So I want to know the story she tells with her profile:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Select her bio,&lt;/item&gt;&lt;item&gt;copy it,&lt;/item&gt;&lt;item&gt;paste into a translator,&lt;/item&gt;&lt;item&gt;look up the exact meaning of some mistranslated German word,&lt;/item&gt;&lt;item&gt;and realize the unexpected poetic meaning she put into these 300 chars.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Except‚Ä¶ I can‚Äôt do that. The text is not selectable/copyable in Bumble app. I have to do a bunch of relatively unsurmountable steps to do what should‚Äôve taken half a minute. Like screenshot the profile and scrape the text with iOS Photos text recognition. Or use some OCR (web)app elsewhere. It‚Äôs‚Ä¶ discouraging. Thus I give up and swipe left. A shame‚Äîshe was beautiful at the very least!&lt;/p&gt;&lt;head rend="h2"&gt;Media #&lt;/head&gt;&lt;p&gt;By making the text in your UI non-selectable, you turn it into‚Ä¶ an image essentially? Images, audio, video, and interactive JS-heavy pages are multidimentional media. Not really manipulable and referenceable in any reasonable way. (Not even with Media Fragments‚Äîthey were turned down by everyone.) You lose a whole dimension (ü•Å) of functionality and benefit by going with such media or their semblance text.&lt;/p&gt;&lt;p&gt; Podcasts are not easy to roll back to useful part. Video transcripts don‚Äôt make sense without the visuals. Web graphics are opaque &lt;code&gt;&amp;lt;canvas&amp;gt;&lt;/code&gt;-es you can‚Äôt gut.

&lt;/p&gt;&lt;p&gt;Text is copyable. Text is translatable. Text is accessible (as in a11y.) Text is lightweight. Text is fundamental to how we people process information.&lt;/p&gt;&lt;p&gt;That‚Äôs why we still use text in our UIs. We want to convey the meaning. We strive to provide unambiguous instructions. We need to be understood. So why make the text harder to process and understand?&lt;/p&gt;&lt;head rend="h2"&gt;Stop It #&lt;/head&gt;&lt;p&gt;Whenever you disable text selection/copying on your UI, you commit a crime against the user. Crime against comprehension. Crime against accessibility. Crime against the meaning. Stop incapacitating your users, allow them to finally use the text.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45360475</guid><pubDate>Wed, 24 Sep 2025 13:56:37 +0000</pubDate></item><item><title>Smartphone Cameras Go Hyperspectral</title><link>https://spectrum.ieee.org/hyperspectral-imaging</link><description>&lt;doc fingerprint="77951f9c0a9da747"&gt;
  &lt;main&gt;
    &lt;p&gt;The human eye is mostly sensitive to only three bands of the electromagnetic spectrum‚Äîred, green, and blue (RGB)‚Äîin the visible range. In contrast, off-the-shelf smartphone camera sensors are potentially hyperspectral in nature, meaning that each pixel is sensitive to far more spectral bands. Now scientists have found a simple way for any conventional smartphone camera to serve as a hyperspectral sensor‚Äîby placing a card with a chart on it within its view. The new patent-pending technique may find applications in defense, security, medicine, forensics, agriculture, environmental monitoring, industrial quality control, and food and beverage quality analysis, the researchers add.&lt;/p&gt;
    &lt;p&gt;‚ÄúAt the heart of this work is a simple but powerful idea‚Äîa photo is never just an image,‚Äù says Semin Kwon, a postdoctoral research associate of biomedical engineering Purdue University in West Lafayette, Ind. ‚ÄúEvery photo carries hidden spectral information waiting to be uncovered. By extracting it, we can turn everyday photography into science.‚Äù&lt;/p&gt;
    &lt;p&gt;Using a smartphone camera and a spectral color chart, researchers can image the transmission spectrum of high-end whiskey, thus determining its authenticity. Semin Kwon/Purdue University&lt;/p&gt;
    &lt;p&gt;Every molecule has a unique spectral signature‚Äîthe degree to which it absorbs or reflects each wavelength of light. The extreme sensitivity to distinguishing color seen in scientific-grade hyperspectral sensors can help them identify chemicals based on their spectral signatures, for applications in a wide range of industries, such as medical diagnostics, distinguishing authentic versus counterfeit whiskey, monitoring air quality, and nondestructive analysis of pigments in artwork, says Young Kim, a professor of biomedical engineering at Purdue.&lt;/p&gt;
    &lt;p&gt;Previous research has pursued a number of different ways to recover spectral details from conventional smartphone RGB camera data. However, machine learning models developed for this purpose typically rely heavily on the task-specific data on which they are trained. This limits their generalizability and makes them susceptible to errors resulting from variations in lighting, image file formats, and more. Another possible avenue involved special hardware attachments, but these can prove expensive and bulky.&lt;/p&gt;
    &lt;p&gt;In the new study, the scientists designed a special color reference chart that can be printed on a card. They also developed an algorithm that can analyze smartphone pictures taken with this card and account for factors such as lighting conditions. This strategy can extract hyperspectral data from raw images with a sensitivity of 1.6 nanometers of difference in wavelength of visible light, comparable to scientific-grade spectrometers.&lt;/p&gt;
    &lt;p&gt;‚ÄúIn short, this technique could turn an ordinary smartphone into a pocket spectrometer,‚Äù Kim says.&lt;/p&gt;
    &lt;p&gt;The scientists are currently pursuing applications for their new technique in digital and mobile-health applications in both domestic and resource-limited settings. ‚ÄúWe are truly excited that this opens the door to making spectroscopy both affordable and accessible,‚Äù Kwon says.&lt;/p&gt;
    &lt;p&gt;The scientists recently detailed their findings in the journal IEEE Transactions on Image Processing.&lt;/p&gt;
    &lt;p&gt;Charles Q. Choi is a science reporter who contributes regularly to IEEE Spectrum. He has written for Scientific American, The New York Times, Wired, and Science, among others.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45360824</guid><pubDate>Wed, 24 Sep 2025 14:20:33 +0000</pubDate></item><item><title>How HubSpot Scaled AI Adoption</title><link>https://product.hubspot.com/blog/context-is-key-how-hubspot-scaled-ai-adoption</link><description>&lt;doc fingerprint="a5420315f9e5e959"&gt;
  &lt;main&gt;
    &lt;p&gt;This post is intended to be the first in a series about empowering product, UX, and engineering teams with AI. We‚Äôre going to focus on how we‚Äôve approached and scaled the use of AI in the context of writing code.&lt;/p&gt;
    &lt;p&gt;AI has fundamentally transformed how we build software at HubSpot. Over the past two years, we've gone from cautious experimentation to achieving near universal adoption of AI coding tools across our engineering organization.&lt;/p&gt;
    &lt;p&gt;This transformation didn't happen overnight. It required strategic investment, organizational commitment, and a willingness to learn. As we've shared our experience with other engineering leaders, we've discovered that many teams are facing similar challenges in scaling AI adoption beyond POCs and early adopters. The conversations we've had with external teams convinced us that our lessons learned could help others navigate this journey more effectively.&lt;/p&gt;
    &lt;p&gt;Adoption of AI coding assistants, % of members in the Engineering organization [sanitized data]&lt;/p&gt;
    &lt;head rend="h1"&gt;In the beginning there was code completion&lt;/head&gt;
    &lt;p&gt;We began experimenting with GitHub Copilot in the Summer of 2023. Our founders Dharmesh and Brian provided us with the push we needed to get started. Dharmesh had recently used GitHub Copilot to build ChatSpot and had a good experience with it, so he and Brian pushed us to evaluate it and connected us with other leaders in the industry who were seeing success.&lt;/p&gt;
    &lt;p&gt;Our proof of concept (POC) successfully validated the tool's potential, and several factors contributed to our success:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Executive buy-in made everything else easier. Support from Dharmesh and Brian accelerated our pilot process significantly. This helped our legal, security, and engineering team have the same goal and urgency for making this happen.&lt;/item&gt;
      &lt;item&gt;We ran a pilot that was sufficiently large: Our strategy was to include entire teams so they could adopt and learn together that had different experience levels, different missions, and worked in different domains. We gave teams over two months to try it..&lt;/item&gt;
      &lt;item&gt;We put energy into enablement. We had setup/training sessions and created a channel where people could ask questions and share what is and is not working.&lt;/item&gt;
      &lt;item&gt;We measured everything. We applied our existing engineering velocity measurement methods to the pilot. This helped us check our biases. We were skeptical at the outset but seeing measured impact chipped away at our skepticism.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The initial results were encouraging: positive qualitative feedback and measurable but modest productivity improvements, across engineers of different tenure and seniority. Our initial gains fell short of some extraordinary claims we were hearing in the market, but they were still significant given Copilot's cost structure of $19/mo/business user at the time. Even modest time savings justified the investment.&lt;/p&gt;
    &lt;p&gt;With a group of committed stakeholders seeing the early value and the potential with the tool, we were willing to be patient and continue our investment. We believed the technology would only improve over time, so we rolled it out with guardrails. As we scaled adoption and people gained more experience with it, we saw increasingly meaningful productivity gains.&lt;/p&gt;
    &lt;head rend="h1"&gt;Leveraging the Power of Central Teams&lt;/head&gt;
    &lt;p&gt;At HubSpot, we've long believed in the leverage that central teams create. Our platform teams build infrastructure, tools, and guardrails that enable small autonomous product teams to move fast while maintaining quality and consistency.&lt;/p&gt;
    &lt;p&gt;When generative AI emerged, we initially relied on teams adjacent to these areas (specifically teams that managed our GitHub setup) to drive adoption. But as demand exploded, the backlog grew exponentially. We realized it was time to create a dedicated team.&lt;/p&gt;
    &lt;p&gt;We created a Developer Experience AI team in October 2024, with an initial focus on:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Driving adoption of coding tools: Once we realized the impact these tools were having, we wanted the entire org on board as soon as possible&lt;/item&gt;
      &lt;item&gt;Increasing the impact of AI tools: HubSpot has a very opinionated stack and we wanted our generated code to reflect these opinions as much as possible. This started very simply with the sharing of Cursor rules files, but quickly evolved to more complex tools that gave agents deep context about our architecture, libraries, and best practices. (More to come on this in the future)&lt;/item&gt;
      &lt;item&gt;Advocacy: We wanted to build a community around AI, by collecting and disseminating what was working for people. We created an open forum for people to post about AI and seeded content to drive engagement. We saw a vibrant community slowly spring up as adoption grew.&lt;/item&gt;
      &lt;item&gt;Adapting procurement for speed: We knew we wanted to try every tool that came out, but our purchasing processes were designed for longer term negotiated agreements and we couldn't always count on a push from our founders to get things moving. We wanted month-to-month contracts and to get started ASAP.&lt;/item&gt;
      &lt;item&gt;Building evaluation capabilities: We didn't want to rely solely on qualitative feedback, so we came up with ways to run pilots and compare tools on merit. We also experienced first-hand how empirical data could combat preconceptions and skepticism.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Central infrastructure teams create leverage for product teams in every facet of their daily work. AI is no different. We started very small with just two people who had infrastructure experience and were already highly engaged with AI. The team grew over time as we branched out into more advanced use cases, many of which we'll cover in this series. But creating the team and focusing these engineers paved the way for our future success without a massive investment to get started.&lt;/p&gt;
    &lt;head rend="h1"&gt;Tipping the Scale&lt;/head&gt;
    &lt;p&gt;As engineers adopted the tools and we collected more data, our conviction grew that these tools would have had a positive impact on our engineering team. Initially, we maintained conservative usage rules due to limited experience and cost concerns. Users had to request a license and agree to follow strict guardrails.&lt;/p&gt;
    &lt;p&gt;We pulled metrics on code review burden, cycle time, velocity comparisons before and after adoption, and production incident rates.&lt;/p&gt;
    &lt;p&gt;Impact of AI adoption on incidents, team level data [sanitized]&lt;/p&gt;
    &lt;p&gt;The data consistently showed the same thing: AI adoption wasn't creating the problems we were initially worried about. The scatter plot above shows one example, showing that there was no correlation between AI adoption and production incidents.&lt;/p&gt;
    &lt;p&gt;In May 2024, we ditched the restrictions. Then we proactively gave everyone a seat, making it as easy as possible to get started. Adoption shot above 50% overnight.&lt;/p&gt;
    &lt;head rend="h1"&gt;Reaching the Late Majority&lt;/head&gt;
    &lt;p&gt;Adoption slowed again as it increased beyond 60%. The latter stages of adoption are where you face skeptics, start to better understand the limitations of the current tools, and see higher levels of change/risk aversion, so we had to change our approach:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Peer validation: Whenever we heard someone did something interesting with AI, we asked them to record a video and share it. We also began recording weekly videos ourselves showing new features and real usage.&lt;/item&gt;
      &lt;item&gt;Quantitative proof: We shared high-level data showing that most people were already using these tools successfully and safely. We deliberately kept the numbers broad rather than getting into precise details. While data was important for making decisions, we wanted people to focus on the clear trend of improvement rather than getting stuck debating exact figures.&lt;/item&gt;
      &lt;item&gt;Provide better tools: We ran POCs for multiple coding assistants to give engineers more options, recognizing that different tools work better for different workflows and preferences.&lt;/item&gt;
      &lt;item&gt;Curated experience: We transparently set up a local MCP server on every machine with default rules and configuration optimized for our development environment. This gave every engineer an experience tailored to our specific stack and best practices right out of the box. We continue to revise and improve this setup over time based on what we learn about effective usage patterns.&lt;/item&gt;
      &lt;item&gt;AI fluency a baseline expectation: Once we hit 90% adoption, we made AI fluency a baseline expectation for engineers by adding it to job descriptions and hiring expectations. By this point, it was easy to see that AI fluency wasn‚Äôt just the right thing for HubSpot, but for engineers it was a necessary investment as they continue to grow in their careers through this transformation. This helped us clearly commit internally and externally to the investment.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Adoption was the beginning and opened the door to everything that followed: taking advantage of coding agents, creating Sidekick (our AI assistant that answers platform questions, creates issues, implements changes, and reviews PRs), developing a way to rapidly prototype UIs with our design system, and building infrastructure that led to 400+ tools that our agents can leverage across our internal, OpenAI, and Anthropic MCP servers.&lt;/p&gt;
    &lt;p&gt;Next: How we transitioned to agentic coding&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45361140</guid><pubDate>Wed, 24 Sep 2025 14:42:49 +0000</pubDate></item><item><title>Who Funds Misfit Research?</title><link>https://blog.spec.tech/p/who-funds-misfit-research</link><description>&lt;doc fingerprint="b85d134d3d65b6c8"&gt;
  &lt;main&gt;
    &lt;p&gt;This piece is an addition to our Research Leader‚Äôs Playbook. We realized that (to our knowledge) nobody had unpacked where the money for ‚Äúmisfit research‚Äù ‚Äî work that is a poor fit for academia, startups, or large companies ‚Äî was coming from. If you are already deep in this world, you probably know all of this already, but it may still be worth a skim in for something that might surprise you.&lt;/p&gt;
    &lt;p&gt;Funding preferences and situations can change quickly, so if any of this is incorrect or incomplete, please leave a note in the comments!&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;Unsurprisingly, there is no default way to fund misfit research: support can range from a group of philanthropists starting a new institute, to DARPA running robot competitions, to DAOs funding longevity projects, to VCs funding research projects gussied up as a company.&lt;/p&gt;
    &lt;p&gt;To get our brains around the funding landscape, it‚Äôs useful to divide this funding into non-dilutive (funding that comes without ownership or expectation of financial return) and dilutive (funding that comes with an expectation of financial return and often involves some ownership of an organization). This division is useful because, in broad strokes, non-dilutive and dilutive funding come with very different expectations, evaluation criteria, and ‚Äúsales‚Äù processes.&lt;/p&gt;
    &lt;p&gt;Be aware that these categories have a lot of fuzziness (like many things in non-traditional research). Several entities, like family offices, do both dilutive and non-dilutive funding; they have their own section. Furthermore, non-dilutive and dilutive funding are not always mutually exclusive: some technology projects get off the ground with a mix of non-dilutive grants from foundations or governments and investments from angel or impact investors. (There are still a lot of gaps in this ‚Äúmessy middle‚Äù between pure-public-goods work and profit-maximizing company.)&lt;/p&gt;
    &lt;p&gt;Below are the major groups in each category and what they're actually funding. The end of this section touches on what to actually do with this information when you‚Äôre trying to fund research.&lt;/p&gt;
    &lt;head rend="h2"&gt;Non-Dilutive Funders&lt;/head&gt;
    &lt;p&gt;Non-dilutive funding doesn‚Äôt come with any expectations of repayment or organizational ownership. This sort of funding is important for research that is a poor financial investment, whether because it will never create capturable value, or has long timescales and high uncertainty. However, non-dilutive funding isn‚Äôt just ‚Äúfree‚Äù money. Raising non-dilutive funding usually takes significantly more time and effort than the equivalent amount of investment dollars; most funders impose much more process up front and restrictions on how money can be spent.&lt;/p&gt;
    &lt;p&gt;Foundations: Foundations are organizations with full-time professional staff deploying money that has been set aside explicitly for philanthropic purposes. Foundations can range in size from a tiny org with one or two staff to hundreds or thousands of employees at the largest foundations like the Rockefeller or Gates Foundations.&lt;/p&gt;
    &lt;p&gt;In aggregate, foundations gave $30B towards research in 2019, which is more than the NSF and comparable to NIH.1 While these numbers are large, the median grant is significantly less than $1M. Even large foundations are usually spending money that comes from the interest on an endowment, so they may have much smaller budgets than you might expect based on the wealth of their founder or size of their endowment.&lt;/p&gt;
    &lt;p&gt;As of 2025, most research funding from foundations goes towards traditional research. The bureaucracy and processes of most foundations make it hard for them to support non-traditional work. Foundations typically deploy money through program officers who work within tight bounds set by the board of trustees on a yearly basis. Programs often have explicit mandates to work within traditional institutions through graduate fellowships or awards to professors.&lt;/p&gt;
    &lt;p&gt;There are, of course, exceptions. The now-defunct Schmidt Futures helped a number of ambitious research organizations get off the ground.&lt;/p&gt;
    &lt;p&gt;Philanthropic Aggregators: Philanthropic aggregators are organizations that use their brand and connections to fundraise for specific projects from wealthy individuals or foundations. Some examples include Renaissance Philanthropy, Founders Pledge, and XPrize. Each philanthropic aggregator has their own process and funding ‚Äúform factor‚Äù: Renaissance Philanthropy creates ‚Äúphilanthropic funds‚Äù that they use to deploy grants, while the X-Prize creates prize competitions.&lt;/p&gt;
    &lt;p&gt;Philanthropic aggregators have funded a lot of non-traditional research. The process of recruiting on a case-by-case basis makes aggregators more flexible than foundations with board-specified programs.&lt;/p&gt;
    &lt;p&gt;Government Organizations: Government is, of course, a major research funder. The vast majority of government research funding is little-c conservative and intended for traditional PI-driven academic work. However, certain agencies and programs have supported some misfit work work. &lt;lb/&gt;DARPA pioneered using Other Transaction Authority (OTA) to run prize competitions like the DARPA Grand Challenge, Urban Challenge, and Robotics Challenge. SBIR (Small Business Innovation Research) grants provide non-dilutive funding towards research-heavy startups. Recently, two British government organizations (ARIA and the Department for Science, Innovation, and Technology) announced that they were funding Focused Research Organizations.&lt;/p&gt;
    &lt;p&gt;Crowdfunding Platforms: Platforms like Experiment.com enable researchers to raise small amounts of money from a large number of people. Crowdfunding can work for non-traditional projects that have public appeal like a citizen science endeavor to catalog ocean plastics or a team building a field microscope. However, crowdfunding rarely raises amounts more than tens of thousands of dollars, so it‚Äôs suited for modest-scale projects or early seed funding.&lt;/p&gt;
    &lt;head rend="h2"&gt;Dilutive Funders&lt;/head&gt;
    &lt;p&gt;Dilutive funding can be double-edged: it injects significant resources, but it may steer the work towards shorter-term commercial goals and away from research work or even longer-term commercial goals. Professional investors need to show their investors portfolio growth and exits, which means they need companies in their portfolio to show year-over-year growth, which can be at odds with the uncertainty baked into research.&lt;/p&gt;
    &lt;p&gt;Angel Investors: Angels are individuals who invest their own money in early-stage startups. Technically, angel investment is driven by prospects of eventual financial return, but some angels think less about whether a startup is a good investment but instead about whether the work would be cool or impactful.&lt;/p&gt;
    &lt;p&gt;Venture Capitalists (VCs): The main context in which professional venture capitalists fund non-traditional work is (almost by definition) bubbles. When an area is ‚Äúhot‚Äù enough, even professional VC funds put money towards work that has no sense of how it becomes a product or a business. There are also a few VC firms that have different structures, like longer fund lifetimes, that enable them to invest differently from other firms. Some examples of VC funding into research includes most quantum computing companies, Colossal Biosciences, and Physical Intelligence.&lt;/p&gt;
    &lt;p&gt;Corporate Research: While corporate research has drastically contracted, some large companies with large margins still support exploratory research. As obvious 2025, AI research is an obvious example. Corporations rarely fund nontraditional research ‚Äì most external research funding goes towards universities primarily as a hiring pipeline. Teams can sometimes carve out a niche within corporate research to develop something ambitious ‚Äì the team that started the Lean FRO worked at Microsoft Research for a long time.&lt;/p&gt;
    &lt;p&gt;Corporate Venture: Corporate venture capital arms invest in startups based on the company‚Äôs ‚Äústrategic interest‚Äù in addition to pure returns. For example, a car company may invest in a research-heavy battery startup or a chip company may invest in a photonics company long before they have a product. Large companies sometimes acquire and continue to fund organizations that focus more on research than products: Hyundai‚Äôs acquisition of Boston Dynamics, for example.&lt;/p&gt;
    &lt;p&gt;Impact Investors: Some investors are willing to accept lower financial returns in exchange for high social or environmental impact. These investors fund for-profit ventures within some impact area (like climate or health) that don‚Äôt fit the profile for normal VC funding because of factors like time scales or capital requirements. This kind of investment is also sometimes called ‚Äúpatient capital‚Äù or ‚Äúconcessionary funding.‚Äù For example, Breakthrough Energy Ventures (BEV) explicitly operates on a 20-year timeline and invests in risky clean energy companies with the understanding that some may only yield societal benefit without huge profits. In the medical world impact investors sometimes invest in exchange for royalties or revenue sharing rather than explosive startup growth.&lt;/p&gt;
    &lt;p&gt;Program-related Investments (PRIs): Foundations and Donor Advised Funds (which we will talk more about in the next section) can make dilutive investments out of their endowments that count towards their legal deployment quotas as long as they are mission aligned. Unpacking that jargon: Foundations (but not DAFs) legally must spend 5% of their assets annually; normally this money is deployed as grants, but dilutive investments that are aligned with the foundation‚Äôs mission can also count towards that 5%. &lt;lb/&gt;Investments with the possibility of a return enable Foundations and DAFs to fulfill their charitable missions without eating as much into their bank accounts. This upside means that PRIs that do happen are often larger than normal grants and theoretically people with DAFs and Foundations should be excited to do them. However, the potential for IRS scrutiny, divisions between investment and granting teams, and DAF sponsors that don‚Äôt support PRIs means that they are fairly rare.&lt;lb/&gt;Two words of caution about funding research-heavy work with dilutive alternatives to venture capital:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;While PRIs and impact investors explicitly fund work that struggles to raise VC funding, they often focus on work that can‚Äôt raise VC funding because of characteristics like timescales, capital requirements, and market sizes, not because they are too research-heavy.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Most dilutive funding puts an organization on a trajectory where they do need to raise venture capital eventually. Many misfit research projects will never be a good fit for venture capital, so raising dilutive funding can be a trap.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Both&lt;/head&gt;
    &lt;p&gt;These entities can (but do not always) do both dilutive and non-dilutive funding.&lt;/p&gt;
    &lt;p&gt;Decentralized Autonomous Organizations (DAOs): DAOs are communities that pool funds for a common purpose by selling blockchain-based ‚Äútokens‚Äù that give their owners a say in the organization‚Äôs governance. Some DAOs, like VitaDAO or CerebrumDAO focus on research. DAOs fund work through many different mechanisms ‚Äî both traditional grants and dilutive funding as well as newer blockchain-based mechanisms for capturing some of the value that the work could create.&lt;/p&gt;
    &lt;p&gt;High-Net-Worth Individuals (HNWIs): Wealthy individuals sometimes bankroll research personally. Individuals have the most flexibility of any funders to do unconventional things. As a result, a lot of non-traditional research has been funded directly by individuals. HNWIs often fund things quietly and make themselves hard to contact for obvious reasons: everybody would be asking them for money and funding strange things can open people up to reputational risk.&lt;/p&gt;
    &lt;p&gt;Family Offices: Family offices are professional organizations that handle the money of a wealthy individual or family. The big thing that differentiates family offices from foundations is that they don‚Äôt have a pile of money that has been explicitly set aside for philanthropy. Instead, they have multiple mandates ‚Äì increase wealth, hedge against risk, maintain liquidity, and do philanthropy.&lt;/p&gt;
    &lt;p&gt;Keep in mind that people and organizations can have completely different focuses, mindsets, and processes whether they‚Äôre doing dilutive or non-dilutive funding: a wealthy individual who will write a million dollar check to a startup the same day might only donate $10k at a time only to projects focusing on a specific disease.&lt;/p&gt;
    &lt;p&gt;http://arxiv.org/abs/2206.10661&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45361154</guid><pubDate>Wed, 24 Sep 2025 14:44:20 +0000</pubDate></item><item><title>The DHS has been harvesting DNA from Americans for years</title><link>https://www.wired.com/story/dhs-has-been-collecting-us-citizens-dna-for-years/</link><description>&lt;doc fingerprint="9199219c0385031a"&gt;
  &lt;main&gt;
    &lt;p&gt;For years, Customs and Border Protection agents have been quietly harvesting DNA from American citizens, including minors, and funneling the samples into an FBI crime database, government data shows. This expansion of genetic surveillance was never authorized by Congress for citizens, children, or civil detainees.&lt;/p&gt;
    &lt;p&gt;According to newly released government data analyzed by Georgetown Law‚Äôs Center on Privacy &amp;amp; Technology, the Department of Homeland Security, which oversees CBP, collected the DNA of nearly 2,000 US citizens between 2020 and 2024 and had it sent to CODIS, the FBI‚Äôs nationwide system for policing investigations. An estimated 95 were minors, some as young as 14. The entries also include travelers never charged with a crime and dozens of cases where agents left the ‚Äúcharges‚Äù field blank. In other files, officers invoked civil penalties as justification for swabs that federal law reserves for criminal arrests.&lt;/p&gt;
    &lt;p&gt;The findings appear to point to a program running outside the bounds of statute or oversight, experts say, with CBP officers exercising broad discretion to capture genetic material from Americans and have it funneled into a law-enforcement database designed in part for convicted offenders. Critics warn that anyone added to the database could endure heightened scrutiny by US law enforcement for life.&lt;/p&gt;
    &lt;p&gt;‚ÄúThose spreadsheets tell a chilling story,‚Äù Stevie Glaberson, director of research and advocacy at Georgetown‚Äôs Center on Privacy &amp;amp; Technology, tells WIRED. ‚ÄúThey show DNA taken from people as young as 4 and as old as 93‚Äîand, as our new analysis found, they also show CBP flagrantly violating the law by taking DNA from citizens without justification.‚Äù&lt;/p&gt;
    &lt;p&gt;DHS did not respond to a request for comment.&lt;/p&gt;
    &lt;p&gt;For more than two decades, the FBI‚Äôs Combined DNA Index System, or CODIS, has been billed as a tool for violent crime investigations. But under both recent policy changes and the Trump administration‚Äôs immigration agenda, the system has become a catchall repository for genetic material collected far outside the criminal justice system.&lt;/p&gt;
    &lt;p&gt;One of the sharpest revelations came from DHS data released earlier this year showing that CBP and Immigrations and Customs Enforcement have been systematically funneling cheek swabs from immigrants‚Äîand, in many cases, US citizens‚Äîinto CODIS. What was once a program aimed at convicted offenders now sweeps in children at the border, families questioned at airports, and people held on civil‚Äînot criminal‚Äîgrounds. WIRED previously reported that DNA from minors as young as 4 had ended up in the FBI‚Äôs database, alongside elderly people in their nineties, with little indication of how or why the samples were taken.&lt;/p&gt;
    &lt;p&gt;The scale is staggering. According to Georgetown researchers, DHS has contributed roughly 2.6 million profiles to CODIS since 2020‚Äîfar above earlier projections and a surge that has reshaped the database. By December 2024, CODIS‚Äôs ‚Äúdetainee‚Äù index contained over 2.3 million profiles; by April 2025, the figure had already climbed to more than 2.6 million. Nearly all of these samples‚Äî97 percent‚Äîwere collected under civil, not criminal, authority. At the current pace, according to Georgetown Law‚Äôs estimates, which are based on DHS projections, Homeland Security files alone could account for one-third of CODIS by 2034.&lt;/p&gt;
    &lt;p&gt;The expansion has been driven by specific legal and bureaucratic levers. Foremost was an April 2020 Justice Department rule that revoked a long-standing waiver allowing DHS to skip DNA collection from immigration detainees, effectively green-lighting mass sampling. Later that summer, the FBI signed off on rules that let police booking stations run arrestee cheek swabs through Rapid DNA machines‚Äîautomated devices that can spit out CODIS-ready profiles in under two hours.&lt;/p&gt;
    &lt;p&gt;The strain of the changes became apparent in subsequent years. Former FBI director Christopher Wray warned during Senate testimony in 2023 that the flood of DNA samples from DHS threatened to overwhelm the bureau‚Äôs systems. The 2020 rule change, he said, had pushed the FBI from a historic average of a few thousand monthly submissions to 92,000 per month‚Äîover 10 times its traditional intake. The surge, he cautioned, had created a backlog of roughly 650,000 unprocessed kits, raising the risk that people detained by DHS could be released before DNA checks produced investigative leads.&lt;/p&gt;
    &lt;p&gt;Under Trump‚Äôs renewed executive order on border enforcement, signed in January 2025, DHS agencies were instructed to deploy ‚Äúany available technologies‚Äù to verify family ties and identity, a directive that explicitly covers genetic testing. This month, federal officials announced they were soliciting new bids to install Rapid DNA at local booking facilities around the country, with combined awards of up to $3 million available.&lt;/p&gt;
    &lt;p&gt;‚ÄúThe Department of Homeland Security has been piloting a secret DNA collection program of American citizens since 2020. Now, the training wheels have come off,‚Äù said Anthony Enriquez, vice president of advocacy at Robert F. Kennedy Human Rights. ‚ÄúIn 2025, Congress handed DHS a $178 billion check, making it the nation‚Äôs costliest law enforcement agency, even as the president gutted its civil rights watchdogs and the Supreme Court repeatedly signed off on unconstitutional tactics.‚Äù&lt;/p&gt;
    &lt;p&gt;Oversight bodies and lawmakers have raised alarms about the program. As early as 2021, the DHS Inspector General found the department lacked central oversight of DNA collection and that years of noncompliance can undermine public safety‚Äîechoing an earlier rebuke from the Office of Special Counsel, which called CBP‚Äôs failures an ‚Äúunacceptable dereliction.‚Äù&lt;/p&gt;
    &lt;p&gt;US senator Ron Wyden more recently pressed DHS and DOJ for explanations about why children‚Äôs DNA is being captured and whether CODIS has any mechanism to reject improperly obtained samples, saying the program was never intended to collect and permanently retain the DNA of all noncitizens, warning the children are likely to be ‚Äútreated by law enforcement as suspects for every investigation of every future crime, indefinitely.‚Äù&lt;/p&gt;
    &lt;p&gt;Rights advocates allege that CBP‚Äôs DNA collection program has morphed into a sweeping genetic surveillance regime, with samples from migrants and even US citizens fed into criminal databases absent transparency, legal safeguards, or limits on retention. Georgetown‚Äôs privacy center points out that once DHS creates and uploads a CODIS profile, the government retains the physical DNA sample indefinitely, with no procedure to revisit or remove profiles when the legality of the detention is in doubt.&lt;/p&gt;
    &lt;p&gt;In parallel, Georgetown and allied groups have sued DHS over its refusal to fully release records about the program, highlighting how little the public knows about how DNA is being used, stored, or shared once it enters CODIS.&lt;/p&gt;
    &lt;p&gt;Taken together, these revelations may suggest a quiet repurposing of CODIS. A system long described as a forensic breakthrough is being remade into a surveillance archive‚Äîsweeping up immigrants, travelers, and US citizens alike, with few checks on the agents deciding whose DNA ends up in the federal government‚Äôs most intimate database.&lt;/p&gt;
    &lt;p&gt;‚ÄúThere‚Äôs much we still don‚Äôt know about DHS‚Äôs DNA collection activities,‚Äù Georgetown‚Äôs Glaberson says. ‚ÄúWe‚Äôve had to sue the agencies just to get them to do their statutory duty, and even then they‚Äôve flouted court orders. The public has a right to know what its government is up to, and we‚Äôll keep fighting to bring this program into the light.‚Äù&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45361239</guid><pubDate>Wed, 24 Sep 2025 14:51:44 +0000</pubDate></item><item><title>The Lambda Calculus ‚Äì Stanford Encyclopedia of Philosophy</title><link>https://plato.stanford.edu/entries/lambda-calculus/</link><description>&lt;doc fingerprint="2e83231764ffdbf1"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;The Lambda Calculus&lt;/head&gt;&lt;p&gt;The \(\lambda\)-calculus is, at heart, a simple notation for functions and application. The main ideas are applying a function to an argument and forming functions by abstraction. The syntax of basic \(\lambda\)-calculus is quite sparse, making it an elegant, focused notation for representing functions. Functions and arguments are on a par with one another. The result is a non-extensional theory of functions as rules of computation, contrasting with an extensional theory of functions as sets of ordered pairs. Despite its sparse syntax, the expressiveness and flexibility of the \(\lambda\)-calculus make it a cornucopia of logic and mathematics. This entry develops some of the central highlights of the field and prepares the reader for further study of the subject and its applications in philosophy, linguistics, computer science, and logic.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;1. Introduction&lt;/item&gt;&lt;item&gt;2. Syntax&lt;/item&gt;&lt;item&gt;3. Brief history of \(\lambda\)-calculus&lt;/item&gt;&lt;item&gt;4. Reduction&lt;/item&gt;&lt;item&gt;5. \(\lambda\)-theories&lt;/item&gt;&lt;item&gt;6. Consistency of the \(\lambda\)-calculus&lt;/item&gt;&lt;item&gt;7. Semantics of \(\lambda\)-calculus&lt;/item&gt;&lt;item&gt;8. Extensions and Variations&lt;/item&gt;&lt;item&gt;9. Applications&lt;/item&gt;&lt;item&gt;Bibliography&lt;/item&gt;&lt;item&gt;Academic Tools&lt;/item&gt;&lt;item&gt;Other Internet Resources&lt;/item&gt;&lt;item&gt;Related Entries&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;1. Introduction&lt;/head&gt;&lt;p&gt;The \(\lambda\)-calculus is an elegant notation for working with applications of functions to arguments. To take a mathematical example, suppose we are given a simple polynomial such as \(x^2 -2\cdot x+5\). What is the value of this expression when \(x = 2\)? We compute this by ‚Äòplugging in‚Äô 2 for \(x\) in the expression: we get \(2^2 -2\cdot 2+5\), which we can further reduce to get the answer 5. To use the \(\lambda\)-calculus to represent the situation, we start with the \(\lambda\)-term&lt;/p&gt;\[ \lambda x[x^2 -2\cdot x+5]. \]&lt;p&gt;The \(\lambda\) operators allows us to abstract over \(x\). One can intuitively read ‚Äò\(\lambda x[x^2 -2\cdot x+5]\)‚Äô as an expression that is waiting for a value \(a\) for the variable \(x\). When given such a value \(a\) (such as the number 2), the value of the expression is \(a^2 -2\cdot a+5\). The ‚Äò\(\lambda\)‚Äô on its own has no significance; it merely binds the variable \(x\), guarding it, as it were, from outside interference. The terminology in \(\lambda\)-calculus is that we want to apply this expression to an argument, and get a value. We write ‚Äò\(Ma\)‚Äô to denote the application of the function \(M\) to the argument \(a\). Continuing with the example, we get:&lt;/p&gt;\[\begin{align} (\lambda x[x^2 -2\cdot x+5])2 \rhd 2^2&amp;amp; -2\cdot 2+5 &amp;amp;\langle \text{Substitute 2 for } x\rangle \\ &amp;amp;= 4-4+5 &amp;amp;\langle\text{Arithmetic}\rangle \\ &amp;amp;= 5 &amp;amp;\langle\text{Arithmetic}\rangle \end{align}\]&lt;p&gt;The first step of this calculation, plugging in ‚Äò2‚Äô for occurrences of \(x\) in the expression ‚Äò\(x^2 - 2\cdot x + 5\)‚Äô, is the passage from an abstraction term to another term by the operation of substitution. The remaining equalities are justified by computing with natural numbers.&lt;/p&gt;&lt;p&gt;This example suggests the central principle of the \(\lambda\)-calculus, called \(\beta\)-reduction, which is also sometimes called \(\beta\)-conversion:&lt;/p&gt;\[ \tag{\(\beta\)} (\lambda x[M])N \rhd M[x := N] \]&lt;p&gt;The understanding is that we can reduce or contract \((\rhd)\) an application \((\lambda xM)N\) of an abstraction term (the left-hand side, \(\lambda xM)\) to something (the right-hand side, \(N)\) by simply plugging in \(N\) for the occurrences of \(x\) inside \(M\) (that‚Äôs what the notation ‚Äò\(M[x := N]\)‚Äô expresses). \(\beta\)-reduction, or \(\beta\)-conversion, is the heart of the \(\lambda\)-calculus. When one actually applies \(\beta\)-reduction to reduce a term, there is an important proviso that has to be observed. But this will be described in Section 2.1, when we discuss bound and free variables.&lt;/p&gt;&lt;head rend="h3"&gt;1.1 Multi-argument operations&lt;/head&gt;&lt;p&gt;What about functions of multiple arguments? Can the \(\lambda\)-calculus represent operations such as computing the length of the hypotenuse of a right triangle:&lt;/p&gt;&lt;p&gt;Hypotenuse of a right triangle with legs of length \(x\) and \(y \Rightarrow \sqrt{x^2 + y^2}\).&lt;/p&gt;&lt;p&gt;The length-of-hypotenuse operation maps two positive real numbers \(x\) and \(y\) to another positive real number. One can represent such multiple-arity operations using the apparatus of the \(\lambda\)-calculus by viewing the operation as taking one input at a time. Thus, the operation can be seen as taking one input, \(x\), a positive real number, and producing as its value not a number, but an operation: namely, the operation that takes a positive real number \(y\) as input and produces as output the positive real number \(\sqrt{x^2 + y^2}\). One could summarize the discussion by saying that the operation, hypotenuse-length, that computes the length of the hypotenuse of a right triangle given the lengths \(a\) and \(b\) of its legs, is:&lt;/p&gt;&lt;p&gt;hypotenuse-length \(:= \lambda a[\lambda b[\sqrt{a^2 + b^2}]]\)&lt;/p&gt;&lt;p&gt;By the principle of \(\beta\)-reduction, we have, for example, that hypotenuse-length 3, the application of hypotenuse-length to 3, is \(\lambda b[\sqrt{3^2 + b^2}]\), which is a function of that is ‚Äòwaiting‚Äô for another argument. The \(\lambda\)-term hypotenuse-length 3 can be viewed as a function that computes the length of the hypotenuse of a right triangle one of whose legs has length 3. We find, finally, that (hypotenuse-length 3)4‚Äîthe application of hypotenuse-length to 3 and then to 4‚Äîis 5, as expected.&lt;/p&gt;&lt;p&gt;Another way to understand the reduction of many-place functions to one-place functions is to imagine a machine \(M\) that initially starts out by loading the first \(a\) of multiple arguments \(a, b,\ldots\) into memory. If one then suspends the machine after it has loaded the first argument into memory, one can view the result as another machine M\(_a\) that is awaiting one fewer input; the first argument is now fixed.&lt;/p&gt;&lt;head rend="h3"&gt;1.2 Non-Extensionality&lt;/head&gt;&lt;p&gt;An important philosophical issue concerning the \(\lambda\)-calculus is the question of its underlying concept of functions. In set theory, a function is standardly understood as a set of argument-value pairs. More specifically, a function is understood as a set \(f\) of ordered pairs satisfying the property that \((x,y) \in f\) and \((x,z) \in f\) implies \(y = z\). If \(f\) is a function and \((x,y) \in f\), this means that the function f assigns the value \(y\) to the argument \(x\). This is the concept of functions-as-sets. Consequently, the notion of equality of functions-as-sets is equality qua sets, which, under the standard principle of extensionality, entails that two functions are equal precisely when they contain the same ordered pairs. In other words, two functions are identical if and only if they assign the same values to the same arguments. In this sense, functions-as-sets are extensional objects.&lt;/p&gt;&lt;p&gt;In contrast, the notion of a function at work in \(\lambda\)-calculus is one where functions are understood as rules: a function is given by a rule for how to determine its values from its arguments. More specifically, we can view a \(\lambda\)-term \(\lambda x[M]\) as a description of an operation that, given \(x\), produces \(M\); the body \(M\) of the abstraction term is, essentially, a rule for what to do with \(x\). This is the conception of functions-as-rules. Intuitively, given rules \(M\) and \(N\), we cannot in general decide whether \(\lambda x[M]\) is equal to \(\lambda x[N]\). The two terms might ‚Äòbehave‚Äô the same (have the same value given the same arguments), but it may not be clear what resources are needed for showing the equality of the terms. In this sense, functions-as-rules are non-extensional objects.&lt;/p&gt;&lt;p&gt;To distinguish the extensional concept of functions-as-sets from the non-extensional concept of functions-as-rules, the latter is often referred to as an ‚Äòintensional‚Äô function concept, in part because of the ostensibly intensional concept of a rule involved. This terminology is particularly predominant in the community of mathematical logicians and philosophers of mathematics working on the foundations of mathematics. But from the perspective of the philosophy of language, the terminology can be somewhat misleading, since in this context, the extensional-intensional distinction has a slightly different meaning.&lt;/p&gt;&lt;p&gt;In the standard possible-worlds framework of philosophical semantics, we would distinguish between an extensional and an intensional function concept as follows. Let us say that two functions are extensionally equivalent at a world if and only if they assign the same values to the same arguments at that world. And let us say that two functions are intensionally equivalent if and only if they assign the same values to the same arguments at every possible-world. To illustrate, consider the functions highest-mountain-on-earth and highest-mountain-in-the-Himalayas, where highest-mountain-on-earth assigns the highest mountain on earth as the value to every argument and highest-mountain-in-the-Himalayas assigns the highest mountain in the Himalayas as the value to every argument. The two functions are extensionally equivalent (at the actual world), but not intensionally so. At the actual world, the two functions assign the same value to every argument, namely Mt. Everest. Now consider a world where Mt. Everest is not the highest mountain on earth, but say, Mt. Rushmore is. Suppose further that this is so, just because Mt. Rushmore is 30.000 feet/9.100 m higher than it is at the actual world, while Mt. Everest, with its roughly 29.000 feet/8.800 m, is still the highest mountain in the Himalayas. At that world, highest-mountain-on-earth now assigns Mt. Rushmore as the value to every argument, while highest-mountain-in-the-Himalayas still assigns Mt. Everest to every object. In other words, highest-mountain-on-earth and highest-mountain-in-the-Himalayas are extensionally equivalent (at the actual world) but not intensionally equivalent.&lt;/p&gt;&lt;p&gt;A function concept may now be called extensional if and only if it requires functions that are extensionally equivalent at the actual world to be identical. And a function concept may be classified as intensional if and only if it requires intensionally equivalent functions to be identical. Note that these classifications are conceptually different from the distinctions commonly used in the foundations of mathematics. On the terminology used in the foundations of mathematics, functions-as-sets are classified as extensional since they use the axiom of extensionality as their criterion of identity, and functions-as-rules are classified as intensional because they rely on the ostensibly intensional concept of a rule. In the present possible-worlds terminology, function concepts are classified as extensional or intensional based of their behavior at possible-worlds.&lt;/p&gt;&lt;p&gt;An issue from which conceptual confusion might arise is that the two terminologies potentially pass different verdicts on the function concept at work in the \(\lambda\)-calculus. To see this, consider the following two functions:&lt;/p&gt;\[\begin{align} \addone &amp;amp;:= \lambda x[x+1] \\ \addtwosubtractone &amp;amp;:= \lambda x[[x+2]-1] \end{align}\]&lt;p&gt;These two functions are clearly extensionally equivalent: they assign the same value to the same input at the actual world. Moreover, given standard assumptions in possible worlds semantics, the two functions are also intensionally equivalent. If we assume that mathematical facts, like facts about addition and subtraction, are necessary in the sense that they are the same at every possible world, then we get that the two functions give the same value to the arguments at every possible world. So, an intensional function concept would require the two functions to be identical. In the \(\lambda\)-calculus, however, it‚Äôs not clear at all that we should identify the two functions. Formally speaking, without the help of some other principle, we cannot show that the two \(\lambda\)-terms denote the same function. Moreover, informally speaking, on the conception of functions-as-rules, it‚Äôs not even clear that we should identify them: the two terms involve genuinely different rules, and so we might be tempted to say that they denote different functions.&lt;/p&gt;&lt;p&gt;A function concept that allows for intensionally equivalent functions to be distinct is called hyperintensional. The point is that in possible-worlds terminology, the function concept at work in the \(\lambda\)-calculus may be regarded not as intentional but hyperintensional‚Äîin contrast to what the terminology common in the foundations of mathematics says. Note that it‚Äôs unclear how an intensional semantic framework, like the possible-worlds framework, could even in principle account for a non-intensional function concept. On the semantics of the \(\lambda\)-calculus, see section 7. The point here was simply to clarify any conceptual confusions that might arise from different terminologies at play in philosophical discourse.&lt;/p&gt;&lt;p&gt;The hyperintensionality of the \(\lambda\)-calculus is particularly important when it comes to its applications as a theory of not only functions, but more generally \(n\)-ary relations. On this, see section 9.3. It is effectively the hyperintensionality of the \(\lambda\)-calculus that makes it an attractive tool in this context. It should be noted, however, that the \(\lambda\)-calculus can be made extensional (as well as intensional) by postulating additional laws concerning the equality of \(\lambda\)-terms. On this, see section 5.&lt;/p&gt;&lt;head rend="h2"&gt;2. Syntax&lt;/head&gt;&lt;p&gt;The official syntax of the \(\lambda\)-calculus is quite simple; it is contained in the next definition.&lt;/p&gt;&lt;p&gt;Definition For the alphabet of the language of the \(\lambda\)-calculus we take the left and right parentheses, left and right square brackets, the symbol ‚Äò\(\lambda\)‚Äô, and an infinite set of variables. The class of \(\lambda\)-terms is defined inductively as follows:&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;Every variable is a \(\lambda\)-term.&lt;/item&gt;&lt;item&gt;If \(M\) and \(N\) are \(\lambda\)-terms, then so is \((MN)\).&lt;/item&gt;&lt;item&gt;If \(M\) is a \(\lambda\)-term and \(x\) is a variable, then \((\lambda x[M])\) is a \(\lambda\)-term.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;By ‚Äòterm‚Äô we always mean ‚Äò\(\lambda\)-term‚Äô. Terms formed according to rule (2) are called application terms. Terms formed according to rule (3) are called abstraction terms.&lt;/p&gt;&lt;p&gt;As is common when dealing with formal languages that have grouping symbols (the left and right parenthesis, in our case), some parentheses will be omitted when it is safe to do so (that is, when they can be reintroduced in only one sensible way). Juxtaposing more than two \(\lambda\)-terms is, strictly speaking, illegal. To avoid the tedium of always writing all needed parentheses, we adopt the following convention:&lt;/p&gt;&lt;p&gt;Convention (association to the left): When more than two terms \(M_1 M_2 M_3 \ldots M_n\) are juxtaposed we can recover the missing parentheses by associating to the left: reading from left to right, group \(M_1\) and \(M_2\) together, yielding \((M_1 M_2)M_3 \ldots M_n\); then group \((M_1 M_2)\) with \(M_3\): \(((M_1 M_2)M_3)\ldots M_n\), and so forth.&lt;/p&gt;&lt;p&gt;The convention thus gives a unique reading to any sequence of \(\lambda\)-terms whose length is greater than 2.&lt;/p&gt;&lt;head rend="h3"&gt;2.1 Variables, bound and free&lt;/head&gt;&lt;p&gt;The function of \(\lambda\) in an abstraction term \((\lambda x[M]\)) is that it binds the variable appearing immediately after it in the term \(M\). Thus \(\lambda\) is analogous to the universal and existential quantifiers \(\forall\) and \(\exists\) of first-order logic. One can define, analogously, the notions of free and bound variable in the expected way, as follows.&lt;/p&gt;&lt;p&gt;Definition The syntactic functions \(\mathbf{FV}\) and \(\mathbf{BV}\) (for ‚Äòfree variable‚Äô and ‚Äòbound variable‚Äô, respectively) are defined on the set of \(\lambda\)-terms by structural induction thus:&lt;/p&gt;&lt;p&gt;For every variable \(x\), term \(M\), and term \(N\):&lt;/p&gt;&lt;p&gt;If \(\mathbf{FV}(M) = \varnothing\) then \(M\) is called a combinator.&lt;/p&gt;&lt;p&gt;Clause (3) in the two definitions supports the intention that \(\lambda\) binds variables (ensures that they are not free). Note the difference between \(\mathbf{BV}\) and \(\mathbf{FV}\) for variables.&lt;/p&gt;&lt;p&gt;As is typical in other subjects where the concepts appear, such as first-order logic, one needs to be careful about the issue; a casual attitude about substitution can lead to syntactic difficulties.[1] We can defend a casual attitude by adopting the convention that we are interested not in terms themselves, but in a certain equivalence class of terms. We now define substitution, and then lay down a convention that allows us to avoid such difficulties.&lt;/p&gt;&lt;p&gt;Definition (substitution) We write ‚Äò\(M[x := N]\)‚Äô to denote the substitution of \(N\) for the free occurrences of \(x\) in \(M\). A precise definition[2] by recursion on the set of \(\lambda\)-terms is as follows: for all terms \(A\), \(B\), and \(M\), and for all variables \(x\) and \(y\), we define&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;\(x[x := M] \equiv M\)&lt;/item&gt;&lt;item&gt;\(y[x := M] \equiv y\) (\(y\) distinct from \(x)\)&lt;/item&gt;&lt;item&gt;\((AB)[x := M] \equiv A[x := M]B[x := M]\)&lt;/item&gt;&lt;item&gt;\((\lambda x[A])[x := M] \equiv \lambda x[A]\)&lt;/item&gt;&lt;item&gt;\((\lambda y[A])[x := M] \equiv \lambda y[A[x := M]]\) (\(y\) distinct from \(x)\)&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Clause (1) of the definition simply says that if we are to substitute \(M\) for \(x\) and we are dealing simply with \(x\), then the result is just \(M\). Clause (2) says that nothing happens when we are dealing (only) with a variable different from \(x\) but we are to substitute something for \(x\). Clause (3) tells us that substitution unconditionally distributes over applications. Clauses (4) and (5) concern abstraction terms and parallel clauses (1) and (2) (or rather, clauses (2) and (1), in opposite order): If the bound variable \(z\) of the abstraction term \(\lambda z[A]\) is identical to the variable \(x\) for which we are to do a substitution, then we do not perform any substitution (that is, substitution ‚Äústops‚Äù). This coheres with the intention that \(M[x := N]\) is supposed to denote the substitution of \(N\) for the free occurrences of \(x\) in \(M\). If \(M\) is an abstraction term \(\lambda x[A]\) whose bound variable is \(x\), then \(x\) does not occurr freely in \(M\), so there is nothing to do. This explains clause 4. Clause (5), finally, says that if the bound variable of an abstraction term differs from \(x\), then at least \(x\) has the ‚Äúchance ‚Äù to occur freely in the abstraction term, and substitution continues into the body of the abstraction term.&lt;/p&gt;&lt;p&gt;Definition (change of bound variables, \(\alpha\)-convertibility). The term \(N\) is obtained from the term \(M\) by a change of bound variables if, roughly, any abstraction term \(\lambda x[A]\) inside \(M\) has been replaced by \(\lambda y[A[x := y]]\).&lt;/p&gt;&lt;p&gt;Let us say that terms \(M\) and \(N\) are \(\alpha\)-convertible if there is a sequence of changes of bound variables starting from \(M\) and ending at \(N\).&lt;/p&gt;&lt;p&gt;Axiom. \(\beta\)-conversion (stated with a no-capture proviso):&lt;/p&gt;&lt;p&gt; \( (\lambda x[M])N \rhd M[x := N]\), &lt;lb/&gt; provided no variable that occurrs free in \(N\) becomes bound after its substitution into \(M\).&lt;/p&gt;&lt;p&gt;Roughly, we need to adhere to the principle that free variables ought to remain free; when an occurrence of a variable is threatened to become bound by a substitution, simply perform enough \(\alpha\)-conversions to sidestep the problem. If we keep this in mind, we can work with \(\lambda\)-calculus without worrying about these nettlesome syntactic difficulties. So, for example, we can‚Äôt apply the function \(\lambda x[\lambda y[x(y-5)]]\) to the argument \(2y\) because upon substitution of ‚Äú\(2y\)‚Äù for ‚Äú\(x\)‚Äù, the ‚Äú\(y\)‚Äù in ‚Äú\(2y\)‚Äù would be captured by the variable-binding operator ‚Äú\(\lambda y\)‚Äù. Such a substitution would yield a function different from the one intended. However, we can first transform \(\lambda x[\lambda y[x(y-5)]]\) to \(\lambda x[\lambda z[x(z-5)]]\) by \(\alpha\)-conversion, and then apply this latter function to the argument \(2y\). So whereas the following is not a valid use of \(\beta\)-conversion: \[ (\lambda x[\lambda y[x(y-5)]])2y \rhd \lambda y[2y(y-5)]\] we can validly use \(\beta\)-conversion to conclude: \[ (\lambda x[\lambda z[x(z-5)]])2y \rhd \lambda z[2y(z-5)]\] This example helps one to see why the proviso to \(\beta\)-conversion is so important. The proviso is really no different from the one used in the statement of an axiom of the predicate calculus, namely: \(\forall x\phi \to \phi^{\tau}_x\), provided no variable that is free in the term \(\tau\) before the substitution becomes bound after the substitution.&lt;/p&gt;&lt;p&gt;The syntax of \(\lambda\)-calculus is quite flexible. One can form all sorts of terms, even self-applications such as \(xx\). Such terms appear at first blush to be suspicious; one might suspect that using such terms could lead to inconsistency, and in any case one might find oneself reaching for a tool with which to forbid such terms. If one were to view functions and sets of ordered pairs of a certain kind, then the \(x\) in \(xx\) would be a function (set of ordered pairs) that contains as an element a pair \((x,y)\) whose first element would be \(x\) itself. But no set can contain itself in this way, lest the axiom of foundation (or regularity) be violated. Thus, from a set theoretical perspective such terms are clearly dubious. Below one can find a brief sketch of one such tool, type theory. But in fact such terms do not lead to inconsistency and serve a useful purpose in the context of \(\lambda\)-calculus. Moreover, forbidding such terms, as in type theory, does not come for free (e.g., some of the expressiveness of untyped \(\lambda\)-calculus is lost).&lt;/p&gt;&lt;head rend="h3"&gt;2.2 Combinators&lt;/head&gt;&lt;p&gt;As defined earlier, a combinator is a \(\lambda\)-term with no free variables. One can intuitively understand combinators as ‚Äòcompletely specified‚Äô operations, since they have no free variables. There are a handful of combinators that have proven useful in the history of \(\lambda\)-calculus; the next table highlights some of these special combinators. Many more could be given (and obviously there are infinitely many combinators), but the following have concise definitions and have proved their utility. Below is a table of some standard \(\lambda\)-terms and combinators.&lt;/p&gt;&lt;table&gt;&lt;row span="2"&gt;&lt;cell&gt;Name&lt;/cell&gt;&lt;cell&gt;Definition &amp;amp; Comments&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;\(\bS\)&lt;/cell&gt;&lt;cell&gt;\(\lambda x[\lambda y[\lambda z[xz(yz)]]]\) &lt;p&gt;Keep in mind that ‚Äò\(xz(yz)\)‚Äô is to be understood as the application \((xz)(yz)\) of \(xz\) to \(yz. \bS\) can thus be understood as a substitute-and-apply operator: \(z\) ‚Äòintervenes‚Äô between \(x\) and \(y\): instead of applying \(x\) to \(y\), we apply \(xz\) to \(yz\).&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;\(\mathbf{K}\)&lt;/cell&gt;&lt;cell&gt;\(\lambda x[\lambda y[x]]\) &lt;p&gt;The value of \(\mathbf{K}M\) is the constant function whose value for any argument is simply \(M.\)&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;\(\mathbf{I}\)&lt;/cell&gt;&lt;cell&gt;\(\lambda x[x]\) &lt;p&gt;The identity function.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;\(\mathbf{B}\)&lt;/cell&gt;&lt;cell&gt;\(\lambda x[\lambda y[\lambda z[x(yz)]]]\) &lt;p&gt;Recall that ‚Äò\(xyz\)‚Äô is to be understood as \((xy)z\), so this combinator is not a trivial identity function.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;\(\mathbf{C}\)&lt;/cell&gt;&lt;cell&gt;\(\lambda x[\lambda y[\lambda z[xzy]]]\) &lt;p&gt;Swaps an argument.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;\(\mathbf{T}\)&lt;/cell&gt;&lt;cell&gt;\(\lambda x[\lambda y[x]]\) &lt;p&gt;Truth value true. Identical to \(\mathbf{K}\). We shall see later how these representations of truth values plays a role in the blending of logic and \(\lambda\)-calculus.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;\(\mathbf{F}\)&lt;/cell&gt;&lt;cell&gt;\(\lambda x[\lambda y[y]]\) &lt;p&gt;Truth value false.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;\(\boldsymbol{\omega}\)&lt;/cell&gt;&lt;cell&gt;\(\lambda x[xx]\) &lt;p&gt;Self-application combinator&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;\(\boldsymbol{\Omega}\)&lt;/cell&gt;&lt;cell&gt;\(\boldsymbol{\omega \omega}\) &lt;p&gt;Self-application of the self-application combinator. Reduces to itself.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;\(\mathbf{Y}\)&lt;/cell&gt;&lt;cell&gt;\(\lambda f[(\lambda x[f(xx)])(\lambda x[f(xx)]\))] &lt;p&gt;Curry‚Äôs paradoxical combinator. For every \(\lambda\)-term \(X\), we have: \[\begin{align} \mathbf{Y}X &amp;amp;\rhd (\lambda x[X(xx)])(\lambda x[X(xx)]) \\ &amp;amp;\rhd X((\lambda x[X(xx)])(\lambda x[X(xx)])) \end{align}\] The first step in the reduction shows that \(\mathbf{Y}\)X reduces to the application term \((\lambda x[X(xx)])(\lambda x[X(xx)]\)), which is recurring in the third step. Thus, \(\mathbf{Y}\) has the curious property that \(\mathbf{Y}\)X and X\((\mathbf{Y}\)X) reduce to a common term.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;\(\boldsymbol{\Theta}\)&lt;/cell&gt;&lt;cell&gt;\((\lambda x[\lambda f[f(xxf)]])(\lambda x[\lambda f[f(xxf)]]\)) &lt;p&gt;Turing‚Äôs fixed-point combinator. For every \(\lambda\)-term \(X\), \(\boldsymbol{\Theta}X\) reduces to \(X(\boldsymbol{\Theta}X)\), which one can confirm by hand. (Curry‚Äôs paradoxical combinator \(\mathbf{Y}\) does not have this property.)&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;Below is a table of notational conventions employed in this entry.&lt;/p&gt;&lt;table&gt;&lt;row span="2"&gt;&lt;cell&gt;Notation&lt;/cell&gt;&lt;cell&gt;Reading &amp;amp; Comments&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;\(MN\)&lt;/cell&gt;&lt;cell&gt;The application of the function \(M\) to the argument \(N\). &lt;p&gt;Usually, parentheses are used to separate the function from the argument, like so: ‚Äò\(M(N)\)‚Äô. However, in \(\lambda\)-calculus and kindred subjects the parentheses are used as grouping symbols. Thus, it is safe to write the function and the argument adjacent to one other.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;\(PQR\)&lt;/cell&gt;&lt;cell&gt;The application of the function \(PQ\)‚Äîwhich is itself the application of the function \(P\) to the argument \(Q\)‚Äîto \(R\). &lt;p&gt;If we do not use parentheses to separate function and argument, how are we to disambiguate expressions that involve three or more terms, such as ‚Äò\(PQR\)‚Äô? Recall our convention that we are to understand such officially illegal expressions by working from left to right, always putting parentheses around adjacent terms. Thus, ‚Äò\(PQR\)‚Äô is to be understood as \((PQ)R\). ‚Äò\(PQRS\)‚Äô is \(((PQ)R)S\). The expression ‚Äò\((PQ)R\)‚Äô is disambiguated; by our convention, it is identical to \(PQR\). The expression ‚Äò\(P(QR)\)‚Äô is also explicitly disambiguated; it is distinct from \(PQR\) because it is the application of \(P\) to the argument \(QR\) (which is itself the application of the function \(Q\) to the argument \(R)\).&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;\((\lambda x[M])\)&lt;/cell&gt;&lt;cell&gt;The \(\lambda\) term that binds the variable \(x\) in the \(\boldsymbol{body}\) term \(M\). &lt;p&gt;The official vocabulary of the \(\lambda\)-calculus consists of the symbol ‚Äò\(\lambda\)‚Äô, left ‚Äò(‚Äôand right ‚Äò)‚Äô parentheses, and a set of variables (assumed to be distinct from the three symbols ‚Äò\(\lambda\)‚Äô, ‚Äò(‚Äô, and ‚Äò)‚Äô lest we have syntactic chaos).&lt;/p&gt;&lt;p&gt;Alternative notation. It is not necessary to include two kinds of grouping symbols (parentheses and square brackets) in the syntax. Parentheses or square brackets alone would obviously suffice. The two kinds of brackets are employed in this entry for the sake of readability. Given the two kinds of grouping symbols, we could economize further and omit the parentheses from abstraction terms, so that ‚Äò\((\lambda x[M]\))‚Äô would be written as ‚Äò\(\lambda x[M]\)‚Äô.&lt;/p&gt;&lt;p&gt;Some authors write ‚Äò\(\lambda x.M\)‚Äô or ‚Äò\(\lambda x\cdot M\)‚Äô, with a full stop or a centered dot separating the bound variable from the body of the abstraction term. As with the square brackets, these devices are intended to assist reading \(\lambda\)-terms; they are usually not part of the official syntax. (One sees this device used in earlier works of logic, such as Principia Mathematica, where the function of the symbol . in expressions such as ‚Äò\(\forall x\).\(\phi\)‚Äô is to get us to read the whole of the formula \(\phi\) as under the scope of the \(\forall x\).)&lt;/p&gt;&lt;p&gt;Some authors write abstraction terms without any device separating the bound variable from the body: such terms are crisply written as, e.g., ‚Äò\(\lambda xx\)‚Äô, ‚Äò\(\lambda yx\)‚Äô. The practice is not without its merits: it is about as concise as one can ask for, and permits an even simpler official syntax of the \(\lambda\)-calculus. But this practice is not flawless. In ‚Äò\(\lambda xyz\)‚Äô, is the bound variable \(x\) or is it \(xy\)? Usually the names of variables are single letters, and theoretically this is clearly sufficient. But it seems unduly restrictive to forbid the practice of giving longer names to variables; indeed, such constructions arise naturally in computer programming languages.&lt;/p&gt;&lt;p&gt;For the sake of uniformity, we will adopt the square bracket notation in this entry. (Incidentally, this notation is used in (Turing, 1937).)&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;\(M[x := A]\)&lt;/cell&gt;&lt;cell&gt;The \(\lambda\)-term that is obtained by substituting the \(\lambda\)-term A for all free occurrences of \(x\) inside \(M\). &lt;p&gt;A bewildering array of notations to represent substitution can be found in the literature on \(\lambda\)-calculus and kindred subjects:&lt;/p&gt;\[ M[x/A], M[A/x], M_{x}^A, M_{A}^x, [x/A]M,\ldots \]&lt;p&gt;Which notation to use for substitution seems to be a personal matter. In this entry we use a linear notation, eschewing superscripts and subscripts. The practice of representing substitution with ‚Äò:=‚Äô comes from computer science, where ‚Äò:=‚Äô is read in some programming languages as assigning a value to a variable.&lt;/p&gt;&lt;p&gt;As with the square brackets employed to write abstraction terms, the square brackets employed to write substitution are not officially part of the syntax of the \(\lambda\)-calculus. \(M\) and A are terms, \(x\) is a variable; \(M[x := A]\) is another term.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;\(M \equiv N\)&lt;/cell&gt;&lt;cell&gt;The \(\lambda\)-terms \(M\) and \(N\) are identical: understood as sequences of symbols, \(M\) and \(N\) have the same length and corresponding symbols of the sequences are identical. &lt;p&gt;The syntactic identity relation \(\equiv\) is not part of the official syntax of \(\lambda\)-calculus; this relation between \(\lambda\)-terms belongs to the metatheory of \(\lambda\)-calculus. It is clearly a rather strict notion of equality between \(\lambda\)-terms. Thus, it is not the case (if \(x\) and \(y\) are distinct variables) that \(\lambda x[x] \equiv \lambda y[y]\), even though these two terms clearly ‚Äòbehave‚Äô in the same way in the sense that both are expressions of the identity operation \(x \Rightarrow x\). Later we will develop formal theories of equality of \(\lambda\)-terms with the aim of capturing this intuitive equality of \(\lambda x[x]\) and \(\lambda y[y]\).&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;head rend="h2"&gt;3. Brief history of \(\lambda\)-calculus&lt;/head&gt;&lt;p&gt;\(\lambda\)-calculus arose from the study of functions as rules. Already the essential ingredients of the subject can be found in Frege‚Äôs pioneering work (Frege, 1893). Frege observed, as we did above, that in the study of functions it is sufficient to focus on unary functions (i.e., functions that take exactly one argument). (The procedure of viewing a multiple-arity operation as a sequence of abstractions that yield an equivalent unary operation is called currying the operation. Perhaps it would be more historically accurate to call the operation fregeing, but there are often miscarriages of justice in the appellation of mathematical ideas.) In the 1920s, the mathematician Moses Sch√∂nfinkel took the subject further with his study of so-called combinators. As was common in the early days of the subject, Sch√∂nfinkel was interested in the kinds of transformations that one sees in formal logic, and his combinators were intended to be a contribution to the foundations of formal logic. By analogy with the reduction that one sees in classical propositional logic with the Sheffer stroke, Sch√∂finkel established the astonishing result that the all functions (in the sense of all transformations) could be given in terms of the combinators \(\mathbf{K}\) and \(\bS\); later we will see the definition of these combinators.&lt;/p&gt;&lt;p&gt;Theorem For every term \(M\) made up of \(\mathbf{K}\) and \(\bS\) and the variable \(x\), there exists a term \(F\) (built only from \(\mathbf{K}\) and \(\bS)\) such that we can derive \(Fx = M\).&lt;/p&gt;&lt;p&gt;(The proof that these two suffice to represent all functions is beyond the scope of this entry. For further discussion, see the entry on combinatory logic.) One can prove the theorem constructively: there is an algorithm that, given \(M\), produces the required \(F\). Church called this \(F\) ‚Äò\(\lambda x[M]\)‚Äô (Church, 1932).[3] From this perspective, the \(\beta\)-rule can be justified: if ‚Äò\(\lambda x[M]\)‚Äô is to be a function \(F\) satisfying \(Fx = M\), then \(\lambda x[M]\)x should transform to \(M\). This is just a special case of the more general principle that for all \(N, (\lambda x[M])N\) should transform to \(M[x := N]\).&lt;/p&gt;&lt;p&gt;Although today we have more clearly delimited systems of abstraction and rewriting, in its early days \(\lambda\)-calculus and combinatory logic (√† la Sch√∂nfinkel) were bound up with investigations of foundations of mathematics. In the hands of Curry, Church, Kleene, and Rosser (some of the pioneers in the subject) the focus was on defining mathematical objects and carrying out logical reasoning inside the these new systems. It turned out that these early attempts at so-called illative \(\lambda\)-calculus and combinatory logic were inconsistent. Curry isolated and polished the inconsistency; the result is now known as Curry‚Äôs paradox. See the entry on Curry‚Äôs paradox and appendix B of (Barendregt, 1985).&lt;/p&gt;&lt;p&gt;The \(\lambda\)-calculus earns a special place in the history of logic because it was the source of the first undecidable problem. The problem is: given \(\lambda\)-terms \(M\) and \(N\), determine whether \(M = N\). (A theory of equational reasoning about \(\lambda\)-terms has not yet been defined; the definition will come later.) This problem was shown to be undecidable.&lt;/p&gt;&lt;p&gt;Another early problem in the \(\lambda\)-calculus was whether it is consistent at all. In this context, inconsistency means that all terms are equal: one can reduce any \(\lambda\)-term \(M\) to any other \(\lambda\)-term \(N\). That this is not the case is an early result of \(\lambda\)-calculus. Initially one had results showing that certain terms were not interconvertible (e.g., \(\mathbf{K}\) and \(\bS)\); later, a much more powerful result, the so-called Church-Rosser theorem, helped shed more light on \(\beta\)-conversion and could be used to give quick proofs of the non-inter-convertibility of whole classes of \(\lambda\)-terms. See below for more detailed discussion of consistency.&lt;/p&gt;&lt;p&gt;The \(\lambda\)-calculus was a somewhat obscure formalism until the 1960s, when, at last, a ‚Äòmathematical‚Äô semantics was found. Its relation to programming languages was also clarified. Till then the only models of \(\lambda\)-calculus were ‚Äòsyntactic‚Äô, that is, were generated in the style of Henkin and consisted of equivalence classes of \(\lambda\)-terms (for suitable notions of equivalence). Applications in the semantics of natural language, thanks to developments by Montague and other linguists, helped to ‚Äòspread the word‚Äô about the subject. Since then the \(\lambda\)-calculus enjoys a respectable place in mathematical logic, computer science, linguistics (see, e.g., Heim and Kratzer 1998), and kindred fields.&lt;/p&gt;&lt;head rend="h2"&gt;4. Reduction&lt;/head&gt;&lt;p&gt;Various notions of reduction for \(\lambda\)-terms are available, but the principal one is \(\beta\)-reduction, which we have already seen earlier. Earlier we used the notation ‚Äò\(\rhd\)‚Äô; we can be more precise. In this section we discuss \(\beta\)-reduction and some extensions.&lt;/p&gt;&lt;p&gt;Definition (one-step \(\beta\)-reduction \(\rhd_{\beta ,1})\) For \(\lambda\)-terms \(A\) and \(B\), we say that \(A\) \(\beta\)-reduces in one step to \(B\), written \(A \rhd_{\beta ,1} B\), just in case there exists an (occurrence of a) subterm \(C\) of \(A\), a variable \(x\), and \(\lambda\)-terms \(M\) and \(N\) such that \(C \equiv(\lambda x[M])N\) and \(B\) is \(A\) except that the occurrence of \(C\) in \(A\) is replaced by \(M[x := N]\).&lt;/p&gt;&lt;p&gt;Here are some examples of \(\beta\)-reduction:&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;&lt;p&gt;The variable \(x\) does not \(\beta\)-reduce to anything. (It does not have the right shape: it is simply a variable, not an application term whose left-hand side is an abstraction term.)&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;\((\lambda x[x])a \rhd_{\beta ,1} a\).&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;If \(x\) and \(y\) are distinct variables, then \((\lambda x[y])a \rhd_{\beta ,1} y\).&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;The \(\lambda\)-term \((\lambda x[(\lambda y[xy])a])b]\) \(\beta\)-reduces in one step to two different \(\lambda\)-terms:&lt;/p&gt;\[ (\lambda x[(\lambda y[xy])a])b \rhd_{\beta ,1} (\lambda y[by])a \]&lt;p&gt;and&lt;/p&gt;\[ (\lambda x[(\lambda y[xy])a])b \rhd_{\beta ,1} (\lambda x[xa])b \]&lt;p&gt;Moreover, one can check that these two terms \(\beta\)-reduce in one step to a common term: \(ba\). We thus have:&lt;/p&gt;&lt;td&gt;\((\lambda y[by])a\)&lt;/td&gt;&lt;td&gt;\(\nearrow\)&lt;/td&gt;&lt;td&gt;\(\searrow\)&lt;/td&gt;&lt;td&gt;\((\lambda x[(\lambda y[xy])a])b\)&lt;/td&gt;&lt;td&gt;\(ba\)&lt;/td&gt;&lt;td&gt;\(\searrow\)&lt;/td&gt;&lt;td&gt;\(\nearrow\)&lt;/td&gt;&lt;td&gt;\((\lambda x[xa])b\)&lt;/td&gt;&lt;/item&gt;&lt;/list&gt;&lt;p&gt;As with any binary relation, one can ask many questions about the relation \(\rhd_{\beta ,1}\) holding between \(\lambda\)-terms, and one can define various derived notions in terms of \(\rhd_{\beta ,1}\).&lt;/p&gt;&lt;p&gt;Definition A \(\beta\)-reduction sequence from a \(\lambda\)-term \(A\) to a \(\lambda\)-term \(B\) is a finite sequence \(s_1 , \ldots s_n\) of \(\lambda\)-terms starting with \(A\), ending with \(B\), and whose adjacent terms \((s_k,s_{k+1})\) satisfy the property that \(s_k \rhd_{\beta ,1} s_{k+1}\).&lt;/p&gt;&lt;p&gt;More generally, any sequence \(s\)‚Äîfinite or infinite‚Äîstarting with a \(\lambda\)-term \(A\) is said to be a \(\beta\)-reduction sequence commencing with \(A\) provided that the adjacent terms \((s_k,s_{k+1})\) of \(s\) satisfy the property that \(s_k \rhd_{\beta ,1} s_{k+1}\).&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;&lt;p&gt;Continuing with \(\beta\)-reduction Example 1, there are no \(\beta\)-reduction sequences at all commencing with the variable \(x\).&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;Continuing with \(\beta\)-reduction Example 2, the two-term sequence&lt;/p&gt;\[ (\lambda x[x])a, a \]&lt;p&gt;is a \(\beta\)-reduction sequence from \((\lambda x[x])a\) to \(a\). If \(a\) is a variable, then this \(\beta\)-reduction sequence cannot be prolonged, and there are no other \(\beta\)-reduction sequences commencing with \((\lambda x[x])a\); thus, the set of \(\beta\)-reduction sequences commencing with \((\lambda x[x])a\) is finite and contains no infinite sequences.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;The combinator \(\boldsymbol{\Omega}\) has the curious property that \(\Omega \rhd_{\beta ,1} \Omega\). Every term of every \(\beta\)-reduction sequence commencing with \(\boldsymbol{\Omega}\) (finite or infinite) is equal to \(\boldsymbol{\Omega}\).&lt;/item&gt;&lt;item&gt;&lt;p&gt;Consider the term \(\mathbf{K}a\boldsymbol{\Omega}\). There are infinitely many reduction sequences commencing with this term:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;\(\bK a\boldsymbol{\Omega} \rhd_{\beta ,1} a\)&lt;/item&gt;&lt;item&gt;\(\bK a\boldsymbol{\Omega} \rhd_{\beta ,1} \bK a\boldsymbol{\Omega} \rhd_{\beta ,1} a\)&lt;/item&gt;&lt;item&gt;\(\bK a\boldsymbol{\Omega} \rhd_{\beta ,1} \bK a\boldsymbol{\Omega} \rhd_{\beta ,1} \bK a\boldsymbol{\Omega} \rhd_{\beta ,1} a\)&lt;/item&gt;&lt;item&gt;\(\bK a\boldsymbol{\Omega} \rhd_{\beta ,1} \bK a\boldsymbol{\Omega} \rhd_{\beta ,1} \bK a\boldsymbol{\Omega} \ldots\)&lt;/item&gt;&lt;/list&gt;&lt;p&gt;If \(a\) is a variable, one can see that all finite reduction sequences commencing with \(\bK a\boldsymbol{\Omega}\) end at \(a\), and there is exactly one infinite reduction sequence.&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Definition A \(\beta\)-redex of a \(\lambda\)-term \(M\) is (an occurrence of) a subterm of \(M\) of the form \((\lambda x[P])Q\). (‚Äòredex‚Äô comes from ‚Äòreducible expression.) A \(\beta\)-redex is simply a candidate for an application of \(\beta\)-reduction. Doing so, one contracts the \(\beta\)-redex. A term is said to be in \(\beta\)-normal form if it has no \(\beta\)-redexes.&lt;/p&gt;&lt;p&gt;(Can a term have multiple \(\beta\)-normal forms? The answer is literally ‚Äòyes‚Äô, but substantially the answer is ‚Äòno‚Äô: If a \(M\) and \(M'\) are \(\beta\)-normal forms of some term, then \(M\) is \(\alpha\)-convertible to \(M'\) Thus, \(\beta\)-normal forms are unique up to changes of bound variables.)&lt;/p&gt;&lt;p&gt;So far we have focused only on one step of \(\beta\)-reduction. One can combine multiple \(\beta\)-reduction steps into one by taking the transitive closure of the relation \(\rhd_{\beta ,1}\).&lt;/p&gt;&lt;p&gt;Definition For \(\lambda\)-terms \(A\) and \(B\), one says that \(A\) \(\beta\)-reduces to \(B\), written \(A \rhd_{\beta} B\), if either \(A \equiv B\) or there exists a finite \(\beta\)-reduction sequence from \(A\) to \(B\).&lt;/p&gt;&lt;p&gt;Definition A term \(M\) has a \(\beta\)-normal form if there exists a term \(N\) such that \(N\) is in \(\beta\)-normal form an \(M \rhd_{\beta} N\).&lt;/p&gt;&lt;p&gt;Reducibility as defined is a one-way relation: it is generally not true that if \(A \rhd_{\beta} B\), then \(B \rhd_{\beta} A\). However, depending on one‚Äôs purposes, one may wish to treat \(A\) and \(B\) as equivalent if either \(A\) reduces to \(B\) or \(B\) reduces to \(A\). Doing so amounts to considering the reflexive, symmetric, and transitive closure of the relation \(\rhd_{\beta ,1,}\).&lt;/p&gt;&lt;p&gt;Definition For \(\lambda\)-terms \(A\) and \(B\), we say that \(A =_{\beta} B\) if either \(A \equiv B\) or there exists a sequence \(s_1 , \ldots s_n\) starting with \(A\), ending with \(B\), and whose adjacent terms \((s_k,s_{k+1})\) are such that either \(s_k \rhd_{\beta ,1} s_{k+1}\) or \(s_{k+1} \rhd_{\beta ,1} s_k\).&lt;/p&gt;&lt;head rend="h3"&gt;4.1 Other notions of reduction&lt;/head&gt;&lt;p&gt;We have thus far developed the theory of \(\beta\)-reduction. This is by no means the only notion of reduction available in the \(\lambda\)-calculus. In addition to \(\beta\)-reduction, a standard relation between \(\lambda\)-terms is that of \(\eta\)-reduction:&lt;/p&gt;&lt;p&gt;Definition (one-step \(\eta\)-reduction) For \(\lambda\)-terms \(A\) and \(B\), we say that \(A\) \(\beta \eta\)-reduces in one step to \(B\), written \(A \rhd_{\beta \eta ,1} B\), just in case there exists an (occurrence of a) subterm \(C\) of \(A\), a variable \(x\), and \(\lambda\)-terms \(M\) and \(N\) such that either&lt;/p&gt;&lt;p&gt;\(C \equiv(\lambda x[M])N\) and \(B\) is \(A\) except that the occurrence of \(C\) in \(A\) is replaced by \(M[x := N]\)&lt;/p&gt;&lt;p&gt;or&lt;/p&gt;&lt;p&gt;\(C \equiv(\lambda x[Mx]\)) and \(B\) is \(A\) except that the occurrence of \(C\) in \(A\) is replaced by \(M\).&lt;/p&gt;&lt;p&gt;The first clause in the definition of \(\rhd_{\beta \eta ,1}\) ensures that the relation extends the relation of one-step \(\beta\)-reduction. As we did for the relation of one-step \(\beta\)-reduction, we can replay the development for \(\eta\)-reduction. Thus, one has the notion of an \(\eta\)-redex, and from \(\rhd_{\eta ,1}\) one can define the relation \(\rhd_{\eta}\) between \(\lambda\)-terms as the reflexive and transitive closure of \(\rhd_{\eta ,1}\), which captures zero-or-more-steps of \(\eta\)-reduction. Then one defines \(=_{\eta}\) as the symmetric and transitive closure of \(\rhd_{\eta}\).&lt;/p&gt;&lt;p&gt;If \(A \rhd_{\eta ,1} B\), then the length of \(B\) is strictly smaller than that of \(A\). Thus, there can be no infinite \(\eta\)-reductions. This is not the case of \(\beta\)-reduction, as we saw above in \(\beta\)-reduction sequence examples 3 and 4.&lt;/p&gt;&lt;p&gt;One can combine notions of reduction. One useful combination is to blend \(\beta\)- and \(\eta\)-reduction.&lt;/p&gt;&lt;p&gt;Definition (one-step \(\beta \eta\)-reduction) \(\lambda x[Mx] \rhd_{\beta \eta ,1} M\) and \((\lambda x[M]N)) \rhd_{\beta \eta ,1} M[x := N]\). A \(\lambda\)-term \(A\) \(\beta \eta\)-reduces in one step to a \(\lambda\)-term \(B\) just in case either \(A\) \(\beta\)-reduces to \(B\) in one step or \(A\) \(\eta\)-reduces to \(B\) in one step.&lt;/p&gt;&lt;p&gt;Again, one can replay the basic concepts of reduction, as we did for \(\beta\)-reduction, for this new notion of reduction \(\beta \eta\).&lt;/p&gt;&lt;head rend="h3"&gt;4.2 Reduction strategies&lt;/head&gt;&lt;p&gt;Recall that a term is said to be in \(\beta\)-normal form if it has no \(\beta\)-redexes, that is, subterms of the shape \((\lambda x[M]\))N. A term has a \(\beta\)-normal form if it can be reduced to a term in \(\beta\)-normal form. It should be intuitively clear that if a term has a \(\beta\)-normal form, then we can find one by exhaustively contracting all all \(\beta\)-redexes of the term, then exhaustively contracting all \(\beta\)-redexes of all resulting terms, and so forth. To say that a term has a \(\beta\)-normal form amounts to saying that this blind search for one will eventually terminate.&lt;/p&gt;&lt;p&gt;Blind search for \(\beta\)-normal forms is not satisfactory. In addition to be aesthetically unpleasant, it can be quite inefficient: there may not be any need to exhaustively contract all \(\beta\)-redexes. What is wanted is a strategy‚Äîpreferably, a computable one‚Äîfor finding a \(\beta\)-normal form. The problem is to effectively decide, if there are multiple \(\beta\)-redexes of a term, which ought to be reduced.&lt;/p&gt;&lt;p&gt;Definition A \(\beta\)-reduction strategy is a function whose domain is the set of all \(\lambda\)-terms and whose value on a term \(M\) not in \(\beta\)-normal form is a redex subterm of \(M\), and whose value on all terms M in \(\beta\)-normal form is simply \(M\).&lt;/p&gt;&lt;p&gt;In other words, a \(\beta\)-reduction strategy selects, whenever a term has multiple \(\beta\)-redexes, which one should be contracted. (If a term is in \(\beta\)-normal form, then nothing is to be done, which is why we require in the definition of \(\beta\)-reduction strategy that it does not change any term in \(\beta\)-normal form.) One can represent a strategy \(S\) as a relation \(\rhd_S\) on \(\lambda\)-terms, with the understanding that \(M \rhd_S N\) provided that \(N\) is obtained from \(M\) in one step by adhering to the strategy S. When viewed as relations, strategies constitute a subrelation of \(\rhd_{\beta ,1}\).&lt;/p&gt;&lt;p&gt;A \(\beta\)-reduction strategy may or may not have the property that adhering to the strategy will ensure that we (eventually) reach a \(\beta\)-normal form, if one exists.&lt;/p&gt;&lt;p&gt;Definition A \(\beta\)-reduction strategy \(S\) is normalizing if for all \(\lambda\)-terms \(M\), if \(M\) has a \(\beta\)-normal form \(N\), then the sequence \(M, S(M), S(S(M)),\ldots\) terminates at \(N\).&lt;/p&gt;&lt;p&gt;Some \(\beta\)-reduction strategies are normalizing, but others are not.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;The rightmost strategy, whereby we always choose to reduce the rightmost \(\beta\)-redex (if there are any \(\beta\)-redexes) is not normalizing. Consider, for example, the term KI\(\Omega\). This term has two \(\beta\)-redexes: itself, and \(\Omega\) (which, recall, is the term \(\omega\omega\equiv(\lambda\)x[\(xx])(\lambda\)x[\(xx]\))). By working with left-hand \(\beta\)-redexes, we can \(\beta\)-reduce KI\(\Omega\) to \(\mathbf{I}\) in two steps. If we insist on working with the rightmost \(\beta\)-redex \(\Omega\) we reduce KI(\(\Omega\)) to \(\mathbf{KI}\)(\(\Omega \)), then \(\mathbf{KI}\)(\(\Omega\)), ‚Ä¶.&lt;/item&gt;&lt;item&gt;The leftmost strategy, whereby we always choose to reduce the leftmost \(\beta\)-redex (if there are any \(\beta\)-redexes) is normalizing. The proof of this fact is beyond the scope of this entry; see (Barendregt, 1985, section 13.2) for details.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Once we have defined a reduction strategy, it is natural to ask whether one can improve it. If a term has a \(\beta\)-normal form, then a strategy will discover a normal form; but might there be a shorter \(\beta\)-reduction sequence that reaches the same normal form (or a term that is \(\alpha\)-convertible to that normal form)? This is the question of optimality. Defining optimal strategies and showing that they are optimal is generally considerably more difficult than simply defining a strategy. For more discussion, see (Barendregt, 1984 chapter 10).&lt;/p&gt;&lt;p&gt;For the sake of concreteness, we have discussed only \(\beta\)-reduction strategies. But in the definitions above the notion of reduction \(\beta\) is but one possibility. For any notion \(R\) of reduction we have the associated theory of \(R\)-reduction strategies, and one can replay the problems of normalizability, optimality, etc., for \(R\).&lt;/p&gt;&lt;head rend="h2"&gt;5. \(\lambda\)-theories&lt;/head&gt;&lt;p&gt;We discussed earlier how the \(\lambda\)-calculus is a non-extensional theory of functions. If, in the non-extensional spirit, we understand \(\lambda\)-terms as descriptions, how should we treat equality of \(\lambda\)-terms? Various approaches are available. In this section, let us treat the equality relation = as a primitive, undefined relation holding between two \(\lambda\)-terms, and try to axiomatize the properties that equality should have. The task is to identity axioms and formulate suitable rules of inference concerning the equality of \(\lambda\)-terms.&lt;/p&gt;&lt;p&gt;Some obvious properties of equality, having nothing to do with \(\lambda\)-calculus, are as follows:&lt;/p&gt;\[\tag{Reflexivity} \frac{}{X=X} \] \[\tag{Symmetry} \frac{X=Y}{Y=X} \] \[\tag{Transitivity} \frac{X=Y \quad Y=Z}{X=Z} \]&lt;p&gt;As is standard in proof theory, the way to read these rules of inference is that above the horizontal rule \(\frac{}{\phantom{X=X}}\) are the premises of the rule (which are equations) and the equation below the horizontal rule is the conclusion of the rule of inference. In the case of the reflexivity rule, nothing is written above the horizontal rule. We understand such a case as saying that, for all terms \(X\), we may infer the equation \(X = X\) from no premises.&lt;/p&gt;&lt;head rend="h3"&gt;5.1 The basic theory \(\lambda\)&lt;/head&gt;&lt;p&gt;The three rules of inference listed in the previous section governing equality have nothing to do with the \(\lambda\)-calculus. The following lists rules of inference that relate the undefined notion of equality and the two term-building operations of the \(\lambda\)-calculus, application and abstraction.&lt;/p&gt;\[ \frac{M=N}{AM=AN} \quad \frac{M=N}{MA=NA} \] \[ \tag{\(\boldsymbol{\xi}\)} \frac{M=N}{\lambda x[M] = \lambda x[N]} \]&lt;p&gt;Together, these rules of inference say that = is a congruence relation on the set of \(\lambda\)-terms: it ‚Äòpreserves‚Äô both the application and abstraction term-building operations&lt;/p&gt;&lt;p&gt;The final rule of inference, \(\beta\)-conversion, is the most important:&lt;/p&gt;\[\tag{\(\boldsymbol{\beta}\)} \frac{}{(\lambda x[M])A = M[x := A]} \]&lt;p&gt;As before with the reflexivity rule, the rule \(\boldsymbol{\beta}\) has no premises: for any variable \(x\) and any terms \(M\) and \(A\), one can infer the equation \((\lambda x[M])A = M[x := A]\) at any point in a formal derivation in the theory \(\boldsymbol{\lambda}\).&lt;/p&gt;&lt;head rend="h3"&gt;5.2 Extending the basic theory \(\lambda\)&lt;/head&gt;&lt;p&gt;A number of extensions to \(\boldsymbol{\lambda}\) are available. Consider, for example, the rule (\(\boldsymbol{\eta}\)), which expresses the principle of \(\eta\)-reduction as a rule of inference:&lt;/p&gt;\[\tag{\(\boldsymbol{\eta}\)} \frac{}{\lambda x[Mx] = M} \text{ provided } x \not\in \mathbf{FV}(M) \]&lt;p&gt;Rule \(\boldsymbol{\eta}\) tells us that a certain kind of abstraction is otiose: it is safe to identify \(M\) with the function that, given an argument \(x\), applies \(M\) to \(x\). Through this rule we can also see that all terms are effectively functions. One can intuitively justify this rule using the principle of \(\beta\)-reduction.&lt;/p&gt;\[\tag{\(\mathbf{Ext}\)} \frac{Mx=Nx}{M=N}\text{ provided } x \not\in \mathbf{FV}(M) \cup \mathbf{FV}(N) \]&lt;p&gt;One can view rule \(\mathbf{Ext}\) as a kind of generalization principle. If we have derived that \(Mx = Nx\), but \(x\) figures in neither \(M\) nor \(N\), then we have effectively shown that \(M\) and \(N\) are alike. Compare this principle to the principle of universal generalization in first-order logic: if we have derived \(\phi(x)\) from a set \(\Gamma\) of hypotheses in which \(x\) is not free, then we can conclude that \(\Gamma\) derives \(\forall x\phi\).&lt;/p&gt;&lt;p&gt;Another productive principle in the \(\lambda\)-calculus permits us to identify terms that ‚Äòact‚Äô the same:&lt;/p&gt;\[\tag{\(\boldsymbol{\omega}\)} \frac{\text{For all terms }x, Mx=Nx}{M=N} \]&lt;p&gt;The rule \(\boldsymbol{\omega}\) has infinitely many hypotheses: on the assumption that \(Mx = Nx\), no matter what \(x\) may be, then we can conclude that \(M = N\). The \(\boldsymbol{\omega}\) rule is an analogue in the \(\lambda\)-calculus of the rule of inference under the same name in formal number theory, according to which one can conclude the universal formula \(\forall x\phi\) provided one has proofs for \(\phi(x := \mathbf{0}), \phi(x := \mathbf{1}),\ldots\) . Note that unlike the rule \(\mathbf{Ext}\), the condition that \(x\) not occur freely in \(M\) or \(N\) does not arise.&lt;/p&gt;&lt;head rend="h2"&gt;6. Consistency of the \(\lambda\)-calculus&lt;/head&gt;&lt;p&gt;Is the \(\lambda\)-calculus consistent? The question might not be well-posed. The \(\lambda\)-calculus is not a logic for reasoning about propositions; there is no apparent notion of contradiction \((\bot)\) or a method of forming absurd propositions (e.g., \(p \wedge \neg p)\). Thus ‚Äòinconsistency‚Äô of the \(\lambda\)-calculus cannot mean that \(\bot\), or some formula tantamount to \(\bot\), is derivable. A suitable notion of ‚Äòconsistent‚Äô is, however, available. Intuitively, a logic is inconsistent if it permits us to derive too much. The theory \(\lambda\) is a theory of equations. We can thus take inconsistency of \(\lambda\) to mean: all equations are derivable. Such a property, if it were true of \(\lambda\), would clearly show that \(\lambda\) is of little use as a formal theory.&lt;/p&gt;&lt;p&gt;Early formulations of the idea of \(\lambda\)-calculus by A. Church were indeed inconsistent; see (Barendregt, 1985, appendix 2) or (Rosser, 1985) for a discussion. To take a concrete problem: how do we know that the equation \(\bK = \mathbf{I}\) is not a theorem of \(\lambda\)? The two terms are obviously intuitively distinct. \(\bK\) is a function of two arguments, whereas \(\mathbf{I}\) is a function of one argument. If we could show that \(\bK = \mathbf{I}\), then we could show that \(\mathbf{KK} = \mathbf{IK}\), whence \(\mathbf{KK} = \bK\) would be a theorem of \(\lambda\), along with many other equations that strike us as intuitively unacceptable. But when we‚Äôre investigating a formal theory such as \(\lambda\), intuitive unacceptability by no means implies underivability. What is missing is a deeper understanding of \(\beta\)-reduction.&lt;/p&gt;&lt;p&gt;An early result that gave such an understanding is known as the Church-Rosser theorem:&lt;/p&gt;&lt;p&gt;Theorem (Church-Rosser) If \(P \rhd_{\beta} Q\) and \(P \rhd_{\beta}\) R, then there exists a term \(S\) such that both \(Q \rhd_{\beta} S\) and \(R \rhd_{\beta} S\).&lt;/p&gt;&lt;p&gt;(The proof of this theorem is quite non-trivial and is well-beyond the scope of this entry.) The result is a deep fact about \(\beta\)-reduction. It says that no matter how we diverge from \(P\) by \(\beta\)-reductions, we can always converge again to a common term.&lt;/p&gt;&lt;p&gt;The Church-Rosser theorem gives us, among other things, that the plain \(\lambda\)-calculus‚Äîthat is, the theory \(\lambda\) of equations between \(\lambda\)-terms‚Äîis consistent, in the sense that not all equations are derivable.&lt;/p&gt;&lt;p&gt;As an illustration, we can use the Church-Rosser theorem to solve the earlier problem of showing that the two terms \(\bK\) and \(\mathbf{I}\) are not identified by \(\lambda\). The two terms are in \(\beta\)-normal form, so from them there are no \(\beta\)-reduction sequences at all. If \(\bK = \mathbf{I}\) were a theorem of \(\lambda\), then there would be a term \(M\) from which there is a \(\beta\)-reduction path to both \(\mathbf{I}\) and \(\bK\). The Church-Rosser theorem then implies the two paths diverging from \(M\) can be merged. But this is impossible, since \(\bK\) and \(\mathbf{I}\) are distinct \(\beta\)-normal forms.&lt;/p&gt;&lt;p&gt;The Church-Rosser theorem implies the existence of \(\beta\)-reduction sequences commencing from \(\bK\) and from \(\mathbf{I}\) that end at a common term. But there are no \(\beta\)-reduction sequences at all commencing from \(\mathbf{I}\), because it is in \(\beta\)-normal form, and likewise for \(\bK\).&lt;/p&gt;&lt;p&gt;Theorem \(\lambda\) is consistent, in the sense that not every equation is a theorem.&lt;/p&gt;&lt;p&gt;To prove the theorem, it is sufficient to produce one underivable equation. We have already worked through an example: we used the Church-Rosser theorem to show that the equation \(\bK = \mathbf{I}\) is not a theorem of \(\lambda\). Of course, there‚Äôs nothing special about these two terms. A significant generalization of this result is available: if \(M\) and \(N\) in \(\beta\)-normal form but \(M\) is distinct from \(N\), then the equation \(M = N\) is not a theorem of \(\lambda\). (This simple condition for underivability does not generally hold if we add additional rules of inference to \(\lambda\).)&lt;/p&gt;&lt;p&gt;The theories \(\lambda \eta\) and \(\lambda \omega\) are likewise consistent. One can prove these consistency results along the lines of the consistency proof for \(\lambda\) by extending the Church-Rosser theorem to the wider senses of derivability of these theories.&lt;/p&gt;&lt;head rend="h2"&gt;7. Semantics of \(\lambda\)-calculus&lt;/head&gt;&lt;p&gt;As we‚Äôve said at the outset, the \(\lambda\)-calculus is, at heart, about functions and their applications. But it is surprisingly difficult to cash this idea out in semantic terms. A natural approach would be to try to associate with every \(\lambda\)-term \(M\) a function \(f_M\) over some domain \(D\) and to interpret application terms \((MN)\) using function application as \(f_M(f_N).\) But this idea quickly runs into difficulties. To begin with, it‚Äôs easy to see that, in this context, we can‚Äôt use the standard set-theoretic concept of functions-as-sets (see section 1.2 of this entry). According to this concept, remember, a function \(f\) is a set of argument-value pairs, where every argument gets assigned a unique value. The problem arises in the context of self-applications. Remember from section 2.1 that the untyped \(\lambda\)-calculus allows \(\lambda\)-terms such as \((xx)\), which intuitively apply \(x\) to itself. On the semantic picture we‚Äôre exploring, we can obtain the associated function \(f_{(xx)}\) for the term \((xx)\) by taking the function \(f_x\) for \(x\) and applying it to itself:&lt;/p&gt;\[ f_{(xx)}=f_x(f_x) \]&lt;p&gt;But following functions-as-sets, this would mean that the set \(f_x\) needs to contain an argument-value pair that has \(f_x\) as its first component and \(f_{(xx)}\) as the second:&lt;/p&gt;\[ f_{x}=\{\ldots, (f_{x},f_{(xx)})), \ldots\} \]&lt;p&gt;But this would make \(f_x\) a non-well-founded object: defining \(f_x\) would involve \(f_x\) itself. In fact, sets like this are excluded in standard axiomatic set theory by the axiom of foundation (also known as the axiom of regularity). ‚ÄîThis is further semantic evidence that the concept of a function underlying the \(\lambda\)-calculus can‚Äôt be the extensional functions-as-sets concept.&lt;/p&gt;&lt;p&gt;But the problem runs even deeper than that. Even when we use a non-extensional notion of a function, such as the functions-as-rules conception (see again section 1.2), we run into difficulties. In the untyped \(\lambda\)-calculus, everything can both be function and an argument to functions. Correspondingly, we should want our domain \(D\) to include, in some sense, the function space \(D^D\), which contains all and only the functions with both arguments and values from \(D\). To see this:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Every element of \(D\) can be a function that applies to elements of \(D\), and what‚Äôs returned can then be again be an argument for elements of \(D\). So, every element of \(D\) intuitively corresponds to a member of \(D^D\).&lt;/item&gt;&lt;item&gt;If, in turn, we take a member of \(D^D\), i.e., a function with arguments and values from \(D\), this is precisely the kind of thing we want to include in our domain \(D\). So, intuitively, we want every member of \(D^D\) to correspond to a member of \(D\).&lt;/item&gt;&lt;/list&gt;&lt;p&gt;In short, we want there to be a one-to-one correspondence between our domain and its own function space, i.e., we want them to satisfy the ‚Äòequation‚Äô \(X\cong X^X\). But this is impossible since it contradicts Cantor‚Äôs theorem.&lt;/p&gt;&lt;p&gt;Given these difficulties, the question arises whether it‚Äôs possible to give a set-theoretic model for the \(\lambda\)-calculus in the first place? It turns out that it is. D. Scott was the first to describe such a model in an unpublished manuscript from 1969. This model, \(D_\infty\), solves the aforementioned problems with Cantor‚Äôs theorem by suitably restricting the function space \(D^D\), by only letting some members of \(D^D\) correspond to members of \(D\). Covering Scott‚Äôs construction goes beyond the scope of this entry, since it involves advanced tools from algebra and topology; see (Meyer 1982), (Barendregt, 1985, chapter 18.2), or (Hindley and Seldin, 2008, chapter 16) for details. Instead, we‚Äôll discuss the more general question: What is a model for the \(\lambda\)-calculus? That is, leaving aside for a moment the question whether sets are functions, rules, or something altogether different, we ask what kind of mathematical structure a model for the \(\lambda\)-calculus is in the first place.&lt;/p&gt;&lt;head rend="h3"&gt;7.1 \(\lambda\)-Models&lt;/head&gt;&lt;p&gt;It turns out that there are multiple, essentially equivalent, ways of defining the notion of a model for the \(\lambda\)-calculus; see (Barendregt, 1985, chapter 5) or (Hindley and Seldin, 2008, chapter 15). In the following, we‚Äôll discuss what we consider the most palatable notion for philosophers familiar with the standard semantics for first-order logic (see, e.g., the entry on Classical Logic ), the so-called syntactical \(\lambda\)-models. These models first appear in the work of (Hindley and Longo, 1980), (Koymans, 1982), and (Meyer 1982). They derive their name from the fact that their clauses closely correspond to the syntactic rules of the calculus \(\boldsymbol{\lambda}\). This is somewhat unsatisfactory and motivates ‚Äòsyntax-free‚Äô definitions (see below). At the same time, the syntactical \(\lambda\)-models provide a fairly transparent and accessible route into the world of \(\lambda\)-models. In addition, despite their conceptual shortcomings, syntactical models have proven a technically useful tool in the semantical study of the \(\lambda\)-calculus.&lt;/p&gt;&lt;p&gt;In order to avoid the set-theoretic problems mentioned above, most definitions of \(\lambda\)-models use so-called applicative structures. The idea is to treat the denotations of \(\lambda\)-terms not as set-theoretic functions, but as unanalyzed, first-order ‚Äòfunction-objects‚Äô, instead. Correspondingly, then, we treat function application as an unanalyzed binary operation on these function-objects:&lt;/p&gt;&lt;p&gt;Definition An applicative structure is a pair \((D,\cdot)\), where \(D\) is some set and \(\cdot\) a binary operation on \(D\). To avoid trivial models, we usually assume that \(D\) has at least two elements.&lt;/p&gt;&lt;p&gt;Applicative structures are, in a sense, first-order models of function spaces that satisfy the problematic equation \(X\cong X^X\). \(\lambda\)-models, in turn, are defined over them.&lt;/p&gt;&lt;p&gt;For the definition of our \(\lambda\)-models, we work with valuations‚Äîa concept familiar from first-order semantics. Valuations assign denotations to the variables and are used primarily in the semantic clauses for the \(\lambda\)-operator. Additionally, they can be used to express general claims over the domain, in a way that is familiar from the semantics for the first-order quantifiers \(\exists x\) and \(\forall x\).&lt;/p&gt;&lt;p&gt;Definition A valuation in an applicative structure \((D,\cdot)\) is a function \(\rho\) that assigns an element \(\rho(x)\in D\) to every variable \(x\).&lt;/p&gt;&lt;p&gt;As a useful piece of notation, for \(\rho\) a valuation in some applicative structure \((D,\cdot)\), \(x\) a variable, and \(d\in D\) an object, we define the valuation \(\rho[x\mapsto d]\) by saying that: \[\rho[x\mapsto d](y)=\begin{cases} d &amp;amp; \text{ if }y=x\\ \rho(y) &amp;amp; \text{otherwise}\end{cases}\] That is, \(\rho[x\mapsto d]\) is the result of changing the value of \(x\) to be \(d\), while leaving all other other values under \(\rho\) unchanged.&lt;/p&gt;&lt;p&gt;Definition A syntactical \(\lambda\)-model is a triple \(\mathfrak{M}=(D,\cdot,\llbracket \ \rrbracket)\), where \((D,\cdot)\) is an applicative structure and \(\llbracket \ \rrbracket\) is a function that assigns to every \(\lambda\)-term M and valuation \(\rho\) a denotation \(\llbracket M\rrbracket_\rho\in D\) subject to the following constraints:&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;\(\llbracket x\rrbracket_\rho=\rho(x)\)&lt;/item&gt;&lt;item&gt;\(\llbracket MN\rrbracket_\rho=\llbracket M\rrbracket_\rho\cdot \llbracket N\rrbracket_\rho\)&lt;/item&gt;&lt;item&gt;\(\llbracket \lambda xM\rrbracket_\rho\cdot d=\llbracket M\rrbracket_{\rho[x\mapsto d]}\), for all \(d\in D\)&lt;/item&gt;&lt;item&gt;\(\llbracket \lambda xM\rrbracket_\rho = \llbracket \lambda xN\rrbracket_\rho\), whenever for all \(d\in D\), we have \(\llbracket M\rrbracket_{\rho[x\mapsto d]}=\llbracket N\rrbracket_{\rho[x\mapsto d]}\)&lt;/item&gt;&lt;item&gt;\(\llbracket M\rrbracket_\rho=\llbracket M\rrbracket_\sigma\), whenever \(\rho(x)=\sigma(x)\) for all \(x\in \mathbf{FV}(M)\)&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Intuitively, in a model \(\mathfrak{M}\), \(\llbracket M\rrbracket_\rho\) is the function-object denoted by the \(\lambda\)-term \(M\) under the valuation \(\rho\).&lt;/p&gt;&lt;p&gt;It is now straight-forward to define what it means for a \(\lambda\)-model \(\mathfrak{M}\) to satisfy an equation \(M=N\), symbolically \(\mathfrak{M}\vDash M=N\):&lt;/p&gt;&lt;p&gt;Definition (satisfaction).&lt;/p&gt;\[\mathfrak{M}\vDash M=N\text{ iff for all }\rho\text{, we have } \llbracket M\rrbracket_\rho=\llbracket N\rrbracket_\rho\]&lt;p&gt;In words: an equation \(M=N\) holds in a model \(\mathfrak{M}\) just in case the \(\lambda\)-terms \(M\) and \(N\) have the same denotation under every valuation in the underlying applicative structure.&lt;/p&gt;&lt;p&gt;Note that clauses 3. and 4. from the definition of a syntactical \(\lambda\)-model directly mirror the \(\boldsymbol{\lambda}\)-rules \(\boldsymbol{\beta}\) and \(\boldsymbol{\xi}\), respectively (see section 5.1 above). This is the ‚Äòsyntactic‚Äô nature of our models. While this might be semantically unsatisfactory (see below), it makes it relatively straight-forward to prove a soundness theorem for the semantics provided by the syntactical \(\lambda\)-models; see (Barendregt, 1985, Theorem 5.3.4) and (Hindley and Seldin, 2008. Theorem 15.12):&lt;/p&gt;&lt;p&gt;Theorem For all terms \(M,N\), if \(M=N\) is derivable in \(\boldsymbol{\lambda}\), then for all syntactical \(\lambda\)-models \(\mathfrak{M}\), we have that \(\mathfrak{M}\vDash M=N\).&lt;/p&gt;&lt;p&gt;This theorem provides a first ‚Äòsanity-check‚Äô for the semantics. But note that, so far, we haven‚Äôt shown that there exist any syntactical \(\lambda\)-models at all.&lt;/p&gt;&lt;p&gt;This worry is addressed by constructing so-called ‚Äòterm models‚Äô, which are not unlike the well-known Henkin constructions from first-order semantics. In order to define these models, we first need the notion of a \(\boldsymbol{\lambda}\)-equivalence class for a given \(\lambda\)-term \(M\). This class contains precisely the terms that \(\boldsymbol{\lambda}\) proves identical to \(M\):&lt;/p&gt;\[ [M]_{\boldsymbol{\lambda}}=\{N:\boldsymbol{\lambda}\text{ proves }M=N\} \]&lt;p&gt;We then define the term model for \(\boldsymbol{\lambda}\), \(\mathfrak{T}\), by setting:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;\(D=\{[M]_\boldsymbol{\lambda}:M\text{ is a }\lambda\text{-term}\}\)&lt;/item&gt;&lt;item&gt;\([M]_\boldsymbol{\lambda}\cdot [N]_\boldsymbol{\lambda}=[MN]_\boldsymbol{\lambda}\)&lt;/item&gt;&lt;item&gt;\(\llbracket M\rrbracket_\rho=[M[x_1:=N_1]\ldots[x_n:=N_n]]_\boldsymbol{\lambda}\), where \(\mathbf{FV}(M)=\{x_1,\ldots,x_n\}\) and \(\rho(x_1)=N_1, \ldots,\rho(x_n)=N_n\)&lt;/item&gt;&lt;/list&gt;&lt;p&gt;It is easily seen that this indeed defines a syntactical \(\lambda\)-model. In fact, it is easily checked that in the term model for \(\boldsymbol{\lambda}\), we have that:&lt;/p&gt;\[ \mathfrak{T}\vDash M=N\text{ iff }\boldsymbol{\lambda}\text{ derives }M=N. \]&lt;p&gt;This paves a way for a very simple completeness proof for \(\boldsymbol{\lambda}\) with respect to the class of syntactical \(\lambda\)-models; see (Meyer, 1982, 98‚Äì99) for one of the few explicit mentions of this kind of result in the literature:&lt;/p&gt;&lt;p&gt;Theorem For all terms \(M,N\), if for all syntactical \(\lambda\)-models \(\mathfrak{M}\), we have that \(\mathfrak{M}\vDash M=N\), then \(M=N\) is derivable in \(\boldsymbol{\lambda}\).&lt;/p&gt;&lt;p&gt;The proof is a simple proof by contraposition, which uses the term model \(\mathfrak{T}\) as a countermodel to any non-derivable identity in \(\boldsymbol{\lambda}\).&lt;/p&gt;&lt;p&gt;But there are reasons to be dissatisfied with the syntactical \(\lambda\)-models as a semantics for the \(\lambda\)-calculus. For one, by virtue of clauses 3. and 4. mirroring rules \(\boldsymbol{\beta}\) and \(\boldsymbol{\xi}\), the soundness result is ‚Äòbaked into‚Äô the semantics, as it were. This is unsatisfactory from a semantic perspective since it means that via the syntactical \(\lambda\)-models, we don‚Äôt really learn anything directly about what conditions an applicative structure needs to satisfy in order to adequately model the \(\lambda\)-calculus.&lt;/p&gt;&lt;p&gt;A related worry is that the clauses 3. and 4. are not recursive in nature. That is, they don‚Äôt allow us to compute the denotation of a complex \(\lambda\)-term from the denotations of its parts and information about the syntactic operation used to combine them. In our syntax (see section 2), there are two ways of constructing complex \(\lambda\)-terms: application terms of the form \(MN\) and abstraction terms of the form \((\lambda x[M])\). Clause 1. of our syntactical \(\lambda\)-models is a recursive clause for the syntactical application operation, but we don‚Äôt have a recursive clause for the syntactical operation of \(\lambda\)-abstraction. Clauses 3. and 4. are rather conditions on the denotation function \(\llbracket \ \rrbracket\) than recursive clauses. This is unsatisfactory since it means that we‚Äôre not really given a compositional semantics for the \(\lambda\)-operator by the syntactical \(\lambda\)-models.&lt;/p&gt;&lt;p&gt;These worries are taken care of in the development of syntax-free \(\lambda\)-models. A comprehensive discussion of syntax-free models goes beyond the scope of this entry; but see (Barendregt, 1985, chapter 5.2) and (Hinley and Seldin, 2008, chapter 15B) for the details. Suffice it to say that the definition of syntax-free \(\lambda\)-models involves determining precisely under which conditions an applicative structure is suitable for interpreting the \(\lambda\)-calculus. The resulting \(\lambda\)-models, then, indeed provide (something much closer to) a recursive, compositional semantics, where the syntactical operation of \(\lambda\)-abstraction is interepreted by a corresponding semantic operation on applicative structures.&lt;/p&gt;&lt;p&gt;It is worth noting, however, that syntactical \(\lambda\)-models and the syntax-free \(\lambda\)-models are, in a certain sense, equivalent: every syntactical \(\lambda\)-model defines a syntax-free \(\lambda\)-model and vice versa; see (Barendregt, 1985, theorem 5.3.6) and (Hinley and Seldin, 2008, theorem 15.20) for the details. From a technical perspective, this result allows us to freely move between the different presentations of \(\lambda\)-models and to use, in a given context, the notion of a model that is most expedient. At the same time, there may be philosophical reasons to prefer one presentation over the other, such as the semantic worries about syntactical \(\lambda\)-models mentioned above.&lt;/p&gt;&lt;p&gt;Before moving to model constructions, let us briefly mention that there are various ways of approaching \(\lambda\)-models. One particularly interesting approach we‚Äôve neglected so far is from the perspective of category theory and categorical logic. There are well-known model descriptions using so-called ‚ÄòCartesian closed categories‚Äô; see (Koymans, 1982). Covering these model descriptions goes beyond the scope of the present entry since it requires a familiarity with a wide range of concepts from category theory; see the entry Category Theory for a sense of the machinery involved. For the details of these model descriptions, instead, (Barendregt, 1985, sections 5.4‚Äì6). In recent years, there has been a renewed interest in categorical approaches to the \(\lambda\)-calculus, which have mainly focused on typed versions of the \(\lambda\)-calculus (see sections 8.2 and 9.1.2 below) but also include the untyped \(\lambda\)-calculus discussed in this article. See, for example, (Hyland, 2017) for a recent discussion.&lt;/p&gt;&lt;head rend="h3"&gt;7.2 Model Constructions&lt;/head&gt;&lt;p&gt;The term model we‚Äôve seen in section 7.1 is rather trivial: it directly reflects the syntactic structure of the \(\lambda\)-terms by modeling precisely syntactic equality modulo \(\boldsymbol{\lambda}\)-provable equality. This makes the term model mathematically and philosophically rather uninteresting. The construction and study of more interesting concrete \(\lambda\)-models is one of the principal aims of the model theory for the \(\lambda\)-calculus.&lt;/p&gt;&lt;p&gt;We‚Äôve already mentioned what‚Äôs perhaps the most important, but was definitely the first non-trivial model for the \(\lambda\)-calculus: Scott‚Äôs \(D_\infty\). But there are also other interesting model constructions, such as Plotkin and Scott‚Äôs graph model \(P_\omega\), first described in (Plotkin 1972) and (Scott, 1974). These model constructions, however, usually rely on fairly involved mathematical methods, both for their definitions and for verifying that they are indeed \(\lambda\)-models. Consequently, covering these constructions goes beyond the scope of this entry; see (Hinley and Seldin, 2008, chapter 16F) for an overview of various model constructions and (Barendregt, 1985, chapter 18) for many of the formal details.&lt;/p&gt;&lt;p&gt;One of the advantages of having different models is that one sees different aspects of equality in the \(\lambda\)-calculus: each of the different models takes a different view on what \(\lambda\)-terms get identified. An interesting question in this context is: What is the \(\lambda\)-theory of a given class of models? In this context, we call a class \(\mathcal{C}\) of \(\lambda\)-models complete just in case every (consistent) \(\lambda\)-theory is satisfied by some model in \(\mathcal{C}\). See (Salibra, 2003) for an overview of various completeness and incompleteness results for interesting classes of \(\lambda\)-models.&lt;/p&gt;&lt;head rend="h2"&gt;8. Extensions and Variations&lt;/head&gt;&lt;head rend="h3"&gt;8.1 Combinatory logic&lt;/head&gt;&lt;p&gt;A sister formalism of the \(\lambda\)-calculus, developed slightly earlier, deals with variable-free combinations. Combinatory logic is indeed even simpler than the \(\lambda\)-calculus, since it lacks a notion of variable binding.&lt;/p&gt;&lt;p&gt;The language of combinatory logic is built up from combinators and variables. There is some flexibility in precisely which combinators are chosen as basic, but some standard ones are \(\mathbf{I}, \bK , \bS, \mathbf{B}\) and \(\mathbf{C}\). (The names are not arbitrary.)&lt;/p&gt;&lt;p&gt;As with the \(\lambda\)-calculus, with combinatory logic one is interested in reducibility and provability. The principal reduction relations are:&lt;/p&gt;&lt;table&gt;&lt;row span="2"&gt;&lt;cell&gt;Combinator&lt;/cell&gt;&lt;cell&gt;Reduction Axiom&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;\(\bI\)&lt;/cell&gt;&lt;cell&gt;\(\bI x = x\)&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;\(\bK\)&lt;/cell&gt;&lt;cell&gt;\(\bK xy = x\)&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;\(\bS\)&lt;/cell&gt;&lt;cell&gt;\(\bS xyz = xz(yz)\)&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;\(\bB\)&lt;/cell&gt;&lt;cell&gt;\(\bB xyz = x(yz)\)&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;\(\bC\)&lt;/cell&gt;&lt;cell&gt;\(\bC xyz = xzy\)&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;There is a passage from \(\lambda\)-calculus to combinatory logic via translation. It turns out that although combinatory logic lacks a notion of abstraction, one can define such a notion and thereby simulate the \(\lambda\)-calculus in combinatory logic. Here is one translation; it is defined recursively.&lt;/p&gt;&lt;table&gt;&lt;row span="4"&gt;&lt;cell&gt;Rule&lt;/cell&gt;&lt;cell&gt;Expression&lt;/cell&gt;&lt;cell&gt;Translation&lt;/cell&gt;&lt;cell&gt;Condition&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;\(x\)&lt;/cell&gt;&lt;cell&gt;\(x\)&lt;/cell&gt;&lt;cell&gt;(unconditional)&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;2&lt;/cell&gt;&lt;cell&gt;\(MN\)&lt;/cell&gt;&lt;cell&gt;M\(^*\)N\(^*\)&lt;/cell&gt;&lt;cell&gt;(unconditional)&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;3&lt;/cell&gt;&lt;cell&gt;\(\lambda x[M]\)&lt;/cell&gt;&lt;cell&gt;\(\bK\)M&lt;/cell&gt;&lt;cell&gt;\(x\) does not occur freely in M&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;4&lt;/cell&gt;&lt;cell&gt;\(\lambda x[x]\)&lt;/cell&gt;&lt;cell&gt;\(\bI\)&lt;/cell&gt;&lt;cell&gt;(unconditional)&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;5&lt;/cell&gt;&lt;cell&gt;\(\lambda x[Mx]\)&lt;/cell&gt;&lt;cell&gt;M&lt;/cell&gt;&lt;cell&gt;\(x\) does not occur freely in M&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;6&lt;/cell&gt;&lt;cell&gt;\(\lambda x[MN]\)&lt;/cell&gt;&lt;cell&gt;\(\bB M(\lambda x[N)]^*\)&lt;/cell&gt;&lt;cell&gt;\(x\) does not occur freely in M&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;7&lt;/cell&gt;&lt;cell&gt;\(\lambda x[MN]\)&lt;/cell&gt;&lt;cell&gt;\(\bC (\lambda x[M])^*\)N&lt;/cell&gt;&lt;cell&gt;\(x\) does not occur freely in \(N\)&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;8&lt;/cell&gt;&lt;cell&gt;\(\lambda x[MN]\)&lt;/cell&gt;&lt;cell&gt;\(\bS M^*N^*\)&lt;/cell&gt;&lt;cell&gt;\(x\) occurs freely in both \(M\) and \(N\)&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;This translation works inside-out, rather than outside-in. To illustrate:&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;&lt;p&gt;The translation of the term \(\lambda y[y]\), a representative of the identity function, is mapped by this translation to the identity combinator \(\bI\) (because of Rule 4), as expected.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;The \(\lambda\)-term \(\lambda x[\lambda y[x]]\) that we have been calling ‚Äò\(\bK\)‚Äôis mapped by this translation to:&lt;/p&gt;\[\begin{align} \lambda x[\lambda y[x]] &amp;amp;\equiv \lambda x[\bK x] &amp;amp;\langle \text{Rule 1}\rangle \\ &amp;amp;\equiv \bK &amp;amp;\langle \text{Rule 3} \rangle \end{align}\]&lt;/item&gt;&lt;item&gt;&lt;p&gt;The \(\lambda\)-term \(\lambda x[\lambda y[yx]]\) that switches its two arguments is mapped by this translation to:&lt;/p&gt;\[\begin{align} \lambda x[\lambda y[yx]] &amp;amp;\equiv \lambda x[\bC(\lambda y[y])^* x] &amp;amp;\langle\text{Rule 8}\rangle \\ &amp;amp;\equiv \lambda x[\bC\bI x] &amp;amp;\langle\lambda y[y] \equiv \bI,\text{ by Rule 4}\rangle \\ &amp;amp;\equiv \bB\bC\bI)(\lambda x[x])^* &amp;amp;\langle\text{Rule 7}\rangle \\ &amp;amp;\equiv \bB(\bC\bI)\bI &amp;amp;\langle(\lambda x[x])^* \equiv \bI,\text{ by Rule 4}\rangle \end{align}\]&lt;p&gt;We can confirm that the \(\lambda\)-term \(\lambda x[\lambda y[yx]]\) and the translated combinatory logic term \(\bB(\bC\bI)\bI\) have analogous applicative behavior: for all \(\lambda\)-terms \(P\) and \(Q\) we have&lt;/p&gt;\[ (\lambda x[\lambda y[yx]])PQ \rhd (\lambda y[yP]) \rhd QP; \]&lt;p&gt;likewise, for all combinatory logic terms \(P\) and \(Q\) we have&lt;/p&gt;\[ \bB(\bC\bI)\bI PQ \rhd (\bC\bI)(\bI P)Q \rhd \bI Q(\bI P) \rhd Q(\bI P) \rhd QP \]&lt;/item&gt;&lt;/list&gt;&lt;p&gt;We can give but a glimpse of combinatory logic; for more on the subject, consult the entry on combinatory logic. Many of the issues discussed here for \(\lambda\)-calculus have analogues in combinatory logic, and vice versa.&lt;/p&gt;&lt;head rend="h3"&gt;8.2 Adding types&lt;/head&gt;&lt;p&gt;In many contexts of reasoning and computing it is natural to distinguish between different kinds of objects. The way this distinction is introduced is by requiring that certain formulas, functions, or relations accept arguments or permit substitution only of some kinds of objects rather than others. We might require, for example, that addition + take numbers as arguments. The effect of this restriction is to forbid, say, the addition of 5 and the identity function \(\lambda x.x\).[4] Regimenting objects into types is also the idea behind the passage from (unsorted, or one-sorted) first-order logic to many-sorted first-order logic. (See (Enderton, 2001) and (Manzano, 2005) for more about many-sorted first-order logic.) As it stands, the \(\lambda\)-calculus does not support this kind of discrimination; any term can be applied to any other term.&lt;/p&gt;&lt;p&gt;It is straightforward to extend the untyped \(\lambda\)-calculus so that it discriminates between different kinds of objects. This entry limits itself to the type-free \(\lambda\)-calculus. See the entries on type theory and Church‚Äôs type theory for a detailed discussion of the extensions of \(\lambda\)-calculus that we get when we add types, and see (Barendregt, Dekkers, Statman, 2013) for a book length treatment of the subject.&lt;/p&gt;&lt;p&gt;From a model-theoretic perspective, it‚Äôs interesting to add that (Scott, 1980) uses the semantic fact that categorical models for the untyped \(\lambda\)-calculus (see section 7.1) derive from the categorical models of the typed \(\lambda\)-calculus to argue for a conceptual priority of the typed over the untyped calculus.&lt;/p&gt;&lt;head rend="h2"&gt;9. Applications&lt;/head&gt;&lt;head rend="h3"&gt;9.1 Logic √† la \(\lambda\)&lt;/head&gt;&lt;p&gt;Here are two senses in which \(\lambda\)-calculus is connected with logic.&lt;/p&gt;&lt;head rend="h4"&gt;9.1.1 Terms as logical constants&lt;/head&gt;&lt;p&gt;In the table of combinators above, we defined combinators \(\bT\) and \(\bF\) and said that they serve as representations in the \(\lambda\)-calculus of the truth values true and false, respectively. How do these terms function as truth values?&lt;/p&gt;&lt;p&gt;It turns out that when one is treating \(\lambda\)-calculus as a kind of programming language, one can write conditional statements ‚ÄúIf \(P\) then \(A\) else \(B\)‚Äù simply as \(PAB\), where of course \(P, A\), and \(B\) are understood as \(\lambda\)-terms. If \(P \rhd \bT\), that is, P is ‚Äòtrue‚Äô, then we have&lt;/p&gt;\[ \text{if-}P\text{-then-}A\text{-else-}B := PAB \rhd \bT AB \rhd A, \]&lt;p&gt;(recall that, by definition, \(\bT \equiv \bK\)) and if \(P \rhd \bF\), that is, \(P\) is ‚Äòfalse‚Äô, then&lt;/p&gt;\[ \text{if-}P\text{-then-}A\text{-else-}B := PAB \rhd \bF AB \rhd B, \]&lt;p&gt;(recall that, by definition, \(\mathbf{F} \equiv \mathbf{KI})\) which is just what we expect from a notion of if-then-else. If \(P\) reduces neither to \(\mathbf{T}\) nor \(\mathbf{F}\), then we cannot in general say what \(\text{if-}P\text{-then-}A\text{-else-}B\) is.&lt;/p&gt;&lt;p&gt;The encoding we‚Äôve just sketched of some of the familiar truth values and logical connectives of classical truth-table logic does not show that \(\lambda\)-calculus and classical logic are intimately related. The encoding shows little more than embeddibility of the rules of computation of classical truth-table logic in \(\lambda\)-calculus. Logics other than classical truth-table logic can likewise be represented in the \(\lambda\)-calculus, if one has sufficient computable ingredients for the logic in question (e.g., if the logical consequence relation is computable, or if a derivability relation is computable, etc.). For more on computing with \(\lambda\)-calculus, see section 9.2 below. A more intrinsic relationship between logic and \(\lambda\)-calculus is discussed in the next section.&lt;/p&gt;&lt;head rend="h4"&gt;9.1.2 Typed \(\lambda\)-calculus and the Curry-Howard-de Bruijn correspondence&lt;/head&gt;&lt;p&gt;The correspondence to be descried here between logic and the \(\lambda\)-calculus is seen with the help of an apparatus known as types. This section sketches the beginnings of the development of the subject known as type theory. We are interested in developing type theory only so far as to make the so-called Curry-Howard-de Bruijn correspondence visible. A more detailed treatment can be found in the entry on type theory; see also (Hindley, 1997) and (Barendregt, Dekkers, Statman, 2013).&lt;/p&gt;&lt;p&gt;Type theory enriches the untyped \(\lambda\)-calculus by requiring that terms be given types. In the untyped \(\lambda\)-calculus, the application \(MN\) is a legal term regardless of what \(M\) and \(N\) are. Such freedom permits one to form such suspicious terms as \(xx\), and thence terms such as the paradoxical combinator \(\mathbf{Y}\). One might wish to exclude terms like \(xx\) on the grounds that \(x\) is serving both as a function (on the left-hand side of the application) and as an argument (on the right-hand side of the application). Type theory gives us the resources for making this intuitive argument more precise.&lt;/p&gt;&lt;p&gt;Assigning types to terms The language of type theory begins with an (infinite) set of type variables (which is assumed to be disjoint from the set of variables of the \(\lambda\)-calculus and from the symbol ‚Äò\(\lambda\)‚Äô itself). The set of types is made up of type variables and the operation \(\sigma \rightarrow \tau\). Variables in type theory now come with a type annotation (unlike the unadorned term variables of untyped \(\lambda\)-calculus). Typed variables are rendered ‚Äò\(x : \sigma\)‚Äô; the intuitive reading is ‚Äòthe variable \(x\) has the type \(\sigma\)‚Äô. The intuitive reading of the judgment ‚Äò\(t : \sigma \rightarrow \tau\)‚Äô is that the term \(t\) is a function that transforms arguments of type \(\sigma\) into arguments of type \(\tau\). Given an assignment of types to term variables, one has the typing rules:&lt;/p&gt;\[ (M : \sigma \rightarrow \tau)(N : \sigma) : \tau \]&lt;p&gt;and&lt;/p&gt;\[ (\lambda x : \sigma[M : \tau]) : \sigma \rightarrow \tau \]&lt;p&gt;The above two rules define the assignment of types to applications and to abstraction terms. The set of terms of type theory is the set of terms built up according to these formation rules.&lt;/p&gt;&lt;p&gt;The above definition of the set of terms of type theory is sufficient to rule out terms such as \(xx\). Of course, ‚Äò\(xx\)‚Äô is not a typed term at all for the simple reason that no types have been assigned to it. What is meant is that there is no type \(\sigma\) that could be assigned to \(x\) such that ‚Äò\(xx\)‚Äô could be annotated in a legal way to make a typed term. We cannot assign to \(x\) a type variable, because then the type of the left-hand \(x\) would fail to be a function type (i.e., a type of the shape ‚Äò\(\sigma \rightarrow \tau\)‚Äô). Moreover, we cannot assign to \(x\) a function type \(\sigma \rightarrow \tau\), because then then \(\sigma\) would be equal to \(\sigma \rightarrow \tau\), which is impossible.&lt;/p&gt;&lt;p&gt;As a leading example, consider the types that are assigned to the combinators \(\bI\), \(\bK\), and \(\bS\):&lt;/p&gt;&lt;table&gt;&lt;row span="2"&gt;&lt;cell&gt;Combinator&lt;/cell&gt;&lt;cell&gt;Type[5]&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;\(\bI\)&lt;/cell&gt;&lt;cell&gt;\(a \rightarrow a\)&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;\(\bK\)&lt;/cell&gt;&lt;cell&gt;\(a \rightarrow(b \rightarrow a)\)&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;\(\bS\)&lt;/cell&gt;&lt;cell&gt;\( (a \rightarrow(b \rightarrow c)) \rightarrow ((a \rightarrow b) \rightarrow(a \rightarrow c))\)&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;(See Hindley (1997) Table of principal types for a more extensive listing.) If we read ‚Äò\(\rightarrow\)‚Äô as implication and type variables as propositional variables, then we recognize three familiar tautologies in the right-hand column of the table. The language used is meager: there are only propositional variables and implication; there are no other connectives.&lt;/p&gt;&lt;p&gt;The table suggests an interesting correspondence between the typed \(\lambda\)-calculus and formal logic. Could it really be that the types assigned to formulas, when understood as logical formulas, are valid? Yes, though ‚Äòvalidity‚Äô needs to understood not as classical validity:&lt;/p&gt;&lt;p&gt;Theorem If \(\tau\) is the type of some \(\lambda\)-term, then \(\tau\) is intuitionistically valid.&lt;/p&gt;&lt;p&gt;The converse of this theorem holds as well:&lt;/p&gt;&lt;p&gt;Theorem If \(\phi\) is an intuitionistically valid logical formula whose only connective is implication \((\rightarrow)\), then \(\phi\) is the type of some \(\lambda\)-term.&lt;/p&gt;&lt;p&gt;The correspondence can be seen when one identifies intuitionistic validity with derivability in a certain natural deduction formalism. For a proof of these two theorems, see (Hindley, 1997, chapter 6).&lt;/p&gt;&lt;p&gt;The correspondence expressed by the previous two theorems between intuitionistic validity and typability is known as the Curry-Howard-de Bruijn correspondence, after three logicians who noticed it independently. The correspondence, as stated, is between only propositional intuitionistic logic, restricted to the fragment containing only the implication connective \(\rightarrow\). One can extend the correspondence to other connectives and to quantifiers, too, but the most crisp correspondence is at the level of the implication-only fragment. For details, see (Howard, 1980).&lt;/p&gt;&lt;head rend="h3"&gt;9.2 Computing&lt;/head&gt;&lt;p&gt;One can represent natural numbers in a simple way, as follows:&lt;/p&gt;&lt;p&gt;Definition (ordered tuples, natural numbers) The ordered tuple \(\langle a_0,\ldots a_n\rangle\) of \(\lambda\)-terms is defined as \(\lambda x[x a_0\ldots a_n]\). One then defines the \(\lambda\)-term \(\ulcorner n\urcorner\) corresponding to the natural number \(n\) as: \(\ulcorner 0\urcorner = \mathbf{I}\) and, for every \(k\), \(\ulcorner k + 1\urcorner = \langle \mathbf{F}, \ulcorner k\urcorner\rangle\).&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;&lt;p&gt;The \(\lambda\)-term corresponding to the number 1, on this representation, is:&lt;/p&gt;\[\begin{align} \ulcorner 1 \urcorner &amp;amp;\equiv \langle\bF,\ulcorner 0\urcorner\rangle \\ &amp;amp;\equiv \langle\bF,\bI\rangle \\ &amp;amp;\equiv \lambda x[x\mathbf{FI}] \end{align}\]&lt;/item&gt;&lt;item&gt;&lt;p&gt;The \(\lambda\)-term corresponding to the number 2, on this representation, is:&lt;/p&gt;\[\begin{align} \ulcorner 2 \urcorner &amp;amp;\equiv \langle\bF,\ulcorner 1\urcorner\rangle \\ &amp;amp;\equiv \lambda x[x\mathbf{F}\lambda x[x\mathbf{FI}]] \end{align}\]&lt;/item&gt;&lt;item&gt;&lt;p&gt;Similarly, \(\ulcorner 3\urcorner\) is \(\lambda x[x\mathbf{F}\lambda x[x\mathbf{F}\lambda x[x\mathbf{FI}]]]\).&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Various representations of natural numbers are available; this representation is but one.[6]&lt;/p&gt;&lt;p&gt;Using the ingredients provided by the \(\lambda\)-calculus, one can represent all recursive functions. This shows that the model is exactly as expressive as other models of computing, such as Turing machines and register machines. For a more detailed discussion of the relation between these different models of computing, see the section comparing the Turing and Church approaches in the entry on the Church-Turing Thesis.&lt;/p&gt;&lt;p&gt;Theorem For every recursive function \(f\) of arity \(n\), there exists a \(\lambda\)-term \(f^*\) such that&lt;/p&gt;&lt;p&gt;for all natural numbers \(a_1,\ldots a_n\): \(f(a_1,\ldots a_n) = y\) iff \(\boldsymbol{\lambda} \vdash f^*\langle \bar{a}_1,\ldots,\bar{a}_n\rangle = \bar{y}\)&lt;/p&gt;&lt;p&gt;For a proof, see the appendix.&lt;/p&gt;&lt;p&gt;Since the class of recursive functions is an adequate representation of the class of all computable (number-theoretic) functions, thanks to the work above we find that all computable (number-theoretic) functions can be faithfully represented in the \(\lambda\)-calculus.&lt;/p&gt;&lt;head rend="h3"&gt;9.3 Relations&lt;/head&gt;&lt;p&gt;The motivation for the \(\lambda\)-calculus given at the beginning of the entry was based on reading \(\lambda\)-expressions as descriptions of functions. Thus, we have understood ‚Äò\(\lambda x[M]\)‚Äô to be a (or the) function that, given \(x\), gives \(M\) (which generally, though not necessarily, involves x). But it is not necessary to read \(\lambda\)-terms as functions. One could understand \(\lambda\)-terms as denoting relations, and read an abstraction term ‚Äò\(\lambda x[M]\)‚Äô as the unary relation (or property) \(R\) that holds of an argument \(x\) just in case \(M\) does (see Carnap 1947, p. 3). On the relational reading, we can understand an application term \(MN\) as a form of predication. One can make sense of these terms using the principle of \(\beta\)-conversion:&lt;/p&gt;\[ (\lambda x[M])a = M[x := A], \]&lt;p&gt;which says that the abstraction relation \(\lambda x[M]\), predicated of A, is the relation obtained by plugging in A for all free occurrences of \(x\) inside \(M\).&lt;/p&gt;&lt;p&gt;As a concrete example of this kind of approach to \(\lambda\)-calculus, consider an extension of first-order logic where one can form new atomic formulas using \(\lambda\)-terms, in the following way:&lt;/p&gt;&lt;p&gt;Syntax: For any formula \(\phi\) and any finite sequence \(x_1 , \ldots ,x_n\) of variables, the expression ‚Äò\(\lambda x_1 \ldots x_n [\phi]\)‚Äô is a predicate symbol of arity n. Extend the notion of free and bound variables (using the functions \(\mathbf{FV}\) and \(\mathbf{BV})\) in such a way that&lt;/p&gt;\[ \mathbf{FV}(\lambda x_1 \ldots x_n [\phi]) = \mathbf{FV}(\phi) - \{ x_1 , \ldots x_n \} \]&lt;p&gt;and&lt;/p&gt;\[ \mathbf{BV}(\lambda x_1 \ldots x_n [\phi]) = \mathbf{BV}(\phi) \cup \{ x_1 , \ldots x_n \} \]&lt;p&gt;Deduction Assume as axioms the universal closures of all equivalences&lt;/p&gt;\[ \lambda x_1 \ldots x_n [\phi](t_1 ,\ldots t_n) \leftrightarrow \phi[x_1 ,\ldots x_n := t_1,\ldots t_n] \]&lt;p&gt;where \(\phi[x_1 ,\ldots x_n := t_1,\ldots t_n]\) denotes the simultaneous substitution of the terms \(t_k\) for the variables \(x_k\) \((1 \le k \le n)\).&lt;/p&gt;&lt;p&gt;Semantics For a first-order structure \(A\) and an assignment \(s\) of elements of \(A\) to variables, define&lt;/p&gt;\[\begin{align} A \vDash &amp;amp;\lambda x_1 \ldots x_n [\phi](t_1 ,\ldots t_n) [s] \text{ iff } \\ &amp;amp;A \vDash \phi[x_1 ,\ldots x_n := t_1,\ldots t_n] [s] \end{align}\]&lt;p&gt;According to this approach, one can use a \(\lambda\) to treat essentially any formula, even complex ones, as if they were atomic. We see the principle of \(\beta\)-reduction in the deductive and semantic parts. That this approach adheres to the relational reading of \(\lambda\) terms can be seen clearly in the semantics: according to the standard Tarski-style semantics for first-order logic, the interpretation of a formula (possibly with free variables) denotes a set of tuples of elements of the structure, as we vary the variable assignment that assigns elements of the structure to the variables.&lt;/p&gt;&lt;p&gt;One can ‚Äòinternalize‚Äô this functional approach. This is done in the case of various property theories, formal theories for reasoning about properties as metaphysical objects (Bealer 1982, Zalta 1983, Menzel 1986, 1993, and Turner 1987). This kind of theory is employed in certain metaphysical investigations where properties are metaphysical entities to be investigated. In these theories, metaphysical relations are (or are among) the objects of interest; just as we add term-building symbols + and \(\times\) in formal theories of arithmetic to build numbers, \(\lambda\) is used in property theory to build relations. This approach contrasts with the approach above. There, \(\lambda\) was added to the grammar of first-order logic by making it a recipe for building atomic formulas; it was a new formula-building operator, like \(\vee\) or \(\rightarrow\) or the other connectives. In the case of property theories, the \(\lambda\) plays a role more like + and \(\times\) in formal theories of arithmetic: it is used to construct relations (which, in this setting, are to be understood as a kind of metaphysical object). Unlike + and \(\times\), though, the \(\lambda\) binds variables.&lt;/p&gt;&lt;p&gt;To give an illustration of how \(\lambda\) is used in this setting, let us inspect the grammar of a typical application (McMichael and Zalta, 1980). One typically has a predication operator (or, more precisely, a family of predication operators) \(p_k (k \ge 0)\). In a language where we have terms \(\mary\) and \(\john\) and a binary relation loves, we can formally express:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;John loves Mary: \(\loves(\john ,\mary)\)&lt;/item&gt;&lt;item&gt;The property that John loves Mary: \(\lambda[\loves(\john ,\mary)]\) (note that the \(\lambda\) is binding no variables; we might call this ‚Äòvacuous binding‚Äô. Such properties can be understood as propositions.)&lt;/item&gt;&lt;item&gt;The property of an object \(x\) that John loves it: \(\lambda x [\loves(\john,x)]\).&lt;/item&gt;&lt;item&gt;The property that Mary is loved by something: \(\lambda[\exists x(\loves(x,\mary))]\) (another instance of vacuous binding, viz., proposition)&lt;/item&gt;&lt;item&gt;The predication of the property of \(x\) that John loves \(x\) to Mary: \(p_1 (\lambda x[\loves(\john,x)],\mary)\).&lt;/item&gt;&lt;item&gt;The (0-ary) predication of the property that John loves Mary: \(p_0 (\lambda x[\loves(\john,\mary)])\).&lt;/item&gt;&lt;item&gt;The property of objects \(x\) and \(y\) that \(x\) loves \(y\): \(\lambda xy[\loves(x,y)]\).&lt;/item&gt;&lt;item&gt;The property of an objects \(x\) that \(x\) loves itself: \(\lambda x[\loves(x,x)]\).&lt;/item&gt;&lt;item&gt;The predication of the property of objects \(x\) and \(y\) that \(x\) loves \(y\) to John and Mary (in that order): \(p_2 (\lambda xy[\loves(x,y)],\john,\mary)\).&lt;/item&gt;&lt;/list&gt;&lt;p&gt;We reason with these \(\lambda\)-terms using a \(\beta\)-conversion principle such as:&lt;/p&gt;\[\begin{align} p_n (\lambda x_1,&amp;amp;\ldots x_n [A], t_1 , \ldots ,t_n) \leftrightarrow \\ &amp;amp;A[x_1 ,\ldots x_n := t_1,\ldots, t_n] \end{align}\]&lt;p&gt;Formally, the predication operator p\(_k\) is a \((k+1)\)-ary predicate symbol. The first argument is intended to be a \(\lambda\)-term of \(k\) arguments, and the rest of the arguments are intended to be the arguments of the body of the \(\lambda\)-term. The \(\beta\)-principle above says that the predication of an \(n\)-ary \(\lambda\)-term \(L\) to \(n\) terms holds precisely when the body of \(L\) holds of those terms.&lt;/p&gt;&lt;p&gt;It turns out that in these theories, we may or may not be able to be fully committed to the principle of \(\beta\)-conversion. Indeed, in some property theories, the full principle of \(\beta\)-conversion leads to paradox, because one can replay a Russell-style argument when the full principle of \(\beta\)-conversion is in place. In such settings, one restricts the formation of \(\lambda\)-formulas by requiring that the body of a \(\lambda\)-term not contain further \(\lambda\)-terms or quantifiers. For further discussion, see (Orilia, 2000).&lt;/p&gt;&lt;p&gt;One of the reasons why property theories formulated in the \(\lambda\)-calculus are of a particular philosophical importance is the hyperintensional nature of the calculus (see section 1.2). A property concept may be called ‚Äòhyperintensional‚Äô if and only if it does not identify necessarily coextensional properties, i.e., properties that are instanciated by exactly the same objects at every possible world. The properties and relations described by the theories of Bealer, Zalta, Menzel, and Turner have exactly this characteristic. In other words, the theories are hyperintensional property theories. Recent years have seen a significant rise of interest in hyperintensional concepts of properties in metaphysics (Nolan 2014), and correspondingly property theories formulated in the \(\lambda\)-calculus will likely experience a rise of interest as well.&lt;/p&gt;&lt;p&gt;In the context of the foundations of mathematics, Zalta and Oppenheimer (2011) argue for the conceptual priority of the relational interpretation of \(\lambda\)-terms over the functional one.&lt;/p&gt;&lt;head rend="h2"&gt;Bibliography&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;Baader, Franz and Tobias Nipkow, 1999, Term Rewriting and All That, Cambridge: Cambridge University Press.&lt;/item&gt;&lt;item&gt;Barendregt, Henk, 1985, The Lambda Calculus: Its Syntax and Semantics (Studies in Logic and the Foundations of Mathematics 103), 2nd edition, Amsterdam: North-Holland.&lt;/item&gt;&lt;item&gt;Barendregt, Henk, 1993, ‚ÄúLambda calculi with types‚Äù, in S. Abramsky, D. Gabbay, T. Maibaum, and H. Barendregt (eds.), Handbook of Logic in Computer Science (Volume 2), New York: Oxford University Press, pp. 117‚Äì309.&lt;/item&gt;&lt;item&gt;Barendregt, Henk, Wil Dekkers, and Richard Statman., 2013, Lambda Calculus With Types, Cambridge: Cambridge University Press.&lt;/item&gt;&lt;item&gt;Bealer, George, 1982, Quality and Concept, Oxford: Clarendon Press.&lt;/item&gt;&lt;item&gt;van Benthem, Johan, 1998, A Manual of Intensional Logic, Stanford: CSLI Publications.&lt;/item&gt;&lt;item&gt;Carnap, Rudolf, 1947, Meaning and Necessity, Chicago: University of Chicago Press.&lt;/item&gt;&lt;item&gt;Church, Alonzo, 1932, ‚ÄúA set of postulates for the foundation of logic‚Äù, Annals of Mathematics (2nd Series), 33(2): 346‚Äì366.&lt;/item&gt;&lt;item&gt;Cutland, Nigel J., 1980, Computability, Cambridge: Cambridge University Press.&lt;/item&gt;&lt;item&gt;Doets, Kees and Jan van Eijk, 2004, The Haskell Road to Logic, Maths and Programming, London: College Publications.&lt;/item&gt;&lt;item&gt;Enderton, Herbert B., 2001, A Mathematical Introduction to Logic, 2nd edition, San Diego: Harcourt/Academic Press.&lt;/item&gt;&lt;item&gt;Frege, Gottlob, 1893, Grundgesetze der Arithmetik, Jena: Verlag Hermann Pohle, Band I; partial translation as The Basic Laws of Arithmetic, M. Furth (trans.), Berkeley: University of California Press, 1964.&lt;/item&gt;&lt;item&gt;Kleene, Stephen C., 1981, ‚ÄúOrigins of recursive function theory‚Äù, Annals of the History of Computing, 3(1): 52‚Äì67.&lt;/item&gt;&lt;item&gt;Heim, Irene and Angelika Kratzer, 1998, Semantics in Generative Grammar, Malden, MA: Blackwell.&lt;/item&gt;&lt;item&gt;Hindley, J. Roger, 1997, Basic Simple Type Theory (Cambridge Tracts in Theoretical Computer Science 42), New York: Cambridge University Press.&lt;/item&gt;&lt;item&gt;Hindley, J. Roger and G. Longo, 1980, ‚ÄúLambda-calculus Models and Extensionality.‚Äù Zeitschrift f√ºr mathematische Logik und Grundlagen der Mathematik, 26: 289‚Äì310.&lt;/item&gt;&lt;item&gt;Hindley, J. Roger and Jonathan P. Seldin, 2008, Lambda-Calculus and Combinators: An Introduction, 2nd edition, Cambridge: Cambridge University Press.&lt;/item&gt;&lt;item&gt;Howard, William A., 1980, ‚ÄúThe formula-as-types notion of construction‚Äù, in J. Hindley and J. Seldin (eds.), To H. B. Curry: Essays on Combinatory Logic, Lambda-Calculus, and Formalism, London: Academic Press, pp. 479‚Äì490.&lt;/item&gt;&lt;item&gt;Hyland, J. Martin E., 2017, ‚ÄúClassical Lambda Calculus in Modern Dress‚Äù, Mathematical Structures in Computer Science, 27(5): 762‚Äì781.&lt;/item&gt;&lt;item&gt;Koymans, C.P.J., 1982, ‚ÄúModels of the Lambda Calculus‚Äù, Information and Control, 52: 306‚Äì332.&lt;/item&gt;&lt;item&gt;Manzano, Maria, 2005, Extensions of First-order Logic (Cambridge Tracts in Theoretical Computer Science 19), Cambridge: Cambridge University Press.&lt;/item&gt;&lt;item&gt;McCarthy, John, 1960, ‚ÄúRecursive functions of symbolic expressions and their computation by machine (Part I)‚Äù, Communications of the ACM, 3(4): 184‚Äì195.&lt;/item&gt;&lt;item&gt;McMichael, Alan and Edward N. Zalta, 1980, ‚ÄúAn alternative theory of nonexistent objects‚Äù, Journal of Philosophical Logic, 9: 297‚Äì313.&lt;/item&gt;&lt;item&gt;Menzel, Christopher, 1986, ‚ÄúA complete, type-free second order logic of properties, relations, and propositions‚Äù, Technical Report #CSLI-86-40, Stanford: CSLI Publications.&lt;/item&gt;&lt;item&gt;Menzel, Christopher, 1993, ‚ÄúThe proper treatment of predication in fine-grained intensional logic‚Äù, Philosophical Perspectives 7: 61‚Äì86.&lt;/item&gt;&lt;item&gt;Meyer, Albert R., 1982, ‚ÄúWhat is a model of the lambda calculus?‚Äù, In Information and Control, 52(1): 87‚Äì122.&lt;/item&gt;&lt;item&gt;Nederpelt, Rob, with Herman Geuvers and Roel de Vriejer (eds.), 1994, Selected Papers on Automath (Studies in Logic and the Foundations of Mathematics 133), Amsterdam: North-Holland.&lt;/item&gt;&lt;item&gt;Nolan, Daniel, 2014, ‚ÄúHyperintensional metaphysics‚Äù, Philosophical Studies, 171(1); 149‚Äì160.&lt;/item&gt;&lt;item&gt;Orilia, Francesco, 2000, ‚ÄúProperty theory and the revision theory of definitions‚Äù, Journal of Symbolic Logic, 65(1): 212‚Äì246.&lt;/item&gt;&lt;item&gt;Partee, Barbara H., with Alice ter Meulen and Robert E. Wall, 1990, Mathematical Methods in Linguistics, Berlin: Springer.&lt;/item&gt;&lt;item&gt;Plotkin, G.D., 1972, A Set-Theoretical Definition of Application, School of Artificial Intelligence, Memo MIP-R-95, University of Edinburgh.&lt;/item&gt;&lt;item&gt;Revesz, George E., 1988, Lambda-Calculus, Combinators, and Functional Programming, Cambridge: Cambridge University Press; reprinted 2008.&lt;/item&gt;&lt;item&gt;Rosser, J. Barkley, 1984, ‚ÄúHighlights of the History of the Lambda-Calculus‚Äù, Annals of the History of Computing, 6(4): 337‚Äì349.&lt;/item&gt;&lt;item&gt;Salibra, Antonio, 2003, ‚ÄúLambda calculus: models and theories‚Äù, in Proceedings of the Third AMAST Workshop on Algebraic Methods in Language Processing (AMiLP-2003), No. 21, University of Twente, pp. 39‚Äì54.&lt;/item&gt;&lt;item&gt;Sch√∂nfinkel, Moses, 1924, ‚ÄúOn the building blocks of mathematical logic‚Äù, in J. van Heijenoort (ed.), From Frege to G√∂del: A Source Book in Mathematical Logic, Cambridge, MA: Harvard University Press, 1967, pp. 355‚Äì366.&lt;/item&gt;&lt;item&gt;Scott, Dana, 1974, ‚ÄúThe LAMBDA language‚Äù, Journal of Symbolic Logic, 39: 425‚Äì427.&lt;/item&gt;&lt;item&gt;‚Äì‚Äì‚Äì, 1980, ‚ÄúLambda Calculus: Some Models, Some Philosophy‚Äù, in J. Barwise, H.J. Keisler, and K. Kunen (eds.), The Kleene Symposium, Amsterdam: North-Holland, pp. 223‚Äì265.&lt;/item&gt;&lt;item&gt;Troelstra, Anne and Helmut Schwichtenberg, 2000, Basic Proof Theory (Cambridge Tracts in Theoretical Computer Science 43), 2nd edition, Cambridge: Cambridge University Press.&lt;/item&gt;&lt;item&gt;Turing, Alan M., 1937, ‚ÄúComputability and \(\lambda\)-definability‚Äù, Journal of Symbolic Logic, 2(4): 153‚Äì163.&lt;/item&gt;&lt;item&gt;Turner, Richard, 1987, ‚ÄúA theory of properties‚Äù, Journal of Symbolic Logic, 52(2): 455‚Äì472.&lt;/item&gt;&lt;item&gt;Zalta, Edward N., 1983, Abstract Objects: An Introduction to Axiomatic Metaphysics, Dordrecht: D. Reidel.&lt;/item&gt;&lt;item&gt;Zalta, Edward N. and Paul Oppenheimer, 2011, ‚ÄúRelations versus functions at the foundations of logic: type-theoretic considerations‚Äù, Journal of Logic and Computation 21: 351‚Äì374.&lt;/item&gt;&lt;item&gt;Zerpa, L., 2021, ‚ÄúThe Teaching and Learning of the Untyped Lambda Calculus Through Web-Based e-Learning Tools‚Äù, in K. Arai Intelligent Computing (Lecture Notes in Networks and Systems: Volume 285), Cham: Springer, pp. 419‚Äì436.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;Academic Tools&lt;/head&gt;&lt;quote&gt;&lt;td&gt;How to cite this entry.&lt;/td&gt;&lt;td&gt;Preview the PDF version of this entry at the Friends of the SEP Society.&lt;/td&gt;&lt;td&gt;Look up topics and thinkers related to this entry at the Internet Philosophy Ontology Project (InPhO).&lt;/td&gt;&lt;td&gt;Enhanced bibliography for this entry at PhilPapers, with links to its database.&lt;/td&gt;&lt;/quote&gt;&lt;head rend="h2"&gt;Other Internet Resources&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;The Lambda Calculator, a tool for working with \(\lambda\)-terms with an eye toward their use in formal semantics of natural language.&lt;/item&gt;&lt;item&gt;Lambda calculus reduction workbench, for visualizing reduction strategies.&lt;/item&gt;&lt;item&gt;‚Äú\(\lambda\)-Calculus: Then and Now,‚Äù useful handout on the milestones in, contributors to, and bibliography on the \(\lambda\)-calculus, presented at the several Turing Centennial conferences. There also exists a video recording of the lecture given on the occasion of Princeton University‚Äôs celebration of the Turing Centennial in 2012.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;Acknowledgments&lt;/head&gt;&lt;p&gt;The first author wishes to acknowledge the contributions of Henk Barendregt, Elizabeth Coppock, Reinhard Kahle, Martin S√∏rensen, and Ed Zalta in helping to craft this entry. He also thanks Nic McPhee for introducing him to the \(\lambda\)-calculus.&lt;/p&gt;&lt;p&gt;The second author would like to acknowledge the useful comments and suggestions of Fabrizio Cariani, Cameron Moy, Peter Percival, and Ed Zalta.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45361344</guid><pubDate>Wed, 24 Sep 2025 15:00:15 +0000</pubDate></item><item><title>How to Be a Leader When the Vibes Are Off</title><link>https://chaoticgood.management/how-to-be-a-leader-when-the-vibes-are-off/</link><description>&lt;doc fingerprint="7a01915d36b1034c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;How to Be a Leader When the Vibes Are Off&lt;/head&gt;
    &lt;head rend="h2"&gt;...and the vibes are definitely off&lt;/head&gt;
    &lt;p&gt;It feels different in tech right now. We‚Äôre coming off a long era where optimism carried the industry. Something has curdled. AI hype, return-to-office mandates, and continued layoffs have shifted the mood. Managers are quicker to fire, existential dread has replaced the confidence that a tight job market for developers provided for decades. The vibes are for sure off.&lt;/p&gt;
    &lt;head rend="h3"&gt;What‚Äôs Changed?&lt;/head&gt;
    &lt;p&gt;(What follows are generalizations. If your company is escaping some or all of these, I applaud you. I‚Äôm sure there are exceptions.)&lt;/p&gt;
    &lt;p&gt;AI has injected some destabilization. ‚ÄúI don‚Äôt need junior devs when I can just pay $20/month for Cursor‚Äù has an effect on everyone even if this turns out to be silly down the road. I see lots of people worried that the aim of all of this is to ultimately have a robot do their entire job. Whether or not this is possible doesn‚Äôt mean people aren‚Äôt going to try. And it‚Äôs the trying that raises people‚Äôs anxiety. On top of that, we‚Äôve also got ‚ÄúAI Workslop‚Äù to contend with as well, which is making work harder for the diligent among us.&lt;/p&gt;
    &lt;p&gt;Return to Office feels like trust has been broken. Teams that continued to work well (or in some cases, better) after everyone in the industry went remote are now being told to come back to desks in offices. I‚Äôve even heard tales of this happening despite there not being enough office space for everyone, which seems very silly. Also, for the first time in my nearly 30-year career, I‚Äôve even heard of people being told they need to be ‚Äúat their desks at 9am‚Äù and ‚Äúexpected to stay until 5pm at a minimum.‚Äù Even before COVID-19 and the mass move to remote work, most companies were flexible on start and stop times. I almost never heard of set hours for software developers until recently. Rules like that scream ‚Äúwe don‚Äôt trust you unless we can see you,‚Äù even if that‚Äôs not really the reason for the mandates. (IMO there are benefits to working in the same location as your colleagues but ham-fisted, poorly thought out mandates are not the way to achieve them.)&lt;/p&gt;
    &lt;p&gt;Layoffs changed the market. For probably 20 years, job security wasn‚Äôt really a concern in the industry. Layoffs happened here and there and companies folded, but the demand was always strong and most people capable of writing code or managing people who write code could lose their job, spend the severance on a nice vacation, and return with the confidence that they‚Äôd be able to land a new gig in a couple of weeks, likely at higher pay. With the acknowledgement that this was a privilege not enjoyed by most of the working world, it is no longer true. The size and scope of layoffs over the last couple of years have injected more anxiety into the tech workforce.&lt;/p&gt;
    &lt;p&gt;C-Suite Energy has changed. Across the board, execs seem more efficiency-focused, financialized, and less mission-driven. The days of ‚Äútake care of the employees and the employees will take care of the business‚Äù feel like they‚Äôre in the rear-view mirror, and a new ‚Äúdo your job, or else!‚Äù mentality has taken its place.&lt;/p&gt;
    &lt;p&gt;You can‚Äôt change the macro forces that are driving these trends, but you can control how you show up for your team.&lt;/p&gt;
    &lt;head rend="h2"&gt;Wearing the ‚ÄòCompany Hat‚Äô vs. Chaotic-Good Leadership&lt;/head&gt;
    &lt;p&gt;My standard advice to anyone with a management role and anyone at the Staff+ level of individual contributor is that ‚Äúwearing the company hat‚Äù should be the default. You‚Äôre not always going to agree with the decisions that come down from the top. Even when you don‚Äôt agree with decisions the company leadership is making, part of your job is representing and facilitating those decisions with full alignment. When acting ‚Äúin public‚Äù (all-hands, department meetings, the #general channel), this is mandatory, as contradicting the bosses in a broad forum can kill the credibility you have the leadership across the wider team. It‚Äôs also a good way to get yourself fired.&lt;/p&gt;
    &lt;head rend="h3"&gt;Let them know you‚Äôre still on their side&lt;/head&gt;
    &lt;p&gt;But you know what also kills trust? Telling your team it‚Äôs sunny out when everyone can plainly see that it‚Äôs raining. Your team is made up of smart adults who can, at the very least, count the number of employees and the number of desks and calculate that ‚Äúeveryone in the office on Wednesday‚Äù isn‚Äôt going to work out well if the people outnumber the chairs. Telling them something else is going to make you look like an idiot toady in their eyes.&lt;/p&gt;
    &lt;p&gt;The right thing to do in this situation is to acknowledge that you see the situation the same way they do, but do it privately, within your immediate team only or in 1-1s. ‚ÄúYeah, this new policy sucks, I get it. It‚Äôs going to affect me in negative ways too.‚Äù It‚Äôs really important that you validate the emotions that all of these aspects are bringing up in people.&lt;/p&gt;
    &lt;head rend="h3"&gt;Don‚Äôt pretend you can fix it&lt;/head&gt;
    &lt;p&gt;You can promise to advocate for saner policies when the opportunity arises if your sphere of influence makes that possible, but don‚Äôt promise to make the problem go away if you can‚Äôt. Broken promises and poor do/say ratio performance will also kill your team‚Äôs faith in you, especially when it‚Äôs about things they really care about. And again, this is not a time for grandstanding. In public, you have to support the policies, but when you‚Äôre in private with your manager and your peers, that‚Äôs the time you can safely push for change.&lt;/p&gt;
    &lt;head rend="h3"&gt;Find small workarounds to make things livable&lt;/head&gt;
    &lt;p&gt;If you can provide some flexibility on seemingly inflexible policies, do it. If your management role includes enforcing the company‚Äôs rules, you can use some discretion about how strictly you want to enforce them. Personally, I would never want to ‚Äúrat out‚Äù a good performer who can‚Äôt get to their desk by 9am sharp because they have to drop off their kids or punish someone who bugged out early once to catch their favourite performer in concert one town over. Small acts demonstrating that you trust your team, even if the C-Suite doesn‚Äôt seem to trust the broader team the way they used to, can go a long way toward maintaining good morale within your group.&lt;/p&gt;
    &lt;p&gt;When things feel shaky in the broader org, people will look more to their direct leader for a sense of stability. The best thing you can do for them is provide it. Quiet honesty builds credibility and fosters loyalty.&lt;/p&gt;
    &lt;head rend="h2"&gt;This too shall pass&lt;/head&gt;
    &lt;p&gt;The industry is going through a period where a lot is changing all at once. We‚Äôve had a few of them before. Things will eventually settle down into a new normal. I‚Äôm not great at predictions, so I‚Äôll refrain from detailing what I think things will look like, but I don‚Äôt think it‚Äôll be entirely unfamiliar to those who were here before this latest inflection point. This is especially true if leaders who care and treat their staff like adults can stay grounded and stay true to their principles, even when that means performing small, quiet acts of rebellion.&lt;/p&gt;
    &lt;p&gt;You can‚Äôt fix the macro trends, but you can try to keep your corner of the tech world a place where people are glad to work.&lt;/p&gt;
    &lt;p&gt;"Off Kilter" by anujd89 is licensed under CC BY 2.0 .&lt;/p&gt;
    &lt;p&gt;Like this? Please feel free to share it on your favourite social media or link site! Share it with friends!&lt;/p&gt;
    &lt;p&gt;Hit subscribe to get new posts delivered to your inbox automatically.&lt;lb/&gt;Feedback? Get in touch!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45361394</guid><pubDate>Wed, 24 Sep 2025 15:03:59 +0000</pubDate></item><item><title>Python on the Edge: Fast, sandboxed, and powered by WebAssembly</title><link>https://wasmer.io/posts/python-on-the-edge-powered-by-webassembly</link><description>&lt;doc fingerprint="87ab2dfdccf438d4"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Python on the Edge: Fast, sandboxed, and powered by WebAssembly&lt;/head&gt;
    &lt;p&gt;We are excited to announce full Python support in Wasmer Edge (Beta)&lt;/p&gt;
    &lt;p&gt;Founder &amp;amp; CEO&lt;/p&gt;
    &lt;p&gt;With AI workloads on the rise, the demand for Python support on WebAssembly on the Edge has grown rapidly.&lt;/p&gt;
    &lt;p&gt;However, bringing Python to WebAssembly isn't trivial as it means supporting native modules like &lt;code&gt;numpy&lt;/code&gt;, &lt;code&gt;pandas&lt;/code&gt;, and &lt;code&gt;pydantic&lt;/code&gt;.&amp;#13;
While projects like &lt;code&gt;pyodide&lt;/code&gt; made strides in running Python in the browser via WebAssembly, their trade-offs don't fully fit server-side needs.&lt;/p&gt;
    &lt;p&gt;After months of hard work, today we're thrilled to announce full Python support in Wasmer Edge (Beta) powered by WebAssembly and WASIX.&lt;/p&gt;
    &lt;p&gt;Now you can run FastAPI, Streamlit, Django, LangChain, and more directly on Wasmer and Wasmer Edge! To accomplish it we had to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Add support for dynamic linking (&lt;code&gt;dlopen&lt;/code&gt;/&lt;code&gt;dlsym&lt;/code&gt;) into WASIX&lt;/item&gt;
      &lt;item&gt;Add &lt;code&gt;libffi&lt;/code&gt;support (so Python libraries using&lt;code&gt;ctypes&lt;/code&gt;could be supported)&lt;/item&gt;
      &lt;item&gt;Polish Sockets and threading support in WASIX&lt;/item&gt;
      &lt;item&gt;Release our own Python Package Index with many of the most popular Python Native libraries compiled to WASIX&lt;/item&gt;
      &lt;item&gt;Create our own alternative to Heroku Buildpacks / Nixpacks / Railpack / Devbox to automatically detect a project type from its source code and deploy it (including running with Wasmer or deploying to Wasmer Edge!). Updates will be shared soon!&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;How fast is it?&lt;/head&gt;
    &lt;p&gt;This Python release is much faster than any of the other Python releases we did in the past.&lt;/p&gt;
    &lt;p&gt;It is fast. Insa‚Ä¶natively fast (it's even faster than our py2wasm project!)&lt;/p&gt;
    &lt;code&gt;$ wasmer run python/python@=0.2.0 --dir=. -- pystone.py&amp;#13;
Pystone(1.1) time for 50000 passes = 0.562538&amp;#13;
This machine benchmarks at 88882.9 pystones/second&amp;#13;
$ wasmer run python/python --dir=. -- pystone.py # Note: first run may take time&amp;#13;
Pystone(1.1) time for 50000 passes = 0.093556&amp;#13;
This machine benchmarks at 534439 pystones/second&amp;#13;
$ python3 pystone.py&amp;#13;
Pystone(1.1) time for 50000 passes = 0.0827736&amp;#13;
This machine benchmarks at 604057 pystones/second
&lt;/code&gt;
    &lt;p&gt;That's 6x faster, and nearly indistinguishable from native Python performance‚Ä¶ quite good, considering that your Python apps can now run fully sandboxed anywhere!&lt;/p&gt;
    &lt;p&gt;Note: the first time you run Python, it will take a few minutes to compile. We are working to improve this so no time will be spent on compilation locally.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;üöÄ Even faster performance coming soon: we are trialing an optimization technique that will boost Python performance in Wasm to 95% of native Python speed. This is already powering our PHP server in production. Result: Near-native Python performance, fully sandboxed. Stay tuned!&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;What it can run&lt;/head&gt;
    &lt;p&gt;Now, you can run any kind of Python API server, powered by &lt;code&gt;fastapi&lt;/code&gt;, &lt;code&gt;django&lt;/code&gt;, &lt;code&gt;flask&lt;/code&gt;, or &lt;code&gt;starlette&lt;/code&gt;, connected to a MySQL database automatically when needed (FastAPI template, Django template).&lt;/p&gt;
    &lt;p&gt;You can run &lt;code&gt;fastapi&lt;/code&gt; with websockets (example repo, demo).&lt;/p&gt;
    &lt;p&gt;You can run &lt;code&gt;mcp&lt;/code&gt; servers (deploy using our MCP template, demo).&lt;/p&gt;
    &lt;p&gt;You can run image processors like &lt;code&gt;pillow&lt;/code&gt;  (example repo, demo).&lt;/p&gt;
    &lt;p&gt;You can run &lt;code&gt;ffmpeg&lt;/code&gt; inside Python (example repo, demo).&lt;/p&gt;
    &lt;p&gt;You can run &lt;code&gt;streamlit&lt;/code&gt; and &lt;code&gt;langchain&lt;/code&gt; (deploy using our LangChain template, demo).&lt;/p&gt;
    &lt;p&gt;You can even run &lt;code&gt;pypandoc&lt;/code&gt;!  (example repo, demo).&lt;/p&gt;
    &lt;p&gt;Soon, we'll have full support for:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;curl_cffi&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;polars&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;code&gt;gevent&lt;/code&gt;/&lt;code&gt;greenlet&lt;/code&gt;(more on this soon!)&lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;Pytorch&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;Wasmer VS alternatives&lt;/head&gt;
    &lt;p&gt;Python on Wasmer Edge is just launching, but it's already worth asking: how does it stack up existing solutions?&lt;/p&gt;
    &lt;head rend="h3"&gt;Quick Comparison&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Feature / Platform&lt;/cell&gt;
        &lt;cell role="head"&gt;Wasmer Edge&lt;/cell&gt;
        &lt;cell role="head"&gt;Cloudflare&lt;/cell&gt;
        &lt;cell role="head"&gt;AWS Lambda&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Native modules (&lt;code&gt;numpy&lt;/code&gt;, &lt;code&gt;pandas&lt;/code&gt;, etc.)&lt;/cell&gt;
        &lt;cell&gt;‚úÖ Supported*&lt;/cell&gt;
        &lt;cell&gt;‚ùå Limited (no &lt;code&gt;libcurl&lt;/code&gt;)&lt;/cell&gt;
        &lt;cell&gt;‚úÖ Full support&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Multithreading &amp;amp; multiprocessing (&lt;code&gt;ffmpeg&lt;/code&gt;, &lt;code&gt;pandoc&lt;/code&gt;)&lt;/cell&gt;
        &lt;cell&gt;‚úÖ Yes&lt;/cell&gt;
        &lt;cell&gt;‚ùå No&lt;/cell&gt;
        &lt;cell&gt;‚úÖ Yes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;ASGI / WSGI frameworks (&lt;code&gt;uvicorn&lt;/code&gt;, &lt;code&gt;daphne&lt;/code&gt;)&lt;/cell&gt;
        &lt;cell&gt;‚úÖ Supported&lt;/cell&gt;
        &lt;cell&gt;‚ùå Patched / limited&lt;/cell&gt;
        &lt;cell&gt;‚ö†Ô∏è Needs wrappers&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;WebSockets (&lt;code&gt;streamlit&lt;/code&gt;)&lt;/cell&gt;
        &lt;cell&gt;‚úÖ Yes&lt;/cell&gt;
        &lt;cell&gt;‚ùå No&lt;/cell&gt;
        &lt;cell&gt;‚ùå No&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Raw sockets (&lt;code&gt;libcurl&lt;/code&gt;)&lt;/cell&gt;
        &lt;cell&gt;‚úÖ Supported&lt;/cell&gt;
        &lt;cell&gt;‚ùå JS &lt;code&gt;fetch&lt;/code&gt; only&lt;/cell&gt;
        &lt;cell&gt;‚úÖ Supported&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Multiple Python versions&lt;/cell&gt;
        &lt;cell&gt;‚úÖ In Roadmap (3.12, 3.14‚Ä¶)&lt;/cell&gt;
        &lt;cell&gt;‚ùå Tied to bundled runtime&lt;/cell&gt;
        &lt;cell&gt;‚úÖ Supported&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Cold starts&lt;/cell&gt;
        &lt;cell&gt;‚ö° Extremely fast&lt;/cell&gt;
        &lt;cell&gt;‚è≥ Medium (V8 isolates)&lt;/cell&gt;
        &lt;cell&gt;‚è≥ Slow&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Code changes required&lt;/cell&gt;
        &lt;cell&gt;‚úÖ None&lt;/cell&gt;
        &lt;cell&gt;‚ö†Ô∏è Some&lt;/cell&gt;
        &lt;cell&gt;‚ö†Ô∏è Wrappers&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Pricing&lt;/cell&gt;
        &lt;cell&gt;üí∞ Affordable&lt;/cell&gt;
        &lt;cell&gt;üí∞ Higher&lt;/cell&gt;
        &lt;cell&gt;üí∞ Higher&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Cloudflare Workers (Python) / Pyodide&lt;/head&gt;
    &lt;quote&gt;&lt;p&gt;‚ÑπÔ∏è Most of the demos that we showcased on this article, are not runnable inside of Cloudflare:&lt;/p&gt;&lt;code&gt;ffmpeg&lt;/code&gt;,&lt;code&gt;streamlit&lt;/code&gt;,&lt;code&gt;pypandoc&lt;/code&gt;.&lt;/quote&gt;
    &lt;p&gt;Cloudflare launched Python support ~18 months ago, by using Pyodide inside workerd, their JavaScript-based Workers runtime.&lt;/p&gt;
    &lt;p&gt;While great for browser-like environments, Pyodide has trade-offs that make it less suitable server-side. Here are the limitations when running Python in Cloudflare:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;‚ùå No support for &lt;code&gt;uvloop&lt;/code&gt;,&lt;code&gt;uvicorn&lt;/code&gt;, or similar event-native frameworks (JS event loop patches break compatibility with native).&lt;/item&gt;
      &lt;item&gt;‚ùå No pthreads or multiprocessing support, you can't call subprocesses like &lt;code&gt;ffmpeg&lt;/code&gt;or&lt;code&gt;pypandoc&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;‚ùå No raw HTTP client sockets (HTTP clients are patched to use JS &lt;code&gt;fetch&lt;/code&gt;, no&lt;code&gt;libcurl&lt;/code&gt;available).&lt;/item&gt;
      &lt;item&gt;‚ùå Limited to a bundled Python version and package set.&lt;/item&gt;
      &lt;item&gt;‚è≥ Cold starts slower due to V8 isolate warmup.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Why the limitations? Cloudflare relies on Pyodide: great in-browser execution, but server-side it implies no sockets, threads, or multiprocessing. The result: convenient for lightweight browser use, but might not be the best fit for real Python workloads on the server.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;In contrast, Wasmer Edge runs real Python on WASIX unmodified, so everything "just works", with near-native speed and fast cold starts.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;Amazon Lambda&lt;/head&gt;
    &lt;p&gt;AWS Lambda doesn't natively run unmodified Python apps:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;‚ùå You need adapters (such as https://github.com/slank/awsgi or https://github.com/Kludex/mangum) for running your WSGI sites.&lt;/item&gt;
      &lt;item&gt;‚ùå WebSockets are unsupported.&lt;/item&gt;
      &lt;item&gt;‚ö†Ô∏è Setup is complex, adapters are often unmaintained.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Why the limits? AWS Lambda requires you to use their HTTP lambda handler, which can cause incompatibility into your own HTTP servers. Also, because their lambda handlers are HTTP-based, there's no easy support for WebSockets.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;In contrast, Wasmer Edge supports any Python HTTP servers without requiring any code adaptation from your side.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;Why Wasmer Edge Stands Out&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Closer to native Python than Pyodide (no JS involvement at all).&lt;/item&gt;
      &lt;item&gt;Faster cold starts and more compatibility than Cloudflare's Workers.&lt;/item&gt;
      &lt;item&gt;More compatible than AWS Lambda (no wrappers/adapters).&lt;/item&gt;
      &lt;item&gt;More affordable across the board.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;üêç It's Showtime!&lt;/head&gt;
    &lt;p&gt;Python support in Wasmer and Wasmer Edge is already available and ready to use. We have set up many Python templates to help you get started in no time.&lt;/p&gt;
    &lt;p&gt;https://wasmer.io/templates?language=python&lt;/p&gt;
    &lt;p&gt;To make things even better, we are working on a MCP server for Wasmer, so you will be able to plug Wasmer into ChatGPT or Anthropic and have your websites deploying from your vibe-coded projects. Stay tuned!&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;‚ö†Ô∏è Python in Wasmer Edge is still in Beta, so expect some rough edges if your project doesn't work out of the box‚Ä¶ if you encounter any issues, please report them so we can work on enabling your workloads on Wasmer Edge.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Create your first MCP Server in Wasmer&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Go to https://wasmer.io/templates/mcp-chatgpt-starter?intent=at_vRxJIdtPCbKe&lt;/item&gt;
      &lt;item&gt;Connect your Github account&lt;/item&gt;
      &lt;item&gt;Create a git repo from the template&lt;/item&gt;
      &lt;item&gt;Deploy and enjoy!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note: source code available here: https://github.com/wasmer-examples/python-mcp-chatgpt-starter&lt;/p&gt;
    &lt;head rend="h2"&gt;Create your first Django app&lt;/head&gt;
    &lt;p&gt;We have set up a template for using Django + Uvicorn in Wasmer Edge.&lt;/p&gt;
    &lt;p&gt;You can start using it very easily, just click Deploy: https://wasmer.io/templates/django-starter?intent=at_WK0DIkt3CeKX&lt;/p&gt;
    &lt;p&gt;Deploying a Django app will create a MySQL DB for you in Wasmer Edge (Postgres support is coming soon), run migrations and prepare everything to run your website seamlessly.&lt;/p&gt;
    &lt;p&gt;Note: source code available here: https://github.com/wasmer-examples/django-wasmer-starter&lt;/p&gt;
    &lt;p&gt;Ready to deploy your first Python app on Wasmer Edge?&lt;/p&gt;
    &lt;p&gt;Here are the best places to begin:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;üöÄ Starter Templates ‚Üí Browse Python templates&lt;/item&gt;
      &lt;item&gt;üìñ Docs &amp;amp; Examples ‚Üí Wasmer GitHub&lt;/item&gt;
      &lt;item&gt;üí¨ Community Support ‚Üí Join our Discord&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;üëâ Deploy your first Python app now&lt;/p&gt;
    &lt;p&gt;With WebAssembly and Wasmer, Python is now portable, sandboxed, and running at near-native speeds. Ready for AI workloads, APIs, and anything you can imagine at the edge.&lt;lb/&gt; The sky is the limit ‚ù§Ô∏è.&lt;/p&gt;
    &lt;head rend="h5"&gt;About the Author&lt;/head&gt;
    &lt;p&gt;Syrus Akbary is an enterpreneur and programmer. Specifically known for his contributions to the field of WebAssembly. He is the Founder and CEO of Wasmer, an innovative company that focuses on creating developer tools and infrastructure for running Wasm&lt;/p&gt;
    &lt;p&gt;Founder &amp;amp; CEO&lt;/p&gt;
    &lt;p&gt;How fast is it?&lt;/p&gt;
    &lt;p&gt;What it can run&lt;/p&gt;
    &lt;p&gt;Wasmer VS alternatives&lt;/p&gt;
    &lt;p&gt;Quick Comparison&lt;/p&gt;
    &lt;p&gt;Cloudflare Workers (Python) / Pyodide&lt;/p&gt;
    &lt;p&gt;Amazon Lambda&lt;/p&gt;
    &lt;p&gt;Why Wasmer Edge Stands Out&lt;/p&gt;
    &lt;p&gt;üêç It's Showtime!&lt;/p&gt;
    &lt;p&gt;Create your first MCP Server in Wasmer&lt;/p&gt;
    &lt;p&gt;Create your first Django app&lt;/p&gt;
    &lt;p&gt;Deploy your first Python site in seconds with our managed cloud solution.&lt;/p&gt;
    &lt;head rend="h5"&gt;Read more&lt;/head&gt;
    &lt;p&gt;wasmerwasmer edgerustprojectsedgeweb scraper&lt;/p&gt;
    &lt;head rend="h6"&gt;Build a Web Scraper in Rust and Deploy to Wasmer Edge&lt;/head&gt;
    &lt;p&gt;RudraAugust 14, 2023&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45362023</guid><pubDate>Wed, 24 Sep 2025 15:48:36 +0000</pubDate></item><item><title>SedonaDB: A new geospatial DataFrame library written in Rust</title><link>https://sedona.apache.org/latest/blog/2025/09/24/introducing-sedonadb-a-single-node-analytical-database-engine-with-geospatial-as-a-first-class-citizen/</link><description>&lt;doc fingerprint="3bdb989c4036eb8a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Introducing SedonaDB: A single-node analytical database engine with geospatial as a first-class citizen&lt;/head&gt;
    &lt;p&gt;The Apache Sedona community is excited to announce the initial release of SedonaDB! √∞&lt;/p&gt;
    &lt;p&gt;SedonaDB is the first open-source, single-node analytical database engine that treats spatial data as a first-class citizen. It is developed as a subproject of Apache Sedona.&lt;/p&gt;
    &lt;p&gt;Apache Sedona powers large-scale geospatial processing on distributed engines like Spark (SedonaSpark), Flink (SedonaFlink), and Snowflake (SedonaSnow). SedonaDB extends the Sedona ecosystem with a single-node engine optimized for small-to-medium data analytics, delivering the simplicity and speed that distributed systems often cannot.&lt;/p&gt;
    &lt;head rend="h2"&gt;√∞¬§ What is SedonaDB¬∂&lt;/head&gt;
    &lt;p&gt;Written in Rust, SedonaDB is lightweight, blazing fast, and spatial-native. Out of the box, it provides:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;√∞¬∫√Ø¬∏ Full support for spatial types, joins, CRS (coordinate reference systems), and functions on top of industry-standard query operations.&lt;/item&gt;
      &lt;item&gt;√¢¬° Query optimizations, indexing, and data pruning features under the hood that make spatial operations just work with high performance.&lt;/item&gt;
      &lt;item&gt;√∞ Pythonic and SQL interfaces familiar to developers, plus APIs for R and Rust.&lt;/item&gt;
      &lt;item&gt;√¢√Ø¬∏ Flexibility to run in single-machine environments on local files or data lakes.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;SedonaDB utilizes Apache Arrow and Apache DataFusion, providing everything you need from a modern, vectorized query engine. What sets it apart is the ability to process spatial workloads natively, without extensions or plugins. Installation is straightforward, and SedonaDB integrates easily into both local development and cloud pipelines, offering a consistent experience across environments.&lt;/p&gt;
    &lt;p&gt;The initial release of SedonaDB provides a comprehensive suite of geometric vector operations and seamlessly integrates with GeoArrow, GeoParquet, and GeoPandas. Future versions will support all popular spatial functions, including functions for raster data.&lt;/p&gt;
    &lt;head rend="h2"&gt;√∞ SedonaDB quickstart example¬∂&lt;/head&gt;
    &lt;p&gt;Start by installing SedonaDB:&lt;/p&gt;
    &lt;code&gt;pip install "apache-sedona[db]"
&lt;/code&gt;
    &lt;p&gt;Now instantiate the connection:&lt;/p&gt;
    &lt;code&gt;import sedona.db

sd = sedona.db.connect()
&lt;/code&gt;
    &lt;p&gt;Let's perform a spatial join using SedonaDB.&lt;/p&gt;
    &lt;p&gt;Suppose you have a &lt;code&gt;cities&lt;/code&gt; table with latitude and longitude points representing the center of each city, and a &lt;code&gt;countries&lt;/code&gt; table with a column containing a polygon of the country's geographic boundaries.&lt;/p&gt;
    &lt;p&gt;Here are a few rows from the &lt;code&gt;cities&lt;/code&gt; table:&lt;/p&gt;
    &lt;code&gt;√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¨√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢
√¢     name     √¢            geometry           √¢
√¢   utf8view   √¢      geometry &amp;lt;epsg:4326&amp;gt;     √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬™√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬°
√¢ Vatican City √¢ POINT(12.4533865 41.9032822)  √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬§
√¢ San Marino   √¢ POINT(12.4417702 43.9360958)  √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬§
√¢ Vaduz        √¢ POINT(9.5166695 47.1337238)   √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬§
&lt;/code&gt;
    &lt;p&gt;And here are a few rows from the countries table:&lt;/p&gt;
    &lt;code&gt;√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¨√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¨√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢
√¢             name            √¢   continent   √¢                      geometry                      √¢
√¢           utf8view          √¢    utf8view   √¢                geometry &amp;lt;epsg:4326&amp;gt;                √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬™√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬™√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬°
√¢ Fiji                        √¢ Oceania       √¢ MULTIPOLYGON(((180 -16.067132663642447,180 -16.55√¢¬¶ √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬§
√¢ United Republic of Tanzania √¢ Africa        √¢ POLYGON((33.90371119710453 -0.9500000000000001,34√¢¬¶ √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬§
√¢ Western Sahara              √¢ Africa        √¢ POLYGON((-8.665589565454809 27.656425889592356,-8√¢¬¶ √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬§
&lt;/code&gt;
    &lt;p&gt;Here√¢s how to perform a spatial join to compute the country of each city:&lt;/p&gt;
    &lt;code&gt;sd.sql(
    """
select
    cities.name as city_name,
    countries.name as country_name,
    continent
from cities
join countries
where ST_Intersects(cities.geometry, countries.geometry)
"""
).show(3)
&lt;/code&gt;
    &lt;p&gt;The code utilizes &lt;code&gt;ST_Intersects&lt;/code&gt; to determine if a city is contained within a given country.&lt;/p&gt;
    &lt;p&gt;Here's the result of the query:&lt;/p&gt;
    &lt;code&gt;√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¨√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¨√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢
√¢   city_name   √¢         country_name        √¢ continent √¢
√¢    utf8view   √¢           utf8view          √¢  utf8view √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬™√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬™√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬°
√¢ Suva          √¢ Fiji                        √¢ Oceania   √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬§
√¢ Dodoma        √¢ United Republic of Tanzania √¢ Africa    √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬§
√¢ Dar es Salaam √¢ United Republic of Tanzania √¢ Africa    √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¥√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¥√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢
&lt;/code&gt;
    &lt;p&gt;The example above performs a point-in-polygon join, mapping city locations (points) to the countries they fall within (polygons). SedonaDB executes these joins efficiently by leveraging spatial indices where beneficial and dynamically adapting join strategies at runtime using input data samples. While many general-purpose engines struggle with the performance of such operations, SedonaDB is purpose-built for spatial workloads and delivers consistently fast results.&lt;/p&gt;
    &lt;head rend="h2"&gt;√∞ Apache Sedona SpatialBench¬∂&lt;/head&gt;
    &lt;p&gt;To test our work on SedonaDB, we also needed to develop a mechanism to evaluate its performance and speed. This led us to develop Apache Sedona SpatialBench, a benchmark for assessing geospatial SQL analytics query performance across database systems.&lt;/p&gt;
    &lt;p&gt;Let's compare the performance of SedonaDB vs. GeoPandas and DuckDB Spatial for some representative spatial queries as defined in SpatialBench.&lt;/p&gt;
    &lt;p&gt;Here are the results from SpatialBench v0.1 for Queries 1√¢12 at scale factor 1 (SF1) and scale factor 10 (SF10).&lt;/p&gt;
    &lt;p&gt;SedonaDB demonstrates balanced performance across all query types and scales effectively to SF 10. DuckDB excels at spatial filters and some geometric operations but faces challenges with complex joins and KNN queries. GeoPandas, while popular in the Python ecosystem, requires manual optimization and parallelization to handle larger datasets effectively. An in-depth performance analysis can be found in the SpatialBench website.&lt;/p&gt;
    &lt;p&gt;Here√¢s an example of the SpatialBench Query #8 that works for SedonaDB and DuckDB:&lt;/p&gt;
    &lt;code&gt;SELECT b.b_buildingkey, b.b_name, COUNT(*) AS nearby_pickup_count
FROM trip t JOIN building b ON ST_DWithin(ST_GeomFromWKB(t.t_pickuploc), ST_GeomFromWKB(b.b_boundary), 0.0045) -- ~500m
GROUP BY b.b_buildingkey, b.b_name
ORDER BY nearby_pickup_count DESC
&lt;/code&gt;
    &lt;p&gt;This query intentionally performs a distance-based spatial join between points and polygons, followed by an aggregation of the results.&lt;/p&gt;
    &lt;p&gt;Here's what the query returns:&lt;/p&gt;
    &lt;code&gt;√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¨√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¨√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢
√¢ b_buildingkey √¢  b_name  √¢ nearby_pickup_count √¢
√¢     int64     √¢ utf8view √¢        int64        √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬™√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬™√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬°
√¢          3779 √¢ linen    √¢                  42 √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬§
√¢         19135 √¢ misty    √¢                  36 √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬§
√¢          4416 √¢ sienna   √¢                  26 √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¥√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¥√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢
&lt;/code&gt;
    &lt;p&gt;Here√¢s the equivalent GeoPandas code:&lt;/p&gt;
    &lt;code&gt;trips_df = pd.read_parquet(data_paths["trip"])
trips_df["pickup_geom"] = gpd.GeoSeries.from_wkb(
    trips_df["t_pickuploc"], crs="EPSG:4326"
)
pickups_gdf = gpd.GeoDataFrame(trips_df, geometry="pickup_geom", crs="EPSG:4326")

buildings_df = pd.read_parquet(data_paths["building"])
buildings_df["boundary_geom"] = gpd.GeoSeries.from_wkb(
    buildings_df["b_boundary"], crs="EPSG:4326"
)
buildings_gdf = gpd.GeoDataFrame(
    buildings_df, geometry="boundary_geom", crs="EPSG:4326"
)

threshold = 0.0045  # degrees (~500m)
result = (
    buildings_gdf.sjoin(pickups_gdf, predicate="dwithin", distance=threshold)
    .groupby(["b_buildingkey", "b_name"], as_index=False)
    .size()
    .rename(columns={"size": "nearby_pickup_count"})
    .sort_values(["nearby_pickup_count", "b_buildingkey"], ascending=[False, True])
    .reset_index(drop=True)
)
&lt;/code&gt;
    &lt;head rend="h2"&gt;√∞¬∫√Ø¬∏ SedonaDB CRS management¬∂&lt;/head&gt;
    &lt;p&gt;SedonaDB manages the CRS when reading/writing files, as well as in DataFrames, making your pipelines safer and saving you from manual work.&lt;/p&gt;
    &lt;p&gt;Let's compute the number of buildings in the state of Vermont to highlight the CRS management features embedded in SedonaDB.&lt;/p&gt;
    &lt;p&gt;Start by reading in a FlatGeobuf file that uses the EPSG 32618 CRS with GeoPandas and then convert it to a SedonaDB DataFrame:&lt;/p&gt;
    &lt;code&gt;import geopandas as gpd

path = "https://raw.githubusercontent.com/geoarrow/geoarrow-data/v0.2.0/example-crs/files/example-crs_vermont-utm.fgb"
gdf = gpd.read_file(path)
vermont = sd.create_data_frame(gdf)
&lt;/code&gt;
    &lt;p&gt;Let√¢s check the schema of the &lt;code&gt;vermont&lt;/code&gt; DataFrame:&lt;/p&gt;
    &lt;code&gt;vermont.schema

SedonaSchema with 1 field:
  geometry: wkb &amp;lt;epsg:32618&amp;gt;
&lt;/code&gt;
    &lt;p&gt;We can see that the &lt;code&gt;vermont&lt;/code&gt; DataFrame maintains the CRS that√¢s specified in the FlatGeobuf file.  SedonaDB doesn√¢t have a native FlatGeobuf reader yet, but it√¢s easy to use the GeoPandas FlatGeobuf reader and then convert it to a SedonaDB DataFrame with a single line of code.&lt;/p&gt;
    &lt;p&gt;Now read a GeoParquet file into a SedonaDB DataFrame.&lt;/p&gt;
    &lt;code&gt;buildings = sd.read_parquet(
    "https://github.com/geoarrow/geoarrow-data/releases/download/v0.2.0/microsoft-buildings_point_geo.parquet"
)
&lt;/code&gt;
    &lt;p&gt;Check the schema of the DataFrame:&lt;/p&gt;
    &lt;code&gt;buildings.schema

SedonaSchema with 1 field:
  geometry: geometry &amp;lt;ogc:crs84&amp;gt;
&lt;/code&gt;
    &lt;p&gt;Let√¢s expose these two tables as views and run a spatial join to see how many buildings are in Vermont:&lt;/p&gt;
    &lt;code&gt;buildings.to_view("buildings", overwrite=True)
vermont.to_view("vermont", overwrite=True)

sd.sql(
    """
select count(*) from buildings
join vermont
where ST_Intersects(buildings.geometry, vermont.geometry)
"""
).show()
&lt;/code&gt;
    &lt;p&gt;This command correctly errors out because the tables have different CRSs. For safety, SedonaDB errors out rather than give you the wrong answer! Here's the error message that's easy to debug:&lt;/p&gt;
    &lt;code&gt;SedonaError: type_coercion
caused by
Error during planning: Mismatched CRS arguments: ogc:crs84 vs epsg:32618
Use ST_Transform() or ST_SetSRID() to ensure arguments are compatible.
&lt;/code&gt;
    &lt;p&gt;Let√¢s rewrite the spatial join to convert the &lt;code&gt;vermont&lt;/code&gt; CRS to EPSG:4326, so it√¢s compatible with the &lt;code&gt;buildings&lt;/code&gt; CRS.&lt;/p&gt;
    &lt;code&gt;sd.sql(
    """
select count(*) from buildings
join vermont
where ST_Intersects(buildings.geometry, ST_Transform(vermont.geometry, 'EPSG:4326'))
"""
).show()
&lt;/code&gt;
    &lt;p&gt;We now get the correct result!&lt;/p&gt;
    &lt;code&gt;√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢
√¢ count(*) √¢
√¢   int64  √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬°
√¢   361856 √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢
&lt;/code&gt;
    &lt;p&gt;SedonaDB tracks the CRS when reading/writing files, converting to/from GeoPandas DataFrames, or when performing DataFrame operations, so your spatial computations run safely and correctly!&lt;/p&gt;
    &lt;head rend="h2"&gt;√∞¬Ø Realistic example with SedonaDB¬∂&lt;/head&gt;
    &lt;p&gt;Let's now turn our attention to a KNN join, which is a more complex spatial operation.&lt;/p&gt;
    &lt;p&gt;Suppose you're analyzing ride-sharing data and want to identify which buildings are most commonly near pickup points, helping understand the relationship between trip origins and nearby landmarks, businesses, or residential structures that might influence ride demand patterns.&lt;/p&gt;
    &lt;p&gt;This query finds the five closest buildings to each trip pickup location using spatial nearest neighbor analysis. For every trip, it identifies the five buildings that are geographically closest to where the passenger was picked up and calculates the exact distance to each of those buildings.&lt;/p&gt;
    &lt;p&gt;Here√¢s the query:&lt;/p&gt;
    &lt;code&gt;WITH trip_with_geom AS (
    SELECT t_tripkey, t_pickuploc, ST_GeomFromWKB(t_pickuploc) as pickup_geom
    FROM trip
),
building_with_geom AS (
    SELECT b_buildingkey, b_name, b_boundary, ST_GeomFromWKB(b_boundary) as boundary_geom
    FROM building
)
SELECT
    t.t_tripkey,
    t.t_pickuploc,
    b.b_buildingkey,
    b.b_name AS building_name,
    ST_Distance(t.pickup_geom, b.boundary_geom) AS distance_to_building
FROM trip_with_geom t JOIN building_with_geom b
ON ST_KNN(t.pickup_geom, b.boundary_geom, 5, FALSE)
ORDER BY distance_to_building ASC, b.b_buildingkey ASC
&lt;/code&gt;
    &lt;p&gt;Here are the results of the query:&lt;/p&gt;
    &lt;code&gt;√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¨√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¨√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¨√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¨√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢
√¢ t_tripkey √¢          t_pickuploc          √¢ b_buildingkey √¢ building_name √¢ distance_to_building √¢
√¢   int64   √¢             binary            √¢     int64     √¢      utf8     √¢        float64       √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬™√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬™√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬™√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬™√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬°
√¢   5854027 √¢ 01010000001afa27b85825504001√¢¬¶ √¢            79 √¢ gainsboro     √¢                  0.0 √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬§
√¢   3326828 √¢ 01010000001bfcc5b8b7a95d4083√¢¬¶ √¢           466 √¢ deep          √¢                  0.0 √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬§
√¢   1239844 √¢ 0101000000ce471770d6ce2a40f9√¢¬¶ √¢           618 √¢ ivory         √¢                  0.0 √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¥√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¥√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¥√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¥√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢
&lt;/code&gt;
    &lt;p&gt;This is one of the queries from SpatialBench.&lt;/p&gt;
    &lt;head rend="h2"&gt;√∞¬¶ Why SedonaDB was built in Rust¬∂&lt;/head&gt;
    &lt;p&gt;SedonaDB is built in Rust, a high-performance, memory-safe language that offers fine-grained memory management and a mature ecosystem of data libraries. It takes full advantage of this ecosystem by integrating with projects such as Apache DataFusion, GeoArrow, and georust/geo.&lt;/p&gt;
    &lt;p&gt;While Spark provides extension points that let SedonaSpark optimize spatial queries in distributed settings, DataFusion offers stable APIs for pruning, spatial operators, and optimizer rules on a single node. This enabled us to embed deep spatial awareness into the engine while preserving full non-spatial functionality. Thanks to the DataFusion project and community, the experience was both possible and enjoyable.&lt;/p&gt;
    &lt;head rend="h2"&gt;√¢√Ø¬∏ Why SedonaDB and SedonaSpark are Both Needed¬∂&lt;/head&gt;
    &lt;p&gt;SedonaSpark is well-suited for large-scale geospatial workloads or environments where Spark is already part of your production stack. For instance, joining a 100 GB vector dataset with a large raster dataset. For smaller datasets, however, Spark's distributed architecture can introduce unnecessary overhead, making it slower to run locally, harder to install, and more difficult to tune.&lt;/p&gt;
    &lt;p&gt;SedonaDB is better for smaller datasets and when running computations locally. The SedonaDB spatial functions are compatible with the SedonaSpark functions, so SQL chunks that work for one engine will usually work for the other. Over time, we will ensure that both project APIs are fully interoperable. Here's an example of a chunk to analyze the Overture buildings table that works for both engines.&lt;/p&gt;
    &lt;code&gt;nyc_bbox_wkt = (
    "POLYGON((-74.2591 40.4774, -74.2591 40.9176, -73.7004 40.9176, -73.7004 40.4774, -74.2591 40.4774))"
)

sd.sql(f"""
SELECT
    id,
    height,
    num_floors,
    roof_shape,
    ST_Centroid(geometry) as centroid
FROM
    buildings
WHERE
    is_underground = FALSE
    AND height IS NOT NULL
    AND height &amp;gt; 20
    AND ST_Intersects(geometry, ST_SetSRID(ST_GeomFromText('{nyc_bbox_wkt}'), 4326))
LIMIT 5;
&lt;/code&gt;
    &lt;head rend="h2"&gt;√∞ Next steps¬∂&lt;/head&gt;
    &lt;p&gt;While SedonaDB is well-tested and provides a core set of features that can perform numerous spatial analyses, it remains an early-stage project with multiple opportunities for new features.&lt;/p&gt;
    &lt;p&gt;Many more ST functions are required. Some are relatively straightforward, but others are complex.&lt;/p&gt;
    &lt;p&gt;The community will add built-in support for other spatial file formats, such as GeoPackage and GeoJSON, to SedonaDB. You can read data in these formats into GeoPandas DataFrames and convert them to SedonaDB DataFrames in the meantime.&lt;/p&gt;
    &lt;p&gt;Raster support is also on the roadmap, which is a complex undertaking, so it's an excellent opportunity to contribute if you're interested in solving challenging problems with Rust.&lt;/p&gt;
    &lt;p&gt;Refer to the SedonaDB v0.2 milestone for more details on the specific tasks outlined for the next release. Additionally, feel free to create issues, comment on the Discord, or start GitHub discussions to brainstorm new features.&lt;/p&gt;
    &lt;head rend="h2"&gt;√∞¬§ Join the community¬∂&lt;/head&gt;
    &lt;p&gt;The Apache Sedona community has an active Discord community, monthly user meetings, and regular contributor meetings.&lt;/p&gt;
    &lt;p&gt;SedonaDB welcomes contributions from the community. Feel free to request to take ownership of an issue, and we will be happy to assign it to you. You're also welcome to join the contributor meetings, and the other active contributors will be glad to help you get your pull request over the finish line!&lt;/p&gt;
    &lt;p&gt;Info&lt;/p&gt;
    &lt;p&gt;We√¢re celebrating the launch of SedonaDB &amp;amp; SpatialBench with a special Apache Sedona Community Office Hour!&lt;/p&gt;
    &lt;p&gt;√∞ October 7, 2025&lt;/p&gt;
    &lt;p&gt;√¢¬∞ 8√¢9 AM Pacific Time&lt;/p&gt;
    &lt;p&gt;√∞ Online&lt;/p&gt;
    &lt;p&gt;√∞ Sign up here&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45362206</guid><pubDate>Wed, 24 Sep 2025 16:00:45 +0000</pubDate></item><item><title>New bacteria, and two potential antibiotics, discovered in soil</title><link>https://www.rockefeller.edu/news/38239-hundreds-of-new-bacteria-and-two-potential-antibiotics-found-in-soil/</link><description>&lt;doc fingerprint="7c5460762d250ab9"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Hundreds of new bacteria, and two potential antibiotics, found in soil&lt;/head&gt;
    &lt;p&gt;Most bacteria cannot be cultured in the lab‚Äîand that‚Äôs been bad news for medicine. Many of our frontline antibiotics originated from microbes, yet as antibiotic resistance spreads and drug pipelines run dry, the soil beneath our feet has a vast hidden reservoir of untapped lifesaving compounds.&lt;/p&gt;
    &lt;p&gt;Now, researchers have developed a way to access this microbial goldmine. Their approach, published in Nature Biotechnology, circumvents the need to grow bacteria in the lab by extracting very large DNA fragments directly from soil to piece together the genomes of previously hidden microbes, and then mines resulting genomes for bioactive molecules.&lt;/p&gt;
    &lt;p&gt;From a single forest sample, the team generated hundreds of complete bacterial genomes never seen before, as well as two new antibiotic leads. The findings offer a scalable way to scour unculturable bacteria for new drug leads‚Äîand expose the vast, uncharted microbial frontier that shapes our environment.&lt;/p&gt;
    &lt;p&gt;‚ÄúWe finally have the technology to see the microbial world that have been previously inaccessible to humans,‚Äù says Sean F. Brady, head of the Laboratory of Genetically Encoded Small Molecules at Rockefeller. ‚ÄúAnd we‚Äôre not just seeing this information; we‚Äôre already turning it into potentially useful antibiotics. This is just the tip of the spear.‚Äù&lt;/p&gt;
    &lt;p&gt;Microbial dark matter&lt;/p&gt;
    &lt;p&gt;When hunting for bacteria, soil is an obvious choice. It‚Äôs the largest, most biodiverse reservoir of bacteria on the planet‚Äîa single teaspoon of it may contain thousands of different species. Many important therapeutics, including most of our antibiotic arsenal, were discovered in the tiny fraction of soil bacteria that can be grown in the laboratory. And soil is dirt cheap.&lt;/p&gt;
    &lt;p&gt;Yet we know very little about the millions of microbes packed into the earth. Scientists suspect that these hidden bacteria hold not only an untapped reservoir of new therapeutics, but clues as to how microbes shape climate, agriculture, and the larger environment that we live in. ‚ÄúAll over the world there‚Äôs this hidden ecosystem of microbes that could have dramatic effects on our lives,‚Äù Brady adds. ‚ÄúWe wanted to finally see them.‚Äù&lt;/p&gt;
    &lt;p&gt;Getting that glimpse involved weaving together several approaches. First, the team optimized a method for isolating large, high-quality DNA fragments directly from soil. Pairing this advance with emerging long-read nanopore sequencing allowed Jan Burian, a postdoctoral associate in the Brady lab, to produce continuous stretches of DNA that were tens of thousands of base pairs long‚Äî200 times longer than any previously existing technology could manage. Soil DNA contains a huge number of different bacteria; without such large DNA sequences to work with, resolving that complex genetic puzzle into complete and contiguous genomes for disparate bacteria proved exceedingly difficult.&lt;/p&gt;
    &lt;p&gt;‚ÄúIt‚Äôs easier to assemble a whole genome out of bigger pieces of DNA, rather than the millions of tiny snippets that were available before,‚Äù Brady says. ‚ÄúAnd that makes a dramatic difference in your confidence in your results.‚Äù&lt;/p&gt;
    &lt;p&gt;Unique small molecules, like antibiotics, that bacteria produce are called ‚Äúnatural products‚Äù. To convert the newly uncovered sequences into bioactive molecules, the team applied a synthetic bioinformatic natural products (synBNP) approach. They bioinformatically predicted the chemical structures of natural products directly from the genome data and then chemically synthesized them in the lab. With the synBNP approach, Brady and colleagues managed to turn the genetic blueprints from uncultured bacteria into actual molecules‚Äîincluding two potent antibiotics.&lt;/p&gt;
    &lt;p&gt;Brady describes the method, which is scalable and can be adapted to virtually any metagenomic space beyond soil, as a three-step strategy that could kick off a new era of microbiology: ‚ÄúIsolate big DNA, sequence it, and computationally convert it into something useful.‚Äù&lt;/p&gt;
    &lt;p&gt;Two new drug candidates, and counting&lt;/p&gt;
    &lt;p&gt;Applied to their single forest soil sample, the team‚Äôs approach produced 2.5 terabase-pairs of sequence data‚Äîthe deepest long-read exploration of a single soil sample to date. Their analysis uncovered hundreds of complete contiguous bacterial genomes, more than 99 percent of which were entirely new to science and identified members from 16 major branches of the bacterial family tree.&lt;/p&gt;
    &lt;p&gt;The two lead compounds discovered could translate into potent antibiotics. One, called erutacidin, disrupts bacterial membranes through an uncommon interaction with the lipid cardiolipin and is effective against even the most challenging drug-resistant bacteria. The other, trigintamicin, acts on a protein-unfolding motor known as ClpX, a rare antibacterial target.&lt;/p&gt;
    &lt;p&gt;Brady emphasizes that these discoveries are only the beginning. The study demonstrates that previously inaccessible microbial genomes can now be decoded and mined for bioactive molecules at scale without culturing the organisms. Unlocking the genetic potential of microbial dark matter may also provide new insights into the hidden microbial networks that sustain ecosystems.&lt;/p&gt;
    &lt;p&gt;‚ÄúWe‚Äôre mainly interested in small molecules as therapeutics, but there are applications beyond medicine,‚Äù Burian says. ‚ÄúStudying culturable bacteria led to advances that helped shape the modern world and finally seeing and accessing the uncultured majority will drive a new generation of discovery.‚Äù&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45362254</guid><pubDate>Wed, 24 Sep 2025 16:03:42 +0000</pubDate></item><item><title>Better Curl Saul: a lightweight API testing CLI focused on UX and simplicity</title><link>https://github.com/DeprecatedLuar/better-curl-saul</link><description>&lt;doc fingerprint="292d1d585d87d1a0"&gt;
  &lt;main&gt;
    &lt;p&gt;In a nutshell, this is disgusting:&lt;/p&gt;
    &lt;code&gt;curl -X POST https://api.github.com/repos/owner/repo/issues \
  -H "Authorization: Bearer ghp_token123" \
  -H "Content-Type: application/json" \
  -H "Accept: application/vnd.github.v3+json" \
  -d '{
    "title": "Bug Report",
    "body": "Something is broken",
    "labels": ["bug", "priority-high"],
    "assignees": ["developer1", "developer2"]
  }'&lt;/code&gt;
    &lt;code&gt;saul github set url https://api.github.com/repos/{?repo_owner}/{?repo_name}/{@endpoint}
saul set method POST
saul set header Authorization="Bearer {@token}" #Btw this {@} prompts you the first time, then you need to choose to update the variable with a flag on call
saul set body title="Bug Report" body="Something is broken" labels=[bug,priority-high] assignees=[developer1,developer2]
saul call&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Workspace-based - Each API gets its own organized folder&lt;/item&gt;
      &lt;item&gt;Smart variables - &lt;code&gt;{@token}&lt;/code&gt;persists,&lt;code&gt;{?name}&lt;/code&gt;prompts every time&lt;/item&gt;
      &lt;item&gt;Response filtering - Show only the fields you care about&lt;/item&gt;
      &lt;item&gt;Git-friendly - TOML files version control beautifully&lt;/item&gt;
      &lt;item&gt;Unix composable - Script it, pipe it, shell it&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Supports: Linux, macOS, Windows (I hope)&lt;/p&gt;
    &lt;code&gt;curl -sSL https://raw.githubusercontent.com/DeprecatedLuar/better-curl-saul/main/install.sh | bash&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Download binary for your OS from releases&lt;/item&gt;
      &lt;item&gt;Make executable: &lt;code&gt;chmod +x saul-*&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Move to PATH: &lt;code&gt;sudo mv saul-* /usr/local/bin/saul&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;I have no idea how to use a windows shell so I expect you to have bash üëç&lt;/p&gt;
    &lt;code&gt;git clone https://github.com/DeprecatedLuar/better-curl-saul.git
cd better-curl-saul
./other/install-local.sh  # Local development build&lt;/code&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;One-line install automatically downloads pre-built binaries or builds from source as fallback so you don't get sad :(&lt;/p&gt;
    &lt;code&gt;saul set url https://raw.githubusercontent.com/DeprecatedLuar/better-curl-saul/main/install.sh &amp;amp;&amp;amp; saul call --raw | bash # (Maybe works, who knows)&lt;/code&gt;
    &lt;code&gt;# Create a test workspace
saul demo set url https://jsonplaceholder.typicode.com/posts/1
saul demo set method GET
saul demo call

# Try with variables
saul api set url https://httpbin.org/post
saul api set method POST
saul api set body name={?your_name} message="Hello from Saul"
saul api call

# Oh... yeah, for nesting just use dot notation like obj.field=idk&lt;/code&gt;
    &lt;p&gt;Alright so you can:&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;set&lt;/code&gt;, &lt;code&gt;check&lt;/code&gt;(will become get), &lt;code&gt;edit&lt;/code&gt;, &lt;code&gt;rm&lt;/code&gt; your
&lt;code&gt;body&lt;/code&gt;, &lt;code&gt;header&lt;/code&gt;, &lt;code&gt;query&lt;/code&gt;, &lt;code&gt;request&lt;/code&gt;, &lt;code&gt;history&lt;/code&gt; or maybe even
&lt;code&gt;response&lt;/code&gt;, also &lt;code&gt;url&lt;/code&gt;, &lt;code&gt;method&lt;/code&gt;, &lt;code&gt;timeout&lt;/code&gt;, &lt;code&gt;history&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;# Configure your API workspace (or preset, same thing)
saul [workspace] set url https://api.example.com
saul set method POST
saul set header Authorization="Bearer {@token}"
saul set body user.name={?username} user.email=john@test.com

# Execute the request
saul call

# Check your configuration, note that preset/workspace name keeps
# stored in memory after first mention on syntax

saul [anoter_workspace] check url
saul check body

# View response history
saul check history&lt;/code&gt;
    &lt;p&gt;Important note about variables mechanics:&lt;/p&gt;
    &lt;p&gt;There are 2 variable types&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;soft variables {?} prompt you at EVERY call&lt;/item&gt;
      &lt;item&gt;hard variables {@} require manual update by running a flag (not yet added sorry) or running &lt;code&gt;saul set variable varname value&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Start watchin Better Call Saul&lt;/item&gt;
      &lt;item&gt;Think of a bad joke&lt;/item&gt;
      &lt;item&gt;Workspace-based configuration&lt;/item&gt;
      &lt;item&gt; Smart variable system (&lt;code&gt;{@}&lt;/code&gt;/&lt;code&gt;{?}&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;In line terminal field editing&lt;/item&gt;
      &lt;item&gt;Response filtering&lt;/item&gt;
      &lt;item&gt;Response history&lt;/item&gt;
      &lt;item&gt;Terminal session memory&lt;/item&gt;
      &lt;item&gt;Bulk operations&lt;/item&gt;
      &lt;item&gt;Fix history response parsing and filtering&lt;/item&gt;
      &lt;item&gt;GET specific response stuff from history (aka Headers/Body...)&lt;/item&gt;
      &lt;item&gt;Flags, we've got none basically&lt;/item&gt;
      &lt;item&gt;Stateless command support&lt;/item&gt;
      &lt;item&gt;Support pasting raw JSON template&lt;/item&gt;
      &lt;item&gt;User config system using the super cool github.com/DeprecatedLuar/toml-vars-letsgooo library&lt;/item&gt;
      &lt;item&gt;Add the eastereggs&lt;/item&gt;
      &lt;item&gt;Polish code&lt;/item&gt;
      &lt;item&gt;Actual Documentation&lt;/item&gt;
      &lt;item&gt;Touch Grass (not a priority)&lt;/item&gt;
      &lt;item&gt;Think of more features&lt;/item&gt;
      &lt;item&gt;Think of even more features&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Beta software - Core features work, documentation in progress.&lt;/p&gt;
    &lt;p&gt;Bug or feedback? I will be very, very, very happy if you let me know your thoughts.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45362486</guid><pubDate>Wed, 24 Sep 2025 16:17:34 +0000</pubDate></item></channel></rss>