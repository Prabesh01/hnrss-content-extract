<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Wed, 03 Sep 2025 06:44:19 +0000</lastBuildDate><item><title>Take something you don’t like and try to like it</title><link>https://dynomight.net/liking/</link><description>&lt;doc fingerprint="6661807dcbeaeefc"&gt;
  &lt;main&gt;
    &lt;p&gt;Here’s one possible hobby:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Take something you don’t like.&lt;/item&gt;
      &lt;item&gt;Try to like it.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It could be food or music or people or just the general situation you’re in. I recommend this hobby, partly because it’s nice to enjoy things, but mostly as an instrument for probing human nature.&lt;/p&gt;
    &lt;head rend="h2"&gt;1.&lt;/head&gt;
    &lt;p&gt;I was in Paris once. By coincidence, I wandered past a bunch of places that were playing Michael Jackson. I thought to myself, “Huh. The French sure do like Michael Jackson.” Gradually I decided, “You know what? They’re right! Michael Jackson is good.” Later, I saw a guy driving around blasting Billy Jean while hanging a hand outside his car with a sparkly white Michael Jackson glove. Again, I thought, “Huh.” That day was June 25, 2009.&lt;/p&gt;
    &lt;head rend="h2"&gt;2.&lt;/head&gt;
    &lt;p&gt;I don’t like cooked spinach. But if I eat some and try to forget that I hate it, it seems OK. Why?&lt;/p&gt;
    &lt;p&gt;Well, as a child, I was subjected to some misguided spinach-related parental interventions. (“You cannot leave this table until you’ve finished this extremely small portion”, etc.) I hated this, but looking back, it wasn’t the innate qualities of spinach the bothered me, so much as that being forced to put something inside my body felt like a violation of my autonomy.&lt;/p&gt;
    &lt;p&gt;When I encountered spinach as an adult, instead of tasting a vegetable, I tasted a grueling battle of will. Spinach was dangerous—if I liked it, that would teach my parents that they were right to control my diet.&lt;/p&gt;
    &lt;p&gt;So I tried telling myself little stories: I’m hiking in the mountains in Japan when suddenly the temperature drops, and it starts pouring rain. Freezing and desperate, I spot a monastery and knock on the door. The monks warm me up and offer me hōrensō no ohitashi, made from some exotic vegetable I’ve never seen before. Presumably, I’d think it was amazing.&lt;/p&gt;
    &lt;p&gt;I can’t fully access that mind-space. But just knowing it exists seems to make a big difference. Using similar techniques, I’ve successfully made myself like (or less dislike) white wine, disco, yoga, non-spicy food, Ezra Klein, Pearl Jam, and Studio Ghibli movies.&lt;/p&gt;
    &lt;p&gt;Lesson: Sometimes we dislike things simply because we have a concept of ourselves as not liking them.&lt;/p&gt;
    &lt;head rend="h2"&gt;3.&lt;/head&gt;
    &lt;p&gt;Meanwhile, I’ve failed to make myself like country music. I mean, I like A Boy Named Sue. Who doesn’t? But what about Stand By Your Man or Dust on the Bottle? I listen to these, and I appreciate what they’re doing. I admire that they aren’t entirely oriented around the concerns of teenagers. But I can’t seem to actually enjoy them.&lt;/p&gt;
    &lt;p&gt;Of course, it seems unlikely that this is unrelated to the fact that no one in my peer group thinks country music is cool. On the other hand, I’m constantly annoyed that my opinions aren’t more unique or interesting. And I subscribe to the idea that what’s really cool is to be a cultural omnivore who appreciates everything.&lt;/p&gt;
    &lt;p&gt;It doesn’t matter. I still can’t like country music. I think the problem is that I don’t actually want to like country music. I only want to want to like country music. The cultural programming is in too deep.&lt;/p&gt;
    &lt;p&gt;Lesson: Certain levels of the subconscious are easier to screw around with than others.&lt;/p&gt;
    &lt;head rend="h2"&gt;4.&lt;/head&gt;
    &lt;p&gt;For years, a friend and I would go on week-long hikes. Before we started, we’d go make our own trail mix, and I’d always insist on adding raisins. Each year, my friend would object more loudly that I don’t actually like raisins. But I do like raisins. So I’d scoff. But after several cycles, I had to admit that while I “liked raisins”, there never came a time that I actually wanted to eat raisins, ever.&lt;/p&gt;
    &lt;p&gt;Related: Once every year or two, I’ll have a rough day, and I’ll say to myself, “OK, screw it. Liking Oasis is the lamest thing that has ever been done by anyone. But the dirty truth is that I love Oasis. So I will listen to Oasis and thereby be comforted.” Then I listen to Oasis, and it just isn’t that good.&lt;/p&gt;
    &lt;p&gt;Lesson: You can have an incorrect concept of self.&lt;/p&gt;
    &lt;head rend="h2"&gt;5.&lt;/head&gt;
    &lt;p&gt;I don’t like this about myself, but I’m a huge snob regarding television. I believe TV can be true art, as high as any other form. (How does My Brilliant Friend only have an 89 on Metacritic?) But even after pretentiously filtering for critical acclaim, I usually feel that most shows are slop and can’t watch them.&lt;/p&gt;
    &lt;p&gt;At first glance, this seems just like country music—I don’t like it because of status-driven memetic desire or whatever. But there’s a difference. Not liking country music is fine (neurotic self-flagellation aside) because there’s an infinite amount of other music. But not liking most TV is really annoying, because often I want to watch TV, but can’t find anything acceptable.&lt;/p&gt;
    &lt;p&gt;I see three possible explanations:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Almost all TV is, in fact, bad.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Lots of TV is fine, but just doesn’t appeal to me.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Lots of TV is fine, but it’s hard to tell yourself stories where you’re hiking in the mountains and a bunch of Japanese monks show you, like, Big Bang Theory.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Whatever it is, it seems hard to change.&lt;/p&gt;
    &lt;p&gt;Lesson: Some things are hard to change.&lt;/p&gt;
    &lt;head rend="h2"&gt;6.&lt;/head&gt;
    &lt;p&gt;On planes, the captain will often invite you to, “sit back and enjoy the ride”. This is confusing. Enjoy the ride? Enjoy being trapped in a pressurized tube and jostled by all the passengers lining up to relieve themselves because your company decided to cram in a few more seats instead of having an adequate number of toilets? Aren’t flights supposed to be endured?&lt;/p&gt;
    &lt;p&gt;At the same time, those invitations seem like a glimpse of a parallel universe. Are there members of my species who sit back and enjoy flights?&lt;/p&gt;
    &lt;p&gt;I have no hard data. But it’s a good heuristic that there are people “who actually X” for approximately all values of X. If one in nine people enjoy going to the dentist, surely at least that many enjoy being on planes.&lt;/p&gt;
    &lt;p&gt;What I think the captain is trying to say is, “While you can’t always control your situation, you have tremendous power over how you experience that situation. You may find a cramped flight to be a torture. But the torture happens inside your head. Some people like your situation. You too, perhaps could like it.”&lt;/p&gt;
    &lt;p&gt;That’s an important message. Though one imagines that giving it as an in-flight announcement would cause more confusion, not less. So the captain does what they can.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45102512</guid></item><item><title>The Little Book of Linear Algebra</title><link>https://github.com/the-litte-book-of/linear-algebra</link><description>&lt;doc fingerprint="b70d9b7e132f9799"&gt;
  &lt;main&gt;
    &lt;p&gt;A concise, beginner-friendly introduction to the core ideas of linear algebra.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Download PDF – print-ready version&lt;/item&gt;
      &lt;item&gt;Download EPUB – e-reader friendly&lt;/item&gt;
      &lt;item&gt;View LaTeX – Latex source&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A scalar is a single numerical quantity, most often taken from the real numbers, denoted by &lt;/p&gt;
    &lt;p&gt;An element of &lt;/p&gt;
    &lt;p&gt;Example 1.1.1.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A 2-dimensional vector: &lt;math-renderer&gt;$(3, -1) \in \mathbb{R}^2$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;A 3-dimensional vector: &lt;math-renderer&gt;$(2, 0, 5) \in \mathbb{R}^3$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;A 1-dimensional vector: &lt;math-renderer&gt;$(7) \in \mathbb{R}^1$&lt;/math-renderer&gt;, which corresponds to the scalar&lt;math-renderer&gt;$7$&lt;/math-renderer&gt;itself.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Vectors are often written vertically in column form, which emphasizes their role in matrix multiplication:&lt;/p&gt;
    &lt;p&gt;The vertical layout makes the structure clearer when we consider linear combinations or multiply matrices by vectors.&lt;/p&gt;
    &lt;p&gt;In &lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;As a point in space, described by its coordinates.&lt;/item&gt;
      &lt;item&gt;As a displacement or arrow, described by a direction and a length.&lt;/item&gt;
      &lt;item&gt;As an abstract element of a vector space, whose properties follow algebraic rules independent of geometry.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Vectors are written in boldface lowercase letters: &lt;math-renderer&gt;$\mathbf{v}, \mathbf{w}, \mathbf{x}$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;The i-th entry of a vector &lt;math-renderer&gt;$\mathbf{v}$&lt;/math-renderer&gt;is written&lt;math-renderer&gt;$v_i$&lt;/math-renderer&gt;, where indices begin at 1.&lt;/item&gt;
      &lt;item&gt;The set of all n-dimensional vectors over &lt;math-renderer&gt;$\mathbb{R}$&lt;/math-renderer&gt;is denoted&lt;math-renderer&gt;$\mathbb{R}^n$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Column vectors will be the default form unless otherwise stated.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Scalars and vectors form the atoms of linear algebra. Every structure we will build-vector spaces, linear transformations, matrices, eigenvalues-relies on the basic notions of number and ordered collection of numbers. Once vectors are understood, we can define operations such as addition and scalar multiplication, then generalize to subspaces, bases, and coordinate systems. Eventually, this framework grows into the full theory of linear algebra, with powerful applications to geometry, computation, and data.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Write three different vectors in &lt;math-renderer&gt;$\mathbb{R}^2$&lt;/math-renderer&gt;and sketch them as arrows from the origin. Identify their coordinates explicitly.&lt;/item&gt;
      &lt;item&gt;Give an example of a vector in &lt;math-renderer&gt;$\mathbb{R}^4$&lt;/math-renderer&gt;. Can you visualize it directly? Explain why high-dimensional visualization is challenging.&lt;/item&gt;
      &lt;item&gt;Let &lt;math-renderer&gt;$\mathbf{v} = (4, -3, 2)$&lt;/math-renderer&gt;. Write&lt;math-renderer&gt;$\mathbf{v}$&lt;/math-renderer&gt;in column form and state&lt;math-renderer&gt;$v_1, v_2, v_3$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;In what sense is the set &lt;math-renderer&gt;$\mathbb{R}^1$&lt;/math-renderer&gt;both a line and a vector space? Illustrate with examples.&lt;/item&gt;
      &lt;item&gt;Consider the vector &lt;math-renderer&gt;$\mathbf{u} = (1,1,\dots,1) \in \mathbb{R}^n$&lt;/math-renderer&gt;. What is special about this vector when&lt;math-renderer&gt;$n$&lt;/math-renderer&gt;is large? What might it represent in applications?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Vectors in linear algebra are not static objects; their power comes from the operations we can perform on them. Two fundamental operations define the structure of vector spaces: addition and scalar multiplication. These operations satisfy simple but far-reaching rules that underpin the entire subject.&lt;/p&gt;
    &lt;p&gt;Given two vectors of the same dimension, their sum is obtained by adding corresponding entries. Formally, if&lt;/p&gt;
    &lt;p&gt;then their sum is&lt;/p&gt;
    &lt;p&gt;Example 1.2.1. Let &lt;/p&gt;
    &lt;p&gt;Geometrically, vector addition corresponds to the parallelogram rule. If we draw both vectors as arrows from the origin, then placing the tail of one vector at the head of the other produces the sum. The diagonal of the parallelogram they form represents the resulting vector.&lt;/p&gt;
    &lt;p&gt;Multiplying a vector by a scalar stretches or shrinks the vector while preserving its direction, unless the scalar is negative, in which case the vector is also reversed. If &lt;/p&gt;
    &lt;p&gt;then&lt;/p&gt;
    &lt;p&gt;Example 1.2.2. Let &lt;/p&gt;
    &lt;p&gt;This corresponds to flipping the vector through the origin and doubling its length.&lt;/p&gt;
    &lt;p&gt;The interaction of addition and scalar multiplication allows us to form linear combinations. A linear combination of vectors &lt;/p&gt;
    &lt;p&gt;Linear combinations are the mechanism by which we generate new vectors from existing ones. The span of a set of vectors-the collection of all their linear combinations-will later lead us to the idea of a subspace.&lt;/p&gt;
    &lt;p&gt;Example 1.2.3. Let &lt;/p&gt;
    &lt;p&gt;Thus &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Addition: &lt;math-renderer&gt;$\mathbf{u} + \mathbf{v}$&lt;/math-renderer&gt;means component-wise addition.&lt;/item&gt;
      &lt;item&gt;Scalar multiplication: &lt;math-renderer&gt;$c\mathbf{v}$&lt;/math-renderer&gt;scales each entry of&lt;math-renderer&gt;$\mathbf{v}$&lt;/math-renderer&gt;by&lt;math-renderer&gt;$c$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Linear combination: a sum of the form &lt;math-renderer&gt;$c_1 \mathbf{v}_1 + \cdots + c_k \mathbf{v}_k$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Vector addition and scalar multiplication are the defining operations of linear algebra. They give structure to vector spaces, allow us to describe geometric phenomena like translation and scaling, and provide the foundation for solving systems of equations. Everything that follows-basis, dimension, transformations-builds on these simple but profound rules.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Compute &lt;math-renderer&gt;$\mathbf{u} + \mathbf{v}$&lt;/math-renderer&gt;where&lt;math-renderer&gt;$\mathbf{u} = (1,2,3)$&lt;/math-renderer&gt;and&lt;math-renderer&gt;$\mathbf{v} = (4, -1, 0)$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Find &lt;math-renderer&gt;$3\mathbf{v}$&lt;/math-renderer&gt;where&lt;math-renderer&gt;$\mathbf{v} = (-2,5)$&lt;/math-renderer&gt;. Sketch both vectors to illustrate the scaling.&lt;/item&gt;
      &lt;item&gt;Show that &lt;math-renderer&gt;$(5,7)$&lt;/math-renderer&gt;can be written as a linear combination of&lt;math-renderer&gt;$(1,0)$&lt;/math-renderer&gt;and&lt;math-renderer&gt;$(0,1)$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Write &lt;math-renderer&gt;$(4,4)$&lt;/math-renderer&gt;as a linear combination of&lt;math-renderer&gt;$(1,1)$&lt;/math-renderer&gt;and&lt;math-renderer&gt;$(1,-1)$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Prove that if &lt;math-renderer&gt;$\mathbf{u}, \mathbf{v} \in \mathbb{R}^n$&lt;/math-renderer&gt;, then&lt;math-renderer&gt;$(c+d)(\mathbf{u}+\mathbf{v}) = c\mathbf{u} + c\mathbf{v} + d\mathbf{u} + d\mathbf{v}$&lt;/math-renderer&gt;for scalars&lt;math-renderer&gt;$c,d \in \mathbb{R}$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The dot product is the fundamental operation that links algebra and geometry in vector spaces. It allows us to measure lengths, compute angles, and determine orthogonality. From this single definition flow the notions of norm and angle, which give geometry to abstract vector spaces.&lt;/p&gt;
    &lt;p&gt;For two vectors in &lt;/p&gt;
    &lt;p&gt;Equivalently, in matrix notation:&lt;/p&gt;
    &lt;p&gt;Example 1.3.1. Let &lt;/p&gt;
    &lt;p&gt;The dot product outputs a single scalar, not another vector.&lt;/p&gt;
    &lt;p&gt;The Euclidean norm of a vector is the square root of its dot product with itself:&lt;/p&gt;
    &lt;p&gt;This generalizes the Pythagorean theorem to arbitrary dimensions.&lt;/p&gt;
    &lt;p&gt;Example 1.3.2. For &lt;/p&gt;
    &lt;p&gt;This is exactly the length of the vector as an arrow in the plane.&lt;/p&gt;
    &lt;p&gt;The dot product also encodes the angle between two vectors. For nonzero vectors &lt;/p&gt;
    &lt;p&gt;where &lt;/p&gt;
    &lt;p&gt;Example 1.3.3. Let &lt;/p&gt;
    &lt;p&gt;Hence&lt;/p&gt;
    &lt;p&gt;The vectors are perpendicular.&lt;/p&gt;
    &lt;p&gt;Two vectors are said to be orthogonal if their dot product is zero:&lt;/p&gt;
    &lt;p&gt;Orthogonality generalizes the idea of perpendicularity from geometry to higher dimensions.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Dot product: &lt;math-renderer&gt;$\mathbf{u} \cdot \mathbf{v}$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Norm (length): &lt;math-renderer&gt;$|\mathbf{v}|$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Orthogonality: &lt;math-renderer&gt;$\mathbf{u} \perp \mathbf{v}$&lt;/math-renderer&gt;if&lt;math-renderer&gt;$\mathbf{u} \cdot \mathbf{v} = 0$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The dot product turns vector spaces into geometric objects: vectors gain lengths, angles, and notions of perpendicularity. This foundation will later support the study of orthogonal projections, Gram–Schmidt orthogonalization, eigenvectors, and least squares problems.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Compute &lt;math-renderer&gt;$\mathbf{u} \cdot \mathbf{v}$&lt;/math-renderer&gt;for&lt;math-renderer&gt;$\mathbf{u} = (1,2,3)$&lt;/math-renderer&gt;,&lt;math-renderer&gt;$\mathbf{v} = (4,5,6)$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Find the norm of &lt;math-renderer&gt;$\mathbf{v} = (2, -2, 1)$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Determine whether &lt;math-renderer&gt;$\mathbf{u} = (1,1,0)$&lt;/math-renderer&gt;and&lt;math-renderer&gt;$\mathbf{v} = (1,-1,2)$&lt;/math-renderer&gt;are orthogonal.&lt;/item&gt;
      &lt;item&gt;Let &lt;math-renderer&gt;$\mathbf{u} = (3,4)$&lt;/math-renderer&gt;,&lt;math-renderer&gt;$\mathbf{v} = (4,3)$&lt;/math-renderer&gt;. Compute the angle between them.&lt;/item&gt;
      &lt;item&gt;Prove that &lt;math-renderer&gt;$|\mathbf{u} + \mathbf{v}|^2 = |\mathbf{u}|^2 + |\mathbf{v}|^2 + 2\mathbf{u}\cdot \mathbf{v}$&lt;/math-renderer&gt;. This identity is the algebraic version of the Law of Cosines.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Orthogonality captures the notion of perpendicularity in vector spaces. It is one of the most important geometric ideas in linear algebra, allowing us to decompose vectors, define projections, and construct special bases with elegant properties.&lt;/p&gt;
    &lt;p&gt;Two vectors &lt;/p&gt;
    &lt;p&gt;This condition ensures that the angle between them is &lt;/p&gt;
    &lt;p&gt;Example 1.4.1. In &lt;/p&gt;
    &lt;p&gt;A collection of vectors is called orthogonal if every distinct pair of vectors in the set is orthogonal. If, in addition, each vector has norm 1, the set is called orthonormal.&lt;/p&gt;
    &lt;p&gt;Example 1.4.2. In &lt;/p&gt;
    &lt;p&gt;form an orthonormal set: each has length 1, and their dot products vanish when the indices differ.&lt;/p&gt;
    &lt;p&gt;Orthogonality makes possible the decomposition of a vector into two components: one parallel to another vector, and one orthogonal to it. Given a nonzero vector &lt;/p&gt;
    &lt;p&gt;The difference&lt;/p&gt;
    &lt;p&gt;is orthogonal to &lt;/p&gt;
    &lt;p&gt;Example 1.4.3. Let &lt;/p&gt;
    &lt;p&gt;Thus&lt;/p&gt;
    &lt;p&gt;where &lt;/p&gt;
    &lt;p&gt;In general, if &lt;/p&gt;
    &lt;p&gt;$$ \mathbf{v} = \text{proj}{\mathbf{u}}(\mathbf{v}) + \big(\mathbf{v} - \text{proj}{\mathbf{u}}(\mathbf{v})\big), $$&lt;/p&gt;
    &lt;p&gt;where the first term is parallel to &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;math-renderer&gt;$\mathbf{u} \perp \mathbf{v}$&lt;/math-renderer&gt;: vectors&lt;math-renderer&gt;$\mathbf{u}$&lt;/math-renderer&gt;and&lt;math-renderer&gt;$\mathbf{v}$&lt;/math-renderer&gt;are orthogonal.&lt;/item&gt;
      &lt;item&gt;An orthogonal set: vectors pairwise orthogonal.&lt;/item&gt;
      &lt;item&gt;An orthonormal set: pairwise orthogonal, each of norm 1.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Orthogonality gives structure to vector spaces. It provides a way to separate independent directions cleanly, simplify computations, and minimize errors in approximations. Many powerful algorithms in numerical linear algebra and data science (QR decomposition, least squares regression, PCA) rely on orthogonality.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Verify that the vectors &lt;math-renderer&gt;$(1,2,2)$&lt;/math-renderer&gt;and&lt;math-renderer&gt;$(2,0,-1)$&lt;/math-renderer&gt;are orthogonal.&lt;/item&gt;
      &lt;item&gt;Find the projection of &lt;math-renderer&gt;$(3,4)$&lt;/math-renderer&gt;onto&lt;math-renderer&gt;$(1,1)$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Show that any two distinct standard basis vectors in &lt;math-renderer&gt;$\mathbb{R}^n$&lt;/math-renderer&gt;are orthogonal.&lt;/item&gt;
      &lt;item&gt;Decompose &lt;math-renderer&gt;$(5,2)$&lt;/math-renderer&gt;into components parallel and orthogonal to&lt;math-renderer&gt;$(2,1)$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Prove that if &lt;math-renderer&gt;$\mathbf{u}, \mathbf{v}$&lt;/math-renderer&gt;are orthogonal and nonzero, then&lt;math-renderer&gt;$(\mathbf{u}+\mathbf{v})\cdot(\mathbf{u}-\mathbf{v}) = 0$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Matrices are the central objects of linear algebra, providing a compact way to represent and manipulate linear transformations, systems of equations, and structured data. A matrix is a rectangular array of numbers arranged in rows and columns.&lt;/p&gt;
    &lt;p&gt;An &lt;/p&gt;
    &lt;p&gt;Each entry &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If &lt;math-renderer&gt;$m = n$&lt;/math-renderer&gt;, the matrix is square.&lt;/item&gt;
      &lt;item&gt;If &lt;math-renderer&gt;$m = 1$&lt;/math-renderer&gt;, the matrix is a row vector.&lt;/item&gt;
      &lt;item&gt;If &lt;math-renderer&gt;$n = 1$&lt;/math-renderer&gt;, the matrix is a column vector.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Thus, vectors are simply special cases of matrices.&lt;/p&gt;
    &lt;p&gt;Example 2.1.1. A &lt;/p&gt;
    &lt;p&gt;Here, &lt;/p&gt;
    &lt;p&gt;Example 2.1.2. A &lt;/p&gt;
    &lt;p&gt;This will later serve as the representation of a linear transformation on &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Matrices are denoted by uppercase bold letters: &lt;math-renderer&gt;$A, B, C$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Entries are written as &lt;math-renderer&gt;$a_{ij}$&lt;/math-renderer&gt;, with the row index first, column index second.&lt;/item&gt;
      &lt;item&gt;The set of all real &lt;math-renderer&gt;$m \times n$&lt;/math-renderer&gt;matrices is denoted&lt;math-renderer&gt;$\mathbb{R}^{m \times n}$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Thus, a matrix is a function &lt;/p&gt;
    &lt;p&gt;Matrices generalize vectors and give us a language for describing linear operations systematically. They encode systems of equations, rotations, projections, and transformations of data. With matrices, algebra and geometry come together: a single compact object can represent both numerical data and functional rules.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Write a &lt;math-renderer&gt;$3 \times 2$&lt;/math-renderer&gt;matrix of your choice and identify its entries&lt;math-renderer&gt;$a_{ij}$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Is every vector a matrix? Is every matrix a vector? Explain.&lt;/item&gt;
      &lt;item&gt;Which of the following are square matrices: &lt;math-renderer&gt;$A \in \mathbb{R}^{4\times4}$&lt;/math-renderer&gt;,&lt;math-renderer&gt;$B \in \mathbb{R}^{3\times5}$&lt;/math-renderer&gt;,&lt;math-renderer&gt;$C \in \mathbb{R}^{1\times1}$&lt;/math-renderer&gt;?&lt;/item&gt;
      &lt;item&gt;Let $D = \begin{bmatrix} 1 &amp;amp; 0 \ 0 &amp;amp; 1 \end{bmatrix}$. What kind of matrix is this?&lt;/item&gt;
      &lt;item&gt;Consider the matrix $E = \begin{bmatrix} a &amp;amp; b \ c &amp;amp; d \end{bmatrix}$. Express &lt;math-renderer&gt;$e_{11}, e_{12}, e_{21}, e_{22}$&lt;/math-renderer&gt;explicitly.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Once matrices are defined, the next step is to understand how they combine. Just as vectors gain meaning through addition and scalar multiplication, matrices become powerful through two operations: addition and multiplication.&lt;/p&gt;
    &lt;p&gt;Two matrices of the same size are added by adding corresponding entries. If&lt;/p&gt;
    &lt;p&gt;then&lt;/p&gt;
    &lt;p&gt;Example 2.2.1. Let&lt;/p&gt;
    &lt;p&gt;Then&lt;/p&gt;
    &lt;p&gt;\begin{bmatrix} 0 &amp;amp; 2 \ 8 &amp;amp; 6 \end{bmatrix}. $$&lt;/p&gt;
    &lt;p&gt;Matrix addition is commutative (&lt;/p&gt;
    &lt;p&gt;For a scalar &lt;/p&gt;
    &lt;p&gt;This stretches or shrinks all entries of the matrix uniformly.&lt;/p&gt;
    &lt;p&gt;Example 2.2.2. If&lt;/p&gt;
    &lt;p&gt;then&lt;/p&gt;
    &lt;p&gt;The defining operation of matrices is multiplication. If&lt;/p&gt;
    &lt;p&gt;then their product is the &lt;/p&gt;
    &lt;p&gt;Thus, the entry in the &lt;/p&gt;
    &lt;p&gt;Example 2.2.3. Let&lt;/p&gt;
    &lt;p&gt;Then&lt;/p&gt;
    &lt;head rend="h1"&gt;$$ AB = \begin{bmatrix} 1\cdot4 + 2\cdot2 &amp;amp; 1\cdot(-1) + 2\cdot5 \ 0\cdot4 + 3\cdot2 &amp;amp; 0\cdot(-1) + 3\cdot5 \end{bmatrix}&lt;/head&gt;
    &lt;p&gt;\begin{bmatrix} 8 &amp;amp; 9 \ 6 &amp;amp; 15 \end{bmatrix}. $$&lt;/p&gt;
    &lt;p&gt;Notice that matrix multiplication is not commutative in general: &lt;/p&gt;
    &lt;p&gt;Matrix multiplication corresponds to the composition of linear transformations. If &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Matrix sum: &lt;math-renderer&gt;$A+B$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Scalar multiple: &lt;math-renderer&gt;$cA$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Product: &lt;math-renderer&gt;$AB$&lt;/math-renderer&gt;, defined only when the number of columns of&lt;math-renderer&gt;$A$&lt;/math-renderer&gt;equals the number of rows of&lt;math-renderer&gt;$B$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Matrix multiplication is the core mechanism of linear algebra: it encodes how transformations combine, how systems of equations are solved, and how data flows in modern algorithms. Addition and scalar multiplication make matrices into a vector space, while multiplication gives them an algebraic structure rich enough to model geometry, computation, and networks.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Compute &lt;math-renderer&gt;$A+B$&lt;/math-renderer&gt;for&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Find &lt;math-renderer&gt;$3A$&lt;/math-renderer&gt;where&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Multiply&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Verify with an explicit example that &lt;math-renderer&gt;$AB \neq BA$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Prove that matrix multiplication is distributive: &lt;math-renderer&gt;$A(B+C) = AB + AC$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Two special operations on matrices-the transpose and the inverse-give rise to deep algebraic and geometric properties. The transpose rearranges a matrix by flipping it across its main diagonal, while the inverse, when it exists, acts as the undo operation for matrix multiplication.&lt;/p&gt;
    &lt;p&gt;The transpose of an &lt;/p&gt;
    &lt;p&gt;Formally,&lt;/p&gt;
    &lt;p&gt;$$ (A^T){ij} = a{ji}. $$&lt;/p&gt;
    &lt;p&gt;Example 2.3.1. If&lt;/p&gt;
    &lt;p&gt;then&lt;/p&gt;
    &lt;p&gt;Properties of the Transpose.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;$ (A^T)^T = A$.&lt;/item&gt;
      &lt;item&gt;$ (A+B)^T = A^T + B^T$.&lt;/item&gt;
      &lt;item&gt;$ (cA)^T = cA^T$, for scalar &lt;math-renderer&gt;$c$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;$ (AB)^T = B^T A^T$.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The last rule is crucial: the order reverses.&lt;/p&gt;
    &lt;p&gt;A square matrix &lt;/p&gt;
    &lt;p&gt;where &lt;/p&gt;
    &lt;p&gt;Not every matrix is invertible. A necessary condition is that &lt;/p&gt;
    &lt;p&gt;Example 2.3.2. Let&lt;/p&gt;
    &lt;p&gt;Its determinant is &lt;/p&gt;
    &lt;p&gt;\begin{bmatrix} -2 &amp;amp; 1 \ 1.5 &amp;amp; -0.5 \end{bmatrix}. $$&lt;/p&gt;
    &lt;p&gt;Verification:&lt;/p&gt;
    &lt;head rend="h1"&gt;$$ AA^{-1} = \begin{bmatrix} 1 &amp;amp; 2 \ 3 &amp;amp; 4 \end{bmatrix} \begin{bmatrix} -2 &amp;amp; 1 \ 1.5 &amp;amp; -0.5 \end{bmatrix}&lt;/head&gt;
    &lt;p&gt;\begin{bmatrix} 1 &amp;amp; 0 \ 0 &amp;amp; 1 \end{bmatrix}. $$&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The transpose corresponds to reflecting a linear transformation across the diagonal. For vectors, it switches between row and column forms.&lt;/item&gt;
      &lt;item&gt;The inverse, when it exists, corresponds to reversing a linear transformation. For example, if &lt;math-renderer&gt;$A$&lt;/math-renderer&gt;scales and rotates vectors,&lt;math-renderer&gt;$A^{-1}$&lt;/math-renderer&gt;rescales and rotates them back.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Transpose: &lt;math-renderer&gt;$A^T$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Inverse: &lt;math-renderer&gt;$A^{-1}$&lt;/math-renderer&gt;, defined only for invertible square matrices.&lt;/item&gt;
      &lt;item&gt;Identity: &lt;math-renderer&gt;$I_n$&lt;/math-renderer&gt;, acts as the multiplicative identity.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The transpose allows us to define symmetric and orthogonal matrices, central to geometry and numerical methods. The inverse underlies the solution of linear systems, encoding the idea of undoing a transformation. Together, these operations set the stage for determinants, eigenvalues, and orthogonalization.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Compute the transpose of&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Verify that &lt;math-renderer&gt;$(AB)^T = B^T A^T$&lt;/math-renderer&gt;for&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Determine whether&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;is invertible. If so, find &lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Find the inverse of&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;and explain its geometric action on vectors in the plane.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Prove that if &lt;math-renderer&gt;$A$&lt;/math-renderer&gt;is invertible, then so is&lt;math-renderer&gt;$A^T$&lt;/math-renderer&gt;, and&lt;math-renderer&gt;$(A^T)^{-1} = (A^{-1})^T$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Certain matrices occur so frequently in theory and applications that they are given special names. Recognizing their properties allows us to simplify computations and understand the structure of linear transformations more clearly.&lt;/p&gt;
    &lt;p&gt;The identity matrix &lt;/p&gt;
    &lt;p&gt;It acts as the multiplicative identity:&lt;/p&gt;
    &lt;p&gt;Geometrically, &lt;/p&gt;
    &lt;p&gt;A diagonal matrix has all off-diagonal entries zero:&lt;/p&gt;
    &lt;p&gt;Multiplication by a diagonal matrix scales each coordinate independently:&lt;/p&gt;
    &lt;p&gt;Example 2.4.1. Let&lt;/p&gt;
    &lt;p&gt;Then&lt;/p&gt;
    &lt;p&gt;A permutation matrix is obtained by permuting the rows of the identity matrix. Multiplying a vector by a permutation matrix reorders its coordinates.&lt;/p&gt;
    &lt;p&gt;Example 2.4.2. Let&lt;/p&gt;
    &lt;p&gt;Then&lt;/p&gt;
    &lt;p&gt;Thus, &lt;/p&gt;
    &lt;p&gt;Permutation matrices are always invertible; their inverses are simply their transposes.&lt;/p&gt;
    &lt;p&gt;A matrix is symmetric if&lt;/p&gt;
    &lt;p&gt;and skew-symmetric if&lt;/p&gt;
    &lt;p&gt;Symmetric matrices appear in quadratic forms and optimization, while skew-symmetric matrices describe rotations and cross products in geometry.&lt;/p&gt;
    &lt;p&gt;A square matrix &lt;/p&gt;
    &lt;p&gt;Equivalently, the rows (and columns) of &lt;/p&gt;
    &lt;p&gt;Example 2.4.3. The rotation matrix in the plane:&lt;/p&gt;
    &lt;p&gt;is orthogonal, since&lt;/p&gt;
    &lt;p&gt;Special matrices serve as the building blocks of linear algebra. Identity matrices define the neutral element, diagonal matrices simplify computations, permutation matrices reorder data, symmetric and orthogonal matrices describe fundamental geometric structures. Much of modern applied mathematics reduces complex problems to operations involving these simple forms.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Show that the product of two diagonal matrices is diagonal, and compute an example.&lt;/item&gt;
      &lt;item&gt;Find the permutation matrix that cycles &lt;math-renderer&gt;$(a,b,c)$&lt;/math-renderer&gt;into&lt;math-renderer&gt;$(b,c,a)$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Prove that every permutation matrix is invertible and its inverse is its transpose.&lt;/item&gt;
      &lt;item&gt;Verify that&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;is orthogonal. What geometric transformation does it represent? 5. Determine whether&lt;/p&gt;
    &lt;p&gt;are symmetric, skew-symmetric, or neither.&lt;/p&gt;
    &lt;p&gt;One of the central motivations for linear algebra is solving systems of linear equations. These systems arise naturally in science, engineering, and data analysis whenever multiple constraints interact. Matrices provide a compact language for expressing and solving them.&lt;/p&gt;
    &lt;p&gt;A linear system consists of equations where each unknown appears only to the first power and with no products between variables. A general system of &lt;/p&gt;
    &lt;p&gt;Here the coefficients &lt;/p&gt;
    &lt;p&gt;The system can be expressed compactly as:&lt;/p&gt;
    &lt;p&gt;where&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;math-renderer&gt;$A \in \mathbb{R}^{m \times n}$&lt;/math-renderer&gt;is the coefficient matrix&lt;math-renderer&gt;$[a_{ij}]$&lt;/math-renderer&gt;,&lt;/item&gt;
      &lt;item&gt;&lt;math-renderer&gt;$\mathbf{x} \in \mathbb{R}^n$&lt;/math-renderer&gt;is the column vector of unknowns,&lt;/item&gt;
      &lt;item&gt;&lt;math-renderer&gt;$\mathbf{b} \in \mathbb{R}^m$&lt;/math-renderer&gt;is the column vector of constants.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This formulation turns the problem of solving equations into analyzing the action of a matrix.&lt;/p&gt;
    &lt;p&gt;Example 3.1.1. The system&lt;/p&gt;
    &lt;p&gt;can be written as&lt;/p&gt;
    &lt;p&gt;\begin{bmatrix} 5 \ 4 \end{bmatrix}. $$&lt;/p&gt;
    &lt;p&gt;A linear system may have:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;No solution (inconsistent): The equations conflict. Example: $ \begin{cases} x + y = 1 \ x + y = 2 \end{cases} $ has no solution.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Exactly one solution (unique): The system’s equations intersect at a single point. Example: The above system with coefficient matrix $ \begin{bmatrix} 1 &amp;amp; 2 \ 3 &amp;amp; -1 \end{bmatrix} $ has a unique solution.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Infinitely many solutions: The equations describe overlapping constraints (e.g., multiple equations representing the same line or plane).&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The nature of the solution depends on the rank of &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;In &lt;math-renderer&gt;$\mathbb{R}^2$&lt;/math-renderer&gt;, each linear equation represents a line. Solving a system means finding intersection points of lines.&lt;/item&gt;
      &lt;item&gt;In &lt;math-renderer&gt;$\mathbb{R}^3$&lt;/math-renderer&gt;, each equation represents a plane. A system may have no solution (parallel planes), one solution (a unique intersection point), or infinitely many (a line of intersection).&lt;/item&gt;
      &lt;item&gt;In higher dimensions, the picture generalizes: solutions form intersections of hyperplanes.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Linear systems are the practical foundation of linear algebra. They appear in balancing chemical reactions, circuit analysis, least-squares regression, optimization, and computer graphics. Understanding how to represent and classify their solutions is the first step toward systematic solution methods like Gaussian elimination.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Write the following system in matrix form: $ \begin{cases} 2x + 3y - z = 7, \ x - y + 4z = 1, \ 3x + 2y + z = 5 \end{cases} $&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Determine whether the system $ \begin{cases} x + y = 1, \ 2x + 2y = 2 \end{cases} $ has no solution, one solution, or infinitely many solutions.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Geometrically interpret the system $ \begin{cases} x + y = 3, \ x - y = 1 \end{cases} $ in the plane.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Solve the system $ \begin{cases} 2x + y = 1, \ x - y = 4 \end{cases} $ and check your solution.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;In&lt;/p&gt;&lt;math-renderer&gt;$\mathbb{R}^3$&lt;/math-renderer&gt;, describe the solution set of $ \begin{cases} x + y + z = 0, \ 2x + 2y + 2z = 0 \end{cases} $. What geometric object does it represent?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To solve linear systems efficiently, we use Gaussian elimination: a systematic method of transforming a system into a simpler equivalent one whose solutions are easier to see. The method relies on elementary row operations that preserve the solution set.&lt;/p&gt;
    &lt;p&gt;On an augmented matrix &lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Row swapping: interchange two rows.&lt;/item&gt;
      &lt;item&gt;Row scaling: multiply a row by a nonzero scalar.&lt;/item&gt;
      &lt;item&gt;Row replacement: replace one row by itself plus a multiple of another row.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These operations correspond to re-expressing equations in different but equivalent forms.&lt;/p&gt;
    &lt;p&gt;A matrix is in row echelon form (REF) if:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;All nonzero rows are above any zero rows.&lt;/item&gt;
      &lt;item&gt;Each leading entry (the first nonzero number from the left in a row) is to the right of the leading entry in the row above.&lt;/item&gt;
      &lt;item&gt;All entries below a leading entry are zero.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Further, if each leading entry is 1 and is the only nonzero entry in its column, the matrix is in reduced row echelon form (RREF).&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Write the augmented matrix for the system.&lt;/item&gt;
      &lt;item&gt;Use row operations to create zeros below each pivot (the leading entry in a row).&lt;/item&gt;
      &lt;item&gt;Continue column by column until the matrix is in echelon form.&lt;/item&gt;
      &lt;item&gt;Solve by back substitution: starting from the last pivot equation and working upward.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If we continue to RREF, the solution can be read off directly.&lt;/p&gt;
    &lt;p&gt;Example 3.2.1. Solve&lt;/p&gt;
    &lt;p&gt;Step 1. Augmented matrix&lt;/p&gt;
    &lt;p&gt;Step 2. Eliminate below the first pivot&lt;/p&gt;
    &lt;p&gt;Subtract 2 times row 1 from row 2, and 3 times row 1 from row 3:&lt;/p&gt;
    &lt;p&gt;Step 3. Pivot in column 2&lt;/p&gt;
    &lt;p&gt;Divide row 2 by -3:&lt;/p&gt;
    &lt;p&gt;Add 7 times row 2 to row 3:&lt;/p&gt;
    &lt;p&gt;Step 4. Pivot in column 3&lt;/p&gt;
    &lt;p&gt;Divide row 3 by -2:&lt;/p&gt;
    &lt;p&gt;Step 5. Back substitution&lt;/p&gt;
    &lt;p&gt;From the last row: $ z = \tfrac{11}{3}. $&lt;/p&gt;
    &lt;p&gt;Second row: $ y - z = -\tfrac{1}{3} \implies y = -\tfrac{1}{3} + \tfrac{11}{3} = \tfrac{10}{3}. $&lt;/p&gt;
    &lt;p&gt;First row: $ x + 2y - z = 3 \implies x + 2\cdot\tfrac{10}{3} - \tfrac{11}{3} = 3. $&lt;/p&gt;
    &lt;p&gt;So $ x + \tfrac{20}{3} - \tfrac{11}{3} = 3 \implies x + 3 = 3 \implies x = 0. $&lt;/p&gt;
    &lt;p&gt;Solution: $ (x,y,z) = \big(0, \tfrac{10}{3}, \tfrac{11}{3}\big). $&lt;/p&gt;
    &lt;p&gt;Gaussian elimination is the foundation of computational linear algebra. It reduces complex systems to a form where solutions are visible, and it forms the basis for algorithms used in numerical analysis, scientific computing, and machine learning.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Solve by Gaussian elimination: $ \begin{cases} x + y = 2, \ 2x - y = 0. \end{cases} $&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Reduce the following augmented matrix to REF: $ \left[\begin{array}{ccc|c} 1 &amp;amp; 1 &amp;amp; 1 &amp;amp; 6 \ 2 &amp;amp; -1 &amp;amp; 3 &amp;amp; 14 \ 1 &amp;amp; 4 &amp;amp; -2 &amp;amp; -2 \end{array}\right]. $&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Show that Gaussian elimination always produces either:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;a unique solution,&lt;/item&gt;
          &lt;item&gt;infinitely many solutions, or&lt;/item&gt;
          &lt;item&gt;a contradiction (no solution).&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Use Gaussian elimination to find all solutions of $ \begin{cases} x + y + z = 0, \ 2x + y + z = 1. \end{cases} $&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Explain why pivoting (choosing the largest available pivot element) is useful in numerical computation.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Gaussian elimination not only provides solutions but also reveals the structure of a linear system. Two key ideas are the rank of a matrix and the consistency of a system. Rank measures the amount of independent information in the equations, while consistency determines whether the system has at least one solution.&lt;/p&gt;
    &lt;p&gt;The rank of a matrix is the number of leading pivots in its row echelon form. Equivalently, it is the maximum number of linearly independent rows or columns.&lt;/p&gt;
    &lt;p&gt;Formally,&lt;/p&gt;
    &lt;p&gt;The rank tells us the effective dimension of the space spanned by the rows (or columns).&lt;/p&gt;
    &lt;p&gt;Example 3.3.1. For&lt;/p&gt;
    &lt;p&gt;row reduction gives&lt;/p&gt;
    &lt;p&gt;Thus, &lt;/p&gt;
    &lt;p&gt;Consider the system &lt;/p&gt;
    &lt;p&gt;where &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If &lt;math-renderer&gt;$\text{rank}(A) = \text{rank}(A|\mathbf{b}) = n$&lt;/math-renderer&gt;(number of unknowns), the system has a unique solution.&lt;/item&gt;
      &lt;item&gt;If &lt;math-renderer&gt;$\text{rank}(A) = \text{rank}(A|\mathbf{b}) &amp;amp;lt; n$&lt;/math-renderer&gt;, the system has infinitely many solutions.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example 3.3.2. Consider&lt;/p&gt;
    &lt;p&gt;The augmented matrix is&lt;/p&gt;
    &lt;p&gt;Row reduction gives&lt;/p&gt;
    &lt;p&gt;Here, &lt;/p&gt;
    &lt;p&gt;Example 3.3.3. For&lt;/p&gt;
    &lt;p&gt;the augmented matrix reduces to&lt;/p&gt;
    &lt;p&gt;Here, &lt;/p&gt;
    &lt;p&gt;Rank is a measure of independence: it tells us how many truly distinct equations or directions are present. Consistency explains when equations align versus when they contradict. These concepts connect linear systems to vector spaces and prepare for the ideas of dimension, basis, and the Rank–Nullity Theorem.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Compute the rank of&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Determine whether the system&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;is consistent.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Show that the rank of the identity matrix&lt;/p&gt;&lt;math-renderer&gt;$I_n$&lt;/math-renderer&gt;is&lt;math-renderer&gt;$n$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Give an example of a system in&lt;/p&gt;&lt;math-renderer&gt;$\mathbb{R}^3$&lt;/math-renderer&gt;with infinitely many solutions, and explain why it satisfies the rank condition.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Prove that for any matrix&lt;/p&gt;&lt;math-renderer&gt;$A \in \mathbb{R}^{m \times n}$&lt;/math-renderer&gt;, $ \text{rank}(A) \leq \min(m,n). $&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A homogeneous system is a linear system in which all constant terms are zero:&lt;/p&gt;
    &lt;p&gt;where &lt;/p&gt;
    &lt;p&gt;Every homogeneous system has at least one solution:&lt;/p&gt;
    &lt;p&gt;This is called the trivial solution. The interesting question is whether nontrivial solutions (nonzero vectors) exist.&lt;/p&gt;
    &lt;p&gt;Nontrivial solutions exist precisely when the number of unknowns exceeds the rank of the coefficient matrix:&lt;/p&gt;
    &lt;p&gt;In this case, there are infinitely many solutions, forming a subspace of &lt;/p&gt;
    &lt;p&gt;where null(A) is the set of all solutions to &lt;/p&gt;
    &lt;p&gt;Example 3.4.1. Consider&lt;/p&gt;
    &lt;p&gt;The augmented matrix is&lt;/p&gt;
    &lt;p&gt;Row reduction:&lt;/p&gt;
    &lt;p&gt;So the system is equivalent to:&lt;/p&gt;
    &lt;p&gt;From the second equation, &lt;/p&gt;
    &lt;p&gt;Thus solutions are:&lt;/p&gt;
    &lt;p&gt;The null space is the line spanned by the vector &lt;/p&gt;
    &lt;p&gt;The solution set of a homogeneous system is always a subspace of &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If &lt;math-renderer&gt;$\text{rank}(A) = n$&lt;/math-renderer&gt;, the only solution is the zero vector.&lt;/item&gt;
      &lt;item&gt;If &lt;math-renderer&gt;$\text{rank}(A) = n-1$&lt;/math-renderer&gt;, the solution set is a line through the origin.&lt;/item&gt;
      &lt;item&gt;If &lt;math-renderer&gt;$\text{rank}(A) = n-2$&lt;/math-renderer&gt;, the solution set is a plane through the origin.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;More generally, the null space has dimension &lt;/p&gt;
    &lt;p&gt;Homogeneous systems are central to understanding vector spaces, subspaces, and dimension. They lead directly to the concepts of kernel, null space, and linear dependence. In applications, homogeneous systems appear in equilibrium problems, eigenvalue equations, and computer graphics transformations.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Solve the homogeneous system&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;What is the dimension of its solution space?&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Find all solutions of&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Show that the solution set of any homogeneous system is a subspace of&lt;/p&gt;&lt;math-renderer&gt;$\mathbb{R}^n$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Suppose&lt;/p&gt;&lt;math-renderer&gt;$A$&lt;/math-renderer&gt;is a&lt;math-renderer&gt;$3 \times 3$&lt;/math-renderer&gt;matrix with&lt;math-renderer&gt;$\text{rank}(A) = 2$&lt;/math-renderer&gt;. What is the dimension of the null space of&lt;math-renderer&gt;$A$&lt;/math-renderer&gt;?&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;For&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;compute a basis for the null space of &lt;/p&gt;
    &lt;p&gt;Up to now we have studied vectors and matrices concretely in &lt;/p&gt;
    &lt;p&gt;A vector space over the real numbers &lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Vector addition: For any &lt;math-renderer&gt;$\mathbf{u}, \mathbf{v} \in V$&lt;/math-renderer&gt;, there is a vector&lt;math-renderer&gt;$\mathbf{u} + \mathbf{v} \in V$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Scalar multiplication: For any scalar &lt;math-renderer&gt;$c \in \mathbb{R}$&lt;/math-renderer&gt;and any&lt;math-renderer&gt;$\mathbf{v} \in V$&lt;/math-renderer&gt;, there is a vector&lt;math-renderer&gt;$c\mathbf{v} \in V$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These operations must satisfy the following axioms (for all &lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Commutativity of addition: &lt;math-renderer&gt;$\mathbf{u} + \mathbf{v} = \mathbf{v} + \mathbf{u}$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Associativity of addition: &lt;math-renderer&gt;$(\mathbf{u} + \mathbf{v}) + \mathbf{w} = \mathbf{u} + (\mathbf{v} + \mathbf{w})$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Additive identity: There exists a zero vector &lt;math-renderer&gt;$\mathbf{0} \in V$&lt;/math-renderer&gt;such that&lt;math-renderer&gt;$\mathbf{v} + \mathbf{0} = \mathbf{v}$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Additive inverses: For each &lt;math-renderer&gt;$\mathbf{v} \in V$&lt;/math-renderer&gt;, there exists&lt;math-renderer&gt;$(-\mathbf{v} \in V$&lt;/math-renderer&gt;such that&lt;math-renderer&gt;$\mathbf{v} + (-\mathbf{v}) = \mathbf{0}$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Compatibility of scalar multiplication: &lt;math-renderer&gt;$a(b\mathbf{v}) = (ab)\mathbf{v}$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Identity element of scalars: &lt;math-renderer&gt;$1 \cdot \mathbf{v} = \mathbf{v}$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Distributivity over vector addition: &lt;math-renderer&gt;$a(\mathbf{u} + \mathbf{v}) = a\mathbf{u} + a\mathbf{v}$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Distributivity over scalar addition: &lt;math-renderer&gt;$(a+b)\mathbf{v} = a\mathbf{v} + b\mathbf{v}$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If a set &lt;/p&gt;
    &lt;p&gt;Example 4.1.1. Standard Euclidean space &lt;/p&gt;
    &lt;p&gt;Example 4.1.2. Polynomials The set of all polynomials with real coefficients, denoted &lt;/p&gt;
    &lt;p&gt;Example 4.1.3. Functions The set of all real-valued functions on an interval, e.g. &lt;/p&gt;
    &lt;p&gt;Not every set with operations qualifies. For instance, the set of positive real numbers under usual addition is not a vector space, because additive inverses (negative numbers) are missing. The axioms must all hold.&lt;/p&gt;
    &lt;p&gt;In familiar cases like &lt;/p&gt;
    &lt;p&gt;The concept of vector space unifies seemingly different mathematical objects under a single framework. Whether dealing with forces in physics, signals in engineering, or data in machine learning, the common language of vector spaces allows us to use the same techniques everywhere.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Verify that &lt;math-renderer&gt;$\mathbb{R}^2$&lt;/math-renderer&gt;with standard addition and scalar multiplication satisfies all eight vector space axioms.&lt;/item&gt;
      &lt;item&gt;Show that the set of integers &lt;math-renderer&gt;$\mathbb{Z}$&lt;/math-renderer&gt;with ordinary operations is not a vector space over&lt;math-renderer&gt;$\mathbb{R}$&lt;/math-renderer&gt;. Which axiom fails?&lt;/item&gt;
      &lt;item&gt;Consider the set of all polynomials of degree at most 3. Show it forms a vector space over &lt;math-renderer&gt;$\mathbb{R}$&lt;/math-renderer&gt;. What is its dimension?&lt;/item&gt;
      &lt;item&gt;Give an example of a vector space where the vectors are not geometric objects.&lt;/item&gt;
      &lt;item&gt;Prove that in any vector space, the zero vector is unique.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A subspace is a smaller vector space living inside a larger one. Just as lines and planes naturally sit inside three-dimensional space, subspaces generalize these ideas to higher dimensions and more abstract settings.&lt;/p&gt;
    &lt;p&gt;Let &lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;math-renderer&gt;$\mathbf{0} \in W$&lt;/math-renderer&gt;(contains the zero vector),&lt;/item&gt;
      &lt;item&gt;For all &lt;math-renderer&gt;$\mathbf{u}, \mathbf{v} \in W$&lt;/math-renderer&gt;, the sum&lt;math-renderer&gt;$\mathbf{u} + \mathbf{v} \in W$&lt;/math-renderer&gt;(closed under addition),&lt;/item&gt;
      &lt;item&gt;For all scalars &lt;math-renderer&gt;$c \in \mathbb{R}$&lt;/math-renderer&gt;and vectors&lt;math-renderer&gt;$\mathbf{v} \in W$&lt;/math-renderer&gt;, the product&lt;math-renderer&gt;$c\mathbf{v} \in W$&lt;/math-renderer&gt;(closed under scalar multiplication).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If these hold, then &lt;/p&gt;
    &lt;p&gt;Example 4.2.1. Line through the origin in &lt;/p&gt;
    &lt;p&gt;is a subspace of &lt;/p&gt;
    &lt;p&gt;Example 4.2.2. The x–y plane in &lt;/p&gt;
    &lt;p&gt;is a subspace of &lt;/p&gt;
    &lt;p&gt;Example 4.2.3. Null space of a matrix For a matrix &lt;/p&gt;
    &lt;p&gt;is a subspace of &lt;/p&gt;
    &lt;p&gt;Not every subset is a subspace.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The set &lt;math-renderer&gt;${ (x,y) \in \mathbb{R}^2 \mid x \geq 0 }$&lt;/math-renderer&gt;is not a subspace: it is not closed under scalar multiplication (a negative scalar breaks the condition).&lt;/item&gt;
      &lt;item&gt;Any line in &lt;math-renderer&gt;$\mathbb{R}^2$&lt;/math-renderer&gt;that does not pass through the origin is not a subspace, because it does not contain&lt;math-renderer&gt;$\mathbf{0}$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Subspaces are the linear structures inside vector spaces.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;In &lt;math-renderer&gt;$\mathbb{R}^2$&lt;/math-renderer&gt;, the subspaces are: the zero vector, any line through the origin, or the entire plane.&lt;/item&gt;
      &lt;item&gt;In &lt;math-renderer&gt;$\mathbb{R}^3$&lt;/math-renderer&gt;, the subspaces are: the zero vector, any line through the origin, any plane through the origin, or the entire space.&lt;/item&gt;
      &lt;item&gt;In higher dimensions, the same principle applies: subspaces are the flat linear pieces through the origin.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Subspaces capture the essential structure of linear problems. Column spaces, row spaces, and null spaces are all subspaces. Much of linear algebra consists of understanding how these subspaces intersect, span, and complement each other.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Prove that the set &lt;math-renderer&gt;$W = { (x,0) \mid x \in \mathbb{R} } \subseteq \mathbb{R}^2$&lt;/math-renderer&gt;is a subspace.&lt;/item&gt;
      &lt;item&gt;Show that the line &lt;math-renderer&gt;${ (1+t, 2t) \mid t \in \mathbb{R} }$&lt;/math-renderer&gt;is not a subspace of&lt;math-renderer&gt;$\mathbb{R}^2$&lt;/math-renderer&gt;. Which condition fails?&lt;/item&gt;
      &lt;item&gt;Determine whether the set of all vectors &lt;math-renderer&gt;$(x,y,z) \in \mathbb{R}^3$&lt;/math-renderer&gt;satisfying&lt;math-renderer&gt;$x+y+z=0$&lt;/math-renderer&gt;is a subspace.&lt;/item&gt;
      &lt;item&gt;For the matrix&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;describe the null space of &lt;/p&gt;
    &lt;p&gt;The ideas of span, basis, and dimension provide the language for describing the size and structure of subspaces. Together, they tell us how a vector space is generated, how many building blocks it requires, and how those blocks can be chosen.&lt;/p&gt;
    &lt;p&gt;Given a set of vectors &lt;/p&gt;
    &lt;p&gt;The span is always a subspace of &lt;/p&gt;
    &lt;p&gt;Example 4.3.1. In &lt;/p&gt;
    &lt;p&gt;A basis of a vector space &lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Span &lt;math-renderer&gt;$V$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Are linearly independent (no vector in the set is a linear combination of the others).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If either condition fails, the set is not a basis.&lt;/p&gt;
    &lt;p&gt;Example 4.3.2. In &lt;/p&gt;
    &lt;p&gt;form a basis. Every vector &lt;/p&gt;
    &lt;p&gt;The dimension of a vector space &lt;/p&gt;
    &lt;p&gt;Examples 4.3.3.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;math-renderer&gt;$\dim(\mathbb{R}^2) = 2$&lt;/math-renderer&gt;, with basis&lt;math-renderer&gt;$(1,0), (0,1)$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;math-renderer&gt;$\dim(\mathbb{R}^3) = 3$&lt;/math-renderer&gt;, with basis&lt;math-renderer&gt;$(1,0,0), (0,1,0), (0,0,1)$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;The set of polynomials of degree at most 3 has dimension 4, with basis &lt;math-renderer&gt;$(1, x, x^2, x^3)$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The span is like the reach of a set of vectors.&lt;/item&gt;
      &lt;item&gt;A basis is the minimal set of directions needed to reach everything in the space.&lt;/item&gt;
      &lt;item&gt;The dimension is the count of those independent directions.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Lines, planes, and higher-dimensional flats can all be described in terms of span, basis, and dimension.&lt;/p&gt;
    &lt;p&gt;These concepts classify vector spaces and subspaces in terms of size and structure. Many theorems in linear algebra-such as the Rank–Nullity Theorem-are consequences of understanding span, basis, and dimension. In practical terms, bases are how we encode data in coordinates, and dimension tells us how much freedom a system truly has.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Show that &lt;math-renderer&gt;$(1,0,0)$&lt;/math-renderer&gt;,&lt;math-renderer&gt;$(0,1,0)$&lt;/math-renderer&gt;,&lt;math-renderer&gt;$(1,1,0)$&lt;/math-renderer&gt;span the&lt;math-renderer&gt;$xy$&lt;/math-renderer&gt;-plane in&lt;math-renderer&gt;$\mathbb{R}^3$&lt;/math-renderer&gt;. Are they a basis?&lt;/item&gt;
      &lt;item&gt;Find a basis for the line &lt;math-renderer&gt;${(2t,-3t,t) : t \in \mathbb{R}}$&lt;/math-renderer&gt;in&lt;math-renderer&gt;$\mathbb{R}^3$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Determine the dimension of the subspace of &lt;math-renderer&gt;$\mathbb{R}^3$&lt;/math-renderer&gt;defined by&lt;math-renderer&gt;$x+y+z=0$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Prove that any two different bases of &lt;math-renderer&gt;$\mathbb{R}^n$&lt;/math-renderer&gt;must contain exactly&lt;math-renderer&gt;$n$&lt;/math-renderer&gt;vectors.&lt;/item&gt;
      &lt;item&gt;Give a basis for the set of polynomials of degree &lt;math-renderer&gt;$\leq 2$&lt;/math-renderer&gt;. What is its dimension?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Once a basis for a vector space is chosen, every vector can be expressed uniquely as a linear combination of the basis vectors. The coefficients in this combination are called the coordinates of the vector relative to that basis. Coordinates allow us to move between the abstract world of vector spaces and the concrete world of numbers.&lt;/p&gt;
    &lt;p&gt;Let &lt;/p&gt;
    &lt;p&gt;be an ordered basis for &lt;/p&gt;
    &lt;p&gt;The scalars &lt;/p&gt;
    &lt;p&gt;Example 4.4.1. Let the basis be&lt;/p&gt;
    &lt;p&gt;To find the coordinates of &lt;/p&gt;
    &lt;p&gt;This gives the system&lt;/p&gt;
    &lt;p&gt;Adding: &lt;/p&gt;
    &lt;p&gt;So,&lt;/p&gt;
    &lt;p&gt;In &lt;/p&gt;
    &lt;p&gt;Relative to this basis, the coordinates of a vector are simply its entries. Thus, column vectors are coordinate representations by default.&lt;/p&gt;
    &lt;p&gt;If &lt;/p&gt;
    &lt;p&gt;with basis vectors as columns. For any vector &lt;/p&gt;
    &lt;p&gt;$$ \mathbf{u} = P[\mathbf{u}]{\mathcal{B}}, \qquad [\mathbf{u}]{\mathcal{B}} = P^{-1}\mathbf{u}. $$&lt;/p&gt;
    &lt;p&gt;Thus, switching between bases reduces to matrix multiplication.&lt;/p&gt;
    &lt;p&gt;Coordinates are the address of a vector relative to a chosen set of directions. Different bases are like different coordinate systems: Cartesian, rotated, skewed, or scaled. The same vector may look very different numerically depending on the basis, but its geometric identity is unchanged.&lt;/p&gt;
    &lt;p&gt;Coordinates turn abstract vectors into concrete numerical data. Changing basis is the algebraic language for rotations of axes, diagonalization of matrices, and principal component analysis in data science. Mastery of coordinates is essential for moving fluidly between geometry, algebra, and computation.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Express &lt;math-renderer&gt;$(4,2)$&lt;/math-renderer&gt;in terms of the basis&lt;math-renderer&gt;$(1,1), (1,-1)$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Find the coordinates of &lt;math-renderer&gt;$(1,2,3)$&lt;/math-renderer&gt;relative to the standard basis of&lt;math-renderer&gt;$\mathbb{R}^3$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;If &lt;math-renderer&gt;$\mathcal{B} = {(2,0), (0,3)}$&lt;/math-renderer&gt;, compute&lt;math-renderer&gt;$[ (4,6) ]_{\mathcal{B}}$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Construct the change of basis matrix from the standard basis of &lt;math-renderer&gt;$\mathbb{R}^2$&lt;/math-renderer&gt;to&lt;math-renderer&gt;$\mathcal{B} = {(1,1), (1,-1)}$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Prove that coordinate representation with respect to a basis is unique.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A central theme of linear algebra is understanding linear transformations: functions between vector spaces that preserve their algebraic structure. These transformations generalize the idea of matrix multiplication and capture the essence of linear behavior.&lt;/p&gt;
    &lt;p&gt;Let &lt;/p&gt;
    &lt;p&gt;is called a linear transformation (or linear map) if for all vectors &lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Additivity:&lt;/p&gt;
        &lt;p&gt;$$ T(\mathbf{u} + \mathbf{v}) = T(\mathbf{u}) + T(\mathbf{v}), $$&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Homogeneity:&lt;/p&gt;
        &lt;p&gt;$$ T(c\mathbf{u}) = cT(\mathbf{u}). $$&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If both conditions hold, then &lt;/p&gt;
    &lt;p&gt;Example 5.1.1. Scaling in &lt;/p&gt;
    &lt;p&gt;This doubles the length of every vector, preserving direction. It is linear.&lt;/p&gt;
    &lt;p&gt;Example 5.1.2. Rotation. Let &lt;/p&gt;
    &lt;p&gt;This rotates vectors by angle &lt;/p&gt;
    &lt;p&gt;Example 5.1.3. Differentiation. Let &lt;/p&gt;
    &lt;p&gt;The map &lt;/p&gt;
    &lt;p&gt;is not linear, because &lt;/p&gt;
    &lt;p&gt;Linear transformations are exactly those that preserve the origin, lines through the origin, and proportions along those lines. They include familiar operations: scaling, rotations, reflections, shears, and projections. Nonlinear transformations bend or curve space, breaking these properties.&lt;/p&gt;
    &lt;p&gt;Linear transformations unify geometry, algebra, and computation. They explain how matrices act on vectors, how data can be rotated or projected, and how systems evolve under linear rules. Much of linear algebra is devoted to understanding these transformations, their representations, and their invariants.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Verify that&lt;/p&gt;&lt;math-renderer&gt;$T(x,y) = (3x-y, 2y)$&lt;/math-renderer&gt;is a linear transformation on&lt;math-renderer&gt;$\mathbb{R}^2$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Show that&lt;/p&gt;&lt;math-renderer&gt;$T(x,y) = (x+1, y)$&lt;/math-renderer&gt;is not linear. Which axiom fails?&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Prove that if&lt;/p&gt;&lt;math-renderer&gt;$T$&lt;/math-renderer&gt;and&lt;math-renderer&gt;$S$&lt;/math-renderer&gt;are linear transformations, then so is&lt;math-renderer&gt;$T+S$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Give an example of a linear transformation from&lt;/p&gt;&lt;math-renderer&gt;$\mathbb{R}^3$&lt;/math-renderer&gt;to&lt;math-renderer&gt;$\mathbb{R}^2$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Let&lt;/p&gt;&lt;math-renderer&gt;$T:\mathbb{R}[x] \to \mathbb{R}[x]$&lt;/math-renderer&gt;be integration:&lt;p&gt;$$ T(p(x)) = \int_0^x p(t),dt. $$&lt;/p&gt;&lt;p&gt;Prove that&lt;/p&gt;&lt;math-renderer&gt;$T$&lt;/math-renderer&gt;is a linear transformation.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Every linear transformation between finite-dimensional vector spaces can be represented by a matrix. This correspondence is one of the central insights of linear algebra: it lets us use the tools of matrix arithmetic to study abstract transformations.&lt;/p&gt;
    &lt;p&gt;Let &lt;/p&gt;
    &lt;p&gt;The action of &lt;/p&gt;
    &lt;p&gt;$$ T(\mathbf{e}j) = \begin{bmatrix} a{1j} \ a_{2j} \ \vdots \ a_{mj} \end{bmatrix}. $$&lt;/p&gt;
    &lt;p&gt;Placing these outputs as columns gives the matrix of &lt;/p&gt;
    &lt;p&gt;Then for any vector &lt;/p&gt;
    &lt;p&gt;Example 5.2.1. Scaling in &lt;/p&gt;
    &lt;p&gt;So the matrix is&lt;/p&gt;
    &lt;p&gt;Example 5.2.2. Rotation in the plane. The rotation transformation &lt;/p&gt;
    &lt;p&gt;Example 5.2.3. Projection onto the x-axis. The map &lt;/p&gt;
    &lt;p&gt;Matrix representations depend on the chosen basis. If &lt;/p&gt;
    &lt;p&gt;Matrices are not just convenient notation-they are linear maps once a basis is fixed. Every rotation, reflection, projection, shear, or scaling corresponds to multiplying by a specific matrix. Thus, studying linear transformations reduces to studying their matrices.&lt;/p&gt;
    &lt;p&gt;Matrix representations make linear transformations computable. They connect abstract definitions to explicit calculations, enabling algorithms for solving systems, finding eigenvalues, and performing decompositions. Applications from graphics to machine learning depend on this translation.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Find the matrix representation of &lt;math-renderer&gt;$T:\mathbb{R}^2 \to \mathbb{R}^2$&lt;/math-renderer&gt;,&lt;math-renderer&gt;$T(x,y) = (x+y, x-y)$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Determine the matrix of the linear transformation &lt;math-renderer&gt;$T:\mathbb{R}^3 \to \mathbb{R}^2$&lt;/math-renderer&gt;,&lt;math-renderer&gt;$T(x,y,z) = (x+z, y-2z)$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;What matrix represents reflection across the line &lt;math-renderer&gt;$y=x$&lt;/math-renderer&gt;in&lt;math-renderer&gt;$\mathbb{R}^2$&lt;/math-renderer&gt;?&lt;/item&gt;
      &lt;item&gt;Show that the matrix of the identity transformation on &lt;math-renderer&gt;$\mathbb{R}^n$&lt;/math-renderer&gt;is&lt;math-renderer&gt;$I_n$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;For the differentiation map &lt;math-renderer&gt;$D:\mathbb{R}_2[x] \to \mathbb{R}_1[x]$&lt;/math-renderer&gt;, where&lt;math-renderer&gt;$\mathbb{R}_k[x]$&lt;/math-renderer&gt;is the space of polynomials of degree at most&lt;math-renderer&gt;$k$&lt;/math-renderer&gt;, find the matrix of&lt;math-renderer&gt;$D$&lt;/math-renderer&gt;relative to the bases&lt;math-renderer&gt;${1,x,x^2}$&lt;/math-renderer&gt;and&lt;math-renderer&gt;${1,x}$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To understand a linear transformation deeply, we must examine what it kills and what it produces. These ideas are captured by the kernel and the image, two fundamental subspaces associated with any linear map.&lt;/p&gt;
    &lt;p&gt;The kernel (or null space) of a linear transformation &lt;/p&gt;
    &lt;p&gt;The kernel is always a subspace of &lt;/p&gt;
    &lt;p&gt;Example 5.3.1. Let &lt;/p&gt;
    &lt;p&gt;In matrix form,&lt;/p&gt;
    &lt;p&gt;To find the kernel, solve&lt;/p&gt;
    &lt;p&gt;This gives the equations &lt;/p&gt;
    &lt;p&gt;a line in &lt;/p&gt;
    &lt;p&gt;The image (or range) of a linear transformation &lt;/p&gt;
    &lt;p&gt;Equivalently, it is the span of the columns of the representing matrix. The image is always a subspace of &lt;/p&gt;
    &lt;p&gt;Example 5.3.2. For the same transformation as above,&lt;/p&gt;
    &lt;p&gt;the columns are &lt;/p&gt;
    &lt;p&gt;For a linear transformation &lt;/p&gt;
    &lt;p&gt;This fundamental result connects the lost directions (kernel) with the achieved directions (image).&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The kernel describes how the transformation flattens space (e.g., projecting a 3D object onto a plane).&lt;/item&gt;
      &lt;item&gt;The image describes the target subspace reached by the transformation.&lt;/item&gt;
      &lt;item&gt;The rank–nullity theorem quantifies the tradeoff: the more dimensions collapse, the fewer remain in the image.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Kernel and image capture the essence of a linear map. They classify transformations, explain when systems have unique or infinite solutions, and form the backbone of important results like the Rank–Nullity Theorem, diagonalization, and spectral theory.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Find the kernel and image of &lt;math-renderer&gt;$T:\mathbb{R}^2 \to \mathbb{R}^2$&lt;/math-renderer&gt;,&lt;math-renderer&gt;$T(x,y) = (x-y, x+y)$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Let $A = \begin{bmatrix} 1 &amp;amp; 2 &amp;amp; 3 \ 0 &amp;amp; 1 &amp;amp; 4 \end{bmatrix}$. Find bases for &lt;math-renderer&gt;$\ker(A)$&lt;/math-renderer&gt;and&lt;math-renderer&gt;$\text{im}(A)$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;For the projection map &lt;math-renderer&gt;$P(x,y,z) = (x,y,0)$&lt;/math-renderer&gt;, describe the kernel and image.&lt;/item&gt;
      &lt;item&gt;Prove that &lt;math-renderer&gt;$\ker(T)$&lt;/math-renderer&gt;and&lt;math-renderer&gt;$\text{im}(T)$&lt;/math-renderer&gt;are always subspaces.&lt;/item&gt;
      &lt;item&gt;Verify the Rank–Nullity Theorem for the transformation in Example 5.3.1.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Linear transformations can look very different depending on the coordinate system we use. The process of rewriting vectors and transformations relative to a new basis is called a change of basis. This concept lies at the heart of diagonalization, orthogonalization, and many computational techniques.&lt;/p&gt;
    &lt;p&gt;Suppose &lt;/p&gt;
    &lt;p&gt;If &lt;/p&gt;
    &lt;p&gt;Equivalently,&lt;/p&gt;
    &lt;p&gt;Here, &lt;/p&gt;
    &lt;p&gt;Let &lt;/p&gt;
    &lt;p&gt;Thus, changing basis corresponds to a similarity transformation of the matrix.&lt;/p&gt;
    &lt;p&gt;Example 5.4.1. Let &lt;/p&gt;
    &lt;p&gt;In the standard basis, its matrix is&lt;/p&gt;
    &lt;p&gt;Now consider the basis &lt;/p&gt;
    &lt;p&gt;Then&lt;/p&gt;
    &lt;p&gt;Computing gives&lt;/p&gt;
    &lt;p&gt;In this new basis, the transformation is diagonal: one direction is scaled by 4, the other collapsed to 0.&lt;/p&gt;
    &lt;p&gt;Change of basis is like rotating or skewing your coordinate grid. The underlying transformation does not change, but its description in numbers becomes simpler or more complicated depending on the basis. Finding a basis that simplifies a transformation (often a diagonal basis) is a key theme in linear algebra.&lt;/p&gt;
    &lt;p&gt;Change of basis connects the abstract notion of similarity to practical computation. It is the tool that allows us to diagonalize matrices, compute eigenvalues, and simplify complex transformations. In applications, it corresponds to choosing a more natural coordinate system-whether in geometry, physics, or machine learning.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Let $A = \begin{bmatrix} 2 &amp;amp; 1 \ 0 &amp;amp; 2 \end{bmatrix}$. Compute its representation in the basis &lt;math-renderer&gt;${(1,0),(1,1)}$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Find the change-of-basis matrix from the standard basis of &lt;math-renderer&gt;$\mathbb{R}^2$&lt;/math-renderer&gt;to&lt;math-renderer&gt;${(2,1),(1,1)}$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Prove that similar matrices (related by &lt;math-renderer&gt;$P^{-1}AP$&lt;/math-renderer&gt;) represent the same linear transformation under different bases.&lt;/item&gt;
      &lt;item&gt;Diagonalize the matrix $A = \begin{bmatrix} 1 &amp;amp; 0 \ 0 &amp;amp; -1 \end{bmatrix}$ in the basis &lt;math-renderer&gt;${(1,1),(1,-1)}$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;In &lt;math-renderer&gt;$\mathbb{R}^3$&lt;/math-renderer&gt;, let&lt;math-renderer&gt;$\mathcal{B} = {(1,0,0),(1,1,0),(1,1,1)}$&lt;/math-renderer&gt;. Construct the change-of-basis matrix&lt;math-renderer&gt;$P$&lt;/math-renderer&gt;and compute&lt;math-renderer&gt;$P^{-1}$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Determinants are numerical values associated with square matrices. At first they may appear as a complicated formula, but their importance comes from what they measure: determinants encode scaling, orientation, and invertibility of linear transformations. They bridge algebra and geometry.&lt;/p&gt;
    &lt;p&gt;For a &lt;/p&gt;
    &lt;p&gt;the determinant is defined as&lt;/p&gt;
    &lt;p&gt;Geometric meaning: If &lt;/p&gt;
    &lt;p&gt;For&lt;/p&gt;
    &lt;p&gt;the determinant can be computed as&lt;/p&gt;
    &lt;p&gt;Geometric meaning: In &lt;/p&gt;
    &lt;p&gt;For &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If &lt;math-renderer&gt;$\det(A) = 0$&lt;/math-renderer&gt;: the transformation squashes space into a lower dimension, so&lt;math-renderer&gt;$A$&lt;/math-renderer&gt;is not invertible.&lt;/item&gt;
      &lt;item&gt;If &lt;math-renderer&gt;$\det(A) &amp;amp;gt; 0$&lt;/math-renderer&gt;: volume is scaled by&lt;math-renderer&gt;$\det(A)$&lt;/math-renderer&gt;, orientation preserved.&lt;/item&gt;
      &lt;item&gt;If &lt;math-renderer&gt;$\det(A) &amp;amp;lt; 0$&lt;/math-renderer&gt;: volume is scaled by&lt;math-renderer&gt;$|\det(A)|$&lt;/math-renderer&gt;, orientation reversed.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Shear in&lt;/p&gt;&lt;math-renderer&gt;$\mathbb{R}^2$&lt;/math-renderer&gt;: $A = \begin{bmatrix} 1 &amp;amp; 1 \ 0 &amp;amp; 1 \end{bmatrix}$. Then&lt;math-renderer&gt;$\det(A) = 1$&lt;/math-renderer&gt;. The transformation slants the unit square into a parallelogram but preserves area.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Projection in&lt;/p&gt;&lt;math-renderer&gt;$\mathbb{R}^2$&lt;/math-renderer&gt;: $A = \begin{bmatrix} 1 &amp;amp; 0 \ 0 &amp;amp; 0 \end{bmatrix}$. Then&lt;math-renderer&gt;$\det(A) = 0$&lt;/math-renderer&gt;. The unit square collapses into a line segment: area vanishes.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Rotation in&lt;/p&gt;&lt;math-renderer&gt;$\mathbb{R}^2$&lt;/math-renderer&gt;: $R_\theta = \begin{bmatrix} \cos\theta &amp;amp; -\sin\theta \ \sin\theta &amp;amp; \cos\theta \end{bmatrix}$. Then&lt;math-renderer&gt;$\det(R_\theta) = 1$&lt;/math-renderer&gt;. Rotations preserve area and orientation.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The determinant is not just a formula-it is a measure of transformation. It tells us whether a matrix is invertible, how it distorts space, and whether it flips orientation. This geometric insight makes the determinant indispensable in analysis, geometry, and applied mathematics.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Compute the determinant of $\begin{bmatrix} 2 &amp;amp; 3 \ 1 &amp;amp; 4 \end{bmatrix}$. What area scaling factor does it represent?&lt;/item&gt;
      &lt;item&gt;Find the determinant of the shear matrix $\begin{bmatrix} 1 &amp;amp; 2 \ 0 &amp;amp; 1 \end{bmatrix}$. What happens to the area of the unit square?&lt;/item&gt;
      &lt;item&gt;For the &lt;math-renderer&gt;$3 \times 3$&lt;/math-renderer&gt;matrix $\begin{bmatrix} 1 &amp;amp; 0 &amp;amp; 0 \ 0 &amp;amp; 2 &amp;amp; 0 \ 0 &amp;amp; 0 &amp;amp; 3 \end{bmatrix}$, compute the determinant. How does it scale volume in&lt;math-renderer&gt;$\mathbb{R}^3$&lt;/math-renderer&gt;?&lt;/item&gt;
      &lt;item&gt;Show that any rotation matrix in &lt;math-renderer&gt;$\mathbb{R}^2$&lt;/math-renderer&gt;has determinant&lt;math-renderer&gt;$1$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Give an example of a &lt;math-renderer&gt;$2 \times 2$&lt;/math-renderer&gt;matrix with determinant&lt;math-renderer&gt;$-1$&lt;/math-renderer&gt;. What geometric action does it represent?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Beyond their geometric meaning, determinants satisfy a collection of algebraic rules that make them powerful tools in linear algebra. These properties allow us to compute efficiently, test invertibility, and understand how determinants behave under matrix operations.&lt;/p&gt;
    &lt;p&gt;Let &lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Identity:&lt;/p&gt;
        &lt;p&gt;$$ \det(I_n) = 1. $$&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Triangular matrices: If&lt;/p&gt;&lt;math-renderer&gt;$A$&lt;/math-renderer&gt;is upper or lower triangular, then&lt;p&gt;$$ \det(A) = a_{11} a_{22} \cdots a_{nn}. $$&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Row/column swap: Interchanging two rows (or columns) multiplies the determinant by&lt;/p&gt;&lt;math-renderer&gt;$-1$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Row/column scaling: Multiplying a row (or column) by a scalar&lt;/p&gt;&lt;math-renderer&gt;$c$&lt;/math-renderer&gt;multiplies the determinant by&lt;math-renderer&gt;$c$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Row/column addition: Adding a multiple of one row to another does not change the determinant.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Transpose:&lt;/p&gt;
        &lt;p&gt;$$ \det(A^T) = \det(A). $$&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Multiplicativity:&lt;/p&gt;
        &lt;p&gt;$$ \det(AB) = \det(A)\det(B). $$&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Invertibility:&lt;/p&gt;&lt;math-renderer&gt;$A$&lt;/math-renderer&gt;is invertible if and only if&lt;math-renderer&gt;$\det(A) \neq 0$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example 6.2.1. For&lt;/p&gt;
    &lt;p&gt;Example 6.2.2. Let&lt;/p&gt;
    &lt;p&gt;Then&lt;/p&gt;
    &lt;p&gt;Since &lt;/p&gt;
    &lt;p&gt;This matches the multiplicativity rule: &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Row swaps: flipping orientation of space.&lt;/item&gt;
      &lt;item&gt;Scaling a row: stretching space in one direction.&lt;/item&gt;
      &lt;item&gt;Row replacement: sliding hyperplanes without altering volume.&lt;/item&gt;
      &lt;item&gt;Multiplicativity: performing two transformations multiplies their scaling factors.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These properties make determinants both computationally manageable and geometrically interpretable.&lt;/p&gt;
    &lt;p&gt;Determinant properties connect computation with geometry and theory. They explain why Gaussian elimination works, why invertibility is equivalent to nonzero determinant, and why determinants naturally arise in areas like volume computation, eigenvalue theory, and differential equations.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Compute the determinant of&lt;/p&gt;
        &lt;p&gt;$$ A = \begin{bmatrix} 1 &amp;amp; 2 &amp;amp; 3 \ 0 &amp;amp; 1 &amp;amp; 4 \ 0 &amp;amp; 0 &amp;amp; 2 \end{bmatrix}. $$&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Show that if two rows of a square matrix are identical, then its determinant is zero.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Verify&lt;/p&gt;&lt;math-renderer&gt;$\det(A^T) = \det(A)$&lt;/math-renderer&gt;for&lt;p&gt;$$ A = \begin{bmatrix} 2 &amp;amp; -1 \ 3 &amp;amp; 4 \end{bmatrix}. $$&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;If&lt;/p&gt;&lt;math-renderer&gt;$A$&lt;/math-renderer&gt;is invertible, prove that&lt;p&gt;$$ \det(A^{-1}) = \frac{1}{\det(A)}. $$&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Suppose&lt;/p&gt;&lt;math-renderer&gt;$A$&lt;/math-renderer&gt;is a&lt;math-renderer&gt;$3\times 3$&lt;/math-renderer&gt;matrix with&lt;math-renderer&gt;$\det(A) = 5$&lt;/math-renderer&gt;. What is&lt;math-renderer&gt;$\det(2A)$&lt;/math-renderer&gt;?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;While determinants of small matrices can be computed directly from formulas, larger matrices require a systematic method. The cofactor expansion (also known as Laplace expansion) provides a recursive way to compute determinants by breaking them into smaller ones.&lt;/p&gt;
    &lt;p&gt;For an &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The minor &lt;math-renderer&gt;$M_{ij}$&lt;/math-renderer&gt;is the determinant of the&lt;math-renderer&gt;$(n-1) \times (n-1)$&lt;/math-renderer&gt;matrix obtained by deleting the&lt;math-renderer&gt;$i$&lt;/math-renderer&gt;-th row and&lt;math-renderer&gt;$j$&lt;/math-renderer&gt;-th column of&lt;math-renderer&gt;$A$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;The cofactor &lt;math-renderer&gt;$C_{ij}$&lt;/math-renderer&gt;is defined by&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The sign factor &lt;/p&gt;
    &lt;p&gt;$$ \begin{bmatrix}&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&amp;amp; - &amp;amp; + &amp;amp; - &amp;amp; \cdots \&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&amp;amp; + &amp;amp; - &amp;amp; + &amp;amp; \cdots \&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&amp;amp; - &amp;amp; + &amp;amp; - &amp;amp; \cdots \ \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \ddots \end{bmatrix}. $$&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The determinant of &lt;/p&gt;
    &lt;p&gt;Example 6.3.1. Compute&lt;/p&gt;
    &lt;p&gt;Expand along the first row:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;For &lt;math-renderer&gt;$C_{11}$&lt;/math-renderer&gt;: $M_{11} = \det \begin{bmatrix} 4 &amp;amp; 5 \ 0 &amp;amp; 6 \end{bmatrix} = 24$, so&lt;math-renderer&gt;$C_{11} = (+1)(24) = 24$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;For &lt;math-renderer&gt;$C_{12}$&lt;/math-renderer&gt;: $M_{12} = \det \begin{bmatrix} 0 &amp;amp; 5 \ 1 &amp;amp; 6 \end{bmatrix} = 0 - 5 = -5$, so&lt;math-renderer&gt;$C_{12} = (-1)(-5) = 5$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;For &lt;math-renderer&gt;$C_{13}$&lt;/math-renderer&gt;: $M_{13} = \det \begin{bmatrix} 0 &amp;amp; 4 \ 1 &amp;amp; 0 \end{bmatrix} = 0 - 4 = -4$, so&lt;math-renderer&gt;$C_{13} = (+1)(-4) = -4$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Thus,&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Expansion along any row or column yields the same result.&lt;/item&gt;
      &lt;item&gt;The cofactor expansion provides a recursive definition of determinant: a determinant of size &lt;math-renderer&gt;$n$&lt;/math-renderer&gt;is expressed in terms of determinants of size&lt;math-renderer&gt;$n-1$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Cofactors are fundamental in constructing the adjugate matrix, which gives a formula for inverses:&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Cofactor expansion breaks down the determinant into contributions from sub-volumes defined by fixing one row or column at a time. Each cofactor measures how that row/column influences the overall volume scaling.&lt;/p&gt;
    &lt;p&gt;Cofactor expansion generalizes the small-matrix formulas and provides a conceptual definition of determinants. While not the most efficient way to compute determinants for large matrices, it is essential for theory, proofs, and connections to adjugates, Cramer’s rule, and classical geometry.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Compute the determinant of&lt;/p&gt;
        &lt;p&gt;$$ \begin{bmatrix} 2 &amp;amp; 0 &amp;amp; 1 \ 3 &amp;amp; -1 &amp;amp; 4 \ 1 &amp;amp; 2 &amp;amp; 0 \end{bmatrix} $$&lt;/p&gt;
        &lt;p&gt;by cofactor expansion along the first column.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Verify that expanding along the second row of Example 6.3.1 gives the same determinant.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Prove that expansion along any row gives the same value.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Show that if a row of a matrix is zero, then its determinant is zero.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Use cofactor expansion to prove that&lt;/p&gt;&lt;math-renderer&gt;$\det(A) = \det(A^T)$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Determinants are not merely algebraic curiosities; they have concrete geometric and computational uses. Two of the most important applications are measuring volumes and testing invertibility of matrices.&lt;/p&gt;
    &lt;p&gt;Given vectors &lt;/p&gt;
    &lt;p&gt;Then &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;In &lt;math-renderer&gt;$\mathbb{R}^2$&lt;/math-renderer&gt;,&lt;math-renderer&gt;$|\det(A)|$&lt;/math-renderer&gt;gives the area of the parallelogram spanned by&lt;math-renderer&gt;$\mathbf{v}_1, \mathbf{v}_2$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;In &lt;math-renderer&gt;$\mathbb{R}^3$&lt;/math-renderer&gt;,&lt;math-renderer&gt;$|\det(A)|$&lt;/math-renderer&gt;gives the volume of the parallelepiped spanned by&lt;math-renderer&gt;$\mathbf{v}_1, \mathbf{v}_2, \mathbf{v}_3$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;In higher dimensions, it generalizes to &lt;math-renderer&gt;$n$&lt;/math-renderer&gt;-dimensional volume (hypervolume).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example 6.4.1. Let&lt;/p&gt;
    &lt;p&gt;Then&lt;/p&gt;
    &lt;p&gt;So the parallelepiped has volume &lt;/p&gt;
    &lt;p&gt;A square matrix &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If &lt;math-renderer&gt;$\det(A) = 0$&lt;/math-renderer&gt;: the transformation collapses space into a lower dimension (area/volume is zero). No inverse exists.&lt;/item&gt;
      &lt;item&gt;If &lt;math-renderer&gt;$\det(A) \neq 0$&lt;/math-renderer&gt;: the transformation scales volume by&lt;math-renderer&gt;$|\det(A)|$&lt;/math-renderer&gt;, and is reversible.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example 6.4.2. The matrix&lt;/p&gt;
    &lt;p&gt;has determinant &lt;/p&gt;
    &lt;p&gt;Determinants also provide an explicit formula for solving systems of linear equations when the matrix is invertible. For &lt;/p&gt;
    &lt;p&gt;where &lt;/p&gt;
    &lt;p&gt;The sign of &lt;/p&gt;
    &lt;p&gt;Determinants condense key information: they measure scaling, test invertibility, and track orientation. These insights are indispensable in geometry (areas and volumes), analysis (Jacobian determinants in calculus), and computation ( solving systems and checking singularity).&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Compute the area of the parallelogram spanned by&lt;/p&gt;&lt;math-renderer&gt;$(2,1)$&lt;/math-renderer&gt;and&lt;math-renderer&gt;$(1,3)$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Find the volume of the parallelepiped spanned by&lt;/p&gt;&lt;math-renderer&gt;$(1,0,0), (1,1,0), (1,1,1)$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Determine whether the matrix $\begin{bmatrix} 1 &amp;amp; 2 \ 3 &amp;amp; 6 \end{bmatrix}$ is invertible. Justify using determinants.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Use Cramer’s rule to solve&lt;/p&gt;
        &lt;p&gt;$$ \begin{cases} x + y = 3, \ 2x - y = 0. \end{cases} $$&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Explain geometrically why a determinant of zero implies no inverse exists.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To extend the geometric ideas of length, distance, and angle beyond &lt;/p&gt;
    &lt;p&gt;An inner product on a real vector space &lt;/p&gt;
    &lt;p&gt;that assigns to each pair of vectors &lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Symmetry:&lt;/p&gt;
        &lt;math-renderer&gt;$\langle \mathbf{u}, \mathbf{v} \rangle = \langle \mathbf{v}, \mathbf{u} \rangle.$&lt;/math-renderer&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Linearity in the first argument:&lt;/p&gt;
        &lt;math-renderer&gt;$\langle a\mathbf{u} + b\mathbf{w}, \mathbf{v} \rangle = a \langle \mathbf{u}, \mathbf{v} \rangle + b \langle \mathbf{w}, \mathbf{v} \rangle.$&lt;/math-renderer&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Positive-definiteness:&lt;/p&gt;&lt;math-renderer&gt;$\langle \mathbf{v}, \mathbf{v} \rangle \geq 0$&lt;/math-renderer&gt;, and equality holds if and only if&lt;math-renderer&gt;$\mathbf{v} = \mathbf{0}$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The standard inner product on &lt;/p&gt;
    &lt;p&gt;The norm of a vector is its length, defined in terms of the inner product:&lt;/p&gt;
    &lt;p&gt;For the dot product in &lt;/p&gt;
    &lt;p&gt;The inner product allows us to define the angle &lt;/p&gt;
    &lt;p&gt;Thus, two vectors are orthogonal if &lt;/p&gt;
    &lt;p&gt;Example 7.1.1. In &lt;/p&gt;
    &lt;p&gt;So,&lt;/p&gt;
    &lt;p&gt;Example 7.1.2. In the function space &lt;/p&gt;
    &lt;p&gt;defines a length&lt;/p&gt;
    &lt;p&gt;This generalizes geometry to infinite-dimensional spaces.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Inner product: measures similarity between vectors.&lt;/item&gt;
      &lt;item&gt;Norm: length of a vector.&lt;/item&gt;
      &lt;item&gt;Angle: measure of alignment between two directions.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These concepts unify algebraic operations with geometric intuition.&lt;/p&gt;
    &lt;p&gt;Inner products and norms allow us to extend geometry into abstract vector spaces. They form the basis of orthogonality, projections, Fourier series, least squares approximation, and many applications in physics and machine learning.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Compute&lt;/p&gt;&lt;math-renderer&gt;$\langle (2,-1,3), (1,4,0) \rangle$&lt;/math-renderer&gt;. Then find the angle between them.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Show that&lt;/p&gt;&lt;math-renderer&gt;$|(x,y)| = \sqrt{x^2+y^2}$&lt;/math-renderer&gt;satisfies the properties of a norm.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;In&lt;/p&gt;&lt;math-renderer&gt;$\mathbb{R}^3$&lt;/math-renderer&gt;, verify that&lt;math-renderer&gt;$(1,1,0)$&lt;/math-renderer&gt;and&lt;math-renderer&gt;$(1,-1,0)$&lt;/math-renderer&gt;are orthogonal.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;In&lt;/p&gt;&lt;math-renderer&gt;$C[0,1]$&lt;/math-renderer&gt;, compute&lt;math-renderer&gt;$\langle f,g \rangle$&lt;/math-renderer&gt;for&lt;math-renderer&gt;$f(x)=x$&lt;/math-renderer&gt;,&lt;math-renderer&gt;$g(x)=1$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Prove the Cauchy–Schwarz inequality:&lt;/p&gt;
        &lt;p&gt;$$ |\langle \mathbf{u}, \mathbf{v} \rangle| \leq |\mathbf{u}| , |\mathbf{v}|. $$&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;One of the most useful applications of inner products is the notion of orthogonal projection. Projection allows us to approximate a vector by another lying in a subspace, minimizing error in the sense of distance. This idea underpins geometry, statistics, and numerical analysis.&lt;/p&gt;
    &lt;p&gt;Let &lt;/p&gt;
    &lt;p&gt;Given a vector &lt;/p&gt;
    &lt;p&gt;The formula is&lt;/p&gt;
    &lt;p&gt;The error vector &lt;/p&gt;
    &lt;p&gt;Let &lt;/p&gt;
    &lt;p&gt;So&lt;/p&gt;
    &lt;p&gt;The error vector is &lt;/p&gt;
    &lt;p&gt;Suppose &lt;/p&gt;
    &lt;p&gt;This is the unique vector in &lt;/p&gt;
    &lt;p&gt;Orthogonal projection explains the method of least squares. To solve an overdetermined system &lt;/p&gt;
    &lt;p&gt;Thus, least squares is just projection in disguise.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Projection finds the closest point in a subspace to a given vector.&lt;/item&gt;
      &lt;item&gt;It minimizes distance (error) in the sense of Euclidean norm.&lt;/item&gt;
      &lt;item&gt;Orthogonality ensures the error vector points directly away from the subspace.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Orthogonal projection is central in both pure and applied mathematics. It underlies the geometry of subspaces, the theory of Fourier series, regression in statistics, and approximation methods in numerical linear algebra. Whenever we fit data with a simpler model, projection is at work.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Compute the projection of &lt;math-renderer&gt;$(2,3)$&lt;/math-renderer&gt;onto the vector&lt;math-renderer&gt;$(1,1)$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Show that &lt;math-renderer&gt;$\mathbf{v} - \text{proj}_{\mathbf{u}}(\mathbf{v})$&lt;/math-renderer&gt;is orthogonal to&lt;math-renderer&gt;$\mathbf{u}$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Let &lt;math-renderer&gt;$W = \text{span}{(1,0,0), (0,1,0)} \subseteq \mathbb{R}^3$&lt;/math-renderer&gt;. Find the projection of&lt;math-renderer&gt;$(1,2,3)$&lt;/math-renderer&gt;onto&lt;math-renderer&gt;$W$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Explain why least squares fitting corresponds to projection onto the column space of &lt;math-renderer&gt;$A$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Prove that projection onto a subspace &lt;math-renderer&gt;$W$&lt;/math-renderer&gt;is unique: there is exactly one closest vector in&lt;math-renderer&gt;$W$&lt;/math-renderer&gt;to a given&lt;math-renderer&gt;$\mathbf{v}$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The Gram–Schmidt process is a systematic way to turn any linearly independent set of vectors into an orthonormal basis. This is especially useful because orthonormal bases simplify computations: inner products become simple coordinate comparisons, and projections take clean forms.&lt;/p&gt;
    &lt;p&gt;Given a linearly independent set of vectors &lt;/p&gt;
    &lt;p&gt;We proceed step by step:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Start with &lt;math-renderer&gt;$\mathbf{v}_1$&lt;/math-renderer&gt;, normalize it to get&lt;math-renderer&gt;$\mathbf{u}_1$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Subtract from &lt;math-renderer&gt;$\mathbf{v}_2$&lt;/math-renderer&gt;its projection onto&lt;math-renderer&gt;$\mathbf{u}_1$&lt;/math-renderer&gt;, leaving a vector orthogonal to&lt;math-renderer&gt;$\mathbf{u}_1$&lt;/math-renderer&gt;. Normalize to get&lt;math-renderer&gt;$\mathbf{u}_2$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;For each &lt;math-renderer&gt;$\mathbf{v}_k$&lt;/math-renderer&gt;, subtract projections onto all previously constructed $\mathbf{u}1, \dots, \mathbf{u}{k-1}$, then normalize.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For &lt;/p&gt;
    &lt;p&gt;$$ \mathbf{w}_k = \mathbf{v}k - \sum{j=1}^{k-1} \langle \mathbf{v}_k, \mathbf{u}_j \rangle \mathbf{u}_j, $$&lt;/p&gt;
    &lt;p&gt;The result &lt;/p&gt;
    &lt;p&gt;Take &lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Normalize &lt;math-renderer&gt;$\mathbf{v}_1$&lt;/math-renderer&gt;:&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Subtract projection of &lt;math-renderer&gt;$\mathbf{v}_2$&lt;/math-renderer&gt;on&lt;math-renderer&gt;$\mathbf{u}_1$&lt;/math-renderer&gt;:&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So&lt;/p&gt;
    &lt;p&gt;Normalize:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Subtract projections from &lt;math-renderer&gt;$\mathbf{v}_3$&lt;/math-renderer&gt;:&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;After computing, normalize to obtain &lt;/p&gt;
    &lt;p&gt;The result is an orthonormal basis of the span of &lt;/p&gt;
    &lt;p&gt;Gram–Schmidt is like straightening out a set of vectors: you start with the original directions and adjust each new vector to be perpendicular to all previous ones. Then you scale to unit length. The process ensures orthogonality while preserving the span.&lt;/p&gt;
    &lt;p&gt;Orthonormal bases simplify inner products, projections, and computations in general. They make coordinate systems easier to work with and are crucial in numerical methods, QR decomposition, Fourier analysis, and statistics (orthogonal polynomials, principal component analysis).&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Apply Gram–Schmidt to &lt;math-renderer&gt;$(1,0), (1,1)$&lt;/math-renderer&gt;in&lt;math-renderer&gt;$\mathbb{R}^2$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Orthogonalize &lt;math-renderer&gt;$(1,1,1), (1,0,1)$&lt;/math-renderer&gt;in&lt;math-renderer&gt;$\mathbb{R}^3$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Prove that each step of Gram–Schmidt yields a vector orthogonal to all previous ones.&lt;/item&gt;
      &lt;item&gt;Show that Gram–Schmidt preserves the span of the original vectors.&lt;/item&gt;
      &lt;item&gt;Explain how Gram–Schmidt leads to the QR decomposition of a matrix.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;An orthonormal basis is a basis of a vector space in which all vectors are both orthogonal to each other and have unit length. Such bases are the most convenient possible coordinate systems: computations involving inner products, projections, and norms become exceptionally simple.&lt;/p&gt;
    &lt;p&gt;A set of vectors &lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;math-renderer&gt;$\langle \mathbf{u}_i, \mathbf{u}_j \rangle = 0$&lt;/math-renderer&gt;whenever&lt;math-renderer&gt;$i \neq j$&lt;/math-renderer&gt;(orthogonality),&lt;/item&gt;
      &lt;item&gt;&lt;math-renderer&gt;$|\mathbf{u}_i| = 1$&lt;/math-renderer&gt;for all&lt;math-renderer&gt;$i$&lt;/math-renderer&gt;(normalization),&lt;/item&gt;
      &lt;item&gt;The set spans &lt;math-renderer&gt;$V$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example 7.4.1. In &lt;/p&gt;
    &lt;p&gt;is orthonormal under the dot product.&lt;/p&gt;
    &lt;p&gt;Example 7.4.2. In &lt;/p&gt;
    &lt;p&gt;is orthonormal.&lt;/p&gt;
    &lt;p&gt;Example 7.4.3. Fourier basis on functions:&lt;/p&gt;
    &lt;p&gt;is an orthogonal set in the space of square-integrable functions on &lt;/p&gt;
    &lt;p&gt;After normalization, it becomes an orthonormal basis.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Coordinate simplicity: If&lt;/p&gt;&lt;math-renderer&gt;${\mathbf{u}_1,\dots,\mathbf{u}_n}$&lt;/math-renderer&gt;is an orthonormal basis of&lt;math-renderer&gt;$V$&lt;/math-renderer&gt;, then any vector&lt;math-renderer&gt;$\mathbf{v}\in V$&lt;/math-renderer&gt;has coordinates&lt;p&gt;$$ [\mathbf{v}] = \begin{bmatrix} \langle \mathbf{v}, \mathbf{u}_1 \rangle \ \vdots \ \langle \mathbf{v}, \mathbf{u}_n \rangle \end{bmatrix}. $$&lt;/p&gt;&lt;p&gt;That is, coordinates are just inner products.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Parseval’s identity: For any&lt;/p&gt;&lt;math-renderer&gt;$\mathbf{v} \in V$&lt;/math-renderer&gt;,&lt;p&gt;$$ |\mathbf{v}|^2 = \sum_{i=1}^n |\langle \mathbf{v}, \mathbf{u}_i \rangle|^2. $$&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Projections: The orthogonal projection onto the span of&lt;/p&gt;&lt;math-renderer&gt;${\mathbf{u}_1,\dots,\mathbf{u}_k}$&lt;/math-renderer&gt;is&lt;p&gt;$$ \text{proj}(\mathbf{v}) = \sum_{i=1}^k \langle \mathbf{v}, \mathbf{u}_i \rangle \mathbf{u}_i. $$&lt;/p&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Start with any linearly independent set, then apply the Gram–Schmidt process to obtain an orthonormal set spanning the same subspace.&lt;/item&gt;
      &lt;item&gt;In practice, orthonormal bases are often chosen for numerical stability and simplicity of computation.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;An orthonormal basis is like a perfectly aligned and equally scaled coordinate system. Distances and angles are computed directly using coordinates without correction factors. They are the ideal rulers of linear algebra.&lt;/p&gt;
    &lt;p&gt;Orthonormal bases simplify every aspect of linear algebra: solving systems, computing projections, expanding functions, diagonalizing symmetric matrices, and working with Fourier series. In data science, principal component analysis produces orthonormal directions capturing maximum variance.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Verify that &lt;math-renderer&gt;$(1/\sqrt{2})(1,1)$&lt;/math-renderer&gt;and&lt;math-renderer&gt;$(1/\sqrt{2})(1,-1)$&lt;/math-renderer&gt;form an orthonormal basis of&lt;math-renderer&gt;$\mathbb{R}^2$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Express &lt;math-renderer&gt;$(3,4)$&lt;/math-renderer&gt;in terms of the orthonormal basis&lt;math-renderer&gt;${(1/\sqrt{2})(1,1), (1/\sqrt{2})(1,-1)}$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Prove Parseval’s identity for &lt;math-renderer&gt;$\mathbb{R}^n$&lt;/math-renderer&gt;with the dot product.&lt;/item&gt;
      &lt;item&gt;Find an orthonormal basis for the plane &lt;math-renderer&gt;$x+y+z=0$&lt;/math-renderer&gt;in&lt;math-renderer&gt;$\mathbb{R}^3$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Explain why orthonormal bases are numerically more stable than arbitrary bases in computations.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The concepts of eigenvalues and eigenvectors reveal the most fundamental behavior of linear transformations. They identify the special directions in which a transformation acts by simple stretching or compressing, without rotation or distortion.&lt;/p&gt;
    &lt;p&gt;Let &lt;/p&gt;
    &lt;p&gt;for some scalar &lt;/p&gt;
    &lt;p&gt;Equivalently, if &lt;/p&gt;
    &lt;p&gt;Example 8.1.1. Let&lt;/p&gt;
    &lt;p&gt;Then&lt;/p&gt;
    &lt;p&gt;So &lt;/p&gt;
    &lt;p&gt;Example 8.1.2. Rotation matrix in &lt;/p&gt;
    &lt;p&gt;If &lt;/p&gt;
    &lt;p&gt;Eigenvalues arise from solving the characteristic equation:&lt;/p&gt;
    &lt;p&gt;This polynomial in &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Eigenvectors are directions that remain unchanged in orientation under a transformation; only their length is scaled.&lt;/item&gt;
      &lt;item&gt;Eigenvalues tell us the scaling factor along those directions.&lt;/item&gt;
      &lt;item&gt;If a matrix has many independent eigenvectors, it can often be simplified (diagonalized) by changing basis.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Stretching along principal axes of an ellipse (quadratic forms).&lt;/item&gt;
      &lt;item&gt;Stable directions of dynamical systems.&lt;/item&gt;
      &lt;item&gt;Principal components in statistics and machine learning.&lt;/item&gt;
      &lt;item&gt;Quantum mechanics, where observables correspond to operators with eigenvalues.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Eigenvalues and eigenvectors are a bridge between algebra and geometry. They provide a lens for understanding linear transformations in their simplest form. Nearly every application of linear algebra-differential equations, statistics, physics, computer science-relies on eigen-analysis.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Find the eigenvalues and eigenvectors of $\begin{bmatrix} 4 &amp;amp; 0 \ 0 &amp;amp; -1 \end{bmatrix}$.&lt;/item&gt;
      &lt;item&gt;Show that every scalar multiple of an eigenvector is again an eigenvector for the same eigenvalue.&lt;/item&gt;
      &lt;item&gt;Verify that the rotation matrix &lt;math-renderer&gt;$R_\theta$&lt;/math-renderer&gt;has no real eigenvalues unless&lt;math-renderer&gt;$\theta = 0$&lt;/math-renderer&gt;or&lt;math-renderer&gt;$\pi$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Compute the characteristic polynomial of $\begin{bmatrix} 1 &amp;amp; 2 \ 2 &amp;amp; 1 \end{bmatrix}$.&lt;/item&gt;
      &lt;item&gt;Explain geometrically what eigenvectors and eigenvalues represent for the shear matrix $\begin{bmatrix} 1 &amp;amp; 1 \ 0 &amp;amp; 1 \end{bmatrix}$.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A central goal in linear algebra is to simplify the action of a matrix by choosing a good basis. Diagonalization is the process of rewriting a matrix so that it acts by simple scaling along independent directions. This makes computations such as powers, exponentials, and solving differential equations far easier.&lt;/p&gt;
    &lt;p&gt;A square matrix &lt;/p&gt;
    &lt;p&gt;where &lt;/p&gt;
    &lt;p&gt;The diagonal entries of &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A matrix is diagonalizable if it has &lt;math-renderer&gt;$n$&lt;/math-renderer&gt;linearly independent eigenvectors.&lt;/item&gt;
      &lt;item&gt;Equivalently, the sum of the dimensions of its eigenspaces equals &lt;math-renderer&gt;$n$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Symmetric matrices (over &lt;math-renderer&gt;$\mathbb{R}$&lt;/math-renderer&gt;) are always diagonalizable, with an orthonormal basis of eigenvectors.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Let&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Characteristic polynomial:&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So eigenvalues are &lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Eigenvectors:&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;For &lt;math-renderer&gt;$\lambda = 4$&lt;/math-renderer&gt;, solve&lt;math-renderer&gt;$(A-4I)\mathbf{v}=0$&lt;/math-renderer&gt;: $\begin{bmatrix} 0 &amp;amp; 1 \ 0 &amp;amp; -2 \end{bmatrix}\mathbf{v} = 0$, giving&lt;math-renderer&gt;$\mathbf{v}_1 = (1,0)$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;For &lt;math-renderer&gt;$\lambda = 2$&lt;/math-renderer&gt;:&lt;math-renderer&gt;$(A-2I)\mathbf{v}=0$&lt;/math-renderer&gt;, giving&lt;math-renderer&gt;$\mathbf{v}_2 = (1,-2)$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Construct $P = \begin{bmatrix} 1 &amp;amp; 1 \ 0 &amp;amp; -2 \end{bmatrix}$. Then&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Thus, &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Computing powers: If&lt;/p&gt;&lt;math-renderer&gt;$A = P D P^{-1}$&lt;/math-renderer&gt;, then&lt;p&gt;$$ A^k = P D^k P^{-1}. $$&lt;/p&gt;&lt;p&gt;Since&lt;/p&gt;&lt;math-renderer&gt;$D$&lt;/math-renderer&gt;is diagonal,&lt;math-renderer&gt;$D^k$&lt;/math-renderer&gt;is easy to compute.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Matrix exponentials:&lt;/p&gt;&lt;math-renderer&gt;$e^A = P e^D P^{-1}$&lt;/math-renderer&gt;, useful in solving differential equations.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Understanding geometry: Diagonalization reveals the directions along which a transformation stretches or compresses space independently.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Not all matrices can be diagonalized.&lt;/p&gt;
    &lt;p&gt;has only one eigenvalue &lt;/p&gt;
    &lt;p&gt;Diagonalization means we have found a basis of eigenvectors. In this basis, the matrix acts by simple scaling along each coordinate axis. It transforms complicated motion into independent 1D motions.&lt;/p&gt;
    &lt;p&gt;Diagonalization is a cornerstone of linear algebra. It simplifies computation, reveals structure, and is the starting point for the spectral theorem, Jordan form, and many applications in physics, engineering, and data science.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Diagonalize&lt;/p&gt;
        &lt;p&gt;$$ A = \begin{bmatrix} 2 &amp;amp; 0 \ 0 &amp;amp; 3 \end{bmatrix}. $$&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Determine whether&lt;/p&gt;
        &lt;p&gt;$$ A = \begin{bmatrix} 1 &amp;amp; 1 \ 0 &amp;amp; 1 \end{bmatrix} $$&lt;/p&gt;
        &lt;p&gt;is diagonalizable. Why or why not?&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Find&lt;/p&gt;&lt;math-renderer&gt;$A^5$&lt;/math-renderer&gt;for&lt;p&gt;$$ A = \begin{bmatrix} 4 &amp;amp; 1 \ 0 &amp;amp; 2 \end{bmatrix} $$&lt;/p&gt;&lt;p&gt;using diagonalization.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Show that any&lt;/p&gt;&lt;math-renderer&gt;$n \times n$&lt;/math-renderer&gt;matrix with&lt;math-renderer&gt;$n$&lt;/math-renderer&gt;distinct eigenvalues is diagonalizable.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Explain why real symmetric matrices are always diagonalizable.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The key to finding eigenvalues is the characteristic polynomial of a matrix. This polynomial encodes the values of &lt;/p&gt;
    &lt;p&gt;For an &lt;/p&gt;
    &lt;p&gt;The roots of &lt;/p&gt;
    &lt;p&gt;Example 8.3.1. Let&lt;/p&gt;
    &lt;p&gt;Then&lt;/p&gt;
    &lt;p&gt;Thus eigenvalues are &lt;/p&gt;
    &lt;p&gt;Example 8.3.2. For&lt;/p&gt;
    &lt;p&gt;(rotation by 90°),&lt;/p&gt;
    &lt;p&gt;Eigenvalues are &lt;/p&gt;
    &lt;p&gt;Example 8.3.3. For a triangular matrix&lt;/p&gt;
    &lt;p&gt;the determinant is simply the product of diagonal entries minus &lt;/p&gt;
    &lt;p&gt;So eigenvalues are &lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;The characteristic polynomial of an&lt;/p&gt;&lt;math-renderer&gt;$n \times n$&lt;/math-renderer&gt;matrix has degree&lt;math-renderer&gt;$n$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The sum of the eigenvalues (counted with multiplicity) equals the trace of&lt;/p&gt;&lt;math-renderer&gt;$A$&lt;/math-renderer&gt;:&lt;p&gt;$$ \text{tr}(A) = \lambda_1 + \cdots + \lambda_n. $$&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The product of the eigenvalues equals the determinant of&lt;/p&gt;&lt;math-renderer&gt;$A$&lt;/math-renderer&gt;:&lt;p&gt;$$ \det(A) = \lambda_1 \cdots \lambda_n. $$&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Similar matrices have the same characteristic polynomial, hence the same eigenvalues.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The characteristic polynomial captures when &lt;/p&gt;
    &lt;p&gt;Characteristic polynomials provide the computational tool to extract eigenvalues. They connect matrix invariants (trace and determinant) with geometry, and form the foundation for diagonalization, spectral theorems, and stability analysis in dynamical systems.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Compute the characteristic polynomial of&lt;/p&gt;
        &lt;p&gt;$$ A = \begin{bmatrix} 4 &amp;amp; 2 \ 1 &amp;amp; 3 \end{bmatrix}. $$&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Verify that the sum of the eigenvalues of $\begin{bmatrix} 5 &amp;amp; 0 \ 0 &amp;amp; -2 \end{bmatrix}$ equals its trace, and their product equals its determinant.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Show that for any triangular matrix, the eigenvalues are just the diagonal entries.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Prove that if&lt;/p&gt;&lt;math-renderer&gt;$A$&lt;/math-renderer&gt;and&lt;math-renderer&gt;$B$&lt;/math-renderer&gt;are similar matrices, then&lt;math-renderer&gt;$p_A(\lambda) = p_B(\lambda)$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Compute the characteristic polynomial of $\begin{bmatrix} 1 &amp;amp; 1 &amp;amp; 0 \ 0 &amp;amp; 1 &amp;amp; 1 \ 0 &amp;amp; 0 &amp;amp; 1 \end{bmatrix}$.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Eigenvalues and eigenvectors are not only central to the theory of linear algebra-they are indispensable tools across mathematics and applied science. Two classic applications are solving systems of differential equations and analyzing Markov chains.&lt;/p&gt;
    &lt;p&gt;Consider the system&lt;/p&gt;
    &lt;p&gt;where &lt;/p&gt;
    &lt;p&gt;If &lt;/p&gt;
    &lt;p&gt;is a solution.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Eigenvalues determine the growth or decay rate:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;If &lt;math-renderer&gt;$\lambda &amp;amp;lt; 0$&lt;/math-renderer&gt;, solutions decay (stable).&lt;/item&gt;
          &lt;item&gt;If &lt;math-renderer&gt;$\lambda &amp;amp;gt; 0$&lt;/math-renderer&gt;, solutions grow (unstable).&lt;/item&gt;
          &lt;item&gt;If &lt;math-renderer&gt;$\lambda$&lt;/math-renderer&gt;is complex, oscillations occur.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;If &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;By combining eigenvector solutions, we can solve general initial conditions.&lt;/p&gt;
    &lt;p&gt;Example 8.4.1. Let&lt;/p&gt;
    &lt;p&gt;Then eigenvalues are &lt;/p&gt;
    &lt;p&gt;Thus one component grows exponentially, the other decays.&lt;/p&gt;
    &lt;p&gt;A Markov chain is described by a stochastic matrix &lt;/p&gt;
    &lt;p&gt;Iterating gives&lt;/p&gt;
    &lt;p&gt;Understanding long-term behavior reduces to analyzing powers of &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The eigenvalue &lt;math-renderer&gt;$\lambda = 1$&lt;/math-renderer&gt;always exists. Its eigenvector gives the steady-state distribution.&lt;/item&gt;
      &lt;item&gt;All other eigenvalues satisfy &lt;math-renderer&gt;$|\lambda| \leq 1$&lt;/math-renderer&gt;. Their influence decays as&lt;math-renderer&gt;$k \to \infty$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example 8.4.2. Consider&lt;/p&gt;
    &lt;p&gt;Eigenvalues are &lt;/p&gt;
    &lt;p&gt;Thus, regardless of the starting distribution, the chain converges to &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;In differential equations, eigenvalues determine the time evolution: exponential growth, decay, or oscillation.&lt;/item&gt;
      &lt;item&gt;In Markov chains, eigenvalues determine the long-term equilibrium of stochastic processes.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Eigenvalue methods turn complex iterative or dynamical systems into tractable problems. In physics, engineering, and finance, they describe stability and resonance. In computer science and statistics, they power algorithms from Google’s PageRank to modern machine learning.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Solve $\tfrac{d}{dt}\mathbf{x} = \begin{bmatrix} 3 &amp;amp; 0 \ 0 &amp;amp; -2 \end{bmatrix}\mathbf{x}$.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Show that if&lt;/p&gt;&lt;math-renderer&gt;$A$&lt;/math-renderer&gt;has a complex eigenvalue&lt;math-renderer&gt;$\alpha \pm i\beta$&lt;/math-renderer&gt;, then solutions of&lt;math-renderer&gt;$\tfrac{d}{dt}\mathbf{x} = A\mathbf{x}$&lt;/math-renderer&gt;involve oscillations of frequency&lt;math-renderer&gt;$\beta$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Find the steady-state distribution of&lt;/p&gt;
        &lt;p&gt;$$ P = \begin{bmatrix} 0.7 &amp;amp; 0.2 \ 0.3 &amp;amp; 0.8 \end{bmatrix}. $$&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Prove that for any stochastic matrix&lt;/p&gt;&lt;math-renderer&gt;$P$&lt;/math-renderer&gt;,&lt;math-renderer&gt;$1$&lt;/math-renderer&gt;is always an eigenvalue.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Explain why all eigenvalues of a stochastic matrix satisfy&lt;/p&gt;&lt;math-renderer&gt;$|\lambda| \leq 1$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A quadratic form is a polynomial of degree two in several variables, expressed neatly using matrices. Quadratic forms appear throughout mathematics: in optimization, geometry of conic sections, statistics (variance), and physics (energy functions).&lt;/p&gt;
    &lt;p&gt;Let &lt;/p&gt;
    &lt;p&gt;Expanded,&lt;/p&gt;
    &lt;p&gt;Because &lt;/p&gt;
    &lt;p&gt;Example 9.1.1. For&lt;/p&gt;
    &lt;p&gt;Example 9.1.2. The quadratic form&lt;/p&gt;
    &lt;p&gt;corresponds to the matrix &lt;/p&gt;
    &lt;p&gt;Example 9.1.3. The conic section equation&lt;/p&gt;
    &lt;p&gt;is described by the quadratic form &lt;/p&gt;
    &lt;p&gt;By choosing a new basis consisting of eigenvectors of &lt;/p&gt;
    &lt;p&gt;Thus quadratic forms can always be expressed as a sum of weighted squares:&lt;/p&gt;
    &lt;p&gt;where &lt;/p&gt;
    &lt;p&gt;Quadratic forms describe geometric shapes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;In 2D: ellipses, parabolas, hyperbolas.&lt;/item&gt;
      &lt;item&gt;In 3D: ellipsoids, paraboloids, hyperboloids.&lt;/item&gt;
      &lt;item&gt;In higher dimensions: generalizations of ellipsoids.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Diagonalization aligns the coordinate axes with the principal axes of the shape.&lt;/p&gt;
    &lt;p&gt;Quadratic forms unify geometry and algebra. They are central in optimization (minimizing energy functions), statistics ( covariance matrices and variance), mechanics (kinetic energy), and numerical analysis. Understanding quadratic forms leads directly to the spectral theorem.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Write the quadratic form &lt;math-renderer&gt;$Q(x,y) = 3x^2 + 4xy + y^2$&lt;/math-renderer&gt;as&lt;math-renderer&gt;$\mathbf{x}^T A \mathbf{x}$&lt;/math-renderer&gt;for some symmetric matrix&lt;math-renderer&gt;$A$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;For $A = \begin{bmatrix} 1 &amp;amp; 2 \ 2 &amp;amp; 1 \end{bmatrix}$, compute &lt;math-renderer&gt;$Q(x,y)$&lt;/math-renderer&gt;explicitly.&lt;/item&gt;
      &lt;item&gt;Diagonalize the quadratic form &lt;math-renderer&gt;$Q(x,y) = 2x^2 + 2xy + 3y^2$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Identify the conic section given by &lt;math-renderer&gt;$Q(x,y) = x^2 - y^2$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Show that if &lt;math-renderer&gt;$A$&lt;/math-renderer&gt;is symmetric, quadratic forms defined by&lt;math-renderer&gt;$A$&lt;/math-renderer&gt;and&lt;math-renderer&gt;$A^T$&lt;/math-renderer&gt;are identical.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Quadratic forms are especially important when their associated matrices are positive definite, since these guarantee positivity of energy, distance, or variance. Positive definiteness is a cornerstone in optimization, numerical analysis, and statistics.&lt;/p&gt;
    &lt;p&gt;A symmetric matrix &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Positive definite if&lt;/p&gt;
        &lt;p&gt;$$ \mathbf{x}^T A \mathbf{x} &amp;gt; 0 \quad \text{for all nonzero } \mathbf{x} \in \mathbb{R}^n. $$&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Positive semidefinite if&lt;/p&gt;
        &lt;p&gt;$$ \mathbf{x}^T A \mathbf{x} \geq 0 \quad \text{for all } \mathbf{x}. $$&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Similarly, negative definite (always &amp;lt; 0) and indefinite (can be both &amp;lt; 0 and &amp;gt; 0) matrices are defined.&lt;/p&gt;
    &lt;p&gt;Example 9.2.1.&lt;/p&gt;
    &lt;p&gt;is positive definite, since&lt;/p&gt;
    &lt;p&gt;for all &lt;/p&gt;
    &lt;p&gt;Example 9.2.2.&lt;/p&gt;
    &lt;p&gt;has quadratic form&lt;/p&gt;
    &lt;p&gt;This matrix is not positive definite, since &lt;/p&gt;
    &lt;p&gt;For a symmetric matrix &lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Eigenvalue test:&lt;/p&gt;&lt;math-renderer&gt;$A$&lt;/math-renderer&gt;is positive definite if and only if all eigenvalues of&lt;math-renderer&gt;$A$&lt;/math-renderer&gt;are positive.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Principal minors test (Sylvester’s criterion):&lt;/p&gt;&lt;math-renderer&gt;$A$&lt;/math-renderer&gt;is positive definite if and only if all leading principal minors ( determinants of top-left&lt;math-renderer&gt;$k \times k$&lt;/math-renderer&gt;submatrices) are positive.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Cholesky factorization:&lt;/p&gt;&lt;math-renderer&gt;$A$&lt;/math-renderer&gt;is positive definite if and only if it can be written as&lt;p&gt;$$ A = R^T R, $$&lt;/p&gt;&lt;p&gt;where&lt;/p&gt;&lt;math-renderer&gt;$R$&lt;/math-renderer&gt;is an upper triangular matrix with positive diagonal entries.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Positive definite matrices correspond to quadratic forms that define ellipsoids centered at the origin.&lt;/item&gt;
      &lt;item&gt;Positive semidefinite matrices define flattened ellipsoids (possibly degenerate).&lt;/item&gt;
      &lt;item&gt;Indefinite matrices define hyperbolas or saddle-shaped surfaces.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Optimization: Hessians of convex functions are positive semidefinite; strict convexity corresponds to positive definite Hessians.&lt;/item&gt;
      &lt;item&gt;Statistics: Covariance matrices are positive semidefinite.&lt;/item&gt;
      &lt;item&gt;Numerical methods: Cholesky decomposition is widely used to solve systems with positive definite matrices efficiently.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Positive definiteness provides stability and guarantees in mathematics and computation. It ensures energy functions are bounded below, optimization problems have unique solutions, and statistical models are meaningful.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Use Sylvester’s criterion to check whether&lt;/p&gt;
        &lt;p&gt;$$ A = \begin{bmatrix} 2 &amp;amp; -1 \ -1 &amp;amp; 2 \end{bmatrix} $$&lt;/p&gt;
        &lt;p&gt;is positive definite.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Determine whether&lt;/p&gt;
        &lt;p&gt;$$ A = \begin{bmatrix} 0 &amp;amp; 1 \ 1 &amp;amp; 0 \end{bmatrix} $$&lt;/p&gt;
        &lt;p&gt;is positive definite, semidefinite, or indefinite.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Find the eigenvalues of&lt;/p&gt;
        &lt;p&gt;$$ A = \begin{bmatrix} 4 &amp;amp; 2 \ 2 &amp;amp; 3 \end{bmatrix}, $$&lt;/p&gt;
        &lt;p&gt;and use them to classify definiteness.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Prove that all diagonal matrices with positive entries are positive definite.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Show that if&lt;/p&gt;&lt;math-renderer&gt;$A$&lt;/math-renderer&gt;is positive definite, then so is&lt;math-renderer&gt;$P^T A P$&lt;/math-renderer&gt;for any invertible matrix&lt;math-renderer&gt;$P$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The spectral theorem is one of the most powerful results in linear algebra. It states that symmetric matrices can always be diagonalized by an orthogonal basis of eigenvectors. This links algebra (eigenvalues), geometry (orthogonal directions), and applications (stability, optimization, statistics).&lt;/p&gt;
    &lt;p&gt;If &lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;All eigenvalues of&lt;/p&gt;&lt;math-renderer&gt;$A$&lt;/math-renderer&gt;are real.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;There exists an orthonormal basis of&lt;/p&gt;&lt;math-renderer&gt;$\mathbb{R}^n$&lt;/math-renderer&gt;consisting of eigenvectors of&lt;math-renderer&gt;$A$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Thus,&lt;/p&gt;&lt;math-renderer&gt;$A$&lt;/math-renderer&gt;can be written as&lt;p&gt;$$ A = Q \Lambda Q^T, $$&lt;/p&gt;&lt;p&gt;where&lt;/p&gt;&lt;math-renderer&gt;$Q$&lt;/math-renderer&gt;is an orthogonal matrix (&lt;math-renderer&gt;$Q^T Q = I$&lt;/math-renderer&gt;) and&lt;math-renderer&gt;$\Lambda$&lt;/math-renderer&gt;is diagonal with eigenvalues of&lt;math-renderer&gt;$A$&lt;/math-renderer&gt;on the diagonal.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Symmetric matrices are always diagonalizable, and the diagonalization is numerically stable.&lt;/item&gt;
      &lt;item&gt;Quadratic forms &lt;math-renderer&gt;$\mathbf{x}^T A \mathbf{x}$&lt;/math-renderer&gt;can be expressed in terms of eigenvalues and eigenvectors, showing ellipsoids aligned with eigen-directions.&lt;/item&gt;
      &lt;item&gt;Positive definiteness can be checked by confirming that all eigenvalues are positive.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Let&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Characteristic polynomial:&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Eigenvalues: &lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Eigenvectors:&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;For &lt;math-renderer&gt;$\lambda=1$&lt;/math-renderer&gt;: solve&lt;math-renderer&gt;$(A-I)\mathbf{v} = 0$&lt;/math-renderer&gt;, giving&lt;math-renderer&gt;$(1,-1)$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;For &lt;math-renderer&gt;$\lambda=3$&lt;/math-renderer&gt;: solve&lt;math-renderer&gt;$(A-3I)\mathbf{v} = 0$&lt;/math-renderer&gt;, giving&lt;math-renderer&gt;$(1,1)$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Normalize eigenvectors:&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Then&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So&lt;/p&gt;
    &lt;p&gt;The spectral theorem says every symmetric matrix acts like independent scaling along orthogonal directions. In geometry, this corresponds to stretching space along perpendicular axes.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Ellipses, ellipsoids, and quadratic surfaces can be fully understood via eigenvalues and eigenvectors.&lt;/item&gt;
      &lt;item&gt;Orthogonality ensures directions remain perpendicular after transformation.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Optimization: The spectral theorem underlies classification of critical points via eigenvalues of the Hessian.&lt;/item&gt;
      &lt;item&gt;PCA (Principal Component Analysis): Data covariance matrices are symmetric, and PCA finds orthogonal directions of maximum variance.&lt;/item&gt;
      &lt;item&gt;Differential equations &amp;amp; physics: Symmetric operators correspond to measurable quantities with real eigenvalues ( stability, energy).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The spectral theorem guarantees that symmetric matrices are as simple as possible: they can always be analyzed in terms of real, orthogonal eigenvectors. This provides both deep theoretical insight and powerful computational tools.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Diagonalize&lt;/p&gt;
        &lt;p&gt;$$ A = \begin{bmatrix} 4 &amp;amp; 2 \ 2 &amp;amp; 3 \end{bmatrix} $$&lt;/p&gt;
        &lt;p&gt;using the spectral theorem.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Prove that all eigenvalues of a real symmetric matrix are real.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Show that eigenvectors corresponding to distinct eigenvalues of a symmetric matrix are orthogonal.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Explain geometrically how the spectral theorem describes ellipsoids defined by quadratic forms.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Apply the spectral theorem to the covariance matrix&lt;/p&gt;
        &lt;p&gt;$$ \Sigma = \begin{bmatrix} 2 &amp;amp; 1 \ 1 &amp;amp; 2 \end{bmatrix}, $$&lt;/p&gt;
        &lt;p&gt;and interpret the eigenvectors as principal directions of variance.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Principal Component Analysis (PCA) is a widely used technique in data science, machine learning, and statistics. At its core, PCA is an application of the spectral theorem to covariance matrices: it finds orthogonal directions (principal components) that capture the maximum variance in data.&lt;/p&gt;
    &lt;p&gt;Given a dataset of vectors &lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Center the data by subtracting the mean vector&lt;/p&gt;&lt;math-renderer&gt;$\bar{\mathbf{x}}$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Form the covariance matrix&lt;/p&gt;
        &lt;p&gt;$$ \Sigma = \frac{1}{m} \sum_{i=1}^m (\mathbf{x}_i - \bar{\mathbf{x}})(\mathbf{x}_i - \bar{\mathbf{x}})^T. $$&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Apply the spectral theorem:&lt;/p&gt;&lt;math-renderer&gt;$\Sigma = Q \Lambda Q^T$&lt;/math-renderer&gt;.&lt;list rend="ul"&gt;&lt;item&gt;Columns of &lt;math-renderer&gt;$Q$&lt;/math-renderer&gt;are orthonormal eigenvectors (principal directions).&lt;/item&gt;&lt;item&gt;Eigenvalues in &lt;math-renderer&gt;$\Lambda$&lt;/math-renderer&gt;measure variance explained by each direction.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Columns of &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The first principal component is the eigenvector corresponding to the largest eigenvalue; it is the direction of maximum variance.&lt;/p&gt;
    &lt;p&gt;Suppose we have two-dimensional data points roughly aligned along the line &lt;/p&gt;
    &lt;p&gt;Eigenvalues are about &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;First principal component: the line &lt;math-renderer&gt;$y = x$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Most variance lies along this direction.&lt;/item&gt;
      &lt;item&gt;Second component is nearly orthogonal (&lt;math-renderer&gt;$y = -x$&lt;/math-renderer&gt;), but variance there is tiny.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Thus PCA reduces the data to essentially one dimension.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Dimensionality reduction: Represent data with fewer features while retaining most variance.&lt;/item&gt;
      &lt;item&gt;Noise reduction: Small eigenvalues correspond to noise; discarding them filters data.&lt;/item&gt;
      &lt;item&gt;Visualization: Projecting high-dimensional data onto top 2 or 3 principal components reveals structure.&lt;/item&gt;
      &lt;item&gt;Compression: PCA is used in image and signal compression.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The covariance matrix &lt;/p&gt;
    &lt;p&gt;PCA demonstrates how abstract linear algebra directly powers modern applications. Eigenvalues and eigenvectors give a practical method for simplifying data, revealing patterns, and reducing complexity. It is one of the most important algorithms derived from the spectral theorem.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Show that the covariance matrix is symmetric and positive semidefinite.&lt;/item&gt;
      &lt;item&gt;Compute the covariance matrix of the dataset &lt;math-renderer&gt;$(1,2), (2,3), (3,4)$&lt;/math-renderer&gt;, and find its eigenvalues and eigenvectors.&lt;/item&gt;
      &lt;item&gt;Explain why the first principal component captures the maximum variance.&lt;/item&gt;
      &lt;item&gt;In image compression, explain how PCA can reduce storage by keeping only the top &lt;math-renderer&gt;$k$&lt;/math-renderer&gt;principal components.&lt;/item&gt;
      &lt;item&gt;Prove that the sum of the eigenvalues of the covariance matrix equals the total variance of the dataset.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Linear algebra is the language of modern computer graphics. Every image rendered on a screen, every 3D model rotated or projected, is ultimately the result of applying matrices to vectors. Rotations, reflections, scalings, and projections are all linear transformations, making matrices the natural tool for manipulating geometry.&lt;/p&gt;
    &lt;p&gt;A counterclockwise rotation by an angle &lt;/p&gt;
    &lt;p&gt;For any vector &lt;/p&gt;
    &lt;p&gt;This preserves lengths and angles, since &lt;/p&gt;
    &lt;p&gt;In three dimensions, rotations are represented by &lt;/p&gt;
    &lt;p&gt;Similar formulas exist for rotations about the &lt;/p&gt;
    &lt;p&gt;More general 3D rotations can be described by axis–angle representation or quaternions, but the underlying idea is still linear transformations represented by matrices.&lt;/p&gt;
    &lt;p&gt;To display 3D objects on a 2D screen, we use projections:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Orthogonal projection: drops the&lt;/p&gt;&lt;math-renderer&gt;$z$&lt;/math-renderer&gt;-coordinate, mapping&lt;math-renderer&gt;$(x,y,z) \mapsto (x,y)$&lt;/math-renderer&gt;.&lt;p&gt;$$ P = \begin{bmatrix} 1 &amp;amp; 0 &amp;amp; 0 \ 0 &amp;amp; 1 &amp;amp; 0 \end{bmatrix}. $$&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Perspective projection: mimics the effect of a camera. A point&lt;/p&gt;&lt;math-renderer&gt;$(x,y,z)$&lt;/math-renderer&gt;projects to&lt;p&gt;$$ \left(\frac{x}{z}, \frac{y}{z}\right), $$&lt;/p&gt;&lt;p&gt;capturing how distant objects appear smaller.&lt;/p&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These operations are linear (orthogonal projection) or nearly linear (perspective projection becomes linear in homogeneous coordinates).&lt;/p&gt;
    &lt;p&gt;To unify translations and projections with linear transformations, computer graphics uses homogeneous coordinates. A 3D point &lt;/p&gt;
    &lt;p&gt;Example: Translation by &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Rotations preserve shape and size, only changing orientation.&lt;/item&gt;
      &lt;item&gt;Projections reduce dimension: from 3D world space to 2D screen space.&lt;/item&gt;
      &lt;item&gt;Homogeneous coordinates allow us to combine multiple transformations (rotation + translation + projection) into a single matrix multiplication.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Linear algebra enables all real-time graphics: video games, simulations, CAD software, and movie effects. By chaining simple matrix operations, complex transformations are applied efficiently to millions of points per second.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Write the rotation matrix for a 90° counterclockwise rotation in &lt;math-renderer&gt;$\mathbb{R}^2$&lt;/math-renderer&gt;. Apply it to&lt;math-renderer&gt;$(1,0)$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Rotate the point &lt;math-renderer&gt;$(1,1,0)$&lt;/math-renderer&gt;about the&lt;math-renderer&gt;$z$&lt;/math-renderer&gt;-axis by 180°.&lt;/item&gt;
      &lt;item&gt;Show that the determinant of any 2D or 3D rotation matrix is 1.&lt;/item&gt;
      &lt;item&gt;Derive the orthogonal projection matrix from &lt;math-renderer&gt;$\mathbb{R}^3$&lt;/math-renderer&gt;to the&lt;math-renderer&gt;$xy$&lt;/math-renderer&gt;-plane.&lt;/item&gt;
      &lt;item&gt;Explain how homogeneous coordinates allow translations to be represented as matrix multiplications.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Linear algebra provides the foundation for many data science techniques. Two of the most important are dimensionality reduction, where high-dimensional datasets are compressed while preserving essential information, and the least squares method, which underlies regression and model fitting.&lt;/p&gt;
    &lt;p&gt;High-dimensional data often contains redundancy: many features are correlated, meaning the data essentially lies near a lower-dimensional subspace. Dimensionality reduction identifies these subspaces.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;PCA (Principal Component Analysis): As introduced earlier, PCA diagonalizes the covariance matrix of the data.&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Eigenvectors (principal components) define orthogonal directions of maximum variance.&lt;/item&gt;
          &lt;item&gt;Eigenvalues measure how much variance lies along each direction.&lt;/item&gt;
          &lt;item&gt;Keeping only the top &lt;math-renderer&gt;$k$&lt;/math-renderer&gt;components reduces data from&lt;math-renderer&gt;$n$&lt;/math-renderer&gt;-dimensional space to&lt;math-renderer&gt;$k$&lt;/math-renderer&gt;-dimensional space while retaining most variability.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example 10.2.1. A dataset of 1000 images, each with 1024 pixels, may have most variance captured by just 50 eigenvectors of the covariance matrix. Projecting onto these components compresses the data while preserving essential features.&lt;/p&gt;
    &lt;p&gt;Often, we have more equations than unknowns-an overdetermined system:&lt;/p&gt;
    &lt;p&gt;An exact solution may not exist. Instead, we seek &lt;/p&gt;
    &lt;p&gt;This leads to the normal equations:&lt;/p&gt;
    &lt;p&gt;The solution is the orthogonal projection of &lt;/p&gt;
    &lt;p&gt;Fit a line &lt;/p&gt;
    &lt;p&gt;Matrix form:&lt;/p&gt;
    &lt;p&gt;Solve &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Dimensionality reduction: Find the best subspace capturing most variance.&lt;/item&gt;
      &lt;item&gt;Least squares: Project the target vector onto the subspace spanned by predictors.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Both are projection problems, solved using inner products and orthogonality.&lt;/p&gt;
    &lt;p&gt;Dimensionality reduction makes large datasets tractable, filters noise, and reveals structure. Least squares fitting powers regression, statistics, and machine learning. Both rely directly on eigenvalues, eigenvectors, and projections-core tools of linear algebra.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Explain why PCA reduces noise in datasets by discarding small eigenvalue components.&lt;/item&gt;
      &lt;item&gt;Compute the least squares solution to fitting a line through &lt;math-renderer&gt;$(0,0), (1,1), (2,2)$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Show that the least squares solution is unique if and only if &lt;math-renderer&gt;$A^T A$&lt;/math-renderer&gt;is invertible.&lt;/item&gt;
      &lt;item&gt;Prove that the least squares solution minimizes the squared error by projection arguments.&lt;/item&gt;
      &lt;item&gt;Apply PCA to the data points &lt;math-renderer&gt;$(1,0), (2,1), (3,2)$&lt;/math-renderer&gt;and find the first principal component.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Graphs and networks provide a natural setting where linear algebra comes to life. From modeling flows and connectivity to predicting long-term behavior, matrices translate network structure into algebraic form. Markov chains, already introduced in Section 8.4, are a central example of networks evolving over time.&lt;/p&gt;
    &lt;p&gt;A network (graph) with &lt;/p&gt;
    &lt;p&gt;For weighted graphs, entries may be positive weights instead of &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The number of walks of length &lt;math-renderer&gt;$k$&lt;/math-renderer&gt;from node&lt;math-renderer&gt;$i$&lt;/math-renderer&gt;to node&lt;math-renderer&gt;$j$&lt;/math-renderer&gt;is given by the entry&lt;math-renderer&gt;$(A^k)_{ij}$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Powers of adjacency matrices thus encode connectivity over time.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Another important matrix is the graph Laplacian:&lt;/p&gt;
    &lt;p&gt;where &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;math-renderer&gt;$L$&lt;/math-renderer&gt;is symmetric and positive semidefinite.&lt;/item&gt;
      &lt;item&gt;The smallest eigenvalue is always &lt;math-renderer&gt;$0$&lt;/math-renderer&gt;, with eigenvector&lt;math-renderer&gt;$(1,1,\dots,1)$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;The multiplicity of eigenvalue &lt;math-renderer&gt;$0$&lt;/math-renderer&gt;equals the number of connected components in the graph.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This connection between eigenvalues and connectivity forms the basis of spectral graph theory.&lt;/p&gt;
    &lt;p&gt;A Markov chain can be viewed as a random walk on a graph. If &lt;/p&gt;
    &lt;p&gt;describes the distribution of positions after &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The steady-state distribution is given by the eigenvector of &lt;math-renderer&gt;$P$&lt;/math-renderer&gt;with eigenvalue&lt;math-renderer&gt;$1$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;The speed of convergence depends on the gap between the largest eigenvalue (which is always &lt;math-renderer&gt;$1$&lt;/math-renderer&gt;) and the second largest eigenvalue.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Consider a simple 3-node cycle graph:&lt;/p&gt;
    &lt;p&gt;This Markov chain cycles deterministically among the nodes. Eigenvalues are the cube roots of unity: &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Search engines: Google’s PageRank algorithm models the web as a Markov chain, where steady-state probabilities rank pages.&lt;/item&gt;
      &lt;item&gt;Network analysis: Eigenvalues of adjacency or Laplacian matrices reveal communities, bottlenecks, and robustness.&lt;/item&gt;
      &lt;item&gt;Epidemiology and information flow: Random walks model how diseases or ideas spread through networks.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Linear algebra transforms network problems into matrix problems. Eigenvalues and eigenvectors reveal connectivity, flow, stability, and long-term dynamics. Networks are everywhere-social media, biology, finance, and the internet-so these tools are indispensable.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Write the adjacency matrix of a square graph with 4 nodes. Compute&lt;/p&gt;&lt;math-renderer&gt;$A^2$&lt;/math-renderer&gt;and interpret the entries.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Show that the Laplacian of a connected graph has exactly one zero eigenvalue.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Find the steady-state distribution of the Markov chain with&lt;/p&gt;
        &lt;p&gt;$$ P = \begin{bmatrix} 0.5 &amp;amp; 0.5 \ 0.4 &amp;amp; 0.6 \end{bmatrix}. $$&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Explain how eigenvalues of the Laplacian can detect disconnected components of a graph.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Describe how PageRank modifies the transition matrix of the web graph to ensure a unique steady-state distribution.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Modern machine learning is built on linear algebra. From the representation of data as matrices to the optimization of large-scale models, nearly every step relies on concepts such as vector spaces, projections, eigenvalues, and matrix decompositions.&lt;/p&gt;
    &lt;p&gt;A dataset with &lt;/p&gt;
    &lt;p&gt;$$ X = \begin{bmatrix}&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&amp;amp; \mathbf{x}_1^T &amp;amp; - \&lt;/item&gt;
      &lt;item&gt;&amp;amp; \mathbf{x}_2^T &amp;amp; - \ &amp;amp; \vdots &amp;amp; \&lt;/item&gt;
      &lt;item&gt;&amp;amp; \mathbf{x}_m^T &amp;amp; - \end{bmatrix}, $$&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;where each row &lt;/p&gt;
    &lt;p&gt;At the heart of machine learning are linear predictors:&lt;/p&gt;
    &lt;p&gt;where &lt;/p&gt;
    &lt;p&gt;This is solved efficiently using matrix factorizations.&lt;/p&gt;
    &lt;p&gt;The SVD of a matrix &lt;/p&gt;
    &lt;p&gt;where &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Singular values measure the importance of directions in feature space.&lt;/item&gt;
      &lt;item&gt;SVD is used for dimensionality reduction (low-rank approximations), topic modeling, and recommender systems.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;PCA (Principal Component Analysis): diagonalization of the covariance matrix identifies directions of maximal variance.&lt;/item&gt;
      &lt;item&gt;Spectral clustering: uses eigenvectors of the Laplacian to group data points into clusters.&lt;/item&gt;
      &lt;item&gt;Stability analysis: eigenvalues of Hessian matrices determine whether optimization converges to a minimum.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Even deep learning, though nonlinear, uses linear algebra at its core:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Each layer is a matrix multiplication followed by a nonlinear activation.&lt;/item&gt;
      &lt;item&gt;Training requires computing gradients, which are expressed in terms of matrix calculus.&lt;/item&gt;
      &lt;item&gt;Backpropagation is essentially repeated applications of the chain rule with linear algebra.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Machine learning models often involve datasets with millions of features and parameters. Linear algebra provides the algorithms and abstractions that make training and inference possible. Without it, large-scale computation in AI would be intractable.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Show that ridge regression leads to the normal equations&lt;/p&gt;
        &lt;p&gt;$$ (X^T X + \lambda I)\mathbf{w} = X^T \mathbf{y}. $$&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Explain how SVD can be used to compress an image represented as a matrix of pixel intensities.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;For a covariance matrix&lt;/p&gt;&lt;math-renderer&gt;$\Sigma$&lt;/math-renderer&gt;, show why its eigenvalues represent variances along principal components.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Give an example of how eigenvectors of the Laplacian matrix can be used for clustering a small graph.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;In a neural network with one hidden layer, write the forward pass in matrix form.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45103436</guid></item><item><title>The staff ate it later</title><link>https://en.wikipedia.org/wiki/The_staff_ate_it_later</link><description>&lt;doc fingerprint="c70edbe3b4ac97c3"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;The staff ate it later&lt;/head&gt;&lt;p&gt;"The staff ate it later" (Japanese: この後、スタッフが美味しくいただきました, romanized: Kono ato, sutaffu ga oishiku itadakimashita) is a caption shown on screen when food appears in a Japanese TV program to indicate that it was not thrown away after filming (it is generally not socially acceptable to discard food in Japan). Some[who?] question the authenticity of this statement or believe the caption lowers the quality of TV programs.&lt;/p&gt;&lt;head rend="h2"&gt;First appearance&lt;/head&gt;[edit]&lt;p&gt;It is thought TV stations first began showing the caption to protect themselves against complaints from viewers who disliked food being handled without consideration in TV variety shows.[1] It is uncertain when this note was first used, but TV producer Kenji Suga stated viewers complained about the waste of food when a performance using small watermelons was broadcast in Downtown no Gaki no Tsukai ya Arahende!! on Nippon TV. The TV station then showed this note on screen the following year in response.[2]&lt;/p&gt;&lt;head rend="h2"&gt;Authenticity&lt;/head&gt;[edit]&lt;p&gt;There are various claims as to whether or not staff actually eat the food that appears in the programs.[1][3][4]&lt;/p&gt;&lt;head rend="h3"&gt;Supporting reports&lt;/head&gt;[edit]&lt;p&gt;According to AOL News in 2014, the crew on one information program claimed: "It's sometimes impossible for the reporter to eat all the food provided by the restaurant. The reporter is told not to eat it all, but the crew will eat the rest out of consideration and a feeling of obligation towards the restaurant."[4]&lt;/p&gt;&lt;p&gt;Food comic artist Raswell Hosoki claimed in Meshizanmai Furusatonoaji (Meshizanmai Taste of Hometown) that the note is true. Eriko Miyazaki , a reporter and TV personality for food shows, also claimed the note is true and stated: "The crew eats the rest of the food, at least at the shows I appear in."[5]&lt;/p&gt;&lt;p&gt;In January 2018, Miwa Asao, former professional beach volleyball player and TV personality, posted photos on her blog of staff eating food after recording "Saturday Night! Otona na TV ". She wrote: "This is an on-site photo. The staff ate the rest of the food."[6]&lt;/p&gt;&lt;head rend="h3"&gt;Refuting reports&lt;/head&gt;[edit]&lt;p&gt;Hitoshi Matsumoto, a comedian and TV host, was asked by sociologist Noritoshi Furuichi about this note in 2014 during the "Wide na Show " (Fuji Television). He said: "To be honest, I've never seen the crew eat the food. But that just means I haven't seen it. They might've eaten it."[7]&lt;/p&gt;&lt;p&gt;Takeshi Kitano (also known as Beat Takeshi), a Japanese comedian, actor, and filmmaker, referred to an instance where cake was smeared on the floor and said in his book Bakaron: "Liars. Who's going to enjoy cake they splattered all over the floor?"[3] Commentator Tsunehira Furuya also stated that the food featured in the show is not eaten by the staff later and is instead simply thrown into garbage bags.[1]&lt;/p&gt;&lt;head rend="h2"&gt;Reception&lt;/head&gt;[edit]&lt;p&gt;Commentator Tetsuya Uetaki has commented on displaying the note, saying: "Producers have become more aware as viewers have become more critical after issues such as the Aru Aru Mondai (a natto shortage caused by a program claiming eating natto would make people lose weight), and it's fine as one method for dealing with that." However, Uetaki went on to say: "This shifts responsibility onto the viewers. We can't let it end as simply an empty concession. I want to see variety shows strive to properly handle information and properly put the show together, from the moment they start building it."[8]&lt;/p&gt;&lt;p&gt;Broadcast writer Sotani commented on the fact that production teams have become more sensitive to this in programs and have begun displaying such notes as an attempt to preempt criticism. He claims this sort of extreme self regulation risks leading to a decline.[9] TV producer Kenji Suga claims it is necessary for programs to be disconnected from real life and society, to be "dumb and idiotic" to produce laughs.[2]&lt;/p&gt;&lt;p&gt;Columnist Takashi Matsuo argues that adults, not TV shows, should teach children the ethics surrounding the importance of food. He also argues that if a parent is uncomfortable with what a comedian expresses on TV, the right course of action would be to change the channel or turn off the TV, not send a complaint to the TV station.[10] Matsuo also points out the inconsistency that "the staff ate it later" caption is not displayed when large numbers of tomatoes are thrown at the festival of Tomatina in Spain or when athletes spray each other with champagne in celebration of a victory.[10]&lt;/p&gt;&lt;head rend="h2"&gt;References&lt;/head&gt;[edit]&lt;list rend="ol"&gt;&lt;item&gt;^ a b c Furuya, Tsunehira (2017). 「道徳自警団」がニッポンを滅ぼす. East Shinsho: East Press. pp. 35–36. ISBN 978-4-7816-5095-1.&lt;/item&gt;&lt;item&gt;^ a b Wake, Shinya (7 February 2016). "グローブ176号 笑いの力 インタビュー 笑わせるってむずかしい プロデューサー・菅賢治". Asahi Shimbun. p. 6.&lt;/item&gt;&lt;item&gt;^ a b Kitano, Takeshi (2017). バカ論. Shinchosha. pp. 36–37. ISBN 978-4-10-610737-5.&lt;/item&gt;&lt;item&gt;^ a b "テレビ番組の食リポ、完食しているのか？「この後スタッフが美味しく...」は本当か" [Is the staff really eating the rest of the dishes used in the TV show?]. AOL News. 16 April 2014. Archived from the original on 16 September 2014. Retrieved 9 January 2020.&lt;/item&gt;&lt;item&gt;^ Raswell Hosoki, Mayumi Kato, Takako Aonuma, Sachiko Orihara, Junko Kubota, Eiko Kasai, Riyo Mizuki, Takotsumuri, Usami☆, and Somei Yoshino, (2017) Meshizanmai Hurusatonoaji, Bunkasha, BUNKASHA COMICS, ISBN 978-4-8211-3416-8&lt;/item&gt;&lt;item&gt;^ "バラエティの「この後スタッフが美味しく頂きました」 予防線を張るテロップどこまで必要？" [Variety's "The staff enjoyed the food afterwards": How much precautionary captioning is necessary?]. Oricon News. 13 February 2018. Archived from the original on 18 September 2024. Retrieved 26 December 2020.&lt;/item&gt;&lt;item&gt;^ "松本人志 バラエティならでの葛藤を吐露「食べ物も笑いの1つの小道具として認めてもらえたら」" [Hitoshi Matsumoto, revealing his struggles with variety: "If people would accept food as a prop for laughter..."]. Nagai Times. 28 October 2014. Archived from the original on 2 December 2024. Retrieved 26 December 2020.&lt;/item&gt;&lt;item&gt;^ "近ごろよく見る『お断りテロップ』『視聴者への配慮』か苦情抗議"先逃れ"か ないよりましだが…『番組精査こそ肝心』識者指摘". Chunichi Shimbun. 4 July 2007. p. 15.&lt;/item&gt;&lt;item&gt;^ "1番ものがたり 人物編 売れっ子放送作家 そーたに氏「見せたくない」で金字塔 PTAの土俵に乗らず". Hokkoku Shimbun. 23 February 2012. p. 36.&lt;/item&gt;&lt;item&gt;^ a b Matsuo, Takashi (17 September 2017). "テレビの過剰なテロップ 苦情逃れの保身が目的？" [Is over-annotation on television a self-protection to escape complaints?]. Mainichi Shimbun Digital. Retrieved 26 December 2020.[dead link]&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45104289</guid></item><item><title>Static sites enable a good time travel experience</title><link>https://hamatti.org/posts/static-sites-enable-a-good-time-travel-experience/</link><description>&lt;doc fingerprint="5895a521cf7dd597"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Static sites enable a good time travel experience&lt;/head&gt;
    &lt;p&gt;Varun wrote about gamifying blogging and personal website maintenance which reminded me of the time when I awarded myself some badges for blogging.&lt;/p&gt;
    &lt;p&gt;I mentioned this to Varun who asked if I had any screenshots of what it looked like on my website. My initial answer was “no”, then I looked at Wayback Machine but there were not pictures of the badges.&lt;/p&gt;
    &lt;p&gt;Then, a bit later it hit me. I don’t need any archived screenshots: my website is built with Eleventy and it's static so I can check out a git commit from the time I had those badges up, fire up Eleventy and see the website — as it was in the spring of 2021.&lt;/p&gt;
    &lt;p&gt;That’s a beauty of a static site generator combined with my workflow of fetching posts from CMS before build time so each commit contains a full snapshot of the website.&lt;/p&gt;
    &lt;p&gt; Comparing this to a website that uses a database for posts (like WordPress) or a flow where posts from CMS are not stored in version control but rather fetched at build time only, my solution makes time travel to (almost) any given moment in time a two-command operation (&lt;code&gt;git checkout&lt;/code&gt;
  with the right commit hash and
  &lt;code&gt;@11ty/eleventy serve&lt;/code&gt; to serve a dev
  server). I say almost because back in the day I wasn’t quite as diligent in
  commiting every change as I was deploying manually and not through version
  control automation.
&lt;/p&gt;
    &lt;p&gt;A year ago, inspired by Alex Chan’s blog post Taking regular screenshots of my website I set up a GitHub Action that takes a snapshot of my front page once a month to keep a record. At the time, I felt bit sad that I hadn’t started it before. However, now that I realised how easy it is for me to go back in time thanks to Eleventy and git, I’m not so worried anymore. Maybe I should do a collage of changes on my design one day by going through my project history.&lt;/p&gt;
    &lt;p&gt;One more major point for static site generators!&lt;/p&gt;
    &lt;p&gt;If something above resonated with you, let's start a discussion about it! Email me at juhamattisantala at gmail dot com and share your thoughts. In 2025, I want to have more deeper discussions with people from around the world and I'd love if you'd be part of that.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45104303</guid></item><item><title>Launch HN: Datafruit (YC S25) – AI for DevOps</title><link>https://news.ycombinator.com/item?id=45104974</link><description>&lt;doc fingerprint="ee5fd4d201abb246"&gt;
  &lt;main&gt;
    &lt;div&gt;&lt;p&gt;Hey HN! We’re Abhi, Venkat, Tom, and Nick and we are building Datafruit (&lt;/p&gt;https://datafruit.dev/&lt;p&gt;), an AI DevOps agent. We’re like Devin for DevOps. You can ask Datafruit to check your cloud spend, look for loose security policies, make changes to your IaC, and it can reason across your deployment standards, design docs, and DevOps practices.&lt;/p&gt;&lt;p&gt;Demo video: https://www.youtube.com/watch?v=2FitSggI7tg.&lt;/p&gt;&lt;p&gt;Right now, we have two main methods to interact with Datafruit:&lt;/p&gt;&lt;p&gt;(1) automated infrastructure audits— agents periodically scan your environment to find cost optimization opportunities, detect infrastructure drift, and validate your infra against compliance requirements.&lt;/p&gt;&lt;p&gt;(2) chat interface (available as a web UI and through slack) — ask the agent questions for real-time insights, or assign tasks directly, such as investigating spend anomalies, reviewing security posture, or applying changes to IaC resources.&lt;/p&gt;&lt;p&gt;Working at FAANG and various high-growth startups, we realized that infra work requires an enormous amount of context, often more than traditional software engineering. The business decisions, codebase, and cloud itself are all extremely important in any task that has been assigned. To maximize the success of the agents, we do a fair amount of context engineering. Not hallucinating is super important!&lt;/p&gt;&lt;p&gt;One thing which has worked incredibly well for us is a multi-agent system where we have specialized sub-agents with access to specific tool calls and documentation for their specialty. Agents choose to “handoff” to each other when they feel like another agent would be more specialized for the task. However, all agents share the same context (https://cognition.ai/blog/dont-build-multi-agents). We’re pretty happy with this approach, and believe it could work in other disciplines which require high amounts of specialized expertise.&lt;/p&gt;&lt;p&gt;Infrastructure is probably the most mission-critical part of any software organization, and needs extremely heavy guardrails to keep it safe. Language models are not yet at the point where they can be trusted to make changes (we’ve talked to a couple of startups where the Claude Code + AWS CLI combo has taken their infra down). Right now, Datafruit receives read-only access to your infrastructure and can only make changes through pull requests to your IaC repositories. The agent also operates in a sandboxed virtual environment so that it could not write cloud CLI commands if it wanted to!&lt;/p&gt;&lt;p&gt;Where LLMs can add significant value is in reducing the constant operational inefficiencies that eat up cloud spend and delay deadlines—the small-but-urgent ops work. Once Datafruit indexes your environment, you can ask it to do things like:&lt;/p&gt;&lt;quote&gt;&lt;code&gt;  "Grant @User write access to analytics S3 bucket for 24 hours"
    -&amp;gt; Creates temporary IAM role, sends least-privilege credentials, auto-revokes tomorrow

  "Find where this secret is used so I can rotate it without downtime"
    -&amp;gt; Discovers all instances of your secret, including old cron-jobs you might not know about, so you can safely rotate your keys


  "Why did database costs spike yesterday?"
    -&amp;gt; Identifies expensive queries, shows optimization options, implements fixes

&lt;/code&gt;&lt;/quote&gt;&lt;p&gt; We charge a straightforward subscription model for a managed version, but we also offer a bring-your-own-cloud model. All of Datafruit can be deployed on Kubernetes using Helm charts for enterprise customers where data can’t leave your VPC. For the time being, we’re installing the product ourselves on customers' clouds. It doesn’t exist in a self-serve form yet. We’ll get there eventually, but in the meantime if you’re interested we’d love for you guys to email us at founders@datafruit.dev.&lt;/p&gt;&lt;p&gt;We would love to hear your thoughts! If you work with cloud infra, we are especially interested in learning about what kinds of work you do which you wish could be offloaded onto an agent.&lt;/p&gt;&lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45104974</guid></item><item><title>'World Models,' an old idea in AI, mount a comeback</title><link>https://www.quantamagazine.org/world-models-an-old-idea-in-ai-mount-a-comeback-20250902/</link><description>&lt;doc fingerprint="3c198b9fef1780ca"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;‘World Models,’ an Old Idea in AI, Mount a Comeback&lt;/head&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;The latest ambition of artificial intelligence research — particularly within the labs seeking “artificial general intelligence,” or AGI — is something called a world model: a representation of the environment that an AI carries around inside itself like a computational snow globe. The AI system can use this simplified representation to evaluate predictions and decisions before applying them to its real-world tasks. The deep learning luminaries Yann LeCun (of Meta), Demis Hassabis (of Google DeepMind) and Yoshua Bengio (of Mila, the Quebec Artificial Intelligence Institute) all believe world models are essential for building AI systems that are truly smart, scientific and safe.&lt;/p&gt;
    &lt;p&gt;The fields of psychology, robotics and machine learning have each been using some version of the concept for decades. You likely have a world model running inside your skull right now — it’s how you know not to step in front of a moving train without needing to run the experiment first.&lt;/p&gt;
    &lt;p&gt;So does this mean that AI researchers have finally found a core concept whose meaning everyone can agree upon? As a famous physicist once wrote: Surely you’re joking. A world model may sound straightforward — but as usual, no one can agree on the details. What gets represented in the model, and to what level of fidelity? Is it innate or learned, or some combination of both? And how do you detect that it’s even there at all?&lt;/p&gt;
    &lt;p&gt;It helps to know where the whole idea started. In 1943, a dozen years before the term “artificial intelligence” was coined, a 29-year-old Scottish psychologist named Kenneth Craik published an influential monograph in which he mused that “if the organism carries a ‘small-scale model’ of external reality … within its head, it is able to try out various alternatives, conclude which is the best of them … and in every way to react in a much fuller, safer, and more competent manner.” Craik’s notion of a mental model or simulation presaged the “cognitive revolution” that transformed psychology in the 1950s and still rules the cognitive sciences today. What’s more, it directly linked cognition with computation: Craik considered the “power to parallel or model external events” to be “the fundamental feature” of both “neural machinery” and “calculating machines.”&lt;/p&gt;
    &lt;p&gt;The nascent field of artificial intelligence eagerly adopted the world-modeling approach. In the late 1960s, an AI system called SHRDLU wowed observers by using a rudimentary “block world” to answer commonsense questions about tabletop objects, like “Can a pyramid support a block?” But these handcrafted models couldn’t scale up to handle the complexity of more realistic settings. By the late 1980s, the AI and robotics pioneer Rodney Brooks had given up on world models completely, famously asserting that “the world is its own best model” and “explicit representations … simply get in the way.”&lt;/p&gt;
    &lt;p&gt;It took the rise of machine learning, especially deep learning based on artificial neural networks, to breathe life back into Craik’s brainchild. Instead of relying on brittle hand-coded rules, deep neural networks could build up internal approximations of their training environments through trial and error and then use them to accomplish narrowly specified tasks, such as driving a virtual race car. In the past few years, as the large language models behind chatbots like ChatGPT began to demonstrate emergent capabilities that they weren’t explicitly trained for — like inferring movie titles from strings of emojis, or playing the board game Othello — world models provided a convenient explanation for the mystery. To prominent AI experts such as Geoffrey Hinton, Ilya Sutskever and Chris Olah, it was obvious: Buried somewhere deep within an LLM’s thicket of virtual neurons must lie “a small-scale model of external reality,” just as Craik imagined.&lt;/p&gt;
    &lt;p&gt;The truth, at least so far as we know, is less impressive. Instead of world models, today’s generative AIs appear to learn “bags of heuristics”: scores of disconnected rules of thumb that can approximate responses to specific scenarios, but don’t cohere into a consistent whole. (Some may actually contradict each other.) It’s a lot like the parable of the blind men and the elephant, where each man only touches one part of the animal at a time and fails to apprehend its full form. One man feels the trunk and assumes the entire elephant is snakelike; another touches a leg and guesses it’s more like a tree; a third grasps the elephant’s tail and says it’s a rope. When researchers attempt to recover evidence of a world model from within an LLM — for example, a coherent computational representation of an Othello game board — they’re looking for the whole elephant. What they find instead is a bit of snake here, a chunk of tree there, and some rope.&lt;/p&gt;
    &lt;p&gt;Of course, such heuristics are hardly worthless. LLMs can encode untold sackfuls of them within their trillions of parameters — and as the old saw goes, quantity has a quality all its own. That’s what makes it possible to train a language model to generate nearly perfect directions between any two points in Manhattan without learning a coherent world model of the entire street network in the process, as researchers from Harvard University and the Massachusetts Institute of Technology recently discovered.&lt;/p&gt;
    &lt;p&gt;So if bits of snake, tree and rope can do the job, why bother with the elephant? In a word, robustness: When the researchers threw their Manhattan-navigating LLM a mild curveball by randomly blocking 1% of the streets, its performance cratered. If the AI had simply encoded a street map whose details were consistent — instead of an immensely complicated, corner-by-corner patchwork of conflicting best guesses — it could have easily rerouted around the obstructions.&lt;/p&gt;
    &lt;p&gt;Given the benefits that even simple world models can confer, it’s easy to understand why every large AI lab is desperate to develop them — and why academic researchers are increasingly interested in scrutinizing them, too. Robust and verifiable world models could uncover, if not the El Dorado of AGI, then at least a scientifically plausible tool for extinguishing AI hallucinations, enabling reliable reasoning, and increasing the interpretability of AI systems.&lt;/p&gt;
    &lt;p&gt;That’s the “what” and “why” of world models. The “how,” though, is still anyone’s guess. Google DeepMind and OpenAI are betting that with enough “multimodal” training data — like video, 3D simulations, and other input beyond mere text — a world model will spontaneously congeal within a neural network’s statistical soup. Meta’s LeCun, meanwhile, thinks that an entirely new (and non-generative) AI architecture will provide the necessary scaffolding. In the quest to build these computational snow globes, no one has a crystal ball — but the prize, for once, may just be worth the hype.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45105710</guid></item><item><title>&lt;template&gt;: The Content Template element</title><link>https://developer.mozilla.org/en-US/docs/Web/HTML/Reference/Elements/template</link><description>&lt;doc fingerprint="2a029f791cad1b64"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;&amp;lt;template&amp;gt;: The Content Template element&lt;/head&gt;&lt;head&gt; Baseline Widely available * &lt;/head&gt;&lt;p&gt;This feature is well established and works across many devices and browser versions. Itâs been available across browsers since â¨November 2015â©.&lt;/p&gt;&lt;p&gt;* Some parts of this feature may have varying levels of support.&lt;/p&gt;&lt;p&gt;The &lt;code&gt;&amp;lt;template&amp;gt;&lt;/code&gt; HTML element serves as a mechanism for holding HTML fragments, which can either be used later via JavaScript or generated immediately into shadow DOM.&lt;/p&gt;&lt;head rend="h2"&gt;Attributes&lt;/head&gt;&lt;p&gt;This element includes the global attributes.&lt;/p&gt;&lt;list rend="dl"&gt;&lt;item rend="dt-1"&gt;&lt;code&gt;shadowrootmode&lt;/code&gt;&lt;/item&gt;&lt;item rend="dd-1"&gt;&lt;p&gt;Creates a shadow root for the parent element. It is a declarative version of the&lt;/p&gt;&lt;code&gt;Element.attachShadow()&lt;/code&gt;method and accepts the same enumerated values.&lt;list rend="dl"&gt;&lt;item rend="dt-2"&gt;&lt;code&gt;open&lt;/code&gt;&lt;/item&gt;&lt;item rend="dd-2"&gt;&lt;p&gt;Exposes the internal shadow root DOM for JavaScript (recommended for most use cases).&lt;/p&gt;&lt;/item&gt;&lt;item rend="dt-3"&gt;&lt;code&gt;closed&lt;/code&gt;&lt;/item&gt;&lt;item rend="dd-3"&gt;&lt;p&gt;Hides the internal shadow root DOM from JavaScript.&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Note: The HTML parser creates a&lt;/p&gt;&lt;code&gt;ShadowRoot&lt;/code&gt;object in the DOM for the first&lt;code&gt;&amp;lt;template&amp;gt;&lt;/code&gt;in a node with this attribute set to an allowed value. If the attribute is not set, or not set to an allowed value â or if a&lt;code&gt;ShadowRoot&lt;/code&gt;has already been declaratively created in the same parent â then an&lt;code&gt;HTMLTemplateElement&lt;/code&gt;is constructed. A&lt;code&gt;HTMLTemplateElement&lt;/code&gt;cannot subsequently be changed into a shadow root after parsing, for example, by setting&lt;code&gt;HTMLTemplateElement.shadowRootMode&lt;/code&gt;.&lt;p&gt;Note: You may find the non-standard&lt;/p&gt;&lt;code&gt;shadowroot&lt;/code&gt;attribute in older tutorials and examples that used to be supported in Chrome 90-110. This attribute has since been removed and replaced by the standard&lt;code&gt;shadowrootmode&lt;/code&gt;attribute.&lt;/item&gt;&lt;item rend="dt-4"&gt;&lt;code&gt;shadowrootclonable&lt;/code&gt;&lt;/item&gt;&lt;item rend="dd-4"&gt;&lt;p&gt;Sets the value of the&lt;/p&gt;&lt;code&gt;clonable&lt;/code&gt;property of a&lt;code&gt;ShadowRoot&lt;/code&gt;created using this element to&lt;code&gt;true&lt;/code&gt;. If set, a clone of the shadow host (the parent element of this&lt;code&gt;&amp;lt;template&amp;gt;&lt;/code&gt;) created with&lt;code&gt;Node.cloneNode()&lt;/code&gt;or&lt;code&gt;Document.importNode()&lt;/code&gt;will include a shadow root in the copy.&lt;/item&gt;&lt;item rend="dt-5"&gt;&lt;code&gt;shadowrootdelegatesfocus&lt;/code&gt;&lt;/item&gt;&lt;item rend="dd-5"&gt;&lt;p&gt;Sets the value of the&lt;/p&gt;&lt;code&gt;delegatesFocus&lt;/code&gt;property of a&lt;code&gt;ShadowRoot&lt;/code&gt;created using this element to&lt;code&gt;true&lt;/code&gt;. If this is set and a non-focusable element in the shadow tree is selected, then focus is delegated to the first focusable element in the tree. The value defaults to&lt;code&gt;false&lt;/code&gt;.&lt;/item&gt;&lt;item rend="dt-6"&gt;&lt;code&gt;shadowrootserializable&lt;/code&gt;Experimental&lt;/item&gt;&lt;item rend="dd-6"&gt;&lt;p&gt;Sets the value of the&lt;/p&gt;&lt;code&gt;serializable&lt;/code&gt;property of a&lt;code&gt;ShadowRoot&lt;/code&gt;created using this element to&lt;code&gt;true&lt;/code&gt;. If set, the shadow root may be serialized by calling the&lt;code&gt;Element.getHTML()&lt;/code&gt;or&lt;code&gt;ShadowRoot.getHTML()&lt;/code&gt;methods with the&lt;code&gt;options.serializableShadowRoots&lt;/code&gt;parameter set&lt;code&gt;true&lt;/code&gt;. The value defaults to&lt;code&gt;false&lt;/code&gt;.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;Usage notes&lt;/head&gt;&lt;p&gt;This element has no permitted content, because everything nested inside it in the HTML source does not actually become the children of the &lt;code&gt;&amp;lt;template&amp;gt;&lt;/code&gt; element. The &lt;code&gt;Node.childNodes&lt;/code&gt; property of the &lt;code&gt;&amp;lt;template&amp;gt;&lt;/code&gt; element is always empty, and you can only access said nested content via the special &lt;code&gt;content&lt;/code&gt; property. However, if you call &lt;code&gt;Node.appendChild()&lt;/code&gt; or similar methods on the &lt;code&gt;&amp;lt;template&amp;gt;&lt;/code&gt; element, then you would be inserting children into the &lt;code&gt;&amp;lt;template&amp;gt;&lt;/code&gt; element itself, which is a violation of its content model and does not actually update the &lt;code&gt;DocumentFragment&lt;/code&gt; returned by the &lt;code&gt;content&lt;/code&gt; property.&lt;/p&gt;&lt;p&gt;Due to the way the &lt;code&gt;&amp;lt;template&amp;gt;&lt;/code&gt; element is parsed, all &lt;code&gt;&amp;lt;html&amp;gt;&lt;/code&gt;, &lt;code&gt;&amp;lt;head&amp;gt;&lt;/code&gt;, and &lt;code&gt;&amp;lt;body&amp;gt;&lt;/code&gt; opening and closing tags inside the template are syntax errors and are ignored by the parser, so &lt;code&gt;&amp;lt;template&amp;gt;&amp;lt;head&amp;gt;&amp;lt;title&amp;gt;Test&amp;lt;/title&amp;gt;&amp;lt;/head&amp;gt;&amp;lt;/template&amp;gt;&lt;/code&gt; is the same as &lt;code&gt;&amp;lt;template&amp;gt;&amp;lt;title&amp;gt;Test&amp;lt;/title&amp;gt;&amp;lt;/template&amp;gt;&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;There are two main ways to use the &lt;code&gt;&amp;lt;template&amp;gt;&lt;/code&gt; element.&lt;/p&gt;&lt;head rend="h3"&gt;Template document fragment&lt;/head&gt;&lt;p&gt;By default, the element's content is not rendered. The corresponding &lt;code&gt;HTMLTemplateElement&lt;/code&gt; interface includes a standard &lt;code&gt;content&lt;/code&gt; property (without an equivalent content/markup attribute). This &lt;code&gt;content&lt;/code&gt; property is read-only and holds a &lt;code&gt;DocumentFragment&lt;/code&gt; that contains the DOM subtree represented by the template.
This fragment can be cloned via the &lt;code&gt;cloneNode&lt;/code&gt; method and inserted into the DOM.&lt;/p&gt;&lt;p&gt;Be careful when using the &lt;code&gt;content&lt;/code&gt; property because the returned &lt;code&gt;DocumentFragment&lt;/code&gt; can exhibit unexpected behavior.
For more details, see the Avoiding DocumentFragment pitfalls section below.&lt;/p&gt;&lt;head rend="h3"&gt;Declarative Shadow DOM&lt;/head&gt;&lt;p&gt;If the &lt;code&gt;&amp;lt;template&amp;gt;&lt;/code&gt; element contains the &lt;code&gt;shadowrootmode&lt;/code&gt; attribute with a value of either &lt;code&gt;open&lt;/code&gt; or &lt;code&gt;closed&lt;/code&gt;, the HTML parser will immediately generate a shadow DOM. The element is replaced in the DOM by its content wrapped in a &lt;code&gt;ShadowRoot&lt;/code&gt;, which is attached to the parent element.
This is the declarative equivalent of calling &lt;code&gt;Element.attachShadow()&lt;/code&gt; to attach a shadow root to an element.&lt;/p&gt;&lt;p&gt;If the element has any other value for &lt;code&gt;shadowrootmode&lt;/code&gt;, or does not have the &lt;code&gt;shadowrootmode&lt;/code&gt; attribute, the parser generates a &lt;code&gt;HTMLTemplateElement&lt;/code&gt;.
Similarly, if there are multiple declarative shadow roots, only the first one is replaced by a &lt;code&gt;ShadowRoot&lt;/code&gt; â subsequent instances are parsed as &lt;code&gt;HTMLTemplateElement&lt;/code&gt; objects.&lt;/p&gt;&lt;head rend="h2"&gt;Examples&lt;/head&gt;&amp;gt;&lt;head rend="h3"&gt;Generating table rows&lt;/head&gt;&lt;p&gt;First we start with the HTML portion of the example.&lt;/p&gt;&lt;code&gt;&amp;lt;table id="producttable"&amp;gt;
  &amp;lt;thead&amp;gt;
    &amp;lt;tr&amp;gt;
      &amp;lt;td&amp;gt;UPC_Code&amp;lt;/td&amp;gt;
      &amp;lt;td&amp;gt;Product_Name&amp;lt;/td&amp;gt;
    &amp;lt;/tr&amp;gt;
  &amp;lt;/thead&amp;gt;
  &amp;lt;tbody&amp;gt;
    &amp;lt;!-- existing data could optionally be included here --&amp;gt;
  &amp;lt;/tbody&amp;gt;
&amp;lt;/table&amp;gt;

&amp;lt;template id="productrow"&amp;gt;
  &amp;lt;tr&amp;gt;
    &amp;lt;td class="record"&amp;gt;&amp;lt;/td&amp;gt;
    &amp;lt;td&amp;gt;&amp;lt;/td&amp;gt;
  &amp;lt;/tr&amp;gt;
&amp;lt;/template&amp;gt;
&lt;/code&gt;&lt;p&gt;First, we have a table into which we will later insert content using JavaScript code. Then comes the template, which describes the structure of an HTML fragment representing a single table row.&lt;/p&gt;&lt;p&gt;Now that the table has been created and the template defined, we use JavaScript to insert rows into the table, with each row being constructed using the template as its basis.&lt;/p&gt;&lt;code&gt;// Test to see if the browser supports the HTML template element by checking
// for the presence of the template element's content attribute.
if ("content" in document.createElement("template")) {
  // Instantiate the table with the existing HTML tbody
  // and the row with the template
  const tbody = document.querySelector("tbody");
  const template = document.querySelector("#productrow");

  // Clone the new row and insert it into the table
  const clone = template.content.cloneNode(true);
  let td = clone.querySelectorAll("td");
  td[0].textContent = "1235646565";
  td[1].textContent = "Stuff";

  tbody.appendChild(clone);

  // Clone the new row and insert it into the table
  const clone2 = template.content.cloneNode(true);
  td = clone2.querySelectorAll("td");
  td[0].textContent = "0384928528";
  td[1].textContent = "Acme Kidney Beans 2";

  tbody.appendChild(clone2);
} else {
  // Find another way to add the rows to the table because
  // the HTML template element is not supported.
}
&lt;/code&gt;&lt;p&gt;The result is the original HTML table, with two new rows appended to it via JavaScript:&lt;/p&gt;&lt;head rend="h3"&gt;Implementing a declarative shadow DOM&lt;/head&gt;&lt;p&gt;In this example, a hidden support warning is included at the beginning of the markup. This warning is later set to be displayed via JavaScript if the browser doesn't support the &lt;code&gt;shadowrootmode&lt;/code&gt; attribute. Next, there are two &lt;code&gt;&amp;lt;article&amp;gt;&lt;/code&gt; elements, each containing nested &lt;code&gt;&amp;lt;style&amp;gt;&lt;/code&gt; elements with different behaviors. The first &lt;code&gt;&amp;lt;style&amp;gt;&lt;/code&gt; element is global to the whole document. The second one is scoped to the shadow root generated in place of the &lt;code&gt;&amp;lt;template&amp;gt;&lt;/code&gt; element because of the presence of the &lt;code&gt;shadowrootmode&lt;/code&gt; attribute.&lt;/p&gt;&lt;code&gt;&amp;lt;p hidden&amp;gt;
  â Your browser doesn't support &amp;lt;code&amp;gt;shadowrootmode&amp;lt;/code&amp;gt; attribute yet.
&amp;lt;/p&amp;gt;
&amp;lt;article&amp;gt;
  &amp;lt;style&amp;gt;
    p {
      padding: 8px;
      background-color: wheat;
    }
  &amp;lt;/style&amp;gt;
  &amp;lt;p&amp;gt;I'm in the DOM.&amp;lt;/p&amp;gt;
&amp;lt;/article&amp;gt;
&amp;lt;article&amp;gt;
  &amp;lt;template shadowrootmode="open"&amp;gt;
    &amp;lt;style&amp;gt;
      p {
        padding: 8px;
        background-color: plum;
      }
    &amp;lt;/style&amp;gt;
    &amp;lt;p&amp;gt;I'm in the shadow DOM.&amp;lt;/p&amp;gt;
  &amp;lt;/template&amp;gt;
&amp;lt;/article&amp;gt;
&lt;/code&gt;&lt;code&gt;const isShadowRootModeSupported = Object.hasOwn(
  HTMLTemplateElement.prototype,
  "shadowRootMode",
);

document
  .querySelector("p[hidden]")
  .toggleAttribute("hidden", isShadowRootModeSupported);
&lt;/code&gt;&lt;head rend="h3"&gt;Declarative Shadow DOM with delegated focus&lt;/head&gt;&lt;p&gt;This example demonstrates how &lt;code&gt;shadowrootdelegatesfocus&lt;/code&gt; is applied to a shadow root that is created declaratively, and the effect this has on focus.&lt;/p&gt;&lt;p&gt;The code first declares a shadow root inside a &lt;code&gt;&amp;lt;div&amp;gt;&lt;/code&gt; element, using the &lt;code&gt;&amp;lt;template&amp;gt;&lt;/code&gt; element with the &lt;code&gt;shadowrootmode&lt;/code&gt; attribute.
This displays both a non-focusable &lt;code&gt;&amp;lt;div&amp;gt;&lt;/code&gt; containing text and a focusable &lt;code&gt;&amp;lt;input&amp;gt;&lt;/code&gt; element.
It also uses CSS to style elements with &lt;code&gt;:focus&lt;/code&gt; to blue, and to set the normal styling of the host element.&lt;/p&gt;&lt;code&gt;&amp;lt;div&amp;gt;
  &amp;lt;template shadowrootmode="open"&amp;gt;
    &amp;lt;style&amp;gt;
      :host {
        display: block;
        border: 1px dotted black;
        padding: 10px;
        margin: 10px;
      }
      :focus {
        outline: 2px solid blue;
      }
    &amp;lt;/style&amp;gt;
    &amp;lt;div&amp;gt;Clickable Shadow DOM text&amp;lt;/div&amp;gt;
    &amp;lt;input type="text" placeholder="Input inside Shadow DOM" /&amp;gt;
  &amp;lt;/template&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/code&gt;&lt;p&gt;The second code block is identical except that it sets the &lt;code&gt;shadowrootdelegatesfocus&lt;/code&gt; attribute, which delegates focus to the first focusable element in the tree if a non-focusable element in the tree is selected.&lt;/p&gt;&lt;code&gt;&amp;lt;div&amp;gt;
  &amp;lt;template shadowrootmode="open" shadowrootdelegatesfocus&amp;gt;
    &amp;lt;style&amp;gt;
      :host {
        display: block;
        border: 1px dotted black;
        padding: 10px;
        margin: 10px;
      }
      :focus {
        outline: 2px solid blue;
      }
    &amp;lt;/style&amp;gt;
    &amp;lt;div&amp;gt;Clickable Shadow DOM text&amp;lt;/div&amp;gt;
    &amp;lt;input type="text" placeholder="Input inside Shadow DOM" /&amp;gt;
  &amp;lt;/template&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/code&gt;&lt;p&gt;Last of all we use the following CSS to apply a red border to the parent &lt;code&gt;&amp;lt;div&amp;gt;&lt;/code&gt; element when it has focus.&lt;/p&gt;&lt;code&gt;div:focus {
  border: 2px solid red;
}
&lt;/code&gt;&lt;p&gt;The results are shown below. When the HTML is first rendered, the elements have no styling, as shown in the first image. For the shadow root that does not have &lt;code&gt;shadowrootdelegatesfocus&lt;/code&gt; set you can click anywhere except the &lt;code&gt;&amp;lt;input&amp;gt;&lt;/code&gt; and the focus does not change (if you select the &lt;code&gt;&amp;lt;input&amp;gt;&lt;/code&gt; element it will look like the second image).&lt;/p&gt;&lt;p&gt;For the shadow root with &lt;code&gt;shadowrootdelegatesfocus&lt;/code&gt; set, clicking on the text (which is non-focusable) selects the &lt;code&gt;&amp;lt;input&amp;gt;&lt;/code&gt; element, as this is the first focusable element in the tree.
This also focuses the parent element as shown below.&lt;/p&gt;&lt;head rend="h2"&gt;Avoiding DocumentFragment pitfalls&lt;/head&gt;&lt;p&gt;When a &lt;code&gt;DocumentFragment&lt;/code&gt; value is passed, &lt;code&gt;Node.appendChild&lt;/code&gt; and similar methods move only the child nodes of that value into the target node. Therefore, it is usually preferable to attach event handlers to the children of a &lt;code&gt;DocumentFragment&lt;/code&gt;, rather than to the &lt;code&gt;DocumentFragment&lt;/code&gt; itself.&lt;/p&gt;&lt;p&gt;Consider the following HTML and JavaScript:&lt;/p&gt;&lt;head rend="h3"&gt;HTML&lt;/head&gt;&lt;code&gt;&amp;lt;div id="container"&amp;gt;&amp;lt;/div&amp;gt;

&amp;lt;template id="template"&amp;gt;
  &amp;lt;div&amp;gt;Click me&amp;lt;/div&amp;gt;
&amp;lt;/template&amp;gt;
&lt;/code&gt;&lt;head rend="h3"&gt;JavaScript&lt;/head&gt;&lt;code&gt;const container = document.getElementById("container");
const template = document.getElementById("template");

function clickHandler(event) {
  event.target.append(" â Clicked this div");
}

const firstClone = template.content.cloneNode(true);
firstClone.addEventListener("click", clickHandler);
container.appendChild(firstClone);

const secondClone = template.content.cloneNode(true);
secondClone.children[0].addEventListener("click", clickHandler);
container.appendChild(secondClone);
&lt;/code&gt;&lt;head rend="h3"&gt;Result&lt;/head&gt;&lt;p&gt;Since &lt;code&gt;firstClone&lt;/code&gt; is a &lt;code&gt;DocumentFragment&lt;/code&gt;, only its children are added to &lt;code&gt;container&lt;/code&gt; when &lt;code&gt;appendChild&lt;/code&gt; is called; the event handlers of &lt;code&gt;firstClone&lt;/code&gt; are not copied. In contrast, because an event handler is added to the first child node of &lt;code&gt;secondClone&lt;/code&gt;, the event handler is copied when &lt;code&gt;appendChild&lt;/code&gt; is called, and clicking on it works as one would expect.&lt;/p&gt;&lt;head rend="h2"&gt;Technical summary&lt;/head&gt;&lt;table&gt;&lt;row span="2"&gt;&lt;cell role="head"&gt;Content categories&lt;/cell&gt;&lt;cell&gt;Metadata content, flow content, phrasing content, script-supporting element&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Permitted content&lt;/cell&gt;&lt;cell&gt;Nothing (see Usage notes)&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Tag omission&lt;/cell&gt;&lt;cell&gt;None, both the starting and ending tag are mandatory.&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Permitted parents&lt;/cell&gt;&lt;cell&gt; Any element that accepts metadata content, phrasing content, or script-supporting elements. Also allowed as a child of a &lt;code&gt;&amp;lt;colgroup&amp;gt;&lt;/code&gt;
        element that does not have a
        &lt;code&gt;span&lt;/code&gt; attribute.
      &lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Implicit ARIA role&lt;/cell&gt;&lt;cell&gt;No corresponding role&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Permitted ARIA roles&lt;/cell&gt;&lt;cell&gt;No &lt;code&gt;role&lt;/code&gt; permitted&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;DOM interface&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;HTMLTemplateElement&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;head rend="h2"&gt;Specifications&lt;/head&gt;&lt;table&gt;&lt;row&gt;&lt;cell role="head"&gt;Specification&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;HTML&amp;gt;&lt;p&gt;# the-template-element&amp;gt;&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;head rend="h2"&gt;Browser compatibility&lt;/head&gt;&lt;p&gt;Loadingâ¦&lt;/p&gt;&lt;head rend="h2"&gt;See also&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;part&lt;/code&gt;and&lt;code&gt;exportparts&lt;/code&gt;HTML attributes&lt;/item&gt;&lt;item&gt;&lt;code&gt;&amp;lt;slot&amp;gt;&lt;/code&gt;HTML element&lt;/item&gt;&lt;item&gt;&lt;code&gt;:has-slotted&lt;/code&gt;,&lt;code&gt;:host&lt;/code&gt;,&lt;code&gt;:host()&lt;/code&gt;, and&lt;code&gt;:host-context()&lt;/code&gt;CSS pseudo-classes&lt;/item&gt;&lt;item&gt;&lt;code&gt;::part&lt;/code&gt;and&lt;code&gt;::slotted&lt;/code&gt;CSS pseudo-elements&lt;/item&gt;&lt;item&gt;&lt;code&gt;ShadowRoot&lt;/code&gt;interface&lt;/item&gt;&lt;item&gt;Using templates and slots&lt;/item&gt;&lt;item&gt;CSS scoping module&lt;/item&gt;&lt;item&gt;Declarative Shadow DOM (with html) in Using Shadow DOM&lt;/item&gt;&lt;item&gt;Declarative shadow DOM on web.dev (2023)&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45106049</guid></item><item><title>Introduction to Ada: a project-based exploration with rosettas</title><link>https://blog.adacore.com/introduction-to-ada-a-project-based-exploration-with-rosettas</link><description>&lt;doc fingerprint="a48888018a6d1bd8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Introduction to Ada: a project-based exploration with rosettas&lt;/head&gt;
    &lt;head rend="h2"&gt;by Romain Gora –&lt;/head&gt;
    &lt;head rend="h2"&gt;Context&lt;/head&gt;
    &lt;p&gt;This practical walkthrough, designed as a short tutorial, was created upon joining AdaCore as a Field Engineer. In this new role, Iâll be working directly with customers to help them succeed with Ada. Although I was first introduced to the language nearly two decades ago, this new position inspired me to revisit its fundamentals, and I used the excellent https://learn.adacore.com portal as a quick refresher.&lt;/p&gt;
    &lt;p&gt;While that platform takes a concept-based approach, I chose to complement it with a project-based method by developing a small, end-to-end Ada program that generates animated rosettas in the form of SVG files. These are technically hypotrochoid curves, producing patterns that many will recognize from the classic Spirographâ¢ toy.&lt;/p&gt;
    &lt;p&gt;In this walkthrough, weâll show that Ada can be fun and easy to learn. Although the language is famous for safety-critical systems, we will use it as a modern, general-purpose programming language and try out some new features from Ada 2022 along the way.&lt;/p&gt;
    &lt;p&gt;Let's dive in!&lt;/p&gt;
    &lt;head rend="h2"&gt;A brief note on Ada&lt;/head&gt;
    &lt;p&gt;This section leans a bit more into background context, with a slightly encyclopedic flavor that's especially useful for readers new to Ada. If you're already familiar with Adaâs history and principles, feel free to joyfully skip ahead to the next section!&lt;/p&gt;
    &lt;p&gt;Ada was created in the late 1970s after a call from the U.S. Department of Defense to unify its fragmented software landscape. The winning proposal became Ada, a language that's been literally battle-tested (!) and built on a deeply thought-out design that continues to evolve today.&lt;/p&gt;
    &lt;p&gt;While Ada is absolutely a general-purpose programming language, it has carved out a strong niche in fields where software correctness and reliability are mission-critical:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Embedded and real-time systems&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Aerospace and defense&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Rail, automotive, and aviation&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Any system where failure is not just a bug, but a risk&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Its strict compile-time checks, safety features, and clear structure make it particularly appealing when you need your software to be dependable from day one and still maintainable ten years later.&lt;/p&gt;
    &lt;p&gt;Ada's design is grounded in a strong and principled philosophy:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Readability over conciseness: Ada favors clarity. It avoids symbols and abbreviations in favor of full keywords, making the language more accessible and less error-prone.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Strong and explicit typing: It is extremely easy to declare new types in Ada, with precise constraints, which makes it much harder to accidentally misuse data. While some functional languages share this strong typing discipline, Ada stands out by requiring the programmer to be very explicit. It uses little to no type inference.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Explicit is better than implicit: Unlike many modern languages that prioritize convenience, Ada leans heavily toward precision. Most types must be explicitly named and matched.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Defined semantics and minimal undefined behavior: Ada offers a level of predictability and safety unmatched in many languages. This makes it a strong choice not only for safety-critical systems, but also for codebases where long-term maintenance, verifiability, and correctness are essential.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Compiler as a partner: Ada compilers are strict by design, not to frustrate, but to help the programmer write clearer, more correct code. This philosophy encourages the developer to communicate intent clearly, both to the compiler and to future readers.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;How the program works&lt;/head&gt;
    &lt;p&gt;Sometimes the best way to figure out how something works is to start at the end. Let's do that!&lt;lb/&gt;In this tutorial, we'll walk through how the program produces its final output â a rosetta SVG file â and use that as a way to explore how Ada's structure, type system, and tooling come together.&lt;lb/&gt;This is a simple command-line program that generates an SVG file. You run it like this:&lt;/p&gt;
    &lt;p&gt;./bin/rosetta&lt;/p&gt;
    &lt;p&gt;The idea was to create something visual: learning is more fun when there's an immediate, satisfying result and generating rosettas fits that goal perfectly.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;Why SVG? Because it's a lightweight and portable vector format that you can view in any modern browser. I wanted to avoid relying on a graphical library, which would have added extra weight and gone beyond the scope of this approach. And while XML isn't the most pleasant format to write by hand, generating it from code is straightforward and gives a surprisingly clean result.&lt;/p&gt;
    &lt;head rend="h2"&gt;Tooling &amp;amp; setup&lt;/head&gt;
    &lt;p&gt;To build and run the project, I used Alire, the Ada package manager. It plays a similar role in the Ada ecosystem as Cargo does for Rust or npm for JavaScript. It's well-documented, and while we won't dive deep into it here, it's a solid and accessible way to manage Ada projects. I encourage anyone curious to get it from https://alire.ada.dev. Interestingly, "Alire" is also the French expression for "Ã lire" â which means "for reading." A fitting name for a tool that supports a language so focused on clarity and readability!&lt;/p&gt;
    &lt;p&gt;Once Alire is set up, the natural next step is choosing where to write the code. You have two excellent options for your development environment. For a dedicated experience, you can download the latest release of GNAT Studio from its GitHub repository. If you prefer a more general-purpose editor, you can install the official Ada &amp;amp; SPARK for Visual Studio Code extension from AdaCore.&lt;/p&gt;
    &lt;p&gt;As a new learner, I also kept https://learn.adacore.com close at hand. Itâs a particularly clear and comprehensive resource â and I especially appreciated being able to download the ebook version and read through it on my phone.&lt;/p&gt;
    &lt;head rend="h2"&gt;Entry point&lt;/head&gt;
    &lt;code&gt;with Rosetta_Renderer;

procedure Main is
begin
   Rosetta_Renderer.Put_SVG_Rosettas;
end Main;&lt;/code&gt;
    &lt;p&gt;There are several interesting things to notice right away:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;The&lt;/p&gt;&lt;code&gt;with&lt;/code&gt;clause is not a preprocessor directive like in C or C++. Itâs a compiled, checked reference to another package â a reliable and explicit way to express a dependency. This eliminates entire classes of bugs related to fragile&lt;code&gt;#include&lt;/code&gt;chains, macro collisions, or dependency order issues.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;This&lt;/p&gt;&lt;code&gt;procedure&lt;/code&gt;is not a&lt;code&gt;function&lt;/code&gt;: it does not return a value. In Ada, procedures are used to perform actions (like printing or modifying state), and functions are used to compute and query values.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The syntax is designed for readability. Youâll find&lt;/p&gt;&lt;code&gt;begin&lt;/code&gt;and&lt;code&gt;end&lt;/code&gt;here instead of&lt;code&gt;{}&lt;/code&gt;as in C/C++, reinforcing Adaâs philosophy that clarity matters more than brevity.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Put_SVG_Rosettas&lt;/code&gt;uses the idiomatic Pascal_Snake_Case naming style. This reflects a common Ada convention and avoids acronyms or compressed identifiers in favor of more descriptive names.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The entry point is minimal but meaningful: it simply calls a procedure which generates the output we'll explore in the next sections.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Geometry and computation (package Rosetta)&lt;/head&gt;
    &lt;p&gt;In Ada, a package is a modular unit that groups related types, procedures, and functions. Following the convention from GNAT (the Ada compiler, part of the GNU Compiler Collection, fondly known as GCC), each package has a specification file (with the&lt;code&gt; .ads&lt;/code&gt; extension â short for Ada Specification) and an implementation file (with the &lt;code&gt;.adb&lt;/code&gt; extension â short for Ada Body). This clear and enforced split means you always know where to find interface definitions versus their implementation.&lt;/p&gt;
    &lt;p&gt;The following code is the package specification for Rosetta. It defines the data types for the rosetta shapes and declares the public interface of operations available to manipulate them.&lt;/p&gt;
    &lt;code&gt;with Ada.Strings.Text_Buffers;

package Rosetta is

   --  A mathematical description of a rosetta (specifically, a hypotrochoid).
   --  formed by tracing a point attached to a circle rolling inside another circle.
   type Hypotrochoid is record
      Outer_Radius : Float;     --  Radius of the fixed outer circle.
      Inner_Radius : Float;     --  Radius of the rolling inner circle.
      Pen_Offset   : Float;     --  From the center of the inner circle to the drawing point.
      Steps        : Positive;  --  Number of steps (points) used to approximate the curve.
   end record;

   --  A 2D coordinate in Cartesian space.
   type Coordinate is record
      X_Coord, Y_Coord : Float;
   end record
     with Put_Image =&amp;gt; Put_Image_Coordinate;
   
   --  Redefines the 'Image attribute for Coordinate.
   procedure Put_Image_Coordinate 
     (Output : in out Ada.Strings.Text_Buffers.Root_Buffer_Type'Class; 
      Value  : Coordinate);

   --  A type for an unconstrained array of 2D points forming a curve.
   --  The actual bounds are set when an array object of this type is declared.
   type Coordinate_Array is array (Natural range &amp;lt;&amp;gt;) of Coordinate;

   --  Computes the coordinates of the rosetta curve defined by Curve (a hypotrochoid).
   --  Returns a centered array of coordinates.
   function Compute_Points (Curve : Hypotrochoid) return Coordinate_Array;

end Rosetta;&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;Rosetta&lt;/code&gt; package is responsible for all the math and curve computation. It defines:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;Hypotrochoid&lt;/code&gt;, type describing the geometry of the rosetta&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Coordinate&lt;/code&gt;, type representing points in 2D space&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Coordinate_Array&lt;/code&gt;, type holding a series of such points&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Compute_Points&lt;/code&gt;, function which calculates all the points of the curve based on the&lt;code&gt;Hypotrochoid&lt;/code&gt;parameters and recenters them around the origin&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This package is focused solely on computation. It doesnât concern itself with how the result is rendered.&lt;/p&gt;
    &lt;p&gt;Fun fact for the curious: when the rolling circle rolls outside the fixed circle rather than inside, the resulting curve is called an epitrochoid.&lt;/p&gt;
    &lt;p&gt;In Ada, a &lt;code&gt;record&lt;/code&gt; is similar to a &lt;code&gt;struct&lt;/code&gt; in C or a class with only data members in other languages. It's a user-defined type composed of named components, making it ideal for modeling structured data.&lt;/p&gt;
    &lt;p&gt;Using a record for &lt;code&gt;Hypotrochoid&lt;/code&gt; was particularly appropriate: it allows grouping all geometric parameters (outer radius, inner radius, pen offset, and steps) into a single, cohesive unit. This improves readability and maintainability. The compiler enforces correctness by ensuring all required values are present and of the expected type â reinforcing Adaâs philosophy of clarity and safety.&lt;/p&gt;
    &lt;p&gt;The type &lt;code&gt;Coordinate_Array&lt;/code&gt; is an unconstrained array type that holds a range of &lt;code&gt;Coordinate&lt;/code&gt; records. In this context, âunconstrainedâ simply means that we donât define the arrayâs size when we declare the type. Instead, the size is defined when we declare an object of that type. This gives us the flexibility to use this type for a variety of shapes.&lt;/p&gt;
    &lt;p&gt;You may also notice the use of &lt;code&gt;Natural range &amp;lt;&amp;gt;. Natural&lt;/code&gt; is a predefined subtype of Integer that only allows non-negative values. And yes, I mean subtype: Adaâs powerful type system allows you to take an existing type and create a more specific, constrained version of it.&lt;/p&gt;
    &lt;head rend="h3"&gt;Highlights from the .adb file&lt;/head&gt;
    &lt;p&gt;Here are a few notable aspects from the implementation (&lt;code&gt;rosetta.adb&lt;/code&gt;) that illustrate Adaâs strengths for writing safe, clear, and structured code:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Declarative and modular design: Both&lt;/p&gt;&lt;code&gt;Generate_Point&lt;/code&gt;and&lt;code&gt;Compute_Points&lt;/code&gt;are pure functions that operate only on their inputs. Their behavior is fully deterministic and encapsulated.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Safe bounds and array handling: The&lt;/p&gt;&lt;code&gt;Points&lt;/code&gt;array is statically bounded using&lt;code&gt;(0 .. Curve.Steps)&lt;/code&gt;, and its access is strictly safe. The compiler ensures that any index outside this range would raise an error at runtime. This immediate error is a feature, not a bug. It stops silent memory corruption and security flaws by ensuring the program fails predictably and safely at the source of the problem.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Use of constants for robustness: Variables such as&lt;/p&gt;&lt;code&gt;Pi&lt;/code&gt;,&lt;code&gt;R_Diff&lt;/code&gt;, and Ratio are declared as constant, enforcing immutability. This helps ensure clarity of intent and prevents accidental reassignment, a common source of subtle bugs in more permissive languages. Ada encourages this explicit declaration style, promoting safer code.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;with Ada.Numerics;
with Ada.Numerics.Elementary_Functions;

use Ada.Numerics;
use Ada.Numerics.Elementary_Functions;

package body Rosetta is

   --  Computes a single point on the hypotrochoid curve for a given angle Theta.
   --  Uses the standard parametric equation of a hypotrochoid.
   function Generate_Point (Curve : Hypotrochoid; Theta : Float) return Coordinate is
      R_Diff : constant Float := Curve.Outer_Radius - Curve.Inner_Radius;
      Ratio  : constant Float := R_Diff / Curve.Inner_Radius;
   begin
      return (
              X_Coord =&amp;gt; R_Diff * Cos (Theta) + Curve.Pen_Offset * Cos (Ratio * Theta),
              Y_Coord =&amp;gt; R_Diff * Sin (Theta) - Curve.Pen_Offset * Sin (Ratio * Theta)
             );
   end Generate_Point;

   --  Computes all the points of the hypotrochoid curve and recenters them.
   --  The result is an array of coordinates centered around the origin.
   function Compute_Points (Curve : Hypotrochoid) return Coordinate_Array is
      Points : Coordinate_Array (0 .. Curve.Steps);
      Max_X  : Float := Float'First;
      Min_X  : Float := Float'Last;
      Max_Y  : Float := Float'First;
      Min_Y  : Float := Float'Last;
      Offset : Coordinate;
   begin
      --  Computes raw points and updates the bounding box extents.
      for J in 0 .. Curve.Steps loop
         declare
            Theta : constant Float := 2.0 * Pi * Float (J) / Float (Curve.Steps) * 50.0;
            P     : constant Coordinate := Generate_Point (Curve, Theta);
         begin
            Points (J) := P;
            Max_X := Float'Max (Max_X, P.X_Coord);
            Min_X := Float'Min (Min_X, P.X_Coord);
            Max_Y := Float'Max (Max_Y, P.Y_Coord);
            Min_Y := Float'Min (Min_Y, P.Y_Coord);
         end;
      end loop;

      --  Computes the center offset based on the bounding box.
      Offset := (
                 X_Coord =&amp;gt; (Max_X + Min_X) / 2.0,
                 Y_Coord =&amp;gt; (Max_Y + Min_Y) / 2.0
                );

      --  Recenters all points by subtracting the center offset.
      for J in Points'Range loop
         Points (J).X_Coord := @ - Offset.X_Coord;
         Points (J).Y_Coord := @ - Offset.Y_Coord;
      end loop;

      return Points;
   end Compute_Points;
   
   --  Redefines the 'Image attribute for Coordinate.
   procedure Put_Image_Coordinate
     (Output : in out Ada.Strings.Text_Buffers.Root_Buffer_Type'Class;
      Value  : Coordinate)
   is   
      X_Text : constant String := Float'Image (Value.X_Coord);
      Y_Text : constant String := Float'Image (Value.Y_Coord);
   begin
      Output.Put (X_Text &amp;amp; "," &amp;amp; Y_Text);
   end Put_Image_Coordinate;

end Rosetta;&lt;/code&gt;
    &lt;head rend="h2"&gt;On style: strict and predictable (and satisfying!)&lt;/head&gt;
    &lt;p&gt;Ada is one of those rare languages that not only compiles your code but asks you to write it properly. With the compiler switch -gnaty, you can enforce a comprehensive set of style rules, many of which are stricter than what you'd see in most languages.&lt;/p&gt;
    &lt;p&gt;This includes things like:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;No trailing whitespace at the end of lines&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;No consecutive blank lines&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Proper indentation and alignment of keywords and parameters&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;A space before â(â when calling a procedure or function&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Consistent casing&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;At first, this can feel surprisingly strict. But once you get used to it, the benefits are clear: it helps enforce a consistent and clean coding style across a codebase. That in turn improves readability, reduces ambiguity, and leads to more maintainable programs.&lt;/p&gt;
    &lt;p&gt;Rather than leaving formatting up to personal taste or optional linter tools, Ada integrates this attention to detail into the compilation process itself. The result is not only more elegant: it's genuinely satisfying. And you can do even more with GNATcheck and GNATformat but itâs outside of the scope of this post.&lt;/p&gt;
    &lt;p&gt;Outputting to SVG (package &lt;code&gt;Rosetta_Renderer&lt;/code&gt;)&lt;/p&gt;
    &lt;p&gt;The Rosetta_Renderer package is responsible for producing the SVG output. It defines a single high-level procedure:&lt;/p&gt;
    &lt;code&gt;package Rosetta_Renderer is

   --  Renders a predefined set of rosettas into an SVG output.
   procedure Put_SVG_Rosettas;

end Rosetta_Renderer;&lt;/code&gt;
    &lt;p&gt;This procedure generates an SVG file directly. It takes care of formatting the SVG structure (header, shapes, animations, and footer) and calls into the math logic defined in the &lt;code&gt;Rosetta &lt;/code&gt;package to generate point data.&lt;/p&gt;
    &lt;p&gt;This separation of concerns is deliberate and beneficial: the math logic doesnât need to know anything about SVG, and the renderer doesnât care how the coordinates were generated.&lt;/p&gt;
    &lt;p&gt;Now let's talk about the body of the package... but not for long. We're keeping it brief because its core is essentially the SVG plumbing required to draw and animate the curves, so we'll skip the fine details. And for those who enjoy seeing how the sausage is made, I've made the fully commented source code available for you right here.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;The procedure &lt;code&gt;Put_Path&lt;/code&gt; handles the creation of the SVG path. Its main job is to take an array of coordinates and write the corresponding command string to the &lt;code&gt;d &lt;/code&gt;attribute of a&lt;code&gt; &amp;lt;path&amp;gt;&lt;/code&gt; element. In SVG, this attribute defines the geometry of the shape. The code iterates over each coordinate, using &lt;code&gt;M &lt;/code&gt;(moveto) for the first point and &lt;code&gt;L&lt;/code&gt; (lineto) for all the others to draw the connecting lines.&lt;/p&gt;
    &lt;code&gt;--  Puts coordinates to a single SVG path string ("d" attribute).
   procedure Put_Path (Stream : File_Type; Points : Coordinate_Array) is
   begin
      Put (Stream, "M "); -- Moves the pen without drawing.
      for J in Points'Range loop
         declare 
            Coord_Text : constant String := Coordinate'Image (Points (J));
         begin   
            Put (Stream, Coord_Text);
            if J &amp;lt; Points'Last then
               Put (Stream, " L "); --  Draws a line.
            end if;
         end;
      end loop;
   end Put_Path;&lt;/code&gt;
    &lt;head rend="h2"&gt;Afterword&lt;/head&gt;
    &lt;p&gt;This small project was an enjoyable and useful way to get back into Ada. It helped me reconnect with the languageâs main strengths and refamiliarize myself with its tools and design. It was a great reminder of how fun, easy to learn, and remarkably modern Ada can be, especially for developers focused on building robust, maintainable, and efficient software.&lt;/p&gt;
    &lt;p&gt;I hope this short walkthrough gives a good idea of that feeling, whether you're already into Ada or just starting to explore it.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45106314</guid></item><item><title>Physically based rendering from first principles</title><link>https://imadr.me/pbr/</link><description>&lt;doc fingerprint="4fae1661081d071f"&gt;
  &lt;main&gt;
    &lt;p&gt;In this interactive article, we will explore the physical phenomena that create light and the fundamental laws governing its interaction with matter. We will learn how our human eyes capture light and how our brains interpret it as visual information. We will then model approximations of these physical interactions and learn how to create physically realistic renderings of various materials.&lt;/p&gt;
    &lt;p&gt;We are all familiar with light: it’s the thing that allows us to see the world, distinguish colors and textures, and keeps the universe from being a dark, lifeless void. But precisely defining what light is has proven to be a tricky question.&lt;/p&gt;
    &lt;p&gt;Throughout history, many philosophers (and later, physicists) studied light in an effort to demystify its nature. Some ancient Greeks considered it to be one of the four fundamental elements that composed the universe: beams of fire emanating from our eyes.&lt;/p&gt;
    &lt;p&gt;Descartes proposed that light behaved like waves, while Newton thought that it consisted of tiny particles of matter called corpuscles.&lt;/p&gt;
    &lt;p&gt;Each of these more or less scientific theories explained some aspects of light's behavior, but none could account for all of them in a single, unified framework. That was until the 1920s when physicists came up with quantum electrodynamics. This theory is, as of now, the most accurate way to describe every interaction of light and matter.&lt;/p&gt;
    &lt;p&gt;You can hover the diagram below to see which light's phenomena can be explained using each model:&lt;/p&gt;
    &lt;p&gt;For the purpose of computer graphics, the ray optics model is accurate enough at simulating light interactions. But for the sake of scientific curiosity, we will explore some aspects of the other models, starting with electromagnetism.&lt;/p&gt;
    &lt;p&gt;One of the fundamental properties of matter is the electric charge, and it comes in two types: positive and negative.&lt;lb/&gt;Charges determine how particles interact: charges of the same type repel each other, while opposite charges attract.&lt;/p&gt;
    &lt;p&gt;The amount of force affecting two charged particles is calculated using Coulomb's law:&lt;/p&gt;
    &lt;p&gt;Where is a constant, and are the quantities of each charge, and is the distance between them.&lt;/p&gt;
    &lt;p&gt;You can drag around these charges to see how the electric force affects them:&lt;/p&gt;
    &lt;p&gt;Every charge contributes to the electric field, it represents the force exerted on other charges at each point in space. We can visualize the electric field with a or a :&lt;/p&gt;
    &lt;p&gt;Another way to visualize the electric field is by coloring each point in space with a color gradient representing the force experienced by a small charge at that point:&lt;/p&gt;
    &lt;p&gt;Imagine a moving object carrying a positive electric charge placed under a cable carrying an electrical current.&lt;lb/&gt; From , the object and the negative charges in the wire are moving, and since the positive and negative charges in the cable compensate each other, the object doesn't experience any force.&lt;/p&gt;
    &lt;p&gt;In the , it appears to be static alongside the negative charges, while the positive charges are moving to the left, and the object still doesn't get affected by any force.&lt;/p&gt;
    &lt;p&gt;Now if we take into account , the moving charges in the wire appear "stretched" due to relativistic effects, causing a change in the distribution of charges. This stretching leads to a repulsive force between the object and the wire, which we interpret as magnetism.&lt;/p&gt;
    &lt;p&gt;Maxwell's equations describe how electric and magnetic fields are created and interact with each other. We will focus on the third and fourth equations.&lt;/p&gt;
    &lt;p&gt;Maxwell's third equation, known as Faraday's law of induction, shows how changing magnetic fields can generate electric currents.&lt;lb/&gt;An example of this is moving a magnet inside a coil, which induces an electric current in the wire due to the changing magnetic field.&lt;/p&gt;
    &lt;p&gt;This is the principle behind electric generators: Mechanical energy (like the flow of steam) is used to move magnets inside coils (a turbine), converting it to electrical energy through electromagnetic induction.&lt;/p&gt;
    &lt;p&gt;By moving the magnet left and right, we can see the voltmeter picking up a current and the electric charges in the coil moving back and forth:&lt;/p&gt;
    &lt;p&gt;Maxwell's fourth and final equation, Ampère's Law, illustrates how electric currents (moving charges) produce magnetic fields around them. This is the basis of how electromagnets function:&lt;/p&gt;
    &lt;p&gt;Together, these laws demonstrate how electric and magnetic fields are interdependent. A changing magnetic field generates an electric field, and a changing electric field generates a magnetic field.&lt;/p&gt;
    &lt;p&gt;This continuous cycle enables self-sustaining, self-propagating electromagnetic waves, which can travel through space without requiring a medium.&lt;/p&gt;
    &lt;p&gt;Electromagnetic radiation consists of waves created by synchronized oscillations of electric and magnetic fields. These waves travel at the speed of light in a vacuum.&lt;/p&gt;
    &lt;p&gt;The amplitude of a wave determines the maximum strength of its electric or magnetic field. It represents the wave's intensity or "brightness". In quantum terms, a higher amplitude corresponds to a greater number of photons.&lt;/p&gt;
    &lt;p&gt;The frequency of a wave determines the energy of the individual photons that compose it. Higher frequencies correspond to shorter wavelengths and more energetic photons.&lt;/p&gt;
    &lt;p&gt;When the wavelength falls between approximately 400 nm and 700 nm, the human eye perceives it as visible light.&lt;/p&gt;
    &lt;p&gt;While other wavelengths are invisible to the human eye, many are quite familiar in everyday life. For example, microwaves are used for Wi-Fi and cooking, X-rays are used in medical imaging, and radio waves enable communication.&lt;/p&gt;
    &lt;p&gt;Some insects, like bees, can see ultraviolet light, which helps them locate flowers by revealing hidden patterns and markings created by specialized pigments, such as flavonoids, that reflect UV wavelengths.&lt;/p&gt;
    &lt;p&gt;On the other end of the spectrum, gamma rays are highly energetic and can be dangerous, they are generated by radioactive decay, nuclear bombs, and space phenomena like supernovas.&lt;/p&gt;
    &lt;p&gt;There are many ways for light to be generated, the two most common ones we encounter everyday are incandescence and electroluminescence.&lt;/p&gt;
    &lt;p&gt;Incandescence is the process by which a material emits visible light due to high temperature. It is how incandescent lightbulbs and the sun generates light.&lt;/p&gt;
    &lt;p&gt;An incandescent lightbulb produces light through the heating of a filament until it starts glowing. The filament is made of tungsten, an element with a high melting point, high durability, and a positive temperature coefficient of resistance, which means its resistance increases with temperature.&lt;/p&gt;
    &lt;p&gt;When we increase the current flowing through the filament, it starts heating up (Due to Joule heating), which increases the resistance in turn causing more heat to get dissipated. This feedback loop stabilizes at around 2500°C.&lt;/p&gt;
    &lt;p&gt;This heat makes the electrons in the filament wiggle and collide with each other, releasing photons in the process. This radiation can be approximated as Black-body radiation.&lt;/p&gt;
    &lt;p&gt;The Sun also generates light by incandescence, but unlike the lightbulb's filament glowing via Joule heating, the Sun’s energy is produced by nuclear fusion in the core, where hydrogen nuclei fuse to form helium and release photons as gamma rays.&lt;/p&gt;
    &lt;p&gt;These photons travel from the core through the radiative zone, getting absorbed and remitted countless times while shifting to longer wavelengths. After hundreds of thousands of years of bouncing around, the photons make it to the surface of the Sun, called the photosphere, where they get radiated away.&lt;/p&gt;
    &lt;p&gt;Most (~49%) of the sun's emissions are in infrared, which is responsible for the heat we get on Earth, ~43% is visible light and the ~8% left is ultraviolet.&lt;/p&gt;
    &lt;p&gt;An interesting fact is that illustrations of the Sun's cross-section typically depict the interior with bright orange or yellow colors. However, if we could actually see a cross-section of the Sun, even the hottest regions like the core would appear dark and opaque, because the radiation generated there isn't in the visible spectrum.&lt;/p&gt;
    &lt;p&gt;Another way to generate light is by electroluminescence, this is the phenomenon that powers LEDs&lt;/p&gt;
    &lt;p&gt;The main component of a light-emitting diode is a semiconductor chip. Semiconductors are materials whose electrical conductivity can be modified by mixing them with impurities in a process known as doping.&lt;/p&gt;
    &lt;p&gt;Depending on the type of impurity (called the dopant) used in the mix, the semiconductor can be turned into either an n-type, which has extra electrons freely moving around, or a p-type, which has a lack of electrons and instead carrying an electron "hole", also moving around and acting as a positive charge.&lt;/p&gt;
    &lt;p&gt;When you stick a p-type and an n-type semiconductor side by side, they form a p-n junction. When a current flows through the junction, the electrons and the holes recombine and emit photons in the process.&lt;/p&gt;
    &lt;p&gt;Aside from incandescence and electroluminescence, which are the two most common sources of light we encounter in everyday life, light can come from other places. Some materials glow when exposed to ultraviolet radiation, others absorb that radiation and re-emit it after some time. Some animals like fireflies use special enzymes to produce light. You can read this page to learn more about other sources of luminescence.&lt;/p&gt;
    &lt;p&gt;In the previous chapter, we examined the nature of light and the various methods by which it can be emitted, we will now focus on how it interacts with matter.&lt;/p&gt;
    &lt;p&gt;When a photon hits a material, it interacts with the electrons in the atoms and molecules of that material, then two things can happen, it can either be absorbed or scattered.&lt;/p&gt;
    &lt;p&gt;The electrons occupy atomic orbitals: regions around the nucleus of the atom where an electron is most likely to be found. A higher orbital corresponds to a higher energy level of the electron.&lt;/p&gt;
    &lt;p&gt;If the photon has the energy needed to excite the electron to a higher energy level, the photon can be absorbed. Eventually the electron returns to a lower level and releases the energy as heat.&lt;/p&gt;
    &lt;p&gt;If the photon does not get absorbed, its electric field will make the electrons oscillate in return and generate secondary waves that interfere constructively and destructively with the photon waves in complicated ways.&lt;/p&gt;
    &lt;p&gt;We can simplify these complicated interactions by making a few assumptions about the material:&lt;/p&gt;
    &lt;p&gt;We can use Maxwell's equations to show that such a perfect flat material splits the incoming light waves into two parts: reflected and refracted.&lt;/p&gt;
    &lt;p&gt;The angle of reflection is equal to the angle of incidence relative to the normal of the surface, as per the law of reflection:&lt;/p&gt;
    &lt;p&gt;The angle of refraction is determined by how much slower (or faster) light travels through the material, that speed is defined by the index of refraction, and the angle is calculated using Snell's law:&lt;/p&gt;
    &lt;p&gt;At a and refractive indices light is no longer refracted and seems to disappear.&lt;/p&gt;
    &lt;p&gt;The amount of light that is reflected and refracted is calculated using Fresnel equations.&lt;/p&gt;
    &lt;p&gt;However, computing the full Fresnel equation in real time can be slow, so in 1994 Christophe Schlick came up with an approximation.&lt;/p&gt;
    &lt;p&gt;First we compute the reflectance at zero degrees from the normal:&lt;/p&gt;
    &lt;p&gt;Then we plug in the approximation function for the reflectance:&lt;/p&gt;
    &lt;p&gt;The transmitted (or refracted) light simply becomes:&lt;/p&gt;
    &lt;p&gt;If we try the where the refracted ray disappeared, we can now see it getting reflected back inside the medium, this is called total internal reflection.&lt;/p&gt;
    &lt;p&gt;Total internal reflection gives rise to an interesting phenomenon called Snell's window. If you dive underwater and look up, the light above the surface is refracted through a circular window 96 degrees wide, and everything outside is a reflection of the bottom of the water.&lt;/p&gt;
    &lt;p&gt;This is what it looks underwater:&lt;/p&gt;
    &lt;p&gt;Like we saw earlier, we can explain light reflecting and refracting using different models, depending on the size of the surface irregularities we are considering.&lt;/p&gt;
    &lt;p&gt;For example, wave optics explains light interacting with matter as light waves diffracting on the surface nanogeometry.&lt;/p&gt;
    &lt;p&gt;If we zoom out a bit and use ray optics, we consider light as straight line rays that reflect and refract on the surface microgeometry. With this model we can use the optical laws we described earlier: law of reflection, Snell's law, Fresnel equations.&lt;/p&gt;
    &lt;p&gt;Now for rendering, we can zoom out even further and consider one pixel at a time, each pixel contains many microgeometry surfaces that we call a microfacet. We can use a statistical average of the microfacets in a pixel to simulate the appearance of the surface at that pixel, without considering each individual microfacet which would be unfeasible in real time.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Size&lt;/cell&gt;
        &lt;cell role="head"&gt;Model&lt;/cell&gt;
        &lt;cell role="head"&gt;Phenomenon&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Nanogeometry&lt;/cell&gt;
        &lt;cell&gt;Wave optics&lt;/cell&gt;
        &lt;cell&gt;Light diffraction&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Microgeometry&lt;/cell&gt;
        &lt;cell&gt;Ray optics&lt;/cell&gt;
        &lt;cell&gt;Reflection/refraction, change in local normal&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Macrogeometry&lt;/cell&gt;
        &lt;cell&gt;BRDF&lt;/cell&gt;
        &lt;cell&gt;Statistical average over a pixel, wider cone -&amp;gt; more roughness&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Here we can see a microgeometry surface, changing the roughness makes it more bumpy and the microfacets normals aren't aligned anymore:&lt;/p&gt;
    &lt;p&gt;At the macrogeometry level, a bigger roughness value means light rays have a wider cone where they can spread out. The function that describes this cone is called bidirectional reflectance distribution function, we will discuss it in the next chapter.&lt;/p&gt;
    &lt;p&gt;In our microfacet model, we distinguish two types of materials by the nature of their interaction with light: metals and non-metals.&lt;/p&gt;
    &lt;p&gt;Metals have a sea of free electrons that absorb light very easily when the photons enter a few nanometers deep inside the surface. The light that isn't absorbed is reflected equally across the visible light spectrum, this is why metals have that distinct "silvery" gray color.&lt;/p&gt;
    &lt;p&gt;Notable exceptions are gold, copper, osmium and caesium.&lt;/p&gt;
    &lt;p&gt;Changing the roughness of a metal only changes its specular reflection, making it more or less mirror-like. But there is no diffuse reflection at all.&lt;/p&gt;
    &lt;p&gt;Also called dielectrics, these are materials that do not conduct electricity (insulators). They include plastic, wood, glass, water, diamond, air...&lt;/p&gt;
    &lt;p&gt;When a photon hits a dielectric material, it only gets absorbed if it's energy matches the electron's energy in the material. So light either gets reflected, and the specular reflection depends on the roughness of the surface.&lt;/p&gt;
    &lt;p&gt;The light can also get refracted inside the dielectric material, it bounces around and interacts with the pigments inside the material until it exits the surface, this is called diffuse reflection.&lt;/p&gt;
    &lt;p&gt;If we take the example of a red apple. When we shine a white light (which contains all visible wavelengths) on it, the apple's pigments (anthocyanins) absorb most of the wavelengths like violet, blue and green wavelengths, thus decreasing the intensity of those colors from the light. The remaining wavelengths, mostly red, gets scattered off the apple's surface making us perceive the apple as red.&lt;/p&gt;
    &lt;p&gt;We can characterize the incoming light by describing the amount of energy it carries at each wavelength using a function called the Spectral Power Distribution or SPD for short.&lt;/p&gt;
    &lt;p&gt;For example, below is the SPD for D65, a theoretical source of light standardized by The International Commission on Illumination (CIE). It represents the spectrum of average midday light in Western Europe or North America:&lt;/p&gt;
    &lt;p&gt;We can compare this SPD to AM0, which is the measured solar radiation in outer space before entering Earth's atmosphere. Notice the absence of a dip in the ultraviolet range:&lt;/p&gt;
    &lt;p&gt;And here is the SPD of a typical tungsten incandescent light:&lt;/p&gt;
    &lt;p&gt;The SPD shows us how much of each "color" a light is composed of. Another interesting function we can look at is called the spectral reflectance curve, which shows the fraction of incident light reflected by an object at each wavelength, effectivly representing the color of said object.&lt;/p&gt;
    &lt;p&gt;Going back to our apple example, since it reflects most of its light in the red wavelength, its spectral reflectance curve might look like this:&lt;/p&gt;
    &lt;p&gt;The light we see is the combination of the light spectral power distribution with the object spectral reflectance.&lt;/p&gt;
    &lt;p&gt;If we shine a light on our red apple, depending on the wavelengths of the light, the final color we see changes. A makes the apple appear red, because it's like multiplying the apple's color by one. We get the same result with a , because the apple reflects mostly in the red spectrum.&lt;lb/&gt;However if we shine a , besides the leaf, the rest of the apple doesn't reflect any light, thus appearing black.&lt;/p&gt;
    &lt;p&gt;On the top right you can see the SPD of the flashlight, under it the reflectance curve of the apple, and the resulting reflected light below it:&lt;/p&gt;
    &lt;p&gt;If we now add a banana and shine a , we can obviously tell the apple and the banana apart, one being red while the other is yellow.&lt;lb/&gt;But what happens when the light is ? Both objects appear reddish to our eyes, because the banana doesn't have any green light to reflect, making it lose its yellow color. This phenomenon is called metamerism.&lt;/p&gt;
    &lt;p&gt;You can display the or the :&lt;/p&gt;
    &lt;p&gt;There are different types of metamerism, depending on when it happens during the light transport process. The apple and banana example is called illuminant metamerism, where objects that reflect light differently appear the same under some specific illumination.&lt;/p&gt;
    &lt;p&gt;Observer metamerism is when objects appear different between observers, a good example of this is colorblindness.&lt;/p&gt;
    &lt;p&gt;The rendering equation gives us the light reflected towards a direction at a point by summing all the incoming lights at that point coming from direction in the hemisphere , weighted by the BRDF at that point and the cosine term.&lt;/p&gt;
    &lt;p&gt;Let's peel off this equation step by step, starting with the easiest part:&lt;/p&gt;
    &lt;p&gt;When a beam of light hits a surface, the area it touches is inversly proportional to the cosine of the angle of incidence. When the angle of incidence is , the area is at minimum and the intensity is concentrated, but the more the angle gets, the larger the area and the intensity gets spread out.&lt;/p&gt;
    &lt;p&gt;The BRDF is arguably the most important part of the rendering equation, it characterizes the surface of our material and its appearance. This is where the we can apply the microfacet theory and energy conservation to make our rendering model physically based.&lt;/p&gt;
    &lt;p&gt;It takes as input the incoming and outgoing light direction, and the roughness of the surface . It equals the diffuse and the specular components weighted by their respective coefficients and .&lt;/p&gt;
    &lt;p&gt;There are many different BRDFs, the most common in realtime rendering is the Cook-Torrance specular microfacet model combined with Lambertian diffuse model.&lt;/p&gt;
    &lt;p&gt;The lambertian diffuse component is the diffuse color, called albedo, multiplied by the cosine factor. But since we already have the cosine factor in the rendering equation, the diffuse equation becomes:&lt;/p&gt;
    &lt;p&gt;The Cook-Torrance specular component itself has three components: the normal distribution function , the geometric function and the Fresnel equation .&lt;/p&gt;
    &lt;p&gt;The normal distribution function is an approximation of the number of microfacets oriented in such a way that they will reflect light from the incoming direction to the outgoing direction .&lt;/p&gt;
    &lt;p&gt;The one we will use is the Trowbridge-Reitz GGX function:&lt;/p&gt;
    &lt;p&gt;is the halfway vector between the incoming and outgoing directions, we calculate it like this:&lt;/p&gt;
    &lt;p&gt;Some incoming rays get occluded by some microfacets before they get a chance to bounce off to the outgoing direction, this is called shadowing. Other rays get occluded by microfacets on their way to the outgoing direction, this is called masking. The geometric function approximates this effect.&lt;/p&gt;
    &lt;p&gt;Here we can see the shadowed rays in red and the masked rays in blue. The yellow rays succesfully reflected to the outgoing direction:&lt;/p&gt;
    &lt;p&gt;We will use the Schlick-GGX geometric function:&lt;/p&gt;
    &lt;p&gt;Where:&lt;/p&gt;
    &lt;p&gt;Like we discussed in the previous chapter, we will use the Fresnel-Schlick approximation which is fast for realtime rendering and accurate enough:&lt;/p&gt;
    &lt;p&gt;Now we can combine the diffuse and specular components to get our final PBR render:&lt;/p&gt;
    &lt;p&gt;Here is a grid of spheres with different roughness and metallic values on each axis:&lt;/p&gt;
    &lt;p&gt;Usually the metallic values is either 0 or 1, but it is useful in PBR rendering to consider intermediate values to smoothly interpolate between metals and non-metals. Take this rusted metal material for example:&lt;/p&gt;
    &lt;p&gt;Physically based rendering is a very vast topic and there is a lot more to cover.&lt;/p&gt;
    &lt;p&gt;In the chapter about the physics of light, I omitted the quantum explanation of light's behaviour using probability amplitudes. We didn't talk about the double slit experiment or the wave-particle duality. I may cover this in the future when I learn more about it, for now I'll leave you with this quote from Richard Feynman's QED book:&lt;/p&gt;
    &lt;quote&gt;The theory of quantum electrodynamics describes Nature as absurd from the point of view of common sense. And it agrees fully with experiment. So I hope you accept Nature as She is — absurd.&lt;/quote&gt;
    &lt;p&gt;We didn't talk about polarization and assumed all our light sources are unpolarized, this isn't very important for general rendering but can be useful for research.&lt;/p&gt;
    &lt;p&gt;We focused on surface rendering, in the future I will cover volume rendering, subsurface scattering, effects like optical dispersion, thin-film interference/iridescence...etc&lt;/p&gt;
    &lt;p&gt;There are a lot more implementation specific details. Whether we are implementing PBR in raytracing or rasterization, we need to use optimization techniques to make the rendering faster while still being accurate. Examples that come to mind are prefiltred envmaps and importance sampling (or efficient sampling in general).&lt;/p&gt;
    &lt;p&gt;This article is mainly based on this SIGGRAPH talk by Naty Hoffman and Physically Based Rendering: From Theory To Implementation&lt;/p&gt;
    &lt;p&gt;My main inspiration for writing interactive articles is this fantastic blog by Bartosz Ciechanowski. A lot of interactive demos in this article are similar to the ones in this post.&lt;/p&gt;
    &lt;p&gt;Other resources include LearnOpenGL, the ScienceClic youtube channel, and 3Blue1Brown of course.&lt;/p&gt;
    &lt;p&gt;I can't recommend enough the famous book QED: The Strange Theory of Light and Matter by Richard Feynman.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45106846</guid></item><item><title>Vijaye Raji to become CTO of Applications with acquisition of Statsig</title><link>https://openai.com/index/vijaye-raji-to-become-cto-of-applications-with-acquisition-of-statsig/</link><description>&lt;doc fingerprint="f974305d9fc3edac"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Vijaye Raji to become CTO of Applications with acquisition of Statsig&lt;/head&gt;
    &lt;p&gt;We’re expanding our Applications leadership, the org responsible for how our research reaches and benefits the world.&lt;/p&gt;
    &lt;p&gt;As we scale ChatGPT and build new applications to serve hundreds of millions of people and businesses around the world, our ambition is to push the frontier of AI research and turn it into intuitive, safe, and useful tools that people love. That takes strong engineering systems, fast iteration, and a long-term focus on quality and reliability.&lt;/p&gt;
    &lt;p&gt;Vijaye Raji will step into a new role as CTO of Applications, reporting to Fidji Simo, following the acquisition of Statsig. As a hands-on builder and trusted leader, Vijaye will head product engineering for ChatGPT and Codex, with responsibilities that span core systems and product lines including infrastructure and Integrity. His experience as founder and CEO of Statsig, and a decade leading large-scale consumer engineering at Meta, brings both entrepreneurial vision and operating expertise to scale our next generation of products.&lt;/p&gt;
    &lt;p&gt;“Vijaye has a remarkable record of building new consumer and B2B products and systems at scale. He’s joining at a time when our models are opening entirely new ways to build, and his leadership will help turn that progress into safe applications that empower people with many new tools to improve their lives, help companies increase their impact and allow developers to build faster and better products.”&lt;/p&gt;
    &lt;p&gt;—Fidji Simo, CEO of Applications, OpenAI&lt;/p&gt;
    &lt;p&gt;As part of this transition, we’re acquiring Statsig, one of the most trusted experimentation platforms in the industry—powering A/B testing, feature flagging, and real-time decisioning for some of the world’s most innovative companies, including OpenAI.&lt;/p&gt;
    &lt;p&gt;Vijaye and his team founded Statsig on the belief that the best products come from rapid experimentation, tight feedback loops, and data-informed decision-making.&lt;/p&gt;
    &lt;p&gt;The Statsig platform has already played a central role in how we ship and learn quickly. Bringing it in-house will strengthen our ability to accelerate experimentation across our Applications org and build even better, more responsive experiences for the people and businesses we serve.&lt;/p&gt;
    &lt;p&gt;“Joining OpenAI as CTO of Applications is an extraordinary opportunity to bring my experience scaling consumer and enterprise products to a mission I deeply believe in: advancing AI in ways that are capable of solving hard problems, reliable, and truly beneficial to people everywhere. The journey with Statsig has been deeply gratifying, leading me to this moment and giving me conviction that we will continue helping teams ship better software every day.”&lt;/p&gt;
    &lt;p&gt;—Vijaye Raji, incoming CTO of Applications, OpenAI&lt;/p&gt;
    &lt;p&gt;Once the acquisition is finalized, Statsig employees will become OpenAI employees. It will continue operating independently and serving its customer base out of its Seattle office. We’ll take a measured approach to any future integration, ensuring continuity for current customers and enabling the team to stay focused on what they do best.&lt;/p&gt;
    &lt;p&gt;The closing of the acquisition is subject to customary closing conditions, including receipt of regulatory approval.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45106981</guid></item><item><title>A staff engineer's journey with Claude Code</title><link>https://www.sanity.io/blog/first-attempt-will-be-95-garbage</link><description>&lt;doc fingerprint="1e680603ea861e8e"&gt;
  &lt;main&gt;
    &lt;p&gt;This started as an internal Sanity workshop where I demoed how I actually use AI. Spoiler: it's running multiple agents like a small team with daily amnesia.&lt;/p&gt;
    &lt;p&gt;Vincent Quigley&lt;/p&gt;
    &lt;p&gt;Vincent Quigley is a Staff Software Engineer at Sanity&lt;/p&gt;
    &lt;p&gt;Published&lt;/p&gt;
    &lt;p&gt;Until 18 months ago, I wrote every line of code myself. Today, AI writes 80% of my initial implementations while I focus on architecture, review, and steering multiple development threads simultaneously.&lt;/p&gt;
    &lt;p&gt;This isn't another "AI will change everything" post. This is about the messy reality of integrating AI into production development workflows: what actually works, what wastes your time, and why treating AI like a "junior developer who doesn't learn" became my mental model for success.&lt;/p&gt;
    &lt;p&gt;The backstory: We run monthly engineering workshops at Sanity where someone presents what they've been experimenting with. Last time was my turn, and I showed how I'd been using Claude Code.&lt;/p&gt;
    &lt;p&gt;This blog post is from my presentation at our internal workshop (10-min recording below).&lt;/p&gt;
    &lt;p&gt;My approach to solving code problems has pivoted four times in my career:&lt;/p&gt;
    &lt;p&gt;For the first 5 years, I was reading books and SDK documentation.&lt;/p&gt;
    &lt;p&gt;Then 12 years of googling for crowd-sourced answers.&lt;/p&gt;
    &lt;p&gt;It was 18 months of using Cursor for AI-assisted coding&lt;/p&gt;
    &lt;p&gt;And recently, 6 weeks of using Claude Code for full AI delegation&lt;/p&gt;
    &lt;p&gt;Each transition happened faster than the last. The shift to Claude Code? That took just hours of use for me to become productive.&lt;/p&gt;
    &lt;p&gt;Here's what my workflow looks like now, stripped of the hype. I use AI mostly "to think with" as I'm working with it towards the code that ends up in production.&lt;/p&gt;
    &lt;p&gt;Forget the promise of one-shot perfect code generation. Your job as an engineer is to find the best solution for the problem, not just write a bunch of code.&lt;/p&gt;
    &lt;p&gt;Then you take the learnings from this attempt and feed it back.&lt;/p&gt;
    &lt;p&gt;This isn't failure; it's the process! Expecting perfection on attempt one is like expecting a junior developer to nail a complex feature without context.&lt;/p&gt;
    &lt;p&gt;The biggest challenge? AI can't retain learning between sessions (unless you spend the time manually giving it the "memories"). So typically, every conversation starts fresh.&lt;/p&gt;
    &lt;p&gt;My solutions:&lt;/p&gt;
    &lt;p&gt;Create a project-specific context file with:&lt;/p&gt;
    &lt;p&gt;Thanks to MCP integrations, I can now connect my AI to:&lt;/p&gt;
    &lt;p&gt;Without this context, you're explaining the same constraints repeatedly. With it, you start at attempt two instead of attempt one.&lt;/p&gt;
    &lt;p&gt;I run multiple Claude instances in parallel now, it's like managing a small team of developers who reset their memory each morning.&lt;/p&gt;
    &lt;p&gt;Key strategies:&lt;/p&gt;
    &lt;p&gt;Writing code is one part of the job, but so is reviewing code. Adopting AI has evolved my code review process as well.&lt;/p&gt;
    &lt;p&gt;This saves me and my peers time and extra rounds.&lt;/p&gt;
    &lt;p&gt;At Sanity, our policy is that the engineer is responsible for the code they ship, even if it's AI generated. I want to make sure that I ship:&lt;/p&gt;
    &lt;p&gt;The key take away: I'm more critical of "my code" now because I didn't type out a lot of it. No emotional attachment means better reviews.&lt;/p&gt;
    &lt;p&gt;We're testing Slack-triggered agents using Cursor for simple tasks:&lt;/p&gt;
    &lt;p&gt;Current limitations:&lt;/p&gt;
    &lt;p&gt;But the potential? Imagine agents handling your backlog's small tickets while you sleep. We're actively exploring this at Sanity, sharing learnings across teams as we figure out what works.&lt;/p&gt;
    &lt;p&gt;Let's talk money. My Claude Code usage costs my company not an insignificant percent of what they pay me monthly.&lt;/p&gt;
    &lt;p&gt;But for that investment:&lt;/p&gt;
    &lt;p&gt;The ROI is obvious, but budget for $1000-1500/month for a senior engineer going all-in on AI development. It's also reasonable to expect engineers to get more efficient with AI spend as they get good with it, but give them time.&lt;/p&gt;
    &lt;p&gt;Not everything in AI-assisted development is smooth. Here are the persistent challenges I find myself in:&lt;/p&gt;
    &lt;p&gt;The learning problem&lt;lb/&gt;AI doesn't learn from mistakes. You fix the same misunderstandings repeatedly. Your solution: better documentation and more explicit instructions.&lt;/p&gt;
    &lt;p&gt;The confidence problem&lt;lb/&gt;AI confidently writes broken code claiming that it's great. Always verify, especially for:&lt;/p&gt;
    &lt;p&gt;The context limit problem&lt;lb/&gt;Large codebases overwhelm AI context windows. Break problems into smaller chunks and provide focused context.&lt;/p&gt;
    &lt;p&gt;The hardest part? Letting go of code ownership. But now I don't care about "my code" anymore; it's just output to review and refine.&lt;/p&gt;
    &lt;p&gt;This detachment is actually quite liberating!&lt;/p&gt;
    &lt;p&gt;If a better AI tool appears tomorrow, I'll switch immediately. The code isn't precious; the problems we solve are.&lt;/p&gt;
    &lt;p&gt;If I were to give advice from an engineer's perspective, if you're a technical leader considering AI adoption:&lt;/p&gt;
    &lt;p&gt;The engineers who adapt to the new AI workflows will find themselves with a new sharp knife in their toolbox: They're becoming orchestrators, handling multiple AI agents while focusing on architecture, review, and complex problem-solving.&lt;/p&gt;
    &lt;p&gt;Pick one small, well-defined feature. Give AI three attempts at implementing it. Review the output like you're mentoring a junior developer.&lt;/p&gt;
    &lt;p&gt;That's it. No huge transformation needed, no process overhaul required. Just one feature, three attempts, and a honest review.&lt;/p&gt;
    &lt;p&gt;The future isn't about AI replacing developers. It's about developers working faster, creating better solutions, and leveraging the best tools available.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45107962</guid></item><item><title>Making a Linux home server sleep on idle and wake on demand (2023)</title><link>https://dgross.ca/blog/linux-home-server-auto-sleep</link><description>&lt;doc fingerprint="96f37d91e92280c3"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Making a Linux home server sleep on idle and wake on demand â the simple way&lt;/head&gt;
    &lt;p&gt;It began with what seemed like a final mundane touch to my home server setup for hosting Time Machine backups: I wanted it to automatically sleep when idle and wake up again when needed. You know, sleep on idle â hasnât Windows had that built in since like Windows 98? How hard could it be to configure on a modern Ubuntu install?&lt;/p&gt;
    &lt;p&gt;To be fair, I wanted more than just sleep on idle, I also wanted wake on request â and that second bit turns out to be the hard part. There were a bunch of dead ends, but I stuck out it to find something that âjust worksâ without the need to manually turn on the server for every backup. Join me on the full adventure further down, or cut to the chase with the setup instructions below.&lt;/p&gt;
    &lt;head rend="h1"&gt;tl;dr&lt;/head&gt;
    &lt;head rend="h4"&gt;Outcome:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Server automatically suspends to RAM when idle&lt;/item&gt;
      &lt;item&gt;Server automatically wakes when needed by anything else on the network, including SSH, Time Machine backups, etc.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Youâll need:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;An always-on Linux device on the same network as your server, e.g. a Raspberry Pi&lt;/item&gt;
      &lt;item&gt;A network interface device for your server that supports wake-on-LAN with unicast packets&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;On the server:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Enable wake-on-LAN with unicast packets (not just magic packets), make it persistent&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;sudo ethtool -s eno1 wol ug
sudo tee /etc/networkd-dispatcher/configuring.d/wol &amp;lt;&amp;lt; EOF
#!/usr/bin/env bash

ethtool -s eno1 wol ug || true
EOF
sudo chmod 755 /etc/networkd-dispatcher/configuring.d/wol
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Set up a cron job to sleep on idle (replace &lt;code&gt;/home/ubuntu&lt;/code&gt;with your desired script location)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;tee /home/ubuntu/auto-sleep.sh &amp;lt;&amp;lt; EOF
#!/bin/bash
logged_in_count=$(who | wc -l)
# We expect 2 lines of output from `lsof -i:548` at idle: one for output headers, another for the 
# server listening for connections. More than 2 lines indicates inbound connection(s).
afp_connection_count=$(lsof -i:548 | wc -l)
if [[ $logged_in_count &amp;lt; 1 &amp;amp;&amp;amp; $afp_connection_count &amp;lt; 3 ]]; then
  systemctl suspend
else
  echo "Not suspending, logged in users: $logged_in_count, connection count: $afp_connection_count"
fi
EOF
chmod +x /home/ubuntu/auto-sleep.sh
sudo crontab -e
# In the editor, add the following line:
*/10 * * * * /home/ubuntu/auto-sleep.sh | logger -t autosuspend
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Disable IPv6: this approach relies on ARP, which IPv6 doesnât use&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;sudo nano /etc/default/grub
# Find GRUB_CMDLINE_LINUX=""
# Change to GRUB_CMDLINE_LINUX="ipv6.disable=1"
sudo update-grub
sudo reboot
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Optional: Configure network services (e.g. Netatalk) to stop before sleep to prevent unwanted wakeups due to network activity&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;sudo tee /etc/systemd/system/netatalk-sleep.service &amp;lt;&amp;lt; EOF
[Unit]
Description=Netatalk sleep hook
Before=sleep.target
StopWhenUnneeded=yes

[Service]
Type=oneshot
RemainAfterExit=yes
ExecStart=-/usr/bin/systemctl stop netatalk
ExecStop=-/usr/bin/systemctl start netatalk

[Install]
WantedBy=sleep.target
EOF
sudo systemctl daemon-reload
sudo systemctl enable netatalk-sleep.service
&lt;/code&gt;
    &lt;head rend="h4"&gt;On the always-on device:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Install ARP Stand-in: a super simple Ruby script that runs as a system service and responds to ARP requests on behalf of another machine. Configure it to respond on behalf of the sleeping server.&lt;/item&gt;
      &lt;item&gt;Optional: Configure Avahi to advertise network services on behalf of the server when itâs sleeping.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;sudo apt install avahi-daemon
sudo tee /etc/systemd/system/avahi-publish.service &amp;lt;&amp;lt; EOF
[Unit]
Description=Publish custom Avahi records
After=network.target avahi-daemon.service
Requires=avahi-daemon.service

[Service]
ExecStart=/usr/bin/avahi-publish -s homeserver _afpovertcp._tcp 548 -H homeserver.local

[Install]
WantedBy=multi-user.target
EOF
sudo systemctl daemon-reload
sudo systemctl enable avahi-publish.service --now
systemctl status avahi-publish.service
&lt;/code&gt;
    &lt;head rend="h4"&gt;Caveats&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The serverâs network device needs to support wake-on-LAN from unicast packets&lt;/item&gt;
      &lt;item&gt;To prevent unwanted wake-ups, youâll need to ensure no device on the network is sending extraneous packets to the server&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;How I got there&lt;/head&gt;
    &lt;p&gt;First, a bit about my hardware, as this solution is somewhat hardware-dependent:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;HP ProDesk 600 G3 SFF&lt;/item&gt;
      &lt;item&gt;CPU: Intel Core i5-7500&lt;/item&gt;
      &lt;item&gt;Network adapter: Intel I219-LM&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Sleeping on idle&lt;/head&gt;
    &lt;p&gt;I started with sleep-on-idle, which boiled down to two questions:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;How to determine if the server is idle or busy at any given moment&lt;/item&gt;
      &lt;item&gt;How to automatically suspend to RAM after being idle for some time&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Most of the guides I found for sleep-on-idle, like this one, were for Ubuntu Desktop â sleep-on-idle doesnât seem to be something thatâs commonly done with Ubuntu Server. I came across a few tools that looked promising, the most notable being &lt;code&gt;circadian&lt;/code&gt;. In general, though, there didnât seem to be a standard/best-practice way to do it, so I decided Iâd roll it myself the simplest way I could.&lt;/p&gt;
    &lt;head rend="h3"&gt;Determining idle/busy state&lt;/head&gt;
    &lt;p&gt;I asked myself what server activity would constitute being busy, and landed on two things:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Logged in SSH sessions&lt;/item&gt;
      &lt;item&gt;In-progress Time Machine backups&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Choosing corresponding metrics was pretty straightforward:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Count of logged in users, using &lt;code&gt;who&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Count of connections on the AFP port (548), using &lt;code&gt;lsof&lt;/code&gt;(Iâm using AFP for Time Machine network shares)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For both metrics, I noted the values first at idle, and then again when the server was busy.&lt;/p&gt;
    &lt;head rend="h3"&gt;Automatically suspending to RAM&lt;/head&gt;
    &lt;p&gt;To keep things simple, I opted for a cron job that triggers a bash script â check out the final version shared above. So far itâs worked fine; if I ever need to account for more metrics in detecting idle state, Iâll consider using a more sophisticated option like &lt;code&gt;circadian&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;Waking on request&lt;/head&gt;
    &lt;p&gt;With sleep-on-idle out of the way, I moved on to figuring out how the server would wake on demand.&lt;/p&gt;
    &lt;p&gt;Could the machine be configured to automatically wake upon receiving a network request? I knew Wake-on-LAN supported waking a computer up using a specially crafted âmagic packetâ, and it was straightforward to get this working. The question was if a regular, non-âmagic packetâ could somehow do the same thing.&lt;/p&gt;
    &lt;head rend="h3"&gt;Wake on PHY?&lt;/head&gt;
    &lt;p&gt;Some online searching yielded a superuser discussion that looked particularly promising. It pointed to the man page for ethtool, the Linux utility used to configure network hardware. It shared ethtoolâs complete wake-on-LAN configuration options:&lt;/p&gt;
    &lt;code&gt;wol p|u|m|b|a|g|s|f|d...
      Sets Wake-on-LAN options.  Not all devices support
      this.  The argument to this option is a string of
      characters specifying which options to enable.

      p   Wake on PHY activity
      u   Wake on unicast messages
      m   Wake on multicast messages
      b   Wake on broadcast messages
      a   Wake on ARP
      g   Wake on MagicPacketâ¢
      s   Enable SecureOnâ¢ password for MagicPacketâ¢
      f   Wake on filter(s)
      d   Disable (wake on nothing).  This option
          clears all previous options.
&lt;/code&gt;
    &lt;p&gt;It pointed in particular to the &lt;code&gt;Wake on PHY activity&lt;/code&gt; option, which seemed perfect for this use-case. It seemed to mean that any packet sent to the network interfaceâs MAC address would wake it. I enabled the flag using &lt;code&gt;ethtool&lt;/code&gt;, manually put the machine to sleep, then tried logging back in using SSH and sending pings. No dice: the machine remained asleep despite multiple attempts. So much for that ð&lt;/p&gt;
    &lt;head rend="h3"&gt;Breakthrough: wake on unicast&lt;/head&gt;
    &lt;p&gt;None of &lt;code&gt;ethtool&lt;/code&gt;âs other wake-on-LAN options seemed relevant, but some more searching pointed to the &lt;code&gt;Wake on unicast messages&lt;/code&gt; as another option to try. I enabled the flag using &lt;code&gt;ethtool&lt;/code&gt;, manually put the machine to sleep, then tried logging back in using SSH. Bingo! This time, the machine woke up. ð With that, I figured I was done.&lt;/p&gt;
    &lt;p&gt;Not so fast â there were two problems:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Sometimes, the server would wake up without any network activity that I knew of&lt;/item&gt;
      &lt;item&gt;Some period of time after the server went to sleep, it would become impossible to wake it again using network activity other than a magic packet&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A closer look at the same superuser discussion above revealed exactly the reason for the second problem: shortly after going to sleep, the machine was effectively disappearing from the network because it was no longer responding to ARP requests.&lt;/p&gt;
    &lt;head rend="h3"&gt;ARP&lt;/head&gt;
    &lt;p&gt;So the cached ARP entry for other machines on the network was expiring, meaning that they had no way to resolve the serverâs IP address to its MAC address. In other words, an attempt to ping my server at &lt;code&gt;192.168.1.2&lt;/code&gt; was failing to even send a packet to the server, because the serverâs MAC address wasnât known. Without a packet being sent, there was no way that server was going to wake up.&lt;/p&gt;
    &lt;head rend="h4"&gt;Static ARP?&lt;/head&gt;
    &lt;p&gt;My first reaction: letâs manually create ARP cache entries on each network client. This is indeed possible on macOS using:&lt;/p&gt;
    &lt;code&gt;sudo arp -s [IP address] [MAC address]
&lt;/code&gt;
    &lt;p&gt;But it also didnât meet the goal of having things âjust workâ: I was not interested in creating static ARP cache entries on each machine that would be accessing the server. On to other options.&lt;/p&gt;
    &lt;head rend="h4"&gt;ARP protocol offload?&lt;/head&gt;
    &lt;p&gt;Some more searching revealed something interesting: this problem had already been solved long ago in the Windows world.&lt;/p&gt;
    &lt;p&gt;It was called ARP protocol offload, and it goes like this:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The network hardware is capable of responding to ARP requests independently of the CPU&lt;/item&gt;
      &lt;item&gt;Before going to sleep, the OS configures the network hardware to respond to ARP requests&lt;/item&gt;
      &lt;item&gt;While sleeping, the network hardware responds to ARP requests on its own, without waking the rest of the machine to use the CPU&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Voila, this was exactly what I needed. I even looked at the datasheet for my network hardware, which lists ARP Offload as a feature on the front page.&lt;/p&gt;
    &lt;p&gt;The only problem? No Linux support. I searched the far reaches of the internet, then finally dug into the Linux driver source code to find that ARP offload isnât supported by the Linux driver. This was when I briefly pondered trying to patch the driver to add ARP offloadâ¦ before reminding myself that successfully patching Linux driver code is far beyond what I could hope to achieve in a little free-time project like this one. (Though maybe one dayâ¦)&lt;/p&gt;
    &lt;head rend="h4"&gt;Other solutions using magic packets&lt;/head&gt;
    &lt;p&gt;Some more searching led me to some other clever and elaborate solutions involving magic packets. The basic idea was to automate sending magic packets. One solution (wake-on-arp) listens for ARP requests to a specified host to trigger sending a magic packet to that host. Another solution implements a web interface and Home Assistant integration to enable triggering a magic packet from a smartphone web browser. These are impressive, but I wanted something simpler that didnât require manually waking up the server.&lt;/p&gt;
    &lt;p&gt;I considered a few other options, but abandoned them because they felt too complex and prone to breaking:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Writing a script to send a magic packet and then immediately trigger a Time Machine backup using &lt;code&gt;tmutil&lt;/code&gt;. The script would need to be manually installed and scheduled to run periodically on each Mac.&lt;/item&gt;
      &lt;item&gt;Using HAProxy to proxy all relevant network traffic through the Raspberry Pi and using a hook to send a magic packet to the server on activity.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Breakthrough: ARP Stand-in&lt;/head&gt;
    &lt;p&gt;What I was attempting didnât seem much different from the static IP mapping thatâs routinely configured on home routers, except that it was for DHCP instead of ARP. Was there no way to make my router do the same thing for ARP?&lt;/p&gt;
    &lt;p&gt;Some more digging into the ARP protocol revealed that ARP resolution doesnât even require a specific, authoritative host to answer requests â any other network device can respond to ARP requests. In other words, my router didnât need to be the one resolving ARP requests, it could be anything. Now how could I just set up something to respond on behalf of the sleeping server?&lt;/p&gt;
    &lt;p&gt;Hereâs what I was trying to do:&lt;/p&gt;
    &lt;p&gt;I thought it must be possible to implement as a Linux network configuration, but the closest thing I found was Proxy ARP, which accomplished a different goal. So I went one level deeper, to network programming.&lt;/p&gt;
    &lt;p&gt;Now, how to go about listening for ARP request packets? This is apparently possible to do using a raw socket, but I also knew that &lt;code&gt;tcpdump&lt;/code&gt; and Wireshark were capable of using filters to capture only packets of a given type. That led me to look into libpcap, the library that powers both of those tools. I learned that using &lt;code&gt;libpcap&lt;/code&gt; had a clear advantage over a raw socket: &lt;code&gt;libpcap&lt;/code&gt; implements very efficient filtering directly in the kernel, whereas a raw socket would require manual packet filtering in user space, which is less performant.&lt;/p&gt;
    &lt;p&gt;Aiming to keep things simple, I decided to try writing the solution in Ruby, which led me to the pcaprub Ruby bindings for &lt;code&gt;libpcap&lt;/code&gt;. From there, I just needed to figure out what filter to use with &lt;code&gt;libpcap&lt;/code&gt;. Some research and trial/error yielded this filter:&lt;/p&gt;
    &lt;code&gt;arp and arp[6:2] == 1 and arp[24:4] == [IP address converted to hex]
&lt;/code&gt;
    &lt;p&gt;For example, using a target IP address of &lt;code&gt;192.168.1.2&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;arp and arp[6:2] == 1 and arp[24:4] == 0xc0a80102
&lt;/code&gt;
    &lt;p&gt;Letâs break this down, using the ARP packet structure definition for byte offets and lengths:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;arp&lt;/code&gt;â ARP packets&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;arp[6:2] == 1&lt;/code&gt;â ARP request packets.&lt;code&gt;[6:2]&lt;/code&gt;means âthe 2 bytes found at byte offset 6â.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;arp[24:4] == [IP address converted to hex]&lt;/code&gt;â ARP packets with the specified target address.&lt;code&gt;[24:4]&lt;/code&gt;means âthe 4 bytes found at byte offset 24â.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The rest is pretty straightforward and the whole solution comes out to only ~50 lines of Ruby code. In short, &lt;code&gt;arp_standin&lt;/code&gt; is a daemon that does the following:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Starts up, taking these configuration options: &lt;list rend="ul"&gt;&lt;item&gt;IP and MAC address of the machine itâs standing in for (the âtargetâ)&lt;/item&gt;&lt;item&gt;Network interface to operate on&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Listens for ARP requests for the targetâs IP address&lt;/item&gt;
      &lt;item&gt;On detecting an ARP request for the targetâs IP address, responds with the targetâs MAC address&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Since the serverâs IP â MAC address mapping is defined statically through the &lt;code&gt;arp_standin&lt;/code&gt; daemonâs configuration, it doesnât matter if the Raspberry Piâs ARP cache entry for the server is expired.&lt;/p&gt;
    &lt;p&gt;Check out the link below to install it or explore the source code further:&lt;/p&gt;
    &lt;p&gt;danielpgross/arp_standin on GitHub&lt;/p&gt;
    &lt;p&gt;ARP is used in IPv4 and is replaced by Neighbor Discovery Protocol (NDP) in IPv6. I donât have any need for IPv6 right now, so I disabled IPv6 entirely on the server using the steps shown above. It should be possible to add support for Neighbor Discovery to the ARP-Standin service as a future enhancement.&lt;/p&gt;
    &lt;p&gt;With the new service running on my Raspberry Pi, I used Wireshark to confirm that ARP requests being sent to the server were triggering responses from the ARP Stand-in. It worked ð â things were looking promising.&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting it all working&lt;/head&gt;
    &lt;p&gt;The big pieces were in place:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;the server went to sleep after becoming idle&lt;/item&gt;
      &lt;item&gt;the server could wake up from unicast packets&lt;/item&gt;
      &lt;item&gt;other machines could resolve the serverâs MAC address using ARP, long after it went to sleep&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With the ARP Stand-in running, I turned on the server and ran a backup from my computer. When the backup was finished, the server went to sleep automatically. But there was a problem: the server was waking up immediately after going to sleep.&lt;/p&gt;
    &lt;head rend="h3"&gt;Unwanted wake-ups&lt;/head&gt;
    &lt;p&gt;First thing I checked was the Linux system logs, but these didnât prove too helpful, since they didnât show what network packet actually triggered the wakeup. Wireshark/tcpdump were no help here either, because they wouldnât be running when the computer was sleeping. Thatâs when I thought to use port mirroring: capturing packets from an intermediary device between the server and the rest of the network. After a brief, unsuccessful attempt to repurpose an extra router running OpenWRT, a search for the least expensive network switch with port mirroring support yielded the TP-Link TL-SG105E for ~$30.&lt;/p&gt;
    &lt;p&gt;With the switch connected and port mirroring enabled, I started capturing with Wireshark and the culprits immediately became clear:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;My Mac, which was configured to use the server as a Time Machine backup host using AFP, was sending AFP packets to the server after it had gone to sleep&lt;/item&gt;
      &lt;item&gt;My Netgear R7000, acting as a wireless access point, was sending frequent, unsolicited NetBIOS &lt;code&gt;NBTSTAT&lt;/code&gt;queries to the server&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Eliminating AFP packets&lt;/head&gt;
    &lt;p&gt;I had a hunch about why the Mac was sending these packets:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The Mac mounted the AFP share to perform a Time Machine backup&lt;/item&gt;
      &lt;item&gt;The Time Machine backup finished, but the share remained mounted&lt;/item&gt;
      &lt;item&gt;The Mac was checking on the status of the share periodically, as would be done normally for a mounted network share&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I also had a corresponding hunch that the solution would be to make sure the share got unmounted before the server went to sleep, so that the Mac would no longer ping the server for its status afterwards. I figured that shutting down the AFP service would trigger unmounting of shares on all its clients, achieving the goal. Now I just needed to ensure the service would shut down when the server was going to sleep, then start again when it woke back up.&lt;/p&gt;
    &lt;p&gt;Fortunately, &lt;code&gt;systemd&lt;/code&gt; supports exactly that, and relatively easily â I defined a dedicated &lt;code&gt;systemd&lt;/code&gt; service to hook into sleep/wake events (check out the configuration shared above). A Wireshark capture confirmed that it did the trick.&lt;/p&gt;
    &lt;head rend="h4"&gt;Eliminating NetBIOS packets&lt;/head&gt;
    &lt;p&gt;This one proved to be harder, because the packets were unsolicited â they seemed random and unrelated to any activity being done by the server. I thought they might be related to Samba services running on the server, but the packets persisted even after I completely removed Samba from the server.&lt;/p&gt;
    &lt;p&gt;Why was my network router sending NetBIOS requests, anyway? Turns out that Netgear routers have a feature called ReadySHARE for sharing USB devices over the network using the SMB protocol. Presumably, the router firmware uses Samba behind the scenes, which uses NetBIOS queries to build and maintain its own representation of NetBIOS hosts on the network. Easy â turn off ReadySHARE, right? Nope, thereâs no way to do that in Netgearâs stock firmware ð.&lt;/p&gt;
    &lt;p&gt;That led me to take the plunge and flash the router with open-source FreshTomato firmware. Iâm glad I did, because the firmware is much better than the stock one anyway, and it immediately stopped the unwanted NetBIOS packets.&lt;/p&gt;
    &lt;head rend="h3"&gt;Time Machine not triggering wake-up&lt;/head&gt;
    &lt;p&gt;I was getting close now: the server remained asleep, and I could reliably wake it up by logging in with SSH, even long after it went to sleep.&lt;/p&gt;
    &lt;p&gt;This was great, but one thing wasnât working: when starting a backup on my Mac, Time Machine would show a loading state indefinitely with &lt;code&gt;Connecting to backup disk...&lt;/code&gt; and eventually give up. Was the server failing to wake up from packets the Mac was sending, or was the Mac not sending packets at all?&lt;/p&gt;
    &lt;p&gt;A port-mirrored Wireshark capture answered that question: the Mac wasnât sending any packets to the server, even long after it started to say &lt;code&gt;Connecting to backup disk...&lt;/code&gt;. Digging into the macOS Time Machine logs with:&lt;/p&gt;
    &lt;code&gt;log show --style syslog --predicate 'senderImagePath contains[cd] "TimeMachine"' --info
&lt;/code&gt;
    &lt;p&gt;A few entries made it clear:&lt;/p&gt;
    &lt;code&gt;(TimeMachine) [com.apple.TimeMachine:Mounting] Attempting to mount 'afp://backup_mbp@homeserver._afpovertcp._tcp.local./tm_mbp'
...
(TimeMachine) [com.apple.TimeMachine:General] Failed to resolve CFNetServiceRef with name = homeserver type = _afpovertcp._tcp. domain = local.
&lt;/code&gt;
    &lt;p&gt;The Mac was using mDNS (a.k.a. Bonjour, Zeroconf) to resolve the backup serverâs IP address using its hostname. The server was asleep and therefore not responding to the requests, so the Mac was failing to resolve its IP address. This explained why the Mac wasnât sending any packets to the server, leaving it asleep.&lt;/p&gt;
    &lt;head rend="h4"&gt;mDNS stand-in&lt;/head&gt;
    &lt;p&gt;I already had an ARP stand-in service, now I needed my Raspberry Pi to also respond to mDNS queries for the server while it slept. I knew that Avahi was one of the main mDNS implementations for Linux. I first tried these instructions using &lt;code&gt;.service&lt;/code&gt; files to configure my Raspberry Pi to respond to mDNS queries on behalf of the server. I used the following on the Mac to check the result:&lt;/p&gt;
    &lt;code&gt;dns-sd -L homeserver _afpovertcp._tcp local
&lt;/code&gt;
    &lt;p&gt;For some reason, that approach just didnât work; Avahi didnât respond on behalf of the server. I experimented instead with &lt;code&gt;avahi-publish&lt;/code&gt; (man page), which (to my pleasant surprise) worked right away using the following:&lt;/p&gt;
    &lt;code&gt;avahi-publish -s homeserver _afpovertcp._tcp 548 -H homeserver.local
&lt;/code&gt;
    &lt;p&gt;With that, I just needed to create a &lt;code&gt;systemd&lt;/code&gt; service definition that would automatically run the &lt;code&gt;avahi-publish&lt;/code&gt; command on boot (check out the configuration shared above).&lt;/p&gt;
    &lt;head rend="h2"&gt;ð Finish&lt;/head&gt;
    &lt;p&gt;With all the wrinkles ironed out, everything has been working well now for over a month. I hope youâve enjoyed following along and that this approach works for you too.&lt;/p&gt;
    &lt;p&gt;This post was discussed on Hacker News and Reddit.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45108066</guid></item><item><title>Google can keep its Chrome browser but will be barred from exclusive contracts</title><link>https://www.cnbc.com/2025/09/02/google-antitrust-search-ruling.html</link><description>&lt;doc fingerprint="19469cf013a53348"&gt;
  &lt;main&gt;
    &lt;p&gt;Alphabet shares popped 8% in extended trading as investors celebrated what they viewed as minimal consequences from a historic defeat last year in the landmark antitrust case.&lt;/p&gt;
    &lt;p&gt;Last year, Google was found to hold an illegal monopoly in its core market of internet search.&lt;/p&gt;
    &lt;p&gt;U.S. District Judge Amit Mehta ruled against the most severe consequences that were proposed by the Department of Justice, including the forced sale of Google's Chrome browser, which provides data that helps its advertising business deliver targeted ads.&lt;/p&gt;
    &lt;p&gt;"Google will not be required to divest Chrome; nor will the court include a contingent divestiture of the Android operating system in the final judgment," the decision stated. "Plaintiffs overreached in seeking forced divestiture of these key assets, which Google did not use to effect any illegal restraints."&lt;/p&gt;
    &lt;p&gt;Mehta, who oversaw the remedies trial in May, ordered the parties to meet by Sept. 10 for the final judgment.&lt;/p&gt;
    &lt;p&gt;In August 2024, the U.S. District Court for the District of Columbia ruled that Google violated Section 2 of the Sherman Act and held a monopoly in search and related advertising.&lt;/p&gt;
    &lt;p&gt;The antitrust trial started in September 2023.&lt;/p&gt;
    &lt;p&gt;"Now the Court has imposed limits on how we distribute Google services, and will require us to share Search data with rivals," Google said in a blog post. "We have concerns about how these requirements will impact our users and their privacy, and we're reviewing the decision closely. The Court did recognize that divesting Chrome and Android would have gone beyond the case's focus on search distribution, and would have harmed consumers and our partners."&lt;/p&gt;
    &lt;p&gt;One of the key areas of focus was the exclusive contracts Google held for distribution.&lt;/p&gt;
    &lt;p&gt;In his decision Tuesday, Mehta said the company can make payments to preload products, but it cannot have exclusive contracts that condition payments or licensing.&lt;/p&gt;
    &lt;p&gt;The DOJ had asked Google to stop the practice of "compelled syndication," which refers to the practice of making certain deals with companies to ensure its search engine remains the default choice in browsers and smartphones.&lt;/p&gt;
    &lt;p&gt;"The court's ruling today recognizes the need for remedies that will pry open the market for general search services, which has been frozen in place for over a decade," the DOJ said in a press release. "The ruling also recognizes the need to prevent Google from using the same anticompetitive tactics for its GenAI products as it used to monopolize the search market, and the remedies will reach GenAI technologies and companies."&lt;/p&gt;
    &lt;p&gt;Google pays Apple billions of dollars per year to be the default search engine on iPhones. It's lucrative for Apple and a valuable way for Google to get more search volume and users.&lt;/p&gt;
    &lt;p&gt;Apple stock rose 4% on Tuesday after hours.&lt;/p&gt;
    &lt;p&gt;"Google will not be barred from making payments or offering other consideration to distribution partners for preloading or placement of Google Search, Chrome, or its GenAI products. Cutting off payments from Google almost certainly will impose substantial—in some cases, crippling—downstream harms to distribution partners, related markets, and consumers, which counsels against a broad payment ban."&lt;/p&gt;
    &lt;p&gt;Google was also ordered to loosen its hold on search data.&lt;/p&gt;
    &lt;p&gt;During the remedies trial in May, the DOJ asked the judge to force Google to share the data it uses for generating search results, such as data about what users click on.&lt;/p&gt;
    &lt;p&gt;Mehta ruled Tuesday that Google will have to make available certain search index data and user interaction data, though "not ads data."&lt;/p&gt;
    &lt;p&gt;Google does not have to share or provide access to granular data with advertisers.&lt;/p&gt;
    &lt;p&gt;The court narrowed the datasets Google will be required to share and said they must occur on "ordinary commercial terms that are consistent with Google's current syndication services."&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45108548</guid></item><item><title>This blog is running on a recycled Google Pixel 5 (2024)</title><link>https://blog.ctms.me/posts/2024-08-29-running-this-blog-on-a-pixel-5/</link><description>&lt;doc fingerprint="37c26d064887b8aa"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;This blog is running on a recycled Google Pixel 5&lt;/head&gt;
    &lt;p&gt;If you glance over this blog, you will see that I am an avid Android fan. After setting up numerous Linux &lt;code&gt;proot&lt;/code&gt; desktops on phones, I wanted to see if I use a phone as a server and run my blog from an Android phone. Since you are reading this, I was successful.&lt;/p&gt;
    &lt;p&gt;I was inspired my a few Mastodon posts earlier this week to give it a go. First, I stumbled on a post from @kaimac who is running a site from an ESP32 microcontroller. In the comments of that post, I saw a mention to compost.party created by user @computersandblues that runs completely on an Android device and a solar panel. Last, @stevelord who is essentially running a homelab on a TP-Link router with OpenWRT installed.&lt;/p&gt;
    &lt;p&gt;I think a lot about power consumption of my homelab and I also love using old hardware for random projects to give them new life. I was truly inspired by the above works, so I got right down to business.&lt;/p&gt;
    &lt;head rend="h2"&gt;The hardware&lt;/head&gt;
    &lt;p&gt;I looked through the devices I had laying around and I chose a Google Pixel 5 my brother-in-law gave me after he upgraded. The Pixel 5 is carrier locked to Verizon, which is notorious for making it impossible to also unlock the bootloader and install custom ROMs. At first I wanted a device that I could install PostmarketOS to run a proper Linux server. In the end, I’m glad I didn’t go that route.&lt;/p&gt;
    &lt;p&gt;Another reason I chose the Pixel 5 is because it supports USB-OTG and can use docks with hard-wired internet. I didn’t want to run the site on wifi and having an ethernet connection was mandatory.&lt;/p&gt;
    &lt;p&gt;Last, it is the most current phone I have. This device is open to the internet, so I wanted to make sure it is an updated as possible.&lt;/p&gt;
    &lt;head rend="h4"&gt;Solar powered blog!&lt;/head&gt;
    &lt;p&gt;This summer I’ve been testing using a 100w solar panel I got from Harbor Freight Tools so I can learn more about how it all works before diving into larger projects. I have that panel connected to a Jackery 160w power station to keep it charged up and we use it to charge our mobile devices. I got the Jackery last year as a power bank I use while on jobsites.&lt;/p&gt;
    &lt;p&gt;Since I already have this set up, I am now using it to power this blog. I’m happy with this setup as I’ve been getting more into permacomputing. Having a website that is fully offgrid using recycled parts is exciting!&lt;/p&gt;
    &lt;head rend="h2"&gt;What I used to create the site (Termux is the GOAT)&lt;/head&gt;
    &lt;p&gt;While considering what projects I could do with this phone, I was thinking I was going to install a &lt;code&gt;proot&lt;/code&gt; desktop and then run from within a Linux environment. Before I started I decided to check out a few packages that are in Termux (the flat out amazing terminal emulator) to see how far I could push it.&lt;/p&gt;
    &lt;p&gt;I checked for some basics and read about setting up an &lt;code&gt;ssh&lt;/code&gt; connection. Then I randomly searched for Hugo, which is what my blog was already built on. Sure enough, it is right there in the Termux repos! Turns out, it has been in there for a long time. I see a lot of posts from 2018 with people using it.&lt;/p&gt;
    &lt;head rend="h2"&gt;How has it been going&lt;/head&gt;
    &lt;p&gt;Great! Site is fast and reliable. I ran into a few hiccups on the first day or so, which were mostly around the version of Hugo on my server and the newer version I am using on the phone. The other is related to my solar setup and keeping an eye on the battery levels.&lt;/p&gt;
    &lt;p&gt;To be honest, I don’t think anyone can tell it is running on an Android phone instead of a x86 Linux box or a hyperscaler VPS.&lt;/p&gt;
    &lt;p&gt;At the moment I have no plans to change this setup and will leave it as-is until some issue arises. But, there’s really not much to report other than it works fantastic.&lt;/p&gt;
    &lt;p&gt;Below are my longform notes on how I set it up. But, the short version is it was way simpler than I thought it would be. You can get up and running with a Hugo site by just installing &lt;code&gt;git&lt;/code&gt;, &lt;code&gt;screen&lt;/code&gt;, your favorite text editor, and &lt;code&gt;hugo&lt;/code&gt; straight from the repos.&lt;/p&gt;
    &lt;p&gt;Not included in this post is how I add new posts to the phone. I can use &lt;code&gt;scp&lt;/code&gt; to send a files, but I prefer to use dufs that is a static file server in that can be accessed in the browser. Using &lt;code&gt;dufs&lt;/code&gt; I can upload files and make quick edits straight in the browser from any device. Surprise! &lt;code&gt;dufs&lt;/code&gt; is also in the Termux repos and is so easy to get up and running. Again, message me if you’d like to see a write-up about it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Installs&lt;/head&gt;
    &lt;p&gt;Of course I need some basic utilities. These are the utilities I need to have at a minimum when working with a Linux system:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;rsync&lt;/item&gt;
      &lt;item&gt;openssh&lt;/item&gt;
      &lt;item&gt;git&lt;/item&gt;
      &lt;item&gt;wget&lt;/item&gt;
      &lt;item&gt;curl&lt;/item&gt;
      &lt;item&gt;fish shell&lt;/item&gt;
      &lt;item&gt;cronie&lt;/item&gt;
      &lt;item&gt;termux-services&lt;/item&gt;
      &lt;item&gt;iperf3&lt;/item&gt;
      &lt;item&gt;speedtest-go&lt;/item&gt;
      &lt;item&gt;screen&lt;/item&gt;
      &lt;item&gt;helix&lt;/item&gt;
      &lt;item&gt;hugo&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Restart Termux and use &lt;code&gt;sv-enable&lt;/code&gt; to run certain items as services. I do this for &lt;code&gt;sshd&lt;/code&gt; and &lt;code&gt;cronie&lt;/code&gt;. It looks like this:&lt;/p&gt;
    &lt;code&gt;$ sv-enable sshd
$ sv-enable cronie
&lt;/code&gt;
    &lt;p&gt;After running &lt;code&gt;sv-enable&lt;/code&gt;, restart Termux.&lt;/p&gt;
    &lt;head rend="h4"&gt;openssh&lt;/head&gt;
    &lt;p&gt;I could build all of this straight from the phone using either the touchscreen keyboard or connecting a standard keyboard and mouse either with a USB-C dock or bluetooth. But, I want to manage this like all of my other servers, which is to &lt;code&gt;ssh&lt;/code&gt; into the device and work from my desk.&lt;/p&gt;
    &lt;p&gt;There is an official guide for setting up an &lt;code&gt;ssh&lt;/code&gt; server. All I will add here is some pointers I learned along the way.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Adding an &lt;code&gt;ssh&lt;/code&gt;key is simple and should be one of the first steps done. After generating the key and importing with&lt;code&gt;ssh-copy-id&lt;/code&gt;from the desktop, edit the&lt;code&gt;sshd&lt;/code&gt;file in&lt;code&gt;$PREFIX/etc/ssh/sshd_config&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Termux generates its own username and cannot be changed. Run &lt;code&gt;whoami&lt;/code&gt;to see what it is.&lt;/item&gt;
      &lt;item&gt;It is the same for the &lt;code&gt;ssh&lt;/code&gt;port. As far as I can tell you cannot change the port, which is automatically set to 8022.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Running the site&lt;/head&gt;
    &lt;p&gt;There are lots of guides out there on how to setup a &lt;code&gt;hugo&lt;/code&gt; site. I have an existing site that I migrated from a VM to this phone, so my notes do not include how to get a &lt;code&gt;hugo&lt;/code&gt; site running. I also do not need to do any port forwarding as I already have a reverse proxy that I just changed where it points for my blog.&lt;/p&gt;
    &lt;p&gt;I would like to hear feedback if there is a need to add those notes here. Message me on Mastodon or by email using the links at the bottom of this post.&lt;/p&gt;
    &lt;p&gt;Below are notes on how I use the package &lt;code&gt;cronie&lt;/code&gt; to start the blog using &lt;code&gt;screen&lt;/code&gt; and the automatically reload the blog occasionally. &lt;code&gt;cronie&lt;/code&gt; is for setting up &lt;code&gt;cron&lt;/code&gt; tasks. Once installed and enabled, run &lt;code&gt;crontab -e&lt;/code&gt; like usual to setup tasks.&lt;/p&gt;
    &lt;p&gt;This is how I do it.&lt;/p&gt;
    &lt;p&gt;First, set a &lt;code&gt;fish&lt;/code&gt; alias for the command to reload the blog:&lt;/p&gt;
    &lt;code&gt;alias blog_run='cd /data/data/com.termux/files/home/&amp;lt;website_root_dir&amp;gt; &amp;amp;&amp;amp; /data/data/com.termux/files/usr/bin/hugo serve --bind=0.0.0.0 --baseURL=https://blog.ctms.me --appendPort=false --environment=production --disableFastRender --cacheDir /data/data/com.termux/files/home/&amp;lt;website_root_dir&amp;gt;/cache'

funcsave blog_run
&lt;/code&gt;
    &lt;p&gt;Now, create a script and place in &lt;code&gt;~/scripts&lt;/code&gt; that closes a previous instance of &lt;code&gt;screen&lt;/code&gt;, clears the cache, and then starts a new &lt;code&gt;screen&lt;/code&gt; session titled “hugo” and execute the alias:&lt;/p&gt;
    &lt;code&gt;#! /bin/bash
screen -X -S "hugo" quit
rm -rf /data/data/com.termux/files/home/&amp;lt;website_root_dir/cache/&amp;lt;site_name_dir&amp;gt;/filecache/getresource/
screen -S hugo -d -m fish -c 'blog_run; exec fish'
&lt;/code&gt;
    &lt;p&gt;Last, set it to run with &lt;code&gt;cron&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;*/5 * * * * cd /data/data/com.termux/files/home/scripts &amp;amp;&amp;amp; sh blog_reload.sh
&lt;/code&gt;
    &lt;head rend="h2"&gt;Backing up&lt;/head&gt;
    &lt;p&gt;Since Termux supports &lt;code&gt;ssh&lt;/code&gt; connections, I can use it on remote machines to pull the files from the phone using &lt;code&gt;rsync&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;First, need to install &lt;code&gt;rsync&lt;/code&gt; on the phone with &lt;code&gt;pkg install rsync&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h4"&gt;Desktop backup&lt;/head&gt;
    &lt;p&gt;Now we can run it from my desktop to pull the files:&lt;/p&gt;
    &lt;code&gt;rsync -aP pixel:~/&amp;lt;website_root_dir&amp;gt; /local/dir/pixel_blog/
&lt;/code&gt;
    &lt;p&gt;On my desktop, I have this for &lt;code&gt;cron&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;@reboot sleep 30 &amp;amp;&amp;amp; rsync -aP pixel:~/&amp;lt;website_root_dir&amp;gt; /local/dir/pixel_blog/ &amp;gt;&amp;gt; $HOME/logs/pixel-hugo-backup.log 2&amp;gt;&amp;amp;1
&lt;/code&gt;
    &lt;head rend="h4"&gt;nas backup&lt;/head&gt;
    &lt;p&gt;This is the same configuration. The only difference is the backup location and the &lt;code&gt;cron&lt;/code&gt; timing.&lt;/p&gt;
    &lt;code&gt;rsync -aP pixel:~/&amp;lt;website_root_dir&amp;gt; /local/dir/pixel_blog
&lt;/code&gt;
    &lt;p&gt;The automation:&lt;/p&gt;
    &lt;code&gt;5 6 * * * rsync -aP pixel:~/&amp;lt;website_root_dir&amp;gt; /local/dir/pixel_blog &amp;gt;&amp;gt; $HOME/logs/pixel-hugo-backup.log 2&amp;gt;&amp;amp;1
&lt;/code&gt;
    &lt;head rend="h4"&gt;git backup&lt;/head&gt;
    &lt;p&gt;I have a local self-hosted git instance I push backups to, but you can totally set it up to send them to Github or whatever forge you use. No instructions here because there are plenty of guides out there on how to set this up.&lt;/p&gt;
    &lt;p&gt;- - - - -&lt;/p&gt;
    &lt;p&gt;Thank you for reading! If you would like to comment on this post you can start a conversation on the Fediverse. Message me on Mastodon at @cinimodev@masto.ctms.me. Or, you may email me at blog.discourse904@8alias.com. This is an intentionally masked email address that will be forwarded to the correct inbox.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45110209</guid></item><item><title>Indices, not Pointers</title><link>https://joegm.github.io/blog/indices-not-pointers/</link><description>&lt;doc fingerprint="2ec99cf839102298"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Indices, not Pointers&lt;/head&gt;
    &lt;p&gt;There is a pattern I’ve learned while using Zig which I’ve never seen used in any other language. It’s an extremely simple trick which - when applied to a data structure - reduces memory usage, reduces memory allocations, speeds up accesses, makes freeing instantaneous, and generally makes everything much, much faster. The trick is to use indices, not pointers.&lt;/p&gt;
    &lt;p&gt;This is something I learned from a talk by Andrew Kelley (Zig’s creator) on data-oriented design. It’s used in Zig’s compiler to make very memory-efficient ASTs, and can be applied to pretty much any node-based data structure, usually trees.&lt;/p&gt;
    &lt;p&gt;So what does this mean exactly? Well, to use indices means to store the nodes of the data structure in a dynamic array, appending new nodes instead of individually allocating them. Nodes can then reference each other via indices instead of pointers.&lt;/p&gt;
    &lt;p&gt;Pretty simple, right? But this strategy has some major performance benefits.&lt;/p&gt;
    &lt;head rend="h2"&gt;Smaller Nodes&lt;/head&gt;
    &lt;p&gt;A pointer costs 8 bytes to store on a modern 64-bit system, but unless your planning on storing over 4 billion nodes in memory, an index can be stored in just 4 bytes.&lt;/p&gt;
    &lt;head rend="h2"&gt;Faster Access&lt;/head&gt;
    &lt;p&gt;Due to the reduced node size and the fact that nodes are stored contiguously in memory, the data structure will fit into fewer memory pages and more nodes will fit in the cpu’s cache line, which generally improves access times significantly.&lt;/p&gt;
    &lt;head rend="h2"&gt;Less Allocation Overhead&lt;/head&gt;
    &lt;p&gt;The way most people learn to implement data structures like trees is to make a separate allocation for each individual node, one at a time. This is a very naive way of allocating memory, however, as each memory allocation comes with a small but significant overhead which can really slow things down for a large number of nodes. Storing nodes in a growable arraylist minimizes this overhead as arraylists grow superlinearly (e.g, doubling in size each time more space is needed) meaning the majority of new nodes can just be placed in the next available slot without requesting more memory!&lt;/p&gt;
    &lt;head rend="h2"&gt;Instant Frees&lt;/head&gt;
    &lt;p&gt;Freeing structures which are allocated in the traditional “nest of pointers” fashion can be very slow, as the entire structure has to be traversed to find and individually free each node. Storing nodes in a single allocation eliminates this problem entirely and freeing the structure becomes just a single free call, as it should be.&lt;/p&gt;
    &lt;head rend="h2"&gt;A Downside - Freeing Single Nodes&lt;/head&gt;
    &lt;p&gt;One disadvantage of storing all the nodes in a contiguous buffer is that it makes it harder to free an individual node as removing a single element from an arraylist would involve shifting over all the elements after it, a linear time operation which is almost always too slow to be practical. In practice this isn’t something you normally need to do as many data structures, like an AST, can be freed all at once, but if you need to be able to free individual nodes and still want to use this technique then the obvious solution would be to use a freelist.&lt;/p&gt;
    &lt;head rend="h3"&gt;Freelists&lt;/head&gt;
    &lt;p&gt;A freelist is, as the name suggests, a list used to track free slots in memory allocators. In our case we can simply use a stack to store indices of free slots in our arraylist and attempt to pop off this stack any time we add a new element. The extra code complexity should be weighed against the actual performance benefit when considering this approach.&lt;/p&gt;
    &lt;head rend="h2"&gt;Code Example&lt;/head&gt;
    &lt;p&gt;Here is a short demo of this technique in Zig (v0.14.1). There are some Zig quirks involved like passing memory allocators and using an enum as an index type but hopefully the general idea is clear.&lt;/p&gt;
    &lt;code&gt;pub fn main() !void {
    var debug_allocator = std.heap.DebugAllocator(.{}).init;
    defer _ = debug_allocator.deinit();

    var tree = Tree{
        // Zig uses a memory allocator interface to allow us to pass in an allocation strategy for the arraylist to use.
        .nodes = ArrayList(Tree.Node).init(debug_allocator.allocator()),
    };
    defer tree.nodes.deinit();

    // append the root node.
    const root = try tree.createNode(45);

    const a = try tree.createNode(-10);
    const b = try tree.createNode(89000);
    const c = try tree.createNode(2);

    tree.setLeftChild(root, a);
    tree.setRightChild(root, b);
    tree.setLeftChild(b, c);

    printTree(&amp;amp;tree);
}

const Tree = struct {
    /// Stores all the nodes in the tree. The root node is at index 0.
    nodes: ArrayList(Node),

    const Node = struct {
        data: i32,
        left_child: NodeIndex = .none,
        right_child: NodeIndex = .none,
    };

    // In Zig it is common to use a non-exhaustive enum instead of a bare integer for indices
    // to add back some of the type safety which is lost since we're not using pointers.
    const NodeIndex = enum(u32) {
        // The root nodes is stored at index 0, so 0 can be used as a null-value for child indices.
        none = 0,
        _,
    };

    fn createNode(tree: *Tree, value: i32) std.mem.Allocator.Error!NodeIndex {
        const index: NodeIndex = @enumFromInt(@as(u32, @intCast(tree.nodes.items.len)));
        try tree.nodes.append(.{ .data = value });
        return index;
    }

    fn setLeftChild(tree: *const Tree, parent: NodeIndex, child: NodeIndex) void {
        tree.nodes.items[@intFromEnum(parent)].left_child = child;
    }

    fn setRightChild(tree: *const Tree, parent: NodeIndex, child: NodeIndex) void {
        tree.nodes.items[@intFromEnum(parent)].right_child = child;
    }
};

fn printTree(tree: *const Tree) void {
    assert(tree.nodes.items.len &amp;gt; 0);

    // print the root node.
    printNode(tree, @enumFromInt(0), 0);
}

fn printNode(tree: *const Tree, node_index: Tree.NodeIndex, depth: u32) void {
    const node = tree.nodes.items[@intFromEnum(node_index)];

    for (0..depth) |_| print("  ", .{});
    print("[{d}] {d}\n", .{ @intFromEnum(node_index), node.data });

    if (node.left_child != .none) printNode(tree, node.left_child, depth + 1);
    if (node.right_child != .none) printNode(tree, node.right_child, depth + 1);
}

const std = @import("std");
const ArrayList = std.ArrayList;
const assert = std.debug.assert;
const print = std.debug.print;
&lt;/code&gt;
    &lt;p&gt;And here is the output:&lt;/p&gt;
    &lt;code&gt;$ zig run indices.zig
[0] 45
  [1] -10
  [2] 89000
    [3] 2
&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45110386</guid></item><item><title>%CPU utilization is a lie</title><link>https://www.brendanlong.com/cpu-utilization-is-a-lie.html</link><description>&lt;doc fingerprint="7287344734add095"&gt;
  &lt;main&gt;
    &lt;p&gt;I deal with a lot of servers at work, and one thing everyone wants to know about their servers is how close they are to being at max utilization. It should be easy, right? Just pull up &lt;code&gt;top&lt;/code&gt; or another system monitor tool, look at network, memory and CPU utilization, and whichever one is the highest tells you how close you are to the limits.&lt;/p&gt;
    &lt;p&gt;And yet, whenever people actually try to project these numbers, they find that CPU utilization doesn't quite increase linearly. But how bad could it possibly be?&lt;/p&gt;
    &lt;p&gt;To answer this question, I ran a bunch of stress tests and monitored both how much work they did and what the system-reported CPU utilization was, then graphed the results.&lt;/p&gt;
    &lt;head rend="h1"&gt;Setup&lt;/head&gt;
    &lt;p&gt;For my test machine, I used a desktop computer running Ubuntu with a Ryzen 9 5900X (12 core / 24 thread) processor. I also enabled Precision Boost Overdrive (i.e. Turbo).&lt;/p&gt;
    &lt;p&gt;I vibe-coded a script that runs stress-ng in a loop, first using 24 workers and attempting to run them each at different utilizations from 1% to 100%, then using 1 to 24 workers all at 100% utilization. It used different stress testing method and measured the number of operations that could be completed ("Bogo ops1").&lt;/p&gt;
    &lt;p&gt;The reason I did two different methods was that operating systems are smart about how they schedule work, and scheduling a small number of workers at 100% utilization can be done optimally (spoilers) but with 24 workers all at 50% utilization it's hard for the OS to do anything other than spreading the work evenly.&lt;/p&gt;
    &lt;head rend="h1"&gt;Results&lt;/head&gt;
    &lt;p&gt;You can see the raw CSV results here.&lt;/p&gt;
    &lt;head rend="h2"&gt;General CPU&lt;/head&gt;
    &lt;p&gt;The most basic test just runs all of stress-ng's CPU stress tests in a loop.&lt;/p&gt;
    &lt;p&gt;You can see that when the system is reporting 50% CPU utilization, it's actually doing 60-65% of the actual maximum work it can do.&lt;/p&gt;
    &lt;head rend="h2"&gt;64-bit Integer Math&lt;/head&gt;
    &lt;p&gt;But maybe that one was just a fluke. What if we just run some random math on 64-bit integers?&lt;/p&gt;
    &lt;p&gt;This one is even worse! At "50% utilization", we're actually doing 65-85% of the max work we can get done. It can't possibly get worse than that though, right?&lt;/p&gt;
    &lt;head rend="h2"&gt;Matrix Math&lt;/head&gt;
    &lt;p&gt;Something is definitely off. Doing matrix math, "50% utilization" is actually 80% to 100% of the max work that can be done.&lt;/p&gt;
    &lt;p&gt;In case you were wondering about the system monitor screenshot from the start of the article, that was a matrix math test running with 12 workers, and you can see that it really did report 50% utilization even though additional workers do absolutely nothing (except make the utilization number go up).&lt;/p&gt;
    &lt;head rend="h1"&gt;What's Going On?&lt;/head&gt;
    &lt;head rend="h2"&gt;Hyperthreading&lt;/head&gt;
    &lt;p&gt;You might notice that this the graph keeps changing at 50%, and I've helpfully added piecewise linear regressions showing the fit.&lt;/p&gt;
    &lt;p&gt;The main reason this is happening is hyperthreading: Half of the "cores" on this machine (and most machines) are sharing resources with other cores. If I run 12 workers on this machine, they each get scheduled on their own physical core with no shared resources, but once I go over that, each additional worker is sharing resources with another. In some cases (general CPU benchmarks), this makes things slightly worse, and in some cases (SIMD-heavy matrix math), there are no useful resources left to share.&lt;/p&gt;
    &lt;head rend="h2"&gt;Turbo&lt;/head&gt;
    &lt;p&gt;It's harder to see, but Turbo is also having an effect. This particular processor runs at 4.9 GHz at low utilization, but slowly drops to 4.3 GHz as more cores become active2.&lt;/p&gt;
    &lt;p&gt;Note the zoomed-in y-axis. The clock speed "only" drops by 15% on this processor.&lt;/p&gt;
    &lt;p&gt;Since CPU utilization is calculated as busy cycles / total cycles, this means the denominator is getting smaller as the numerator gets larger, so we get yet another reason why actual CPU utilization increases faster than linearly.&lt;/p&gt;
    &lt;head rend="h1"&gt;Does This Matter?&lt;/head&gt;
    &lt;p&gt;If you look at CPU utilization and assume it will increase linearly, you're going to have a rough time. If you're using the CPU efficiently (running above "50%" utilization), the reported utilization is an underestimate, sometimes significantly so.&lt;/p&gt;
    &lt;p&gt;And keep in mind that I've only shown results for one processor, but hyperthreading performance and Turbo behavior can vary wildly between different processors, especially from different companies (AMD vs Intel).&lt;/p&gt;
    &lt;p&gt;The best way I know to work around this is to run benchmarks and monitor actual work done:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Benchmark how much work your server can do before having errors or unacceptable latency.&lt;/item&gt;
      &lt;item&gt;Report how much work your server is currently doing.&lt;/item&gt;
      &lt;item&gt;Compare those two metrics instead of CPU utilization.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Bogo ops is presumably a reference to BogoMIPS, a "bogus" benchmark that Linux does at startup to very roughly understand CPU performance.Â â©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;One of the main constraints processors operate under is needing to dissipate heat fast enough. When only one core is running, the processor can give that core some of the heat headroom that other cores aren't using and run it faster, but it can't do that all of the cores are running.Power usage works similarly and can be a constraint in some environments (usually not in a desktop computer, but frequently in servers).Â â©&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45110688</guid></item><item><title>Show HN: LightCycle, a FOSS game in Rust based on Tron</title><link>https://github.com/Tortured-Metaphor/LightCycle</link><description>&lt;doc fingerprint="d9086396b91ed9f"&gt;
  &lt;main&gt;
    &lt;p&gt;A classic TRON-inspired light cycle game built with Rust and ggez.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Single-player and Two-player modes - Battle against AI or a friend&lt;/item&gt;
      &lt;item&gt;Adjustable AI Difficulty - Easy, Medium, and Hard AI opponents&lt;/item&gt;
      &lt;item&gt;Boost Mechanic - Limited energy boost system for strategic gameplay&lt;/item&gt;
      &lt;item&gt;Visual Effects - Particle trails, screen shake, and glow effects&lt;/item&gt;
      &lt;item&gt;Pause Menu - Full pause functionality with in-game controls&lt;/item&gt;
      &lt;item&gt;Retro Aesthetic - 8-bit styled graphics with neon colors&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;1&lt;/code&gt;- Start single-player game&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;2&lt;/code&gt;- Start two-player game&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;D&lt;/code&gt;- Cycle AI difficulty (Easy/Medium/Hard)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;W&lt;/code&gt;/&lt;code&gt;A&lt;/code&gt;/&lt;code&gt;S&lt;/code&gt;/&lt;code&gt;D&lt;/code&gt;- Movement&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Left Shift&lt;/code&gt;- Boost&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;Arrow Keys&lt;/code&gt;- Movement&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Right Shift&lt;/code&gt;- Boost&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;P&lt;/code&gt;- Pause/Resume&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;ESC&lt;/code&gt;- Return to menu&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Rust (latest stable version)&lt;/item&gt;
      &lt;item&gt;Cargo&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Clone the repository
git clone https://github.com/Tortured-Metaphor/LightCycle.git
cd LightCycle

# Build the project
cargo build --release

# Run the game
cargo run --release&lt;/code&gt;
    &lt;p&gt;Navigate your light cycle around the arena, leaving a trail behind you. Avoid crashing into walls, your own trail, or your opponent's trail. The last cycle standing wins!&lt;/p&gt;
    &lt;p&gt;Use your boost strategically - it doubles your speed but drains energy quickly. Energy regenerates when not boosting.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Easy: Shorter reaction time, makes mistakes more often&lt;/item&gt;
      &lt;item&gt;Medium: Balanced gameplay, moderate challenge&lt;/item&gt;
      &lt;item&gt;Hard: Advanced pathfinding, optimal decision making, aggressive boost usage&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Built with:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;v0.2.0 - Added pause menu, boost mechanics, AI difficulties, visual effects&lt;/item&gt;
      &lt;item&gt;v0.1.0 - Initial game implementation&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This project is open source and available under the MIT License.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45110748</guid></item><item><title>Zig Software Foundation 2025 Financial Report and Fundraiser</title><link>https://ziglang.org/news/2025-financials/</link><description>&lt;doc fingerprint="8f4c508dc8b747ac"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;2025 Financial Report and Fundraiser&lt;/head&gt;
    &lt;head rend="h3"&gt;September 02, 2025&lt;/head&gt;
    &lt;p&gt;Zig Software Foundation is a 501(c)(3) non-profit organization which I am proud to say makes extremely efficient use of monetary resources. Unlike many of our peers, our primary expense is direct payments to contributors for their enhancements to the Zig project.&lt;/p&gt;
    &lt;p&gt;Donât take my word for it - letâs look at some numbers.&lt;/p&gt;
    &lt;head rend="h2"&gt;2024 Expenditures&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Expense Name&lt;/cell&gt;
        &lt;cell role="head"&gt;2024 Cost&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Contractors&lt;/cell&gt;
        &lt;cell&gt;$306,362.09&lt;/cell&gt;
        &lt;cell&gt;Direct compensation to contributors working on Zig at a rate of $60/hour.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Employees&lt;/cell&gt;
        &lt;cell&gt;$154,263.32&lt;/cell&gt;
        &lt;cell&gt;ZSF has one employee which is yours truly, Andrew Kelley, serving the role of Lead Software Engineer. In 2024, I also volunteered my time as President of Zig Software Foundation, volunteered as Accounting Clerk, and volunteered as Development Director. In the future, it would be nice to have dedicated staff for these roles so that I could focus more on being Lead Software Engineer.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Accounting&lt;/cell&gt;
        &lt;cell&gt;$18,463.62&lt;/cell&gt;
        &lt;cell&gt;This is entirely paying our accountant, Strada Financial Group, to keep the American legal system happy and keep our organization tax-exempt. I suspected we were being overcharged, and in 2025 switched to a different accountant.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;CI &amp;amp; Website&lt;/cell&gt;
        &lt;cell&gt;$14,986.73&lt;/cell&gt;
        &lt;cell&gt;Zig has great cross-compiling abilities in part due to investing in testing infrastructure for different systems. Some of these costs were one-time costs to purchase machines that sit in our homes and offices while others are market-rate Hetzner bare metal machines that we run GitHub Actions on.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Taxes&lt;/cell&gt;
        &lt;cell&gt;$13,089.07&lt;/cell&gt;
        &lt;cell&gt;Although ZSF is a tax-exempt organization, employees are still required to pay income tax.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Travel&lt;/cell&gt;
        &lt;cell&gt;$6,955.61&lt;/cell&gt;
        &lt;cell&gt;In a 2024 meeting, the board decided that the previous year's travel budget successfully helped grow Zig adoption, and retained the same budget of $15,000. In 2024, ZSF spent $6,956 of those allocated funds, increasing Zig's presence in Italy and Germany.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Sponsorships&lt;/cell&gt;
        &lt;cell&gt;$5,846.24&lt;/cell&gt;
        &lt;cell&gt;The Zig project is mostly comprised of in-house code, however, it also relies on third party projects. Today, every Zig installation includes some source files or ported code from musl libc, mingw-w64, and others. ZSF donates money to these projects as a way to say thanks, give back to the ecosystem, and increase the sustainability of Zig's dependencies.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Bank Fees&lt;/cell&gt;
        &lt;cell&gt;$782.23&lt;/cell&gt;
        &lt;cell&gt;This is a tiny slice of the pie, but every time ZSF wires money, there is a transaction fee. Our contractors graciously bill infrequently when possible to help reduce this cost.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Total Expenses&lt;/cell&gt;
        &lt;cell&gt;$520,748.91&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Even with a 13% bigger budget, we still managed to spend 92% of our money in 2024 paying contributors for their time.&lt;/p&gt;
    &lt;head rend="h2"&gt;Major Initiatives in 2024&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Zig 0.13.0 Released&lt;/item&gt;
      &lt;item&gt;Zig 0.14.0 Released&lt;list rend="ul"&gt;&lt;item&gt;Greatly expanded support for more targets that can be correctly cross-compiled and run on.&lt;/item&gt;&lt;item&gt;Major language enhancements&lt;/item&gt;&lt;item&gt;Major standard library enhancements&lt;/item&gt;&lt;item&gt;Major build system enhancements&lt;/item&gt;&lt;item&gt;Zig 0.14.1 released with bug fixes only.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So far so good. You can see weâve been hard at work spending our esteemed donorsâ money on advancing the mission statement.&lt;/p&gt;
    &lt;p&gt;However, if we look at the trend of donations over time for the year 2024, we see overall a slow decline. This is why weâre doing another fundraiser this year.&lt;/p&gt;
    &lt;head rend="h2"&gt;2024-2025 Donations Per Month&lt;/head&gt;
    &lt;p&gt;The big spike is half of Mitchell Hashimotoâs pledge.&lt;/p&gt;
    &lt;p&gt;More to the point, the second half will buy us another year to raise donations in order to keep our bank balance positive. ZSF neither borrows money nor invests money; we convert donations directly into progress on the Zig project.&lt;/p&gt;
    &lt;head rend="h2"&gt;2024-2025 Cash On Hand&lt;/head&gt;
    &lt;p&gt;Meanwhile, user activity continues to skyrocket. A rapidly increasing user base is adding Zig to their software stacks, filing issues, sending pull requests, asking for help, and shipping software that depends on Zig.&lt;/p&gt;
    &lt;head rend="h2"&gt;2024 GitHub Issues Per Month&lt;/head&gt;
    &lt;p&gt;The top line is Total Issues Opened and the bottom line is Total Issues Closed. The gap is widening - more users are demanding more attention than Zig core team has time for.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Average time to close issues&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;All Time&lt;/cell&gt;
        &lt;cell&gt;7 months&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Past Year&lt;/cell&gt;
        &lt;cell&gt;11 months&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Past Month&lt;/cell&gt;
        &lt;cell&gt;over 1 year&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Average time to close pull requests&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;All Time&lt;/cell&gt;
        &lt;cell&gt;16 days&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Past Year&lt;/cell&gt;
        &lt;cell&gt;30 days&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Past Month&lt;/cell&gt;
        &lt;cell&gt;2 months&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Source: Repo Trends&lt;/p&gt;
    &lt;head rend="h2"&gt;Total GitHub Stars&lt;/head&gt;
    &lt;p&gt;Source: OSS Insight&lt;/p&gt;
    &lt;p&gt;In response to this rising demand, we added Alex RÃ¸nne Petersen to the Zig core team. Thanks to the income that was available to us in 2024, we were able to offer new contracts.&lt;/p&gt;
    &lt;head rend="h2"&gt;2024 Income&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Income Name&lt;/cell&gt;
        &lt;cell role="head"&gt;2024 Amount&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;GitHub Sponsors&lt;/cell&gt;
        &lt;cell&gt;$170,656.04&lt;/cell&gt;
        &lt;cell&gt;Zig on GitHub Sponsors. This category contains a numerous amount of individuals and companies - each less than $1000/month. We recommend those donating via GitHub Sponsors to switch to Every.org since they process receipts and are a non-profit organization themselves and are not in the process of neglecting their core product in the face of the AI boom.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Mitchell Hashimoto&lt;/cell&gt;
        &lt;cell&gt;$150,000.00&lt;/cell&gt;
        &lt;cell&gt;A generous individual. While his family's donation has helped ZSF immensely in 2024 and will continue to do so in 2025, neither ZSF nor Mitchell himself wants him to be this large a slice of the pie!&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Every.org&lt;/cell&gt;
        &lt;cell&gt;$90,097.45&lt;/cell&gt;
        &lt;cell&gt;Every.org is a fellow 501(c)(3) non-profit that manages donation collecting for other non-profits. They've been good to us; it's our preferred method of receiving donations. This category is mostly individuals along with a few small donations from companies.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Bun&lt;/cell&gt;
        &lt;cell&gt;$60,000.00&lt;/cell&gt;
        &lt;cell&gt;Bun is a fast JavaScript all-in-one toolkit.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;TigerBeetle&lt;/cell&gt;
        &lt;cell&gt;$60,000.00&lt;/cell&gt;
        &lt;cell&gt;TigerBeetle is a financial transaction database with 1000x faster OLTP performance, mission critical safety, and indestructible storage fault tolerance.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Benevity&lt;/cell&gt;
        &lt;cell&gt;$36,195.58&lt;/cell&gt;
        &lt;cell&gt;They help us collect company-matched donations from employees. This category contains a number of individuals.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;ZML&lt;/cell&gt;
        &lt;cell&gt;$33,000.00&lt;/cell&gt;
        &lt;cell&gt;ZML offers high performance inference on any model for any hardware.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Mitchell Kember&lt;/cell&gt;
        &lt;cell&gt;$21,027.00&lt;/cell&gt;
        &lt;cell&gt;A generous individual.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Individuals&lt;/cell&gt;
        &lt;cell&gt;$19,312.52&lt;/cell&gt;
        &lt;cell&gt;This category contains many people who donated via paper checks or other miscellaneous ways.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Russel Simmons&lt;/cell&gt;
        &lt;cell&gt;$16,384.00&lt;/cell&gt;
        &lt;cell&gt;A generous individual.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Blacksmith&lt;/cell&gt;
        &lt;cell&gt;$14,000.00&lt;/cell&gt;
        &lt;cell&gt;Blacksmith is a dead simple, drop-in replacement that costs 75% less than GitHub runners.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Total Income&lt;/cell&gt;
        &lt;cell&gt;$670,672.59&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;However, with our current level of recurring income, we will not be able to renew everyoneâs contracts, nor offer new contracts to Zig core team members.&lt;/p&gt;
    &lt;head rend="h2"&gt;A Plea for Donations&lt;/head&gt;
    &lt;p&gt;We have extremely talented Zig core team members who want to renew their contracts, and others who are interested to start getting paid for their valuable work for the first time.&lt;/p&gt;
    &lt;p&gt;In order to do this, we need more recurring donations. I for one do not enjoy asking for money, but in the interest of our users and contributors, it would be irresponsible not to.&lt;/p&gt;
    &lt;p&gt;Please sign up for a monthly donation if you can. Our preferred donation method is via Every.org. A fellow 501(c)(3) non-profit, they seamlessly manage gift receipts, and are not pivoting to AI like GitHub is currently doing.&lt;/p&gt;
    &lt;p&gt;More details including our EIN and address for paper checks&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Companies&lt;/cell&gt;
        &lt;cell&gt;Contact us to get your logo on ziglang.org in exchange for a monthly donation.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Employees&lt;/cell&gt;
        &lt;cell&gt;Check if your company matches donations to charities such as Zig Software Foundation. That 2x multiplier makes a huge difference. We're already in the system.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Venture Capitalists&lt;/cell&gt;
        &lt;cell&gt;We are aware of a few startups betting on Zig as their language and toolchain of choice to build tomorrow's critical infrastructure. Helping the Zig Software Foundation reach v1.0 faster is one of the most efficient uses of capital you can make to boost your portfolio.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Individuals&lt;/cell&gt;
        &lt;cell&gt;Can you spare $10 per month? This is our favorite kind of donation because it helps diversify ZSF's income, keeping us free from undue influence from any single party. If not, don't sweat it. We'll be OK.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Huge thanks to all who graciously donate funds to our cause. Together we serve the users!&lt;/p&gt;
    &lt;p&gt;-Andrew&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45111405</guid></item><item><title>Comic Sans typeball designed to work with the IBM Selectric typewriters</title><link>https://www.printables.com/model/441233-comic-sans-typeball-for-the-ibm-selectric-typewrit</link><description>&lt;doc fingerprint="16569da08d87a55b"&gt;
  &lt;main&gt;
    &lt;p&gt;&lt;lb/&gt;Update, July 7 2023: Dave Hayden took the resin-printed typeball concept and improved on it greatly. I'm extremely grateful that he took on all the hard work of iteratively going through and dialing in the perfect values to make a functional ball, and I'm pleased to think I contributed in some way to his achievements.&lt;/p&gt;
    &lt;p&gt;-----------------------------------------&lt;lb/&gt;This is a Comic Sans typeball designed to work with the IBM Selectric typewriters that take 88-character type elements. More information about the 3D printed typeballs can be found at the Github repo for the project.&lt;lb/&gt;I have not yet printed and tested this exact model! I tested a previous revision which was just a bit too tall and 90Â° off; I've corrected the letter rotations and shaved 0.2 mm off the height.&lt;lb/&gt;To affix the typeball to the typewriter, you will need a small clip such as this one or a bent wire.&lt;lb/&gt;The blank ball (not the letters) is an edited version of the OpenSCAD Selectric Typeball by 1944GPW and is licensed with the Creative Commons - Attribution license.&lt;/p&gt;
    &lt;p&gt;The author marked this model as their own original creation.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45111909</guid></item><item><title>Finnish City Inaugurates 1 MW/100 MWh Sand Battery</title><link>https://cleantechnica.com/2025/08/30/finnish-city-inaugurates-1-mw-100-mwh-sand-battery/</link><description>&lt;doc fingerprint="ba7080ffaaaf37b1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Finnish City Inaugurates 1 MW/100 MWh Sand Battery&lt;/head&gt;
    &lt;p&gt;Support CleanTechnica's work through a Substack subscription or on Stripe.&lt;/p&gt;
    &lt;p&gt;There are more ways to store energy than just using batteries. Some are using fire bricks, particularly for process heat for industries that rely on high heat in manufacturing. Others propose an arrangement of massive concrete blocks that move up and down like the weights of a giant grandfather clock, converting kinetic energy to potential energy and back again. In Finland, two intrepid engineers began experimenting with a sand battery a few years ago.&lt;/p&gt;
    &lt;p&gt;As we reported when the first prototype was unveiled three years ago, the idea of a sand battery began with two Finnish engineers, Markku Ylönen and Tommi Eronen. The concept is simplicity itself. Make a really big pile of sand. Heat it with excess renewable electricity to around 500°C (932°F), then use that heat later to heat homes, factories, even swimming pools. They say the sand can stay hot for 3 months or more. The pair have founded Polar Night Energy, which constructed a prototype consisting of 100 tons of sand inside what looks like a silo in the town of Kankaanpää.&lt;/p&gt;
    &lt;p&gt;Many Americans are unfamiliar with the concept of district heating, but it is widely used in other counties, especially in Scandinavia where keeping schools, municipal buildings, arenas, factories, and homes warm in winter is a challenge.&lt;/p&gt;
    &lt;p&gt;Loviisan Lämpö is a Finnish district heating company that supplies district heating to customers in Loviisa, Pukkila, Pornainen, and Pyhtää. It has collaborated with Polar Night on a new sand battery — one that is much larger than the prototype — which began operating in the city of Pornainen in southern Finland this month, where it is expected to reduce carbon emissions from district heating by 70 percent.&lt;/p&gt;
    &lt;p&gt;Previously, the majority of heat needed for the system came from burning oil, but that has now been completely eliminated. The system will continue to burn wood chips to supplement the sand battery. Wood chips are at least carbon neutral, although not an ideal solution since it takes years for trees to grow but only minutes for the chips to burn.&lt;/p&gt;
    &lt;p&gt;At the commissioning ceremony for the new battery, Mikko Paajanen, CEO of Loviisan Lämpö, said, “A couple of years ago, we started considering how to take district heating in Pornainen to the next level. It would have been easy to simply replace the old wood chip power plant with a new one of the same kind, but that didn’t align with our goals. We evaluated every possible alternative, and the Sand Battery proved to be the best option.”&lt;/p&gt;
    &lt;p&gt;The battery is a 42 foot tall, 50 foot wide steel cylinder filled with 2,000 tons of crushed stone. According to Fast Company, when extra renewable electricity is available, the system uses it to heat up the crushed stone, where it is stored until needed. Then the heat from the battery travels to other buildings through a system of pipes filled with hot water. Each building has its own equipment to distribute the heat to radiators, floor heaters, or other heating devices.&lt;/p&gt;
    &lt;p&gt;“We have already learnt that our system has even more potential than we initially calculated. It’s been a positive surprise,” said Ylönen after the prototype was placed in service. “Whenever there’s a high surge of available green electricity, we want to be able to get it into the storage really quickly.” The need to use energy more wisely was driven home for Finns after Russia stopped providing electricity, methane, and oil to Finland when it voted to join NATO. Finland and Russia share a common border.&lt;/p&gt;
    &lt;head rend="h3"&gt;Sand Battery Is Simple &amp;amp; Efficient&lt;/head&gt;
    &lt;p&gt;The sand battery is simplicity itself. “We just heat air and [circulate it] through sand,” says Liisa Naskali, COO of Polar Night Energy. But materials other than sand can be used. The new battery actually uses crushed soapstone chips from a local fireplace manufacturer. Sand, or other material crushed into sand-size particles, has the ability to store heat for weeks. Unlike some other batteries, the system doesn’t rely on chemicals, doesn’t degrade, and won’t catch on fire. In operation, the sand battery has demonstrated a round trip efficiency of 90 percent.&lt;/p&gt;
    &lt;p&gt;Inside the steel tank, a heat exchanger and a closed loop system are used to circulate the heat. Software runs heaters when electricity prices are low. So far this summer, the district heating operator has paid only about 10 percent of the average price of electricity because heating the system only occurred at optimal times. That helps make the technology cost competitive, although the initial installation cost is fairly high.&lt;/p&gt;
    &lt;p&gt;Polar Night is now in talks with other district heating companies and factory owners with a need for high temperature process heat. For the company, the project in Pornainen is a critical proof point. “This is really important for us because now we can show that this really works,” a spokesperson for Polar Night said.&lt;/p&gt;
    &lt;head rend="h3"&gt;Investment Opportunities&lt;/head&gt;
    &lt;p&gt;Polar Night and its partners see a bright financial future for sand batteries because they can participate in electricity reserve markets, reduce dependence on single energy sources in heat production, and serve as an excellent example of sector integration between electricity and heat.&lt;/p&gt;
    &lt;p&gt;“For us, the sand battery is a great commercial investment, but we also wanted to boldly support an innovative solution that benefits customers, the municipality, and the entire electricity market. This is a concrete example of a cost efficient and sustainable investment. If it works here, it will work anywhere,” said Sauli Antila, the investment director at CapMan Infra, the corporate owner of Loviisan Lämpö.&lt;/p&gt;
    &lt;p&gt;The profitability of the sand battery is based on charging it according to electricity prices and Fingrid’s reserve markets. Its large storage capacity enables balancing the electricity grid and optimizing consumption over several days or even weeks. The reserve market operations and optimization of the Pornainen Sand Battery are managed by the software unit division of Elisa Industriq.&lt;/p&gt;
    &lt;p&gt;“The Pornainen plant can be adjusted quickly and precisely, and it also has a remarkably long energy buffer, making it well suited for reserve market optimization. Our AI solution automatically identifies the best times to charge and discharge the Sand Battery and allocates flexibility capacity to the reserve products that need it most. Continuous optimization makes it a genuinely profitable investment,” explained Jukka-Pekka Salmenkaita, vice president of AI and special projects at Elisa Industriq.&lt;/p&gt;
    &lt;p&gt;Polar Night has a clear vision for the future. Construction of an electricity production pilot will begin in the coming weeks in Valkeakoski, Finland, and the company is in active negotiations for several large-scale thermal storage projects in district heating, hot air, and process steam production. “Industrial applications are particularly promising, especially where heat above 100°C is required, something electric boilers and heat pumps cannot provide,” said Polar Night COO Liisa Naskali.&lt;/p&gt;
    &lt;p&gt;This technology is never going to replace grid-scale battery storage, but could be useful in many situations where battery storage is not. A comment on the YouTube video below complained, “Not a word about return on investment in the presentation. That means it’ll never pay off. They just wasted taxpayers’ money to stroke their own egos.” MAGAlomaniacs are everywhere these days.&lt;/p&gt;
    &lt;p&gt;Sign up for CleanTechnica's Weekly Substack for Zach and Scott's in-depth analyses and high level summaries, sign up for our daily newsletter, and follow us on Google News!&lt;/p&gt;
    &lt;p&gt;Have a tip for CleanTechnica? Want to advertise? Want to suggest a guest for our CleanTech Talk podcast? Contact us here.&lt;/p&gt;
    &lt;p&gt;Sign up for our daily newsletter for 15 new cleantech stories a day. Or sign up for our weekly one on top stories of the week if daily is too frequent.&lt;/p&gt;
    &lt;p&gt;CleanTechnica uses affiliate links. See our policy here.&lt;/p&gt;
    &lt;p&gt;CleanTechnica's Comment Policy&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45112653</guid></item></channel></rss>