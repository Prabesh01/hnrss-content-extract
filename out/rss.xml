<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Mon, 03 Nov 2025 14:10:58 +0000</lastBuildDate><item><title>URLs are state containers</title><link>https://alfy.blog/2025/10/31/your-url-is-your-state.html</link><description>&lt;doc fingerprint="abc21e28a1a26d89"&gt;
  &lt;main&gt;
    &lt;p&gt;Couple of weeks ago when I was publishing The Hidden Cost of URL Design I needed to add SQL syntax highlighting. I headed to PrismJS website trying to remember if it should be added as a plugin or what. I was overwhelmed with the amount of options in the download page so I headed back to my code. I checked the file for PrismJS and at the top of the file, I found a comment containing a URL:&lt;/p&gt;
    &lt;code&gt;/* https://prismjs.com/download.html#themes=prism&amp;amp;languages=markup+css+clike+javascript+bash+css-extras+markdown+scss+sql&amp;amp;plugins=line-highlight+line-numbers+autolinker */
&lt;/code&gt;
    &lt;p&gt;I had completely forgotten about this. I clicked the URL, and it was the PrismJS download page with every checkbox, dropdown, and option pre-selected to match my exact configuration. Themes chosen. Languages selected. Plugins enabled. Everything, perfectly reconstructed from that single URL.&lt;/p&gt;
    &lt;p&gt;It was one of those moments where something you once knew suddenly clicks again with fresh significance. Here was a URL doing far more than just pointing to a page. It was storing state, encoding intent, and making my entire setup shareable and recoverable. No database. No cookies. No localStorage. Just a URL.&lt;/p&gt;
    &lt;p&gt;This got me thinking: how often do we, as frontend engineers, overlook the URL as a state management tool? We reach for all sorts of abstractions to manage state such as global stores, contexts, and caches while ignoring one of the webâs most elegant and oldest features: the humble URL.&lt;/p&gt;
    &lt;p&gt;In my previous article, I wrote about the hidden costs of bad URL design. Today, I want to flip that perspective and talk about the immense value of good URL design. Specifically, how URLs can be treated as first-class state containers in modern web applications.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Overlooked Power of URLs&lt;/head&gt;
    &lt;p&gt;Scott Hanselman famously said âURLs are UIâ and heâs absolutely right. URLs arenât just technical addresses that browsers use to fetch resources. Theyâre interfaces. Theyâre part of the user experience.&lt;/p&gt;
    &lt;p&gt;But URLs are more than UI. Theyâre state containers. Every time you craft a URL, youâre making decisions about what information to preserve, what to make shareable, and what to make bookmarkable.&lt;/p&gt;
    &lt;p&gt;Think about what URLs give us for free:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Shareability: Send someone a link, and they see exactly what you see&lt;/item&gt;
      &lt;item&gt;Bookmarkability: Save a URL, and youâve saved a moment in time&lt;/item&gt;
      &lt;item&gt;Browser history: The back button just works&lt;/item&gt;
      &lt;item&gt;Deep linking: Jump directly into a specific application state&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;URLs make web applications resilient and predictable. Theyâre the webâs original state management solution, and theyâve been working reliably since 1991. The question isnât whether URLs can store state. Itâs whether weâre using them to their full potential.&lt;/p&gt;
    &lt;p&gt;Before we dive into examples, letâs break down how URLs encode state. Hereâs a typical stateful URL:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;For many years, these were considered the only components of a URL. That changed with the introduction of Text Fragments, a feature that allows linking directly to a specific piece of text within a page. You can read more about it in my article Smarter than âCtrl+Fâ: Linking Directly to Web Page Content.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Different parts of the URL encode different types of state:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Path Segments (&lt;code&gt;/path/to/myfile.html&lt;/code&gt;). Best used for hierarchical resource navigation:&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;/users/123/posts&lt;/code&gt;- User 123âs posts&lt;/item&gt;&lt;item&gt;&lt;code&gt;/docs/api/authentication&lt;/code&gt;- Documentation structure&lt;/item&gt;&lt;item&gt;&lt;code&gt;/dashboard/analytics&lt;/code&gt;- Application sections&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Query Parameters (&lt;code&gt;?key1=value1&amp;amp;key2=value2&lt;/code&gt;). Perfect for filters, options, and configuration:&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;?theme=dark&amp;amp;lang=en&lt;/code&gt;- UI preferences&lt;/item&gt;&lt;item&gt;&lt;code&gt;?page=2&amp;amp;limit=20&lt;/code&gt;- Pagination&lt;/item&gt;&lt;item&gt;&lt;code&gt;?status=active&amp;amp;sort=date&lt;/code&gt;- Data filtering&lt;/item&gt;&lt;item&gt;&lt;code&gt;?from=2025-01-01&amp;amp;to=2025-12-31&lt;/code&gt;- Date ranges&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;&lt;del rend="overstrike"&gt;Anchor&lt;/del&gt;Fragment (&lt;code&gt;#SomewhereInTheDocument&lt;/code&gt;). Ideal for client-side navigation and page sections:&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;#L20-L35&lt;/code&gt;- GitHub line highlighting&lt;/item&gt;&lt;item&gt;&lt;code&gt;#features&lt;/code&gt;- Scroll to section&lt;/item&gt;&lt;item&gt;&lt;code&gt;#/dashboard&lt;/code&gt;- Single-page app routing (though itâs rarely used these days)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Common Patterns That Work for Query Parameters&lt;/head&gt;
    &lt;head rend="h4"&gt;Multiple values with delimiters&lt;/head&gt;
    &lt;p&gt;Sometimes youâll see multiple values packed into a single key using delimiters like commas or plus signs. Itâs compact and human-readable, though it requires manual parsing on the server side.&lt;/p&gt;
    &lt;code&gt;?languages=javascript+typescript+python
?tags=frontend,react,hooks
&lt;/code&gt;
    &lt;head rend="h4"&gt;Nested or structured data&lt;/head&gt;
    &lt;p&gt;Developers often encode complex filters or configuration objects into a single query string. A simple convention uses keyâvalue pairs separated by commas, while others serialize JSON or even Base64-encode it for safety.&lt;/p&gt;
    &lt;code&gt;?filters=status:active,owner:me,priority:high
?config=eyJyaWNrIjoicm9sbCJ9==  (base64-encoded JSON)
&lt;/code&gt;
    &lt;head rend="h4"&gt;Boolean flags&lt;/head&gt;
    &lt;p&gt;For flags or toggles, itâs common to pass booleans explicitly or to rely on the keyâs presence as truthy. This keeps URLs shorter and makes toggling features easy.&lt;/p&gt;
    &lt;code&gt;?debug=true&amp;amp;analytics=false
?mobile  (presence = true)
&lt;/code&gt;
    &lt;head rend="h4"&gt;Arrays (Bracket notation)&lt;/head&gt;
    &lt;code&gt;?tags[]=frontend&amp;amp;tags[]=react&amp;amp;tags[]=hooks
&lt;/code&gt;
    &lt;p&gt;Another old pattern is bracket notation, which represents arrays in query parameters. It originated from early web frameworks like PHP where appending &lt;code&gt;[]&lt;/code&gt; to a parameter name signals that multiple values should be grouped together.&lt;/p&gt;
    &lt;code&gt;?tags[]=frontend&amp;amp;tags[]=react&amp;amp;tags[]=hooks
?ids[0]=42&amp;amp;ids[1]=73
&lt;/code&gt;
    &lt;p&gt;Many modern frameworks and parsers (like Nodeâs &lt;code&gt;qs&lt;/code&gt; library or Express middleware) still recognize this pattern automatically. However, itâs not officially standardized in the URL specification, so behavior can vary depending on the server or client implementation. Notice how it even breaks the syntax highlighting on my website.&lt;/p&gt;
    &lt;p&gt;The key is consistency. Pick patterns that make sense for your application and stick with them.&lt;/p&gt;
    &lt;head rend="h2"&gt;State via URL Parameters&lt;/head&gt;
    &lt;p&gt;Letâs look at real-world examples of URLs as state containers:&lt;/p&gt;
    &lt;p&gt;PrismJS Configuration&lt;/p&gt;
    &lt;code&gt;https://prismjs.com/download.html#themes=prism&amp;amp;languages=markup+css+clike+javascript&amp;amp;plugins=line-numbers
&lt;/code&gt;
    &lt;p&gt;The entire syntax highlighter configuration encoded in the URL. Change anything in the UI, and the URL updates. Share the URL, and someone else gets your exact setup. This one uses anchor and not query parameters, but the concept is the same.&lt;/p&gt;
    &lt;p&gt;GitHub Line Highlighting&lt;/p&gt;
    &lt;code&gt;https://github.com/zepouet/Xee-xCode-4.5/blob/master/XeePhotoshopLoader.m#L108-L136
&lt;/code&gt;
    &lt;p&gt;It links to a specific file while highlighting lines 108 through 136. Click this link anywhere, and youâll land on the exact code section being discussed.&lt;/p&gt;
    &lt;p&gt;Google Maps&lt;/p&gt;
    &lt;code&gt;https://www.google.com/maps/@22.443842,-74.220744,19z
&lt;/code&gt;
    &lt;p&gt;Coordinates, zoom level, and map type all in the URL. Share this link, and anyone can see the exact same view of the map.&lt;/p&gt;
    &lt;p&gt;Figma and Design Tools&lt;/p&gt;
    &lt;code&gt;https://www.figma.com/file/abc123/MyDesign?node-id=123:456&amp;amp;viewport=100,200,0.5
&lt;/code&gt;
    &lt;p&gt;Before shareable design links, finding an updated screen or component in a large file was a chore. Someone had to literally show you where it lived, scrolling and zooming across layers. Today, a Figma link carries all that context like canvas position, zoom level, selected element. Literally everything needed to drop you right into the workspace.&lt;/p&gt;
    &lt;p&gt;E-commerce Filters&lt;/p&gt;
    &lt;code&gt;https://store.com/laptops?brand=dell+hp&amp;amp;price=500-1500&amp;amp;rating=4&amp;amp;sort=price-asc
&lt;/code&gt;
    &lt;p&gt;This is one of the most common real-world patterns youâll encounter. Every filter, sort option, and price range preserved. Users can bookmark their exact search criteria and return to it anytime. Most importantly, they can come back to it after navigating away or refreshing the page.&lt;/p&gt;
    &lt;head rend="h2"&gt;Frontend Engineering Patterns&lt;/head&gt;
    &lt;p&gt;Before we discuss implementation details, we need to establish a clear guideline for what should go into the URL. Not all state belongs in URLs. Hereâs a simple heuristic:&lt;/p&gt;
    &lt;p&gt;Good candidates for URL state:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Search queries and filters&lt;/item&gt;
      &lt;item&gt;Pagination and sorting&lt;/item&gt;
      &lt;item&gt;View modes (list/grid, dark/light)&lt;/item&gt;
      &lt;item&gt;Date ranges and time periods&lt;/item&gt;
      &lt;item&gt;Selected items or active tabs&lt;/item&gt;
      &lt;item&gt;UI configuration that affects content&lt;/item&gt;
      &lt;item&gt;Feature flags and A/B test variants&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Poor candidates for URL state:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Sensitive information (passwords, tokens, PII)&lt;/item&gt;
      &lt;item&gt;Temporary UI states (modal open/closed, dropdown expanded)&lt;/item&gt;
      &lt;item&gt;Form input in progress (unsaved changes)&lt;/item&gt;
      &lt;item&gt;Extremely large or complex nested data&lt;/item&gt;
      &lt;item&gt;High-frequency transient states (mouse position, scroll position)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you are not sure if a piece of state belongs in the URL, ask yourself: If someone else clicking this URL, should they see the same state? If so, it belongs in the URL. If not, use a different state management approach.&lt;/p&gt;
    &lt;head rend="h3"&gt;Implementation using Plain JavaScript&lt;/head&gt;
    &lt;p&gt;The modern &lt;code&gt;URLSearchParams&lt;/code&gt; API makes URL state management straightforward:&lt;/p&gt;
    &lt;code&gt;// Reading URL parameters
const params = new URLSearchParams(window.location.search);
const view = params.get('view') || 'grid';
const page = params.get('page') || 1;

// Updating URL parameters
function updateFilters(filters) {
  const params = new URLSearchParams(window.location.search);

  // Update individual parameters
  params.set('status', filters.status);
  params.set('sort', filters.sort);

  // Update URL without page reload
  const newUrl = `${window.location.pathname}?${params.toString()}`;
  window.history.pushState({}, '', newUrl);

  // Now update your UI based on the new filters
  renderContent(filters);
}

// Handling back/forward buttons
window.addEventListener('popstate', () =&amp;gt; {
  const params = new URLSearchParams(window.location.search);
  const filters = {
    status: params.get('status') || 'all',
    sort: params.get('sort') || 'date'
  };
  renderContent(filters);
});
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;popstate&lt;/code&gt; event fires when the user navigates with the browserâs Back or Forward buttons. It lets you restore the UI to match the URL, which is essential for keeping your appâs state and history in sync. Usually your frameworkâs router handles this for you, but itâs good to know how it works under the hood.&lt;/p&gt;
    &lt;head rend="h3"&gt;Implementation using React&lt;/head&gt;
    &lt;p&gt;React Router and Next.js provide hooks that make this even cleaner:&lt;/p&gt;
    &lt;code&gt;import { useSearchParams } from 'react-router-dom';
// or for Next.js 13+: import { useSearchParams } from 'next/navigation';

function ProductList() {
  const [searchParams, setSearchParams] = useSearchParams();

  // Read from URL (with defaults)
  const color = searchParams.get('color') || 'all';
  const sort = searchParams.get('sort') || 'price';

  // Update URL
  const handleColorChange = (newColor) =&amp;gt; {
    setSearchParams(prev =&amp;gt; {
      const params = new URLSearchParams(prev);
      params.set('color', newColor);
      return params;
    });
  };

  return (
    &amp;lt;div&amp;gt;
      &amp;lt;select value={color} onChange={e =&amp;gt; handleColorChange(e.target.value)}&amp;gt;
        &amp;lt;option value="all"&amp;gt;All Colors&amp;lt;/option&amp;gt;
        &amp;lt;option value="silver"&amp;gt;Silver&amp;lt;/option&amp;gt;
        &amp;lt;option value="black"&amp;gt;Black&amp;lt;/option&amp;gt;
      &amp;lt;/select&amp;gt;

      {/* Your filtered products render here */}
    &amp;lt;/div&amp;gt;
  );
}
&lt;/code&gt;
    &lt;head rend="h3"&gt;Best Practices for URL State Management&lt;/head&gt;
    &lt;p&gt;Now that weâve seen how URLs can hold application state, letâs look at a few best practices that keep them clean, predictable, and user-friendly.&lt;/p&gt;
    &lt;head rend="h4"&gt;Handling Defaults Gracefully&lt;/head&gt;
    &lt;p&gt;Donât pollute URLs with default values:&lt;/p&gt;
    &lt;code&gt;// Bad: URL gets cluttered with defaults
?theme=light&amp;amp;lang=en&amp;amp;page=1&amp;amp;sort=date

// Good: Only non-default values in URL
?theme=dark  // light is default, so omit it
&lt;/code&gt;
    &lt;p&gt;Use defaults in your code when reading parameters:&lt;/p&gt;
    &lt;code&gt;function getTheme(params) {
  return params.get('theme') || 'light'; // Default handled in code
}
&lt;/code&gt;
    &lt;head rend="h4"&gt;Debouncing URL Updates&lt;/head&gt;
    &lt;p&gt;For high-frequency updates (like search-as-you-type), debounce URL changes:&lt;/p&gt;
    &lt;code&gt;import { debounce } from 'lodash';

const updateSearchParam = debounce((value) =&amp;gt; {
  const params = new URLSearchParams(window.location.search);
  if (value) {
    params.set('q', value);
  } else {
    params.delete('q');
  }
  window.history.replaceState({}, '', `?${params.toString()}`);
}, 300);

// Use replaceState instead of pushState to avoid flooding history
&lt;/code&gt;
    &lt;head rend="h4"&gt;pushState vs. replaceState&lt;/head&gt;
    &lt;p&gt;When deciding between &lt;code&gt;pushState&lt;/code&gt; and &lt;code&gt;replaceState&lt;/code&gt;, think about how you want the browser history to behave. &lt;code&gt;pushState&lt;/code&gt; creates a new history entry, which makes sense for distinct navigation actions like changing filters, pagination, or navigating to a new view â users can then use the Back button to return to the previous state. On the other hand, &lt;code&gt;replaceState&lt;/code&gt; updates the current entry without adding a new one, making it ideal for refinements such as search-as-you-type or minor UI adjustments where you donât want to flood the history with every keystroke.&lt;/p&gt;
    &lt;head rend="h2"&gt;URLs as Contracts&lt;/head&gt;
    &lt;p&gt;When designed thoughtfully, URLs become more than just state containers. They become contracts between your application and its consumers. A good URL defines expectations for humans, developers, and machines alike&lt;/p&gt;
    &lt;head rend="h3"&gt;Clear Boundaries&lt;/head&gt;
    &lt;p&gt;A well-structured URL draws the line between whatâs public and whatâs private, client and server, shareable and session-specific. It clarifies where state lives and how it should behave. Developers know whatâs safe to persist, users know what they can bookmark, and machines know whats worth indexing.&lt;/p&gt;
    &lt;p&gt;URLs, in that sense, act as interfaces: visible, predictable, and stable.&lt;/p&gt;
    &lt;head rend="h3"&gt;Communicating Meaning&lt;/head&gt;
    &lt;p&gt;Readable URLs explain themselves. Consider the difference between the two URLs below.&lt;/p&gt;
    &lt;code&gt;https://example.com/p?id=x7f2k&amp;amp;v=3
https://example.com/products/laptop?color=silver&amp;amp;sort=price
&lt;/code&gt;
    &lt;p&gt;The first one hides intent. The second tells a story. A human can read it and understand what theyâre looking at. A machine can parse it and extract meaningful structure.&lt;/p&gt;
    &lt;p&gt;Jim Nielsen calls these âexamples of great URLsâ. URLs that explain themselves.&lt;/p&gt;
    &lt;head rend="h3"&gt;Caching and Performance&lt;/head&gt;
    &lt;p&gt;URLs are cache keys. Well-designed URLs enable better caching strategies:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Same URL = same resource = cache hit&lt;/item&gt;
      &lt;item&gt;Query params define cache variations&lt;/item&gt;
      &lt;item&gt;CDNs can cache intelligently based on URL patterns&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You can even visualize a userâs journey without any extra tracking code:&lt;/p&gt;
    &lt;quote&gt;graph LR A["/products"] --&amp;gt; |selects category| B["/products?category=laptops"] B --&amp;gt; |adds price filter| C["/products?category=laptops&amp;amp;price=500-1000"] style A fill:#e9edf7,stroke:#455d8d,stroke-width:2px; style B fill:#e9edf7,stroke:#455d8d,stroke-width:2px; style C fill:#e9edf7,stroke:#455d8d,stroke-width:2px;&lt;/quote&gt;
    &lt;p&gt;Your analytics tools can track this flow without additional instrumentation. Every URL parameter becomes a dimension you can analyze.&lt;/p&gt;
    &lt;head rend="h3"&gt;Versioning and Evolution&lt;/head&gt;
    &lt;p&gt;URLs can communicate API versions, feature flags, and experiments:&lt;/p&gt;
    &lt;code&gt;?v=2                   // API version
?beta=true             // Beta features
?experiment=new-ui     // A/B test variant
&lt;/code&gt;
    &lt;p&gt;This makes gradual rollouts and backwards compatibility much more manageable.&lt;/p&gt;
    &lt;head rend="h2"&gt;Anti-Patterns to Avoid&lt;/head&gt;
    &lt;p&gt;Even with the best intentions, itâs easy to misuse URL state. Here are common pitfalls:&lt;/p&gt;
    &lt;head rend="h3"&gt;âState Only in Memoryâ SPAs&lt;/head&gt;
    &lt;p&gt;The classic single-page app mistake:&lt;/p&gt;
    &lt;code&gt;// User hits refresh and loses everything
const [filters, setFilters] = useState({});
&lt;/code&gt;
    &lt;p&gt;If your app forgets its state on refresh, youâre breaking one of the webâs fundamental features. Users expect URLs to preserve context. I remember a viral video from years ago where a Reddit user vented about an e-commerce site: every time she hit âBack,â all her filters disappeared. Her frustration summed it up perfectly. If users lose context, they lose patience.&lt;/p&gt;
    &lt;head rend="h3"&gt;Sensitive Data in URLs&lt;/head&gt;
    &lt;p&gt;This one seems obvious, but itâs worth repeating:&lt;/p&gt;
    &lt;code&gt;// NEVER DO THIS
?password=secret123
&lt;/code&gt;
    &lt;p&gt;URLs are logged everywhere: browser history, server logs, analytics, referrer headers. Treat them as public.&lt;/p&gt;
    &lt;head rend="h3"&gt;Inconsistent or Opaque Naming&lt;/head&gt;
    &lt;code&gt;// Unclear and inconsistent
?foo=true&amp;amp;bar=2&amp;amp;x=dark

// Self-documenting and consistent
?mobile=true&amp;amp;page=2&amp;amp;theme=dark
&lt;/code&gt;
    &lt;p&gt;Choose parameter names that make sense. Future you (and your team) will thank you.&lt;/p&gt;
    &lt;head rend="h3"&gt;Overloading URLs with Complex State&lt;/head&gt;
    &lt;code&gt;?config=eyJtZXNzYWdlIjoiZGlkIHlvdSByZWFsbHkgdHJpZWQgdG8gZGVjb2RlIHRoYXQ_IiwiZmlsdGVycyI6eyJzdGF0dXMiOlsiYWN0aXZlIiwicGVuZGluZyJdLCJwcmlvcml0eSI6WyJoaWdoIiwibWVkaXVtIl0sInRhZ3MiOlsiZnJvbnRlbmQiLCJyZWFjdCIsImhvb2tzIl0sInJhbmdlIjp7ImZyb20iOiIyMDI0LTAxLTAxIiwidG8iOiIyMDI0LTEyLTMxIn19LCJzb3J0Ijp7ImZpZWxkIjoiY3JlYXRlZEF0Iiwib3JkZXIiOiJkZXNjIn0sInBhZ2luYXRpb24iOnsicGFnZSI6MSwibGltaXQiOjIwfX0==
&lt;/code&gt;
    &lt;p&gt;If you need to base64-encode a massive JSON object, the URL probably isnât the right place for that state.&lt;/p&gt;
    &lt;head rend="h3"&gt;URL Length Limits&lt;/head&gt;
    &lt;p&gt;Browsers and servers impose practical limits on URL length (usually between 2,000 and 8,000 characters) but the reality is more nuanced. As this detailed Stack Overflow answer explains, limits come from a mix of browser behavior, server configurations, CDNs, and even search engine constraints. If youâre bumping against them, itâs a sign you need to rethink your approach.&lt;/p&gt;
    &lt;head rend="h3"&gt;Breaking the Back Button&lt;/head&gt;
    &lt;code&gt;// Replacing state incorrectly
history.replaceState({}, '', newUrl); // Used when pushState was needed
&lt;/code&gt;
    &lt;p&gt;Respect browser history. If a user action should be âundoableâ via the back button, use &lt;code&gt;pushState&lt;/code&gt;. If itâs a refinement, use &lt;code&gt;replaceState&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;Closing Thought&lt;/head&gt;
    &lt;p&gt;That PrismJS URL reminded me of something important: good URLs donât just point to content. They describe a conversation between the user and the application. They capture intent, preserve context, and enable sharing in ways that no other state management solution can match.&lt;/p&gt;
    &lt;p&gt;Weâve built increasingly sophisticated state management libraries like Redux, MobX, Zustand, Recoil and others. They all have their place but sometimes the best solution is the one thatâs been there all along.&lt;/p&gt;
    &lt;p&gt;In my previous article, I wrote about the hidden costs of bad URL design. Today, weâve explored the flip side: the immense value of good URL design. URLs arenât just addresses. Theyâre state containers, user interfaces, and contracts all rolled into one.&lt;/p&gt;
    &lt;p&gt;If your app forgets its state when you hit refresh, youâre missing one of the webâs oldest and most elegant features.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45789474</guid><pubDate>Sun, 02 Nov 2025 11:12:58 +0000</pubDate></item><item><title>Tongyi DeepResearch – open-source 30B MoE Model that rivals OpenAI DeepResearch</title><link>https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/</link><description>&lt;doc fingerprint="efd86393ad1b861a"&gt;
  &lt;main&gt;
    &lt;p&gt;GITHUB HUGGINGFACE MODELSCOPE SHOWCASE&lt;/p&gt;
    &lt;head rend="h2"&gt;From Chatbot to Autonomous Agent&lt;/head&gt;
    &lt;p&gt;We are proud to present Tongyi DeepResearch, the first fully open‑source Web Agent to achieve performance on par with OpenAI’s DeepResearch across a comprehensive suite of benchmarks. Tongyi DeepResearch demonstrates state‑of‑the‑art results, scoring 32.9 on the academic reasoning task Humanity’s Last Exam (HLE), 43.4 on BrowseComp and 46.7 on BrowseComp‑ZH in extremely complex information‑seeking tasks, and achieving a score of 75 on the user‑centric xbench‑DeepSearch benchmark, systematically outperforming all existing proprietary and open‑source Deep Research agents.&lt;/p&gt;
    &lt;p&gt;Beyond the model, we share a complete and battle‑tested methodology for creating such advanced agents. Our contribution details a novel data synthesis solution applied across the entire training pipeline, from Agentic Continual Pre‑training (CPT) and Supervised Fine‑Tuning (SFT) for cold‑starting, to the final Reinforcement Learning (RL) stage. For RL, we provide a full‑stack solution, including algorithmic innovations, automated data curation, and robust infrastructure. For inference, the vanilla ReAct framework showcases the model’s powerful intrinsic capabilities without any prompt engineering, while the advanced Heavy Mode (test‑time‑scaling) demonstrates the upper limits of its complex reasoning and planning potential.&lt;/p&gt;
    &lt;head rend="h2"&gt;Continual Pre‑training and Post‑training Empowered by Fully Synthetic Data&lt;/head&gt;
    &lt;head rend="h3"&gt;Continual Pre‑training Data&lt;/head&gt;
    &lt;p&gt;We introduce Agentic CPT to deep research agent training, creating powerful agentic foundation models for post‑training. We propose AgentFounder, a systematic and scalable solution for large‑scale data synthesis that creates a data flywheel with data from the post‑training pipeline.&lt;/p&gt;
    &lt;p&gt;Data Reorganization and Question Construction. We continuously collect data from various sources, including documents, publicly available crawled data, knowledge graphs, and historical trajectories and tool invocation records (e.g., search results with links). As shown in the figure, these diverse data sources are restructured into an entity‑anchored open‑world knowledge memory. Based on randomly sampled entities and their corresponding knowledge, we generate multi‑style (question,answer) pairs.&lt;/p&gt;
    &lt;p&gt;Action Synthesis. Based on diverse problems and historical trajectories, we construct first‑order action synthesis data and higher‑order action synthesis data. Our method enables large‑scale and comprehensive exploration of the potential reasoning‑action space within offline environments, thereby thereby eliminating the need for additional commercial tool API calls. Specifically, for the higher‑order action synthesis, we remodel trajectories as multi‑step decision‑making processes to enhance the model’s decision‑making capabilities.&lt;/p&gt;
    &lt;head rend="h3"&gt;Post-training Data&lt;/head&gt;
    &lt;p&gt;High-quality synthetic QA pairs&lt;/p&gt;
    &lt;p&gt;We develop an end‑to‑end solution for synthetic data generation. This fully automated process requires no human intervention to construct super‑human quality datasets, designed to push the boundaries of AI agent performance. Through long‑term exploration and iteration‑from early methods like reverse‑engineering QA pairs from clickstreams (WebWalker) to the more systematic graph‑based synthesis (WebSailor and WebSailor‑V2), then the formalized task modeling (WebShaper)‑our approach ensures both exceptional data quality and massive scalability, breaking through the upper limits of model capabilities.&lt;/p&gt;
    &lt;p&gt;To address complex, high‑uncertainty questions, we synthesize web‑based QA data through a novel pipeline. The process begins by constructing a highly interconnected knowledge graph via random walks and isomorphic tables towards tabular data fusion from real‑world websites , ensuring a realistic information structure. We then sample subgraphs and subtables to generate initial questions and answers. The crucial step involves intentionally increasing difficulty by strategically obfuscating or blurring information within the question. This practical approach is grounded in a complete theoretical framework, where we formally model QA difficulty as a series of controllable “atomic operations” (e.g., merging entities with similar attributes) on entity relationships, allowing us to systematically increase complexity.&lt;/p&gt;
    &lt;p&gt;To further reduce inconsistencies between the organized information structure and the reasoning structure of QA, enable more controllable difficulty and structure scaling of reasoning, we proposed a formal modeling of the information‑seeking problem based on set theory. With this formalization, we developed agents that expands the problem in a controlled manner, and minimizes reasoning shortcuts and structural redundancy, leading to further improved QA quality. Moreover, this formal modeling also allows for efficient verification of QA correctness, effectively addressing the challenge of validating synthetic information‑seeking data for post‑training.&lt;/p&gt;
    &lt;p&gt;Furthermore, we have developed an automated data engine to scale up the creation of PhD‑level research questions. This engine begins with a multi‑disciplinary knowledge base, generating “seed” QA pairs that require multi‑source reasoning. Each seed then enters a self‑guided loop of “iterative complexity upgrades”, where a question‑crafting agent is equipped with a powerful toolset including web search, academic retrieval, and a Python execution environment. In each iteration, the agent expands knowledge boundaries, deepens conceptual abstraction, and even constructs computational tasks, creating a virtuous cycle where the output of one round becomes the more complex input for the next, ensuring a controllable and systematic escalation of task difficulty.&lt;/p&gt;
    &lt;p&gt;Unleashing Agent Capabilities with Diverse Reasoning Pattern&lt;/p&gt;
    &lt;p&gt;To bootstrap the model’s initial capabilities, we constructed a set of trajectories via rejection sampling, based on the ReAct and IterResearch frameworks (for details, see below). On one hand, ReAct, as a classic and foundational multi-turn reasoning format, instills rich reasoning behaviors and reinforces the model’s ability to adhere to structured formats.&lt;/p&gt;
    &lt;p&gt;On the other hand, we introduce IterResearch, an innovative agent paradigm (detailed below). It unleashes the model’s full reasoning potential by dynamically reconstructing a streamlined workspace in each turn, ensuring that every decision is deliberate and well-considered. Leveraging IterResearch, we constructed a set of trajectories that integrate reasoning, planning, and tool-use, thereby strengthening the model’s capacity for sustained planning when confronted with Long-Horizon tasks.&lt;/p&gt;
    &lt;head rend="h2"&gt;Rollout Mode&lt;/head&gt;
    &lt;p&gt;We have conducted extensive exploration into the rollout paradigms for DeepResearch‑type agents. As a result, our final model supports multiple rollout formats, including the native ReAct Mode and the context‑managing Heavy Mode.&lt;/p&gt;
    &lt;head rend="h3"&gt;Native ReAct Mode&lt;/head&gt;
    &lt;p&gt;Our model demonstrates excellent performance using the native ReAct reasoning paradigm without any prompt engineering. It strictly adheres to the Thought‑Action‑Observation cycle, performing multiple iterations to solve problems. With a model context length of 128K, it can handle a large number of interaction rounds, fully achieving scaling in its interaction with the environment. ReAct’s simplicity and universality provide the clearest benchmark for a model’s intrinsic capabilities and the efficacy of our training pipeline.&lt;/p&gt;
    &lt;p&gt;Our choice of ReAct is heavily informed by “The Bitter Lesson”, which posits that general methods leveraging scalable computation ultimately outperform approaches that rely on complex, human‑engineered knowledge and intricate designs.&lt;/p&gt;
    &lt;head rend="h3"&gt;Heavy Mode&lt;/head&gt;
    &lt;p&gt;In addition to the native ReAct mode, we have developed a “Heavy Mode” for complex, multi‑step research tasks. This mode is built on our new IterResearch paradigm, designed to push the agent’s capabilities to their limit.&lt;/p&gt;
    &lt;p&gt;The IterResearch paradigm was created to solve the “cognitive suffocation” and “noise pollution” that occurs when agents accumulate all information into a single, ever‑expanding context. Instead, IterResearch deconstructs a task into a series of “research rounds”.&lt;/p&gt;
    &lt;p&gt;In each round, the agent reconstructs a streamlined workspace using only the most essential outputs from the previous round. Within this focused workspace, the agent analyzes the problem, integrates key findings into a continuously evolving central report, and then decides its next action‑either gathering more information or providing a final answer. This iterative process of “synthesis and reconstruction” allows the agent to maintain a clear “cognitive focus” and high reasoning quality throughout long tasks.&lt;/p&gt;
    &lt;p&gt;Building on this, we propose the Research‑Synthesis framework. In this model, multiple Research Agents use the IterResearch process to explore a problem in parallel. A final Synthesis Agent then integrates their refined reports and conclusions to produce a more comprehensive final answer. This parallel structure enables the model to consider a wider range of research paths within a limited context window, pushing its performance to the limit.&lt;/p&gt;
    &lt;head rend="h2"&gt;End-to‑End Agent Training Pipeline&lt;/head&gt;
    &lt;p&gt;Training an agentic model like this required us to rethink the entire model training pipeline, from pre‑training to fine‑tuning to reinforcement learning. We established a new paradigm for agent model training that connects Agentic CPT → Agentic SFT → Agentic RL, creating a seamless end‑to‑end training loop for an AI agent. Here’s how we tackled the final stage with reinforcement learning, which was crucial for aligning the agent’s behavior with high‑level goals:&lt;/p&gt;
    &lt;head rend="h3"&gt;On‑Policy Agent Reinforcement Learning (RL)&lt;/head&gt;
    &lt;p&gt;Constructing a high‑quality agent through RL is a complex system engineering challenge; if this entire development process is viewed as a “reinforcement learning” loop, any instability or lack of robustness in its components can lead to erroneous “reward” signals. We will now share our practices in RL, covering both the algorithmic and infrastructure sides.&lt;/p&gt;
    &lt;p&gt;For RL algorithm, we made several algorithmic breakthroughs, using a customized on‑policy Group Relative Policy Optimization (GRPO). We employ a strictly on‑policy training regimen, ensuring that the learning signal is always relevant to the model’s current capabilities. The training objective is optimized using a token‑level policy gradient loss. Second, to further reduce variance in the advantage estimation, we adopt a leave‑one‑out strategy. Furthermore, we employ a conservative strategy for negative samples, having observed that an unfiltered set of negative trajectories significantly degrades training stability. This can manifest as a “format collapse” phenomenon after extended training. To mitigate this, we selectively exclude certain negative samples from the loss calculation, for instance, those that do not yield a final answer because they exceed a length limit. For the sake of efficiency, we do not employ dynamic sampling. We instead leverage larger batch and group sizes, which serve to maintain smaller variance and provide adequate supervision.&lt;/p&gt;
    &lt;p&gt;The training dynamics demonstrate effective learning, with a consistent upward trend in reward. Meanwhile, policy entropy remains consistently high, indicating sustained exploration and preventing premature convergence. We attribute this to the non‑stationary nature of the web environment, which naturally fosters a robust, adaptive policy and obviates the need for explicit entropy regularization.&lt;/p&gt;
    &lt;p&gt;We consider that the algorithm is important but not the only decisive factor in the success of Agentic RL. We have experimented with many different algorithms and tricks, and find that data and stability of the training environment are likely the more critical components in determining whether the RL works. Interestingly, we have tested to train the model directly on the BrowseComp testing set, but the results are substantially poorer than when using our synthetic data. We hypothesize that this disparity arises because the synthetic data offers a more consistent distribution, which allows the model to be more effectively tailored. Conversely, the human‑annotated data (such as BrowseComp) is inherently noisier. Given its limited scale, it is difficult to approximate a learnable underlying distribution, which consequently hinders the model to learn and generalize from it.&lt;/p&gt;
    &lt;p&gt;On the infrastructure side, training an agent with tools required us to develop a highly stable and efficient environment:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Synthetic Training Environment: Relying on live web APIs for development is expensive, slow, and inconsistent. We addressed this by creating a simulated training environment using an offline Wikipedia database and a custom tool suite. By adapting our data pipeline to generate high‑quality, complex tasks for this environment, we created a cost‑effective, fast, and controllable platform that dramatically accelerates our research and iteration.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Stable &amp;amp; Efficient Tool Sandbox: To ensure reliable tool use during agent training and evaluation, we developed a unified sandbox. The sandbox handles concurrency and failure gracefully by caching results, retrying failed calls, and using redundant providers as fallbacks (e.g., a backup search API). This provides the agent with a fast and deterministic experience, which is crucial for preventing tool errors from corrupting its learning trajectory.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Automatic Data Curation: Data is the core driver of model capability enhancement; its importance even surpasses that of the algorithm. The quality of the data directly determines the upper bound on the model’s ability to generalize to out‑of‑distribution scenarios through self‑exploration. To address this challenge, we optimize data in real time, guided by training dynamics. This optimization is achieved through a fully automated data synthesis and filtering pipeline that dynamically adjusts the training set. By closing the loop between data generation and model training, this approach not only ensures training stability but also delivers substantial performance gains.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;On‑Policy Asynchronous Framework: We implemented a custom step‑level asynchronous RL training loop on top of rLLM. Multiple agent instances interact with the (simulated or real) environment in parallel, each producing trajectories.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Through these measures, we “closed the loop” on agent training. Starting from a raw model, we did Agentic pre‑training to initialize tool‑use skills, then supervised finetuning on expert‑like data to cold start, and finally on‑policy RL to let the model conduct self‑evolution. This full‑stack approach ‑ now proven with Tongyi DeepResearch ‑ presents a new paradigm for training AI agents that can robustly solve complex tasks in dynamic environments.&lt;/p&gt;
    &lt;p&gt;(Our RL approach is inspired by several past work from Agentica. We adapt their rLLM framework and extend it to train our web agents.)&lt;/p&gt;
    &lt;head rend="h2"&gt;Real‑World Applications and Impact&lt;/head&gt;
    &lt;p&gt;Tongyi DeepResearch is not just a research showcase; it’s already powering real applications within Alibaba and beyond, demonstrating its value in practical scenarios:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Gaode Mate (Map &amp;amp; Navigation Agent): Collaborating with Amap (Gaode) Team, we co‑developed “Xiao Gao,” an AI copilot that leverages the app’s rich toolset. It can execute complex travel planning commands, like creating a multi‑day driving tour that includes specific scenic spots and pet‑friendly hotels. Through multi‑step reasoning, Xiao Gao autonomously researches and integrates information to produce a detailed, personalized itinerary, offering an intelligent planning experience that far surpasses standard navigation.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tongyi FaRui (Legal Research Agent): Empowered by our DeepResearch architecture, FaRui now functions as a true legal agent. It autonomously executes complex, multi‑step research tasks that mirror a junior attorney’s workflow‑systematically retrieving case law, cross‑referencing statutes, and synthesizing analysis. Crucially, all conclusions are grounded in verifiable judicial sources and delivered with precise case and statute citations, ensuring professional‑grade accuracy and credibility.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Limitations&lt;/head&gt;
    &lt;p&gt;Our future work will address three key limitations. First, the current 128k context length is still insufficient for the most complex long‑horizon tasks, requiring us to explore expanded context windows and more sophisticated information management. Second, our training pipeline’s scalability remains unproven on foundation models significantly larger than our 30B‑scale MoE, and we plan to validate our methods on larger‑scale models. Lastly, we aim to improve the efficiency of our reinforcement learning framework by investigating techniques like partial rollouts, which will necessitate solving the challenges of off‑policy training, such as distributional shift.&lt;/p&gt;
    &lt;head rend="h2"&gt;Series Work&lt;/head&gt;
    &lt;p&gt;Tongyi DeepResearch also has an extensive deep research agent family. You can find more information in the following papers:&lt;/p&gt;
    &lt;p&gt;[1] WebWalker: Benchmarking LLMs in Web Traversal&lt;/p&gt;
    &lt;p&gt;[2] WebDancer: Towards Autonomous Information Seeking Agency&lt;/p&gt;
    &lt;p&gt;[3] WebSailor: Navigating Super‑human Reasoning for Web Agent&lt;/p&gt;
    &lt;p&gt;[4] WebShaper: Agentically Data Synthesizing via Information‑Seeking Formalization&lt;/p&gt;
    &lt;p&gt;[5] WebWatcher: Breaking New Frontier of Vision‑Language Deep Research Agent&lt;/p&gt;
    &lt;p&gt;[6] WebResearch: Unleashing reasoning capability in Long‑Horizon Agents&lt;/p&gt;
    &lt;p&gt;[7] ReSum: Unlocking Long‑Horizon Search Intelligence via Context Summarization&lt;/p&gt;
    &lt;p&gt;[8] WebWeaver: Structuring Web‑Scale Evidence with Dynamic Outlines for Open‑Ended Deep Research&lt;/p&gt;
    &lt;p&gt;[10] Scaling Agents via Continual Pre‑training&lt;/p&gt;
    &lt;p&gt;[11] Towards General Agentic Intelligence via Environment Scaling&lt;/p&gt;
    &lt;p&gt;Our team has a long‑standing commitment to the research and development of deep research agents. Over the past six months, we have consistently published one technical report per month, totaling five to date. Today, we are excited to simultaneously release six new reports and share our Tongyi DeepResearch‑30B‑A3B model with the community.&lt;/p&gt;
    &lt;p&gt;Stay tuned for our next generation of agentic models.&lt;/p&gt;
    &lt;code&gt;@misc{tongyidr,
  author={Tongyi DeepResearch Team},
  title={Tongyi DeepResearch: A New Era of Open-Source AI Researchers},
  year={2025},
  howpublished={\url{https://github.com/Alibaba-NLP/DeepResearch}}
}
&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45789602</guid><pubDate>Sun, 02 Nov 2025 11:43:26 +0000</pubDate></item><item><title>Why don't you use dependent types?</title><link>https://lawrencecpaulson.github.io//2025/11/02/Why-not-dependent.html</link><description>&lt;doc fingerprint="a748029f0d0bb7f0"&gt;
  &lt;main&gt;&lt;head rend="h2"&gt;"Why don't you use dependent types?"&lt;/head&gt;[&lt;code&gt;&lt;nobr&gt;memories&lt;/nobr&gt;&lt;/code&gt; 
  
    
    &lt;code&gt;&lt;nobr&gt;AUTOMATH&lt;/nobr&gt;&lt;/code&gt; 
  
    
    &lt;code&gt;&lt;nobr&gt;LCF&lt;/nobr&gt;&lt;/code&gt; 
  
    
    &lt;code&gt;&lt;nobr&gt;type theory&lt;/nobr&gt;&lt;/code&gt; 
  
    
    &lt;code&gt;&lt;nobr&gt;Martin-Löf type theory&lt;/nobr&gt;&lt;/code&gt; 
  
    
    &lt;code&gt;&lt;nobr&gt;NG de Bruijn&lt;/nobr&gt;&lt;/code&gt; 
  
    
    &lt;code&gt;&lt;nobr&gt;ALEXANDRIA&lt;/nobr&gt;&lt;/code&gt; 
  
]

&lt;p&gt;To be fair, nobody asks me this exact question. But people have regularly asked why Isabelle dispenses with proof objects. The two questions are essentially the same, because proof objects are intrinsic to all the usual type theories. They are also completely unnecessary and a huge waste of space. As described in an earlier post, type checking in the implementation language (rather than in the logic) can ensure that only legitimate proof steps are executed. Robin Milner had this fundamental insight 50 years ago, giving us the LCF architecture with its proof kernel. But the best answer to the original question is simply this: I did use dependent types, for years.&lt;/p&gt;&lt;head rend="h3"&gt;My time with AUTOMATH&lt;/head&gt;&lt;p&gt;I was lucky enough to get some personal time with N G de Bruijn when he came to Caltech in 1977 to lecture about AUTOMATH. I never actually got to use this system. Back then, researchers used the nascent Internet (the ARPAnet) not to download software so much as to run software directly on the host computer, since most software was not portable. But Eindhoven University was not on the ARPAnet, and AUTOMATH was configured to run on a computer we did not have:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Until September 1973, the computer was the Electrologica X8, after that Burroughs 6700. In both cases the available multiprogranming systems required the use of ALGOL 60.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;I did however read many of the research reports, including the PhD dissertation by LS Jutting, where he presents his translation of Landau’s text Grundlagen der Analysis (described last time) from German into AUTOMATH. It is no coincidence that many of my papers, from the earliest to the latest, copied the idea of formalising a text and attempting to be faithful to it, if possible line by line.&lt;/p&gt;&lt;p&gt;As an aside, note that while AUTOMATH was a system of dependent types, it did not embody the Curry–Howard correspondence (sometimes wrongly called the Curry–Howard–de Bruijn correspondence). That correspondence involves having a type theory strong enough to represent the predicate calculus directly in the form of types. In AUTOMATH you had to introduce the symbols and inference rules of your desired calculus in the form of axioms, much as you do with Isabelle. In short, AUTOMATH was a logical framework:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;like a big restaurant that serves all sorts of food: vegetarian, kosher, or anything else the customer wants&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;De Bruijn did not approve of the increasingly powerful type theories being developed in the 1990s. AUTOMATH was a weak language, a form of λ-calculus including a general product construction just powerful enough to express the inference rules of a variety of formalisms and to make simple definitions, again clearly the inspiration for Isabelle. Isabelle aims to be generic, like the big AUTOMATH restaurant. Only these days everybody prefers the same cuisine, higher-order logic, so Isabelle/HOL has become dominant. Unfortunately, I last spoke to Dick (as he was known to friends) when I was putting all my effort into Isabelle/ZF. He simply loathed set theory and saw mathematics as essentially typed. He never lived to see the enormous amount of advanced mathematics that would be formalised using types in Isabelle/HOL.&lt;/p&gt;&lt;p&gt;I annoyed him in another way. I kept asking, AUTOMATH looks natural, but how do we know that it is right? He eventually sent me a 300 page volume entitled The Language Theory of Automath. It describes AUTOMATH’s formal properties such as strong normalisation and Church–Rosser properties, but this was not the answer I wanted at all. I got that answer for a quite different type theory.&lt;/p&gt;&lt;head rend="h3"&gt;Martin-Löf type theory&lt;/head&gt;&lt;p&gt;In response to kind invitations from Bengt Nordström and Kent Petersson, I paid a number of visits to Chalmers University in Gothenburg to learn about Martin-Löf type theory. I was particularly impressed by its promise of a systematic and formal approach to program synthesis. I had already encountered intuitionism through a course on the philosophy of mathematics at Stanford University, as I recall taught by Ian Hacking. The “rightness” of Martin-Löf type theory was obvious, because it directly embodied the principles of intuition truth as outlined by Heyting: for example, that a proof of $A\land B$ consists of a proof of $A$ paired with a proof of $B$.&lt;/p&gt;&lt;p&gt;I devoted several years of research to Martin-Löf type theory. This included a whole year of intricate hand derivations to produce a paper that I once thought would be important, and the very first version of Isabelle. Yes: Isabelle began as an implementation of Martin-Löf type theory, which is still included in the distribution even today as Isabelle/CTT. But eventually I tired of what seemed to me a doctrinaire attitude bordering on a cult of personality around Per Martin-Löf. The sudden switch to intensional equality (everyone was expected to adopt the new approach) wrecked most of my work. Screw that.&lt;/p&gt;&lt;p&gt;You might ask, what about the calculus of constructions, which arose during that time and eventually gave us Rocq and Lean? (Not to mention LEGO.) To me they raised, and continue to raise, the same question I had put to de Bruijn. Gérard Huet said something like “it is nothing but function application”, which did not convince me. It’s clear that I am being fussy,1 because thousands of people find these formalisms perfectly natural and believable. But it is also true that the calculus of constructions underwent numerous changes over the past four decades. There seem to be several optional axioms that people sometimes adopt while attempting to minimise their use, like dieters enjoying an occasional croissant.&lt;/p&gt;&lt;head rend="h3"&gt;Decisions, decisions&lt;/head&gt;&lt;p&gt;We can see all this as an example of the choices we make in research. People were developing new formalisms. This specific fact was the impetus for making Isabelle generic in the first place. But we have to choose whether to spend our time developing formalisms or instead to choose a fixed formalism and see how far you can push it. Both are legitimate research goals.&lt;/p&gt;&lt;p&gt;For example, already in 1985, Mike Gordon was using higher-order logic to verify hardware. He was not distracted by the idea that some dependent type theory might work better for n-bit words and the like. The formalism that he implemented was essentially the same as the simple theory of types outlined by Alonzo Church in 1940. He made verification history using this venerable formalism, and John Harrison later found a clever way to encode the dimension of vector types including words. Isabelle/HOL also implements Church’s simple type theory, with one extension: axiomatic type classes. Isabella users also derive much power from the locale concept, a kind of module sysstem that lies outside any particular logic.&lt;/p&gt;&lt;p&gt;During all this time, both Martin-Löf type theory and the calculus of constructions went through several stages of evolution. It’s remarkable how the Lean community, by running with a certain version of the calculus, quickly formalised a vast amount of mathematics.&lt;/p&gt;&lt;head rend="h3"&gt;Pushing higher-order logic to its limit&lt;/head&gt;&lt;p&gt;I felt exceptionally lucky to win funding from the European Research Council for the advanced grant ALEXANDRIA. When I applied, homotopy type theory was still all the rage, so the proposal emphasised Isabelle’s specific advantages: its automation, its huge libraries and the legibility of its proofs.&lt;/p&gt;&lt;p&gt;The team started work with enthusiasm. Nevertheless, I fully expected that we would hit a wall, reaching mathematical material that could not easily be formalised in higher-order logic. Too much of Isabelle’s analysis library identified topological spaces with types. Isabelle’s abstract algebra library was old and crufty. A number of my research colleagues were convinced that higher-logic was not adequate for serious mathematics. But Anthony Bordg took up the challenge, leading a subproject to formalise Grothendieck schemes.&lt;/p&gt;&lt;p&gt;For some reason I had a particular fear of the field extension $F[a]$, which extends the field $F$ with some $a$ postulated to be a root of some polynomial over $F$. (For example, the field of complex numbers is precisely $\mathbb{R}[i]$, where $i$ is postulated to be a root of $x^2+1=0$.) And yet an early outcome of ALEXANDRIA was a proof, by Paulo Emílio de Vilhena and Martin Baillon, that every field admits an algebraically closed extension. This was the first proof of that theorem in any proof assistant, and its proof involves an infinite series of field extensions.&lt;/p&gt;&lt;p&gt;We never hit any wall. As our group went on to formalise more and more advanced results, such as the Balog–Szemerédi–Gowers theorem, people stopped saying “you can’t formalise mathematics without dependent types” and switched to saying “dependent types give you nicer proofs”. But they never proved this claim.&lt;/p&gt;&lt;p&gt;Now that dependent type theory has attained maturity and has an excellent tool in the form of Lean, shall I go back to dependent types? I am not tempted. The only aspects of Lean that I envy are its huge community and the Blueprint tool. I hear too many complaints about Lean’s performance. I’ve heard of too many cases where dependent types played badly with intensional equality – I sat through an entire talk on this topic – or otherwise made life difficult. Quite a few people have told me that the secret of dependent types is knowing when not to use them. And so, to me, they have too much in common with Tesla’s Full Self-Driving.&lt;/p&gt;&lt;p&gt;Addendum: somebody commented on Hacker News that higher-order logic is too weak (in terms of proof-theoretic strength) to formalise post-WWII mathematics. This is not quite right. It is true that higher-order logic is much, much weaker than ZF set theory. But one of the most striking findings of ALEXANDRIA is that this is no obstacle to doing advanced mathematics, say to formalise Grothendieck schemes. Such elaborate towers of definitions do not seem to ascend especially high in the set-theoretic hierarchy. I can only recall a couple of proofs (this one, and that one) that required strengthening higher-order logic with the ZF axioms (which is easily done). These were theorems that referred to ZF entities in their very statements.&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;&lt;p&gt;Especially as regards constructive mathematics. To its founders, intuitionism is a philosophy suspicious of language, which it relegates to the purpose of recording and communicating mathematical thoughts. This is the opposite of today’s “constructive mathematics”, which refers the use of a formalism satisfying certain syntactic properties. ↩&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45790827</guid><pubDate>Sun, 02 Nov 2025 15:06:36 +0000</pubDate></item><item><title>Lisp: Notes on its Past and Future (1980)</title><link>https://www-formal.stanford.edu/jmc/lisp20th/lisp20th.html</link><description>&lt;doc fingerprint="9449101996ab135c"&gt;
  &lt;main&gt;
    &lt;p&gt; John McCarthy &lt;lb/&gt; Computer Science Department &lt;lb/&gt; Stanford University &lt;lb/&gt; Stanford, CA 94305 &lt;lb/&gt; jmc@cs.stanford.edu &lt;lb/&gt; http://www-formal.stanford.edu/jmc/&lt;/p&gt;
    &lt;p&gt; JanFebMarAprMayJun JulAugSepOctNovDec , :&amp;lt; 10 0 &lt;/p&gt;
    &lt;p&gt;LISP has survived for 21 years because it is an approximate local optimum in the space of programming languages. However, it has accumulated some barnacles that should be scraped off, and some long-standing opportunities for improvement have been neglected. It would benefit from some co-operative maintenance especially in creating and maintaining program libraries. Computer checked proofs of program correctness are now possible for pure LISP and some extensions, but more theory and some smoothing of the language itself are required before we can take full advantage of LISP's mathematical basis.&lt;/p&gt;
    &lt;p&gt;1999 note: This article was included in the 1980 Lisp conference held at Stanford. Since it almost entirely corresponds to my present opinions, I should have asked to have it reprinted in the 1998 Lisp users conference proceedings at which I gave a talk with the same title.&lt;/p&gt;
    &lt;p/&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45792579</guid><pubDate>Sun, 02 Nov 2025 19:05:32 +0000</pubDate></item><item><title>Alleged Jabber Zeus Coder 'MrICQ' in U.S. Custody</title><link>https://krebsonsecurity.com/2025/11/alleged-jabber-zeus-coder-mricq-in-u-s-custody/</link><description>&lt;doc fingerprint="62a059d43823868d"&gt;
  &lt;main&gt;
    &lt;p&gt;A Ukrainian man indicted in 2012 for conspiring with a prolific hacking group to steal tens of millions of dollars from U.S. businesses was arrested in Italy and is now in custody in the United States, KrebsOnSecurity has learned.&lt;/p&gt;
    &lt;p&gt;Sources close to the investigation say Yuriy Igorevich Rybtsov, a 41-year-old from the Russia-controlled city of Donetsk, Ukraine, was previously referenced in U.S. federal charging documents only by his online handle “MrICQ.” According to a 13-year-old indictment (PDF) filed by prosecutors in Nebraska, MrICQ was a developer for a cybercrime group known as “Jabber Zeus.”&lt;/p&gt;
    &lt;p&gt;The Jabber Zeus name is derived from the malware they used — a custom version of the ZeuS banking trojan — that stole banking login credentials and would send the group a Jabber instant message each time a new victim entered a one-time passcode at a financial institution website. The gang targeted mostly small to mid-sized businesses, and they were an early pioneer of so-called “man-in-the-browser” attacks, malware that can silently intercept any data that victims submit in a web-based form.&lt;/p&gt;
    &lt;p&gt;Once inside a victim company’s accounts, the Jabber Zeus crew would modify the firm’s payroll to add dozens of “money mules,” people recruited through elaborate work-at-home schemes to handle bank transfers. The mules in turn would forward any stolen payroll deposits — minus their commissions — via wire transfers to other mules in Ukraine and the United Kingdom.&lt;/p&gt;
    &lt;p&gt;The 2012 indictment targeting the Jabber Zeus crew named MrICQ as “John Doe #3,” and said this person handled incoming notifications of newly compromised victims. The Department of Justice (DOJ) said MrICQ also helped the group launder the proceeds of their heists through electronic currency exchange services.&lt;/p&gt;
    &lt;p&gt;Two sources familiar with the Jabber Zeus investigation said Rybtsov was arrested in Italy, although the exact date and circumstances of his arrest remain unclear. A summary of recent decisions (PDF) published by the Italian Supreme Court states that in April 2025, Rybtsov lost a final appeal to avoid extradition to the United States.&lt;/p&gt;
    &lt;p&gt;According to the mugshot website lockedup[.]wtf, Rybtsov arrived in Nebraska on October 9, and was being held under an arrest warrant from the U.S. Federal Bureau of Investigation (FBI).&lt;/p&gt;
    &lt;p&gt;The data breach tracking service Constella Intelligence found breached records from the business profiling site bvdinfo[.]com showing that a 41-year-old Yuriy Igorevich Rybtsov worked in a building at 59 Barnaulska St. in Donetsk. Further searching on this address in Constella finds the same apartment building was shared by a business registered to Vyacheslav “Tank” Penchukov, the leader of the Jabber Zeus crew in Ukraine.&lt;/p&gt;
    &lt;p&gt;Penchukov was arrested in 2022 while traveling to meet his wife in Switzerland. Last year, a federal court in Nebraska sentenced Penchukov to 18 years in prison and ordered him to pay more than $73 million in restitution.&lt;/p&gt;
    &lt;p&gt;Lawrence Baldwin is founder of myNetWatchman, a threat intelligence company based in Georgia that began tracking and disrupting the Jabber Zeus gang in 2009. myNetWatchman had secretly gained access to the Jabber chat server used by the Ukrainian hackers, allowing Baldwin to eavesdrop on the daily conversations between MrICQ and other Jabber Zeus members.&lt;/p&gt;
    &lt;p&gt;Baldwin shared those real-time chat records with multiple state and federal law enforcement agencies, and with this reporter. Between 2010 and 2013, I spent several hours each day alerting small businesses across the country that their payroll accounts were about to be drained by these cybercriminals.&lt;/p&gt;
    &lt;p&gt;Those notifications, and Baldwin’s tireless efforts, saved countless would-be victims a great deal of money. In most cases, however, we were already too late. Nevertheless, the pilfered Jabber Zeus group chats provided the basis for dozens of stories published here about small businesses fighting their banks in court over six- and seven-figure financial losses.&lt;/p&gt;
    &lt;p&gt;Baldwin said the Jabber Zeus crew was far ahead of its peers in several respects. For starters, their intercepted chats showed they worked to create a highly customized botnet directly with the author of the original Zeus Trojan — Evgeniy Mikhailovich Bogachev, a Russian man who has long been on the FBI’s “Most Wanted” list. The feds have a standing $3 million reward for information leading to Bogachev’s arrest.&lt;/p&gt;
    &lt;p&gt;The core innovation of Jabber Zeus was an alert that MrICQ would receive each time a new victim entered a one-time password code into a phishing page mimicking their financial institution. The gang’s internal name for this component was “Leprechaun,” (the video below from myNetWatchman shows it in action). Jabber Zeus would actually re-write the HTML code as displayed in the victim’s browser, allowing them to intercept any passcodes sent by the victim’s bank for multi-factor authentication.&lt;/p&gt;
    &lt;p&gt;“These guys had compromised such a large number of victims that they were getting buried in a tsunami of stolen banking credentials,” Baldwin told KrebsOnSecurity. “But the whole point of Leprechaun was to isolate the highest-value credentials — the commercial bank accounts with two-factor authentication turned on. They knew these were far juicier targets because they clearly had a lot more money to protect.”&lt;/p&gt;
    &lt;p&gt;Baldwin said the Jabber Zeus trojan also included a custom “backconnect” component that allowed the hackers to relay their bank account takeovers through the victim’s own infected PC.&lt;/p&gt;
    &lt;p&gt;“The Jabber Zeus crew were literally connecting to the victim’s bank account from the victim’s IP address, or from the remote control function and by fully emulating the device,” he said. “That trojan was like a hot knife through butter of what everyone thought was state-of-the-art secure online banking at the time.”&lt;/p&gt;
    &lt;p&gt;Although the Jabber Zeus crew was in direct contact with the Zeus author, the chats intercepted by myNetWatchman show Bogachev frequently ignored the group’s pleas for help. The government says the real leader of the Jabber Zeus crew was Maksim Yakubets, a 38-year Ukrainian man with Russian citizenship who went by the hacker handle “Aqua.”&lt;/p&gt;
    &lt;p&gt;The Jabber chats intercepted by Baldwin show that Aqua interacted almost daily with MrICQ, Tank and other members of the hacking team, often facilitating the group’s money mule and cashout activities remotely from Russia.&lt;/p&gt;
    &lt;p&gt;The government says Yakubets/Aqua would later emerge as the leader of an elite cybercrime ring of at least 17 hackers that referred to themselves internally as “Evil Corp.” Members of Evil Corp developed and used the Dridex (a.k.a. Bugat) trojan, which helped them siphon more than $100 million from hundreds of victim companies in the United States and Europe.&lt;/p&gt;
    &lt;p&gt;This 2019 story about the government’s $5 million bounty for information leading to Yakubets’s arrest includes excerpts of conversations between Aqua, Tank, Bogachev and other Jabber Zeus crew members discussing stories I’d written about their victims. Both Baldwin and I were interviewed at length for a new weekly six-part podcast by the BBC that delves deep into the history of Evil Corp. Episode One focuses on the evolution of Zeus, while the second episode centers on an investigation into the group by former FBI agent Jim Craig.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45793244</guid><pubDate>Sun, 02 Nov 2025 20:40:54 +0000</pubDate></item><item><title>Paris had a moving sidewalk in 1900, and a Thomas Edison film captured it (2020)</title><link>https://www.openculture.com/2020/03/paris-had-a-moving-sidewalk-in-1900.html</link><description>&lt;doc fingerprint="6f8a1d567f3f91af"&gt;
  &lt;main&gt;
    &lt;p&gt;It’s fair to say that few of us now marvel at moving walkways, those standard infrastructural elements of such utilitarian spaces as airport terminals, subway stations, and big-box stores. But there was a time when they astounded even residents of one of the most cosmopolitan cities in the world. The innovation of the moving sidewalk demonstrated at the Paris Exposition of 1900 (previously seen here on Open Culture when we featured Lumière Brothers footage of that period) commanded even Thomas Edison’s attention. As Paleofuture’s Matt Novak tells it at Smithsonian magazine, “Thomas Edison sent one of his producers, James Henry White, to the Exposition and Mr. White shot at least 16 movies,” a clip of which footage you can see above.&lt;/p&gt;
    &lt;p&gt;White “had brought along a new panning-head tripod that gave his films a newfound sense of freedom and flow. Watching the film, you can see children jumping into frame and even a man doffing his cap to the camera, possibly aware that he was being captured by an exciting new technology while a fun novelty of the future chugs along under his feet.”&lt;/p&gt;
    &lt;p&gt;Novak also includes hand-colored photographs from the Paris Exhibition and quotes a New York Observer correspondent describing the moving sidewalk as a “novelty” consisting of “three elevated platforms, the first being stationary, the second moving at a moderate rate of speed, and the third at the rate of about six miles an hour.” Thus “the circuit of the Exposition can be made with rapidity and ease by this contrivance. It also affords a good deal of fun, for most of the visitors are unfamiliar with this mode of transit, and are awkward in its use.”&lt;/p&gt;
    &lt;p&gt;Novak features contemporary images of the Paris Exhibition’s moving sidewalk at Paleofuture, found in the book Paris Exposition Reproduced From the Official Photographs. Its authors describe the trottoir roulant as “a detached structure like a railway train, arriving at and passing certain points at stated times” without a break. “In engineers’ language, it is an ‘endless floor’ raised thirty feet above the level of the ground, ever and ever gliding along the four sides of the square — a wooden serpent with its tail in its mouth.” But the history of the moving walkway didn’t start in Paris: “In 1871 inventor Alfred Speer patented a system of moving sidewalks that he thought would revolutionize pedestrian travel in New York City,” as Novak notes, and the first one actually built was built for Chicago’s 1893 Columbian Exposition — but it cost a nickel to ride and “was undependable and prone to breaking down,” making Paris’ version the more impressive spectacle.&lt;/p&gt;
    &lt;p&gt;Still, the Columbian Exposition’s visitors must have got a kick out of gliding down the pier without having to do the walking themselves. You can learn more about this first moving walkway and its successors, the one at the Paris Exhibition included, from the Little Car video above. However much these early models may look like quaint turn-of-the century novelties, some still see in the technology genuine promise for the future of public transit. Moving walkways work well, writes Treehugger’s Lloyd Alter, “when the walking distance and time is just a bit too long.” And they remind us that “transportation should be about more than just getting from A to B; it should be a pleasure as well.” Parisians “kept the Eiffel Tower from the exhibition” — it had been built for the 1889 World’s Fair — but “it is too bad they didn’t keep this, a sort of moving High Line that is both transportation and entertainment.”&lt;/p&gt;
    &lt;p&gt;If you would like to sign up for Open Culture’s free email newsletter, please find it here. It’s a great way to see our new posts, all bundled in one email, each day.&lt;/p&gt;
    &lt;p&gt;If you would like to support the mission of Open Culture, consider making a donation to our site. It’s hard to rely 100% on ads, and your contributions will help us continue providing the best free cultural and educational materials to learners everywhere. You can contribute through PayPal, Patreon, and Venmo (@openculture). Thanks!&lt;/p&gt;
    &lt;p&gt;Related Content:&lt;/p&gt;
    &lt;p&gt;How French Artists in 1899 Envisioned Life in the Year 2000: Drawing the Future&lt;/p&gt;
    &lt;p&gt;Based in Seoul, Colin Marshall writes and broadcasts on cities, language, and culture. His projects include the book The Stateless City: a Walk through 21st-Century Los Angeles and the video series The City in Cinema. Follow him on Twitter at @colinmarshall or on Facebook.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45793466</guid><pubDate>Sun, 02 Nov 2025 21:08:15 +0000</pubDate></item><item><title>FurtherAI (YC W24) Is Hiring Across Software and AI</title><link>https://news.ycombinator.com/item?id=45793652</link><description>&lt;doc fingerprint="ad5c709c2efe2c46"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;FurtherAI (Series A, a16z + YC) is hiring Software Engineers, AI Engineers, and Forward-Deployed Engineers.&lt;/p&gt;
      &lt;p&gt;We're building AI Agents for the insurance industry and are already post-PMF with strong enterprise adoption.&lt;/p&gt;
      &lt;p&gt;Highlights:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;  - $25M Series A led by Andreessen Horowitz (a16z)
  - &amp;gt;10× revenue growth this year
  - Seed -&amp;gt; Series A in under a year
  - Small, talent-dense team - 6/15 are founders (incl. 4 YC founders)
  - Team backgrounds include staff engineers and researchers from Apple, Microsoft, Amazon
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt; Hiring strong engineers based in SF who want high ownership, fast shipping, and real impact.&lt;/p&gt;
      &lt;p&gt;If you/someone you know is interested, feel free to reach out directly to Sashank (CTO) - sg+hn@furtherai.com&lt;/p&gt;
      &lt;p&gt;Jobs link - https://jobs.ashbyhq.com/furtherai&lt;/p&gt;
      &lt;p&gt;PS: We also have a $10k referral bonus per hire, so plz share it across!&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45793652</guid><pubDate>Sun, 02 Nov 2025 21:35:02 +0000</pubDate></item><item><title>New prompt injection papers: Agents rule of two and the attacker moves second</title><link>https://simonwillison.net/2025/Nov/2/new-prompt-injection-papers/</link><description>&lt;doc fingerprint="3f52930f49b4d725"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;New prompt injection papers: Agents Rule of Two and The Attacker Moves Second&lt;/head&gt;
    &lt;p&gt;2nd November 2025&lt;/p&gt;
    &lt;p&gt;Two interesting new papers regarding LLM security and prompt injection came to my attention this weekend.&lt;/p&gt;
    &lt;head rend="h4"&gt;Agents Rule of Two: A Practical Approach to AI Agent Security&lt;/head&gt;
    &lt;p&gt;The first is Agents Rule of Two: A Practical Approach to AI Agent Security, published on October 31st on the Meta AI blog. It doesn’t list authors but it was shared on Twitter by Meta AI security researcher Mick Ayzenberg.&lt;/p&gt;
    &lt;p&gt;It proposes a “Rule of Two” that’s inspired by both my own lethal trifecta concept and the Google Chrome team’s Rule Of 2 for writing code that works with untrustworthy inputs:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;At a high level, the Agents Rule of Two states that until robustness research allows us to reliably detect and refuse prompt injection, agents must satisfy no more than two of the following three properties within a session to avoid the highest impact consequences of prompt injection.&lt;/p&gt;
      &lt;p&gt;[A] An agent can process untrustworthy inputs&lt;/p&gt;
      &lt;p&gt;[B] An agent can have access to sensitive systems or private data&lt;/p&gt;
      &lt;p&gt;[C] An agent can change state or communicate externally&lt;/p&gt;
      &lt;p&gt;It’s still possible that all three properties are necessary to carry out a request. If an agent requires all three without starting a new session (i.e., with a fresh context window), then the agent should not be permitted to operate autonomously and at a minimum requires supervision --- via human-in-the-loop approval or another reliable means of validation.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;It’s accompanied by this handy diagram:&lt;/p&gt;
    &lt;p&gt;I like this a lot.&lt;/p&gt;
    &lt;p&gt;I’ve spent several years now trying to find clear ways to explain the risks of prompt injection attacks to developers who are building on top of LLMs. It’s frustratingly difficult.&lt;/p&gt;
    &lt;p&gt;I’ve had the most success with the lethal trifecta, which boils one particular class of prompt injection attack down to a simple-enough model: if your system has access to private data, exposure to untrusted content and a way to communicate externally then it’s vulnerable to private data being stolen.&lt;/p&gt;
    &lt;p&gt;The one problem with the lethal trifecta is that it only covers the risk of data exfiltration: there are plenty of other, even nastier risks that arise from prompt injection attacks against LLM-powered agents with access to tools which the lethal trifecta doesn’t cover.&lt;/p&gt;
    &lt;p&gt;The Agents Rule of Two neatly solves this, through the addition of “changing state” as a property to consider. This brings other forms of tool usage into the picture: anything that can change state triggered by untrustworthy inputs is something to be very cautious about.&lt;/p&gt;
    &lt;p&gt;It’s also refreshing to see another major research lab concluding that prompt injection remains an unsolved problem, and attempts to block or filter them have not proven reliable enough to depend on. The current solution is to design systems with this in mind, and the Rule of Two is a solid way to think about that.&lt;/p&gt;
    &lt;p&gt;Update: On thinking about this further there’s one aspect of the Rule of Two model that doesn’t work for me: the Venn diagram above marks the combination of untrustworthy inputs and the ability to change state as “safe”, but that’s not right. Even without access to private systems or sensitive data that pairing can still produce harmful results. Unfortunately adding an exception for that pair undermines the simplicity of the “Rule of Two” framing!&lt;/p&gt;
    &lt;p&gt;Which brings me to the second paper...&lt;/p&gt;
    &lt;head rend="h4"&gt;The Attacker Moves Second: Stronger Adaptive Attacks Bypass Defenses Against LLM Jailbreaks and Prompt Injections&lt;/head&gt;
    &lt;p&gt;This paper is dated 10th October 2025 on Arxiv and comes from a heavy-hitting team of 14 authors—Milad Nasr, Nicholas Carlini, Chawin Sitawarin, Sander V. Schulhoff, Jamie Hayes, Michael Ilie, Juliette Pluto, Shuang Song, Harsh Chaudhari, Ilia Shumailov, Abhradeep Thakurta, Kai Yuanqing Xiao, Andreas Terzis, Florian Tramèr—including representatives from OpenAI, Anthropic, and Google DeepMind.&lt;/p&gt;
    &lt;p&gt;The paper looks at 12 published defenses against prompt injection and jailbreaking and subjects them to a range of “adaptive attacks”—attacks that are allowed to expend considerable effort iterating multiple times to try and find a way through.&lt;/p&gt;
    &lt;p&gt;The defenses did not fare well:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;By systematically tuning and scaling general optimization techniques—gradient descent, reinforcement learning, random search, and human-guided exploration—we bypass 12 recent defenses (based on a diverse set of techniques) with attack success rate above 90% for most; importantly, the majority of defenses originally reported near-zero attack success rates.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Notably the “Human red-teaming setting” scored 100%, defeating all defenses. That red-team consisted of 500 participants in an online competition they ran with a $20,000 prize fund.&lt;/p&gt;
    &lt;p&gt;The key point of the paper is that static example attacks—single string prompts designed to bypass systems—are an almost useless way to evaluate these defenses. Adaptive attacks are far more powerful, as shown by this chart:&lt;/p&gt;
    &lt;p&gt;The three automated adaptive attack techniques used by the paper are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Gradient-based methods—these were the least effective, using the technique described in the legendary Universal and Transferable Adversarial Attacks on Aligned Language Models paper from 2023.&lt;/item&gt;
      &lt;item&gt;Reinforcement learning methods—particularly effective against black-box models: “we allowed the attacker model to interact directly with the defended system and observe its outputs”, using 32 sessions of 5 rounds each.&lt;/item&gt;
      &lt;item&gt;Search-based methods—generate candidates with an LLM, then evaluate and further modify them using LLM-as-judge and other classifiers.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The paper concludes somewhat optimistically:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;[...] Adaptive evaluations are therefore more challenging to perform, making it all the more important that they are performed. We again urge defense authors to release simple, easy-to-prompt defenses that are amenable to human analysis. [...] Finally, we hope that our analysis here will increase the standard for defense evaluations, and in so doing, increase the likelihood that reliable jailbreak and prompt injection defenses will be developed.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Given how totally the defenses were defeated, I do not share their optimism that reliable defenses will be developed any time soon.&lt;/p&gt;
    &lt;p&gt;As a review of how far we still have to go this paper packs a powerful punch. I think it makes a strong case for Meta’s Agents Rule of Two as the best practical advice for building secure LLM-powered agent systems today in the absence of prompt injection defenses we can rely on.&lt;/p&gt;
    &lt;head rend="h2"&gt;More recent articles&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Hacking the WiFi-enabled color screen GitHub Universe conference badge - 28th October 2025&lt;/item&gt;
      &lt;item&gt;Video: Building a tool to copy-paste share terminal sessions using Claude Code for web - 23rd October 2025&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45794245</guid><pubDate>Sun, 02 Nov 2025 23:11:22 +0000</pubDate></item><item><title>Syllabi – Open-source agentic AI with tools, RAG, and multi-channel deploy</title><link>https://www.syllabi-ai.com/</link><description>&lt;doc fingerprint="f7c22e69c0a7aeba"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Create Custom AIChatbots That Deploy Everywhere&lt;/head&gt;&lt;p&gt;Build intelligent chatbots with knowledge bases, multi-app integrations, and omnichannel deployment. Open-source, customizable, and ready to scale.&lt;/p&gt;&lt;head rend="h2"&gt;Features&lt;/head&gt;&lt;head rend="h3"&gt;Build from Your Knowledge&lt;/head&gt;&lt;p&gt;Transform your documents, websites, and data into an intelligent knowledge base. Your chatbot retrieves accurate information with citations and source highlighting.&lt;/p&gt;&lt;head rend="h4"&gt;Multi-Format Support&lt;/head&gt;&lt;p&gt;Import PDFs, videos, websites, Google Drive, Notion, and more.&lt;/p&gt;&lt;head rend="h4"&gt;Smart Retrieval (RAG)&lt;/head&gt;&lt;p&gt;Advanced retrieval technology finds the most relevant information instantly.&lt;/p&gt;&lt;head rend="h4"&gt;Source Citations&lt;/head&gt;&lt;p&gt;Highlights exact passages from documents with clickable references.&lt;/p&gt;&lt;head rend="h3"&gt;Deploy Across Channels&lt;/head&gt;&lt;p&gt;One chatbot, multiple channels. Deploy to web, embed in your website, or connect to Slack and Discord for team collaboration.&lt;/p&gt;&lt;head rend="h4"&gt;Web &amp;amp; Embedded Widget&lt;/head&gt;&lt;p&gt;Add AI-powered chat to any website with customizable widgets.&lt;/p&gt;&lt;head rend="h4"&gt;Slack &amp;amp; Discord&lt;/head&gt;&lt;p&gt;Integrate with team messaging platforms for instant access to knowledge.&lt;/p&gt;&lt;head rend="h4"&gt;API Access&lt;/head&gt;&lt;p&gt;Build custom integrations with our REST API.&lt;/p&gt;&lt;head rend="h3"&gt;Extend with Skills&lt;/head&gt;&lt;p&gt;Make your chatbot agentic by adding custom skills and tools. Connect to external services, trigger actions, or create custom API integrations.&lt;/p&gt;&lt;head rend="h4"&gt;Custom Tools &amp;amp; Webhooks&lt;/head&gt;&lt;p&gt;Create custom actions via webhooks and API integrations.&lt;/p&gt;&lt;head rend="h4"&gt;Integration Skills&lt;/head&gt;&lt;p&gt;Send Slack messages, trigger workflows, and connect external services.&lt;/p&gt;&lt;head rend="h4"&gt;Agentic Behavior&lt;/head&gt;&lt;p&gt;Your chatbot intelligently decides when and how to use available tools.&lt;/p&gt;&lt;head rend="h2"&gt;More Than Just Text&lt;/head&gt;&lt;p&gt;Rich, interactive conversations that go beyond simple Q&amp;amp;A. Documents, diagrams, code, and multimedia—all in one place.&lt;/p&gt;&lt;head rend="h3"&gt;Document Highlighting&lt;/head&gt;&lt;p&gt;Click on citations to see the exact source highlighted in the original document. Navigate directly to relevant passages.&lt;/p&gt;&lt;head rend="h3"&gt;Diagrams &amp;amp; Visualizations&lt;/head&gt;&lt;p&gt;Generate Mermaid diagrams, flowcharts, and visual explanations to make complex concepts easier to understand.&lt;/p&gt;&lt;head rend="h3"&gt;Native Code Execution&lt;/head&gt;&lt;p&gt;Run Python and R code directly in the browser using Pyodide and WebR. Generate, execute, and visualize results instantly—no server needed.&lt;/p&gt;&lt;head rend="h3"&gt;Multimedia Support&lt;/head&gt;&lt;p&gt;Embed and play videos, display images, and share rich media content directly in conversations.&lt;/p&gt;&lt;head rend="h2"&gt;Built for Every Team and Use Case&lt;/head&gt;&lt;p&gt;From education to customer support, Syllabi adapts to your needs&lt;/p&gt;&lt;head rend="h3"&gt;Course Assistant for Students&lt;/head&gt;&lt;p&gt;Teachers create chatbots from course materials. Students get instant answers with source citations, making learning interactive and efficient.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Document upload&lt;/item&gt;&lt;item&gt;Citation tracking&lt;/item&gt;&lt;item&gt;Q&amp;amp;A support&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;Embedded Website Support&lt;/head&gt;&lt;p&gt;Add AI-powered support to your website. Automatically answer questions from your docs, reducing support tickets and improving customer satisfaction.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Embedded widget&lt;/item&gt;&lt;item&gt;24/7 availability&lt;/item&gt;&lt;item&gt;Brand customization&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;Team Knowledge via Slack/Discord&lt;/head&gt;&lt;p&gt;Connect your team's knowledge base to Slack or Discord. Get instant answers from company docs without leaving your workflow.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Slack/Discord integration&lt;/item&gt;&lt;item&gt;Private data&lt;/item&gt;&lt;item&gt;Team collaboration&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;API &amp;amp; Code Documentation&lt;/head&gt;&lt;p&gt;Help developers find answers in technical documentation. Generate code examples and explain complex concepts with diagrams.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Code generation&lt;/item&gt;&lt;item&gt;Diagram support&lt;/item&gt;&lt;item&gt;Technical docs&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;Full Control Over Behavior &amp;amp; Appearance&lt;/head&gt;&lt;p&gt;Customize every aspect of your chatbot to match your brand and business needs&lt;/p&gt;&lt;head rend="h3"&gt;Brand Customization&lt;/head&gt;&lt;p&gt;Match your brand with custom themes, colors, and styling. Choose from pre-built themes or create your own.&lt;/p&gt;&lt;head rend="h3"&gt;Model Configuration&lt;/head&gt;&lt;p&gt;Choose OpenAI models (GPT-4, GPT-3.5) and configure temperature, personality, and behavior. More model providers coming soon.&lt;/p&gt;&lt;head rend="h3"&gt;Conversation Design&lt;/head&gt;&lt;p&gt;Set custom welcome messages, suggested questions, and conversation flows to guide user interactions.&lt;/p&gt;&lt;head rend="h3"&gt;Analytics &amp;amp; Insights&lt;/head&gt;&lt;p&gt;Track conversations, measure engagement, and understand user needs with detailed analytics dashboards.&lt;/p&gt;&lt;head rend="h2"&gt;Integrate with Tools You Already Use&lt;/head&gt;&lt;p&gt;Connect Syllabi to your favorite apps. Use them as knowledge sources, deployment channels, or to automate actions and workflows.&lt;/p&gt;&lt;head rend="h3"&gt;Knowledge Sources&lt;/head&gt;&lt;head rend="h4"&gt;Google Drive&lt;/head&gt;Knowledge Source&lt;p&gt;Sync Google Drive files and docs as knowledge sources.&lt;/p&gt;&lt;head rend="h4"&gt;Notion&lt;/head&gt;Knowledge Source&lt;p&gt;Sync Notion pages and databases.&lt;/p&gt;&lt;head rend="h4"&gt;Website URLs&lt;/head&gt;Knowledge Source&lt;p&gt;Feed in direct website URLs and web pages.&lt;/p&gt;&lt;head rend="h3"&gt;Deploy Channels&lt;/head&gt;&lt;head rend="h4"&gt;Discord&lt;/head&gt;Deploy Channel&lt;p&gt;Deploy chatbot to Discord servers.&lt;/p&gt;&lt;head rend="h4"&gt;Web Widget&lt;/head&gt;Deploy Channel&lt;p&gt;Embed on any website with a chat widget.&lt;/p&gt;&lt;head rend="h4"&gt;Standalone App&lt;/head&gt;Deploy Channel&lt;p&gt;Deploy as a standalone chat app.&lt;/p&gt;&lt;head rend="h3"&gt;Actions &amp;amp; Extensions&lt;/head&gt;&lt;head rend="h4"&gt;Gmail&lt;/head&gt;Action/Workflow&lt;p&gt;Send emails based on chatbot interactions.&lt;/p&gt;&lt;head rend="h4"&gt;Google Calendar&lt;/head&gt;Action/Workflow&lt;p&gt;Create and manage calendar events.&lt;/p&gt;&lt;head rend="h4"&gt;Custom APIs&lt;/head&gt;Action/Workflow&lt;p&gt;Connect to any custom API or webhook.&lt;/p&gt;&lt;head rend="h3"&gt;Powerhouse Integrations&lt;/head&gt;&lt;p&gt;Multi-purpose integrations that work as knowledge sources, channels, AND actions&lt;/p&gt;&lt;head rend="h4"&gt;Slack&lt;/head&gt;&lt;p&gt;Build knowledge base from Slack, deploy chatbot, trigger Slack actions.&lt;/p&gt;&lt;head rend="h4"&gt;Google Workspace&lt;/head&gt;&lt;p&gt;Connect Google Drive, Gmail, Calendar, and Docs as knowledge sources.&lt;/p&gt;&lt;p&gt;Coming soon: Zapier, Make (Integromat), GitHub, Microsoft Teams, Stripe, Zendesk, HubSpot, and many more. Plus, custom API integrations for unlimited possibilities.&lt;/p&gt;&lt;head rend="h2"&gt;Open Source. Self-Hosted. Your Data, Your Control.&lt;/head&gt;&lt;p&gt;Built for developers and teams who value transparency, privacy, and full control over their AI infrastructure.&lt;/p&gt;&lt;head rend="h3"&gt;MIT Licensed&lt;/head&gt;&lt;p&gt;Free to use, modify, and deploy. No vendor lock-in, no hidden costs.&lt;/p&gt;&lt;head rend="h3"&gt;Self-Hosted&lt;/head&gt;&lt;p&gt;Run on your infrastructure with full data control and privacy.&lt;/p&gt;&lt;head rend="h3"&gt;Extensible&lt;/head&gt;&lt;p&gt;Build custom integrations, skills, and features to match your needs.&lt;/p&gt;&lt;head rend="h3"&gt;Community-Driven&lt;/head&gt;&lt;p&gt;Active development with contributions from developers worldwide.&lt;/p&gt;&lt;head rend="h3"&gt;Privacy-First&lt;/head&gt;&lt;p&gt;Your data never leaves your servers. Complete control over security.&lt;/p&gt;&lt;p&gt;⭐ Star us on GitHub&lt;/p&gt;&lt;head rend="h3"&gt;Open Source&lt;/head&gt;&lt;p&gt;MIT licensed and community-driven. Self-host and customize freely.&lt;/p&gt;&lt;head rend="h3"&gt;Multi-Channel&lt;/head&gt;&lt;p&gt;Deploy to Web, Embedded, Slack, and Discord.&lt;/p&gt;&lt;head rend="h3"&gt;Agentic &amp;amp; Extensible&lt;/head&gt;&lt;p&gt;Add custom skills and tools to extend chatbot capabilities.&lt;/p&gt;&lt;head rend="h3"&gt;Advanced RAG&lt;/head&gt;&lt;p&gt;Powered by retrieval-augmented generation for accurate, cited responses.&lt;/p&gt;&lt;head rend="h2"&gt;Start Building Your Custom Chatbot Today&lt;/head&gt;&lt;p&gt;Join developers and businesses creating intelligent chatbots with Syllabi. Open-source, free, and ready to deploy everywhere.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45795186</guid><pubDate>Mon, 03 Nov 2025 01:59:07 +0000</pubDate></item><item><title>Oxy is Cloudflare's Rust-based next generation proxy framework (2023)</title><link>https://blog.cloudflare.com/introducing-oxy/</link><description>&lt;doc fingerprint="b5fad91d343278d3"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;In this blog post, we are proud to introduce Oxy - our modern proxy framework, developed using the Rust programming language. Oxy is a foundation of several Cloudflare projects, including the Zero Trust Gateway, the iCloud Private Relay second hop proxy, and the internal egress routing service.&lt;/p&gt;
      &lt;p&gt;Oxy leverages our years of experience building high-load proxies to implement the latest communication protocols, enabling us to effortlessly build sophisticated services that can accommodate massive amounts of daily traffic.&lt;/p&gt;
      &lt;p&gt;We will be exploring Oxy in greater detail in upcoming technical blog posts, providing a comprehensive and in-depth look at its capabilities and potential applications. For now, let us embark on this journey and discover what Oxy is and how we built it.&lt;/p&gt;
      &lt;p&gt;We refer to Oxy as our "next-generation proxy framework". But what do we really mean by âproxy frameworkâ? Picture a server (like NGINX, that reader might be familiar with) that can proxy traffic with an array of protocols, including various predefined common traffic flow scenarios that enable you to route traffic to specific destinations or even egress with a different protocol than the one used for ingress. This server can be configured in many ways for specific flows and boasts tight integration with the surrounding infrastructure, whether telemetry consumers or networking services.&lt;/p&gt;
      &lt;p&gt;Now, take all of that and add in the ability to programmatically control every aspect of the proxying: protocol decapsulation, traffic analysis, routing, tunneling logic, DNS resolution, and so much more. And this is what Oxy proxy framework is: a feature-rich proxy server tightly integrated with our internal infrastructure that's customizable to meet application requirements, allowing engineers to tweak every component.&lt;/p&gt;
      &lt;p&gt;This design is in line with our belief in an iterative approach to development, where a basic solution is built first and then gradually improved over time. With Oxy, you can start with a basic solution that can be deployed to our servers and then add additional features as needed, taking advantage of the many extensibility points offered by Oxy. In fact, you can avoid writing any code, besides a few lines of bootstrap boilerplate and get a production-ready server with a wide variety of startup configuration options and traffic flow scenarios.&lt;/p&gt;
      &lt;p&gt;High-level Oxy architecture&lt;/p&gt;
      &lt;p&gt;For example, suppose you'd like to implement an HTTP firewall. With Oxy, you can proxy HTTP(S) requests right out of the box, eliminating the need to write any code related to production services, such as request metrics and logs. You simply need to implement an Oxy hook handler for HTTP requests and responses. If you've used Cloudflare Workers before, then you should be familiar with this extensibility model.&lt;/p&gt;
      &lt;p&gt;Similarly, you can implement a layer 4 firewall by providing application hooks that handle ingress and egress connections. This goes beyond a simple block/accept scenario, as you can build authentication functionality or a traffic router that sends traffic to different destinations based on the geographical information of the ingress connection. The capabilities are incredibly rich, and we've made the extensibility model as ergonomic and flexible as possible. As an example, if information obtained from layer 4 is insufficient to make an informed firewall decision, the app can simply ask Oxy to decapsulate the traffic and process it with HTTP firewall.&lt;/p&gt;
      &lt;p&gt;The aforementioned scenarios are prevalent in many products we build at Cloudflare, so having a foundation that incorporates ready solutions is incredibly useful. This foundation has absorbed lots of experience we've gained over the years, taking care of many sharp and dark corners of high-load service programming. As a result, application implementers can stay focused on the business logic of their application with Oxy taking care of the rest. In fact, we've been able to create a few privacy proxy applications using Oxy that now serve massive amounts of traffic in production with less than a couple of hundred lines of code. This is something that would have taken multiple orders of magnitude more time and lines of code before.&lt;/p&gt;
      &lt;p&gt;As previously mentioned, we'll dive deeper into the technical aspects in future blog posts. However, for now, we'd like to provide a brief overview of Oxy's capabilities. This will give you a glimpse of the many ways in which Oxy can be customized and used.&lt;/p&gt;
      &lt;p&gt;On-ramp defines a combination of transport layer socket type and protocols that server listeners can use for ingress traffic.&lt;/p&gt;
      &lt;p&gt;Oxy supports a wide variety of traffic on-ramps:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;
          &lt;p&gt;HTTP 1/2/3 (including various CONNECT protocols for layer 3 and 4 traffic)&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;TCP and UDP traffic over Proxy Protocol&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;general purpose IP traffic, including ICMP&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;With Oxy, you have the ability to analyze and manipulate traffic at multiple layers of the OSI model - from layer 3 to layer 7. This allows for a wide range of possibilities in terms of how you handle incoming traffic.&lt;/p&gt;
      &lt;p&gt;One of the most notable and powerful features of Oxy is the ability for applications to force decapsulation. This means that an application can analyze traffic at a higher level, even if it originally arrived at a lower level. For example, if an application receives IP traffic, it can choose to analyze the UDP traffic encapsulated within the IP packets. With just a few lines of code, the application can tell Oxy to upgrade the IP flow to a UDP tunnel, effectively allowing the same code to be used for different on-ramps.&lt;/p&gt;
      &lt;p&gt;The application can even go further and ask Oxy to sniff UDP packets and check if they contain HTTP/3 traffic. In this case, Oxy can upgrade the UDP traffic to HTTP and handle HTTP/3 requests that were originally received as raw IP packets. This allows for the simultaneous processing of traffic at all three layers (L3, L4, L7), enabling applications to analyze, filter, and manipulate the traffic flow from multiple perspectives. This provides a robust toolset for developing advanced traffic processing applications.&lt;/p&gt;
      &lt;p&gt;Multi-layer traffic processing in Oxy applications&lt;/p&gt;
      &lt;p&gt;Off-ramp defines a combination of transport layer socket type and protocols that proxy server connectors can use for egress traffic.&lt;/p&gt;
      &lt;p&gt;Oxy offers versatility in its egress methods, supporting a range of protocols including HTTP 1 and 2, UDP, TCP, and IP. It is equipped with internal DNS resolution and caching, as well as customizable resolvers, with automatic fallback options for maximum system reliability. Oxy implements happy eyeballs for TCP, advanced tunnel timeout logic and has the ability to route traffic to internal services with accompanying metadata.&lt;/p&gt;
      &lt;p&gt;Additionally, through collaboration with one of our internal services (which is an Oxy application itself!) Oxy is able to offer geographical egress â allowing applications to route traffic to the public Internet from various locations in our extensive network covering numerous cities worldwide. This complex and powerful feature can be easily utilized by Oxy application developers at no extra cost, simply by adjusting configuration settings.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Tunneling and request handling&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;We've discussed Oxy's communication capabilities with the outside world through on-ramps and off-ramps. In the middle, Oxy handles efficient stateful tunneling of various traffic types including TCP, UDP, QUIC, and IP, while giving applications full control over traffic blocking and redirection.&lt;/p&gt;
      &lt;p&gt;Additionally, Oxy effectively handles HTTP traffic, providing full control over requests and responses, and allowing it to serve as a direct HTTP or API service. With built-in tools for streaming analysis of HTTP bodies, Oxy makes it easy to extract and process data, such as form data from uploads and downloads.&lt;/p&gt;
      &lt;p&gt;In addition to its multi-layer traffic processing capabilities, Oxy also supports advanced HTTP tunneling methods, such as CONNECT-UDP and CONNECT-IP, using the latest extensions to HTTP 3 and 2 protocols. It can even process HTTP CONNECT request payloads on layer 4 and recursively process the payload as HTTP if the encapsulated traffic is HTTP.&lt;/p&gt;
      &lt;p&gt;Recursive processing of HTTP CONNECT body payload in HTTP pipeline&lt;/p&gt;
      &lt;p&gt;The modern Internet is unimaginable without traffic encryption, and Oxy, of course, provides this essential aspect. Oxy's cryptography and TLS are based on BoringSSL, providing both a FIPS-compliant version with a limited set of certified features and the latest version that supports all the currently available TLS features. Oxy also allows applications to switch between the two versions in real-time, on a per-request or per-connection basis.&lt;/p&gt;
      &lt;p&gt;Oxy's TLS client is designed to make HTTPS requests to upstream servers, with the functionality and security of a browser-grade client. This includes the reconstruction of certificate chains, certificate revocation checks, and more. In addition, Oxy applications can be secured with TLS v1.3, and optionally mTLS, allowing for the extraction of client authentication information from x509 certificates.&lt;/p&gt;
      &lt;p&gt;Oxy has the ability to inspect and filter HTTPS traffic, including HTTP/3, and provides the means for dynamically generating certificates, serving as a foundation for implementing data loss prevention (DLP) products. Additionally, Oxy's internal fork of BoringSSL, which is not FIPS-compliant, supports the use of raw public keys as an alternative to WebPKI, making it ideal for internal service communication. This allows for all the benefits of TLS without the hassle of managing root certificates.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Gluing everything together&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Oxy is more than just a set of building blocks for network applications. It acts as a cohesive glue, handling the bootstrapping of the entire proxy application with ease, including parsing and applying configurations, setting up an asynchronous runtime, applying seccomp hardening and providing automated graceful restarts functionality.&lt;/p&gt;
      &lt;p&gt;With built-in support for panic reporting to Sentry, Prometheus metrics with a Rust-macro based API, Kibana logging, distributed tracing, memory and runtime profiling, Oxy offers comprehensive monitoring and analysis capabilities. It can also generate detailed audit logs for layer 4 traffic, useful for billing and network analysis.&lt;/p&gt;
      &lt;p&gt;To top it off, Oxy includes an integration testing framework, allowing for easy testing of application interactions using TypeScript-based tests.&lt;/p&gt;
      &lt;p&gt;To take full advantage of Oxy's capabilities, one must understand how to extend and configure its features. Oxy applications are configured using YAML configuration files, offering numerous options for each feature. Additionally, application developers can extend these options by leveraging the convenient macros provided by the framework, making customization a breeze.&lt;/p&gt;
      &lt;p&gt;Suppose the Oxy application uses a key-value database to retrieve user information. In that case, it would be beneficial to expose a YAML configuration settings section for this purpose. With Oxy, defining a structure and annotating it with the &lt;code&gt;#[oxy_app_settings]&lt;/code&gt; attribute is all it takes to accomplish this:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;///Applicationâs key-value (KV) database settings
#[oxy_app_settings]
pub struct MyAppKVSettings {
    /// Key prefix.
    pub prefix: Option&amp;lt;String&amp;gt;,
    /// Path to the UNIX domain socket for the appropriate KV 
    /// server instance.
    pub socket: Option&amp;lt;String&amp;gt;,
}&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;Oxy can then generate a default YAML configuration file listing available options and their default values, including those extended by the application. The configuration options are automatically documented in the generated file from the Rust doc comments, following best Rust practices.&lt;/p&gt;
      &lt;p&gt;Moreover, Oxy supports multi-tenancy, allowing a single application instance to expose multiple on-ramp endpoints, each with a unique configuration. But, sometimes even a YAML configuration file is not enough to build a desired application, this is where Oxy's comprehensive set of hooks comes in handy. These hooks can be used to extend the application with Rust code and cover almost all aspects of the traffic processing.&lt;/p&gt;
      &lt;p&gt;To give you an idea of how easy it is to write an Oxy application, here is an example of basic Oxy code:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;struct MyApp;

// Defines types for various application extensions to Oxy's
// data types. Contexts provide information and control knobs for
// the different parts of the traffic flow and applications can extend // all of them with their custom data. As was mentioned before,
// applications could also define their custom configuration.
// Itâs just a matter of defining a configuration object with
// `#[oxy_app_settings]` attribute and providing the object type here.
impl OxyExt for MyApp {
    type AppSettings = MyAppKVSettings;
    type EndpointAppSettings = ();
    type EndpointContext = ();
    type IngressConnectionContext = MyAppIngressConnectionContext;
    type RequestContext = ();
    type IpTunnelContext = ();
    type DnsCacheItem = ();

}
   
#[async_trait]
impl OxyApp for MyApp {
    fn name() -&amp;gt; &amp;amp;'static str {
        "My app"
    }

    fn version() -&amp;gt; &amp;amp;'static str {
        env!("CARGO_PKG_VERSION")
    }

    fn description() -&amp;gt; &amp;amp;'static str {
        "This is an example of Oxy application"
    }

    async fn start(
        settings: ServerSettings&amp;lt;MyAppSettings, ()&amp;gt;
    ) -&amp;gt; anyhow::Result&amp;lt;Hooks&amp;lt;Self&amp;gt;&amp;gt; {
        // Here the application initializes various hooks, with each
        // hook being a trait implementation containing multiple
        // optional callbacks invoked during the lifecycle of the
        // traffic processing.
        let ingress_hook = create_ingress_hook(&amp;amp;settings);
        let egress_hook = create_egress_hook(&amp;amp;settings);
        let tunnel_hook = create_tunnel_hook(&amp;amp;settings);
        let http_request_hook = create_http_request_hook(&amp;amp;settings);
        let ip_flow_hook = create_ip_flow_hook(&amp;amp;settings);

        Ok(Hooks {
            ingress: Some(ingress_hook),
            egress: Some(egress_hook),
            tunnel: Some(tunnel_hook),
            http_request: Some(http_request_hook),
            ip_flow: Some(ip_flow_hook),
            ..Default::default()
        })
    }
}

// The entry point of the application
fn main() -&amp;gt; OxyResult&amp;lt;()&amp;gt; {
    oxy::bootstrap::&amp;lt;MyApp&amp;gt;()
}&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;Oxy leverages the safety and performance benefits of Rust as its implementation language. At Cloudflare, Rust has emerged as a popular choice for new product development, and there are ongoing efforts to migrate some of the existing products to the language as well.&lt;/p&gt;
      &lt;p&gt;Rust offers memory and concurrency safety through its ownership and borrowing system, preventing issues like null pointers and data races. This safety is achieved without sacrificing performance, as Rust provides low-level control and the ability to write code with minimal runtime overhead. Rust's balance of safety and performance has made it popular for building safe performance-critical applications, like proxies.&lt;/p&gt;
      &lt;p&gt;We intentionally tried to stand on the shoulders of the giants with this project and avoid reinventing the wheel. Oxy heavily relies on open-source dependencies, with hyper and tokio being the backbone of the framework. Our philosophy is that we should pull from existing solutions as much as we can, allowing for faster iteration, but also use widely battle-tested code. If something doesn't work for us, we try to collaborate with maintainers and contribute back our fixes and improvements. In fact, we now have two team members who are core team members of tokio and hyper projects.&lt;/p&gt;
      &lt;p&gt;Even though Oxy is a proprietary project, we try to give back some love to the open-source community without which the project wouldnât be possible by open-sourcing some of the building blocks such as https://github.com/cloudflare/boring and https://github.com/cloudflare/quiche.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;The road to implementation&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;At the beginning of our journey, we set out to implement a proof-of-concept Â for an HTTP firewall using Rust for what would eventually become Zero Trust Gateway product. This project was originally part of the WARP service repository. However, as the PoC rapidly advanced, it became clear that it needed to be separated into its own Gateway proxy for both technical and operational reasons.&lt;/p&gt;
      &lt;p&gt;Later on, when tasked with implementing a relay proxy for iCloud Private Relay, we saw the opportunity to reuse much of the code from the Gateway proxy. The Gateway project could also benefit from the HTTP/3 support that was being added for the Private Relay project. In fact, early iterations of the relay service were forks of the Gateway server.&lt;/p&gt;
      &lt;p&gt;It was then that we realized we could extract common elements from both projects to create a new framework, Oxy. The history of Oxy can be traced back to its origins in the commit history of the Gateway and Private Relay projects, up until its separation as a standalone framework.&lt;/p&gt;
      &lt;p&gt;Since our inception, we have leveraged the power of Oxy to efficiently roll out multiple projects that would have required a significant amount of time and effort without it. Our iterative development approach has been a strength of the project, as we have been able to identify common, reusable components through hands-on testing and implementation.&lt;/p&gt;
      &lt;p&gt;Our small core team is supplemented by internal contributors from across the company, ensuring that the best subject-matter experts are working on the relevant parts of the project. This contribution model also allows us to shape the framework's API to meet the functional and ergonomic needs of its users, while the core team ensures that the project stays on track.&lt;/p&gt;
      &lt;p&gt;Although Pingora, another proxy server developed by us in Rust, shares some similarities with Oxy, it was intentionally designed as a separate proxy server with a different objective. Pingora was created to serve traffic from millions of our clientâs upstream servers, including those with ancient and unusual configurations. Non-UTF 8 URLs or TLS settings that are not supported by most TLS libraries being just a few such quirks among many others. This focus on handling technically challenging unusual configurations sets Pingora apart from other proxy servers.&lt;/p&gt;
      &lt;p&gt;The concept of Pingora came about during the same period when we were beginning to develop Oxy, and we initially considered merging the two projects. However, we quickly realized that their objectives were too different to do that. Pingora is specifically designed to establish Cloudflareâs HTTP connectivity with the Internet, even in its most technically obscure corners. On the other hand, Oxy is a multipurpose platform that supports a wide variety of communication protocols and aims to provide a simple way to develop high-performance proxy applications with business logic.&lt;/p&gt;
      &lt;p&gt;Oxy is a proxy framework that we have developed to meet the demanding needs of modern services. It has been designed Â to provide a flexible and scalable solution that can be adapted to meet the unique requirements of each project and by leveraging the power of Rust, we made it both safe and fast.&lt;/p&gt;
      &lt;p&gt;Looking forward, Oxy is poised to play one of the critical roles in our company's larger effort to modernize and improve our architecture. It provides a solid block in foundation on which we can keep building the better Internet.&lt;/p&gt;
      &lt;p&gt;As the framework continues to evolve and grow, we remain committed to our iterative approach to development, constantly seeking out new opportunities to reuse existing solutions and improve our codebase. This collaborative, community-driven approach has already yielded impressive results, and we are confident that it will continue to drive the future success of Oxy.&lt;/p&gt;
      &lt;p&gt;Stay tuned for more tech savvy blog posts on the subject!&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45795511</guid><pubDate>Mon, 03 Nov 2025 03:13:46 +0000</pubDate></item><item><title>ECL Runs Maxima in a Browser</title><link>https://mailman3.common-lisp.net/hyperkitty/list/ecl-devel@common-lisp.net/thread/T64S5EMVV6WHDPKWZ3AQHEPO3EQE2K5M/</link><description>&lt;doc fingerprint="6673f6864c066aa3"&gt;
  &lt;main&gt;
    &lt;p&gt; 27 Jan 2025 27 Jan '25 &lt;/p&gt;
    &lt;p&gt; 4:06 p.m. &lt;/p&gt;
    &lt;p&gt;Thought you would be interested to know that Marius Gerbershagen has used ecl to compile Maxima to wasm to run in a browser. I believe it's a pretty complete implementation with fancy TeX display of formulas and nice graphics via gnuplot compiled to wasm. See it at http://maxima-on-wasm.pages.dev/ -- Ray&lt;/p&gt;
    &lt;p&gt; 280 &lt;/p&gt;
    &lt;p&gt; Age (days ago) &lt;/p&gt;
    &lt;p&gt; 280 &lt;/p&gt;
    &lt;p&gt; Last active (days ago) &lt;/p&gt;
    &lt;p&gt; 0 comments &lt;/p&gt;
    &lt;p&gt; 1 participants &lt;/p&gt;
    &lt;head rend="h3"&gt;participants (1)&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; Raymond Toy&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45796351</guid><pubDate>Mon, 03 Nov 2025 06:21:20 +0000</pubDate></item><item><title>Tiny electric motor can produce more than 1,000 horsepower</title><link>https://supercarblondie.com/electric-motor-yasa-more-powerful-tesla-mercedes/</link><description>&lt;doc fingerprint="b5e8d0560977a510"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Tiny electric motor is as powerful as four Tesla motors put together and outperforms record holder by 40%&lt;/head&gt;
    &lt;p&gt;Published on Nov 02, 2025 at 1:48 AM (UTC+4)&lt;lb/&gt; by Jason Fan&lt;/p&gt;
    &lt;p&gt;Last updated on Oct 29, 2025 at 10:11 PM (UTC+4)&lt;lb/&gt; Edited by Mason Jones&lt;/p&gt;
    &lt;p&gt;UK-based YASA has just built a tiny electric motor that makes Tesla motors look like slackers, and this invention could potentially reshape the future of EVs.&lt;/p&gt;
    &lt;p&gt;The company has unveiled a new prototype that’s breaking records for power and performance density.&lt;/p&gt;
    &lt;p&gt;It’s smaller and lighter than traditional motors, yet it’s somehow more powerful.&lt;/p&gt;
    &lt;p&gt;Perhaps the best part is that it’s a fully functional motor, rather than some lab-only concept.&lt;/p&gt;
    &lt;p&gt;SBX CARS – View live supercar auctions powered by Supercar Blondie&lt;/p&gt;
    &lt;head rend="h2"&gt;This tiny electric motor can produce more than 1,000 horsepower&lt;/head&gt;
    &lt;p&gt;The new YASA axial flux motor weighs just 28 pounds, or about the same as a small dog.&lt;/p&gt;
    &lt;p&gt;However, it delivers a jaw-dropping 750 kilowatts of power, which is the equivalent of 1,005 horsepower.&lt;/p&gt;
    &lt;p&gt;That’s about the same as two Tesla Model 3 Performance cars combined, or four individual Tesla motors.&lt;/p&gt;
    &lt;p&gt;In comparison, the previous record holder, which was also produced by the same company, weighed 28.8 pounds, and achieved a peak power of 550 kilowatts (737 horsepower).&lt;/p&gt;
    &lt;p&gt;This makes the current electric motor 40 percent better than the previous edition.&lt;/p&gt;
    &lt;p&gt;It can also sustain between 350 and 400 kilowatts (469–536 horsepower) continuously, meaning it’s not just built for short bursts, as it can deliver massive power all day long.&lt;/p&gt;
    &lt;p&gt;According to YASA, this is achieved without using exotic or expensive materials, so the design could actually be scalable once the demand kicks in.&lt;/p&gt;
    &lt;p&gt;“This record demonstrates what makes YASA unique,” said CEO Joerg Miska.&lt;/p&gt;
    &lt;p&gt;“With three times the performance density of today’s leading radial flux motors, we’re redefining what’s possible in electric motor design.”&lt;/p&gt;
    &lt;p&gt;In simpler terms, the company has made a motor that is small, light, and ridiculously powerful.&lt;/p&gt;
    &lt;head rend="h2"&gt;YASA already produces motors for many expensive cars&lt;/head&gt;
    &lt;p&gt;That’s a big deal for EVs.&lt;/p&gt;
    &lt;p&gt;A lighter motor means a lighter car, which means better efficiency, faster acceleration, and longer range from the same battery.&lt;/p&gt;
    &lt;p&gt;For EVs, every pound matters, so saving weight without compromising performance could be a gamechanger.&lt;/p&gt;
    &lt;p&gt;YASA, which is a wholly owned subsidiary of Mercedes-Benz, already produces motors that power some of the world’s fastest and most expensive cars.&lt;/p&gt;
    &lt;p&gt;These include the Mercedes-AMG GT XX concept, and the Ferrari 296 GTB.&lt;/p&gt;
    &lt;p&gt;Perhaps as production scales and prices drop, these ultra-efficient motors could find their way into everyday EVs, like the Nissan Leaf EV, which is the cheapest EV in the US.&lt;/p&gt;
    &lt;p&gt;For now, the company’s tiny electric motor proves that big things can come in small packages, and that performance need not be sacrificed.&lt;/p&gt;
    &lt;p&gt;DISCOVER SBX CARS: The global premium car auction platform powered by Supercar Blondie&lt;/p&gt;
    &lt;p&gt;Jason Fan is an experienced content creator who graduated from Nanyang Technological University in Singapore with a degree in communications. He then relocated to Australia during a millennial mid-life crisis. A fan of luxury travel and high-performance machines, he politely thanks chatbots just in case the AI apocalypse ever arrives. Jason covers a wide variety of topics, with a special focus on technology, planes and luxury.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45797242</guid><pubDate>Mon, 03 Nov 2025 09:20:01 +0000</pubDate></item><item><title>Update and shut down no longer restarts PC, 25H2 patch addresses decades-old bug</title><link>https://www.windowslatest.com/2025/11/02/update-and-shut-down-no-longer-restarts-pc-as-windows-11-25h2-patch-addresses-a-decades-old-bug/</link><description>&lt;doc fingerprint="349ee9773b2654d6"&gt;
  &lt;main&gt;
    &lt;p&gt;Starting with Windows 11 25H2 Build 26200.7019 (or 26100.7019 on 24H2) and newer, your PC will finally shut down when you explicitly choose “Update and shut down.”&lt;/p&gt;
    &lt;p&gt;If your PC restarts after “Update and shut down,” you’re not alone. It affects Windows 11 and 10, and is one of the most reported issues. Microsoft shipped a broken “Update and shut down” toggle with Windows 10, and it never acknowledged it until now.&lt;/p&gt;
    &lt;p&gt;I don’t want to recall the countless number of times I’ve been deceived by “Update and shut down.” When it’s 11 PM and I’ve to go to work the next morning, but there’s a pending Windows Update. I’d select Update and shut down, and go to bed, but the next morning, Windows would be on the login screen if its battery didn’t drain out.&lt;/p&gt;
    &lt;p&gt;Because those update options sit side by side, you might assume you hit “Update and restart” instead of “Update and shut down,” which would explain the return to the login screen. But no, it was a Windows bug all along, and you’re not alone if you can’t trust the ‘Update and Shut Down’ button.&lt;/p&gt;
    &lt;p&gt;We don’t know what actually causes “Update and shut down” to restart Windows. But Microsoft confirmed that the October 2025 optional update (KB5067036) finally fixed an underlying issue that blocked “Update and shut down” from working in some cases.&lt;/p&gt;
    &lt;p&gt;“Addressed underlying issue which can cause ‘Update and shutdown’ to not actually shut down your PC after updating,” Microsoft noted in a support document.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why “Update and shut down” was broken in Windows 11 and Windows 10&lt;/head&gt;
    &lt;p&gt;Microsoft won’t tell us what really happened, but there’s a chance it was a race condition or an issue with the Windows Servicing Stack.&lt;/p&gt;
    &lt;p&gt;When you use Update and shut down, Windows has two tasks to perform. First, it’ll begin installing all pending updates. Second, it’ll power off the computer at the end of the process, but the catch is that the process isn’t just about “install update and turn off.”&lt;/p&gt;
    &lt;p&gt;Windows can’t skip a reboot just because you told it to shut down after updating. It must reboot into an offline servicing phase, which is when you see “working on updates” on your screen. This step is required because Windows cannot finish replacing files when it’s running.&lt;/p&gt;
    &lt;p&gt;After the “working on updates” phase, Windows is supposed to finally power off, but it doesn’t, and Windows boots to the login screen. The issue was most likely with the Servicing Stack, and the “power off” task is never carried across Windows reboots. It’s either cleared or a race condition, like Fast Startup, that blocks it.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45797934</guid><pubDate>Mon, 03 Nov 2025 11:22:59 +0000</pubDate></item><item><title>OSS Alternative to Open WebUI – ChatGPT-Like UI, API and CLI</title><link>https://github.com/ServiceStack/llms</link><description>&lt;doc fingerprint="ac0f9f590da61b82"&gt;
  &lt;main&gt;
    &lt;p&gt;Lightweight CLI, API and ChatGPT-like alternative to Open WebUI for accessing multiple LLMs, entirely offline, with all data kept private in browser storage.&lt;/p&gt;
    &lt;p&gt;Configure additional providers and models in llms.json&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Mix and match local models with models from different API providers&lt;/item&gt;
      &lt;item&gt;Requests automatically routed to available providers that supports the requested model (in defined order)&lt;/item&gt;
      &lt;item&gt;Define free/cheapest/local providers first to save on costs&lt;/item&gt;
      &lt;item&gt;Any failures are automatically retried on the next available provider&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Lightweight: Single llms.py Python file with single &lt;code&gt;aiohttp&lt;/code&gt;dependency (Pillow optional)&lt;/item&gt;
      &lt;item&gt;Multi-Provider Support: OpenRouter, Ollama, Anthropic, Google, OpenAI, Grok, Groq, Qwen, Z.ai, Mistral&lt;/item&gt;
      &lt;item&gt;OpenAI-Compatible API: Works with any client that supports OpenAI's chat completion API&lt;/item&gt;
      &lt;item&gt;Built-in Analytics: Built-in analytics UI to visualize costs, requests, and token usage&lt;/item&gt;
      &lt;item&gt;Configuration Management: Easy provider enable/disable and configuration management&lt;/item&gt;
      &lt;item&gt;CLI Interface: Simple command-line interface for quick interactions&lt;/item&gt;
      &lt;item&gt;Server Mode: Run an OpenAI-compatible HTTP server at &lt;code&gt;http://localhost:{PORT}/v1/chat/completions&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Image Support: Process images through vision-capable models &lt;list rend="ul"&gt;&lt;item&gt;Auto resizes and converts to webp if exceeds configured limits&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Audio Support: Process audio through audio-capable models&lt;/item&gt;
      &lt;item&gt;Custom Chat Templates: Configurable chat completion request templates for different modalities&lt;/item&gt;
      &lt;item&gt;Auto-Discovery: Automatically discover available Ollama models&lt;/item&gt;
      &lt;item&gt;Unified Models: Define custom model names that map to different provider-specific names&lt;/item&gt;
      &lt;item&gt;Multi-Model Support: Support for over 160+ different LLMs&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Access all your local all remote LLMs with a single ChatGPT-like UI:&lt;/p&gt;
    &lt;p&gt;More Features and Screenshots.&lt;/p&gt;
    &lt;p&gt;Check the status of configured providers to test if they're configured correctly, reachable and what their response times is for the simplest &lt;code&gt;1+1=&lt;/code&gt; request:&lt;/p&gt;
    &lt;code&gt;# Check all models for a provider:
llms --check groq

# Check specific models for a provider:
llms --check groq kimi-k2 llama4:400b gpt-oss:120b&lt;/code&gt;
    &lt;p&gt;As they're a good indicator for the reliability and speed you can expect from different providers we've created a test-providers.yml GitHub Action to test the response times for all configured providers and models, the results of which will be frequently published to /checks/latest.txt&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Improved Responsive Layout with collapsible Sidebar&lt;/item&gt;
      &lt;item&gt;Watching config files for changes and auto-reloading&lt;/item&gt;
      &lt;item&gt;Add cancel button to cancel pending request&lt;/item&gt;
      &lt;item&gt;Return focus to textarea after request completes&lt;/item&gt;
      &lt;item&gt;Clicking outside model or system prompt selector will collapse it&lt;/item&gt;
      &lt;item&gt;Clicking on selected item no longer deselects it&lt;/item&gt;
      &lt;item&gt;Support &lt;code&gt;VERBOSE=1&lt;/code&gt;for enabling&lt;code&gt;--verbose&lt;/code&gt;mode (useful in Docker)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Dark Mode&lt;/item&gt;
      &lt;item&gt;Drag n' Drop files in Message prompt&lt;/item&gt;
      &lt;item&gt;Copy &amp;amp; Paste files in Message prompt&lt;/item&gt;
      &lt;item&gt;Support for GitHub OAuth and optional restrict access to specified Users&lt;/item&gt;
      &lt;item&gt;Support for Docker and Docker Compose&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;pip install llms-py&lt;/code&gt;
    &lt;p&gt;Set environment variables for the providers you want to use:&lt;/p&gt;
    &lt;code&gt;export OPENROUTER_API_KEY="..."&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Provider&lt;/cell&gt;
        &lt;cell role="head"&gt;Variable&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
        &lt;cell role="head"&gt;Example&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;openrouter_free&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;OPENROUTER_API_KEY&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;OpenRouter FREE models API key&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;sk-or-...&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;groq&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;GROQ_API_KEY&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Groq API key&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;gsk_...&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;google_free&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;GOOGLE_FREE_API_KEY&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Google FREE API key&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;AIza...&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;codestral&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;CODESTRAL_API_KEY&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Codestral API key&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;...&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;ollama&lt;/cell&gt;
        &lt;cell&gt;N/A&lt;/cell&gt;
        &lt;cell&gt;No API key required&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;openrouter&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;OPENROUTER_API_KEY&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;OpenRouter API key&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;sk-or-...&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;GOOGLE_API_KEY&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Google API key&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;AIza...&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;anthropic&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;ANTHROPIC_API_KEY&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Anthropic API key&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;sk-ant-...&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;openai&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;OPENAI_API_KEY&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;OpenAI API key&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;sk-...&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;grok&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;GROK_API_KEY&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Grok (X.AI) API key&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;xai-...&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;qwen&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;DASHSCOPE_API_KEY&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Qwen (Alibaba) API key&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;sk-...&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;z.ai&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;ZAI_API_KEY&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Z.ai API key&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;sk-...&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;mistral&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;MISTRAL_API_KEY&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Mistral API key&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;...&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Start the UI and an OpenAI compatible API on port 8000:&lt;/p&gt;
    &lt;code&gt;llms --serve 8000&lt;/code&gt;
    &lt;p&gt;Launches UI at &lt;code&gt;http://localhost:8000&lt;/code&gt; and OpenAI Endpoint at &lt;code&gt;http://localhost:8000/v1/chat/completions&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;To see detailed request/response logging, add &lt;code&gt;--verbose&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;llms --serve 8000 --verbose&lt;/code&gt;
    &lt;code&gt;llms "What is the capital of France?"&lt;/code&gt;
    &lt;p&gt;Any providers that have their API Keys set and enabled in &lt;code&gt;llms.json&lt;/code&gt; are automatically made available.&lt;/p&gt;
    &lt;p&gt;Providers can be enabled or disabled in the UI at runtime next to the model selector, or on the command line:&lt;/p&gt;
    &lt;code&gt;# Disable free providers with free models and free tiers
llms --disable openrouter_free codestral google_free groq

# Enable paid providers
llms --enable openrouter anthropic google openai grok z.ai qwen mistral&lt;/code&gt;
    &lt;p&gt;Run the server on port &lt;code&gt;8000&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;docker run -p 8000:8000 -e GROQ_API_KEY=$GROQ_API_KEY ghcr.io/servicestack/llms:latest&lt;/code&gt;
    &lt;p&gt;Get the latest version:&lt;/p&gt;
    &lt;code&gt;docker pull ghcr.io/servicestack/llms:latest&lt;/code&gt;
    &lt;p&gt;Use custom &lt;code&gt;llms.json&lt;/code&gt; and &lt;code&gt;ui.json&lt;/code&gt; config files outside of the container (auto created if they don't exist):&lt;/p&gt;
    &lt;code&gt;docker run -p 8000:8000 -e GROQ_API_KEY=$GROQ_API_KEY \
  -v ~/.llms:/home/llms/.llms \
  ghcr.io/servicestack/llms:latest&lt;/code&gt;
    &lt;p&gt;Download and use docker-compose.yml:&lt;/p&gt;
    &lt;code&gt;curl -O https://raw.githubusercontent.com/ServiceStack/llms/refs/heads/main/docker-compose.yml&lt;/code&gt;
    &lt;p&gt;Update API Keys in &lt;code&gt;docker-compose.yml&lt;/code&gt; then start the server:&lt;/p&gt;
    &lt;code&gt;docker-compose up -d&lt;/code&gt;
    &lt;code&gt;git clone https://github.com/ServiceStack/llms

docker-compose -f docker-compose.local.yml up -d --build&lt;/code&gt;
    &lt;p&gt;After the container starts, you can access the UI and API at &lt;code&gt;http://localhost:8000&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;See DOCKER.md for detailed instructions on customizing configuration files.&lt;/p&gt;
    &lt;p&gt;llms.py supports optional GitHub OAuth authentication to secure your web UI and API endpoints. When enabled, users must sign in with their GitHub account before accessing the application.&lt;/p&gt;
    &lt;code&gt;{
    "auth": {
        "enabled": true,
        "github": {
            "client_id": "$GITHUB_CLIENT_ID",
            "client_secret": "$GITHUB_CLIENT_SECRET",
            "redirect_uri": "http://localhost:8000/auth/github/callback",
            "restrict_to": "$GITHUB_USERS"
        }
    }
}&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;GITHUB_USERS&lt;/code&gt; is optional but if set will only allow access to the specified users.&lt;/p&gt;
    &lt;p&gt;See GITHUB_OAUTH_SETUP.md for detailed setup instructions.&lt;/p&gt;
    &lt;p&gt;The configuration file llms.json is saved to &lt;code&gt;~/.llms/llms.json&lt;/code&gt; and defines available providers, models, and default settings. If it doesn't exist, &lt;code&gt;llms.json&lt;/code&gt; is auto created with the latest
configuration, so you can re-create it by deleting your local config (e.g. &lt;code&gt;rm -rf ~/.llms&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;Key sections:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;headers&lt;/code&gt;: Common HTTP headers for all requests&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;text&lt;/code&gt;: Default chat completion request template for text prompts&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;image&lt;/code&gt;: Default chat completion request template for image prompts&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;audio&lt;/code&gt;: Default chat completion request template for audio prompts&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;file&lt;/code&gt;: Default chat completion request template for file prompts&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;check&lt;/code&gt;: Check request template for testing provider connectivity&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;limits&lt;/code&gt;: Override Request size limits&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;convert&lt;/code&gt;: Max image size and length limits and auto conversion settings&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Each provider configuration includes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;enabled&lt;/code&gt;: Whether the provider is active&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;type&lt;/code&gt;: Provider class (OpenAiProvider, GoogleProvider, etc.)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;api_key&lt;/code&gt;: API key (supports environment variables with&lt;code&gt;$VAR_NAME&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;base_url&lt;/code&gt;: API endpoint URL&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;models&lt;/code&gt;: Model name mappings (local name → provider name)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;pricing&lt;/code&gt;: Pricing per token (input/output) for each model&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;default_pricing&lt;/code&gt;: Default pricing if not specified in&lt;code&gt;pricing&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;check&lt;/code&gt;: Check request template for testing provider connectivity&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Simple question
llms "Explain quantum computing"

# With specific model
llms -m gemini-2.5-pro "Write a Python function to sort a list"
llms -m grok-4 "Explain this code with humor"
llms -m qwen3-max "Translate this to Chinese"

# With system prompt
llms -s "You are a helpful coding assistant" "How do I reverse a string in Python?"

# With image (vision models)
llms --image image.jpg "What's in this image?"
llms --image https://example.com/photo.png "Describe this photo"

# Display full JSON Response
llms "Explain quantum computing" --raw&lt;/code&gt;
    &lt;p&gt;By default llms uses the &lt;code&gt;defaults/text&lt;/code&gt; chat completion request defined in llms.json.&lt;/p&gt;
    &lt;p&gt;You can instead use a custom chat completion request with &lt;code&gt;--chat&lt;/code&gt;, e.g:&lt;/p&gt;
    &lt;code&gt;# Load chat completion request from JSON file
llms --chat request.json

# Override user message
llms --chat request.json "New user message"

# Override model
llms -m kimi-k2 --chat request.json&lt;/code&gt;
    &lt;p&gt;Example &lt;code&gt;request.json&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;{
  "model": "kimi-k2",
  "messages": [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user",   "content": ""}
  ],
  "temperature": 0.7,
  "max_tokens": 150
}&lt;/code&gt;
    &lt;p&gt;Send images to vision-capable models using the &lt;code&gt;--image&lt;/code&gt; option:&lt;/p&gt;
    &lt;code&gt;# Use defaults/image Chat Template (Describe the key features of the input image)
llms --image ./screenshot.png

# Local image file
llms --image ./screenshot.png "What's in this image?"

# Remote image URL
llms --image https://example.org/photo.jpg "Describe this photo"

# Data URI
llms --image "data:image/png;base64,$(base64 -w 0 image.png)" "Describe this image"

# With a specific vision model
llms -m gemini-2.5-flash --image chart.png "Analyze this chart"
llms -m qwen2.5vl --image document.jpg "Extract text from this document"

# Combined with system prompt
llms -s "You are a data analyst" --image graph.png "What trends do you see?"

# With custom chat template
llms --chat image-request.json --image photo.jpg&lt;/code&gt;
    &lt;p&gt;Example of &lt;code&gt;image-request.json&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;{
    "model": "qwen2.5vl",
    "messages": [
        {
            "role": "user",
            "content": [
                {
                    "type": "image_url",
                    "image_url": {
                        "url": ""
                    }
                },
                {
                    "type": "text",
                    "text": "Caption this image"
                }
            ]
        }
    ]
}&lt;/code&gt;
    &lt;p&gt;Supported image formats: PNG, WEBP, JPG, JPEG, GIF, BMP, TIFF, ICO&lt;/p&gt;
    &lt;p&gt;Image sources:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Local files: Absolute paths (&lt;code&gt;/path/to/image.jpg&lt;/code&gt;) or relative paths (&lt;code&gt;./image.png&lt;/code&gt;,&lt;code&gt;../image.jpg&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Remote URLs: HTTP/HTTPS URLs are automatically downloaded&lt;/item&gt;
      &lt;item&gt;Data URIs: Base64-encoded images (&lt;code&gt;data:image/png;base64,...&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Images are automatically processed and converted to base64 data URIs before being sent to the model.&lt;/p&gt;
    &lt;p&gt;Popular models that support image analysis:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;OpenAI: GPT-4o, GPT-4o-mini, GPT-4.1&lt;/item&gt;
      &lt;item&gt;Anthropic: Claude Sonnet 4.0, Claude Opus 4.1&lt;/item&gt;
      &lt;item&gt;Google: Gemini 2.5 Pro, Gemini Flash&lt;/item&gt;
      &lt;item&gt;Qwen: Qwen2.5-VL, Qwen3-VL, QVQ-max&lt;/item&gt;
      &lt;item&gt;Ollama: qwen2.5vl, llava&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Images are automatically downloaded and converted to base64 data URIs.&lt;/p&gt;
    &lt;p&gt;Send audio files to audio-capable models using the &lt;code&gt;--audio&lt;/code&gt; option:&lt;/p&gt;
    &lt;code&gt;# Use defaults/audio Chat Template (Transcribe the audio)
llms --audio ./recording.mp3

# Local audio file
llms --audio ./meeting.wav "Summarize this meeting recording"

# Remote audio URL
llms --audio https://example.org/podcast.mp3 "What are the key points discussed?"

# With a specific audio model
llms -m gpt-4o-audio-preview --audio interview.mp3 "Extract the main topics"
llms -m gemini-2.5-flash --audio interview.mp3 "Extract the main topics"

# Combined with system prompt
llms -s "You're a transcription specialist" --audio talk.mp3 "Provide a detailed transcript"

# With custom chat template
llms --chat audio-request.json --audio speech.wav&lt;/code&gt;
    &lt;p&gt;Example of &lt;code&gt;audio-request.json&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;{
    "model": "gpt-4o-audio-preview",
    "messages": [
        {
            "role": "user",
            "content": [
                {
                    "type": "input_audio",
                    "input_audio": {
                        "data": "",
                        "format": "mp3"
                    }
                },
                {
                    "type": "text",
                    "text": "Please transcribe this audio"
                }
            ]
        }
    ]
}&lt;/code&gt;
    &lt;p&gt;Supported audio formats: MP3, WAV&lt;/p&gt;
    &lt;p&gt;Audio sources:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Local files: Absolute paths (&lt;code&gt;/path/to/audio.mp3&lt;/code&gt;) or relative paths (&lt;code&gt;./audio.wav&lt;/code&gt;,&lt;code&gt;../recording.m4a&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Remote URLs: HTTP/HTTPS URLs are automatically downloaded&lt;/item&gt;
      &lt;item&gt;Base64 Data: Base64-encoded audio&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Audio files are automatically processed and converted to base64 data before being sent to the model.&lt;/p&gt;
    &lt;p&gt;Popular models that support audio processing:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;OpenAI: gpt-4o-audio-preview&lt;/item&gt;
      &lt;item&gt;Google: gemini-2.5-pro, gemini-2.5-flash, gemini-2.5-flash-lite&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Audio files are automatically downloaded and converted to base64 data URIs with appropriate format detection.&lt;/p&gt;
    &lt;p&gt;Send documents (e.g. PDFs) to file-capable models using the &lt;code&gt;--file&lt;/code&gt; option:&lt;/p&gt;
    &lt;code&gt;# Use defaults/file Chat Template (Summarize the document)
llms --file ./docs/handbook.pdf

# Local PDF file
llms --file ./docs/policy.pdf "Summarize the key changes"

# Remote PDF URL
llms --file https://example.org/whitepaper.pdf "What are the main findings?"

# With specific file-capable models
llms -m gpt-5               --file ./policy.pdf   "Summarize the key changes"
llms -m gemini-flash-latest --file ./report.pdf   "Extract action items"
llms -m qwen2.5vl           --file ./manual.pdf   "List key sections and their purpose"

# Combined with system prompt
llms -s "You're a compliance analyst" --file ./policy.pdf "Identify compliance risks"

# With custom chat template
llms --chat file-request.json --file ./docs/handbook.pdf&lt;/code&gt;
    &lt;p&gt;Example of &lt;code&gt;file-request.json&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;{
  "model": "gpt-5",
  "messages": [
    {
      "role": "user",
      "content": [
        {
          "type": "file",
          "file": {
            "filename": "",
            "file_data": ""
          }
        },
        {
          "type": "text",
          "text": "Please summarize this document"
        }
      ]
    }
  ]
}&lt;/code&gt;
    &lt;p&gt;Supported file formats: PDF&lt;/p&gt;
    &lt;p&gt;Other document types may work depending on the model/provider.&lt;/p&gt;
    &lt;p&gt;File sources:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Local files: Absolute paths (&lt;code&gt;/path/to/file.pdf&lt;/code&gt;) or relative paths (&lt;code&gt;./file.pdf&lt;/code&gt;,&lt;code&gt;../file.pdf&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Remote URLs: HTTP/HTTPS URLs are automatically downloaded&lt;/item&gt;
      &lt;item&gt;Base64/Data URIs: Inline &lt;code&gt;data:application/pdf;base64,...&lt;/code&gt;is supported&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Files are automatically downloaded (for URLs) and converted to base64 data URIs before being sent to the model.&lt;/p&gt;
    &lt;p&gt;Popular multi-modal models that support file (PDF) inputs:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;OpenAI: gpt-5, gpt-5-mini, gpt-4o, gpt-4o-mini&lt;/item&gt;
      &lt;item&gt;Google: gemini-flash-latest, gemini-2.5-flash-lite&lt;/item&gt;
      &lt;item&gt;Grok: grok-4-fast (OpenRouter)&lt;/item&gt;
      &lt;item&gt;Qwen: qwen2.5vl, qwen3-max, qwen3-vl:235b, qwen3-coder, qwen3-coder-flash (OpenRouter)&lt;/item&gt;
      &lt;item&gt;Others: kimi-k2, glm-4.5-air, deepseek-v3.1:671b, llama4:400b, llama3.3:70b, mai-ds-r1, nemotron-nano:9b&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Run as an OpenAI-compatible HTTP server:&lt;/p&gt;
    &lt;code&gt;# Start server on port 8000
llms --serve 8000&lt;/code&gt;
    &lt;p&gt;The server exposes a single endpoint:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;POST /v1/chat/completions&lt;/code&gt;- OpenAI-compatible chat completions&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example client usage:&lt;/p&gt;
    &lt;code&gt;curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "kimi-k2",
    "messages": [
      {"role": "user", "content": "Hello!"}
    ]
  }'&lt;/code&gt;
    &lt;code&gt;# List enabled providers and models
llms --list
llms ls

# List specific providers
llms ls ollama
llms ls google anthropic

# Enable providers
llms --enable openrouter
llms --enable anthropic google_free groq

# Disable providers
llms --disable ollama
llms --disable openai anthropic

# Set default model
llms --default grok-4&lt;/code&gt;
    &lt;code&gt;pip install llms-py --upgrade&lt;/code&gt;
    &lt;code&gt;# Use custom config file
llms --config /path/to/config.json "Hello"

# Get raw JSON response
llms --raw "What is 2+2?"

# Enable verbose logging
llms --verbose "Tell me a joke"

# Custom log prefix
llms --verbose --logprefix "[DEBUG] " "Hello world"

# Set default model (updates config file)
llms --default grok-4

# Pass custom parameters to chat request (URL-encoded)
llms --args "temperature=0.7&amp;amp;seed=111" "What is 2+2?"

# Multiple parameters with different types
llms --args "temperature=0.5&amp;amp;max_completion_tokens=50" "Tell me a joke"

# URL-encoded special characters (stop sequences)
llms --args "stop=Two,Words" "Count to 5"

# Combine with other options
llms --system "You are helpful" --args "temperature=0.3" --raw "Hello"&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;--args&lt;/code&gt; option allows you to pass URL-encoded parameters to customize the chat request sent to LLM providers:&lt;/p&gt;
    &lt;p&gt;Parameter Types:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Floats: &lt;code&gt;temperature=0.7&lt;/code&gt;,&lt;code&gt;frequency_penalty=0.2&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Integers: &lt;code&gt;max_completion_tokens=100&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Booleans: &lt;code&gt;store=true&lt;/code&gt;,&lt;code&gt;verbose=false&lt;/code&gt;,&lt;code&gt;logprobs=true&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Strings: &lt;code&gt;stop=one&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Lists: &lt;code&gt;stop=two,words&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Common Parameters:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;temperature&lt;/code&gt;: Controls randomness (0.0 to 2.0)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;max_completion_tokens&lt;/code&gt;: Maximum tokens in response&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;seed&lt;/code&gt;: For reproducible outputs&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;top_p&lt;/code&gt;: Nucleus sampling parameter&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;stop&lt;/code&gt;: Stop sequences (URL-encode special chars)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;store&lt;/code&gt;: Whether or not to store the output&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;frequency_penalty&lt;/code&gt;: Penalize new tokens based on frequency&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;presence_penalty&lt;/code&gt;: Penalize new tokens based on presence&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;logprobs&lt;/code&gt;: Include log probabilities in response&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;parallel_tool_calls&lt;/code&gt;: Enable parallel tool calls&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;prompt_cache_key&lt;/code&gt;: Cache key for prompt&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;reasoning_effort&lt;/code&gt;: Reasoning effort (low, medium, high, *minimal, *none, *default)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;safety_identifier&lt;/code&gt;: A string that uniquely identifies each user&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;seed&lt;/code&gt;: For reproducible outputs&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;service_tier&lt;/code&gt;: Service tier (free, standard, premium, *default)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;top_logprobs&lt;/code&gt;: Number of top logprobs to return&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;top_p&lt;/code&gt;: Nucleus sampling parameter&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;verbosity&lt;/code&gt;: Verbosity level (0, 1, 2, 3, *default)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;enable_thinking&lt;/code&gt;: Enable thinking mode (Qwen)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;stream&lt;/code&gt;: Enable streaming responses&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The &lt;code&gt;--default MODEL&lt;/code&gt; option allows you to set the default model used for all chat completions. This updates the &lt;code&gt;defaults.text.model&lt;/code&gt; field in your configuration file:&lt;/p&gt;
    &lt;code&gt;# Set default model to gpt-oss
llms --default gpt-oss:120b

# Set default model to Claude Sonnet
llms --default claude-sonnet-4-0

# The model must be available in your enabled providers
llms --default gemini-2.5-pro&lt;/code&gt;
    &lt;p&gt;When you set a default model:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The configuration file (&lt;code&gt;~/.llms/llms.json&lt;/code&gt;) is automatically updated&lt;/item&gt;
      &lt;item&gt;The specified model becomes the default for all future chat requests&lt;/item&gt;
      &lt;item&gt;The model must exist in your currently enabled providers&lt;/item&gt;
      &lt;item&gt;You can still override the default using &lt;code&gt;-m MODEL&lt;/code&gt;for individual requests&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;pip install llms-py --upgrade&lt;/code&gt;
    &lt;p&gt;Pipe Markdown output to glow to beautifully render it in the terminal:&lt;/p&gt;
    &lt;code&gt;llms "Explain quantum computing" | glow&lt;/code&gt;
    &lt;p&gt;Any OpenAI-compatible providers and their models can be added by configuring them in llms.json. By default only AI Providers with free tiers are enabled which will only be "available" if their API Key is set.&lt;/p&gt;
    &lt;p&gt;You can list the available providers, their models and which are enabled or disabled with:&lt;/p&gt;
    &lt;code&gt;llms ls&lt;/code&gt;
    &lt;p&gt;They can be enabled/disabled in your &lt;code&gt;llms.json&lt;/code&gt; file or with:&lt;/p&gt;
    &lt;code&gt;llms --enable &amp;lt;provider&amp;gt;
llms --disable &amp;lt;provider&amp;gt;&lt;/code&gt;
    &lt;p&gt;For a provider to be available, they also require their API Key configured in either your Environment Variables or directly in your &lt;code&gt;llms.json&lt;/code&gt;.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Provider&lt;/cell&gt;
        &lt;cell role="head"&gt;Variable&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
        &lt;cell role="head"&gt;Example&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;openrouter_free&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;OPENROUTER_API_KEY&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;OpenRouter FREE models API key&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;sk-or-...&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;groq&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;GROQ_API_KEY&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Groq API key&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;gsk_...&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;google_free&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;GOOGLE_FREE_API_KEY&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Google FREE API key&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;AIza...&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;codestral&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;CODESTRAL_API_KEY&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Codestral API key&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;...&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;ollama&lt;/cell&gt;
        &lt;cell&gt;N/A&lt;/cell&gt;
        &lt;cell&gt;No API key required&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;openrouter&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;OPENROUTER_API_KEY&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;OpenRouter API key&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;sk-or-...&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;GOOGLE_API_KEY&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Google API key&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;AIza...&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;anthropic&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;ANTHROPIC_API_KEY&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Anthropic API key&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;sk-ant-...&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;openai&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;OPENAI_API_KEY&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;OpenAI API key&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;sk-...&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;grok&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;GROK_API_KEY&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Grok (X.AI) API key&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;xai-...&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;qwen&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;DASHSCOPE_API_KEY&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Qwen (Alibaba) API key&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;sk-...&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;z.ai&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;ZAI_API_KEY&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Z.ai API key&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;sk-...&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;mistral&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;MISTRAL_API_KEY&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Mistral API key&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;...&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Type: &lt;code&gt;OpenAiProvider&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Models: GPT-5, GPT-5 Codex, GPT-4o, GPT-4o-mini, o3, etc.&lt;/item&gt;
      &lt;item&gt;Features: Text, images, function calling&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;export OPENAI_API_KEY="your-key"
llms --enable openai&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Type: &lt;code&gt;OpenAiProvider&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Models: Claude Opus 4.1, Sonnet 4.0, Haiku 3.5, etc.&lt;/item&gt;
      &lt;item&gt;Features: Text, images, large context windows&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;export ANTHROPIC_API_KEY="your-key"
llms --enable anthropic&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Type: &lt;code&gt;GoogleProvider&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Models: Gemini 2.5 Pro, Flash, Flash-Lite&lt;/item&gt;
      &lt;item&gt;Features: Text, images, safety settings&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;export GOOGLE_API_KEY="your-key"
llms --enable google_free&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Type: &lt;code&gt;OpenAiProvider&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Models: 100+ models from various providers&lt;/item&gt;
      &lt;item&gt;Features: Access to latest models, free tier available&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;export OPENROUTER_API_KEY="your-key"
llms --enable openrouter&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Type: &lt;code&gt;OpenAiProvider&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Models: Grok-4, Grok-3, Grok-3-mini, Grok-code-fast-1, etc.&lt;/item&gt;
      &lt;item&gt;Features: Real-time information, humor, uncensored responses&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;export GROK_API_KEY="your-key"
llms --enable grok&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Type: &lt;code&gt;OpenAiProvider&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Models: Llama 3.3, Gemma 2, Kimi K2, etc.&lt;/item&gt;
      &lt;item&gt;Features: Fast inference, competitive pricing&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;export GROQ_API_KEY="your-key"
llms --enable groq&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Type: &lt;code&gt;OllamaProvider&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Models: Auto-discovered from local Ollama installation&lt;/item&gt;
      &lt;item&gt;Features: Local inference, privacy, no API costs&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Ollama must be running locally
llms --enable ollama&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Type: &lt;code&gt;OpenAiProvider&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Models: Qwen3-max, Qwen-max, Qwen-plus, Qwen2.5-VL, QwQ-plus, etc.&lt;/item&gt;
      &lt;item&gt;Features: Multilingual, vision models, coding, reasoning, audio processing&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;export DASHSCOPE_API_KEY="your-key"
llms --enable qwen&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Type: &lt;code&gt;OpenAiProvider&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Models: GLM-4.6, GLM-4.5, GLM-4.5-air, GLM-4.5-x, GLM-4.5-airx, GLM-4.5-flash, GLM-4:32b&lt;/item&gt;
      &lt;item&gt;Features: Advanced language models with strong reasoning capabilities&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;export ZAI_API_KEY="your-key"
llms --enable z.ai&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Type: &lt;code&gt;OpenAiProvider&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Models: Mistral Large, Codestral, Pixtral, etc.&lt;/item&gt;
      &lt;item&gt;Features: Code generation, multilingual&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;export MISTRAL_API_KEY="your-key"
llms --enable mistral&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Type: &lt;code&gt;OpenAiProvider&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Models: Codestral&lt;/item&gt;
      &lt;item&gt;Features: Code generation&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;export CODESTRAL_API_KEY="your-key"
llms --enable codestral&lt;/code&gt;
    &lt;p&gt;The tool automatically routes requests to the first available provider that supports the requested model. If a provider fails, it tries the next available provider with that model.&lt;/p&gt;
    &lt;p&gt;Example: If both OpenAI and OpenRouter support &lt;code&gt;kimi-k2&lt;/code&gt;, the request will first try OpenRouter (free), then fall back to Groq than OpenRouter (Paid) if requests fails.&lt;/p&gt;
    &lt;code&gt;{
  "defaults": {
    "headers": {"Content-Type": "application/json"},
    "text": {
      "model": "kimi-k2",
      "messages": [{"role": "user", "content": ""}]
    }
  },
  "providers": {
    "groq": {
      "enabled": true,
      "type": "OpenAiProvider",
      "base_url": "https://api.groq.com/openai",
      "api_key": "$GROQ_API_KEY",
      "models": {
        "llama3.3:70b": "llama-3.3-70b-versatile",
        "llama4:109b": "meta-llama/llama-4-scout-17b-16e-instruct",
        "llama4:400b": "meta-llama/llama-4-maverick-17b-128e-instruct",
        "kimi-k2": "moonshotai/kimi-k2-instruct-0905",
        "gpt-oss:120b": "openai/gpt-oss-120b",
        "gpt-oss:20b": "openai/gpt-oss-20b",
        "qwen3:32b": "qwen/qwen3-32b"
      }
    }
  }
}&lt;/code&gt;
    &lt;code&gt;{
  "providers": {
    "openrouter": {
      "enabled": false,
      "type": "OpenAiProvider",
      "base_url": "https://openrouter.ai/api",
      "api_key": "$OPENROUTER_API_KEY",
      "models": {
        "grok-4": "x-ai/grok-4",
        "glm-4.5-air": "z-ai/glm-4.5-air",
        "kimi-k2": "moonshotai/kimi-k2",
        "deepseek-v3.1:671b": "deepseek/deepseek-chat",
        "llama4:400b": "meta-llama/llama-4-maverick"
      }
    },
    "anthropic": {
      "enabled": false,
      "type": "OpenAiProvider",
      "base_url": "https://api.anthropic.com",
      "api_key": "$ANTHROPIC_API_KEY",
      "models": {
        "claude-sonnet-4-0": "claude-sonnet-4-0"
      }
    },
    "ollama": {
      "enabled": false,
      "type": "OllamaProvider",
      "base_url": "http://localhost:11434",
      "models": {},
      "all_models": true
    }
  }
}&lt;/code&gt;
    &lt;code&gt;usage: llms [-h] [--config FILE] [-m MODEL] [--chat REQUEST] [-s PROMPT] [--image IMAGE] [--audio AUDIO] [--file FILE]
            [--args PARAMS] [--raw] [--list] [--check PROVIDER] [--serve PORT] [--enable PROVIDER] [--disable PROVIDER]
            [--default MODEL] [--init] [--root PATH] [--logprefix PREFIX] [--verbose]

llms v2.0.24

options:
  -h, --help            show this help message and exit
  --config FILE         Path to config file
  -m, --model MODEL     Model to use
  --chat REQUEST        OpenAI Chat Completion Request to send
  -s, --system PROMPT   System prompt to use for chat completion
  --image IMAGE         Image input to use in chat completion
  --audio AUDIO         Audio input to use in chat completion
  --file FILE           File input to use in chat completion
  --args PARAMS         URL-encoded parameters to add to chat request (e.g. "temperature=0.7&amp;amp;seed=111")
  --raw                 Return raw AI JSON response
  --list                Show list of enabled providers and their models (alias ls provider?)
  --check PROVIDER      Check validity of models for a provider
  --serve PORT          Port to start an OpenAI Chat compatible server on
  --enable PROVIDER     Enable a provider
  --disable PROVIDER    Disable a provider
  --default MODEL       Configure the default model to use
  --init                Create a default llms.json
  --root PATH           Change root directory for UI files
  --logprefix PREFIX    Prefix used in log messages
  --verbose             Verbose output
&lt;/code&gt;
    &lt;p&gt;The easiest way to run llms-py is using Docker:&lt;/p&gt;
    &lt;code&gt;# Using docker-compose (recommended)
docker-compose up -d

# Or pull and run directly
docker run -p 8000:8000 \
  -e OPENROUTER_API_KEY="your-key" \
  ghcr.io/servicestack/llms:latest&lt;/code&gt;
    &lt;p&gt;Pre-built Docker images are automatically published to GitHub Container Registry:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Latest stable: &lt;code&gt;ghcr.io/servicestack/llms:latest&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Specific version: &lt;code&gt;ghcr.io/servicestack/llms:v2.0.24&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Main branch: &lt;code&gt;ghcr.io/servicestack/llms:main&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Pass API keys as environment variables:&lt;/p&gt;
    &lt;code&gt;docker run -p 8000:8000 \
  -e OPENROUTER_API_KEY="sk-or-..." \
  -e GROQ_API_KEY="gsk_..." \
  -e GOOGLE_FREE_API_KEY="AIza..." \
  -e ANTHROPIC_API_KEY="sk-ant-..." \
  -e OPENAI_API_KEY="sk-..." \
  ghcr.io/servicestack/llms:latest&lt;/code&gt;
    &lt;p&gt;Create a &lt;code&gt;docker-compose.yml&lt;/code&gt; file (or use the one in the repository):&lt;/p&gt;
    &lt;code&gt;version: '3.8'

services:
  llms:
    image: ghcr.io/servicestack/llms:latest
    ports:
      - "8000:8000"
    environment:
      - OPENROUTER_API_KEY=${OPENROUTER_API_KEY}
      - GROQ_API_KEY=${GROQ_API_KEY}
      - GOOGLE_FREE_API_KEY=${GOOGLE_FREE_API_KEY}
    volumes:
      - llms-data:/home/llms/.llms
    restart: unless-stopped

volumes:
  llms-data:&lt;/code&gt;
    &lt;p&gt;Create a &lt;code&gt;.env&lt;/code&gt; file with your API keys:&lt;/p&gt;
    &lt;code&gt;OPENROUTER_API_KEY=sk-or-...
GROQ_API_KEY=gsk_...
GOOGLE_FREE_API_KEY=AIza...&lt;/code&gt;
    &lt;p&gt;Start the service:&lt;/p&gt;
    &lt;code&gt;docker-compose up -d&lt;/code&gt;
    &lt;p&gt;Build the Docker image from source:&lt;/p&gt;
    &lt;code&gt;# Using the build script
./docker-build.sh

# Or manually
docker build -t llms-py:latest .

# Run your local build
docker run -p 8000:8000 \
  -e OPENROUTER_API_KEY="your-key" \
  llms-py:latest&lt;/code&gt;
    &lt;p&gt;To persist configuration and analytics data between container restarts:&lt;/p&gt;
    &lt;code&gt;# Using a named volume (recommended)
docker run -p 8000:8000 \
  -v llms-data:/home/llms/.llms \
  -e OPENROUTER_API_KEY="your-key" \
  ghcr.io/servicestack/llms:latest

# Or mount a local directory
docker run -p 8000:8000 \
  -v $(pwd)/llms-config:/home/llms/.llms \
  -e OPENROUTER_API_KEY="your-key" \
  ghcr.io/servicestack/llms:latest&lt;/code&gt;
    &lt;p&gt;Customize llms-py behavior by providing your own &lt;code&gt;llms.json&lt;/code&gt; and &lt;code&gt;ui.json&lt;/code&gt; files:&lt;/p&gt;
    &lt;p&gt;Option 1: Mount a directory with custom configs&lt;/p&gt;
    &lt;code&gt;# Create config directory with your custom files
mkdir -p config
# Add your custom llms.json and ui.json to config/

# Mount the directory
docker run -p 8000:8000 \
  -v $(pwd)/config:/home/llms/.llms \
  -e OPENROUTER_API_KEY="your-key" \
  ghcr.io/servicestack/llms:latest&lt;/code&gt;
    &lt;p&gt;Option 2: Mount individual config files&lt;/p&gt;
    &lt;code&gt;docker run -p 8000:8000 \
  -v $(pwd)/my-llms.json:/home/llms/.llms/llms.json:ro \
  -v $(pwd)/my-ui.json:/home/llms/.llms/ui.json:ro \
  -e OPENROUTER_API_KEY="your-key" \
  ghcr.io/servicestack/llms:latest&lt;/code&gt;
    &lt;p&gt;With docker-compose:&lt;/p&gt;
    &lt;code&gt;volumes:
  # Use local directory
  - ./config:/home/llms/.llms

  # Or mount individual files
  # - ./my-llms.json:/home/llms/.llms/llms.json:ro
  # - ./my-ui.json:/home/llms/.llms/ui.json:ro&lt;/code&gt;
    &lt;p&gt;The container will auto-create default config files on first run if they don't exist. You can customize these to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Enable/disable specific providers&lt;/item&gt;
      &lt;item&gt;Add or remove models&lt;/item&gt;
      &lt;item&gt;Configure API endpoints&lt;/item&gt;
      &lt;item&gt;Set custom pricing&lt;/item&gt;
      &lt;item&gt;Customize chat templates&lt;/item&gt;
      &lt;item&gt;Configure UI settings&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;See DOCKER.md for detailed configuration examples.&lt;/p&gt;
    &lt;p&gt;Change the port mapping to run on a different port:&lt;/p&gt;
    &lt;code&gt;# Run on port 3000 instead of 8000
docker run -p 3000:8000 \
  -e OPENROUTER_API_KEY="your-key" \
  ghcr.io/servicestack/llms:latest&lt;/code&gt;
    &lt;p&gt;You can also use the Docker container for CLI commands:&lt;/p&gt;
    &lt;code&gt;# Run a single query
docker run --rm \
  -e OPENROUTER_API_KEY="your-key" \
  ghcr.io/servicestack/llms:latest \
  llms "What is the capital of France?"

# List available models
docker run --rm \
  -e OPENROUTER_API_KEY="your-key" \
  ghcr.io/servicestack/llms:latest \
  llms --list

# Check provider status
docker run --rm \
  -e GROQ_API_KEY="your-key" \
  ghcr.io/servicestack/llms:latest \
  llms --check groq&lt;/code&gt;
    &lt;p&gt;The Docker image includes a health check that verifies the server is responding:&lt;/p&gt;
    &lt;code&gt;# Check container health
docker ps

# View health check logs
docker inspect --format='{{json .State.Health}}' llms-server&lt;/code&gt;
    &lt;p&gt;The Docker images support multiple architectures:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;linux/amd64&lt;/code&gt;(x86_64)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;linux/arm64&lt;/code&gt;(ARM64/Apple Silicon)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Docker will automatically pull the correct image for your platform.&lt;/p&gt;
    &lt;p&gt;Config file not found&lt;/p&gt;
    &lt;code&gt;# Initialize default config
llms --init

# Or specify custom path
llms --config ./my-config.json&lt;/code&gt;
    &lt;p&gt;No providers enabled&lt;/p&gt;
    &lt;code&gt;# Check status
llms --list

# Enable providers
llms --enable google anthropic&lt;/code&gt;
    &lt;p&gt;API key issues&lt;/p&gt;
    &lt;code&gt;# Check environment variables
echo $ANTHROPIC_API_KEY

# Enable verbose logging
llms --verbose "test"&lt;/code&gt;
    &lt;p&gt;Model not found&lt;/p&gt;
    &lt;code&gt;# List available models
llms --list

# Check provider configuration
llms ls openrouter&lt;/code&gt;
    &lt;p&gt;Enable verbose logging to see detailed request/response information:&lt;/p&gt;
    &lt;code&gt;llms --verbose --logprefix "[DEBUG] " "Hello"&lt;/code&gt;
    &lt;p&gt;This shows:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Enabled providers&lt;/item&gt;
      &lt;item&gt;Model routing decisions&lt;/item&gt;
      &lt;item&gt;HTTP request details&lt;/item&gt;
      &lt;item&gt;Error messages with stack traces&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;llms/main.py&lt;/code&gt;- Main script with CLI and server functionality&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;llms/llms.json&lt;/code&gt;- Default configuration file&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;llms/ui.json&lt;/code&gt;- UI configuration file&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;requirements.txt&lt;/code&gt;- Python dependencies, required:&lt;code&gt;aiohttp&lt;/code&gt;, optional:&lt;code&gt;Pillow&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;OpenAiProvider&lt;/code&gt;- Generic OpenAI-compatible provider&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;OllamaProvider&lt;/code&gt;- Ollama-specific provider with model auto-discovery&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;GoogleProvider&lt;/code&gt;- Google Gemini with native API format&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;GoogleOpenAiProvider&lt;/code&gt;- Google Gemini via OpenAI-compatible endpoint&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Create a provider class inheriting from &lt;code&gt;OpenAiProvider&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Implement provider-specific authentication and formatting&lt;/item&gt;
      &lt;item&gt;Add provider configuration to &lt;code&gt;llms.json&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Update initialization logic in &lt;code&gt;init_llms()&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Contributions are welcome! Please submit a PR to add support for any missing OpenAI-compatible providers.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45798193</guid><pubDate>Mon, 03 Nov 2025 12:05:28 +0000</pubDate></item><item><title>The Case Against PGVector</title><link>https://alex-jacobs.com/posts/the-case-against-pgvector/</link><description>&lt;doc fingerprint="231d8b761d4497ae"&gt;
  &lt;main&gt;&lt;head rend="h2"&gt;Everyone Loves pgvector (in theory)&lt;/head&gt;&lt;p&gt;If you’ve spent any time in the vector search space over the past year, you’ve probably read blog posts explaining why pgvector is the obvious choice for your vector database needs. The argument goes something like this: you already have Postgres, vector embeddings are just another data type, why add complexity with a dedicated vector database when you can keep everything in one place?&lt;/p&gt;&lt;p&gt;It’s a compelling story. And like most of the AI influencer bullshit that fills my timeline, it glosses over the inconvenient details.&lt;/p&gt;&lt;p&gt;I’m not here to tell you pgvector is bad. It’s not. It’s a useful extension that brings vector similarity search to Postgres. But after spending some time trying to build a production system on top of it, I’ve learned that the gap between “works in a demo” and “scales in production” is&amp;amp;mldr; significant.&lt;/p&gt;&lt;head rend="h2"&gt;Nobody’s actually run this in production&lt;/head&gt;&lt;p&gt;What bothers me most: the majority of content about pgvector reads like it was written by someone who spun up a local Postgres instance, inserted 10,000 vectors, ran a few queries, and called it a day. The posts are optimistic, the benchmarks are clean, and the conclusions are confident.&lt;/p&gt;&lt;p&gt;They’re also missing about 80% of what you actually need to know.&lt;/p&gt;&lt;p&gt;I’ve read through dozens of these posts.&lt;/p&gt;They all cover the same ground: here’s how to install pgvector, here’s how to create a vector column, here’s a simple similarity search query. Some of them even mention that you should probably add an index.&lt;p&gt;What they don’t tell you is what happens when you actually try to run this in production.&lt;/p&gt;&lt;head rend="h2"&gt;Picking an index (there are no good options)&lt;/head&gt;&lt;p&gt;Let’s start with indexes, because this is where the tradeoffs start.&lt;/p&gt;&lt;p&gt;pgvector gives you two index types: IVFFlat and HNSW. The blog posts will tell you that HNSW is newer and generally better, which is&amp;amp;mldr; technically true but deeply unhelpful.&lt;/p&gt;&lt;head rend="h3"&gt;IVFFlat&lt;/head&gt;&lt;p&gt;IVFFlat (Inverted File with Flat quantization) partitions your vector space into clusters. During search, it identifies the nearest clusters and only searches within those.&lt;/p&gt;&lt;p&gt;The good:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Lower memory footprint during index creation&lt;/item&gt;&lt;item&gt;Reasonable query performance for many use cases&lt;/item&gt;&lt;item&gt;Index creation is faster than HNSW&lt;/item&gt;&lt;/list&gt;&lt;p&gt;The bad:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Requires you to specify the number of lists (clusters) upfront&lt;/item&gt;&lt;item&gt;That number significantly impacts both recall and query performance&lt;/item&gt;&lt;item&gt;The commonly recommended formula (&lt;code&gt;rows / 1000&lt;/code&gt;) is a starting point at best&lt;/item&gt;&lt;item&gt;Recall can be&amp;amp;mldr; disappointing depending on your data distribution&lt;/item&gt;&lt;item&gt;New vectors get assigned to existing clusters, but clusters don’t rebalance without a full rebuild&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Image source: IVFFlat or HNSW index for similarity search? by Simeon Emanuilov&lt;/p&gt;&lt;head rend="h3"&gt;HNSW&lt;/head&gt;&lt;p&gt;HNSW (Hierarchical Navigable Small World) builds a multi-layer graph structure for search.&lt;/p&gt;&lt;p&gt;The good:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Better recall than IVFFlat for most datasets&lt;/item&gt;&lt;item&gt;More consistent query performance&lt;/item&gt;&lt;item&gt;Scales well to larger datasets&lt;/item&gt;&lt;/list&gt;&lt;p&gt;The bad:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Significantly higher memory requirements during index builds&lt;/item&gt;&lt;item&gt;Index creation is slow—painfully slow for large datasets&lt;/item&gt;&lt;item&gt;The memory requirements aren’t theoretical; they are real, and they’ll take down your database if you’re not careful&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Image source: IVFFlat or HNSW index for similarity search? by Simeon Emanuilov&lt;/p&gt;&lt;p&gt;None of the blogs mention that building an HNSW index on a few million vectors can consume 10+ GB of RAM or more (depending on your vector dimensions and dataset size). On your production database. While it’s running. For potentially hours.&lt;/p&gt;&lt;head rend="h2"&gt;Real-time search is basically impossible&lt;/head&gt;&lt;p&gt;In a typical application, you want newly uploaded data to be searchable immediately. User uploads a document, you generate embeddings, insert them into your database, and they should be available in search results. Simple, right?&lt;/p&gt;&lt;head rend="h3"&gt;How index updates actually work&lt;/head&gt;&lt;p&gt;When you insert new vectors into a table with an index, one of two things happens:&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;&lt;p&gt;IVFFlat: The new vectors are inserted into the appropriate clusters based on the existing structure. This works, but it means your cluster distribution gets increasingly suboptimal over time. The solution is to rebuild the index periodically. Which means downtime, or maintaining a separate index and doing an atomic swap, or accepting degraded search quality.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;HNSW: New vectors are added to the graph structure. This is better than IVFFlat, but it’s not free. Each insertion requires updating the graph, which means memory allocation, graph traversals, and potential lock contention.&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Neither of these is a deal-breaker in isolation. But here’s what happens in practice: you’re inserting vectors continuously throughout the day. Each insertion is individually cheap, but the aggregate load adds up. Your database is now handling your normal transactional workload, analytical queries, AND maintaining graph structures in memory for vector search.&lt;/p&gt;&lt;head rend="h3"&gt;Handling new inserts&lt;/head&gt;&lt;p&gt;Let’s say you’re building a document search system. Users upload PDFs, you extract text, generate embeddings, and insert them. The user expects to immediately search for that document.&lt;/p&gt;&lt;p&gt;Here’s what actually happens:&lt;/p&gt;&lt;p&gt;With no index: The insert is fast, the document is immediately available, but your searches do a full sequential scan. This works fine for a few thousand documents. At a few hundred thousand? Your searches start taking seconds. Millions? Good luck.&lt;/p&gt;&lt;p&gt;With IVFFlat: The insert is still relatively fast. The vector gets assigned to a cluster. But whoops, a problem. Those initial cluster assignments were based on the data distribution when you built the index. As you add more data, especially if it’s not uniformly distributed, some clusters get overloaded. Your search quality degrades. You rebuild the index periodically to fix this, but during the rebuild (which can take hours for large datasets), what do you do with new inserts? Queue them? Write to a separate unindexed table and merge later?&lt;/p&gt;&lt;p&gt;With HNSW: The graph gets updated on each insert through incremental insertion, which sounds great. But updating an HNSW graph isn’t free—you’re traversing the graph to find the right place to insert the new node and updating connections. Each insert acquires locks on the graph structure. Under heavy write load, this becomes a bottleneck. And if your write rate is high enough, you start seeing lock contention that slows down both writes and reads.&lt;/p&gt;&lt;head rend="h3"&gt;The operational reality&lt;/head&gt;&lt;p&gt;Here’s the real nightmare: you’re not just storing vectors. You have metadata—document titles, timestamps, user IDs, categories, etc. That metadata lives in other tables (or other columns in the same table). You need that metadata and the vectors to stay in sync.&lt;/p&gt;&lt;p&gt;In a normal Postgres table, this is easy—transactions handle it. But when you’re dealing with index builds that take hours, keeping everything consistent gets complicated. For IVFFlat, periodic rebuilds are basically required to maintain search quality. For HNSW, you might need to rebuild if you want to tune parameters or if performance has degraded.&lt;/p&gt;&lt;p&gt;The problem is that index builds are memory-intensive operations, and Postgres doesn’t have a great way to throttle them. You’re essentially asking your production database to allocate multiple (possibly dozens) gigabytes of RAM for an operation that might take hours, while continuing to serve queries.&lt;/p&gt;&lt;p&gt;You end up with strategies like:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Write to a staging table, build the index offline, then swap it in (but now you have a window where searches miss new data)&lt;/item&gt;&lt;item&gt;Maintain two indexes and write to both (double the memory, double the update cost)&lt;/item&gt;&lt;item&gt;Build indexes on replicas and promote them&lt;/item&gt;&lt;item&gt;Accept eventual consistency (users upload documents that aren’t searchable for N minutes)&lt;/item&gt;&lt;item&gt;Provision significantly more RAM than your “working set” would suggest&lt;/item&gt;&lt;/list&gt;&lt;p&gt;None of these are “wrong” exactly. But they’re all workarounds for the fact that pgvector wasn’t really designed for high-velocity real-time ingestion.&lt;/p&gt;&lt;head rend="h2"&gt;Pre- vs. Post-Filtering (or: why you need to become a query planner expert)&lt;/head&gt;&lt;p&gt;Okay but let’s say you solve your index and insert problems. Now you have a document search system with millions of vectors. Documents have metadata—maybe they’re marked as &lt;code&gt;draft&lt;/code&gt;, &lt;code&gt;published&lt;/code&gt;, or &lt;code&gt;archived&lt;/code&gt;. A user searches for something, and you only want to return published documents.&lt;/p&gt;&lt;code&gt;1SELECT * FROM documents
2WHERE status = 'published'
3ORDER BY embedding &amp;lt;-&amp;gt; query_vector
4LIMIT 10;
&lt;/code&gt;&lt;p&gt;Simple enough. But now you have a problem: should Postgres filter on status first (pre-filter) or do the vector search first and then filter (post-filter)?&lt;/p&gt;&lt;p&gt;This seems like an implementation detail. It’s not. It’s the difference between queries that take 50ms and queries that take 5 seconds. It’s also the difference between returning the most relevant results and&amp;amp;mldr; not.&lt;/p&gt;&lt;p&gt;Pre-filter works great when the filter is highly selective (1,000 docs out of 10M). It works terribly when the filter isn’t selective—you’re still searching millions of vectors.&lt;/p&gt;&lt;p&gt;Post-filter works when your filter is permissive. Here’s where it breaks: imagine you ask for 10 results with &lt;code&gt;LIMIT 10&lt;/code&gt;. pgvector finds the 10 nearest neighbors, then applies your filter. Only 3 of those 10 are published. You get 3 results back, even though there might be hundreds of relevant published documents slightly further away in the embedding space.&lt;/p&gt;&lt;p&gt;The user searched, got 3 mediocre results, and has no idea they’re missing way better matches that didn’t make it into the initial k=10 search.&lt;/p&gt;&lt;p&gt;You can work around this by fetching more vectors (say, &lt;code&gt;LIMIT 100&lt;/code&gt;) and then filtering, but now:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;You’re doing way more distance calculations than needed&lt;/item&gt;&lt;item&gt;You still don’t know if 100 is enough&lt;/item&gt;&lt;item&gt;Your query performance suffers&lt;/item&gt;&lt;item&gt;You’re guessing at the right oversampling factor&lt;/item&gt;&lt;/list&gt;&lt;p&gt;With pre-filter, you avoid this problem, but you get the performance problems I mentioned. Pick your poison.&lt;/p&gt;&lt;head rend="h3"&gt;Multiple filters&lt;/head&gt;&lt;p&gt;Now add another dimension: you’re filtering by user_id AND category AND date_range.&lt;/p&gt;&lt;code&gt;1SELECT * FROM documents
2WHERE user_id = 'user123'
3  AND category = 'technical'
4  AND created_at &amp;gt; '2024-01-01'
5ORDER BY embedding &amp;lt;-&amp;gt; query_vector
6LIMIT 10;
&lt;/code&gt;&lt;p&gt;What’s the right strategy now?&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Apply all filters first, then search? (Pre-filter)&lt;/item&gt;&lt;item&gt;Search first, then apply all filters? (Post-filter)&lt;/item&gt;&lt;item&gt;Apply some filters first, search, then apply remaining filters? (Hybrid)&lt;/item&gt;&lt;item&gt;Which filters should you apply in which order?&lt;/item&gt;&lt;/list&gt;&lt;p&gt;The planner will look at table statistics, index selectivity, and estimated row counts and come up with a plan. That plan will probably be wrong, or at least suboptimal, because the planner’s cost model wasn’t built for vector similarity search.&lt;/p&gt;&lt;p&gt;And it gets worse: you’re inserting new vectors throughout the day. Your index statistics are outdated. The plans get increasingly suboptimal until you ANALYZE the table. But ANALYZE on a large table with millions of rows takes time and resources. And it doesn’t really understand vector data distribution in a meaningful way—it can tell you how many rows match &lt;code&gt;user_id = 'user123'&lt;/code&gt;, but not how clustered those vectors are in the embedding space, which is what actually matters for search performance.&lt;/p&gt;&lt;head rend="h3"&gt;Workarounds&lt;/head&gt;&lt;p&gt;You end up with hacks: query rewriting for different user types, partitioning your data into separate tables, CTE optimization fences to force the planner’s hand, or just fetching way more results than needed and filtering in application code.&lt;/p&gt;&lt;p&gt;None of these are sustainable at scale.&lt;/p&gt;&lt;head rend="h3"&gt;What vector databases do&lt;/head&gt;&lt;p&gt;Dedicated vector databases have solved this. They understand the cost model of filtered vector search and make intelligent decisions:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Adaptive strategies: Some databases dynamically choose pre-filter or post-filter based on estimated selectivity&lt;/item&gt;&lt;item&gt;Configurable modes: Others let you specify the strategy explicitly when you know your data distribution&lt;/item&gt;&lt;item&gt;Specialized indexes: Some build indexes that support efficient filtered search (like filtered HNSW)&lt;/item&gt;&lt;item&gt;Query optimization: They track statistics specific to vector operations and optimize accordingly&lt;/item&gt;&lt;/list&gt;&lt;p&gt;OpenSearch’s k-NN plugin, for example, lets you specify pre-filter or post-filter behavior. Pinecone automatically handles filter selectivity. Weaviate has optimizations for common filter patterns.&lt;/p&gt;&lt;p&gt;With pgvector, you get to build all of this yourself. Or live with suboptimal queries. Or hire a Postgres expert to spend weeks tuning your query patterns.&lt;/p&gt;&lt;head rend="h2"&gt;Hybrid search? Build it yourself&lt;/head&gt;&lt;p&gt;Oh, and if you want hybrid search—combining vector similarity with traditional full-text search—you get to build that yourself too.&lt;/p&gt;&lt;p&gt;Postgres has excellent full-text search capabilities. pgvector has excellent vector search capabilities. Combining them in a meaningful way? That’s on you.&lt;/p&gt;&lt;p&gt;You need to:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Decide how to weight vector similarity vs. text relevance&lt;/item&gt;&lt;item&gt;Normalize scores from two different scoring systems&lt;/item&gt;&lt;item&gt;Tune the balance for your use case&lt;/item&gt;&lt;item&gt;Probably implement Reciprocal Rank Fusion or something similar&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Again, not impossible. Just another thing that many dedicated vector databases provide out of the box.&lt;/p&gt;&lt;head rend="h2"&gt;pgvectorscale (it doesn’t solve everything)&lt;/head&gt;&lt;p&gt;Timescale has released pgvectorscale, which addresses some of these issues. It adds:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;StreamingDiskANN, a new search backend that’s more memory-efficient&lt;/item&gt;&lt;item&gt;Better support for incremental index builds&lt;/item&gt;&lt;item&gt;Improved filtering performance&lt;/item&gt;&lt;/list&gt;&lt;p&gt;This is great! It’s also an admission that pgvector out of the box isn’t sufficient for production use cases.&lt;/p&gt;&lt;p&gt;pgvectorscale is still relatively new, and adopting it means adding another dependency, another extension, another thing to manage and upgrade. For some teams, that’s fine. For others, it’s just more evidence that maybe the “keep it simple, use Postgres” argument isn’t as simple as it seemed.&lt;/p&gt;&lt;p&gt;Oh, and if you’re running on RDS, pgvectorscale isn’t available. AWS doesn’t support it. So enjoy managing your own Postgres instance if you want these improvements, or just&amp;amp;mldr; keep dealing with the limitations of vanilla pgvector.&lt;/p&gt;&lt;p&gt;The “just use Postgres” simplicity keeps getting simpler.&lt;/p&gt;&lt;head rend="h2"&gt;Just use a real vector database&lt;/head&gt;&lt;p&gt;I get the appeal of pgvector. Consolidating your stack is good. Reducing operational complexity is good. Not having to manage another database is good.&lt;/p&gt;&lt;p&gt;But here’s what I’ve learned: for most teams, especially small teams, dedicated vector databases are actually simpler.&lt;/p&gt;&lt;head rend="h3"&gt;What you actually get&lt;/head&gt;&lt;p&gt;With a managed vector database (Pinecone, Weaviate, Turbopuffer, etc.), you typically get:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Intelligent query planning for filtered searches&lt;/item&gt;&lt;item&gt;Hybrid search built in&lt;/item&gt;&lt;item&gt;Real-time indexing without memory spikes&lt;/item&gt;&lt;item&gt;Horizontal scaling without complexity&lt;/item&gt;&lt;item&gt;Monitoring and observability designed for vector workloads&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;It’s probably cheaper than you think&lt;/head&gt;&lt;p&gt;Yes, it’s another service to pay for. But compare:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;The cost of a managed vector database for your workload&lt;/item&gt;&lt;item&gt;vs. the cost of over-provisioning your Postgres instance to handle index builds&lt;/item&gt;&lt;item&gt;vs. the engineering time to tune queries and manage index rebuilds&lt;/item&gt;&lt;item&gt;vs. the opportunity cost of not building features because you’re fighting your database&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Turbopuffer starts at $64 month with generous limits.&lt;/p&gt;&lt;p&gt;For a lot of teams, the managed service is actually cheaper.&lt;/p&gt;&lt;head rend="h2"&gt;What I wish someone had told me&lt;/head&gt;&lt;p&gt;pgvector is an impressive piece of technology. It brings vector search to Postgres in a way that’s technically sound and genuinely useful for many applications.&lt;/p&gt;&lt;p&gt;But it’s not a panacea. Understand the tradeoffs.&lt;/p&gt;&lt;p&gt;If you’re building a production vector search system:&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;&lt;p&gt;Index management is hard. Rebuilds are memory-intensive, time-consuming, and disruptive. Plan for this from day one.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;Query planning matters. Filtered vector search is a different beast than traditional queries, and Postgres’s planner wasn’t built for this.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;Real-time indexing has costs. Either in memory, in search quality, or in engineering time to manage it.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;The blog posts are lying to you (by omission). They’re showing you the happy path and ignoring the operational reality.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;Managed offerings exist for a reason. There’s a reason that Pinecone, Weaviate, Qdrant, and others exist and are thriving. Vector search at scale has unique challenges that general-purpose databases weren’t designed to handle.&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;p&gt;The question isn’t “should I use pgvector?” It’s “am I willing to take on the operational complexity of running vector search in Postgres?”&lt;/p&gt;&lt;p&gt;For some teams, the answer is yes. You have database expertise, you need the tight integration, you’re willing to invest the time.&lt;/p&gt;&lt;p&gt;For many teams—maybe most teams—the answer is probably no. Use a tool designed for the job. Your future self will thank you.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45798479</guid><pubDate>Mon, 03 Nov 2025 12:50:27 +0000</pubDate></item><item><title>Why Nextcloud feels slow to use</title><link>https://ounapuu.ee/posts/2025/11/03/nextcloud-slow/</link><description>&lt;doc fingerprint="57c20352a0823c6c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Why Nextcloud feels slow to use&lt;/head&gt;
    &lt;p&gt;Nextcloud. I really want to like it, but it’s making it really difficult.&lt;/p&gt;
    &lt;p&gt;I like what Nextcloud offers with its feature set and how easily it replaces a bunch of services under one roof (files, calendar, contacts, notes, to-do lists, photos etc.), but no matter how hard I try and how much I optimize its resources on my home server, it feels slow to use, even on hardware that is ranging from decent to good. Then I opened developer tools and found the culprit.&lt;/p&gt;
    &lt;p&gt;It’s the Javascript.&lt;/p&gt;
    &lt;p&gt;On a clean page load, you will be downloading about 15-20 MB of Javascript, which does compress down to about 4-5 MB in transit, but that is still a huge amount of Javascript. For context, I consider 1 MB of Javascript to be on the heavy side for a web page/app.&lt;/p&gt;
    &lt;p&gt;Yes, that Javascript will be cached in the browser for a while, but you will still be executing all of that on each visit to your Nextcloud instance, and that will take a long time due to the sheer amount of code your browser now has to execute on the page.&lt;/p&gt;
    &lt;p&gt;A significant contributor to this heft seems to be the &lt;code&gt;core-common.js&lt;/code&gt; bundle, which based on its name seems to provide
some common functionality that’s shared across different Nextcloud apps that one can install. It’s coming in at 4.71
MB at the time of writing.&lt;/p&gt;
    &lt;p&gt;Then you want notifications, right? &lt;code&gt;NotificationsApp.chunk.mjs&lt;/code&gt; is here to cover you, at 1.06 MB.&lt;/p&gt;
    &lt;p&gt;Then there are the app-specific views. The Calendar app is taking up 5.94 MB to show a basic calendar view.&lt;/p&gt;
    &lt;p&gt;Files app includes a bunch of individual scripts, such as &lt;code&gt;EditorOutline&lt;/code&gt; (1.77 MB), &lt;code&gt;previewUtils&lt;/code&gt; (1.17 MB),
&lt;code&gt;index&lt;/code&gt; (1.09 MB), &lt;code&gt;emoji-picker&lt;/code&gt; (0.9 MB which I’ve never used!) and many smaller ones.&lt;/p&gt;
    &lt;p&gt;Notes app with its basic bare-bones editor? 4.36 MB for the &lt;code&gt;notes-main.js&lt;/code&gt;!&lt;/p&gt;
    &lt;p&gt;This means that even on an iPhone 13 mini, opening the Tasks app (to-do list), will take a ridiculously long time. Imagine opening your shopping list at the store and having to wait 5-10 seconds before you see anything, even with a solid 5G connection. Sounds extremely annoying, right?&lt;/p&gt;
    &lt;p&gt;I suspect that a lot of this is due to how Nextcloud is architected. There’s bound to be some hefty common libraries and tools that allow app developers to provide a unified experience, but even then there is something seriously wrong with the end result, the functionality to bundle size ratio is way off.&lt;/p&gt;
    &lt;p&gt;As a result, I’ve started branching out some things from Nextcloud, such as replacing the Tasks app with using a private Vikunja instance, and Photos to a private Immich instance. Vikunja is not perfect, but its 1.5 MB of Javascript is an order of magnitude smaller compared to Nextcloud, making it feel incredibly fast in comparison.&lt;/p&gt;
    &lt;p&gt;However, with other functionality I have to admit that the convenience of Nextcloud is enough to dissuade me from replacing it elsewhere, due to the available feature set comparing well to alternatives.&lt;/p&gt;
    &lt;p&gt;For now.&lt;/p&gt;
    &lt;p&gt;I’m sure that there are some legitimate reasons behind the current state, and overworked development teams and volunteers are unfortunately the norm in the industry, but it doesn’t take away the fact that the user experience and accessibility suffers as a result.&lt;/p&gt;
    &lt;p&gt;I’d like to thank Alex Russell for writing about web performance and why it matters, with supporting evidence and actionable advice, it has changed how I view websites and web apps and has pushed me to be better in my own work. I highly suggest reading his content, starting with the performance inequality gap series. It’s educational, insightful and incredibly irritating once you learn how crap most things are and how careless a lot of development teams are towards performance and accessibility.&lt;/p&gt;
    &lt;p&gt;Subscribe to new posts via the RSS feed.&lt;/p&gt;
    &lt;p&gt;Not sure what RSS is, or how to get started? Check this guide!&lt;/p&gt;
    &lt;p&gt;You can reach me via e-mail or LinkedIn.&lt;/p&gt;
    &lt;p&gt;If you liked this post, consider sharing it!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45798681</guid><pubDate>Mon, 03 Nov 2025 13:21:09 +0000</pubDate></item><item><title>Offline Math: Converting LaTeX to SVG with MathJax</title><link>https://sigwait.org/~alex/blog/2025/10/07/3t8acq.html</link><description>&lt;doc fingerprint="ae42081a27e3a30c"&gt;
  &lt;main&gt;&lt;head rend="h2"&gt;Offline Math: Converting LaTeX to SVG with MathJax&lt;/head&gt;&lt;p&gt;Latest update: &lt;/p&gt;&lt;p&gt;Pandoc can prepare LaTeX math for MathJax via its eponymous &lt;code&gt;--mathjax&lt;/code&gt; option. It wraps formulas in &lt;code&gt;&amp;lt;span class="math"&amp;gt;&lt;/code&gt;
elements and injects a &lt;code&gt;&amp;lt;script&amp;gt;&lt;/code&gt; tag that points to
cdn.jsdelivr.net, which means rendering won't work offline or in
case of the 3rd-party server failure. You can mitigate this by
providing your own copy of the MathJax library, but the mechanism
still fails when the target device doesn't support JavaScript (e.g.,
many epub readers).&lt;/p&gt;&lt;p&gt;At the same time, practically all browsers support MathML. Use it (pandoc's &lt;code&gt;--mathml&lt;/code&gt; option), if you care only about the information
superhighway: your formulas will look good on every modern device and
scale delightfully. Otherwise, SVGs are the only truly portable
option.&lt;/p&gt;&lt;p&gt;Now, how can we transform the html produced by&lt;/p&gt;&lt;quote&gt;&lt;code&gt;$ echo 'Ohm'\''s law: $I = \frac{V}{R}$.' |
  pandoc -s -f markdown --mathjax
&lt;/code&gt;&lt;/quote&gt;&lt;p&gt;into a fully standalone document where the formula gets converted into SVG nodes?&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;Use an html parser like Nokogiri, and replace each &lt;code&gt;&amp;lt;span class="math"&amp;gt;&lt;/code&gt; node with an image. There are multiple ways to
convert a TeX-looking string to an SVG: using MathJax itself (which
provides a corresponding CLI example), or by doing it in a
'classical' fashion with pdflatex. (You can read more about this
method in A practical guide to EPUB, chapters 3.4 and 4.6.)&lt;/item&gt;&lt;/list&gt;&lt;list start="2" rend="ol"&gt;&lt;item&gt;Alternatively, load the page into a headless browser, inject MathJax scripts, and serialise the modified DOM back to html.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;I tried the 2nd approach in 2016 with the now-defunct phantomjs. It worked, but debugging was far from enjoyable due to the strangest bugs in phantomjs. I can still run the old code, but it depends on an ancient version of the MathJax library that, for obvious reasons, isn't easily upgradable within the phantomjs pre-es6 environment.&lt;/p&gt;&lt;p&gt;Nowadays, Puppeteer would certainly do, but for this kind of task I prefer something more lightweight.&lt;/p&gt;&lt;p&gt;There's also jsdom. Back in 2016, I tried it as well, but it was much slower than running phantomjs. Recently, I gave jsdom another try and was pleasantly surprised. I'm not sure what exactly tipped the scales: computers, v8, or jsdom itself, but it no longer feels slow in combination with MathJax.&lt;/p&gt;&lt;quote&gt;&lt;code&gt;$ wc -l *js *conf.json
  24 loader.js
 105 mathjax-embed.js
  12 mathjax.conf.json
 141 total
&lt;/code&gt;&lt;/quote&gt;&lt;p&gt;Roughly 50% of the code is nodejs infrastructure junk (including CL parsing), the rest is a MathJax config and jsdom interactions:&lt;/p&gt;&lt;quote&gt;&lt;code&gt;let dom = new JSDOM(html, {
  url: `file://${base}/`,
  runScripts: /* very */ 'dangerously',
  resources: new MyResourceLoader(), // block ext. absolute urls
})

dom.window.my_exit = function() {
  cleanup(dom.window.document) // remove mathjax &amp;lt;script&amp;gt; tags
  console.log(dom.serialize())
}

dom.window.my_mathjax_conf = mathjax_conf // user-provided

let script = new Script(read(`${import.meta.dirname}/loader.js`))
let vmContext = dom.getInternalVMContext()
script.runInContext(vmContext)
&lt;/code&gt;&lt;/quote&gt;&lt;p&gt;The most annoying step here is setting &lt;code&gt;url&lt;/code&gt; property that jsdom uses
to resolve paths to relative resources. &lt;code&gt;my_exit()&lt;/code&gt; function is called
by MathJax when its job is supposedly finished. &lt;code&gt;loader.js&lt;/code&gt; script is
executed in the context of the loaded html:&lt;/p&gt;&lt;quote&gt;&lt;code&gt;window.MathJax = {
  output: { fontPath: '@mathjax/%%FONT%%-font' },
  startup: {
    ready() {
      MathJax.startup.defaultReady()
      MathJax.startup.promise.then(window.my_exit)
    }
  }
}

Object.assign(window.MathJax, window.my_mathjax_conf)

function main() {
  var script = document.createElement('script')
  script.src = 'mathjax/startup.js'
  document.head.appendChild(script)
}

document.addEventListener('DOMContentLoaded', main)
&lt;/code&gt;&lt;/quote&gt;&lt;p&gt;The full source is on Github.&lt;/p&gt;&lt;p&gt;Intended use is as follows:&lt;/p&gt;&lt;quote&gt;&lt;code&gt;$ echo 'Ohm'\''s law: $I = \frac{V}{R}$.' |
  pandoc -s -f markdown --mathjax |
  mathjax-embed &amp;gt; 1.html
&lt;/code&gt;&lt;/quote&gt;&lt;p&gt;The resulting html doesn't use JavaScript and doesn't fetch any external MathJax resources. &lt;code&gt;mathjax-embed&lt;/code&gt; script itself always works
offline.&lt;/p&gt;&lt;lb/&gt;Tags: Ð¾Ð¹ÑÑ&lt;lb/&gt;Authors: ag&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45798746</guid><pubDate>Mon, 03 Nov 2025 13:29:17 +0000</pubDate></item><item><title>Google Suspended My Company's Google Cloud Account for the Third Time</title><link>https://www.agwa.name/blog/post/google_suspended_sslmates_cloud_account_again</link><description>&lt;doc fingerprint="2f28c8d1158633c5"&gt;
  &lt;main&gt;
    &lt;p&gt;November 3, 2025&lt;/p&gt;
    &lt;head rend="h2"&gt;Google Just Suspended My Company's Google Cloud Account for the Third Time&lt;/head&gt;
    &lt;p&gt;On each of the last two Fridays, Google has suspended SSLMate's Google Cloud access without notification, having previously suspended it in 2024 without notification. But this isn't just another cautionary tale about using Google Cloud Platform; it's also a story about usable security and how Google's capriciousness is forcing me to choose between weakening security or reducing usability.&lt;/p&gt;
    &lt;p&gt;Apart from testing and experimentation, the only reason SSLMate still has a Google Cloud presence is to enable integrations with our customers' Google Cloud accounts so that we can publish certificate validation DNS records and discover domain names to monitor on their behalf. We create a service account for each customer under our Google Cloud project, and ask the customer to authorize this service account to access Cloud DNS and Cloud Domains. When SSLMate needs to access a customer's Google Cloud account, it impersonates the corresponding service account. I developed this system based on a suggestion in Google's own documentation (under "How can I access data from my users' Google Cloud project using Cloud APIs?") and it works really well. It is both very easy for the customer to configure, and secure: there are no long-lived credentials or confused deputy vulnerabilities.&lt;/p&gt;
    &lt;p&gt;Easy and secure: I love it when that's possible!&lt;/p&gt;
    &lt;p&gt;The only problem is that Google keeps suspending our Google Cloud access.&lt;/p&gt;
    &lt;head rend="h4"&gt;The First Suspension&lt;/head&gt;
    &lt;p&gt;Google suspended us for the first time in 2024. Our customer integrations began failing, and logging into the Google Cloud console returned this error:&lt;/p&gt;
    &lt;p&gt;Although Google's customer support people were surprisingly responsive considering Google's rock-bottom reputation in this area, the process to recover our account was super frustrating:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Google required me to email them from the address associated with the account, but when I did so, the message was bounced with the error "The account [redacted] is disabled" (the redacted portion being the email address I sent from). When I emailed from a different address, the message went through, but the support people initially refused to communicate with it because it was the wrong address.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;At one point Google asked me to provide the IDs of our Google Cloud projects - information which I could not retrieve because I couldn't log in to the console. Have you saved your project IDs in a safe place in case your account gets suspended?&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;After several emails back and forth with Google support, and verifying a phone number, I was able to log back into the Google Cloud console, but two of our projects were still suspended, including the one needed for the customer integrations. (At the time, we still had some domains registered through Google Cloud Domains, and thankfully the project for this was accessible, allowing me to begin transferring all of our domains out to a more dependable registrar.)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The day after I regained access to the console, I received an automated email from no-reply@accounts.google.com stating that my access to Google Cloud Platform had been restricted. Once again, I could no longer access the console, but the error message was different this time:&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Twelve hours later, I received multiple automated emails from google-cloud-compliance@google.com stating that my Google Cloud projects had been "reinstated" but I still could not access the console.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Seven hours after that, I got another automated email from no-reply@accounts.google.com stating that my access to Google Cloud Platform had been restored. Everything began working after this.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I was never told why our account was suspended or what could be done to prevent it from happening again. Although Google claims to send emails when an account or project is suspended, they never did so for the initial suspension. Since errors with customer integrations were only being displayed in our customers' SSLMate consoles (usually an error indicates the customer made a mistake), I didn't learn about the suspension right away. I fixed this by adding a health check that fails if a large percentage of Google Cloud integrations have errors.&lt;/p&gt;
    &lt;head rend="h4"&gt;The Second Suspension&lt;/head&gt;
    &lt;p&gt;Two Fridays ago, that health check failed. I immediately investigated and saw that all but one Google Cloud integrations were failing with the same error as during last year's suspension ("Invalid grant: account not found"). Groaning, I tried logging into the Google Cloud console, bracing myself for another Kafkaesque reinstatement process. At least I know the project IDs this time, I reassured myself. Surprisingly, I was able to log in successfully. Then I got emails, one per Google Cloud project, informing me that my projects had been reinstated "based on information that [I] have provided." Naturally, I had received no emails that they had been suspended in the first place. The integrations started working again.&lt;/p&gt;
    &lt;head rend="h4"&gt;The Third Suspension&lt;/head&gt;
    &lt;p&gt;Last Friday, the health check failed again. I logged in to the Google Cloud console, unsure what to expect. This time, I was presented with a third type of error message:&lt;/p&gt;
    &lt;p&gt;Most, but not all, of SSLMate's Google Cloud projects were suspended, including the one needed for customer integrations.&lt;/p&gt;
    &lt;p&gt;I submitted an appeal on Friday. On Sunday, I received an email from Google. Was it a response to the appeal? Nope! It was an automated email stating that SSLMate's access to Google Cloud was now completely suspended.&lt;/p&gt;
    &lt;head rend="h4"&gt;The Lucky Customer&lt;/head&gt;
    &lt;p&gt;Incredibly, we have one lucky customer whose integration has continued to work during every suspension, even though it uses a service account in the same suspended project as all the other customer integrations.&lt;/p&gt;
    &lt;head rend="h4"&gt;What Now?&lt;/head&gt;
    &lt;p&gt;Clearly, I cannot rely on having a Google account for production use cases. Google has built a complex, unreliable system in which some or all of the following can be suspended: an entire Google account, a Google Cloud Platform account, or individual Google Cloud projects.&lt;/p&gt;
    &lt;p&gt;Unfortunately, the alternatives for integrations are not great.&lt;/p&gt;
    &lt;p&gt;The first alternative is to ask customers to create a service account for SSLMate and have SSLMate authenticate to it using a long-lived key. This is pretty easy, but less secure since the long-lived key could leak and can never be rotated in practice.&lt;/p&gt;
    &lt;p&gt;The second alternative is to use OpenID Connect, aka OIDC. In recent years, OIDC has become the de facto standard for integrations between service providers. For example, you can use OIDC to let GitHub Actions access your Google Cloud account without the need for long-lived credentials. SSLMate's Azure integration uses OIDC and it works well.&lt;/p&gt;
    &lt;p&gt;Unfortunately, Google has made setting up OIDC unnecessarily difficult. What is currently a simple one step process for our customers to add an integration (assign some roles to a service account) would become a complicated seven step process:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Enable the IAM Service Account Credentials API.&lt;/item&gt;
      &lt;item&gt;Create a service account.&lt;/item&gt;
      &lt;item&gt;Create a workload identity pool.&lt;/item&gt;
      &lt;item&gt;Create a workload identity provider in the pool created in step 3.&lt;/item&gt;
      &lt;item&gt;Allow SSLMate to impersonate the service account created in step 2 (this requires knowing the ID of the pool created in step 3).&lt;/item&gt;
      &lt;item&gt;Assign roles to the service account created in step 2.&lt;/item&gt;
      &lt;item&gt;Provide SSLMate with the ID of the service account created in step 2, and the ID of the workload identity provider created in step 4.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Since many of the steps require knowing the identifiers of resources created in previous steps, it's hard for SSLMate to provide easy-to-follow instructions.&lt;/p&gt;
    &lt;p&gt;This is more complicated than it needs to be:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Creating a service account (steps 1, 2, and 5) should not be necessary. While it is possible to forgo a service account and assign roles directly to an identity from the pool, not all Google Cloud services support this. If you want your integration to work with all current and future services, you have to impersonate a service account. Google should stop treating OIDC like a second-class citizen and guarantee that all current and future services will directly support it.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Creating an identity pool shouldn't be necessary either. While I'm sure some use cases are nicely served by pools, it seems like most setups are going to have just one provider per pool, making the extra step of creating a pool nothing but unnecessary busy work.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Even creating a provider shouldn't be necessary; it should be possible to assign roles directly to an OIDC issuer URL and subject. You should only have to create a provider if you need to do more advanced configuration, such as mapping attributes.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I find this state of affairs unacceptable, because it's really, really important to move away from long-lived credentials and Google ought to be doing everything possible to encourage more secure alternatives. Sadly, SSLMate's current solution of provider-created service accounts is susceptible to arbitrary account suspensions, and OIDC is hampered by an unnecessarily complicated setup process.&lt;/p&gt;
    &lt;p&gt;In summary, when setting up cross-provider access with Google Cloud, you can have only two of the following:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;No dangerous long-lived credentials.&lt;/item&gt;
      &lt;item&gt;Easy for the customer to set up.&lt;/item&gt;
      &lt;item&gt;Safe from arbitrary account suspensions.&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Provider-created service accounts&lt;/cell&gt;
        &lt;cell role="head"&gt;Service account + key&lt;/cell&gt;
        &lt;cell role="head"&gt;OpenID Connect&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;No long-lived keys&lt;/cell&gt;
        &lt;cell&gt;No long-lived keys&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Easy setup&lt;/cell&gt;
        &lt;cell&gt;Easy setup&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Safe from suspension&lt;/cell&gt;
        &lt;cell&gt;Safe from suspension&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Which two would you pick?&lt;/p&gt;
    &lt;head rend="h3"&gt;Comments&lt;/head&gt;
    &lt;p&gt;No comments yet.&lt;/p&gt;
    &lt;head rend="h3"&gt;Post a Comment&lt;/head&gt;
    &lt;p&gt;Your comment will be public. To contact me privately, email me. Please keep your comment polite, on-topic, and comprehensible. Your comment may be held for moderation before being published.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45798827</guid><pubDate>Mon, 03 Nov 2025 13:39:18 +0000</pubDate></item><item><title>VimGraph</title><link>https://resources.wolframcloud.com/FunctionRepository/resources/VimGraph/</link><description>&lt;doc fingerprint="797c6f35ce5cddfb"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Wolfram Function Repository&lt;/head&gt;
    &lt;p&gt;Instant-use add-on functions for the Wolfram Language&lt;/p&gt;
    &lt;p&gt;Function Repository Resource:&lt;/p&gt;
    &lt;p&gt;Construct a graph of simple Vim-style movements in text&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;ResourceFunction["VimGraph"][text]&lt;/p&gt;
          &lt;p&gt;returns a graph with letters as vertices and Vim-style movements as edges.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Shortcut&lt;/cell&gt;
        &lt;cell&gt;Movement Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;h / l&lt;/cell&gt;
        &lt;cell&gt;Move one character left / right on the same line&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;k / j&lt;/cell&gt;
        &lt;cell&gt;Move one character up / down; jumps to end of target line if shorter than current horizontal position&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;w / b&lt;/cell&gt;
        &lt;cell&gt;Jump to the beginning of the next / previous word, across lines&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;e&lt;/cell&gt;
        &lt;cell&gt;Jump to the end of the next word, across lines&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;^/$&lt;/cell&gt;
        &lt;cell&gt;Move to the beginning/end of the current line&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Vim graph for the movements: up, right, and to the beginning of the next word, respectively:&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;In[1]:=&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;Out[1]=&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The same, with nicer formatting:&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;In[2]:=&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;Out[2]=&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Returns a minimal sequence of keystrokes needed to move from one letter to another:&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;In[3]:=&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;Out[3]=&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Illustrates the relationship between the maximum keystroke distance required to navigate between two letters in a text and the number of randomly inserted newlines:&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;In[4]:=&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;Out[4]=&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Use the "CustomPatterns" option to define new movements by passing a string pattern to "StringPattern", with optional shortcuts for jumping forward or backward to the nearest match:&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;In[5]:=&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;Out[5]=&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Wolfram Language 13.0 (December 2021) or above&lt;/p&gt;
    &lt;p&gt;This work is licensed under a Creative Commons Attribution 4.0 International License&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45798838</guid><pubDate>Mon, 03 Nov 2025 13:40:47 +0000</pubDate></item><item><title>Tech workers' fight for living wages and a 32-hour workweek is a battle for all</title><link>https://thechiefleader.com/stories/tech-workers-fight-for-living-wages-and-a-32-hour-workweek-is-a-battle-for-workers-everywhere,55298</link><description>&lt;doc fingerprint="b904746361ccaff7"&gt;
  &lt;main&gt;
    &lt;p&gt;A few of our stories and columns are now in front of the paywall. We at The Chief-Leader remain committed to independent reporting on labor and civil service. It's been our mission since 1897. You can have a hand in ensuring that our reporting remains relevant in the decades to come. Consider supporting The Chief, which you can do for as little as $3.20 a month.&lt;/p&gt;
    &lt;p&gt;Joan Wright is a rank-and-file member of Kickstarter United and a member of her union’s contract action team.&lt;/p&gt;
    &lt;p&gt;Militant labor action and strong picket lines that shut down business profits have always been the most powerful tool working people have to win demands in workplaces and communities. As the gap between the working class and the rich in the U.S. grows wider every year, working people must take bold action against the attacks on labor and living standards. We must rebuild a fighting labor movement willing to fight for bold demands, such as living wages and a 32-hour work week — and demonstrate that we can fight and win.&lt;/p&gt;
    &lt;p&gt;My union, Kickstarter United, part of the Office and Professional Employees International Union (OPEIU) Local 153, is the union of workers at Kickstarter and the first-ever union in the American tech industry. On Oct. 2, we went on strike over our demands for a 32-hour work week with no loss of pay and livable wages for all workers.&lt;/p&gt;
    &lt;p&gt;Kickstarter is a crowdfunding platform where large and small businesses and independent creators raise money from customers who back their projects. The company makes profits by taking a 5-percent cut of all money pledged. Our union includes 59 workers like me who provide a range of services to creators and customers, including customer support, marketing, outreach, research, trust and safety and software development for Kickstarter’s platform.&lt;/p&gt;
    &lt;p&gt;Kickstarter’s founders and its current executives pride themselves on being a “public benefit corporation" to “harness the power of private enterprise to create public benefit.” They also decided to become a “B corporation,” a designation that purports that the business exists “to benefit all people, communities, and the planet.”&lt;/p&gt;
    &lt;p&gt;Yet none of these designations and labels change the fact that Kickstarter is fundamentally like any other corporation under capitalism. The company makes profits for its wealthy major shareholders on the backs of the labor of its workers. Profits for the bosses of any corporation (both the top executives and the biggest shareholders) originate from paying workers a fraction of the wealth they generate from their labor. Kickstarter revenue is in the tens of millions, while some of its fewer than 100 workers struggle to get by, living paycheck to paycheck.&lt;/p&gt;
    &lt;p&gt;The major shareholders and investors of Kickstarter include massive venture capital funds and tech billionaires and multi-millionaires. Most recently, Kickstarter received $100 million in funding from the venture capital fund Andreessen Horowitz, founded by Mark Andreessen and Ben Horowitz, tech investors who have donated millions of dollars to Trump-aligned political action committees.&lt;/p&gt;
    &lt;p&gt;These wealthy interests are now bankrolling the same union-busting playbook as any other corporation against us. To break our strike, Kickstarter management hired the notorious union-busting law firm Littler Mendelson, the same firm used by Amazon and Starbucks. In our initial union drive in 2020, Kickstarter management launched a brutal union-busting campaign and fired union organizers. Now, instead of meeting our demands for living wages, the CEO has announced a new round of jobs hirings internationally, outside of the union, in a blatant attempt to undermine our strike and the union altogether. Externally, the company touts the victories that our union has won as job benefits, and then internally tells us that they can’t pay workers a living wage.&lt;/p&gt;
    &lt;p&gt;Workers create all the wealth at Kickstarter, just as workers create all wealth in society. There is more than enough wealth for every worker to have a living wage, even for a 32-hour week. Auto workers at Ford, General Motors and Stellantis, who are unionized with the United Auto Workers, demanded a 32-hour-work-week for 40 hours’ pay as part of their historic 2023 strike. This was the union’s boldest demand, and while the auto workers didn’t get that win among their other substantial victories, it was groundbreaking for UAW to have included the demand to help set the sights higher in the entire labor movement. We work to live, we don't live to work!&lt;/p&gt;
    &lt;p&gt;The labor movement formally won the national eight-hour day and the 40-hour workweek in 1938. But the demands for those standards were initiated decades earlier through a militant fight, including mass strike action led by socialists. Incensed by this working-class victory, the capitalists unleashed violence and repression on the labor movement, resulting in workers being beaten, arrested and tortured, with some even losing their lives.&lt;/p&gt;
    &lt;p&gt;The labor movement needed to become militant to counter this assault by the bosses, and hundreds of thousands of rank-and-file workers played a courageous role.&lt;/p&gt;
    &lt;p&gt;Now we need to build a similar fighting movement for a 32-hour week and living wages for all workers. The bosses will fight tooth and nail to prevent workers from securing those victories anywhere. The history of the labor movement shows that, in fact, the bosses will never relent in even minor skirmishes, because they understand that working-class victories of any magnitude raise the morale of workers to win even more. They know that when we, the workers, stand together in solidarity, we can win.&lt;lb/&gt;Which is why we are calling for broad working-class solidarity in our fight. Anyone can support our strike by signing our petition, donating to our solidarity fund, or joining us on our virtual picket line. We have held online and in-person strike rallies in Seattle and New York. &lt;/p&gt;
    &lt;p&gt;Winning our demands and building these into larger mobilizations and mass rallies will require a fighting movement. We urge all tech workers, rank-and-file union members and union leadership across the entire labor movement to join us in building the movement for a 32-hour week and living wages for all workers. If we can win our demands, it will be an unprecedented victory — one that will generate forward momentum for workers everywhere.&lt;/p&gt;
    &lt;p&gt;Kickstarter bosses and tech industry owners will continue to do anything to break our strike. They know that if we win any of our demands, workers across the tech industry and further afield will get organized and fight for more, beyond even our workplaces. Tech workers have been organizing against the genocide in Gaza, such as with No Tech for Apartheid and in our union local. We should be crystal clear: our fight is an uphill one. Workers in the tech industry are getting hit with layoffs and benefit cuts, despite tech billionaires and multimillionaires making record profits. And this is also precisely why tech workers have to stand up and fight back.&lt;/p&gt;
    &lt;p&gt;No comments on this item Please log in to comment by clicking here&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45798973</guid><pubDate>Mon, 03 Nov 2025 13:56:11 +0000</pubDate></item></channel></rss>