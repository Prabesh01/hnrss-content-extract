<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Fri, 12 Sep 2025 18:12:42 +0000</lastBuildDate><item><title>Astrophysics Source Code Library</title><link>http://ascl.net/</link><description>&lt;doc fingerprint="eeb0eafba791d14b"&gt;
  &lt;main&gt;
    &lt;p&gt;The Astrophysics Source Code Library (ASCL) is a free online registry and repository for source codes of interest to astronomers and astrophysicists, including solar system astronomers, and lists codes that have been used in research that has appeared in, or been submitted to, peer-reviewed publications. The ASCL is indexed by the SAO/NASA Astrophysics Data System (ADS) and Web of Science and is citable by using the unique ascl ID assigned to each code. The ascl ID can be used to link to the code entry by prefacing the number with ascl.net (i.e., ascl.net/1201.001).&lt;/p&gt;
    &lt;p&gt;adstex automatically identifies all citation keys in a TeX source file and builds the corresponding bibliography file (.bib file) by fetching the reference information from NASA's Astrophysics Data System (ADS). adstex recognizes all variants of the cite commands in TeX, and works with various styles of citation keys, including arXiv IDs, DOIs, and ADS bibcodes. When a citation key is in the format of first-author name and year, adstex will query NASA's ADS and return a list of possible reference matches for the user to select the intended one. When a reference entry has updated information on NASA's ADS, adstex can detect such changes and fetch the new information and update the user's bibliography file. adstex supports any reference entry that is available on NASA's ADS, and allows the authors to write papers without manually searching for the bibliography entries.&lt;/p&gt;
    &lt;p&gt;IAR_Model fits unequally spaced time series from the Irregular Autoregressive (IAR). Available as Python and R functions, IAR_Model can generate observations for each process, compute the negative of the log likelihood of these process, fit each model to irregularly sampled data, and test the significance of the estimate.&lt;/p&gt;
    &lt;p&gt;fm4ar (flow matching for atmospheric retrievals) infers atmospheric properties of exoplanets from observed spectra. It uses flow matching posterior estimation (FMPE) for its machine learning (ML) approach to atmospheric retrieval; this approach provides many of the advantages of neural posterior estimation (NPE) while also providing greater architectural flexibility and scalability. The package uses importance sampling (IS) to verify and correct ML results, and to compute an estimate of the Bayesian evidence. fm4ar's ML models are conditioned on the assumed noise level of a spectrum (i.e., error bars), thus making them adaptable to different noise models.&lt;/p&gt;
    &lt;p&gt;AGNI simulates the atmospheric temperature-, height-, and compositional-structures of atmospheres overlying magma oceans while ensuring that radiative-convective equilibrium is maintained throughout the atmosphere. The code also supports real gas equations of state, self-gravitation, and various spectral surface compositions. Accounting for these energy transport processes permits AGNI to calculate atmospheric structure, which also yields realistic cooling rates for young rocky planets with magma oceans.&lt;/p&gt;
    &lt;p&gt;FiCUS (FItting the stellar Continuum of Uv Spectra) fit the stellar continuum of extragalactic ultraviolet (UV) spectra. The code takes observed-frame wavelength, flux density (with errors) and user-defined mask arrays as inputs, and returns an estimation of the galaxy stellar age, metallicity and dust extinction, as well as other secondary Spectral Energy Distribution (SED) parameters. FiCUS has two scripts; the first reads the INPUT file provided by the user and performs the fit according to selected options. It then gives the best-fit parameters and creates the OUTPUT files and figures. The second script includes pre-defined routines for spectral analysis, loading INPUT files and handling with data and models, as well as functions for the fitting routine, SED-parameters calculations and plotting, and imports functions into the first script.&lt;/p&gt;
    &lt;p&gt;pyStarburst99 is a Python version of the Starburst99 (ascl:1104.003) population synthesis code for star-forming galaxies. This Python version includes new evolutionary tracks and synthetic spectral energy distributions. pyStarburst99 provides wider coverage in metallicity, mass, and resolution, and includes evolutionary and spectral models of stars up to 300–500 M⊙.&lt;/p&gt;
    &lt;p&gt;The SIGWAY data analysis pipeline computes second-order, scalar induced gravitational wave signals emitted by curvature perturbations in the early universe. The package solves the Mukhanov-Sasaki equation for single field ultra-slow roll inflationary models and computes the primordial scalar power spectrum Pζ. SIGWAY also computes the second order gravitational wave power spectrum ΩGW from P ζ for reentry during radiation domination or a phase of early matter domination.&lt;/p&gt;
    &lt;p&gt;sMV (serial MultiView) scripts provide a semi-automatic and easy-to-use workflow for serial MultiView phase plane estimation. The phase plane is iteratively rotated based on the time series of calibrator residual phases; because time-domain information is included in the iterations, phase ambiguities are accurately and automatically identified. sMV enables efficient, high-accuracy differential astrometry and artifact-reduced imaging for astrophysical studies.&lt;/p&gt;
    &lt;p&gt;Built on Flax (ascl:2504.026), DeepSSM emulates gravitational wave (GW) spectra produced by sound waves during cosmological first-order phase transitions in the radiation-dominated era. It uses neural networks trained on an enhanced version of the Sound Shell Model (SSM). The code provides instantaneous predictions of GW spectra given the phase transition parameters, while achieving agreement with the enhanced SSM model. DeepSSM is particularly suitable for direct Bayesian inference on phase transition parameters without relying on empirical templates, such as broken power-law models.&lt;/p&gt;
    &lt;p&gt;The flux transport model HipFT implements advection, diffusion, and data assimilation on the solar surface on a logically rectangular nonuniform spherical grid. It is parallelized for use with multi-core CPUs and GPUs using a combination of Fortran's standard parallel do concurrent (DC), OpenMP Target data directives, and MPI. Serving as the computational core of the Open-source Flux Transport (OFT) software suite (ascl:2508.013), HipFT incorporates various differential rotation, meridional flow, super granular convective flow, and data assimilation models. HipRT also computes multiple realizations in a single run spanning multiple choices of parameters.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45220843</guid></item><item><title>The treasury is expanding the Patriot Act to attack Bitcoin self custody</title><link>https://www.tftc.io/treasury-iexpanding-patriot-act/</link><description>&lt;doc fingerprint="365d59be7c351fbd"&gt;
  &lt;main&gt;
    &lt;p&gt;We shouldn't have to cater to the lowest common denominator.&lt;/p&gt;
    &lt;p&gt;We warned a couple of months ago when the Trump administration's "Crypto Brief" was released that there was some language in the brief that advised the government to expand the Patriot Act to account for digital assets. Well, it looks like FinCen and the Treasury have been working on guidelines and a rough outline is shared above courtesy of The Rage, and they are absolutely horrid.&lt;/p&gt;
    &lt;p&gt;It seems that FinCen and the Treasury are preparing to outlaw the use of CoinJoin, atomic swaps, single address use, and transaction broadcast timing delays. All of which are common best use practices that I would recommend any bitcoiner leveraging self-custody practice. This is an all out attack on financial privacy within bitcoin. If enacted, any user who leverages these tools will be flagged as a suspicious, any attempts to send a UTXO that has touched any of these tools will be rejected by regulated services, and could potentially be sent to prison.&lt;/p&gt;
    &lt;p&gt;This is an absurd affront to common sensibilities and freedom in the digital age. The fact that they want to prevent people from using single addresses for individual UTXOs is patently absurd. Not only is it a massive infringement on privacy, but it makes bitcoin usage less economically efficient and degrades the security of every bitcoiner. Loading up a single address with too many UTXOs degrades the entropy of a public-private key pair and makes it easier to brute force a user's private key.&lt;/p&gt;
    &lt;p&gt;Instead of expanding the Patriot Act, it should be abolished. Instead of trying to eliminate financial privacy for the 99.9% of law abiding citizens in this country, the government should be actively trying to foster an environment in which it can be improved. The proposed solutions will do nothing but put good Americans in harm's way and degrade the security of their savings.&lt;/p&gt;
    &lt;p&gt;We shouldn't have to live in a world where standards cater to the lowest common denominator, in this case criminals, and make things worse off for the overwhelming majority of the population. It's crazy that this even has to be said. The onus is on law enforcement to be so good at their jobs that they are able to prevent crimes from happening before they occur and effectively bring criminals to heel after they commit crimes. It shouldn't be on a neutral protocol and the industry being built on top of it that, when used effectively, provides people with a stable monetary system that respects user privacy and equips them with the tools to receive and spend in a way that provides them with peace of mind.&lt;/p&gt;
    &lt;p&gt;Why should everyone have to suffer because of a few bad apples? Isn't that letting the terrorist win?&lt;/p&gt;
    &lt;p&gt;Mel Mattison revealed a fascinating shift in Bitcoin's market dynamics that challenges conventional crypto wisdom. He pointed out that Bitcoin futures now exhibit lower volatility than platinum futures - a remarkable transformation for an asset once synonymous with wild price swings. The proliferation of ETFs, options, futures, and other traditional financial instruments has fundamentally altered Bitcoin's behavior, creating what Mel calls "volatility suppression." This institutionalization comes with trade-offs: while reducing dramatic downswings, it also caps explosive upside potential.&lt;/p&gt;
    &lt;quote&gt;"Bitcoin is becoming a TradFi security instrument and it's getting TradFi vol." - Mel Mattison&lt;/quote&gt;
    &lt;p&gt;Mel argued that the relationship between volatility and returns means investors must recalibrate expectations. Where 100% annual gains once seemed routine, he now considers 50% returns "massive" for this new era of Bitcoin. This maturation reflects Bitcoin's evolution from speculative experiment to financial infrastructure - less exciting perhaps, but ultimately more sustainable for long-term adoption.&lt;/p&gt;
    &lt;p&gt;Check out the full podcast here for more on China's gold strategy, Fed independence battles, and housing market manipulation plans.&lt;/p&gt;
    &lt;p&gt;New Bill for Strategic Bitcoin Reserve - via X&lt;/p&gt;
    &lt;p&gt;SEC to Host Crypto Roundtable October 17 - via X&lt;/p&gt;
    &lt;p&gt;Research Proposes Bitcoin for Mars Trade Standard - via X&lt;/p&gt;
    &lt;p&gt;Tom Honzik has helped 1,000+ people secure more than 5,000 BTC. Now, TFTC and Unchained are teaming up for a live online session on bitcoin custody.What you’ll learn:&lt;/p&gt;
    &lt;p&gt;Stick around for the AMA to ask Tom Honzik and Marty Bent anything—from privacy considerations to the tradeoffs of different multisig quorums.&lt;/p&gt;
    &lt;p&gt;Created by Carl Dong (former Bitcoin Core contributor), unlike other VPNs, it can’t log your activity by design, delivering verifiable privacy you can trust.&lt;/p&gt;
    &lt;p&gt;Outsmarts internet censorship: works even on the most restrictive Wi-Fi networks where other VPNs fail.&lt;lb/&gt;Pay with bitcoin over Lightning: better privacy and low fees.&lt;lb/&gt;No email required: accounts are generated like bitcoin wallets.&lt;lb/&gt;No trade-offs: browse freely with fast, reliable speeds.&lt;/p&gt;
    &lt;p&gt;Exclusive Deal for TFTC Listeners:&lt;lb/&gt;Sign up at obscura.net and use code TFTC25 for 25% off your first 12 months.&lt;/p&gt;
    &lt;p&gt;Now available on macOS, iOS, and WireGuard, with more platforms coming soon — so your privacy travels with you wherever you go.&lt;/p&gt;
    &lt;p&gt;Ten31, the largest bitcoin-focused investor, has deployed $200M across 30+ companies through three funds. I am a Managing Partner at Ten31 and am very proud of the work we are doing. Learn more at ten31.vc/invest.&lt;/p&gt;
    &lt;p&gt;Final thought...&lt;/p&gt;
    &lt;p&gt;Rest in peace, Charlie Kirk. Pray for humanity and for peace.&lt;/p&gt;
    &lt;p&gt;Download our free browser extension, Opportunity Cost: https://www.opportunitycost.app/ start thinking in SATS today.&lt;/p&gt;
    &lt;p&gt;Get this newsletter sent to your inbox daily: https://www.tftc.io/bitcoin-brief/&lt;/p&gt;
    &lt;p&gt;Subscribe to our YouTube channels and follow us on Nostr and X:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45221274</guid></item><item><title>Chat Control faces blocking minority in the EU</title><link>https://twitter.com/TutaPrivacy/status/1966384776883142661</link><description>&lt;doc fingerprint="d635f48b34542867"&gt;
  &lt;main&gt;
    &lt;p&gt;We’ve detected that JavaScript is disabled in this browser. Please enable JavaScript or switch to a supported browser to continue using x.com. You can see a list of supported browsers in our Help Center.&lt;/p&gt;
    &lt;p&gt;Help Center&lt;/p&gt;
    &lt;p&gt;Terms of Service Privacy Policy Cookie Policy Imprint Ads info © 2025 X Corp.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45221580</guid></item><item><title>Over 100 ships have sailed with fake insurance from the Norwegian Ro Marine</title><link>https://www.nrk.no/vestland/xl/over-100-ships-have-sailed-without-legitimate-insurance-from-the-norwegian-company-ro-marine-1.17565216</link><description>&lt;doc fingerprint="a25c819ab4077144"&gt;
  &lt;main&gt;
    &lt;p&gt;This ship is transporting Russian missiles.&lt;/p&gt;
    &lt;p&gt;The ship purchased fake insurance from the Norwegian company Ro Marine.&lt;/p&gt;
    &lt;p&gt;The company acted in Putin’s interests.&lt;/p&gt;
    &lt;p&gt;The scale shocks experts.&lt;/p&gt;
    &lt;head rend="h1"&gt;He made a fool of the world&lt;/head&gt;
    &lt;head rend="h1"&gt;Over 100 ships have sailed without legitimate insurance from the Norwegian company Ro Marine&lt;/head&gt;
    &lt;p&gt;All large ships must have insurance, and Ro Marine has provided this in a big way.&lt;/p&gt;
    &lt;p&gt;The Norwegian company did not have permission to sell insurance but did it anyway — to clients worldwide.&lt;/p&gt;
    &lt;p&gt;In March, we reported how select ships used false papers to deceive inspectors in NATO countries.&lt;/p&gt;
    &lt;p&gt;Now, NRK and Dossier Center can reveal how extensive and global the fraud was: over 100 ships have sailed with illegitimate insurance documents from Ro Marine.&lt;/p&gt;
    &lt;p&gt;“It's very serious and unusual that such a serious fraud happens with the help of a Norwegian company. At worst, it could undermine the trust in the Norwegian maritime industry,” says Thomas Angell Bergh from the Norwegian Maritime Authority.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;head rend="h4"&gt;Thomas Angell Bergh&lt;/head&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Most of the fake insurance papers were for ships transporting goods out of Russia, mainly oil.&lt;/p&gt;
    &lt;p&gt;NRK has contacted dozens of Ro Marine’s clients. Only a few were willing to speak with us.&lt;/p&gt;
    &lt;p&gt;One of the customers says he was scammed by Ro Marine because he believed the insurance policies he purchased were valid.&lt;/p&gt;
    &lt;p&gt;Another says that "everyone" knows Ro Marine is fake, but that the ships need Western insurance documents. Such documents can make it easier to sail freely, without Western countries interfering in the transport.&lt;/p&gt;
    &lt;p&gt;This signature appears time and again on documents issued by Ro Marine.&lt;/p&gt;
    &lt;p&gt;The search for the owner of the signature led us to a Russian website, where you can download many different signatures. The one used by Ro Marine belongs to a "doctor," says the website.&lt;/p&gt;
    &lt;head rend="h2"&gt;Norwegian-Russian scam&lt;/head&gt;
    &lt;p&gt;Behind the global hoax is this man.&lt;/p&gt;
    &lt;p&gt;His name is Andrey Mochalin, a Russian citizen and resident of St. Petersburg. Mochalin has experience working for a reputable Norwegian insurance company.&lt;/p&gt;
    &lt;p&gt;In March, he, two Norwegians and a Bulgarian were charged with forging documents and operating an insurance business without a permit. Mochalin is also being investigated for violating international sanctions.&lt;/p&gt;
    &lt;p&gt;Through an attorney, the Norwegians say they do not understand the charges (see their full response in the fact box later in the article). The Bulgarian tells NRK he is innocent.&lt;/p&gt;
    &lt;p&gt;For several months, NRK and Dossier Center have tried unsuccessfully to get in touch with Mochalin. At the same time, we investigated Ro Marine's operations. What we found surprised experts.&lt;/p&gt;
    &lt;head rend="h2"&gt;“Among the worst of the worst”&lt;/head&gt;
    &lt;p&gt;Sanctions expert David Tannenbaum is shocked by the the scale of it. He knows how far Russia is willing to go to protect its oil exports, which are crucial for funding Putin’s illegal war in Ukraine.&lt;/p&gt;
    &lt;p&gt;Sanctions against Russia can make it difficult for tankers carrying Russian oil to obtain insurance that is approved in the West. But they are finding ways around it.&lt;/p&gt;
    &lt;p&gt;This is where Ro Marine enters the picture.&lt;/p&gt;
    &lt;p&gt;“Seems like Ro Marine is popular with sanctions evaders. You don't have this roster by accident,” says David Tannenbaum from Deep Blue Intelligence. The American company specializes in detecting sanctions evasion.&lt;/p&gt;
    &lt;p&gt;For Tannenbaum, it appears that Ro Marine primarily serves the shadow fleet or ships engaged in illegal activities or sanctions evasion.&lt;/p&gt;
    &lt;p&gt;“Is Ro Marine the worst of the worst? I think they're definitely in contention,” he both asks and answers.&lt;/p&gt;
    &lt;p&gt;Our documentation shows, for example, that Gatik, known as one of the largest players in the Russian shadow fleet, appears to have placed almost all of their ships with Ro Marine.&lt;/p&gt;
    &lt;p&gt;In addition, six ships linked to the Russian gas giant Novatek have had fake insurance from the Norwegian company. All six have sailed along the Norwegian coast towards the gas facility Arctic LNG2 in Russia, which is sanctioned by the USA. The ships are sanctioned by the EU.&lt;/p&gt;
    &lt;p&gt;Ships linked to the sanctioned Iranian oil industry and Iranian military have also been customers of Ro Marine.&lt;/p&gt;
    &lt;head rend="h2"&gt;Dangerous cargo from Russia&lt;/head&gt;
    &lt;p&gt;Among the cargo ships that have purchased invalid insurance from Ro Marine, we found "Agattu". Here, the vessel is sailing between Denmark and Sweden with explosives in the cargo, bound for Algeria.&lt;/p&gt;
    &lt;p&gt;Three tonnes of missile weapons were transported from St. Petersburg in Russia, according to Russian port records.&lt;/p&gt;
    &lt;p&gt;The ship joins the ranks of Ro Marine customers who have contributed to Russia's export revenues by transporting goods from Russian ports. This does not apply to all customers, but the vast majority, according to research by NRK and Dossier Center.&lt;/p&gt;
    &lt;head rend="h2"&gt;Provoked a NATO country&lt;/head&gt;
    &lt;p&gt;Not long ago, it was difficult to imagine that ordinary shipping in Europe could lead to military confrontation. Today, the situation is different. European countries may intervene in oil shipments that violate Western sanctions, which are intended to oppose Putin's bloody war in Ukraine. Russia has its countermeasures.&lt;/p&gt;
    &lt;p&gt;An illustrative example happened to a Ro Marine customer in mid-May: The oil tanker "Blint".&lt;/p&gt;
    &lt;p&gt;The Estonian navy suspected the ship was sailing without a flag—a clear violation of international regulations. The navy radioed the vessel, but according to Estonian authorities, the captain refused to cooperate.&lt;/p&gt;
    &lt;p&gt;Suddenly, a Russian fighter jet came whizzing over them, violating the NATO country’s airspace. Instead of stopping, the sanctioned tanker sailed on to the Russian oil port of Primorsk.&lt;/p&gt;
    &lt;p&gt;This video of the incident is filmed from inside the ship.&lt;/p&gt;
    &lt;p&gt;Like many other tankers transporting Russian oil, "Blint" has had fake insurance from Ro Marine, NRK can document.&lt;/p&gt;
    &lt;p&gt;In this way, Ro Marine has acted in line with the interests of the Russian authorities.&lt;/p&gt;
    &lt;p&gt;Russia's president, October 2023:&lt;/p&gt;
    &lt;p&gt;“Thanks to the actions of companies and authorities, the tanker fleet has grown, new mechanisms for payment, insurance and reinsurance of our cargo have been created.”&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;head rend="h4"&gt;Vladimir Putin&lt;/head&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Researcher Åse Gilje Østensen at the Norwegian Naval Academy tells that “sometimes, entities act in the Russian interest on their own initiative. Sometimes, Putin or other central figures around him have signaled that certain initiatives are welcome. In such cases, actors will often seek to please the regime.”&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;head rend="h4"&gt;Åse Gilje Østensen&lt;/head&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;“Russia is an authoritarian regime that can force civilian actors to assist the regime. Other times, state bodies may be more directly involved. What is the case regarding Ro Marine is difficult to know.”&lt;lb/&gt; The Russian Embassy in Norway does not answer NRK's questions about Ro Marine's operations because the company is Norwegian and refers us to the Norwegian authorities. They also do not respond to whether Ro Marine has acted in accordance with the interests of Russian authorities, or any other statements and findings in this case.&lt;/p&gt;
    &lt;p&gt;The embassy does however choose to point out that the sanctions against the "shadow fleet" are contrary to international law.&lt;/p&gt;
    &lt;head rend="h2"&gt;Approved by the largest flag state&lt;/head&gt;
    &lt;p&gt;For several years, the Norwegian company operated without permission and with fake documents without any authorities noticing — neither in Norway nor abroad.&lt;/p&gt;
    &lt;p&gt;The earliest objectionable activity was in 2021, according to NRK’s investigation.&lt;/p&gt;
    &lt;p&gt;At that time, Ro Marine applied to be recognized as an insurance company by the world's largest flag state, Panama, despite missing the necessary approval from Norwegian authorities.&lt;/p&gt;
    &lt;p&gt;Ro Marine sent the flag state a forged reference, which was originally given to a completely different company, sources tell NRK. With this reference, Ro Marine was recognized by the flag state of Panama in December 2021.&lt;/p&gt;
    &lt;p&gt;In the time that followed, several flag states were fooled by Ro Marine, including by forged documents that looked like they were from the Norwegian Financial Supervisory Authority.&lt;/p&gt;
    &lt;p&gt;The illegal activity continued unencumbered until NRK alerted the flag states.&lt;/p&gt;
    &lt;head rend="h2"&gt;Russian owner worked many years for a Norwegian company&lt;/head&gt;
    &lt;p&gt;The Russian owner of Ro Marine, Andrey Mochalin, has gone underground. Mochalin has not responded to any of the numerous inquiries from NRK and Dossier Center.&lt;/p&gt;
    &lt;p&gt;For over ten years, he worked for a legitimate Norwegian insurance company. Most of the time, he worked from St. Petersburg.&lt;/p&gt;
    &lt;p&gt;Occasionally, he visited his employer's office in Oslo. Here he is pictured with his former colleagues.&lt;/p&gt;
    &lt;p&gt;At this time, two of his Norwegian managers also owned another company that offered insurance. This company later became Ro Marine.&lt;/p&gt;
    &lt;p&gt;A few weeks after Russia's war against Ukraine began in 2022, Ro Marine passed from Norwegian to Russian ownership.&lt;/p&gt;
    &lt;p&gt;This is when Mochalin bought Ro Marine from the company of the two Norwegians for almost two million NOK.&lt;/p&gt;
    &lt;p&gt;These two Norwegians, along with Andrey Mochalin and a Bulgarian citizen, are charged with forging documents and conducting illegal insurance business.&lt;/p&gt;
    &lt;p&gt;Out of respect for the ongoing investigation, the Norwegians did not want to be interviewed by NRK, according to their lawyer.&lt;/p&gt;
    &lt;head rend="h2"&gt;Money trail in Russia&lt;/head&gt;
    &lt;p&gt;Alongside the Norwegian company Ro Marine, Andrey Mochalin runs a company in St. Petersburg with direct links to Ro Marine.&lt;/p&gt;
    &lt;p&gt;NRK and Dossier Center have obtained access to bank documents for his Russian company.&lt;/p&gt;
    &lt;p&gt;The money transfers are many, and some stand out.&lt;/p&gt;
    &lt;p&gt;Last year, there were 36 payments totaling approximately five million NOK that we can connect to Ro Marine.&lt;/p&gt;
    &lt;p&gt;The bank transfers were marked with the name of the ship and the policy number.&lt;/p&gt;
    &lt;p&gt;The number corresponds to insurance documents issued by Ro Marine.&lt;/p&gt;
    &lt;p&gt;The company’s account also shows salary payments to Mochalin.&lt;/p&gt;
    &lt;head rend="h2"&gt;Those charged&lt;/head&gt;
    &lt;p&gt;Here’s what we can share about the Russian Andrey Mochalin and the others charged in the case (in parentheses is the time period they had official roles in Ro Marine):&lt;/p&gt;
    &lt;p&gt;A. Mochalin (2022-2025):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Sole owner of Ro Marine during the period when the company's illegal insurance activities were most widespread, according to our documentation.&lt;/item&gt;
      &lt;item&gt;Majority owner of the Russian company that received payments worth millions of NOK last year marked for Ro Marine.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Norwegian 1 (2016-2023):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Registered as co-owner in the Russian company that last year received payments worth millions of NOK marked for Ro Marine.&lt;/item&gt;
      &lt;item&gt;Owner and board chairman in 2021 when someone sent a forged reference on behalf of Ro Marine to Panama. At the time the company lacked a permit to sell insurance.&lt;/item&gt;
      &lt;item&gt;In 2024, a year after he left the board, contributed to ensuring Ro Marine’s continued operations by securing a new board member: the charged Bulgarian.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Norwegian 2 (2017-2023):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Owner and managing director in 2021 when someone sent a forged reference on behalf of Ro Marine to Panama. At the time the company lacked a permit to sell insurance.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Bulgarian (2024-2025):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Car mechanic without any experience in marine insurance.&lt;/item&gt;
      &lt;item&gt;Tenant of Norwegian 1 for years.&lt;/item&gt;
      &lt;item&gt;Board member in Ro Marine for over half a year, up to March 2025.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;After the Norwegians left the board in 2023, the Norwegian company had a problem. With a Russian as the only board member, the company was in violation of the Companies Act of Norway, which requires at least one board member to be from an EEA country. Since Bulgaria is a member of the EEA, the Bulgarian could solve the issue.&lt;/p&gt;
    &lt;p&gt;According to the Bulgarian, his Norwegian landlord arranged the board position to help him financially.&lt;/p&gt;
    &lt;p&gt;However, he never received the money he was promised and left the board because he realized something was wrong, the Bulgarian says. He does not understand the police's suspicion towards him.&lt;/p&gt;
    &lt;p&gt;“I had no idea what they were doing. I have nothing to hide,” says the Bulgarian to NRK. He has been contacted by the police and says he is fully cooperating with them.&lt;/p&gt;
    &lt;head rend="h2"&gt;The consequences of uncovering the scam&lt;/head&gt;
    &lt;p&gt;After NRK's revelation in March, several flag states have issued stop orders to ships that used Ro Marine.&lt;/p&gt;
    &lt;p&gt;Panama alone has banned 16 ships from sailing because the ships have not shown new, real insurance within the deadline they were given.&lt;/p&gt;
    &lt;p&gt;The UK has sanctioned Ro Marine. Ro Marines’s website has been taken down. In July, the Oslo District Court forcibly dissolved the company for breach of accounting obligations, because Ro Marine had not submitted annual accounts for 2023.&lt;/p&gt;
    &lt;p&gt;However, the Russian company in St. Petersburg, which received payments worth millions of NOK marked for Ro Marine, is still active.&lt;/p&gt;
    &lt;p&gt;One month after NRK's revelation in March, another ship in the Russian shadow fleet presented a fake insurance certificate from Ro Marine. Inspectors at the oil port of Primorsk were presented with a document "signed in Oslo."&lt;/p&gt;
    &lt;p&gt;The expiration date of the fake insurance?&lt;/p&gt;
    &lt;p&gt;April next year.&lt;/p&gt;
    &lt;p&gt;NRK has contacted the companies that operate the mentioned ships "Agattu" and "Blint", with no response. We have not been able to contact Gatik. Novatek has not responded to our questions.&lt;/p&gt;
    &lt;head rend="h2"&gt;Contact us&lt;/head&gt;
    &lt;p&gt;Hi!&lt;/p&gt;
    &lt;p&gt;NRK made this article in cooperation with investigative journalists in Dossier Center.&lt;/p&gt;
    &lt;p&gt;Earlier, NRK, together with Danwatch, uncovered how forged documents were used to trick NATO countries in controls of shadow fleet ships.&lt;/p&gt;
    &lt;p&gt;In collaboration with Follow The Money and investigative journalists in eleven other editorial teams, NRK has mapped out in The Shadow Fleet Secrets how much of Putin's shadow fleet originates in Europe and in particular in Norway.&lt;/p&gt;
    &lt;p&gt;Do you have tips regarding the Ro Marine case or the shadow fleet? Or do you have information on other issues we should investigate further?&lt;/p&gt;
    &lt;p&gt;Send us an email or contact us securely via Signal or WhatsApp at +47 920 26 425.&lt;lb/&gt; Editor: Jenny Duesund.&lt;lb/&gt; Head of investigative journalism NRK West: Anniken Hjertholm.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45221996</guid></item><item><title>3D modeling with paper</title><link>https://www.arvinpoddar.com/blog/3d-modeling-with-paper</link><description>&lt;doc fingerprint="bd5add43313c429e"&gt;
  &lt;main&gt;
    &lt;p&gt;August 31, 2025&lt;/p&gt;
    &lt;head rend="h1"&gt;3D Modeling with Paper&lt;/head&gt;
    &lt;p&gt;Over the past several years, I've enjoyed the hobby of paper modeling (or papercraft), the art of creating 3D models from cut and glued parts from paper sheets. This hobby is a superset of origami, in that it allows for cutting and gluing, as well as for multiple sheets of paper for a single model. The alleviation of these constraints means that papercraft allows for more complex models that are easier to assemble.&lt;/p&gt;
    &lt;p&gt;Over many years, I've built models designed by others as well as designed my own. In this post, I want to share everything I've learned along the way, covering the entire process from design to assembly.&lt;/p&gt;
    &lt;p&gt;I love this hobby for three reasons:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;It is extremely accessible. There is no fancy hardware or software involved. As we'll see, the core tools are paper, scissors, and glue; everything else is an addon to make the experience better. All software tools can be free. Accidentally messed up during assembly and need a replacement part? Just print out another page. The entire creation of a model can be done in the ballpark of a few cents.&lt;/item&gt;
      &lt;item&gt;It is equally technical and creative. As we'll see, many of the problems faced in papercraft require an engineering-like approach and a willingness to experiment and iterate on designs. While it may appear outwardly like a craft project, the end-to-end process involves constraints and optimizing within them.&lt;/item&gt;
      &lt;item&gt;There's no limits on what you can make. What you decide to build is limited by your patience and imagination. Theoretically, nearly any object can be represented as a paper model.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Let's dive in. My most recent model is a papercraft plane inspired by the SR-71 Blackbird, a reconnaissance plane that to this day holds many records for being one of the fastest aircrafts ever. It's now one of most iconic planes ever designed and an engineering masterpiece. The program was ultimately retired in 1999.&lt;/p&gt;
    &lt;p&gt;We're going to walk through the full model design and assembly process, while referencing specific examples I encountered during creating this SR-71.&lt;/p&gt;
    &lt;head rend="h2"&gt;Constraints#&lt;/head&gt;
    &lt;p&gt;Let's set some constraints for how we're allowed to model our creation. These are self-imposed limitations that fit my preferred-style for model design:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;All parts in the assembled model must be made of paper.&lt;/item&gt;
      &lt;item&gt;Each part must be a single, solid color. The parts must not use any printed textures or designs.&lt;/item&gt;
      &lt;item&gt;The model must be represented as a simple polyhedron. There may be no curvatures, holes, two-dimensional surfaces, or surface-to-surface contact. If the figure we're trying to capture has any of these features, we must find a way to approximate it using only flat faces. The object must be manifold (an edge is only shared by 2 faces).&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Why constraints?#&lt;/head&gt;
    &lt;p&gt;It may feel weird to impose constraints on an art. However, I find that these constraints encourage a better designed model that can be assembled easily and predictably, including by others.&lt;/p&gt;
    &lt;p&gt;Using features like curvatures, printing with textures, etc. are shortcuts. For example, printing textures helps fill in details that aren't captured inherently by the model; curvatures and 2d surfaces are flimsy and introduce variances in how a model can be assembled. Simply polyhedral designs with single color parts ensure that the 3D form itself captures the object being depicted, and can be assembled in a structurally sound, predictable way.&lt;/p&gt;
    &lt;head rend="h2"&gt;Goals#&lt;/head&gt;
    &lt;p&gt;In addition to constraints, we also have some goals that we're optimzing for. These goals will be considered in each step of our design process.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Ease of assembly: By far the most important goal, our model should be easy to put together. Given the nature of paper and glue, a model that is difficult to assemble will almost certainly look bad. A model can have a well-designed topology, but still be difficult to assemble based on the parts design we put together.&lt;/item&gt;
      &lt;item&gt;Aesthetic appeal: This is an art, after all. The model we design should be aesthetically pleasing and resemble the object of interest.&lt;/item&gt;
      &lt;item&gt;Minimal consumption of resources: We should aim to minimize waste and use our materials efficiently.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As in engineering, we have to consider trade-offs between these goals, and optimize for these goals within our constraints.&lt;/p&gt;
    &lt;head rend="h2"&gt;Steps#&lt;/head&gt;
    &lt;p&gt;The process of designing a paper model is iterative. Each iteration consists of the following steps:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Mesh modeling - using software to create a 3D polyhedron mesh of our desired form&lt;/item&gt;
      &lt;item&gt;Mesh unfolding - unfolding the mesh into a 2D layout of parts&lt;/item&gt;
      &lt;item&gt;Assembly - putting the parts together to create the final model&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The remainder of this article will be walking through each step in detail. The discussion of each step will be centered around the goals and constraints declared from above.&lt;/p&gt;
    &lt;head rend="h2"&gt;Mesh Modeling#&lt;/head&gt;
    &lt;p&gt;Related goals: Ease of assembly, aesthetic appeal&lt;/p&gt;
    &lt;p&gt;In this phase, we design the mesh for our model. We aim to capture the essence of an object in a way that can feasibly be built with paper. Depending on how you approach this, this can easily be the most complicated step.&lt;/p&gt;
    &lt;p&gt;What do I mean by "feasibly built with paper"? Our mesh is a collection of polygons that represent a 3D object. The closeness of that representation is largely determined by how many polygons we use. We could use many really small polygons to closely match the subtle curves of our plane, but this would be hard to assemble in reality. Alternatively, we could simplifiy our representation down to a triangular pyramid. This would be trivially easy to assemble, but it wouldn't look a lot like our plane.&lt;/p&gt;
    &lt;p&gt;We can now see that our goals of ease of assembly and aesthetic appeal are at odds. Imagine that we have a continuum, where on the left we have a triangular pyramid (the simplest possible polyhedron) and on the right we have a mesh of the SR-71 with an arbitrarily high number (millions) of polygons.&lt;/p&gt;
    &lt;p&gt;Generally, an "easy" to assemble model will have somewhere around a few hundred polygons. Thus, our ideal model exists somewhere on the far left of this spectrum.&lt;/p&gt;
    &lt;p&gt;The challenge here is what I call "allocation of resolution" - we have a finite number of polygons to distribute across the features of our object. Certain features will naturally require more polygons to be accurately captured than others. For example, curved features require more polygons than flat features - in this model, the cylindrical engines will require more detail, than say, the flat wings.&lt;/p&gt;
    &lt;p&gt;In addition to the number of polygons and their concentrations, the arrangement of the polygons themselves matters - this is the topology of the mesh. Most discourse on 3D mesh topology is related to shading and animation. For our purposes, we're considered with ease of assembly. Certain topologies are easier to assemble and more structurally sound. Generally, here's some positive topological qualities for papercraft:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Symmetries: a good mesh design is symetrical when possible. Symmetrical shapes are intuitive and easier to reason about when assembling.&lt;/item&gt;
      &lt;item&gt;No narrow shapes: really narrow shapes are hard to cut out, hard to fold, and hard to glue. Avoid them at all costs.&lt;/item&gt;
      &lt;item&gt;Use quads: quad faces have an aesthetic appeal to them.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If all of this is sounding hard, we've got some options, in increasing order of difficulty:&lt;/p&gt;
    &lt;head rend="h3"&gt;Easy: Use an existing mesh#&lt;/head&gt;
    &lt;p&gt;The easiest way past this step is to find an existing mesh. There's a whole genre of 3D modeling called "low-poly" that you can find with a quick search on Thingiverse or Printables. These are usually designed for video games or 3D printing, but can be taken up for papercraft.&lt;/p&gt;
    &lt;head rend="h3"&gt;Medium: Converting an existing mesh#&lt;/head&gt;
    &lt;p&gt;Sometimes, you can find a high-resolution mesh of your desired object, but not a low-poly one. In this case, there are tools available to reduce the polygon count while preserving the overall shape. This is called "mesh simplification" or "mesh decimation."&lt;/p&gt;
    &lt;p&gt;This Instructable goes over the process of doing this with Meshlab, but there's many other software alternatives out there.&lt;/p&gt;
    &lt;p&gt;The pitfall of this approach is that automatic mesh decimation typically results in some nasty topologies, and there's not a lot you can do to control the output. To get around this, we could add an additional refinement step where we take the raw decimated mesh output and "clean it up" using a mesh editor software.&lt;/p&gt;
    &lt;p&gt;As an example, let's try this with a SR-71 mesh on Thingiverse. The original mesh has more than 1.2 million faces, and we're going to try decimating down to ~1,000. Here's what we get from Meshlab:&lt;/p&gt;
    &lt;p&gt;In this case, the output is not usable - it's wildly asymetric and is full of self-intersections. Refining this topology would take just as long (if not longer) as creating a model from scratch.&lt;/p&gt;
    &lt;head rend="h3"&gt;Hard: Creating your own mesh#&lt;/head&gt;
    &lt;p&gt;The most difficult option is to create your own mesh from scratch. This option gives you full control over the design, and is what I chose for the SR-71 model.&lt;/p&gt;
    &lt;p&gt;My software of choice for this is Blender. Blender has a steep learning curve, but the type of mesh design we're doing for this project doesn't begin to scratch the surface of its full capabilities. I highly recommend this low-poly tutorial if you've never used Blender before and need somewhere to start. Two things I found very handy were the mirror modifier to enforce symmetry, and the 3D Print Toolbox to auto-cleanup the mesh and check for manifoldness.&lt;/p&gt;
    &lt;p&gt;This process is very tedious. My advice here is: simplify your mesh to the point where you feel uncomfortable. Recall that we're largely optimizing for ease of assembly. When modeling, it's very tempting to capture finer details, but fine details have costs (small parts, hard to glue regions, etc.) that are not worth it during the assembly phase. Scrutinize every feature, and zoom out once in a while. When you zoom out, your omissions won't feel as weird.&lt;/p&gt;
    &lt;p&gt;After many days, here's the initial mesh I created. It contains 732 triangles. Note the symmetry along the y-axis.&lt;/p&gt;
    &lt;head rend="h2"&gt;Mesh Unfolding#&lt;/head&gt;
    &lt;p&gt;Related goals: Ease of assembly, minimal consumption of resources&lt;/p&gt;
    &lt;p&gt;Once we have a mesh, we have to convert it into a 2D template of parts that can be printed and assembled. This process is called unfolding. Each of the faces of our mesh are grouped into parts, and the arrangement of our parts is a layout, or template.&lt;/p&gt;
    &lt;p&gt;To do this, we're going to turn to software again. The most popular unfolding tool (and my favorite) is Pepakura Designer. Pepakura is not free (at the time of this writing, it's a one time $70 purchase) and it only runs on Windows. There's also Unfolder for Mac, which is $30. If you can't use either of these, Blender can save the day again with its free Paper Model plugin.&lt;/p&gt;
    &lt;p&gt;I believe that the unfolding step is one that does not get as much attention as it deserves. There is a noticable difference between a good template and a bad one. A good template has parts that make intuitive sense, with logical groupings and clear flow. The faces themselves are grouped into parts that are easy to cut out and handle. All of this equates to a better building experience, which means a better looking model.&lt;/p&gt;
    &lt;p&gt;Part of unfolding is also deciding the scale of your model. You can make your model as big or small as you want, but again, ease of assembly should be top of mind when deciding. A model that's too small will end up with parts that are hard to cut out and fold. Bigger models are easier to assemble, but you're limited to the point where the faces of your model must fit on a page.&lt;/p&gt;
    &lt;p&gt;I ended up making this model 25 inches long. With the original SR-71 being about 107 feet long, this puts our model at around a 1:50 ratio.&lt;/p&gt;
    &lt;head rend="h3"&gt;Creating many parts#&lt;/head&gt;
    &lt;p&gt;Let's start off with the creation of parts. In most unfolding software, the software will auto-unfold for you, and from there you can regroup faces into whatever parts you want. Here's Pepakura's auto unfold:&lt;/p&gt;
    &lt;p&gt;The parts it generated are pretty complicated, so we have some work to do.&lt;/p&gt;
    &lt;p&gt;If you have a mesh with faces, you can have anywhere from 1 (all the faces in a single part) to total parts (each part is a single face). We want our model to be easy to assemble, and neither of these extremes are easy.&lt;/p&gt;
    &lt;p&gt;Rather than trying to fix the number of parts and going from there, I recommend creating parts that are logical. Identify features that can be captured in a single part, and go from there. For example, in the SR-71, each engine intake spike makes sense as a single part. So does the nose cone.&lt;/p&gt;
    &lt;p&gt;If your mesh has an axis of symmetry, then your parts have symmetrical pairings as well. The same feature on either side of the axis should be represented with a mirrored part. In the SR-71, the entire plane is symmetrical on the vertical axis, so all parts across this axis are mirrored. This is good because once someone builds one side, they can more easily reason about the other side.&lt;/p&gt;
    &lt;p&gt;I ended up dividing this model into 42 parts. These parts were carefully divided in such a way that I felt would make them easier to assemble. If you look at any part in particular, chances are it'll have a symmetric counterpart.&lt;/p&gt;
    &lt;p&gt;They're arranged pretty haphazardly right now, but we'll cleanup this up in the next step.&lt;/p&gt;
    &lt;head rend="h3"&gt;Arranging the parts#&lt;/head&gt;
    &lt;p&gt;Again, most software will automatically arrange the parts for you as part of unfolding. Here's the 14 page arrangement Pepakura decided for the parts I created:&lt;/p&gt;
    &lt;p&gt;I highlighted all the parts on the first two pages so you can see where the are on the finished model. Notice that they're scattered throughout different sections. That's why I typically don't like auto-arrangement - they're designed to minimize paper usage, but they often result in a less intuitive assembly process. You can't look at any particular page and loosely know where its parts will go.&lt;/p&gt;
    &lt;p&gt;A good part layout reads like a story. Parts are arranged in a logical order, with related parts grouped together. I like to arrange mine left to right, top to bottom on a page. Here's my layout, with the first two pages highlighted.&lt;/p&gt;
    &lt;p&gt;All the parts that are near each other in the layout are also near each other in the final assembly. In this case, I even was able to reduce the page count down to 12 from the starting 14.&lt;/p&gt;
    &lt;head rend="h3"&gt;Flap structure#&lt;/head&gt;
    &lt;p&gt;Flaps, or tabs, are the appendages on each part that allow for gluing parts together. Each flap has a singular counterpart edge that it's glued to - this is known as an edge/flap pair. Most software will auto-assign a shared number between an edge and its flap to make identify pairs easy during the assembly process.&lt;/p&gt;
    &lt;p&gt;For an edge/flap pair, most unfolding software will allow us to swap the flap across parts. Doing this strategically is critical for creating an easy to assemble model, and also has implications for the structural integrity of the final build.&lt;/p&gt;
    &lt;p&gt;For example, consider the two example parts shown above. These two parts that meet at two shared edges, so these parts have two edge/flap pairs between them. We could arrange the flaps so that one part has both of them:&lt;/p&gt;
    &lt;p&gt;We could also interlace the flaps, so each part has one flap on each side.&lt;/p&gt;
    &lt;p&gt;Interlacing flaps between parts can create a more stable structure, since there's only one way for the parts to meet. If two flaps are on the same side, they can over-extend when glued to the edge. That being said, same-side flaps can be easier to work with, especially when reaching the closing stages of a model.&lt;/p&gt;
    &lt;p&gt;In general, I like to using interlaced flaps wherever possible to create an overall stronger model, and use same-side flaps selectively.&lt;/p&gt;
    &lt;p&gt;Once we have an arrangement we like, we can export our layout as a PDF.&lt;/p&gt;
    &lt;head rend="h2"&gt;Assembly#&lt;/head&gt;
    &lt;p&gt;With our layout PDF ready, we can now print it and move on to assembly. We'll finally get to see our design come to life.&lt;/p&gt;
    &lt;head rend="h3"&gt;Materials and Tools#&lt;/head&gt;
    &lt;p&gt;For our materials, we'll need:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;65lb (176 ) cardstock: This is the ideal paper weight for creating sturdy models, while still being thin/flexible enough to pass through a normal printer and be easy to fold.&lt;/item&gt;
      &lt;item&gt;Adhesive. My recommended adhesive is tacky glue: it's strong, dries clear, but is forgiving enough to allow for repositioning during assembly. Specifically, I use Aleene's Original Tacky Glue. I've also had past success with a glue stick.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We'll also need some tools, which I've listed these in order of importance. The ones with asterisks are essential. Everything else is a nice-to-have.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Printer*: You'll need access to a printer to print the template on the cardstock. Laser jet printers are great because the prints don't smudge.&lt;/item&gt;
      &lt;item&gt;Cutting tools*: You'll need a pair of scissors or a craft knife to cut out the parts. Use sharp tools for clean cuts - it makes a difference.&lt;/item&gt;
      &lt;item&gt;Ruler*: Cutting/scoring perfectly straight lines is a must. Steel rulers are great for their consistent edge, and they don't catch against your tools. That being said, I used a clear plastic ruler for this model. Being able to see through the ruler helps with alignment.&lt;/item&gt;
      &lt;item&gt;Scoring tool*: This will help you prepare a part for folding. You can use a bone folder or scoring wheel. I use an embossing tool I found at a dollar store, but before that, I used a ballpoint pen than ran out of ink. Anything with a precise (but not too sharp) tip will do.&lt;/item&gt;
      &lt;item&gt;Toothpicks: I use toothpicks to spread blobs of glue into thin layers and get into tight spaces.&lt;/item&gt;
      &lt;item&gt;Assembly surface: A cutting mat or piece of cardboard will protect your work surface and give you a stable surface to cut/score your parts.&lt;/item&gt;
      &lt;item&gt;Tweezers: Tweezers are helpful for handling small parts and getting into tight spaces, especially while holding parts together as glue dries.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you want to get fancy, you can also purchase an automatic cutting machine, like a Cricut or Silhouette. These machines can precisely cut/score your parts from cardstock. Getting the template into their software takes some extra effort, but it results in the best quality parts. I did not use a machine for this project.&lt;/p&gt;
    &lt;p&gt;To match the real SR-71, I printed my template on black cardstock. Darker cardstocks are harder to work with because of the low contrast between the ink and the paper itself. If you're new to the hobby, I would recommend starting with a lighter color.&lt;/p&gt;
    &lt;head rend="h3"&gt;Assembly phases#&lt;/head&gt;
    &lt;p&gt;The assembly of a model has 4 steps:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Cutting: Cutting the parts out from the paper with your cutting tool of choice. Scissors are quicker, but the combination of ruler and craft knife results in cleaner cuts.&lt;/item&gt;
      &lt;item&gt;Scoring: Running a scoring tool over fold lines to get cleaner folds. This may be tempting to skip, but I cannot emphasize the importance of this step enough. Scoring is especially important when dealing with thicker paper.&lt;/item&gt;
      &lt;item&gt;Folding: Folding the parts in prep for gluing. There's only two types of folds: mountain folds and valley folds.&lt;/item&gt;
      &lt;item&gt;Gluing: Gluing the parts together.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;How you decide to batch these steps is up to you. For example, you could cut all the parts out at once, then score all of them, etc. This approach is effective because you can develop a rhythm by doing each phase only once, so you're not constantly switching between tools; the downside is that you only get to start assembly after a pretty lengthy process. Alternatively, you can do it per part: cut one part out, score it, fold it, and secure it to the assembly. Here, the pros and cons are flipped: you get to see the model come together quicker, but there's a lot of context switching between phases. I've tried both of these approaches, and find that the latter results in a non-negligible increase in the assembly time of the model.&lt;/p&gt;
    &lt;p&gt;To strike a balance, the approach I took for this model was performing the phases at the granularity of sections (engines, wings, fuselage, etc.) of the model. This approach has the added final step of assembling all the standalone sections together into the final model.&lt;/p&gt;
    &lt;p&gt;Here's some pictures I took during the assembly process. In total, assembly took 6-8 hours.&lt;/p&gt;
    &lt;head rend="h3"&gt;Tips#&lt;/head&gt;
    &lt;p&gt;Use little glue: When gluing parts together, apply as little glue as possible. Using too much glue will result in spillover when the flaps/edges are put together, and this spillover is hard to wipe away from a porous surface like paper. Too much glue can even result in subtle paper warping. In the recommended tools, I suggested a toothpick. I apply a small bead of glue to a flap and use the toothpick to spread it into a thin film. This prevents any spillage and keeps the model clean.&lt;/p&gt;
    &lt;p&gt;Start in complex areas: As you progress further in gluing parts together, the degrees of freedom of your model will reduce. This is why I recommend starting with more complicated areas of your model where you'll need those degrees of freedom. In this model, this meant starting with precise features, like the engine inlet spikes or the vertical stabilizers.&lt;/p&gt;
    &lt;p&gt;Finish in hidden areas: This goes hand in hand with the tip above. As you reach to the end of your model, gluing the final parts together can be very hard, which means the final edges may come out a bit sloppy. Why does this happen? Any minor imperfections we made throughout the assembly process result in stresses in our model that will be felt at the end. Gluing the last part may be challenging because it'll feel misaligned, and it has the added challenge of attempting to close a 3D object from the outside. That's why I always recommend choosing an assembly order that results in the last parts being glued in an area that is out of sight. For the SR-71, that happens to be the underside of the fuselage.&lt;/p&gt;
    &lt;head rend="h3"&gt;Final Model#&lt;/head&gt;
    &lt;p&gt;Here's the final model, displayed on a stand (also made from paper):&lt;/p&gt;
    &lt;head rend="h2"&gt;Iteration#&lt;/head&gt;
    &lt;p&gt;No matter how much you scrutinize the modeling and layout phases, you will inevitably find areas for improvement as you assemble. In the case of the SR-71, I spotted a few minor assymetries in part tabs, and more importantly, an opportunity to reduce face count by simplifying the topology of the bottom of the plane and the nose cone.&lt;/p&gt;
    &lt;p&gt;I took my mesh back into Blender, and was able to get the triangle count down to 636, which is almost a full 100 faces fewer than the original mesh.&lt;/p&gt;
    &lt;p&gt;Below, you can see the old mesh (left) next to the new mesh (right). It's hard to tell the difference, yet the new one has almost 15% fewer faces.&lt;/p&gt;
    &lt;p&gt;A faster way to iterate is to render the model rather than physically building it. This allows you to quickly identify and fix visual issues without going through the hours of assembly. Here's some renders (in Blender) of the final iteration:&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion#&lt;/head&gt;
    &lt;p&gt;In total, the full cycle of designing the mesh, creating the parts layout, assembly, and subsequent refinement iterations occurred over the course of a few months. The process is long, but the results are well worth it.&lt;/p&gt;
    &lt;p&gt;If you're interested in making this model yourself, you can download the PDFs for the first iteration of the model below. I've included a template for the stand as well.&lt;/p&gt;
    &lt;p&gt;Hope you enjoy!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45222369</guid></item><item><title>Many hard LeetCode problems are easy constraint problems</title><link>https://buttondown.com/hillelwayne/archive/many-hard-leetcode-problems-are-easy-constraint/</link><description>&lt;doc fingerprint="cd8a25908f10ff1f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Many Hard Leetcode Problems are Easy Constraint Problems&lt;/head&gt;
    &lt;head rend="h2"&gt;Use the right tool for the job.&lt;/head&gt;
    &lt;p&gt;In my first interview out of college I was asked the change counter problem:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Given a set of coin denominations, find the minimum number of coins required to make change for a given number. IE for USA coinage and 37 cents, the minimum number is four (quarter, dime, 2 pennies).&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I implemented the simple greedy algorithm and immediately fell into the trap of the question: the greedy algorithm only works for "well-behaved" denominations. If the coin values were &lt;code&gt;[10, 9, 1]&lt;/code&gt;, then making 37 cents would take 10 coins in the greedy algorithm but only 4 coins optimally (&lt;code&gt;10+9+9+9&lt;/code&gt;). The "smart" answer is to use a dynamic programming algorithm, which I didn't know how to do. So I failed the interview.&lt;/p&gt;
    &lt;p&gt;But you only need dynamic programming if you're writing your own algorithm. It's really easy if you throw it into a constraint solver like MiniZinc and call it a day.&lt;/p&gt;
    &lt;code&gt;int: total;
array[int] of int: values = [10, 9, 1];
array[index_set(values)] of var 0..: coins;

constraint sum (c in index_set(coins)) (coins[c] * values[c]) == total;
solve minimize sum(coins);
&lt;/code&gt;
    &lt;p&gt;You can try this online here. It'll give you a prompt to put in &lt;code&gt;total&lt;/code&gt; and then give you successively-better solutions:&lt;/p&gt;
    &lt;code&gt;coins = [0, 0, 37];
----------
coins = [0, 1, 28];
----------
coins = [0, 2, 19];
----------
coins = [0, 3, 10];
----------
coins = [0, 4, 1];
----------
coins = [1, 3, 0];
----------
&lt;/code&gt;
    &lt;p&gt;Lots of similar interview questions are this kind of mathematical optimization problem, where we have to find the maximum or minimum of a function corresponding to constraints. They're hard in programming languages because programming languages are too low-level. They are also exactly the problems that constraint solvers were designed to solve. Hard leetcode problems are easy constraint problems.1 Here I'm using MiniZinc, but you could just as easily use Z3 or OR-Tools or whatever your favorite generalized solver is.&lt;/p&gt;
    &lt;head rend="h3"&gt;More examples&lt;/head&gt;
    &lt;p&gt;This was a question in a different interview (which I thankfully passed):&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Given a list of stock prices through the day, find maximum profit you can get by buying one stock and selling one stock later.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;It's easy to do in O(n^2) time, or if you are clever, you can do it in O(n). Or you could be not clever at all and just write it as a constraint problem:&lt;/p&gt;
    &lt;code&gt;array[int] of int: prices = [3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5, 8];
var int: buy;
var int: sell;
var int: profit = prices[sell] - prices[buy];

constraint sell &amp;gt; buy;
constraint profit &amp;gt; 0;
solve maximize profit;
&lt;/code&gt;
    &lt;p&gt;Reminder, link to trying it online here. While working at that job, one interview question we tested out was:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Given a list, determine if three numbers in that list can be added or subtracted to give 0?&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This is a satisfaction problem, not a constraint problem: we don't need the "best answer", any answer will do. We eventually decided against it for being too tricky for the engineers we were targeting. But it's not tricky in a solver;&lt;/p&gt;
    &lt;code&gt;include "globals.mzn";
array[int] of int: numbers = [3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5, 8];
array[index_set(numbers)] of var {0, -1, 1}: choices;

constraint sum(n in index_set(numbers)) (numbers[n] * choices[n]) = 0;
constraint count(choices, -1) + count(choices, 1) = 3;
solve satisfy;
&lt;/code&gt;
    &lt;p&gt;Okay, one last one, a problem I saw last year at Chipy AlgoSIG. Basically they pick some leetcode problems and we all do them. I failed to solve this one:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Given an array of integers heights representing the histogram's bar height where the width of each bar is 1, return the area of the largest rectangle in the histogram.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The "proper" solution is a tricky thing involving tracking lots of bookkeeping states, which you can completely bypass by expressing it as constraints:&lt;/p&gt;
    &lt;code&gt;array[int] of int: numbers = [2,1,5,6,2,3];

var 1..length(numbers): x; 
var 1..length(numbers): dx;
var 1..: y;

constraint x + dx &amp;lt;= length(numbers);
constraint forall (i in x..(x+dx)) (y &amp;lt;= numbers[i]);

var int: area = (dx+1)*y;
solve maximize area;

output ["(\(x)-&amp;gt;\(x+dx))*\(y) = \(area)"]
&lt;/code&gt;
    &lt;p&gt;There's even a way to automatically visualize the solution (using &lt;code&gt;vis_geost_2d&lt;/code&gt;), but I didn't feel like figuring it out in time for the newsletter.&lt;/p&gt;
    &lt;head rend="h3"&gt;Is this better?&lt;/head&gt;
    &lt;p&gt;Now if I actually brought these questions to an interview the interviewee could ruin my day by asking "what's the runtime complexity?" Constraint solvers runtimes are unpredictable and almost always than an ideal bespoke algorithm because they are more expressive, in what I refer to as the capability/tractability tradeoff. But even so, they'll do way better than a bad bespoke algorithm, and I'm not experienced enough in handwriting algorithms to consistently beat a solver.&lt;/p&gt;
    &lt;p&gt;The real advantage of solvers, though, is how well they handle new constraints. Take the stock picking problem above. I can write an O(n²) algorithm in a few minutes and the O(n) algorithm if you give me some time to think. Now change the problem to&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Maximize the profit by buying and selling up to&lt;/p&gt;&lt;code&gt;max_sales&lt;/code&gt;stocks, but you can only buy or sell one stock at a given time and you can only hold up to&lt;code&gt;max_hold&lt;/code&gt;stocks at a time?&lt;/quote&gt;
    &lt;p&gt;That's a way harder problem to write even an inefficient algorithm for! While the constraint problem is only a tiny bit more complicated:&lt;/p&gt;
    &lt;code&gt;include "globals.mzn";
int: max_sales = 3;
int: max_hold = 2;
array[int] of int: prices = [3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5, 8];
array [1..max_sales] of var int: buy;
array [1..max_sales] of var int: sell;
array [index_set(prices)] of var 0..max_hold: stocks_held;
var int: profit = sum(s in 1..max_sales) (prices[sell[s]] - prices[buy[s]]);

constraint forall (s in 1..max_sales) (sell[s] &amp;gt; buy[s]);
constraint profit &amp;gt; 0;

constraint forall(i in index_set(prices)) (stocks_held[i] = (count(s in 1..max_sales) (buy[s] &amp;lt;= i) - count(s in 1..max_sales) (sell[s] &amp;lt;= i)));
constraint alldifferent(buy ++ sell);
solve maximize profit;

output ["buy at \(buy)\n", "sell at \(sell)\n", "for \(profit)"];
&lt;/code&gt;
    &lt;p&gt;Most constraint solving examples online are puzzles, like Sudoku or "SEND + MORE = MONEY". Solving leetcode problems would be a more interesting demonstration. And you get more interesting opportunities to teach optimizations, like symmetry breaking.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Because my dad will email me if I don't explain this: "leetcode" is slang for "tricky algorithmic interview questions that have little-to-no relevance in the actual job you're interviewing for." It's from leetcode.com. ↩&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you're reading this on the web, you can subscribe here. Updates are once a week. My main website is here.&lt;/p&gt;
    &lt;p&gt;My new book, Logic for Programmers, is now in early access! Get it here.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45222695</guid></item><item><title>Crates.io phishing attempt</title><link>https://fasterthanli.me/articles/crates-io-phishing-attempt</link><description>&lt;doc fingerprint="7900925fe1544c5e"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;crates.io phishing attempt&lt;/head&gt;
    &lt;p&gt;Thanks to my sponsors: Reto Trappitsch, Brandon Piña, Matt Jackson, Justin Smith, Michał Bartoszkiewicz, David E Disch, Michael, Alan O'Donnell, René Ribaud, Benjamin Röjder Delnavaz, bbutkovic, Astrid, std__mpa, Marky Mark, Wojciech Smołka, Evan Relf, Raine Godmaire, Tomas Sedovic, Ronen Cohen, Jesse Luehrs and 279 more&lt;/p&gt;
    &lt;p&gt;Earlier this week, an npm supply chain attack.&lt;/p&gt;
    &lt;p&gt;It’s turn for crates.io, the main public repository for Rust crates (packages).&lt;/p&gt;
    &lt;p&gt;The phishing e-mail looks like this:&lt;/p&gt;
    &lt;p&gt;And it leads to a GitHub login page that looks like this:&lt;/p&gt;
    &lt;p&gt;Several maintainers received it — the issue is being discussed on GitHub.&lt;/p&gt;
    &lt;p&gt;The crates.io team has acknowledged the attack and said they’d see if they can do something about it.&lt;/p&gt;
    &lt;p&gt;No compromised packages have been identified as of yet (Sep 12, 14:10 UTC).&lt;/p&gt;
    &lt;p&gt;Important links:&lt;/p&gt;
    &lt;p&gt;Here's another article just for you:&lt;/p&gt;
    &lt;head rend="h2"&gt;Working with strings in Rust&lt;/head&gt;
    &lt;p&gt;There’s a question that always comes up when people pick up the Rust programming language: why are there two string types? Why is there &lt;code&gt;String&lt;/code&gt;, and &lt;code&gt;&amp;amp;str&lt;/code&gt;?&lt;/p&gt;
    &lt;p&gt;My Declarative Memory Management article answers the question partially, but there is a lot more to say about it, so let’s run a few experiments and see if we can conjure up a thorough defense of Rust’s approach over, say, C’s.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45222772</guid></item><item><title>Oq: Terminal OpenAPI Spec Viewer</title><link>https://github.com/plutov/oq</link><description>&lt;doc fingerprint="3599bed529cf6d98"&gt;
  &lt;main&gt;
    &lt;code&gt;oq openapi.yaml
# or
cat openapi.yaml | oq
# or
curl https://api.example.com/openapi.json | oq&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;↑/↓ or k/j - Navigate up/down through items&lt;/item&gt;
      &lt;item&gt;Tab - Switch between Endpoints and Components views&lt;/item&gt;
      &lt;item&gt;Enter or Space - Toggle fold/unfold for endpoint and component details&lt;/item&gt;
      &lt;item&gt;q or Ctrl+C - Quit the application&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;&lt;code&gt;oq&lt;/code&gt; supports both modern major OpenAPI specification versions:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;OpenAPI 3.0.x&lt;/item&gt;
      &lt;item&gt;OpenAPI 3.1.x&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Both JSON and YAML formats are supported.&lt;/p&gt;
    &lt;code&gt;git clone git@github.com:plutov/oq.git
cd oq
go build -o oq .&lt;/code&gt;
    &lt;p&gt;MIT License - see LICENSE file for details.&lt;/p&gt;
    &lt;p&gt;Contributions are welcome! Please feel free to submit issues and pull requests.&lt;/p&gt;
    &lt;p&gt;When contributing:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Ensure tests pass: &lt;code&gt;go test -v&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Test with both OpenAPI 3.0 and 3.1 examples&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45222799</guid></item><item><title>Doom-ada: Doom Emacs Ada language module with syntax, LSP and Alire support</title><link>https://github.com/tomekw/doom-ada</link><description>&lt;doc fingerprint="72e18e3db801abaa"&gt;
  &lt;main&gt;
    &lt;p&gt;This is a Doom Emacs &lt;code&gt;:lang ada&lt;/code&gt; module providing:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tree-sitter highlighting via &lt;code&gt;ada-ts-mode&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;LSP support with &lt;code&gt;ada_language_server&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Autocomplete (company-capf)&lt;/item&gt;
      &lt;item&gt;Alire integration (&lt;code&gt;alr build&lt;/code&gt;,&lt;code&gt;alr run&lt;/code&gt;,&lt;code&gt;alr clean&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Clone into your Doom modules folder:&lt;/p&gt;
    &lt;code&gt;git clone https://github.com/tomekw/doom-ada ~/.doom.d/modules/lang/ada&lt;/code&gt;
    &lt;p&gt;Enable in &lt;code&gt;~/.doom.d/init.el&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;:lang
ada&lt;/code&gt;
    &lt;p&gt;Sync Doom:&lt;/p&gt;
    &lt;code&gt;doom sync&lt;/code&gt;
    &lt;p&gt;Restart Emacs.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;SPC m b&lt;/code&gt;→ build with&lt;code&gt;alr build&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;SPC m r&lt;/code&gt;→ run with&lt;code&gt;alr run&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;SPC m c&lt;/code&gt;→ clean with&lt;code&gt;alr clean&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Errors are parsed into the compilation buffer, and &lt;code&gt;eglot&lt;/code&gt; provides inline diagnostics and completions.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45222993</guid></item><item><title>Show HN: DWS OS, a Plan 9 Inspired Web "OS"</title><link>https://dws.rip</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=45223053</guid></item><item><title>Show HN: An MCP Gateway to block the lethal trifecta</title><link>https://github.com/Edison-Watch/open-edison</link><description>&lt;doc fingerprint="45455d8b897b9b2"&gt;
  &lt;main&gt;
    &lt;quote&gt;
      &lt;p&gt;The Secure MCP Control Panel&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Connect AI to your data/software with additional security controls to help reduce data exfiltration risks. Gain visibility, monitor potential threats, and get alerts on the data your agent is reading/writing.&lt;/p&gt;
    &lt;p&gt;OpenEdison helps address the lethal trifecta problem, which can increase risks of agent hijacking &amp;amp; data exfiltration by malicious actors.&lt;/p&gt;
    &lt;p&gt;Join our Discord for feedback, feature requests, and to discuss MCP security for your use case: discord.gg/tXjATaKgTV&lt;/p&gt;
    &lt;head rend="h2"&gt;📧 To get visibility, control and exfiltration blocker into AI's interaction with your company software, systems of record, DBs, Contact us to discuss.&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;🛑 Data leak monitoring - Edison detects and blocks potential data leaks through configurable security controls&lt;/item&gt;
      &lt;item&gt;🕰️ Controlled execution - Provides structured execution controls to reduce data exfiltration risks.&lt;/item&gt;
      &lt;item&gt;🗂️ Easily configurable - Easy to configure and manage your MCP servers&lt;/item&gt;
      &lt;item&gt;📊 Visibility into agent interactions - Track and monitor your agents and their interactions with connected software/data via MCP calls&lt;/item&gt;
      &lt;item&gt;🔗 Simple API - REST API for managing MCP servers and proxying requests&lt;/item&gt;
      &lt;item&gt;🐳 Docker support - Run in a container for easy deployment&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Edison helps you gain observability, control, and policy enforcement for AI interactions with systems of records, existing company software and data. Reduce risks of AI-caused data leakage with streamlined setup for cross-system governance.&lt;/p&gt;
    &lt;p&gt;The fastest way to get started:&lt;/p&gt;
    &lt;code&gt;# Installs uv (via Astral installer) and launches open-edison with uvx.
# Note: This does NOT install Node/npx. Install Node if you plan to use npx-based tools like mcp-remote.
curl -fsSL https://raw.githubusercontent.com/Edison-Watch/open-edison/main/curl_pipe_bash.sh | bash&lt;/code&gt;
    &lt;p&gt;Run locally with uvx: &lt;code&gt;uvx open-edison&lt;/code&gt;
That will run the setup wizard if necessary.&lt;/p&gt;
    &lt;head&gt;⬇️ Install Node.js/npm (optional for MCP tools)&lt;/head&gt;
    &lt;p&gt;If you need &lt;code&gt;npx&lt;/code&gt; (for Node-based MCP tools like &lt;code&gt;mcp-remote&lt;/code&gt;), install Node.js as well:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;uv: &lt;code&gt;curl -fsSL https://astral.sh/uv/install.sh | sh&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Node/npx: &lt;code&gt;brew install node&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;uv: &lt;code&gt;curl -fsSL https://astral.sh/uv/install.sh | sh&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Node/npx: &lt;code&gt;sudo apt-get update &amp;amp;&amp;amp; sudo apt-get install -y nodejs npm&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;uv: &lt;code&gt;powershell -ExecutionPolicy ByPass -c "irm https://astral.sh/uv/install.ps1 | iex"&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Node/npx: &lt;code&gt;winget install -e --id OpenJS.NodeJS&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;After installation, ensure that &lt;code&gt;npx&lt;/code&gt; is available on PATH.&lt;/p&gt;
    &lt;head&gt;Install from PyPI&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Pipx/uvx&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Using uvx
uvx open-edison

# Using pipx
pipx install open-edison
open-edison&lt;/code&gt;
    &lt;p&gt;Run with a custom config directory:&lt;/p&gt;
    &lt;code&gt;open-edison run --config-dir ~/edison-config
# or via environment variable
OPEN_EDISON_CONFIG_DIR=~/edison-config open-edison run&lt;/code&gt;
    &lt;head&gt;Run with Docker&lt;/head&gt;
    &lt;p&gt;There is a dockerfile for simple local setup.&lt;/p&gt;
    &lt;code&gt;# Single-line:
git clone https://github.com/Edison-Watch/open-edison.git &amp;amp;&amp;amp; cd open-edison &amp;amp;&amp;amp; make docker_run

# Or
# Clone repo
git clone https://github.com/Edison-Watch/open-edison.git
# Enter repo
cd open-edison
# Build and run
make docker_run&lt;/code&gt;
    &lt;p&gt;The MCP server will be available at &lt;code&gt;http://localhost:3000&lt;/code&gt; and the api + frontend at &lt;code&gt;http://localhost:3001&lt;/code&gt;. 🌐&lt;/p&gt;
    &lt;head&gt;⚙️ Run from source&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Clone the repository:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;git clone https://github.com/Edison-Watch/open-edison.git
cd open-edison&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Set up the project:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;make setup&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Edit &lt;code&gt;config.json&lt;/code&gt;to configure your MCP servers. See the full file: config.json, it looks like:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;{
  "server": { "host": "0.0.0.0", "port": 3000, "api_key": "..." },
  "logging": { "level": "INFO", "database_path": "sessions.db" },
  "mcp_servers": [
    { "name": "filesystem", "command": "uvx", "args": ["mcp-server-filesystem", "/tmp"], "enabled": true },
    { "name": "github", "enabled": false, "env": { "GITHUB_PERSONAL_ACCESS_TOKEN": "..." } }
  ]
}&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Run the server:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;make run
# or, from the installed package
open-edison run&lt;/code&gt;
    &lt;p&gt;The server will be available at &lt;code&gt;http://localhost:3000&lt;/code&gt;. 🌐&lt;/p&gt;
    &lt;head&gt;🔌 MCP Connection&lt;/head&gt;
    &lt;p&gt;Connect any MCP client to Open Edison (requires Node.js/npm for &lt;code&gt;npx&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;npx -y mcp-remote http://localhost:3000/mcp/ --http-only --header "Authorization: Bearer your-api-key"&lt;/code&gt;
    &lt;p&gt;Or add to your MCP client config:&lt;/p&gt;
    &lt;code&gt;{
  "mcpServers": {
    "open-edison": {
      "command": "npx",
      "args": ["-y", "mcp-remote", "http://localhost:3000/mcp/", "--http-only", "--header", "Authorization: Bearer your-api-key"]
    }
  }
}&lt;/code&gt;
    &lt;head&gt;🧭 Usage&lt;/head&gt;
    &lt;p&gt;See API Reference for full API documentation.&lt;/p&gt;
    &lt;head&gt;🛠️ Development&lt;/head&gt;
    &lt;p&gt;Setup from source as above.&lt;/p&gt;
    &lt;p&gt;Server doesn't have any auto-reload at the moment, so you'll need to run &amp;amp; ctrl-c this during development.&lt;/p&gt;
    &lt;code&gt;make run&lt;/code&gt;
    &lt;p&gt;We expect &lt;code&gt;make ci&lt;/code&gt; to return cleanly.&lt;/p&gt;
    &lt;code&gt;make ci&lt;/code&gt;
    &lt;head&gt;⚙️ Configuration (config.json)&lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;config.json&lt;/code&gt; file contains all configuration:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;server.host&lt;/code&gt;- Server host (default: localhost)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;server.port&lt;/code&gt;- Server port (default: 3000)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;server.api_key&lt;/code&gt;- API key for authentication&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;logging.level&lt;/code&gt;- Log level (DEBUG, INFO, WARNING, ERROR)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;mcp_servers&lt;/code&gt;- Array of MCP server configurations&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Each MCP server configuration includes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;name&lt;/code&gt;- Unique name for the server&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;command&lt;/code&gt;- Command to run the MCP server&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;args&lt;/code&gt;- Arguments for the command&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;env&lt;/code&gt;- Environment variables (optional)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;enabled&lt;/code&gt;- Whether to auto-start this server&lt;/item&gt;
    &lt;/list&gt;
    &lt;head&gt;🔱 The lethal trifecta, agent lifecycle management&lt;/head&gt;
    &lt;p&gt;Open Edison includes a comprehensive security monitoring system that tracks the "lethal trifecta" of AI agent risks, as described in Simon Willison's blog post:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Private data access - Access to sensitive local files/data&lt;/item&gt;
      &lt;item&gt;Untrusted content exposure - Exposure to external/web content&lt;/item&gt;
      &lt;item&gt;External communication - Ability to write/send data externally&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The configuration allows you to classify these risks across tools, resources, and prompts using separate configuration files.&lt;/p&gt;
    &lt;p&gt;In addition to trifecta, we track Access Control Level (ACL) for each tool call, that is, each tool has an ACL level (one of PUBLIC, PRIVATE, or SECRET), and we track the highest ACL level for each session. If a write operation is attempted to a lower ACL level, it can be blocked based on your configuration.&lt;/p&gt;
    &lt;p&gt;Defines security classifications for MCP tools. See full file: tool_permissions.json, it looks like:&lt;/p&gt;
    &lt;code&gt;{
  "_metadata": { "last_updated": "2025-08-07" },
  "builtin": {
    "get_security_status": { "enabled": true, "write_operation": false, "read_private_data": false, "read_untrusted_public_data": false, "acl": "PUBLIC" }
  },
  "filesystem": {
    "read_file": { "enabled": true, "write_operation": false, "read_private_data": true, "read_untrusted_public_data": false, "acl": "PRIVATE" },
    "write_file": { "enabled": true, "write_operation": true, "read_private_data": true, "read_untrusted_public_data": false, "acl": "PRIVATE" }
  }
}&lt;/code&gt;
    &lt;head&gt;📁 Resource Permissions (`resource_permissions.json`)&lt;/head&gt;
    &lt;p&gt;Defines security classifications for resource access patterns. See full file: resource_permissions.json, it looks like:&lt;/p&gt;
    &lt;code&gt;{
  "_metadata": { "last_updated": "2025-08-07" },
  "builtin": { "config://app": { "enabled": true, "write_operation": false, "read_private_data": false, "read_untrusted_public_data": false } }
}&lt;/code&gt;
    &lt;head&gt;💬 Prompt Permissions (`prompt_permissions.json`)&lt;/head&gt;
    &lt;p&gt;Defines security classifications for prompt types. See full file: prompt_permissions.json, it looks like:&lt;/p&gt;
    &lt;code&gt;{
  "_metadata": { "last_updated": "2025-08-07" },
  "builtin": { "summarize_text": { "enabled": true, "write_operation": false, "read_private_data": false, "read_untrusted_public_data": false } }
}&lt;/code&gt;
    &lt;p&gt;All permission types support wildcard patterns:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tools: &lt;code&gt;server_name/*&lt;/code&gt;(e.g.,&lt;code&gt;filesystem/*&lt;/code&gt;matches all filesystem tools)&lt;/item&gt;
      &lt;item&gt;Resources: &lt;code&gt;scheme:*&lt;/code&gt;(e.g.,&lt;code&gt;file:*&lt;/code&gt;matches all file resources)&lt;/item&gt;
      &lt;item&gt;Prompts: &lt;code&gt;type:*&lt;/code&gt;(e.g.,&lt;code&gt;template:*&lt;/code&gt;matches all template prompts)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;All items must be explicitly configured - unknown tools/resources/prompts will be rejected for security.&lt;/p&gt;
    &lt;p&gt;Use the &lt;code&gt;get_security_status&lt;/code&gt; tool to monitor your session's current risk level and see which capabilities have been accessed. When the lethal trifecta is achieved (all three risk flags set), further potentially dangerous operations are blocked.&lt;/p&gt;
    &lt;p&gt;📚 Complete documentation available in &lt;code&gt;docs/&lt;/code&gt;&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;🚀 Getting Started - Quick setup guide&lt;/item&gt;
      &lt;item&gt;⚙️ Configuration - Complete configuration reference&lt;/item&gt;
      &lt;item&gt;📡 API Reference - REST API documentation&lt;/item&gt;
      &lt;item&gt;🧑💻 Development Guide - Contributing and development&lt;/item&gt;
    &lt;/list&gt;
    &lt;head&gt;📄 License&lt;/head&gt;
    &lt;p&gt;GPL-3.0 License - see LICENSE for details.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45223102</guid></item><item><title>A beginner's guide to extending Emacs</title><link>https://blog.tjll.net/a-beginners-guide-to-extending-emacs/</link><description>&lt;doc fingerprint="8c00b9736825d3b5"&gt;
  &lt;main&gt;&lt;p&gt;This post isn’t about the virtues of some editors versus others: that's already been written by somebody else (and it’s really good) – if you want to know why I use emacs, I suggest reading that instead.&lt;/p&gt;&lt;p&gt; This post will help you understand why "extensibility" and "introspectability" are such prominent emacs features even without an emacs lisp background. Bridging the gap from spacemacs or doom emacs to a bespoke configuration wasn't easy for me because I didn’t know how to learn emacs, so I'm going to stumble through one of my own use cases to demonstrate how this process goes if you're peeking in from outside the emacs ecosystem, &lt;del&gt;horrified&lt;/del&gt; curious about how this all works. &lt;/p&gt;&lt;p&gt;Let's talk about reStructuredText.&lt;/p&gt;&lt;head rend="h4"&gt;Table of Contents&lt;/head&gt;&lt;head rend="h4"&gt;reStructuredText&lt;/head&gt;&lt;p&gt; At my day job I write our user documentation using Sphinx. It expects my stilted prose in &lt;code&gt;.rst&lt;/code&gt; format, which is kind of like Markdown if you squint.
&lt;/p&gt;&lt;p&gt; I do an awful lot of cross-referencing between references (or &lt;code&gt;refs&lt;/code&gt;) to link concepts across the documentation.
You define a reference like this:
&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Font used for directives and roles.&lt;/item&gt;&lt;item&gt;Font used for all other defining constructs.&lt;/item&gt;&lt;/list&gt;&lt;quote&gt;&lt;code&gt;.. _code_example:&lt;/code&gt;&lt;code&gt;.. code::&lt;/code&gt;&lt;code&gt;echo "HELP I'M TRAPPED IN A CODE EXAMPLE"&lt;/code&gt;&lt;/quote&gt;&lt;p&gt;…and then link to it later like this:&lt;/p&gt;ReST&lt;list rend="ul"&gt;&lt;item&gt;Font used for field names and interpreted text.&lt;/item&gt;&lt;item&gt;Font used for directives and roles.&lt;/item&gt;&lt;/list&gt;&lt;code&gt;This :ref:`doesn't look like anything to me &amp;lt;code_example&amp;gt;`.
&lt;/code&gt;&lt;p&gt; …or like this (if the &lt;code&gt;ref&lt;/code&gt; is associated with a title of some sort):
&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Font used for field names and interpreted text.&lt;/item&gt;&lt;item&gt;Font used for directives and roles.&lt;/item&gt;&lt;/list&gt;&lt;code&gt;Don't say :ref:`code_example`.
&lt;/code&gt;&lt;p&gt;My problem is that I have an assload of references across the all of the documentation and my brain cannot recall them on the spot. What I really need is the ability to call up the list of references to easily discover and select from that list – this is basically auto-completion but for documentation headers (or titles).&lt;/p&gt;&lt;p&gt;I am ready to write some shitty elisp with the help of aliens.&lt;/p&gt;&lt;head rend="h5"&gt;A Parentheses Prelude&lt;/head&gt;&lt;p&gt;Before we dig into emacs' guts, here are some principles that I learned after my first elisp experiments that might help somebody digging into this ecosystem for the first time:&lt;/p&gt;&lt;head rend="h6"&gt;1. Emacs Wants You to Extend It&lt;/head&gt;&lt;p&gt; I haven't written plugins for other editors extensively, but I can tell you this: emacs doesn't just make deep customization available, but it actively encourages you to make an absolute customization &lt;del&gt;messes&lt;/del&gt; masterpieces. Core editor functions aren't just documented, but often include tidbits about "you probably want to see this other variable" or "here's how you should use this". &lt;/p&gt;&lt;p&gt; Not only that, but emacs happily hands you functions shaped like nuclear warheads like &lt;code&gt;advice-add&lt;/code&gt; (that let you override any function) that can absolutely obliterate your editor if you hold it the wrong way.
Of course, this also grants you unlimited power.
&lt;/p&gt;&lt;p&gt;Remember that emacs is designed to be torn apart and rearranged.&lt;/p&gt;&lt;head rend="h6"&gt;2. Geriatric Software&lt;/head&gt;&lt;p&gt;The first public release of GNU emacs happened in 1985. Literally 40 years of development sits inside of emacs and its developers are still adding non-trivial features (native language server support landed in version 29 in 2023).&lt;/p&gt;&lt;p&gt;The ecosystem is vast and the language has evolved for a long time. There's nearly always something useful if you need a particular piece of functionality, so even moreso than with other ecosystems: remember to do your homework first.&lt;/p&gt;&lt;head rend="h6"&gt;3. Lisp for for the un-Lisped&lt;/head&gt;&lt;p&gt;The syntax is polarizing, I know. Gurus will wince when I get this wrong, but:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Writing lisp is like writing any other code, just with the parentheses wrapping everything instead of just arguments. &lt;code&gt;print("Macrodata Refinement")&lt;/code&gt;becomes&lt;code&gt;(print "Macrodata Refinement")&lt;/code&gt;&lt;/item&gt;&lt;item&gt;Sometimes you don't get functions, you get macros that behave special ways. For example, &lt;code&gt;let&lt;/code&gt;sets variables for an inner block of code. Like this:&lt;code&gt;(let (name "Mark S.") (print name))&lt;/code&gt;&lt;/item&gt;&lt;item&gt;Lispers say "this is actually data and not calling code" by doing this with single quotes: &lt;code&gt;'("list" "of" "strings")&lt;/code&gt;&lt;/item&gt;&lt;/list&gt;&lt;p&gt;I'm out of my depth in lisp, but if you're a novice, those notes might help.&lt;/p&gt;&lt;head rend="h4"&gt;Extensible MACroS&lt;/head&gt;&lt;p&gt;With that prelude out of the way, let's begin.&lt;/p&gt;&lt;p&gt;Inside of emacs you can call up a list of potential completions by using the keyboard shortcut M-. (that’s "hit the meta key along with period", where "meta" is the Alt key for me). This applies in a wide variety of scenarios, like when completing class names or variables. If we want to ask emacs to hand us a list of potential references, then the system we want to hook into is this completions system.&lt;/p&gt;&lt;p&gt; (This is the only time I'll assume we know where to go without crawling through documentation. You could discover it yourself looking for "&lt;code&gt;completion&lt;/code&gt;" or similar string in emacs docs).
&lt;/p&gt;&lt;p&gt; To start our hero’s journey, we figure out what the hell M-. actually does. We can ask emacs this by calling the function &lt;code&gt;describe-key&lt;/code&gt;, which is bound to C-h k.
Hitting Ctrl-h, then k, then M-. drops us into a help buffer that looks like this:
&lt;/p&gt;&lt;quote&gt;&lt;code&gt;M-. runs the command completion-at-point (found in&lt;/code&gt;&lt;code&gt;evil-insert-state-map), which is an interactive native-compiled Lisp&lt;/code&gt;&lt;code&gt;function in ‘minibuffer.el’.&lt;/code&gt;&lt;code&gt;It is bound to M-..&lt;/code&gt;&lt;code&gt;(completion-at-point)&lt;/code&gt;&lt;code&gt;Perform completion on the text around point.&lt;/code&gt;&lt;code&gt;The completion method is determined by ‘completion-at-point-functions’.&lt;/code&gt;&lt;code&gt;Probably introduced at or before Emacs version 23.2.&lt;/code&gt;&lt;/quote&gt;&lt;p&gt; We have the next breadcrumb to follow, which is the variable &lt;code&gt;completion-at-point-functions&lt;/code&gt;.
Running &lt;code&gt;completion-at-point&lt;/code&gt; by hitting M-. consults that variable to hand us completion candidates, so we &lt;code&gt;describe-variable&lt;/code&gt; it with C-h v and then choose &lt;code&gt;completion-at-point-functions&lt;/code&gt; from the list of variables:
&lt;/p&gt;&lt;quote&gt;&lt;code&gt;completion-at-point-functions is a variable defined in ‘minibuffer.el’.&lt;/code&gt;&lt;code&gt;Its value is (cape-dict cape-file tags-completion-at-point-function)&lt;/code&gt;&lt;code&gt;Special hook to find the completion table for the entity at point.&lt;/code&gt;&lt;code&gt;Each function on this hook is called in turn without any argument and&lt;/code&gt;&lt;code&gt;should return either nil, meaning it is not applicable at point,&lt;/code&gt;&lt;code&gt;or a function of no arguments to perform completion (discouraged),&lt;/code&gt;&lt;code&gt;or a list of the form (START END COLLECTION . PROPS)&lt;/code&gt;&lt;/quote&gt;&lt;p&gt; …and it goes on from there. You can see some existing completion functions in there: I use a package called cape to offer helpful suggestions like file paths if I start typing in something like &lt;code&gt;./filename&lt;/code&gt;.
&lt;/p&gt;&lt;p&gt;The description for this variable instructs us about how to add our own functions (scary!) You’ll note that emacs calls this a "hook", which is most often just a term used to describe a variable that is a list of functions that get called at a specific time (hooks show up everywhere).&lt;/p&gt;&lt;p&gt; I elided the full description for &lt;code&gt;completion-at-point-functions&lt;/code&gt; – which is lengthy! – but if you parse it all out, you learn the following:
&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Your completion at point function should return either &lt;code&gt;nil&lt;/code&gt;(the elisp "null") – which means your completion function doesn’t apply right now – or another function (which emacs discourages), or a list, which is what we’ll do because it sounds like the most-correct thing to do.&lt;/item&gt;&lt;item&gt;The list we return is &lt;code&gt;(START END COLLECTION . PROPS)&lt;/code&gt;:&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;START&lt;/code&gt;and&lt;code&gt;END&lt;/code&gt;should be positions in the buffer between which emacs will replace the completed symbol with our candidate. That is, if your cursor is calling a method on a Python object like&lt;code&gt;file.ope|&lt;/code&gt;(where the bar is your cursor), emacs will replace just&lt;code&gt;ope&lt;/code&gt;when you select&lt;code&gt;open&lt;/code&gt;from a list of completions and not the entire&lt;code&gt;file.ope&lt;/code&gt;string.&lt;/item&gt;&lt;item&gt;&lt;code&gt;COLLECTION&lt;/code&gt;is the juicy bit. The documentation calls it a completion "table", and there’s probably hidden meaning there, but you can just return a list of candidates and move on with your day, which is what I'll do.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Okay, so we need to write something to find the bounds of a string to replace and a function that returns that list.&lt;/p&gt;&lt;head rend="h5"&gt;Completions Abound&lt;/head&gt;&lt;p&gt; I fooled around with some regular expressions for a while until I did the right thing and examined how other completion backends do it. If you have the package installed, the aforementioned &lt;code&gt;cape-file&lt;/code&gt; function gives us a hint: hit M-x, then choose &lt;code&gt;find-function&lt;/code&gt;, select &lt;code&gt;cape-file&lt;/code&gt;, and poke around. You’ll find the use of a function called &lt;code&gt;bounds-of-thing-at-point&lt;/code&gt;.
Describing it with C-h f &lt;code&gt;bounds-of-thing-at-point&lt;/code&gt; gives us:
&lt;/p&gt;&lt;quote&gt;&lt;code&gt;Determine the start and end buffer locations for the THING at point.&lt;/code&gt;&lt;code&gt;THING should be a symbol specifying a type of syntactic entity.&lt;/code&gt;&lt;code&gt;Possibilities include ‘symbol’, ‘list’, ‘sexp’, ‘defun’, ‘number’,&lt;/code&gt;&lt;code&gt;‘filename’, ‘url’, ‘email’, ‘uuid’, ‘word’, ‘sentence’, ‘whitespace’,&lt;/code&gt;&lt;code&gt;‘line’, and ‘page’.&lt;/code&gt;&lt;/quote&gt;&lt;p&gt; And that is useful for our &lt;code&gt;START&lt;/code&gt; and &lt;code&gt;END&lt;/code&gt; needs.
You can take it for a test drive at any time with M-: &lt;code&gt;(bounds-of-thing-at-point 'word)&lt;/code&gt; to see where emacs thinks the word at your cursor starts and ends.
This is a common theme when developing elisp: try out functions all the time within the editor since they’re near at hand.
&lt;/p&gt;&lt;p&gt; The argument to &lt;code&gt;bounds-of-thing-at-point&lt;/code&gt; is a symbol for a literal thing that is predefined by the function &lt;code&gt;define-thing-chars&lt;/code&gt;.
We pass &lt;code&gt;define-thing-chars&lt;/code&gt; a name for our "thing" and a regex, and we can call &lt;code&gt;bounds-of-thing-at-point&lt;/code&gt; with it from that point on.
The function documentation in &lt;code&gt;thingatpt.el&lt;/code&gt; that emacs refers you to explains more if you’re interested.
&lt;/p&gt;&lt;p&gt;&lt;code&gt;define-thing-chars&lt;/code&gt; expects a string with characters to put into a regex character class (like &lt;code&gt;[...]&lt;/code&gt;) - just any valid character.
This is a pretty standard character class and we can start with something super simple.
I can’t be bothered to look up whatever the reStructedText spec is for references, but let’s start with "word characters, dashes, and underscores".
That expressed as a "thing" looks like this:
&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Font used to highlight strings.&lt;/item&gt;&lt;item&gt;Font used to highlight keywords.&lt;/item&gt;&lt;/list&gt;&lt;code&gt;(define-thing-chars rst-ref "[:alpha:]_-")
&lt;/code&gt;&lt;p&gt; Now we have a thing called &lt;code&gt;rst-ref&lt;/code&gt; we can use with &lt;code&gt;bounds-of-thing-at-point&lt;/code&gt;.
In typical emacs fashion, we can run elisp ad-hoc in our editor just to tinker, so let’s do that now.
&lt;/p&gt;&lt;p&gt; Remember: we’re trying to write a function to give us the &lt;code&gt;start&lt;/code&gt; and &lt;code&gt;end&lt;/code&gt; of whatever piece of text we intend for a completion to replace.
Let’s try it out: in any sort of buffer, put a piece of fake &lt;code&gt;.rst&lt;/code&gt; text with a reference, like this:
&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Font used for field names and interpreted text.&lt;/item&gt;&lt;item&gt;Font used for directives and roles.&lt;/item&gt;&lt;/list&gt;&lt;code&gt;This is a :ref:`other-reference`.
&lt;/code&gt;&lt;p&gt; Place your point somewhere within "&lt;code&gt;other-reference&lt;/code&gt;" and try out your &lt;code&gt;thing&lt;/code&gt;:
&lt;/p&gt;&lt;p&gt; M-: &lt;code&gt;(bounds-of-thing-at-point 'rst-ref)&lt;/code&gt;
&lt;/p&gt;&lt;p&gt; You’ll see something like &lt;code&gt;(number . number)&lt;/code&gt; in the echo area (the little minibuffer at the bottom of the emacs window frame).
Congratulations!
We’ve got the first part of the problem solved.
&lt;/p&gt;&lt;head rend="h4"&gt;Gathering Completions&lt;/head&gt;&lt;p&gt;Recall the structure of what our "completion backend" needs to return to emacs:&lt;/p&gt;ELisp&lt;code&gt;(START END COLLECTION . PROPS)
&lt;/code&gt;&lt;p&gt; We can construct &lt;code&gt;START&lt;/code&gt; and &lt;code&gt;END&lt;/code&gt; with &lt;code&gt;bounds-of-thing-at-point&lt;/code&gt;, now we just need &lt;code&gt;COLLECTION&lt;/code&gt;, which is a list of potential candidates.
&lt;/p&gt;&lt;p&gt;Conceptually the task isn’t hard: we should find all instances of strings of the form:&lt;/p&gt;ReST&lt;list rend="ul"&gt;&lt;item&gt;Font used for all other defining constructs.&lt;/item&gt;&lt;/list&gt;&lt;code&gt;.. _my-reference:
&lt;/code&gt;&lt;p&gt; in our document and capture &lt;code&gt;my-reference&lt;/code&gt;.
Where do we start?
&lt;/p&gt;&lt;p&gt; Once again you can rely on discovery mechanisms like searching for functions that sound related (by browsing &lt;code&gt;describe-function&lt;/code&gt;) or look at existing code.
Personally, I found this:
&lt;/p&gt;&lt;quote&gt;&lt;code&gt;(re-search-forward REGEXP &amp;amp;optional BOUND NOERROR COUNT)&lt;/code&gt;&lt;code&gt;Search forward from point for regular expression REGEXP.&lt;/code&gt;&lt;/quote&gt;&lt;p&gt;The documentation refers you to some other related functions, like this one:&lt;/p&gt;&lt;quote&gt;&lt;code&gt;(match-beginning SUBEXP)&lt;/code&gt;&lt;code&gt;Return position of start of text matched by last search.&lt;/code&gt;&lt;code&gt;SUBEXP, a number, specifies which parenthesized expression in the last&lt;/code&gt;&lt;code&gt;regexp.&lt;/code&gt;&lt;/quote&gt;&lt;p&gt; So we can &lt;code&gt;(re-search-forward)&lt;/code&gt; for something then invoke &lt;code&gt;(match-beginning 1)&lt;/code&gt;, for example, if we used a regex capture group to grab the reference’s label.
Cool: we can start there.
&lt;/p&gt;&lt;p&gt;As you get deeper into elisp you’ll find that regular expressions are everywhere, and this case is no different. We need a solid regex to search through a reStructuredText buffer (and honor any quirks in emacs’ regular expression engine), so we’ll use this opportunity to kick the tires on interactively developing regular expressions in emacs.&lt;/p&gt;&lt;head rend="h5"&gt;Regexes&lt;/head&gt;&lt;p&gt;Geriatric millennial software engineers like myself grew up on https://regexr.com/ when it was still a Flash application. Unless you’re a masochist that lives and breathes regular expressions, it’s kind of hard to develop a good regex without live feedback, which sites like https://regexr.com/ provide.&lt;/p&gt;&lt;p&gt;Little did I know that emacs comes with its own live regular expression builder and it's goooood.&lt;/p&gt;&lt;p&gt; Within any emacs buffer, run M-x &lt;code&gt;re-builder&lt;/code&gt; to open the regex builder window split alongside the current buffer.
If I then enter the string &lt;code&gt;"re-\\(builder\\)"&lt;/code&gt; into that buffer, that string a) gets highlighted in my original buffer and b) the capture group gets highlighted in its own unique group color.
&lt;/p&gt;&lt;p&gt; You can do this all day long to fine-tune a regular expression, but there’s yet another trick when writing regular expressions, which is to use the &lt;code&gt;rx&lt;/code&gt; macro.
&lt;/p&gt;&lt;p&gt; My previous example regular expression &lt;code&gt;"re-\\(builder\\)"&lt;/code&gt; works, but the quirks when writing emacs regular expressions pile up quickly: escaping characters is one example but there are more, too.
&lt;/p&gt;&lt;p&gt; Instead, the &lt;code&gt;rx&lt;/code&gt; macro will let you define a regular expression in lisp-y form and evaluate it into a typical string-based regular expression you can use normally, so it works any place emacs expects a string-based regular expression.
For example, if you evaluate this with M-::
&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Font used to highlight strings.&lt;/item&gt;&lt;item&gt;Font used to highlight keywords.&lt;/item&gt;&lt;/list&gt;&lt;code&gt;(rx "re-" (group "builder"))
&lt;/code&gt;&lt;p&gt;This is what emacs returns:&lt;/p&gt;ELisp&lt;list rend="ul"&gt;&lt;item&gt;Font for backslashes in Lisp regexp grouping constructs.&lt;/item&gt;&lt;item&gt;Font used to highlight strings.&lt;/item&gt;&lt;/list&gt;&lt;code&gt;"re-\\(builder\\)"
&lt;/code&gt;&lt;p&gt; Identical! The &lt;code&gt;rx&lt;/code&gt; documentation explains all the constructs available to you.
&lt;/p&gt;&lt;p&gt; Jumping back to &lt;code&gt;re-builder&lt;/code&gt;, with the &lt;code&gt;re-builder&lt;/code&gt; window active, invoke M-x &lt;code&gt;reb-change-syntax&lt;/code&gt; and choose &lt;code&gt;rx&lt;/code&gt;.
Now you can interactively build regular expressions with the &lt;code&gt;rx&lt;/code&gt; macro!
In the &lt;code&gt;re-builder&lt;/code&gt; window, you’ve got to enter a weird syntax to get it to take &lt;code&gt;rx&lt;/code&gt; constructs (I’m… not sure why this is), but you end up with the same outcome:
&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Font used to highlight strings.&lt;/item&gt;&lt;/list&gt;&lt;code&gt;'(: "re-" (group "builder"))
&lt;/code&gt;&lt;p&gt;Watch the regex get highlighted live just as it was in the string-based regex mode.&lt;/p&gt;&lt;p&gt; To bring this full circle, hop into a buffer with an example &lt;code&gt;.rst&lt;/code&gt; document like this one:
&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Font used for all other defining constructs.&lt;/item&gt;&lt;item&gt;Font used for the adornment of a section header.&lt;/item&gt;&lt;item&gt;Default font for section title text at level 1.&lt;/item&gt;&lt;/list&gt;&lt;quote&gt;&lt;code&gt;A Heading&lt;/code&gt;&lt;code&gt;=========&lt;/code&gt;&lt;code&gt;.. _my-reference:&lt;/code&gt;&lt;code&gt;Link to me!&lt;/code&gt;&lt;/quote&gt;&lt;p&gt; Using our newfound &lt;code&gt;re-builder&lt;/code&gt; knowledge, let’s build a regex interactively to make short work of it:
&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Invoke M-x &lt;code&gt;re-builder&lt;/code&gt;&lt;/item&gt;&lt;item&gt;Change the engine to something easier with M-x &lt;code&gt;reb-change-syntax&lt;/code&gt;and choose&lt;code&gt;rx&lt;/code&gt;&lt;/item&gt;&lt;item&gt;Start trying out solutions&lt;/item&gt;&lt;/list&gt;&lt;p&gt; I’ll refer here to the rx constructs documentation which lists out all the possibilities that you can plug into the &lt;code&gt;rx&lt;/code&gt; macro.
Here’s a recorded example of what developing it looks like from start to finish, ending up with a functional &lt;code&gt;rx&lt;/code&gt; construct:
&lt;/p&gt;&lt;p&gt;Live-highlighting regex development. Nice. If you add more groups, more colors show up. In this example the rx constructs I’m using are:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Any strings end up as literal matches&lt;/item&gt;&lt;item&gt;Special symbols &lt;code&gt;bol&lt;/code&gt;and&lt;code&gt;eol&lt;/code&gt;for "beginning of line" and "end of line", respectively&lt;/item&gt;&lt;item&gt;Symbols like &lt;code&gt;+&lt;/code&gt;behave like their regex counterparts ("at least one")&lt;/item&gt;&lt;item&gt;Some symbols like &lt;code&gt;not&lt;/code&gt;are nice little shortcuts (in this case, to negate the next form)&lt;/item&gt;&lt;/list&gt;&lt;p&gt; Because &lt;code&gt;rx&lt;/code&gt; is a macro, we don’t ever actually need to compile its regular expressions to use elsewhere - we can always just use &lt;code&gt;rx&lt;/code&gt; when we need a regex.
&lt;/p&gt;&lt;head rend="h4"&gt;Gathering Completions: Continued&lt;/head&gt;&lt;p&gt;Okay, we've cut our teeth on emacs regular expressions. Let's use 'em. (Not our teeth. Regexes.)&lt;/p&gt;&lt;p&gt; To start, let's save our reStructuredText regular expression to find a &lt;code&gt;ref&lt;/code&gt; so we can easily grab it later.
I'll save the one I came up with to the name &lt;code&gt;tmp/re&lt;/code&gt; (this name is arbitrary, I drop temporary variables into &lt;code&gt;tmp/&amp;lt;name&amp;gt;&lt;/code&gt; out of habit)
&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Font used to highlight built-in function names.&lt;/item&gt;&lt;item&gt;Font used to highlight strings.&lt;/item&gt;&lt;item&gt;Font used to highlight keywords.&lt;/item&gt;&lt;/list&gt;&lt;code&gt;(setq tmp/re (rx bol ".." (+ blank) "_" (group (+ (not ":"))) ":" eol))
&lt;/code&gt;&lt;p&gt; Now we can reference it easily. I mentioned before that &lt;code&gt;re-search-forward&lt;/code&gt; accepts a regex, so let's hop into a reStructuredText rev up the regex.
&lt;/p&gt;&lt;p&gt;Here's my sample text that I'll work with:&lt;/p&gt;ReST&lt;list rend="ul"&gt;&lt;item&gt;Font used for directives and roles.&lt;/item&gt;&lt;item&gt;Font used for all other defining constructs.&lt;/item&gt;&lt;item&gt;Font used for the adornment of a section header.&lt;/item&gt;&lt;item&gt;Default font for section title text at level 1.&lt;/item&gt;&lt;/list&gt;&lt;quote&gt;&lt;code&gt;A Title&lt;/code&gt;&lt;code&gt;=======&lt;/code&gt;&lt;code&gt;Beware the Jabberwock, my son.&lt;/code&gt;&lt;code&gt;.. _my-reference:&lt;/code&gt;&lt;code&gt;You are like a little baby. Watch this.&lt;/code&gt;&lt;code&gt;.. _code-sample:&lt;/code&gt;&lt;code&gt;.. code:: python&lt;/code&gt;&lt;code&gt;print("emacs needs telemetry")&lt;/code&gt;&lt;code&gt;The end?&lt;/code&gt;&lt;/quote&gt;&lt;p&gt; The &lt;code&gt;re-search-forward&lt;/code&gt; documentation indicates that it starts at the &lt;code&gt;point&lt;/code&gt;'s current position, so head to the start of the buffer, hit M-: to enter the elisp &lt;code&gt;Eval&lt;/code&gt; prompt, and try:
&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Font used to highlight built-in function names.&lt;/item&gt;&lt;/list&gt;&lt;code&gt;(re-search-forward tmp/re)
&lt;/code&gt;&lt;p&gt;This is anticlimactic because you'll just see the point move to the end of one of the references. BUT. This means that the search succeeded. So… what now?&lt;/p&gt;&lt;p&gt; More reading in the &lt;code&gt;re-search-forward&lt;/code&gt; documentation will educate you about emacs global match data.
In non-functional-programming style, functions like &lt;code&gt;match-beginning&lt;/code&gt; and &lt;code&gt;match-end&lt;/code&gt; serve to interrogate a global state that functions like &lt;code&gt;re-search-forward&lt;/code&gt; will modify.
In concise terms, our regular expression defines one match group and we can grab it with &lt;code&gt;(match-string-no-properties 1)&lt;/code&gt; to get the first group match (&lt;code&gt;match-string&lt;/code&gt; will return a string with "properties", which is a bunch of data like font styling that we don't want).
&lt;/p&gt;&lt;p&gt;Within our example buffer, executing this after the regex search should return our match:&lt;/p&gt;ELisp&lt;list rend="ul"&gt;&lt;item&gt;Font used to highlight function names.&lt;/item&gt;&lt;/list&gt;&lt;code&gt;(match-string-no-properties 1)
&lt;/code&gt;&lt;p&gt; I see &lt;code&gt;"my-reference"&lt;/code&gt; from this command.
Now we're cooking like it's 1985, baby.
You can enter the minibuffer again with M-:, press ↑ to find the &lt;code&gt;re-search-forward&lt;/code&gt; command again, and repeat this process again to watch the point move to the next match, after which you can see the matched string with &lt;code&gt;match-string-no-properties&lt;/code&gt;.
&lt;/p&gt;&lt;p&gt;Note that running this a few times will eventually error out after no matches exist past your point. We'll address this.&lt;/p&gt;&lt;p&gt;If you're a human (or Claude) at this point, you can see the path ahead – we need to write some elisp that will:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Move the point to the beginning of the buffer (important, remember that &lt;code&gt;re-search-forward&lt;/code&gt;relies upon the current position of your point)&lt;/item&gt;&lt;item&gt;Iteratively execute an &lt;code&gt;re-search-forward&lt;/code&gt;command to aggregate reference targets&lt;/item&gt;&lt;item&gt;Conclude when there aren't any more matches&lt;/item&gt;&lt;/list&gt;&lt;p&gt;I'll start with the code and then explain which demons the parentheses are summoning afterward:&lt;/p&gt;ELisp&lt;list rend="ul"&gt;&lt;item&gt;Font used to highlight function names.&lt;/item&gt;&lt;item&gt;Font used to highlight strings.&lt;/item&gt;&lt;item&gt;Font used to highlight special form names.&lt;/item&gt;&lt;item&gt;Font used to highlight built-in function names.&lt;/item&gt;&lt;item&gt;Font used to highlight keywords.&lt;/item&gt;&lt;item&gt;Font used to highlight comments.&lt;/item&gt;&lt;item&gt;Font used to highlight comment delimiters.&lt;/item&gt;&lt;/list&gt;&lt;quote&gt;&lt;code&gt;;; This function will save the current position of the cursor and then&lt;/code&gt;&lt;code&gt;;; return it to this position once the code that it wraps has finished&lt;/code&gt;&lt;code&gt;;; executing, which lets us hop around the buffer without driving the&lt;/code&gt;&lt;code&gt;;; programmer insane. Important for any functions that move the point&lt;/code&gt;&lt;code&gt;;; around.&lt;/code&gt;&lt;code&gt;(save-excursion&lt;/code&gt;&lt;code&gt;;; progn is a simple function that just executes each lisp form&lt;/code&gt;&lt;code&gt;;; step-by-step.&lt;/code&gt;&lt;code&gt;(progn&lt;/code&gt;&lt;code&gt;;; Step one: go to the beginning of the buffer.&lt;/code&gt;&lt;code&gt;(goto-char (point-min))&lt;/code&gt;&lt;code&gt;;; Step two: loop&lt;/code&gt;&lt;code&gt;;;&lt;/code&gt;&lt;code&gt;;; cl-loop is a macro with a long and venerable heritage stemming&lt;/code&gt;&lt;code&gt;;; from the common lisp family of macros, which it mimics the&lt;/code&gt;&lt;code&gt;;; behavior of. You could spend hours honing your ability to wield&lt;/code&gt;&lt;code&gt;;; the common lisp `loop` macro, but we'll just explain the parts&lt;/code&gt;&lt;code&gt;;; we're using:&lt;/code&gt;&lt;code&gt;;;&lt;/code&gt;&lt;code&gt;;; `while` runs the loop until its argument evalutates to a falsy&lt;/code&gt;&lt;code&gt;;; value. We can overload our use of `re-search-forward` here: we&lt;/code&gt;&lt;code&gt;;; can use it to step our loop forward each time and also rely&lt;/code&gt;&lt;code&gt;;; upon it returning `nil` once it stops matching substrings in&lt;/code&gt;&lt;code&gt;;; the buffer and we should finish up.&lt;/code&gt;&lt;code&gt;(cl-loop while (re-search-forward&lt;/code&gt;&lt;code&gt;(rx bol ".." (+ blank) "_" (group (+ (not ":"))) ":" eol)&lt;/code&gt;&lt;code&gt;;; The aforementioned `while` termination case&lt;/code&gt;&lt;code&gt;;; relies upon this `t` parameter, which says&lt;/code&gt;&lt;code&gt;;; "don't error out with no matches, just return&lt;/code&gt;&lt;code&gt;;; nil". Once no more matches are found, the loop&lt;/code&gt;&lt;code&gt;;; exits.&lt;/code&gt;&lt;code&gt;nil t)&lt;/code&gt;&lt;code&gt;;; The `collect` keyword instructs `cl-loop` how to form&lt;/code&gt;&lt;code&gt;;; its return value. We can helpfully summarize the regex&lt;/code&gt;&lt;code&gt;;; match item by pulling out the global match data.&lt;/code&gt;&lt;code&gt;collect (match-string-no-properties 1))))&lt;/code&gt;&lt;/quote&gt;&lt;p&gt;The code is less intimidating without comments:&lt;/p&gt;ELisp&lt;list rend="ul"&gt;&lt;item&gt;Font used to highlight function names.&lt;/item&gt;&lt;item&gt;Font used to highlight strings.&lt;/item&gt;&lt;item&gt;Font used to highlight special form names.&lt;/item&gt;&lt;item&gt;Font used to highlight built-in function names.&lt;/item&gt;&lt;item&gt;Font used to highlight keywords.&lt;/item&gt;&lt;/list&gt;&lt;quote&gt;&lt;code&gt;(save-excursion&lt;/code&gt;&lt;code&gt;(progn&lt;/code&gt;&lt;code&gt;(goto-char (point-min))&lt;/code&gt;&lt;code&gt;(cl-loop while (re-search-forward&lt;/code&gt;&lt;code&gt;(rx bol ".." (+ blank) "_" (group (+ (not ":"))) ":" eol)&lt;/code&gt;&lt;code&gt;nil t)&lt;/code&gt;&lt;code&gt;collect (match-string-no-properties 1))))&lt;/code&gt;&lt;/quote&gt;&lt;p&gt; Without belaboring the point, you can – like I did – discover most of these functions by skimming existing elisp code and using it as a launch pad. Many of these functions are bog standard and show up all over the place in emacs packages (&lt;code&gt;save-excursion&lt;/code&gt;, &lt;code&gt;progn&lt;/code&gt;, &lt;code&gt;goto-char&lt;/code&gt;…)
&lt;/p&gt;&lt;p&gt; Here's the result when I run this code against our example &lt;code&gt;.rst&lt;/code&gt; file:
&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Font used to highlight strings.&lt;/item&gt;&lt;/list&gt;&lt;code&gt;("my-reference" "code-sample")
&lt;/code&gt;&lt;p&gt;Looks good!&lt;/p&gt;&lt;head rend="h4"&gt;Completing the Completion Backend&lt;/head&gt;&lt;p&gt;We're now armed with the ability to:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Identify the bounds of the string we want to replace, and&lt;/item&gt;&lt;item&gt;Collect a list of targets for completion candidates&lt;/item&gt;&lt;/list&gt;&lt;p&gt;We are so close. Recall the description of the variable we need to modify:&lt;/p&gt;&lt;quote&gt;&lt;code&gt;completion-at-point-functions is a variable defined in ‘minibuffer.el’.&lt;/code&gt;&lt;code&gt;Its value is (cape-dict cape-file tags-completion-at-point-function)&lt;/code&gt;&lt;code&gt;Special hook to find the completion table for the entity at point.&lt;/code&gt;&lt;code&gt;Each function on this hook is called in turn without any argument and&lt;/code&gt;&lt;code&gt;should return either nil, meaning it is not applicable at point,&lt;/code&gt;&lt;code&gt;or a function of no arguments to perform completion (discouraged),&lt;/code&gt;&lt;code&gt;or a list of the form (START END COLLECTION . PROPS)&lt;/code&gt;&lt;/quote&gt;&lt;p&gt; To return the list that &lt;code&gt;completion-at-point-functions&lt;/code&gt; expects, we already have the ability to identify the bounds of a &lt;code&gt;thing&lt;/code&gt; and sweep up a list of candidates in our buffer.
Note the comment about returning &lt;code&gt;nil&lt;/code&gt;: we probably don't always want to run our backend, so we should short-circuit our function to eagerly return nil to avoid tying up emacs with a regex loop we don't need.
&lt;/p&gt;&lt;p&gt;With all that said, consider the following:&lt;/p&gt;ELisp&lt;list rend="ul"&gt;&lt;item&gt;Font used to highlight special form names.&lt;/item&gt;&lt;item&gt;Font to highlight quoted Lisp symbols.&lt;/item&gt;&lt;item&gt;Font used to highlight built-in function names.&lt;/item&gt;&lt;item&gt;Font used to highlight function names.&lt;/item&gt;&lt;item&gt;Font used to highlight documentation embedded in program code. It is typically used for special documentation comments or strings.&lt;/item&gt;&lt;item&gt;Font used to highlight function names.&lt;/item&gt;&lt;item&gt;Font used to highlight strings.&lt;/item&gt;&lt;item&gt;Font used to highlight keywords.&lt;/item&gt;&lt;item&gt;Font used to highlight comments.&lt;/item&gt;&lt;item&gt;Font used to highlight comment delimiters.&lt;/item&gt;&lt;/list&gt;&lt;quote&gt;&lt;code&gt;;; Our reStructuredText reference "thing"&lt;/code&gt;&lt;code&gt;(define-thing-chars rst-ref "[:alpha:]_-")&lt;/code&gt;&lt;code&gt;(defun my/rst-internal-reference-capf ()&lt;/code&gt;&lt;code&gt;"Completion backend for buffer reStructuredText references"&lt;/code&gt;&lt;code&gt;;; Only applies when we're within a reference - outside of a&lt;/code&gt;&lt;code&gt;;; reference, we bail out with nil.&lt;/code&gt;&lt;code&gt;(when (looking-back (rx ":ref:`" (* (not "`"))) (point-at-bol))&lt;/code&gt;&lt;code&gt;;; Get potential bounds for the string to replace&lt;/code&gt;&lt;code&gt;(let* ((bounds (or (bounds-of-thing-at-point 'rst-ref)&lt;/code&gt;&lt;code&gt;;; Fallback to the current position&lt;/code&gt;&lt;code&gt;(cons (point) (point))))&lt;/code&gt;&lt;code&gt;(start (car bounds))&lt;/code&gt;&lt;code&gt;(end (cdr bounds))&lt;/code&gt;&lt;code&gt;;; Collect all reference candidates&lt;/code&gt;&lt;code&gt;(candidates&lt;/code&gt;&lt;code&gt;;; Our previously-noted reference collector&lt;/code&gt;&lt;code&gt;(save-excursion&lt;/code&gt;&lt;code&gt;(progn&lt;/code&gt;&lt;code&gt;(goto-char (point-min))&lt;/code&gt;&lt;code&gt;(cl-loop while (re-search-forward&lt;/code&gt;&lt;code&gt;(rx bol ".." (+ blank) "_" (group (+ (not ":"))) ":" eol)&lt;/code&gt;&lt;code&gt;nil t)&lt;/code&gt;&lt;code&gt;collect (match-string-no-properties 1))))))&lt;/code&gt;&lt;code&gt;;; Return value suitable for `completion-at-point-functions`&lt;/code&gt;&lt;code&gt;(list start end candidates))))&lt;/code&gt;&lt;/quote&gt;&lt;list rend="ul"&gt;&lt;item&gt;We're following some naming conventions by calling this a "&lt;code&gt;capf&lt;/code&gt;" (a "completion-at-point function) and prefixing with&lt;code&gt;my/&lt;/code&gt;(a habit to namespace your own functions)&lt;/item&gt;&lt;item&gt;Our short-circuit takes the form of using &lt;code&gt;looking-back&lt;/code&gt;to ask, "are we inside of a reStructuredText reference"? Note the use of&lt;code&gt;rx&lt;/code&gt;here again to clean up our lisp.&lt;/item&gt;&lt;item&gt;We use our &lt;code&gt;rst-ref&lt;/code&gt;&lt;code&gt;thing&lt;/code&gt;to easily snag the&lt;code&gt;start&lt;/code&gt;and&lt;code&gt;end&lt;/code&gt;of the string to replace – note our fallback to just the immediate point if we can't find the bounds of our&lt;code&gt;thing&lt;/code&gt;.&lt;/item&gt;&lt;/list&gt;&lt;p&gt; We wrap it all up with &lt;code&gt;list&lt;/code&gt;.
Personally, even as somebody relatively new to writing Lisps, I find the code pleasant to read and self-evident.
We did a lot in 17 lines of code!
&lt;/p&gt;&lt;p&gt; Inside of our test &lt;code&gt;.rst&lt;/code&gt; buffer, we can test drive this function.
First, invoke M-x &lt;code&gt;eval-defun&lt;/code&gt; with your cursor somewhere in the function to evaluate it, which makes &lt;code&gt;my/rst-internal-reference-capf&lt;/code&gt; available.
Then run:
&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Font to highlight quoted Lisp symbols.&lt;/item&gt;&lt;item&gt;Font used to highlight variable names.&lt;/item&gt;&lt;item&gt;Font used to highlight function names.&lt;/item&gt;&lt;/list&gt;&lt;code&gt;(add-hook 'completion-at-point-functions 'my/rst-internal-reference-capf)
&lt;/code&gt;&lt;p&gt; Huzzah! Our function is now live in emacs' completion framework. You can trigger the completion by calling &lt;code&gt;completion-at-point&lt;/code&gt; at a relevant spot in a buffer.
Many batteries-included emacs distributions like spacemacs or doom emacs slap nice-looking porcelain on top of the completion framework; here's an example that uses the corfu package:
&lt;/p&gt;&lt;p&gt;Congratulations, you've extended emacs for the first time!&lt;/p&gt;&lt;head rend="h4"&gt;Dressing Up the Bones&lt;/head&gt;&lt;p&gt;Okay, this is a pretty basic setup. You could improve it in many ways, but here are a few ideas about potential directions:&lt;/p&gt;&lt;head rend="h5"&gt;Mode Hooks&lt;/head&gt;&lt;p&gt; Manually adding your custom completion function to the &lt;code&gt;completion-at-point-functions&lt;/code&gt; hook is tedious, but there's a way to automate it.
Recall that in emacs parlance, a "hook" is usually a variable that holds a list of functions that get called at a specific time.
&lt;/p&gt;&lt;p&gt; If you use rst-mode, then opening an &lt;code&gt;.rst&lt;/code&gt; file will drop you into &lt;code&gt;rst-mode&lt;/code&gt; and implicitly call the &lt;code&gt;rst-mode-hook&lt;/code&gt; functions.
That means that this line is sufficient to integrate our completion function:
&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Font to highlight quoted Lisp symbols.&lt;/item&gt;&lt;item&gt;Font to highlight Lisp quotes.&lt;/item&gt;&lt;item&gt;Font used to highlight keywords.&lt;/item&gt;&lt;item&gt;Font used to highlight variable names.&lt;/item&gt;&lt;item&gt;Font used to highlight function names.&lt;/item&gt;&lt;/list&gt;&lt;quote&gt;&lt;code&gt;(add-hook 'rst-mode-hook (lambda ()&lt;/code&gt;&lt;code&gt;(add-hook 'completion-at-point-functions #'my/rst-internal-reference-capf 0 t)))&lt;/code&gt;&lt;/quote&gt;&lt;p&gt; This says: "when I open an &lt;code&gt;.rst&lt;/code&gt; file, run this lambda that modifies &lt;code&gt;completion-at-point-functions&lt;/code&gt; only for this buffer by adding my internal reference completion function".
It's a little nested which makes it less obvious with the two &lt;code&gt;add-hook&lt;/code&gt; calls.
&lt;/p&gt;&lt;head rend="h5"&gt;Other Files&lt;/head&gt;&lt;p&gt;Okay, our example works for references in the same buffer but this is sort of pointless for uses across files.&lt;/p&gt;&lt;p&gt;You can solve this too, although my post is already too long so we won't solve this step-by-step. However, here's how I solved it:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Turn my &lt;code&gt;capf&lt;/code&gt;into a minor mode that manages the completion variables&lt;/item&gt;&lt;item&gt;Doesn't search the buffer every time but instead does so once and then rebuilds it with a hook in &lt;code&gt;after-change-functions&lt;/code&gt;, saving it to a hash cache&lt;/item&gt;&lt;item&gt;Walk all &lt;code&gt;.rst&lt;/code&gt;files in the current project and run the reference collection function for each, storing the results into a hash cache for all files that don't have live buffers&lt;/item&gt;&lt;item&gt;When it comes time to call the completion function, combine the hash for completions for files without buffers along with each &lt;code&gt;.rst&lt;/code&gt;buffer's cached list of references&lt;/item&gt;&lt;/list&gt;&lt;p&gt; It sounds complicated, but it works! Functions like &lt;code&gt;with-temp-buffer&lt;/code&gt; make this pretty easy by aggregating reference targets for files using the exact same function we do for live buffers.
&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Font used to highlight built-in function names.&lt;/item&gt;&lt;item&gt;Font used to highlight keywords.&lt;/item&gt;&lt;/list&gt;&lt;quote&gt;&lt;code&gt;(with-temp-buffer&lt;/code&gt;&lt;code&gt;(insert-file-contents file)&lt;/code&gt;&lt;code&gt;(my/rst-internal-references))&lt;/code&gt;&lt;/quote&gt;&lt;head rend="h5"&gt;Fancy Completion&lt;/head&gt;&lt;p&gt; Emacs' long history includes company-mode, which is a third-party completion framework that integrates with the &lt;code&gt;completion-at-point&lt;/code&gt; set of functions.
Some &lt;code&gt;company-mode&lt;/code&gt; features include additional metadata about completion candidates, and I found two that were useful: &lt;code&gt;company-kind&lt;/code&gt; and &lt;code&gt;company-doc-buffer&lt;/code&gt;.
&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;company-kind&lt;/code&gt;is a simple key that just tells the completion caller what the completion cadidate is. In our case we can add some eye candy by indicating it's&lt;code&gt;'text&lt;/code&gt;.&lt;/item&gt;&lt;item&gt;&lt;code&gt;company-doc-buffer&lt;/code&gt;lets us add additional context to a completion candidate. I leveraged this to include a couple of lines following the reference line to help me figure out what exactly the link refers to. It's easier to show what this looks like rather than tell:&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Notes:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;I'm using GUI emacs here for the nicer completion popup with corfu which displays a transparent, floating frame&lt;/item&gt;&lt;item&gt;My completion candidate "context" is a real excerpt from the text around the reference, complete with styling, etc.&lt;/item&gt;&lt;item&gt;The small icon to the left of each candidate comes from the &lt;code&gt;company-kind&lt;/code&gt;attribute.&lt;/item&gt;&lt;item&gt;The &lt;code&gt;~&lt;/code&gt;syntax is part of orderless&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Completion candidate context is an extra frill but very helpful.&lt;/p&gt;&lt;head rend="h4"&gt;Summary&lt;/head&gt;&lt;p&gt;My experience extending a core emacs function was an instructive and interesting exercise. I don't know what the future of emacs looks like in an increasingly LLM-crazed world, but I hope that future includes an open and powerful way to extend and customize the tools we use to write software.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45223239</guid></item><item><title>Advanced Scheme Techniques (2004) [pdf]</title><link>https://people.csail.mit.edu//jhbrown/scheme/continuationslides04.pdf</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45223419</guid></item><item><title>Ankit Gupta Joins YC as General Partner</title><link>https://www.ycombinator.com/blog/welcome-ankit/</link><description>&lt;doc fingerprint="14289b4594b78f68"&gt;
  &lt;main&gt;
    &lt;p&gt;by Garry Tan9/12/2025&lt;/p&gt;
    &lt;p&gt;We’re thrilled to announce that Ankit Gupta is joining YC as our newest General Partner.&lt;/p&gt;
    &lt;p&gt;Ankit has already worked with dozens of YC founders as a visiting partner in recent batches. He has a rare blend of deep machine learning expertise and firsthand startup experience that make him an ideal mentor for founders building at the bleeding edge of AI. Though he'll frequently commute to SF, Ankit will primarily be based out of Cambridge, Massachusetts — re-establishing a YC foothold at the site of our original office!&lt;/p&gt;
    &lt;p&gt;Ankit’s YC journey began in the Winter 2018 batch, when he co-founded Reverie Labs, a biotech company applying machine learning to drug discovery. The team at Reverie developed machine learning models for small-molecule design, partnered with pharmaceutical companies, and advanced their own medicines before eventually being acquired by Ginkgo Bioworks in 2024.&lt;/p&gt;
    &lt;p&gt;Before founding Reverie, Ankit earned a B.A. and M.S. in Computer Science at Harvard and published research at ICML and other conferences on deep learning and large-scale model training. It’s that blend of research and founder experience that makes Ankit uniquely equipped to help the next generation of AI startups succeed.&lt;/p&gt;
    &lt;p&gt;We are super excited to have him and are certain his expertise will continue to support YC founders at every stage. Welcome, Ankit!&lt;/p&gt;
    &lt;p&gt;Categories&lt;/p&gt;
    &lt;p&gt;Other Posts&lt;/p&gt;
    &lt;p&gt;Garry is the President &amp;amp; CEO of Y Combinator. Previously, he was the co-founder &amp;amp; Managing Partner of Initialized Capital. Before that, he co-founded Posterous (YC S08) which was acquired by Twitter.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45223603</guid></item><item><title>OpenAI Grove</title><link>https://openai.com/index/openai-grove/</link><description>&lt;doc fingerprint="b4871141422eeff0"&gt;
  &lt;main&gt;
    &lt;p&gt;Today, we’re announcing the OpenAI Grove, a new program for technical talent at the very start of their company-building journey. The Grove is not a startup accelerator or traditional program: it offers pre-idea individuals deeply curious about building in AI a dense talent network, co-building with OpenAI researchers, and resources designed to accelerate your journey. As participants explore early concepts, they will receive counsel from the OpenAI team and community with peers in OpenAI Grove. &lt;lb/&gt;This program is the starting point of a long-term network. It will begin with five weeks of content and programming hosted in the OpenAI San Francisco HQ, including in-person workshops, weekly office hours, and mentoring from OpenAI technical leaders. In addition to technical support and community, participants will also have the opportunity to get hands-on with new OpenAI tools and models prior to general availability. Following the program, participants will be able to explore raising capital or pursue another avenue, internally or externally to OpenAI. &lt;/p&gt;
    &lt;p&gt;The first Grove cohort will consist of approximately fifteen participants. We recommend individuals from all backgrounds, disciplines, and experience levels apply. To apply, please submit the form below by September 24th, 2025.&lt;/p&gt;
    &lt;p&gt;Applications are due by September 24th, 2025.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45223660</guid></item><item><title>VaultGemma: The most capable differentially private LLM</title><link>https://research.google/blog/vaultgemma-the-worlds-most-capable-differentially-private-llm/</link><description>&lt;doc fingerprint="3e58915235154e3a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;VaultGemma: The world's most capable differentially private LLM&lt;/head&gt;
    &lt;p&gt;September 12, 2025&lt;/p&gt;
    &lt;p&gt;Amer Sinha, Software Engineer, and Ryan McKenna, Research Scientist, Google Research&lt;/p&gt;
    &lt;p&gt;We introduce VaultGemma, the most capable model trained from scratch with differential privacy.&lt;/p&gt;
    &lt;head rend="h2"&gt;Quick links&lt;/head&gt;
    &lt;p&gt;As AI becomes more integrated into our lives, building it with privacy at its core is a critical frontier for the field. Differential privacy (DP) offers a mathematically robust solution by adding calibrated noise to prevent memorization. However, applying DP to LLMs introduces trade-offs. Understanding these trade-offs is crucial. Applying DP noise alters traditional scaling laws — rules describing performance dynamics — by reducing training stability (the model's ability to learn consistently without experiencing catastrophic events like loss spikes or divergence) and significantly increasing batch size (a collection of input prompts sent to the model simultaneously for processing) and computation costs.&lt;/p&gt;
    &lt;p&gt;Our new research, “Scaling Laws for Differentially Private Language Models”, conducted in partnership with Google DeepMind, establishes laws that accurately model these intricacies, providing a complete picture of the compute-privacy-utility trade-offs. Guided by this research, we’re excited to introduce VaultGemma, the largest (1B-parameters), open model trained from scratch with differential privacy. We are releasing the weights on Hugging Face and Kaggle, alongside a technical report, to advance the development of the next generation of private AI.&lt;/p&gt;
    &lt;head rend="h2"&gt;Understanding the scaling laws&lt;/head&gt;
    &lt;p&gt;With a carefully thought-out experimental methodology, we aimed to quantify the benefit of increasing model sizes, batch sizes, and iterations in the context of DP training. Our work required making some simplifying assumptions to overcome the exponential number of combinations one might consider trying. We assumed that how well the model learns depends mostly on the "noise-batch ratio” which compares the amount of random noise we add for privacy to the size of the data groups (batches) we use for training. This assumption works because the privacy noise we add is much greater than any natural randomness that comes from sampling the data.&lt;/p&gt;
    &lt;p&gt;To establish a DP scaling law, we conducted a comprehensive set of experiments to evaluate performance across a variety of model sizes and noise-batch ratios. The resulting empirical data, together with known deterministic relationships between other variables, allows us to answer a variety of interesting scaling-laws–style queries, such as, “For a given compute budget, privacy budget, and data budget, what is the optimal training configuration to achieve the lowest possible training loss?”&lt;/p&gt;
    &lt;head rend="h2"&gt;Key findings: A powerful synergy&lt;/head&gt;
    &lt;p&gt;Before diving into the full scaling laws, it’s useful to understand the dynamics and synergies between the compute budget, privacy budget, and data budget from a privacy accounting perspective — i.e., understand how these factors influence the noise-batch ratio for a fixed model size and number of iterations. This analysis is significantly cheaper to do as it does not require any model training, yet it yields a number of useful insights. For instance, increasing the privacy budget in isolation leads to diminishing returns, unless coupled with a corresponding increase in either the compute budget (FLOPs) or data budget (tokens).&lt;/p&gt;
    &lt;p&gt;To explore this synergy further, the visualization below shows how the optimal training configuration changes based on different constraints. As the privacy and compute budgets change, notice how the recommendation shifts between investing in a larger model versus training with larger batch sizes or more iterations.&lt;/p&gt;
    &lt;p&gt;This data provides a wealth of useful insights for practitioners. While all the insights are reported in the paper, a key finding is that one should train a much smaller model with a much larger batch size than would be used without DP. This general insight should be unsurprising to a DP expert given the importance of large batch sizes. While this general insight holds across many settings, the optimal training configurations do change with the privacy and data budgets. Understanding the exact trade-off is crucial to ensure that both the compute and privacy budgets are used judiciously in real training scenarios. The above visualizations also reveal that there is often wiggle room in the training configurations — i.e., a range of model sizes might provide very similar utility if paired with the correct number of iterations and/or batch size.&lt;/p&gt;
    &lt;head rend="h2"&gt;Applying the scaling laws to build VaultGemma&lt;/head&gt;
    &lt;p&gt;The Gemma models are designed with responsibility and safety at their core. This makes them a natural foundation for developing a production-quality, DP-trained model like VaultGemma.&lt;/p&gt;
    &lt;head rend="h3"&gt;Algorithmic advancements: Training at scale&lt;/head&gt;
    &lt;p&gt;The scaling laws we derived above represent an important first step towards training a useful Gemma model with DP. We used the scaling laws to determine both how much compute we needed to train a compute-optimal 1B parameter Gemma 2-based model with DP, and how to allocate that compute among batch size, iterations, and sequence length to achieve the best utility.&lt;/p&gt;
    &lt;p&gt;One prominent gap between the research underlying the scaling laws and the actual training of VaultGemma was our handling of Poisson sampling, which is a central component of DP-SGD. We initially used a straightforward method of loading data in uniform batches but then switched to Poisson sampling to get the best privacy guarantees with the least amount of noise. This method posed two main challenges: it created batches of different sizes, and it required a specific, randomized order for processing the data. We solved this by using our recent work on Scalable DP-SGD, which allows us to process data in fixed-size batches — either by adding extra padding or trimming them — while still maintaining strong privacy protections.&lt;/p&gt;
    &lt;head rend="h2"&gt;Results&lt;/head&gt;
    &lt;p&gt;Armed with our new scaling laws and advanced training algorithms, we built VaultGemma, to date the largest (1B-parameters) open model fully pre-trained with differential privacy with an approach that can yield high-utility models.&lt;/p&gt;
    &lt;p&gt;From training VaultGemma, we found our scaling laws to be highly accurate. The final training loss of VaultGemma was remarkably close to what our equations predicted, validating our research and providing the community with a reliable roadmap for future private model development.&lt;/p&gt;
    &lt;p&gt;We also compare downstream performance of our model against its non-private counterpart across a range of standard academic benchmarks (i.e., HellaSwag, BoolQ, PIQA, SocialIQA, TriviaQA, ARC-C, ARC-E ). To put this performance in perspective and quantify the current resource investment required for privacy, we also include a comparison to an older similar-sized GPT-2 model, which performs similarly on these benchmarks. This comparison illustrates that today’s private training methods produce models with utility comparable to that of non-private models from roughly 5 years ago, highlighting the important gap our work will help the community systematically close.&lt;/p&gt;
    &lt;p&gt;Finally, the model comes with strong theoretical and empirical privacy protections.&lt;/p&gt;
    &lt;head rend="h3"&gt;Formal privacy guarantee&lt;/head&gt;
    &lt;p&gt;In general, both the privacy parameters (ε, δ) and the privacy unit are important considerations when doing DP training, as these together determine what the trained model can learn. VaultGemma was trained with a sequence-level DP guarantee of (ε ≤ 2.0, δ ≤ 1.1e-10), where a sequence consists of 1024 consecutive tokens extracted from heterogeneous data sources. Specifically, we used the same training mixture that was used to train the Gemma 2 model, consisting of a number of documents of varying lengths. During pre-processing, long documents are split up and tokenized into multiple sequences, and shorter documents are packed together into a single sequence. While the sequence-level privacy unit was a natural choice for our training mixture, in situations where there is a clear mapping between data and users, user-level differential privacy would be a better choice.&lt;/p&gt;
    &lt;p&gt;What does this mean in practice? Informally speaking, because we provide protection at the sequence level, if information relating to any (potentially private) fact or inference occurs in a single sequence, then VaultGemma essentially does not know that fact: the response to any query will be statistically similar to the result from a model that never trained on the sequence in question. However, if many training sequences contain information relevant to a particular fact, then in general VaultGemma will be able to provide that information.&lt;/p&gt;
    &lt;head rend="h3"&gt;Empirical memorization&lt;/head&gt;
    &lt;p&gt;Sequence-level DP provably bounds the influence of any single training sequence (example) on the final model. We prompted the model with a 50-token prefix from a training document to see if it would generate the corresponding 50-token suffix. VaultGemma 1B shows no detectable memorization of its training data and successfully demonstrates the efficacy of DP training.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;VaultGemma represents a significant step forward in the journey toward building AI that is both powerful and private by design. By developing and applying a new, robust understanding of the scaling laws for DP, we have successfully trained and released the largest open, DP-trained language model to date.&lt;/p&gt;
    &lt;p&gt;While a utility gap still exists between DP-trained and non-DP–trained models, we believe this gap can be systematically narrowed with more research on mechanism design for DP training. We hope that VaultGemma and our accompanying research will empower the community to build the next generation of safe, responsible, and private AI for everyone.&lt;/p&gt;
    &lt;head rend="h2"&gt;Acknowledgements&lt;/head&gt;
    &lt;p&gt;We'd like to thank the entire Gemma and Google Privacy teams for their contributions and support throughout this project, in particular, Peter Kairouz, Brendan McMahan and Dan Ramage for feedback on the blog post, Mark Simborg and Kimberly Schwede for help with visualizations, and the teams at Google that helped with algorithm design, infrastructure implementation, and production maintenance. The following people directly contributed to the work presented here (ordered alphabetically): Borja Balle, Zachary Charles, Christopher A. Choquette-Choo, Lynn Chua, Prem Eruvbetine, Badih Ghazi, Steve He, Yangsibo Huang, Armand Joulin, George Kaissis, Pritish Kamath, Ravi Kumar, Daogao Liu, Ruibo Liu, Pasin Manurangsi, Thomas Mesnard, Andreas Terzis, Tris Warkentin, Da Yu, and Chiyuan Zhang.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45223726</guid></item><item><title>Vector database that can index 1B vectors in 48M</title><link>https://www.vectroid.com/blog/why-and-how-we-built-Vectroid</link><description>&lt;doc fingerprint="1b10db7e119a9f88"&gt;
  &lt;main&gt;
    &lt;p&gt;We are excited to announce Vectroid, a serverless vector search solution that delivers exceptional accuracy and low latency in a cost effective package. Vectroid is not just another vector search solution—it’s a search engine that performs and scales in all scenarios.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why we built Vectroid&lt;/head&gt;
    &lt;p&gt;Talk to any team working with large, low latency vector workloads and you’ll hear a familiar story: something always has to give. Vector databases often make significant tradeoffs between speed, accuracy, and cost. That’s the nature of the mathematical underpinnings of vector search works—taking algorithmic shortcuts to get near-perfect results in a short amount of time.&lt;/p&gt;
    &lt;p&gt;There are some common permutations of these tradeoffs:&lt;/p&gt;
    &lt;p&gt;Based on the existing vector database landscape, it would seem that building a cost effective vector database requires sacrificing either speed or accuracy at scale. We believe that’s a false pretense: building a cost-efficient database is possible with high accuracy and low latency. We just need to rethink our underlying mechanism.&lt;/p&gt;
    &lt;head rend="h3"&gt;Our “aha” moment&lt;/head&gt;
    &lt;p&gt;Query speed and recall are largely a function of the chosen ANN algorithm. Algorithms which are both fast and accurate like HNSW (Hierarchical Navigable Small Worlds) are memory intensive and expensive to index. The traditional assumption is that these types of algorithms are untenable for a cost-conscious system.&lt;/p&gt;
    &lt;p&gt;We had two major realizations which challenged this assumption.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Demand for in-memory HNSW is not static. Real world usage patterns are bursty and uneven. A cost efficient database can optimize for this reality by making resource allocation dynamic and by individually scaling system components as needed.&lt;/item&gt;
      &lt;item&gt;HNSW’s memory footprint is tunable. It can be easily be flattened (ex. by compressing vectors using quantization) and expanded (ex. by increasing layer count), which gives us the flexibility to experiment with different configurations to find a goldilocks setup.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;What is Vectroid?&lt;/head&gt;
    &lt;p&gt;Vectroid is a serverless vector database with premium performance. It delivers the same or stronger balance of speed and recall promised by high-end offerings, but costs less than competitors.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Performant vector search: HNSW for ultra fast, high recall similarity search.&lt;/item&gt;
      &lt;item&gt;Near real-time search capabilities: Newly ingested records are searchable almost instantly.&lt;/item&gt;
      &lt;item&gt;Massive scalability: Seamlessly handles billions of vectors in a single namespace.&lt;/item&gt;
      &lt;item&gt;Cost efficient resource utilization: Scaling each layer (ingest, index, query) separately.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;How Vectroid performs&lt;/head&gt;
    &lt;p&gt;The core philosophy of Vectroid is that optimizing for one metric at any cost to the others doesn’t make for a robust system. Instead, vector search should be designed for balanced performance across recall, latency, and cost so users don’t have to make painful tradeoffs as workloads grow.&lt;/p&gt;
    &lt;p&gt;When tested against other state-of-the-art vector search, Vectroid is not only competitive but the most consistent across the board. Across all of our tests, Vectroid is the only databases that was able to maintain over 90% recall while scaling to 10 query threads per second—all while maintaining good latency scores.&lt;/p&gt;
    &lt;head rend="h3"&gt;Some early benchmarks:&lt;/head&gt;
    &lt;p&gt;We’ll be releasing the full benchmark suite (with setup details so anyone can reproduce them) in an upcoming post. For now, these numbers highlight the kind of scale and performance we designed Vectroid to handle.&lt;/p&gt;
    &lt;head rend="h3"&gt;How Vectroid works&lt;/head&gt;
    &lt;p&gt;Vectroid is composed of two independently scalable microservices for writes and reads.&lt;/p&gt;
    &lt;p&gt;As the diagram shows, index state, vector data, and metadata are persisted to cloud object storage (GCS for now, S3 coming soon). Disk, cache, and in-memory storage layers each employ a usage-aware model for index lifecycle in which indexes are lazily loaded from object storage on demand and evicted when idle.&lt;/p&gt;
    &lt;p&gt;For fast, high-recall ANN search, we chose the HNSW algorithm. It offers excellent latency and accuracy tradeoffs, supports incremental updates, and performs well across large-scale workloads. To patch its limitations, we added a number of targeted optimizations:&lt;/p&gt;
    &lt;head rend="h3"&gt;Final Thoughts&lt;/head&gt;
    &lt;p&gt;We’re just getting started. If you’re building applications that rely on fast, scalable vector search (or you’re running up against the limits of your current stack), we’d love to hear from you. Start using Vectroid today or sign up for our newsletter to follow along as we continue building.&lt;/p&gt;
    &lt;p&gt;Written by Talip Ozturk&lt;/p&gt;
    &lt;p&gt;Founder of Vectroid&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45224141</guid></item><item><title>Humanely Dealing with Humungus Crawlers</title><link>https://flak.tedunangst.com/post/humanely-dealing-with-humungus-crawlers</link><description>&lt;doc fingerprint="a46841793b2754f7"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;humanely dealing with humungus crawlers&lt;/head&gt;
    &lt;p&gt;I host a bunch of hobby code on my server. I would think it’s really only interesting to me, but it turns out every day, thousands of people from all over the world are digging through my code, reviewing years old changesets. On the one hand, wow, thanks, this is very flattering. On the other hand, what the heck is wrong with you?&lt;/p&gt;
    &lt;p&gt;This has been building up for a while, and I’ve been intermittently developing and deploying countermeasures. It’s been a lot like solving a sliding block puzzle. Lots of small moves and changes, and eventually it starts coming together.&lt;/p&gt;
    &lt;p&gt;My primary principle is that I’d rather not annoy real humans more than strictly intended. If there’s a challenge, it shouldn’t be too difficult, but ideally, we want to minimize the number of challenges presented. You should never suspect that I suspected you of being an enemy agent.&lt;/p&gt;
    &lt;p&gt;First measure is we only challenge on the deep URLs. So, for instance, I can link to the anticrawl repo no problem, or even the source for anticrawl.go, and that’ll be served immediately. All the pages any casual browser would visit make up less than 1% of the possible URLs that exist, but probably contain 99% of the interesting content.&lt;/p&gt;
    &lt;p&gt;Also, these pages get cached by the reverse proxy first, so anticrawl doesn’t even evaluate them. We’ve already done the work to render the page, and we’re trying to shed load, so why would I want to increase load by generating challenges and verifying responses? It annoys me when I click a seemingly popular blog post and immediately get challenged, when I’m 99.9% certain that somebody else clicked it two seconds before me. Why isn’t it in cache? We must have different objectives in what we’re trying to accomplish. Or who we’re trying to irritate.&lt;/p&gt;
    &lt;p&gt;The next step is that anybody loading &lt;code&gt;style.css&lt;/code&gt; gets marked friendly. Big Basilisk doesn’t care about my artisanal styles, but most everybody else loves them. So if you start at a normal page, and then start clicking deeper, that’s fine, still no challenge. (Sorry lynx browsers, but don’t worry, it’s not game over for you yet.)&lt;/p&gt;
    &lt;p&gt;And then let’s say somebody directly links to a changeset like /r/vertigo/v/b5ea481ff167. The first visitor will probably hit a challenge, but then we record that URL as in use. The bots are shotgun crawling all over the place, but if a single link is visited more than once, I’ll assume it’s human traffic, and bypass the challenge. No promises, but clicking that link will mostly likely just return content, no challenge.&lt;/p&gt;
    &lt;p&gt;The very first version of anticrawl relied on a weak POW challenge (find a SHA hash with first byte 42), just to get something launched, but this does seem counter intuitive. Why are we making humans solve a challenge optimized for machines? Instead I have switched to a much more diabolical challenge. You are asked how many Rs in strawberry. Or maybe something else. To be changed as necessary. But really, the key observation is that any challenge, anything at all, easily sheds like 99.99% of the crawling load.&lt;/p&gt;
    &lt;p&gt;Notably, because the challenge does not include its own javascript solver, even a smart crawler isn’t going to solve it automatically. If you include the solution on the challenge page, at least some bots are going to use it. All anticrawl challenges now require some degree of contemplation, not just blind interpretation.&lt;/p&gt;
    &lt;p&gt;It took a few iterations because the actual deployment involves a few pieces. I had to reduce the &lt;code&gt;style.css&lt;/code&gt; cache time, so that visitors would periodically refresh it (and thus their humanity). And then exclude it from the caching proxy, so that the request would be properly observed. Basically, a few minutes tinkering now and then while I wait for my latte to arrive, and now I think I’ve gotten things to the point where it’s unlikely to burden anybody except malignant crawlers.&lt;/p&gt;
    &lt;head rend="h3"&gt;elsewhere&lt;/head&gt;
    &lt;p&gt;I have focused my bot detection efforts on humungus because the ratio of crawler to legit traffic was out of control. But now that I know what to look for, I see the same patterns scraping everywhere else. Seems really unlikely a worldwide colelctive of Opera users is suddenly interested in my old honks. I’m starting to deploy similar countermeasures.&lt;/p&gt;
    &lt;head rend="h3"&gt;appendix&lt;/head&gt;
    &lt;p&gt;Some log samples. There’s always somebody to insist these could be real humans, and I have somehow misjudged them. Make your own decision.&lt;/p&gt;
    &lt;p&gt;&lt;head&gt;logs&lt;/head&gt;&lt;code&gt;136.158.49.199 810.886µs humungus.tedunangst.com [2025/09/07 11:31:06] "GET /r/old-flak/v/d720c11fbb57 HTTP/1.1" 402 904 "" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36"
179.42.10.152 831.235µs humungus.tedunangst.com [2025/09/07 11:31:31] "GET /r/flak/v/8b31c923ca0f HTTP/1.1" 402 900 "" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36"
43.128.250.84 863.565µs humungus.tedunangst.com [2025/09/07 11:31:32] "GET /r/vertigo/v/71df18bb3819 HTTP/1.1" 402 961 "" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/133.0.0.0 Safari/537.36"
78.182.153.38 639.086µs humungus.tedunangst.com [2025/09/07 11:31:46] "GET /r/gerc/v/692abbdefe18 HTTP/1.1" 402 900 "" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36"
119.28.100.182 525.152µs humungus.tedunangst.com [2025/09/07 11:31:47] "GET /r/honk3/v/462b7440c563 HTTP/1.1" 402 959 "" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36"
185.163.26.50 678.609µs humungus.tedunangst.com [2025/09/07 11:31:49] "GET /r/azorius/v/b48a3aa3e060 HTTP/1.1" 402 961 "" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
43.167.157.150 758.749µs humungus.tedunangst.com [2025/09/07 11:32:01] "GET /r/humungus/v/0442f94c95fc HTTP/1.1" 402 904 "" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36"
43.134.75.63 574.875µs humungus.tedunangst.com [2025/09/07 11:32:03] "GET /r/azorius/v/969314b9f388 HTTP/1.1" 402 903 "" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36" 
119.28.203.219 497.67µs humungus.tedunangst.com [2025/09/07 11:32:04] "GET /r/gerc/v/8ddbf7307214 HTTP/1.1" 402 900 "" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36"
43.128.84.91 727.05µs humungus.tedunangst.com [2025/09/07 11:32:06] "GET /r/vertigo/v/eb31940f6fa2 HTTP/1.1" 402 903 "" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/133.0.0.0 Safari/537.36"
178.163.207.155 566.9µs humungus.tedunangst.com [2025/09/07 11:32:09] "GET /r/lua-tedu/v/300e67089469 HTTP/1.1" 402 904 "" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36"
181.120.225.184 561.48µs humungus.tedunangst.com [2025/09/07 11:32:12] "GET /r/azorius/v/43120b8aac5a HTTP/1.1" 402 961 "" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
150.109.20.69 612.716µs humungus.tedunangst.com [2025/09/07 11:32:13] "GET /r/gojxl/v/tip/f/pool.go HTTP/1.1" 404 19 "" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
49.51.170.84 629.056µs humungus.tedunangst.com [2025/09/07 11:32:27] "GET /r/gerc/v/41b8b28ee893 HTTP/1.1" 402 958 "" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36"
5.62.146.6 843.157µs humungus.tedunangst.com [2025/09/07 11:32:33] "GET /r/honk/v/f6b8a7bee881 HTTP/1.1" 402 900 "" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/133.0.0.0 Safari/537.36"
129.226.112.105 595.173µs humungus.tedunangst.com [2025/09/07 11:32:40] "GET /r/azorius/v/6179155ac315 HTTP/1.1" 402 903 "" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.81 Safari/537.36"
43.167.204.48 514.371µs humungus.tedunangst.com [2025/09/07 11:32:42] "GET /r/vertigo/v/fae0082c32c2 HTTP/1.1" 402 961 "" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36"
43.132.108.190 715.327µs humungus.tedunangst.com [2025/09/07 11:32:56] "GET /r/vertigo/v/8b5fcd06f8c6 HTTP/1.1" 402 903 "" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36"&lt;/code&gt;&lt;lb/&gt;The big brain solution is you just cache all these requests, but unfortunately I have slightly less than 2TB RAM. Dealing with these relentless scans renders the cache for actual content less useful because everything gets LRUd out.&lt;/p&gt;
    &lt;p&gt;Take a look at this guy. Apparently a pro starcraft player has taken up speed browsing as a side hustle, middle clicking ten times per second. These links aren’t even on the same page, so he’s switching tabs between clicks, too. Amazing. But he doesn’t solve a single challenge. I thought gamers liked puzzles? Maybe that’s why this totally real human quit gaming.&lt;/p&gt;
    &lt;p&gt;&lt;head&gt;logs&lt;/head&gt;&lt;code&gt;46.183.108.190 1.361957ms humungus.tedunangst.com [2025/07/08 13:41:32] "GET /r/azorius/v/a0824eb087c7 HTTP/1.1" 402 1808 "" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:140.0) Gecko/20100101 Firefox/140.0"
46.183.108.190 2.450003ms humungus.tedunangst.com [2025/07/08 13:41:32] "GET /r/azorius/v/1a4d35ff94ef HTTP/1.1" 402 1808 "" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:140.0) Gecko/20100101 Firefox/140.0"
46.183.108.190 3.049854ms humungus.tedunangst.com [2025/07/08 13:41:32] "GET /r/azorius/v/9ca7ed390641 HTTP/1.1" 402 1808 "" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:140.0) Gecko/20100101 Firefox/140.0"
46.183.108.190 3.215804ms humungus.tedunangst.com [2025/07/08 13:41:32] "GET /r/humungus/v/67e77258e203 HTTP/1.1" 402 1809 "" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:140.0) Gecko/20100101 Firefox/140.0"
46.183.108.190 3.26703ms humungus.tedunangst.com [2025/07/08 13:41:32] "GET /r/honk/v/2fc6f904deaa HTTP/1.1" 402 1805 "" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:140.0) Gecko/20100101 Firefox/140.0"
46.183.108.190 830.924µs humungus.tedunangst.com [2025/07/08 13:41:32] "GET /r/humungus/v/1437b0d26457 HTTP/1.1" 402 1809 "" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:140.0) Gecko/20100101 Firefox/140.0"
46.183.108.190 1.23004ms humungus.tedunangst.com [2025/07/08 13:41:32] "GET /r/honk/v/945572a3b51d HTTP/1.1" 402 1805 "" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:140.0) Gecko/20100101 Firefox/140.0"
46.183.108.190 789.496µs humungus.tedunangst.com [2025/07/08 13:41:32] "GET /r/humungus/v/63f9c1f17606 HTTP/1.1" 402 1809 "" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:140.0) Gecko/20100101 Firefox/140.0"
46.183.108.190 841.414µs humungus.tedunangst.com [2025/07/08 13:41:32] "GET /r/humungus/v/b5711a883e66 HTTP/1.1" 402 1809 "" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:140.0) Gecko/20100101 Firefox/140.0"
46.183.108.190 916.233µs humungus.tedunangst.com [2025/07/08 13:41:32] "GET /r/humungus/v/a5307922b3f5 HTTP/1.1" 402 1809 "" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:140.0) Gecko/20100101 Firefox/140.0"
46.183.108.190 565.518µs humungus.tedunangst.com [2025/07/08 13:41:33] "GET /r/honk/v/ab1e84cac5e6 HTTP/1.1" 402 1805 "" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:140.0) Gecko/20100101 Firefox/140.0"
46.183.108.190 574.083µs humungus.tedunangst.com [2025/07/08 13:41:33] "GET /r/honk/v/a9043d011e41 HTTP/1.1" 402 1805 "" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:140.0) Gecko/20100101 Firefox/140.0"
46.183.108.190 602.026µs humungus.tedunangst.com [2025/07/08 13:41:33] "GET /r/azorius/v/1cc1393b6832 HTTP/1.1" 402 1808 "" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:140.0) Gecko/20100101 Firefox/140.0"
46.183.108.190 497.831µs humungus.tedunangst.com [2025/07/08 13:41:33] "GET /r/azorius/v/4d53be2bdbd5 HTTP/1.1" 402 1808 "" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:140.0) Gecko/20100101 Firefox/140.0"
46.183.108.190 516.365µs humungus.tedunangst.com [2025/07/08 13:41:33] "GET /r/honk/v/302e58335796 HTTP/1.1" 402 1805 "" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:140.0) Gecko/20100101 Firefox/140.0"
46.183.108.190 614.239µs humungus.tedunangst.com [2025/07/08 13:41:33] "GET /r/azorius/v/1a4d35ff94ef HTTP/1.1" 402 1808 "" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:140.0) Gecko/20100101 Firefox/140.0"
46.183.108.190 530.161µs humungus.tedunangst.com [2025/07/08 13:41:33] "GET /r/azorius/v/a0824eb087c7 HTTP/1.1" 402 1808 "" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:140.0) Gecko/20100101 Firefox/140.0"
46.183.108.190 625.91µs humungus.tedunangst.com [2025/07/08 13:41:33] "GET /r/azorius/v/9ca7ed390641 HTTP/1.1" 402 1808 "" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:140.0) Gecko/20100101 Firefox/140.0"
46.183.108.190 757.497µs humungus.tedunangst.com [2025/07/08 13:41:33] "GET /r/humungus/v/67e77258e203 HTTP/1.1" 402 1809 "" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:140.0) Gecko/20100101 Firefox/140.0"
46.183.108.190 841.494µs humungus.tedunangst.com [2025/07/08 13:41:33] "GET /r/vertigo/v/6b3ffb3b21f5 HTTP/1.1" 402 1808 "" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:140.0) Gecko/20100101 Firefox/140.0"&lt;/code&gt;&lt;lb/&gt;I think there’s a common perception that 10 req/s just isn’t that much, based on some simple benchmarks. But that doesn’t account for TLS, etc. 10 handshakes/s requires a bit more juice than GET hello. I’ve worked to keep response times in the low millisecond range, just seems like good sense, but I think people should be allowed to program in slower languages and frameworks. You shouldn’t need a fairly substantial EPYC server like I have, either.&lt;/p&gt;
    &lt;p&gt;And there’s always stuff happening in the background. Mastodon hits me once a second every time anybody deletes something. Lemmy hits me twice a second every time somebody likes anything. There’s a bunch of nitwits with misconfigured RSS readers.&lt;/p&gt;
    &lt;p&gt;A 4x overhead in one area doesn’t matter, but a 1/4 as powerful CPU, with 1/4 as many cores, and 1/4 as fast language, all of which are entirely realistic, and pretty soon we’re running close to the edge.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45224246</guid></item><item><title>How FOSS Projects Handle Legal Takedown Requests</title><link>https://f-droid.org/2025/09/10/how-foss-projects-handle-legal-takedown-requests.html</link><description>&lt;doc fingerprint="f21a83016ff37250"&gt;
  &lt;main&gt;&lt;head rend="h2"&gt;How FOSS Projects Handle Legal Takedown Requests&lt;/head&gt;Posted on Sep 10, 2025 by F-Droid&lt;p&gt;When a legal takedown request arrives, whether itâs about copyright, censorship, privacy, or something more vague, how a Free and Open Source Software (FOSS) project responds can make all the difference.&lt;/p&gt;&lt;p&gt;Handled well, a takedown request can be a manageable administrative step. Handled poorly, it can cause panic, disrupt infrastructure, or even put contributors at legal risk.&lt;/p&gt;&lt;p&gt;As part of our legal resilience research, we spoke with a range of legal experts, software freedom advocates, and maintainers of mature FOSS infrastructure to understand how others manage these moments. In this article, we share what we learned, and how F-Droid is incorporating these lessons into its own approach.&lt;/p&gt;&lt;head rend="h2"&gt;A Pattern Emerges&lt;/head&gt;&lt;p&gt;Despite differences in jurisdiction, size, and mission, a few common themes from our research emerged when we asked how other projects handle takedown requests:&lt;/p&gt;&lt;head rend="h3"&gt;1. Donât Be a Soft Target&lt;/head&gt;&lt;p&gt;Legal threats often follow the path of least resistance. FOSS projects that publish a formal takedown policy, require legal submissions through specific channels, and insist on a valid legal basis are much less likely to receive, or comply with, vague or harassing demands.&lt;/p&gt;&lt;p&gt;One FOSS organization, for example, requires all legal correspondence to be submitted by postal mail in the national language and citing local law. Most complaints evaporate once asked to comply.&lt;/p&gt;&lt;head rend="h3"&gt;2. Creating a transparent and documented process&lt;/head&gt;&lt;p&gt;Several digital rights organizations advised setting up structured response steps:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Require submissions to a dedicated legal@ or abuse@ email.&lt;/item&gt;&lt;item&gt;Insist on full documentation: legal basis, jurisdiction, evidence of the infringement and identity of the complainant.&lt;/item&gt;&lt;item&gt;Review for sufficiency, proportionality, and standing before acting.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;This creates proper documentation to process valid claims, while protecting projects from illegitimate or unfounded requests.&lt;/p&gt;&lt;head rend="h3"&gt;3. Use Jurisdiction Strategically&lt;/head&gt;&lt;p&gt;Projects based in civil law jurisdictions, particularly in Europe, are often better positioned to deflect legal demands from foreign entities. Several organizations emphasized that complying with vague or extrajudicial requests, especially those originating outside your jurisdiction, can increase risk unnecessarily. Instead, they recommended requiring a valid legal basis grounded in the projectâs home country. Formal legal processes, such as court orders or official government channels, were seen as the appropriate threshold, not informal emails or unverifiable demands.&lt;/p&gt;&lt;head rend="h2"&gt;Notification and Appeals: Fairness and Transparency&lt;/head&gt;&lt;p&gt;All of the projects we consulted emphasized the importance of notifying developers whose apps are being targeted, informing them (if possible) of the seriousness of the claim, and the proposed strategy F-Droid is taking to handle the claim.Â&lt;/p&gt;&lt;p&gt;If a threat is deemed to be valid and a developerâs content is flagged for takedown:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;The developer or maintainer is informed, unless prohibited by law (gag orders).&lt;/item&gt;&lt;item&gt;A window for response (commonly 14 days) is offered, unless unfeasible due to seriousness and time restraints of the request itself&lt;/item&gt;&lt;item&gt;If the developer disputes the claim and provides supporting information (e.g. license, public domain status, fair use justification), the claim is reviewed.&lt;/item&gt;&lt;item&gt;If the claim is upheld, the content is removed, but always with an internal record and opportunity to appeal.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;This mirrors principles embedded in international norms (like the Manila Principles and GitHubâs DMCA takedown policy) and avoids overcompliance with weak or abusive claims.&lt;/p&gt;&lt;head rend="h2"&gt;Transparency, Censorship, and What You Can (Legally) Publish&lt;/head&gt;&lt;p&gt;Takedown requests occupy a complex space between legal enforcement and censorship. While some are legitimate claims, like copyright violations or privacy breaches, others are vague, politically motivated, or intended to silence dissent. For FOSS projects that have a global user base, itâs not always obvious how to respond. Complying too quickly can reinforce censorship practices; resisting without process can lead to full website shut downs, domain names being taken away (as in the US) or large and costly legal battles.&lt;/p&gt;&lt;p&gt;One strategy that helps balance this tension is radical transparency. Several projects we spoke with emphasized the importance of documenting what actions were taken and why, not just for accountability, but as a form of resistance. A well-known example is GitHubâs DMCA takedown policy (as of July 2025), which mandates compliance with valid takedown requests, but also posts each one publicly in their github/dmca repository. The result: potential abusers know their requests will face public scrutiny, which acts as a deterrent.&lt;/p&gt;&lt;p&gt;However, not all jurisdictions allow this kind of transparency. In India, for example, we were informed that it is often illegal to disclose that you have received a government request, even to the developer of the affected app. In contrast, in Russia, takedown requests can often be legally posted, though by doing so you may be putting yourself at risk for retaliation, additional takedown requests and legal troubles.&lt;/p&gt;&lt;p&gt;With that in mind, some best practices for FOSS projects include:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Publishing biannual transparency reports, even if redacted or aggregated.&lt;/item&gt;&lt;item&gt;Maintaining an internal log of all takedown activity, with public disclosure where legally possible.&lt;/item&gt;&lt;item&gt;Explaining the general reasons for content removals, who made the request, under what law, and what action was taken, unless legally prohibited.&lt;/item&gt;&lt;item&gt;Being explicit about what cannot be shared, and why.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Transparency wonât prevent all forms of censorship, but it can slow them down, raise awareness, and provide a record that strengthens the broader FOSS ecosystem.&lt;/p&gt;&lt;head rend="h2"&gt;What Weâre Doing at F-Droid&lt;/head&gt;&lt;p&gt;F-Droid is revising its own takedown policy, informed by:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Dutch law and EU regulations&lt;/item&gt;&lt;item&gt;The structural support provided by The Commons Conservancy&lt;/item&gt;&lt;item&gt;Practical lessons from long-standing FOSS organizations&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Our draft process includes:&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;Written takedown submission request to legal@f-droid.org including the required information.:&lt;/item&gt;&lt;/list&gt;&lt;list rend="ul"&gt;&lt;item&gt;Identify the specific material in question (e.g. app name)&lt;/item&gt;&lt;item&gt;Include valid legal basis under applicable jurisdiction (e.g. copyright law, court order statutory basis)&lt;/item&gt;&lt;item&gt;Indicate jurisdiction in which the legal basis is claimed to apply&lt;/item&gt;&lt;item&gt;Include sufficient evidence of the alleged infringement (e.g. copyright certificate, ownership declaration)&lt;/item&gt;&lt;item&gt;Clearly state that the complaintant is authorized to act on behalf of the rights holder&lt;/item&gt;&lt;item&gt;Include full contact details and a verifiable identity (subject to exceptions, such as gag orders or whistleblower protection)&lt;/item&gt;&lt;/list&gt;&lt;list rend="ol"&gt;&lt;item&gt;Verification of jurisdiction and legal basis, including evidence&lt;/item&gt;&lt;item&gt;Developer notification and appeal procedures&lt;/item&gt;&lt;item&gt;Rejection of requests lacking documentation or legal authority may be rejected or ignored&lt;/item&gt;&lt;item&gt;Biannual transparency reports and public tracking of takedown requests&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Weâre also working to improve contributor education about potential exposure when contributing to F-Droid, document internal escalation paths, and ensure consistent handling of international claims.&lt;/p&gt;&lt;head rend="h2"&gt;Final Thoughts&lt;/head&gt;&lt;p&gt;Takedown requests are not going away in fact, theyâre becoming more frequent and more complex. But FOSS projects donât have to face them unprepared.&lt;/p&gt;&lt;p&gt;By building processes, establishing clear jurisdiction, and protecting individuals through structure and policy, we can handle these challenges with the seriousness they deserve without letting them derail our mission.&lt;/p&gt;&lt;head rend="h2"&gt;Legal Disclaimer&lt;/head&gt;&lt;p&gt;The content provided in this article is for informational purposes only and does not constitute legal advice. While we strive to provide accurate and up-to-date information, F-Droid makes no representations or warranties of any kind, express or implied, about the completeness, accuracy, or suitability of the information contained herein.&lt;/p&gt;&lt;p&gt;F-Droid is not a law firm and does not offer legal services. Any reliance you place on the information provided is strictly at your own risk. If you have questions about legal obligations, rights, or compliance, we strongly recommend consulting a qualified legal professional familiar with your jurisdiction.&lt;/p&gt;&lt;p&gt;F-Droid and its contributors disclaim all liability for any loss or damage arising from the use or misuse of this content.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45224421</guid></item><item><title>Epistemic Collapse at the WSJ</title><link>https://www.math.columbia.edu/~woit/wordpress/?p=15206</link><description>&lt;doc fingerprint="ed054dd76344849f"&gt;
  &lt;main&gt;
    &lt;p&gt;For a long time now fundamental theoretical physics has been suffering not just from a slowdown in progress, but from a sort of intellectual collapse (I wrote about this here a while back in the context of “epistemic collapse”: the collapse of a shared reality, caused by the loss of reliable sources for distinguishing what is true from what is false.). The Wall Street Journal has a new article entitled The Rise of ‘Conspiracy Physics’ with summary:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Streamers are building huge audiences by attacking academic physics as just another corrupt establishment. Scientists are starting to worry about the consequences.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;If you replaced “Streamers” by “Sabine Hossenfelder” this would be reasonably accurate, and a serious discussion of this would have been interesting and worthwhile. Instead, the article is an excellent example of the sort of epistemic collapse we’re now living in. There’s zero intelligent content about the underlying scientific issues (is fundamental theoretical physics in trouble?), just a random collection of material about podcasts, written by someone who clearly knows nothing about the topic he’s writing about. The epistemic collapse is total when traditional high-quality information sources like the Wall Street Journal are turned over to uninformed writers getting their information from Joe Rogan podcasts. Any hope of figuring out what is true and what is false is now completely gone.&lt;/p&gt;
    &lt;p&gt;I was planning on writing something explaining what exactly the WSJ story gets wrong, but now realize this is hopeless (and I’m trying to improve my mental health this week, not make it worse). Sorting through a pile of misinformation, trying to rebuild something true out of a collapsed mess of some truth buried in a mixture of nonsense and misunderstandings is a losing battle.&lt;/p&gt;
    &lt;p&gt;Maybe some day our information environment will become healthy again, but for now I’m not sure what to do about this. Be aware that if you’re trying to understand the state of fundamental theoretical physics, watching Joe Rogan, Piers Morgan, Professor Dave, etc. podcasts is just going to fill your mind with crap. Reading articles about these podcasts is worse. If a podcaster (e.g. Sabine Hossenfelder) has a book, read the book (Lost in Math is pretty good) rather than watching the podcasts. In general, reading books is a good idea (I can also recommend this one).&lt;/p&gt;
    &lt;p&gt;Update: John Baez comments here:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;This quagmire is getting bigger. It’s another part of what William Gibson recently called the Singularity of Stupid.&lt;/p&gt;
    &lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45224649</guid></item></channel></rss>