<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Tue, 23 Sep 2025 18:45:02 +0000</lastBuildDate><item><title>Go has added Valgrind support</title><link>https://go-review.googlesource.com/c/go/+/674077</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=45344708</guid><pubDate>Tue, 23 Sep 2025 09:26:57 +0000</pubDate></item><item><title>Permeable materials in homes act as sponges for harmful chemicals: study</title><link>https://news.uci.edu/2025/09/22/indoor-surfaces-act-as-massive-sponges-for-harmful-chemicals-uc-irvine-led-study-shows/</link><description>&lt;doc fingerprint="75ffb4dcae8f068c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Indoor surfaces act as massive sponges for harmful chemicals, UC Irvine-led study shows&lt;/head&gt;
    &lt;p&gt;Permeable materials in homes can retain volatile organic compounds for up to a year&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Scientists injected volatile organic compounds into a test house and found large reservoirs for the potentially hazardous chemicals in porous surfaces such as wood, concrete and paint.&lt;/item&gt;
      &lt;item&gt;VOCs contained in insecticides, cigarette smoke and wildfire smoke can remain on indoor surfaces for as long as one year.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Irvine, Calif., Sept. 22, 2025 — Indoor surfaces have an unexpectedly strong ability to absorb and hold harmful chemical compounds that can threaten human health for as long as a year, according to air chemistry researchers at the University of California, Irvine.&lt;/p&gt;
    &lt;p&gt;In a paper published today in Proceedings of the National Academy of Sciences, the UC Irvine scientists quantify how various indoor surfaces absorb volatile organic compounds, which can result in unhealthy conditions for people and animals when inhaled or absorbed through skin contact.&lt;/p&gt;
    &lt;p&gt;The sources of VOCs are many, such as cooking, spray cleaning, personal care and other consumer products. Additional significant contributors include tobacco smoke and, increasingly, air pollution caused by wildfires. The researchers note that health risks come from inhaling compounds when they “off gas” from surfaces and through dermal uptake when contaminated surfaces are touched.&lt;/p&gt;
    &lt;p&gt;In the spring of 2022, co-author Jonathan Abbatt, professor of chemistry at the University of Toronto, led the Chemical Assessment of Surfaces and Air study, which utilized simulation chambers in the National Institute of Standards and Technology’s Net-Zero Energy Residential Test Facility. Contaminants were injected into a structure mimicking a home environment, with typical building materials. The research team used mass spectrometry instruments to track the movement and persistence of VOCs in the controlled indoor environment.&lt;/p&gt;
    &lt;p&gt;“Scientists in the air chemistry research community have known for a long time that many indoor contaminants can be absorbed by indoor surfaces, but the size of indoor surface reservoirs inside homes and buildings had not been established,” said Manabu Shiraiwa, UC Irvine professor of chemistry, who was responsible for modeling observations and is a corresponding author on the PNAS paper. “Our modeling found that surfaces inside homes have a much greater size to absorb and hold chemicals than previously realized. We can think of these surfaces as massive chemical sponges that soak up VOCs.”&lt;/p&gt;
    &lt;p&gt;Before this study, thin organic films with nanometer thickness were thought to be main surface reservoirs. However, this work proves that permeable and porous materials such as painted surfaces, concrete and wood are likely the major surface reservoirs in a home.&lt;/p&gt;
    &lt;p&gt;“This discovery has significant implications for human health,” Shiraiwa said. “It means people can be exposed to harmful chemicals long after their initial introduction into indoor spaces, and compounds can later be released back into the air or transferred to humans through direct contact with contaminated surfaces.”&lt;/p&gt;
    &lt;p&gt;He added, “This result significantly impacts our understanding of VOC fate and human exposure in indoor environments. With such a large partitioning capacity, organic contaminants will have much longer indoor residence times than previously predicted.”&lt;/p&gt;
    &lt;p&gt;The research explains why certain odors and contaminants persist indoors even after their sources are removed. For example, it provides scientific evidence for why tobacco smoke odors linger in rooms long after smoking has stopped: The residual compounds, known as “thirdhand smoke,” slowly partition back into the air from surface reservoirs.&lt;/p&gt;
    &lt;p&gt;The findings suggest that regular ventilation alone may be insufficient to remove many indoor contaminants. Physical cleaning activities such as vacuuming, mopping and dusting are necessary to effectively remove compounds with high partition coefficients from surface reservoirs.&lt;/p&gt;
    &lt;p&gt;Joining Shiraiwa and Abbatt in this study were Pascale Lakey, project scientist in chemistry at UC Irvine; Jie Yu and Xing Wang at the University of Toronto; Jenna Ditto at Washington University in St. Louis, Missouri; Han Huynh and Marina Vance at the University of Colorado Boulder; Michael Link, Dustin Poppendieck and Stephen Zimmerman at the National Institute of Standards and Technology; and Delphine Farmer at Colorado State University.&lt;/p&gt;
    &lt;p&gt;The research was supported by funding from the Alfred P. Sloan Foundation.&lt;/p&gt;
    &lt;p&gt;About the University of California, Irvine: Founded in 1965, UC Irvine is a member of the prestigious Association of American Universities and is ranked among the nation’s top 10 public universities by U.S. News &amp;amp; World Report. The campus has produced five Nobel laureates and is known for its academic achievement, premier research, innovation and anteater mascot. Led by Chancellor Howard Gillman, UC Irvine has more than 36,000 students and offers 224 degree programs. It’s located in one of the world’s safest and most economically vibrant communities and is Orange County’s second-largest employer, contributing $7 billion annually to the local economy and $8 billion statewide. For more on UC Irvine, visit www.uci.edu.&lt;/p&gt;
    &lt;p&gt;Media access: Radio programs/stations may, for a fee, use an on-campus studio with a Comrex IP audio codec to interview UC Irvine faculty and experts, subject to availability and university approval. For more UC Irvine news, visit news.uci.edu. Additional resources for journalists may be found at https://news.uci.edu/media-resources.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45344756</guid><pubDate>Tue, 23 Sep 2025 09:33:34 +0000</pubDate></item><item><title>Structured Outputs in LLMs</title><link>https://parthsareen.com/blog.html#sampling.md</link><description>&lt;doc fingerprint="d3fff7ff6f587ef7"&gt;
  &lt;main&gt;
    &lt;p&gt;Writings&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45345207</guid><pubDate>Tue, 23 Sep 2025 10:40:41 +0000</pubDate></item><item><title>Cache of devices capable of crashing cell network is found in NYC</title><link>https://www.nytimes.com/2025/09/23/us/politics/secret-service-sim-cards-servers-un.html</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45345514</guid><pubDate>Tue, 23 Sep 2025 11:29:13 +0000</pubDate></item><item><title>Zinc (YC W14) Is Hiring a Senior Back End Engineer (NYC)</title><link>https://app.dover.com/apply/Zinc/4d32fdb9-c3e6-4f84-a4a2-12c80018fe8f/?rs=76643084</link><description>&lt;doc fingerprint="e10fcdab2cdf53e4"&gt;
  &lt;main&gt;
    &lt;p&gt;You need to enable JavaScript to run this app.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45345742</guid><pubDate>Tue, 23 Sep 2025 12:00:00 +0000</pubDate></item><item><title>Mesh: I tried Htmx, then ditched it</title><link>https://ajmoon.com/posts/mesh-i-tried-htmx-then-ditched-it</link><description>&lt;doc fingerprint="ae1a9a5068051682"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;MESH: I tried HTMX, then ditched it&lt;/head&gt;
    &lt;p&gt;There is a kind of exciting movement in Web dev right now. Web devs are talking about "JavaScript Fatigue", "Framework Fatigue", the "Revival of Hypermedia" and "HTML Over The Wire". In a word: we're asking ourselves why we're building HTML in JavaScript.&lt;/p&gt;
    &lt;p&gt;The figurehead for this movement is undoubtedly HTMX. It shows that much of what we do in JavaScript could instead be done declaratively, with HTML attributes. If browsers adopted these semantics natively, many websites - and even apps - wouldn't need JavaScript at all. I love this idea! Writing HTML first and adding JS on top is the way the Web should work.&lt;/p&gt;
    &lt;p&gt;At present, we write JavaScript first, and we use it to generate HTML. How did we get it so backwards? I believe the answer is pretty straightforward: SPA frameworks are a joy to use. They impose structure, enforcing conventions, ultimately making it easy to keep concerns separated in one's mind.&lt;/p&gt;
    &lt;p&gt;My big problem with HTMX, as it stands, is that it lacks that structure. Taking a look at HTMX the first time, my reaction was: "...so, declarative jQuery." I could see, as if before my very eyes, the spaghetti that inevitably grows out of a library like this. HTMX leaves it up to the developer to impose discipline on their code, however they see fit.&lt;/p&gt;
    &lt;p&gt;So, I decided to accept the challenge. I want to do modular SSR the way HTMX encourages, but I want to do it with something like an SPA framework. I want nestable components, each with their own HTML, CSS, and JS - and back-end code - sitting side by side. I want there to be one, and only one, right way to do something.&lt;/p&gt;
    &lt;p&gt;The result of this journey is MESH - modular element SSR with hydration. MESH is based on a simple principle: one component = one endpoint. This is a powerful idea - it allows us to write a HTML-first back-end in such a way that it feels like writing an SPA.&lt;/p&gt;
    &lt;p&gt;This write-up includes a lot of code snippets. I've tried to keep these minimal. If you want to follow along with more context, you can find the whole commit history for MESH on GitHub.&lt;/p&gt;
    &lt;head rend="h2"&gt;Basic Interactivity&lt;/head&gt;
    &lt;p&gt;Looking around, it seemed the back-end of choice for HTMX devs is Go with Templ. I've never really had my "Damascus moment" with Go, but this was a good opportunity to get my feet wet. I will say this much: it is a joy to work with something genuinely blazingly fast to build and deploy.&lt;/p&gt;
    &lt;p&gt;I also wanted to have a go with proper vibe coding - writing code without reading it - with Junie. What fun this was! I can see why people would be tempted to write whole apps this way. I'll only say this much: as someone who's battled addiction in the past, I didn't like what I noticed my brain was doing with it. That's a subject for another blog post another time.&lt;/p&gt;
    &lt;p&gt;My mission was, in a word, to write something like an opinionated framework or "harness" for HTMX which would give me a standard way to use it with Web Components. Specifically, what I had in mind was a "one component one endpoint" model. HTMX would always swap the entire component, which would then be "hydrated".&lt;/p&gt;
    &lt;p&gt;It turns out there is a standard way to do server-side rendered custom elements, called Declarative Shadow DOM (DSD). Others have already had some success using HTMX and DSD together. The combo looked promising.&lt;/p&gt;
    &lt;p&gt;There is one significant limitation, however: HTMX will not cross shadow root boundaries. This is by design, to be clear - this is how we should expect HTMX to behave. No sweat, we can do a simple hack to make it work - and, at the same time, to enforce component-level swaps:&lt;/p&gt;
    &lt;code&gt;import type {HtmxBeforeSwapDetail} from "./types/htmx";

function enforceComponentSwap(evt: CustomEvent&amp;lt;HtmxBeforeSwapDetail&amp;gt;) {
  const detail = evt.detail;
  let elt = detail.elt;
  let root = elt.getRootNode();
  
  if (root instanceof ShadowRoot) {
    detail.target = root.host as HTMLElement;
    detail.swapOverride = "outerHTML";
  }
}

document.body.addEventListener("htmx:beforeSwap", enforceComponentSwap as EventListener);
&lt;/code&gt;
    &lt;p&gt;With this little helper, I can now start building out a very simple Trello clone to prove the concept. Let's build a little editable card component:&lt;/p&gt;
    &lt;code&gt;package card

import (
  "mesh/src/services"
  "fmt"
)

type CardProps struct {
  *services.Card
}

templ Card(props CardProps) {
  &amp;lt;mesh-card
    id={ fmt.Sprintf("card-%d", props.Card.ID) }
  &amp;gt;
    &amp;lt;template shadowrootmode="open"&amp;gt;
      &amp;lt;base href="/"/&amp;gt;
      &amp;lt;link rel="stylesheet" href="/static/css/components/card.css"/&amp;gt;
      &amp;lt;div data-view class="card"&amp;gt;
        &amp;lt;div class="card-header"&amp;gt;
          &amp;lt;h3&amp;gt;{ props.Card.Title }&amp;lt;/h3&amp;gt;
        &amp;lt;/div&amp;gt;
        &amp;lt;div class="card-content"&amp;gt;
          { props.Card.Content }
        &amp;lt;/div&amp;gt;
        &amp;lt;div class="actions"&amp;gt;
          &amp;lt;button type="button" mesh-click="edit"&amp;gt;Edit&amp;lt;/button&amp;gt;
        &amp;lt;/div&amp;gt;
      &amp;lt;/div&amp;gt;
      &amp;lt;form data-form class="card hide" hx-patch="/card"&amp;gt;
        &amp;lt;input type="hidden" name="cardID" value={ props.Card.ID } /&amp;gt;
        &amp;lt;label&amp;gt;
          Title
          &amp;lt;input type="text" name="title" value={ props.Data.Title } /&amp;gt;
        &amp;lt;/label&amp;gt;
        &amp;lt;label&amp;gt;
          Content
          &amp;lt;textarea name="content"&amp;gt;{ props.Data.Content }&amp;lt;/textarea&amp;gt;
        &amp;lt;/label&amp;gt;
        &amp;lt;div class="actions"&amp;gt;
          &amp;lt;button type="button" mesh-click="cancel"&amp;gt;Cancel&amp;lt;/button&amp;gt;
          &amp;lt;button type="submit"&amp;gt;Save&amp;lt;/button&amp;gt;
        &amp;lt;/div&amp;gt;
      &amp;lt;/form&amp;gt;
    &amp;lt;/template&amp;gt;
  &amp;lt;/mesh-card&amp;gt;
}
&lt;/code&gt;
    &lt;p&gt;Now let's hydrate it. I'm going to start with a simple base element that'll ensure that our shadow root is attached properly and processed by HTMX:&lt;/p&gt;
    &lt;code&gt;export class MeshElement extends HTMLElement {
  connectedCallback() { 
    if (!this.shadowRoot) {
      // the browser should do this for us - oh well, what can you do?
      const root = this.attachShadow({ mode: 'open' });
      const template = this.querySelector('template[shadowrootmode="open"]');
      if (template) {
        root.appendChild((template as any).content.cloneNode(true));
      }
    }
    if (window.htmx) {
      window.htmx.process(this);
      if (this.shadowRoot) {
        window.htmx.process(this.shadowRoot);
      }
    }
    this.bindListeners();
  }

  protected bindListeners() {
    const supportedEvents = ['click'];

    supportedEvents.forEach(eventName =&amp;gt; {
      const attribute = "mesh-" + eventName;
      this.all('[' + attribute + ']', el =&amp;gt; {
        const methodName = el.getAttribute(attribute);
        if (!methodName) {
          return;
        }
        const method = (this as any)[methodName];
        if (!method || typeof method !== 'function') {
          console.error(`Method ${methodName} is not a function`);
          return;
        }

        el.addEventListener(eventName, method.bind(this));
      });
    });
  }

  all(selector: string, cb: (el: HTMLElement) =&amp;gt; void) {
    return this.shadowRoot!.querySelectorAll(selector).forEach(e =&amp;gt; cb(e as HTMLElement));
  }
}
&lt;/code&gt;
    &lt;p&gt;Then our card element is straightforward to implement:&lt;/p&gt;
    &lt;code&gt;import {MeshElement} from "../base/mesh-element.ts";

export class Card extends MeshElement {
  edit() {
    this.show('[data-form]');
    this.hide('[data-view]');
  }
  
  cancel() {
    this.hide('[data-form]');
    this.show('[data-view]');
  }

  show(selector: string) {
    this.all(selector, el =&amp;gt; {
      el.classList.remove('hide');
    });
  }

  hide(selector: string) {
    this.all(selector, el =&amp;gt; {
      el.classList.add('hide');
    });
  }
}
window.customElements.define('mesh-card', Card);
&lt;/code&gt;
    &lt;p&gt;This works great! I've enhanced my card component with some basic JS to show that it can be done, and otherwise this is all just plain old HTMX.&lt;/p&gt;
    &lt;p&gt;The next step is adding functionality to move the cards between columns. This is where we'll run into a common difficulty with HTMX: how to swap out "parent" components given an update on a child component.&lt;/p&gt;
    &lt;p&gt;Now, HTMX devs have a number of differing opinions on how best to go about this. One common practice is to "expand the target", which means your component needs to be aware of parent components. Another way is to trigger events in the response headers - this is better, in that it moves responsibility for this back to the server. I believe front-end components shouldn't know anything their own placement on the page.&lt;/p&gt;
    &lt;p&gt;Fortunately, HTMX gives us another way to do this - and it appears to be the emerging "best practice" - with "out of band" (OOB) swaps. If, in our response to a call to the card endpoint, we return any other components that need updating, and simply flag them as OOB, HTMX will handle the swaps for us. This best reflects my own aims for MESH, so let's see how we get along doing it this way.&lt;/p&gt;
    &lt;p&gt;Let's add "promote" functionality to our card component - this will simply move the card one column to the right:&lt;/p&gt;
    &lt;code&gt;package card

import (
  "mesh/src/services"
  "fmt"
)

const PutActionPromote = "promote"

type CardProps struct {
  *services.Card
  CanPromote bool
}

templ Card(props CardProps) {
  &amp;lt;mesh-card
    id={ fmt.Sprintf("card-%d", props.Card.ID) }
  &amp;gt;
    &amp;lt;template shadowrootmode="open"&amp;gt;
      &amp;lt;base href="/"/&amp;gt;
      &amp;lt;link rel="stylesheet" href="/static/css/components/card.css"/&amp;gt;
      &amp;lt;div data-view class="card"&amp;gt;
        &amp;lt;div class="card-header"&amp;gt;
          &amp;lt;h3&amp;gt;{ props.Card.Title }&amp;lt;/h3&amp;gt;
        &amp;lt;/div&amp;gt;
        &amp;lt;div class="card-content"&amp;gt;
          { props.Card.Content }
        &amp;lt;/div&amp;gt;
        &amp;lt;div class="actions"&amp;gt;
          if props.CanPromote {
            &amp;lt;form hx-put="/card"&amp;gt;
              &amp;lt;input type="hidden" name="action" value="promote" /&amp;gt;
              &amp;lt;input type="hidden" name="cardID" value={props.Card.ID} /&amp;gt;
              &amp;lt;button type="submit" aria-label="Move to next column"&amp;gt;
                &amp;lt;i data-lucide="arrow-right"&amp;gt;&amp;lt;/i&amp;gt;
              &amp;lt;/button&amp;gt;
            &amp;lt;/form&amp;gt;
          }
        &amp;lt;/div&amp;gt;
      &amp;lt;/div&amp;gt;
    &amp;lt;/template&amp;gt;
  &amp;lt;/mesh-card&amp;gt;
}
&lt;/code&gt;
    &lt;p&gt;To handle the OOB updates, we'll write a "context-enriched" pub-sub:&lt;/p&gt;
    &lt;code&gt;type EventContext struct {
	Context        context.Context
	ResponseWriter http.ResponseWriter
}

func (e *EventContext) Write(component templ.Component) {
	err := component.Render(e.Context, e.ResponseWriter)
	if err != nil {
		http.Error(e.ResponseWriter, "Failed to render OOB updates", http.StatusInternalServerError)
	}
}

func (e *EventService) Publish(event Event, w http.ResponseWriter, ctx context.Context) {
  eventContext := EventContext{
    Context:        ctx,
    ResponseWriter: w,
  }
  for _, subscriber := range e.subscribers[event.Key()] {
    subscriber(event, eventContext)
  }
}

func (e *EventService) Subscribe(key string, subscriber func(event Event, context EventContext)) {
  e.subscribers[key] = append(e.subscribers[key], subscriber)
}
&lt;/code&gt;
    &lt;p&gt;Then we can publish in the card handler:&lt;/p&gt;
    &lt;code&gt;func (h *Handler) Put(w http.ResponseWriter, r *http.Request) {
  card, err := h.getCardFromRequest(r)
  if err != nil {
    http.Error(w, err.Error(), http.StatusNotFound)
    return
  }
  action := r.FormValue("action")
  switch action {
  case PutActionPromote:
    fromColumn, toColumn, err := h.CardService.Promote(card.ID)
    if err != nil {
      http.Error(w, err.Error(), http.StatusBadRequest)
    } else {
      h.EventService.PublishCardMoved(card.ID, fromColumn.ID, toColumn.ID, w, r.Context())
    }
    break
  }
}
&lt;/code&gt;
    &lt;p&gt;And we can subscribe in the column handler:&lt;/p&gt;
    &lt;code&gt;func (h *Handler) OnCardMoved(event *services.CardMovedEvent, context services.EventContext) {
  column, err := h.CardService.GetColumn(event.ToColumnID)
  if err == nil {
    context.Write(h.RenderComponent(column, true))
  } else {
    http.Error(context.ResponseWriter, err.Error(), http.StatusInternalServerError)
  }
  column, err = h.CardService.GetColumn(event.FromColumnID)
  if err == nil {
    context.Write(h.RenderComponent(column, true))
  } else {
    http.Error(context.ResponseWriter, err.Error(), http.StatusInternalServerError)
  }
}
&lt;/code&gt;
    &lt;p&gt;This way, our components can communicate with each other without needing to know about each other. The subscriber takes the request context from the publisher and simply writes to the response. The result is a response from the back-end with&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;the component-specific update, followed by&lt;/item&gt;
      &lt;item&gt;any other OOB updates simply appended to the response.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This works surprisingly well.&lt;/p&gt;
    &lt;p&gt;Unfortunately, we once again run into the same limitation as before: HTMX will not cross shadow root boundaries. At this point, it seems clear that, if we want to use HTMX as intended, we are going to have to give up on shadow DOM entirely. Again, this is intended behaviour. JavaScript should not cross shadow root boundaries by default. What this means is that, if we're wedded to shadow DOM, we are going to have to fight HTMX all the way.&lt;/p&gt;
    &lt;p&gt;I'm undeterred, of course - all we need is another little hack:&lt;/p&gt;
    &lt;code&gt;function findInShadow(root: any, id: string): any {
  const element = root.getElementById?.(id);
  if (element) {
    return element;
  }
  const allElements = root.querySelectorAll('*');
  for (let element of allElements) {
    if (element.shadowRoot) {
      const found = findInShadow(element.shadowRoot, id);
      if (found) {
        return found;
      }
    }
  }
  return null;
}

function enableOobSwap(evt: CustomEvent&amp;lt;any&amp;gt;) {
  const id = evt.detail.content.id;
  const found = findInShadow(document, id);
  if (found) {
    found.outerHTML = evt.detail.content.outerHTML;
    evt.preventDefault();
  }
}

document.body.addEventListener("htmx:oobErrorNoTarget", enableOobSwap as EventListener);
&lt;/code&gt;
    &lt;p&gt;You'll notice we've done the &lt;code&gt;outerHTML&lt;/code&gt; swap ourselves here, overriding HTMX entirely. I'm not a fan of this. I tried as many approaches as I could think of to get HTMX to do the swap - and, thus, leverage existing functionality HTMX provides for these (maintaining scroll position and focus and so on) - but did not succeed.&lt;/p&gt;
    &lt;p&gt;Nevertheless, this works for my purposes. I'm happy to leave the problem alone for now. Let's add some drag-and-drop functionality to our cards. In our card component:&lt;/p&gt;
    &lt;code&gt;import {MeshElement} from "../base/mesh-element.ts";

export class Card extends MeshElement {
  setupDragAndDrop() {
    this.one('.grip', grip =&amp;gt; {
      grip.draggable = true;
      this.addEventListener('dragstart', this.handleDragStart.bind(this));
      this.addEventListener('dragend', this.handleDragEnd.bind(this));
    });
  }

  handleDragStart(e: any) {
    e.dataTransfer.setData('text/plain', this.dataset.id);
    this.classList.add('dragging');
    e.dataTransfer.effectAllowed = 'move';
  }

  handleDragEnd() {
    this.classList.remove('dragging');
  }
}
&lt;/code&gt;
    &lt;p&gt;And in our column component:&lt;/p&gt;
    &lt;code&gt;import {MeshElement} from "../base/mesh-element.ts";

export class Column extends MeshElement {
  setupDropTarget() {
    this.addEventListener('dragover', this.handleDragOver.bind(this));
    this.addEventListener('drop', this.handleDrop.bind(this));
  }

  handleDragOver(e: any) {
    e.preventDefault();
    e.dataTransfer.dropEffect = 'move';
  }

  handleDrop(e: any) {
    e.preventDefault();
    this.classList.remove('drag-over');

    const cardId = e.dataTransfer.getData('text/plain');
    const columnId = this.dataset.id;
    if (!cardId || !columnId) {
      throw new Error('Missing card or column ID');
    }

    const position = this.calculateDropPosition(e);
    this.moveCard(cardId, +columnId, position);
  }

  async moveCard(cardId: number, columnId: number, position: number) {
    window.htmx.ajax('put', '/card', {
      swap: 'none',
      values: {
        action: 'move',
        cardID: cardId,
        columnID: columnId,
        position: position,
      }
    } as any);
  }
}
&lt;/code&gt;
    &lt;p&gt;This is great! This use case is precisely why HTMX provides the ajax JS API. With a bit of hacking, we've demonstrated that it's possible to use HTMX to handle modular SSR based on the premise of "component = endpoint". I'm pretty happy with how this has turned out.&lt;/p&gt;
    &lt;head rend="h2"&gt;Realtime Collaboration&lt;/head&gt;
    &lt;p&gt;From the moment I conceived of this project, one of the things I wanted to do was to support realtime collaboration with server-sent events (SSE). HTMX supports SSE with a standard plugin which is easy enough to set up:&lt;/p&gt;
    &lt;code&gt;&amp;lt;html lang="en" hx-ext="sse"&amp;gt;
&amp;lt;body hx-ext="sse" sse-connect="/sse" sse-swap="oob-update"&amp;gt;
&lt;/code&gt;
    &lt;p&gt;In theory this should just work once I've written my SSE back-end. I ended up using r3labs/sse for this, which I found very easy to use. We wrap this in a service and provide a "broadcast" method that sends OOB updates to all subscribed clients immediately:&lt;/p&gt;
    &lt;code&gt;func (s *SSEService) BroadcastOOBUpdate(component templ.Component) {
   var buf strings.Builder
   err := component.Render(context.Background(), &amp;amp;buf)
   if err != nil {
       s.log.Error("Failed to render component for SSE broadcast", "error", err)
       return
   }

   html := buf.String()
   s.server.Publish("oob-updates", &amp;amp;sse.Event{
       Event: []byte("oob-update"),
       Data:  html,
   })
}
&lt;/code&gt;
    &lt;p&gt;Then we call it in our handler:&lt;/p&gt;
    &lt;code&gt;func (h *Handler) OnCardMoved(event *services.CardMovedEvent) {
  column, err := h.CardService.GetColumn(event.ToColumnID)
  if err == nil {
    component := h.RenderComponent(column, true)
    h.SSEService.BroadcastOOBUpdate(component)
  } else {
    h.Log.Error("Failed to get to-column for SSE broadcast", "columnID", event.ToColumnID, "error", err)
  }

  column, err = h.CardService.GetColumn(event.FromColumnID)
  if err == nil {
    h.SSEService.BroadcastOOBUpdate(h.RenderComponent(column, true))
  } else {
    h.Log.Error("Failed to get from-column for SSE broadcast", "columnID", event.FromColumnID, "error", err)
  }
}
&lt;/code&gt;
    &lt;p&gt;This makes our back-end code a lot cleaner! We no longer need to pass the request context around with our event, and we no longer need to append a bunch of OOB updates to the response. Having done this both ways, I have come to believe that SSE is the most natural way to do these kind of asynchronous cross-context modular updates, even with only a single user.&lt;/p&gt;
    &lt;p&gt;I was hoping this would also allow me to get rid of my &lt;code&gt;outerHTML&lt;/code&gt; hack, but alas it was not to be. The longer I worked on this project, the more it became clear to me that I'm not really using HTMX the way it's intended to be used. More importantly, there is a lot of other HTMX functionality that I'm not using at all.&lt;/p&gt;
    &lt;p&gt;Naturally, I was intrigued to see if I could just get rid of HTMX entirely. So I did, and the result is a lot cleaner and easier to reason about. We are left with two JS modules - one for the custom elements:&lt;/p&gt;
    &lt;code&gt;export class MeshElement extends HTMLElement {
  connectedCallback() {
    if (!this.shadowRoot) {
      const root = this.attachShadow({ mode: 'open' });
      const template = this.querySelector('template[shadowrootmode="open"]');
      if (template) {
        root.appendChild((template as any).content.cloneNode(true));
      }
    }
    this.bindFormHandlers();
  }

  protected bindFormHandlers() {
    const supported = [
      'get', 'post', 'put', 'patch', 'delete',
    ];

    supported.forEach(verb =&amp;gt; {
      const attribute = "mesh-" + verb;
      this.all('[' + attribute + ']', el =&amp;gt; {
        const form = el as HTMLFormElement;
        form.addEventListener('submit', (event: Event) =&amp;gt; {
          event.preventDefault();
          const method = verb.toUpperCase();
          const url = form.getAttribute(attribute);

          if (!url) {
            console.error('No URL specified for form submission');
            return;
          }

          const formData = new FormData(form);
          this.makeRequest(method, url, formData)
            .then(response =&amp;gt; {
              if (response.ok) {
                return response.text();
              } else {
                throw new Error('Form submission failed: ' + response.statusText);
              }
            })
            .then(html =&amp;gt; this.outerHTML = html)
            .catch(error =&amp;gt; console.error('Form submission failed:', error));
        });
      });
    });
  }

  protected async makeRequest(method: string, url: string, formData: FormData): Promise&amp;lt;Response&amp;gt; {
    const options: RequestInit = {
      method,
      headers: {
        'X-Requested-With': 'XMLHttpRequest',
      },
    };

    if (method === 'GET') {
      const params = new URLSearchParams(formData as any);
      url += (url.includes('?') ? '&amp;amp;' : '?') + params.toString();
    } else {
      options.body = formData;
    }

    return fetch(url, options);
  }
}
&lt;/code&gt;
    &lt;p&gt;and one for SSE:&lt;/p&gt;
    &lt;code&gt;export class SSEManager {
  private eventSource: EventSource | null = null;

  constructor(private url: string = '/sse?stream=oob-updates') {
    this.connect();
  }

  private connect() {
    if (this.eventSource) {
      this.eventSource.close();
    }

    this.eventSource = new EventSource(this.url);

    this.eventSource.addEventListener('oob-update', (event) =&amp;gt; {
      this.processOOBUpdate(event as MessageEvent);
    });

    this.eventSource.onerror = (error) =&amp;gt; {
      console.error('SSE connection error:', error);
      setTimeout(() =&amp;gt; this.connect(), 5000);
    };
  }

  private processOOBUpdate(html: string) {
    const template = document.createElement('template');
    template.innerHTML = html.trim();

    for (const content of template.content.querySelectorAll('[mesh-swap-oob]')) {
      const id = content.id;
      const target = this.findInShadow(document, id);

      if (target) {
        target.outerHTML = content.outerHTML;
      } else {
        console.warn('OOB target not found:', id);
      }
    }
  }

  private findInShadow(root: Document | ShadowRoot | Element, id: string): Element | null {
    let element = root.querySelector(`#${id}`);
    if (element) {
      return element;
    }

    const allElements = root.querySelectorAll('*');
    for (const el of allElements) {
      if (el.shadowRoot) {
        element = this.findInShadow(el.shadowRoot, id);
        if (element) {
          return element;
        }
      }
    }

    return null;
  }
}

new SSEManager();
&lt;/code&gt;
    &lt;p&gt;And that's it! That's all the JS it takes to replace all of HTMX that I'm using for this project.&lt;/p&gt;
    &lt;head rend="h2"&gt;Takeaways&lt;/head&gt;
    &lt;p&gt;This was a fun project. First, let me say, if you're writing apps with jQuery, please check out HTMX! It's very dev-friendly and a proper 2020s way of doing that kind of dev. Personally, however, I am happy to have convinced myself it's not for me.&lt;/p&gt;
    &lt;p&gt;I, for one, don't believe the HTMX spec, or something like it, will be merged back into HTML, at least until it can answer one fundamental question: what is the default swap behaviour? When I declare a &lt;code&gt;form&lt;/code&gt; with a &lt;code&gt;method&lt;/code&gt;, I understand how that form will behave: it will reload the entire page. What happens when I declare a &lt;code&gt;form&lt;/code&gt; with &lt;code&gt;hx-post&lt;/code&gt; or equivalent? The default behaviour in HTMX is that the &lt;code&gt;innerHTML&lt;/code&gt; of the form itself becomes the swap target. This does not seem like a sane default to me.&lt;/p&gt;
    &lt;p&gt;So what's the answer? Well, as anyone who's aware of the state of the art on SSR will have noticed, all I've actually done with MESH is reinvent HotWire, LiveWire, LiveView and friends. Personally, I find this encouraging! It is clear to me that there is a kind of best practice to be found here.&lt;/p&gt;
    &lt;p&gt;I believe the default swap behaviour should be: always swap the whole component. One component, one endpoint. This is how these frameworks do it. My problem with them is they lock you into a specific back-end. I believe the principle is generalisable, that there is a way to do this kind of modular SSR in a back-end-agnostic way, like HTMX does. MESH is my attempt to show what that would look like.&lt;/p&gt;
    &lt;p&gt;I will certainly keep using MESH for my projects, fleshing it out as I go. The Trello clone will always be online for anyone to play with.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45345950</guid><pubDate>Tue, 23 Sep 2025 12:18:01 +0000</pubDate></item><item><title>Getting More Strategic</title><link>https://cate.blog/2025/09/23/getting-more-strategic/</link><description>&lt;doc fingerprint="66e3193529eb0183"&gt;
  &lt;main&gt;
    &lt;p&gt;Strategy – how to be strategic, and how to be seen as strategic – is one of my ongoing obsessions. Years ago, I read Good Strategy/Bad Strategy, and it’s guided my thinking ever since.&lt;/p&gt;
    &lt;p&gt;One of the things that book helps clarify is that being strategic and being seen as strategic can work against each other – good strategy is obvious, and usually it is executed on more than it’s talked about. An ongoing frustration for other under indexed people in tech I talk to, as we build products and organizations without drama, whilst being told we’re just “not strategic” enough. The strategy required to sidestep problems that never happen or that creates optionality to quickly resolve is somehow invisible.&lt;/p&gt;
    &lt;p&gt;But I think as we rise up the org chart, strategy is the job. Strategy defines your job, and evolves it to meet the organizational need. Not just one strategy, but multiple strategies that need to fit together and be coherent.&lt;/p&gt;
    &lt;p&gt;Your product strategy. Your technical strategy. Your team strategy. Your you-as-a-leader-but-also-a-human-being strategy.&lt;/p&gt;
    &lt;p&gt;As we find our groove in the resource constrained era we are in currently as opposed to the everything strategy of ZIRP (zero interest rates), by definition we need to make more harder choices, and strategy is how we know what those choices are, and when and how to make them.&lt;/p&gt;
    &lt;p&gt;This is the first rule of strategy: strategy is contextual. A crucial insight, because often when leaders fail, it’s because they tried to apply a strategy that worked in one context, to a different one, without considering the difference.&lt;/p&gt;
    &lt;p&gt;This is true when you change companies, and I think the reason why there is such a high failure rate for executive hires*. Ones I’ve watched fail came in with a playbook, usually including the org chart they wanted, and expended all the goodwill and capital in pursuit of that goal, whilst achieving very little.&lt;/p&gt;
    &lt;p&gt;It’s also the case that when the market changes, our strategy must change. One of the core features of ZIRP-era engineering leadership was hiring for the sake of it, and number of people as a proxy for many things it maybe (probably) shouldn’t have been. One of the biggest shifts has been the layoffs and the mantra of “doing more with less”. Regardless of personal feelings on this topic and what is actually realistic, it is apparent that hard choices and discipline are a key feature of the post-ZIRP era.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;We could talk about these strategies – product, technical, team, you, like some balanced stool. But realistically, I think it’s more like the image above. The product strategy is a storm (especially pre-product market fit). The technical strategy is a half built shelter (you’ll get to it properly once you have product market fit). The team strategy is an umbrella (the most flexible and controllable). And the you as a human strategy is nowhere to be found.&lt;/p&gt;
    &lt;p&gt;This is the second rule of strategy: timeframe varies with the level of uncertainty you’re navigating.&lt;/p&gt;
    &lt;p&gt;The idea of a proximate objective is the next logical step in pursuit of your overall strategy, if you achieve it, you confirm your course. If you fail, you learn and reconsider your options.&lt;/p&gt;
    &lt;p&gt;We often talk about strategy like it’s defining the end state, setting and describing the destination. But strategy is about defining the incremental steps – the proximate objectives – that can take us towards that end state. Strategy is understanding where we are at – context – and the path from there to where we need to go**. Any strategic “plan”, is best executed as a set of proximate objectives.&lt;/p&gt;
    &lt;p&gt;This mistake of how strategy is talked about is why it can be so hard for some people to be seen as strategic. When we think strategy is depicting the end state, and undervalue the proximate objective definitions and execution that it takes to get there, the person who talks more about the end state can be seen as more strategic than the person who actually reaches it.&lt;/p&gt;
    &lt;p&gt;We need four things for strategy:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Time – energy – to think deeply about it&lt;/item&gt;
      &lt;item&gt;Context to situate it&lt;/item&gt;
      &lt;item&gt;Direction to identify proximate objectives&lt;/item&gt;
      &lt;item&gt;Expertise to chart the path&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;All of these need to come together to create and deliver an effective strategy. It’s a balance between all of them, leaning into different ones at different times.&lt;/p&gt;
    &lt;p&gt;To illustrate, why each of these are important, I think it’s helpful to consider the extremes of each.&lt;/p&gt;
    &lt;p&gt;When someone is all time, we call them a political operator. This is the person who manages up to get credit, but the people underneath them ask what it is that they do.&lt;/p&gt;
    &lt;p&gt;When someone is all context, we say they can’t see the forest for the trees. They miss the big picture fixating on the details.&lt;/p&gt;
    &lt;p&gt;When someone is all proximate objectives, we call them a thought leader and it’s not a compliment. Execution is an exercise left to the reader.&lt;/p&gt;
    &lt;p&gt;When someone is all expertise, they present solutions in search of problems. They don’t seem to understand impact.&lt;/p&gt;
    &lt;p&gt;Devaluing these things gives us a reason not to do them. So many engineers will tell you they hate politics, and yes, there is definitely toxic workplace politics. But there’s a baseline where politics is getting things done. It’s convincing people that the idea is good, and that it can be executed. My favourite explanation of this is Nik Means talking about Eiffel’s tower.&lt;/p&gt;
    &lt;p&gt;Context is important. Yes, you’re delivering something bigger, but the details need to add up. You can’t gloss over all of them, you need to learn how to distinguish which are important and which are not.&lt;/p&gt;
    &lt;p&gt;Proximate objectives chart your path. They explain the steps you expect to take between where you are and where you plan to be. Explaining them helps bring people along with you.&lt;/p&gt;
    &lt;p&gt;Expertise is ultimately how you deliver things, you need to understand how to deliver and how to validate. Execution is when the strategy becomes real.&lt;/p&gt;
    &lt;p&gt;Strategy is hard, and being seen as strategic – especially for under-indexed people – can be even harder. We need all of these four things to develop our strategy and move things forward. And we need to be recognized as doing all of them in order to be seen as strategic.&lt;/p&gt;
    &lt;p&gt;Coming back to our problems of strategy – the product, technical, team, and you.&lt;/p&gt;
    &lt;p&gt;Product strategy drives your proximate objectives. Whilst product strategy may seem like the job of product management – and to a certain extent it is, but hopefully your product team does not operate in a vacuum. Engineering needs to provide input, but engineering also needs to understand the product strategy, because everything else needs to fit in with it.&lt;/p&gt;
    &lt;p&gt;Your team exists for a purpose, and the clearest part of that purpose is delivery of the product strategy. You need direction and alignment to identify proximate objectives. Direction – where the product strategy is going, alignment on what is most important, and what will be delivered when.&lt;/p&gt;
    &lt;p&gt;Technical strategy evolves the context. Your technical strategy is often about surfacing the underlying work that allows you to deliver on the business need. It has to be well justified, because ideally it’s pro-active rather than reactive – i.e. you implement it before the emergency rather than during it.&lt;/p&gt;
    &lt;p&gt;Any technical strategy needs to start with what problem is being solved. A problem is not the absence of a technology – unless, I understand, that technology is AI – but rather the problems that technology would solve. So “we don’t have containers” is not a problem. Number of incidents or environment inconsistencies is. Good technical strategy changes the context over time – making more possible – like building roads on the territory you’ve chartered.&lt;/p&gt;
    &lt;p&gt;Your team strategy must be grounded in execution. The product and technical strategy define the organizational need. Your team strategy is about how your team is going to meet that organizational need, within the constraints of the business.&lt;/p&gt;
    &lt;p&gt;Post-ZIRP, this has been a big challenge. Doing more with less means having fewer people, less flexibility, less margin of error. You need to figure out how you retain key people when money is tighter and promotions are harder to come by. But amidst all of these challenges, you have to execute. If in a ZIRP era, you could build the team then deliver, now you must deliver as you build the team.&lt;/p&gt;
    &lt;p&gt;The you as a person strategy requires that you carve out time to be strategic. In this market, many of us are doing-doing-doing to prove that we’re worth keeping around, but at some point, your job is no longer what is being done this week, and more about what is possible next quarter (and the quarters after that). It’s never been easier to be DDOS’d by the job and think that means we’re doing a good one, but you could be missing key things if you’re too focused on the day to day, or week to week and not enough on the month to month.&lt;/p&gt;
    &lt;p&gt;To wrap up, strategy is about more than just a vision; it’s about navigating the path to get there. We need to balance time, context, direction, and expertise to ensure we’re not only seen as strategic but are genuinely creating a strategic path forward for the teams we’re responsible for – and our own evolving needs to competently lead them.&lt;/p&gt;
    &lt;p&gt;* I can’t find a great source here, although the search results suggest it’s commonly accepted #. # possibly, which links out to a site requiring login.&lt;/p&gt;
    &lt;p&gt;** I love Tanya Reilly’s description of the map in The Staff Engineer’s Path.&lt;/p&gt;
    &lt;p&gt;Image credit: Joe Groove&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45346219</guid><pubDate>Tue, 23 Sep 2025 12:41:36 +0000</pubDate></item><item><title>Restrictions on house sharing by unrelated roommates</title><link>https://marginalrevolution.com/marginalrevolution/2025/08/the-war-on-roommates-why-is-sharing-a-house-illegal.html</link><description>&lt;doc fingerprint="28b9c150fec148ac"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The War on Roommates: Why Is Sharing a House Illegal?&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;Boarding houses were made illegal by zoning that enforced single family homes and by rules limiting occupancy, demanding every room have a private bathroom, outlawing shared kitchens, requiring parking spaces for every resident etc.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;How States and Cities Decimated Americans’ Lowest-Cost Housing Option is an excellent, hard-hitting piece making and extending these points and significantly it’s not from a libertarian think tank but Pew:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Low-cost micro-units, often called single-room occupancies, or SROs, were once a reliable form of housing for the United States’ poorest residents of, and newcomers to, New York, Chicago, San Francisco, and many other major U.S. cities. Well into the 20th century, SROs were the least expensive option on the housing market, providing a small room with a shared bathroom and sometimes a shared kitchen for a price that is unimaginable today—as little as $100 to $300 a month (in 2025 dollars).&lt;/p&gt;
      &lt;p&gt;In the late 19th and early 20th centuries, landlords converted thousands of houses, hotels, apartment buildings, and commercial buildings into SROs, and by 1950, SRO units made up about 10% of all rental units in some major cities. But beginning in the mid-1950s, as some politicians and vocal members of the public turned against SROs and the people who lived in them, major cities across the country revised zoning and building codes to force or encourage landlords to eliminate SRO units and to prohibit the development of new ones. Over the next several decades, governments and developers gradually demolished thousands of SROs or converted them to other uses, including boutique hotels for tourists. And as SROs disappeared, homelessness—which had been rare from at least the end of the Great Depression to the late 1970s—exploded nationwide.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The Pew piece does an excellent job of documenting how laws are beginning to change. I especially appreciated this point: the simplest reform is to stop making it illegal for unrelated people to share a home!&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Perhaps the simplest method of creating low-cost shared housing is to allow unrelated individuals to share a house in the same way that relatives are allowed to share a house.67 But many communities limit the number of unrelated people who can live together—in some places, to as few as two. Such laws make sharing a house for a group of roommates—which usually enables rents lower than having an individual apartment—illegal. The U.S. has a record number of unused bedrooms, but many cannot be rented because of restrictions on house sharing by unrelated roommates, even if that would be the most profitable use for the landlord and the most affordable option for the tenants.68 To enable this low-cost housing option, Iowa, Oregon, and Colorado all passed bipartisan legislation to strike down local codes that prohibit house-sharing (in 2017, 2021, and 2024, respectively).69&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;So many of our problems are created by busybodies and do-gooders who prevent people from using their own property.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45347043</guid><pubDate>Tue, 23 Sep 2025 13:51:56 +0000</pubDate></item><item><title>Libghostty is coming</title><link>https://mitchellh.com/writing/libghostty-is-coming</link><description>&lt;doc fingerprint="14cbd038e7943b89"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Mitchell Hashimoto&lt;/head&gt;&lt;head rend="h1"&gt;Libghostty Is Coming&lt;/head&gt;&lt;p&gt;Over two years ago, in one of my first public talks about Ghostty, I shared my vision for &lt;code&gt;libghostty&lt;/code&gt;: an embeddable library for any application
to embed their own fully functional, modern, and fast terminal emulator.
Libghostty is finally starting to take shape, and I'm excited to share
more details about my plans for it.&lt;/p&gt;&lt;p&gt;The first libghostty library will be &lt;code&gt;libghostty-vt&lt;/code&gt;: a zero-dependency
library that provides an API for parsing terminal sequences and
maintaining terminal state, extracted directly from Ghostty's real-world proven
core. It doesn't even require libc!&lt;/p&gt;&lt;p&gt;Disclaimer: This post is predominantly a roadmap update for libghostty and announcing what will be the first shippable component of it. The Zig API is available for testing now, but the C API is not ready and will be coming very shortly. In both cases, it is early testing quality and not ready for general usage.&lt;/p&gt;&lt;head rend="h2"&gt;Why libghostty?&lt;/head&gt;&lt;p&gt;Let's start with some background on why I believe libghostty must exist.&lt;/p&gt;&lt;p&gt;There are hundreds of programs that implement some form of terminal emulation. The most obvious are the actual general purpose terminal emulators like Ghostty, Kitty, iTerm2, etc. But terminal multiplexers like tmux or zellij are also full terminal emulators!1 Editors embed their own terminal emulators too, such as jediterm for JetBrains products, Xterm.js for VS Code, or Alacritty in Zed.&lt;/p&gt;&lt;p&gt;In addition to fully functional terminal emulators, many websites and applications implement read-only terminal emulation to display logs or command output. For example, GitHub Actions output parses simple color sequences (but not much else). And hosting providers like Vercel or Render implement a simple form of terminal emulation allowing line clearing and redrawing within build logs as well as parsing colors.&lt;/p&gt;&lt;p&gt;Many of these implementations are ad-hoc, one-off solutions. They aren't using any shared library or codebase.2 Terminal emulation is a classic problem that appears simple on the surface but is riddled with unexpected complexities and edge cases.3 As a result, most of these implementations are incomplete, buggy, and slow.4&lt;/p&gt;&lt;p&gt;Beyond correctness, implementing any form of terminal emulation is a waste of time for most developers. Terminal emulation is not the core business of JetBrains, Visual Studio Code, GitHub, Vercel, Render, etc. It'd benefit them if they could have a stable, reusable solution that's consistent everywhere.&lt;/p&gt;&lt;p&gt;My answer to this is libghostty: a cross-platform, minimal dependency library that exposes a C API so feature-rich, correct, and fast terminal functionality can be embedded by any application anywhere.&lt;/p&gt;&lt;head rend="h2"&gt;&lt;code&gt;libghostty-vt&lt;/code&gt;&lt;/head&gt; The Beginning: &lt;p&gt;The first libghostty library will be &lt;code&gt;libghostty-vt&lt;/code&gt;: a zero-dependency
(not even libc) library that provides an API for parsing terminal sequences
and maintaining terminal state such as cursor position, current styles,
text wrapping, and more.&lt;/p&gt;&lt;p&gt;Parsing terminal sequences is the most core functionality of a terminal emulator, and is required by full terminal emulators like Ghostty down to simple read-only style-only views such as GitHub Actions or Vercel build output.&lt;/p&gt;&lt;p&gt;The state diagram might appear relatively simple at first glance, but an implementation is unexpectedly challenging to get right. For example, Jediterm doesn't handle intermediates correctly, causing the widely supported "change cursor shape" sequence to swallow a character in every JetBrains editor at the time of this post.&lt;/p&gt;&lt;p&gt;For style-only parsing, many developers skip the full state diagram. Instead, they do some light web searching, parse simple ANSI sequences such as &lt;code&gt;\e[31&lt;/code&gt; or &lt;code&gt;\e[41m&lt;/code&gt;, and claim "color support."
But style-only sequences are vastly more complex than that, for example they
support RGB which itself can be in a dozen formats.
And I still haven't found a single web console that renders
this complex style sequence
correctly.5&lt;/p&gt;&lt;p&gt;&lt;code&gt;libghostty-vt&lt;/code&gt; aims to fix all of this.&lt;/p&gt;&lt;p&gt;&lt;code&gt;libghostty-vt&lt;/code&gt; is extracted from Ghostty and inherits all of the real world
benefits: SIMD-optimized parsing, very good Unicode support, highly optimized
memory usage, a robust fuzzed and Valgrind-tested codebase, excellent feature
compatibility such as parsing Kitty Graphics Protocol or Tmux Control Mode, and more.&lt;/p&gt;&lt;p&gt;All of this is packaged up into a single zero-dependency C API (it doesn't even rely on libc), allowing it to be easily embedded into any popular language ecosystem.&lt;/p&gt;&lt;p&gt;Given the minimal footprint, &lt;code&gt;libghostty-vt&lt;/code&gt; will be widely portable.
Initial targets will be macOS and Linux for both &lt;code&gt;x86_64&lt;/code&gt; and &lt;code&gt;aarch64&lt;/code&gt;
architectures, since those are the primary targets for Ghostty the
application. But I plan to expand support to additional targets such as Windows,
embedded devices, and the web via WASM. &lt;code&gt;libghostty&lt;/code&gt; will have broader
support than Ghostty the GUI, due to its tighter scope.&lt;/p&gt;&lt;head rend="h2"&gt;The Long Term&lt;/head&gt;&lt;p&gt;&lt;code&gt;libghostty-vt&lt;/code&gt; is just the beginning. Longer term, we will provide more
&lt;code&gt;libghostty-&amp;lt;x&amp;gt;&lt;/code&gt; libs that expose additional functionality such as input
handling (keyboard encoding is a big one), GPU rendering (provide us with
an OpenGL or Metal surface and we'll take care of the rest), GTK widgets
and Swift frameworks that handle the entire terminal view, and more.&lt;/p&gt;&lt;p&gt;As fundamental pieces stabilize, we will continue to offer more and more functionality. These will be structured as a family of libraries to minimize dependency requirements, code size, and overall maintenance complexity.&lt;/p&gt;&lt;head rend="h2"&gt;&lt;code&gt;libghostty-vt&lt;/code&gt; Status&lt;/head&gt;&lt;p&gt;I just merged the pull request exposing &lt;code&gt;libghostty-vt&lt;/code&gt; as a Zig module.
This PR includes a minimal example program, too. If you're a Zig developer,
you can start experimenting with &lt;code&gt;libghostty-vt&lt;/code&gt; immediately.&lt;/p&gt;&lt;p&gt;The C API isn't ready yet, but it is what I'm working on right now and it'll be available for testing soon. All of the work required is defining the C API, since the core logic is of course all there and has been used by Ghostty for years. Plus, the Ghostty macOS app already consumes an internal-only C API.&lt;/p&gt;&lt;p&gt;If you look at the internal-only C header, please ignore the mess. It isn't a good C API. It is internal-only and exists to satisfy the needs of the macOS application. This isn't a generally consumable libghostty, although it is used by real commercial products already to embed Ghostty. We'll take a clean slate approach defining the C API for wider usage.&lt;/p&gt;&lt;p&gt;I plan to version &lt;code&gt;libghostty&lt;/code&gt; separately from Ghostty the application.
This blog post marks the public alpha (not promising API stability)
and I'm hoping to motivate some developers to come use it and eventually
write some language bindings once the C API is ready.&lt;/p&gt;&lt;p&gt;I hope to ship a tagged version of &lt;code&gt;libghostty-vt&lt;/code&gt; within the next 6 months,
but it'll all depend on if its ready or not.&lt;/p&gt;&lt;head rend="h2"&gt;Looking for Feedback&lt;/head&gt;&lt;p&gt;We're at the critical stage of libghostty where we're designing the API, and the best way to design an API is with feedback from real consumers. Ghostty is one consumer, we have some community members working on other libghostty-consuming projects, but we could use as many as we can get!&lt;/p&gt;&lt;p&gt;If you see a use case in your projects or organization for libghostty, please join the Ghostty Discord and collaborate with the developers working on this. If you don't want to join Discord, email me (email in the footer of this website).&lt;/p&gt;&lt;p&gt;The state of &lt;code&gt;libghostty&lt;/code&gt; at this stage could be considered alpha,
so don't expect a polished, stable experience. We're looking for hackers
that want to get on the ground early.&lt;/p&gt;&lt;p&gt;The "alpha" quality is with respect to the API (functions and types) itself. The core logic is shared with Ghostty and is extremely stable and proven in the real world.&lt;/p&gt;&lt;head rend="h2"&gt;The Next Frontier&lt;/head&gt;&lt;p&gt;I'm super excited that Ghostty the application has finally reached the stability where we can start moving towards the &lt;code&gt;libghostty&lt;/code&gt; goal.
&lt;code&gt;libghostty&lt;/code&gt; is the next frontier for Ghostty and I think it has the
ability to make a far larger impact than Ghostty can as a standalone
application itself.&lt;/p&gt;&lt;p&gt;Don't worry, there's plenty of changes for Ghostty the application and none of this diminishes my excitement or plans for that. Wider usage of &lt;code&gt;libghostty&lt;/code&gt; will result in a more feature rich and stable
Ghostty application, too, since Ghostty itself is a consumer of
&lt;code&gt;libghostty&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;Boo. 👻&lt;/p&gt;&lt;head rend="h2"&gt;Footnotes&lt;/head&gt;&lt;list rend="ol"&gt;&lt;item&gt;&lt;p&gt;These own the pty to their children, parse all the escape codes, manage screen state, and ultimately "render" by emitting their own escape codes to a parent terminal emulator. ↩&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;Many websites do use Xterm.js and the GTK ecosystem has libvte. But this only covers a tiny fraction of the landscape and each of the aforementioned examples are limited in their own scope. ↩&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;I've spent the better part of 3 years working on terminal emulation and we're still finding weird edge cases. ↩&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;Here are a couple real examples of impactful issues: Jediterm didn't handle intermediates properly, Apple's Terminal.app just bleeds DCS sequences into the output. I'm not calling anyone out specifically, just showing that these issues exist and terminal emulation is very hard to get right. ↩&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;A lot of general terminal emulators also got this wrong, including Ghostty just 9 months ago. ↩&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45347117</guid><pubDate>Tue, 23 Sep 2025 13:56:28 +0000</pubDate></item><item><title>OpenDataLoader-PDF: An open source tool for structured PDF parsing</title><link>https://github.com/opendataloader-project/opendataloader-pdf</link><description>&lt;doc fingerprint="3390da1012851aca"&gt;
  &lt;main&gt;
    &lt;p&gt;Safe, Open, High-Performance — PDF for AI&lt;/p&gt;
    &lt;p&gt;OpenDataLoader-PDF converts PDFs into JSON, Markdown or Html — ready to feed into modern AI stacks (LLMs, vector search, and RAG).&lt;/p&gt;
    &lt;p&gt;It reconstructs document layout (headings, lists, tables, and reading order) so the content is easier to chunk, index, and query. Powered by fast, heuristic, rule-based inference, it runs entirely on your local machine and delivers high-throughput processing for large document sets. AI-safety is enabled by default and automatically filters likely prompt-injection content embedded in PDFs to reduce downstream risk.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;🧾 Rich, Structured Output — JSON, Markdown or Html&lt;/item&gt;
      &lt;item&gt;🧩 Layout Reconstruction — Headings, Lists, Tables, Images, Reading Order&lt;/item&gt;
      &lt;item&gt;⚡ Fast &amp;amp; Lightweight — Rule-Based Heuristic, High-Throughput, No GPU&lt;/item&gt;
      &lt;item&gt;🔒 Local-First Privacy — Runs fully on your machine&lt;/item&gt;
      &lt;item&gt;🛡️ AI-Safety — Auto-Filters likely prompt-injection content - Learn more about AI-Safety&lt;/item&gt;
      &lt;item&gt;🖍️ Annotated PDF Visualization — See detected structures overlaid on the original&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;🖨️ OCR for scanned PDFs — Extract data from image-only pages&lt;/item&gt;
      &lt;item&gt;🧠 Table AI option — Higher accuracy for tables with borderless or merged cells&lt;/item&gt;
      &lt;item&gt;⚡ Performance Benchmarks — Transparent evaluations with open datasets and metrics, reported regularly&lt;/item&gt;
      &lt;item&gt;🛡️ AI Red Teaming — Transparent adversarial benchmarks with datasets and metrics, reported regularly&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Java 11 or higher must be installed and available in your system's PATH.&lt;/item&gt;
      &lt;item&gt;Python 3.9+&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;pip install -U opendataloader-pdf&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;input_path can be either the path to a single document or the path to a folder.&lt;/item&gt;
      &lt;item&gt;If you don’t specify an output_folder, the output data will be saved in the same directory as the input document.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;import opendataloader_pdf

opendataloader_pdf.run(
    input_path="path/to/document.pdf",
    output_folder="path/to/output",
    generate_markdown=True,
    generate_html=True,
    generate_annotated_pdf=True,
)&lt;/code&gt;
    &lt;p&gt;The main function to process PDFs.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Parameter&lt;/cell&gt;
        &lt;cell role="head"&gt;Type&lt;/cell&gt;
        &lt;cell role="head"&gt;Required&lt;/cell&gt;
        &lt;cell role="head"&gt;Default&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;
          &lt;code&gt;input_path&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;str&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;✅ Yes&lt;/cell&gt;
        &lt;cell&gt;—&lt;/cell&gt;
        &lt;cell&gt;Path to the input PDF file or folder.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;
          &lt;code&gt;output_folder&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;str&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;input folder&lt;/cell&gt;
        &lt;cell&gt;Path to the output folder.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;
          &lt;code&gt;password&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;str&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;None&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Password for the PDF file.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;
          &lt;code&gt;replace_invalid_chars&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;str&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;" "&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Character to replace invalid or unrecognized characters (e.g., �, \u0000)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;
          &lt;code&gt;content_safety_off&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;str&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;None&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Disables one or more content safety filters. Accepts a comma-separated list of filter names. Arguments: all, hidden-text, off-page, tiny, hidden-ocg.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;
          &lt;code&gt;generate_markdown&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;bool&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;False&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;If &lt;code&gt;True&lt;/code&gt;, generates a Markdown output file.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;
          &lt;code&gt;generate_html&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;bool&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;False&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;If &lt;code&gt;True&lt;/code&gt;, generates an HTML output file.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;
          &lt;code&gt;generate_annotated_pdf&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;bool&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;False&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;If &lt;code&gt;True&lt;/code&gt;, generates an annotated PDF output file.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;
          &lt;code&gt;keep_line_breaks&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;bool&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;False&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;If &lt;code&gt;True&lt;/code&gt;, keeps line breaks in the output.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;
          &lt;code&gt;html_in_markdown&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;bool&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;False&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;If &lt;code&gt;True&lt;/code&gt;, uses HTML in the Markdown output.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;
          &lt;code&gt;add_image_to_markdown&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;bool&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;False&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;If &lt;code&gt;True&lt;/code&gt;, adds images to the Markdown output.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;debug&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;bool&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;False&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;If &lt;code&gt;True&lt;/code&gt;, prints CLI messages to the console during execution.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Note: This package is a wrapper around a Java CLI and is intended for use in a Node.js backend environment. It cannot be used in a browser-based frontend.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Java 11 or higher must be installed and available in your system's PATH.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;npm install @opendataloader/pdf&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;inputPath&lt;/code&gt;can be either the path to a single document or the path to a folder.&lt;/item&gt;
      &lt;item&gt;If you don’t specify an &lt;code&gt;outputFolder&lt;/code&gt;, the output data will be saved in the same directory as the input document.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;import { run } from '@opendataloader/pdf';

async function main() {
  try {
    const output = await run('path/to/document.pdf', {
      outputFolder: 'path/to/output',
      generateMarkdown: true,
      generateHtml: true,
      generateAnnotatedPdf: true,
      debug: true,
    });
    console.log('PDF processing complete.', output);
  } catch (error) {
    console.error('Error processing PDF:', error);
  }
}

main();&lt;/code&gt;
    &lt;p&gt;
      &lt;code&gt;run(inputPath: string, options?: RunOptions): Promise&amp;lt;string&amp;gt;&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;The main function to process PDFs.&lt;/p&gt;
    &lt;p&gt;Parameters&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Parameter&lt;/cell&gt;
        &lt;cell role="head"&gt;Type&lt;/cell&gt;
        &lt;cell role="head"&gt;Required&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;inputPath&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;string&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;✅ Yes&lt;/cell&gt;
        &lt;cell&gt;Path to the input PDF file or folder.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;options&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;RunOptions&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;Configuration options for the run.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;RunOptions&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Property&lt;/cell&gt;
        &lt;cell role="head"&gt;Type&lt;/cell&gt;
        &lt;cell role="head"&gt;Default&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;outputFolder&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;string&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;undefined&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Path to the output folder. If not set, output is saved next to the input.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;password&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;string&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;undefined&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Password for the PDF file.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;replaceInvalidChars&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;string&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;" "&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Character to replace invalid or unrecognized characters (e.g., , \u0000).&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;contentSafetyOff&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;string&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;undefined&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Disables one or more content safety filters. Accepts a comma-separated list of filter names. Arguments: all, hidden-text, off-page, tiny, hidden-ocg.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;generateMarkdown&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;boolean&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;false&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;If &lt;code&gt;true&lt;/code&gt;, generates a Markdown output file.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;generateHtml&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;boolean&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;false&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;If &lt;code&gt;true&lt;/code&gt;, generates an HTML output file.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;generateAnnotatedPdf&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;boolean&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;false&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;If &lt;code&gt;true&lt;/code&gt;, generates an annotated PDF output file.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;keepLineBreaks&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;boolean&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;false&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;If &lt;code&gt;true&lt;/code&gt;, keeps line breaks in the output.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;htmlInMarkdown&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;boolean&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;false&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;If &lt;code&gt;true&lt;/code&gt;, uses HTML in the Markdown output.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;addImageToMarkdown&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;boolean&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;false&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;If &lt;code&gt;true&lt;/code&gt;, adds images to the Markdown output.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;debug&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;boolean&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;false&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;If &lt;code&gt;true&lt;/code&gt;, prints CLI messages to the console during execution.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;For various example templates, including Gradle and Maven, please refer to https://github.com/opendataloader-project/opendataloader-pdf/tree/main/examples/java.&lt;/p&gt;
    &lt;p&gt;To include OpenDataLoader PDF in your Maven project, add the dependency below to your &lt;code&gt;pom.xml&lt;/code&gt; file.&lt;/p&gt;
    &lt;p&gt;Check for the latest version on Maven Central.&lt;/p&gt;
    &lt;code&gt;&amp;lt;project&amp;gt;
    &amp;lt;!-- other configurations... --&amp;gt;

    &amp;lt;dependencies&amp;gt;
        &amp;lt;dependency&amp;gt;
            &amp;lt;groupId&amp;gt;org.opendataloader&amp;lt;/groupId&amp;gt;
            &amp;lt;artifactId&amp;gt;opendataloader-pdf-core&amp;lt;/artifactId&amp;gt;
            &amp;lt;version&amp;gt;1.0.0&amp;lt;/version&amp;gt;
        &amp;lt;/dependency&amp;gt;
    &amp;lt;/dependencies&amp;gt;

    &amp;lt;repositories&amp;gt;
        &amp;lt;repository&amp;gt;
            &amp;lt;snapshots&amp;gt;
                &amp;lt;enabled&amp;gt;true&amp;lt;/enabled&amp;gt;
            &amp;lt;/snapshots&amp;gt;
            &amp;lt;id&amp;gt;vera-dev&amp;lt;/id&amp;gt;
            &amp;lt;name&amp;gt;Vera development&amp;lt;/name&amp;gt;
            &amp;lt;url&amp;gt;https://artifactory.openpreservation.org/artifactory/vera-dev&amp;lt;/url&amp;gt;
        &amp;lt;/repository&amp;gt;
    &amp;lt;/repositories&amp;gt;
    &amp;lt;pluginRepositories&amp;gt;
        &amp;lt;pluginRepository&amp;gt;
            &amp;lt;snapshots&amp;gt;
                &amp;lt;enabled&amp;gt;false&amp;lt;/enabled&amp;gt;
            &amp;lt;/snapshots&amp;gt;
            &amp;lt;id&amp;gt;vera-dev&amp;lt;/id&amp;gt;
            &amp;lt;name&amp;gt;Vera development&amp;lt;/name&amp;gt;
            &amp;lt;url&amp;gt;https://artifactory.openpreservation.org/artifactory/vera-dev&amp;lt;/url&amp;gt;
        &amp;lt;/pluginRepository&amp;gt;
    &amp;lt;/pluginRepositories&amp;gt;

    &amp;lt;!-- other configurations... --&amp;gt;
&amp;lt;/project&amp;gt;&lt;/code&gt;
    &lt;p&gt;To integrate Layout recognition API into Java code, one can follow the sample code below.&lt;/p&gt;
    &lt;code&gt;import org.opendataloader.pdf.api.Config;
import org.opendataloader.pdf.api.OpenDataLoaderPDF;

import java.io.IOException;

public class Sample {

    public static void main(String[] args) {
        Config config = new Config();
        config.setOutputFolder("path/to/output");
        config.setGeneratePDF(true);
        config.setGenerateMarkdown(true);
        config.setGenerateHtml(true);

        try {
            OpenDataLoaderPDF.processFile("path/to/document.pdf", config);
        } catch (Exception exception) {
            //exception during processing
        }
    }
}&lt;/code&gt;
    &lt;p&gt;The full API documentation is available at javadoc&lt;/p&gt;
    &lt;p&gt;Download sample PDF&lt;/p&gt;
    &lt;code&gt;curl -L -o 1901.03003.pdf https://arxiv.org/pdf/1901.03003&lt;/code&gt;
    &lt;p&gt;Run opendataloader-pdf in Docker container&lt;/p&gt;
    &lt;code&gt;docker run --rm -v "$PWD":/work ghcr.io/opendataloader-project/opendataloader-pdf-cli:latest /work/1901.03003.pdf --markdown --html --pdf
&lt;/code&gt;
    &lt;p&gt;Build and install using Maven command:&lt;/p&gt;
    &lt;code&gt;mvn clean install -f java/pom.xml&lt;/code&gt;
    &lt;p&gt;If the build is successful, the resulting &lt;code&gt;jar&lt;/code&gt; file will be created in the path below.&lt;/p&gt;
    &lt;code&gt;java/opendataloader-pdf-cli/target&lt;/code&gt;
    &lt;code&gt;java -jar opendataloader-pdf-cli-&amp;lt;VERSION&amp;gt;.jar [options] &amp;lt;INPUT FILE OR FOLDER&amp;gt;&lt;/code&gt;
    &lt;p&gt;This generates a JSON file with layout recognition results in the specified output folder. Additionally, annotated PDF with recognized structures, Markdown and Html are generated if options &lt;code&gt;--pdf&lt;/code&gt;, &lt;code&gt;--markdown&lt;/code&gt; and &lt;code&gt;--html&lt;/code&gt; are specified.&lt;/p&gt;
    &lt;p&gt;By default all line breaks and hyphenation characters are removed, the Markdown does not include any images and does not use any HTML.&lt;/p&gt;
    &lt;p&gt;The option &lt;code&gt;--keep-line-breaks&lt;/code&gt; to preserve the original line breaks text content in JSON and Markdown output.
The option &lt;code&gt;--content-safety-off&lt;/code&gt; disables one or more content safety filters. Accepts a comma-separated list of filter names.
The option &lt;code&gt;--markdown-with-html&lt;/code&gt; enables use of HTML in Markdown, which may improve Markdown preview in processors that support HTML tags.
The option &lt;code&gt;--markdown-with-images&lt;/code&gt; enables inclusion of image references into the output Markdown.
The option &lt;code&gt;--replace-invalid-chars&lt;/code&gt; replaces invalid or unrecognized characters (e.g., �, \u0000) with the specified character.
The images are extracted from PDF as individual files and stored in a subfolder next to the Markdown output.&lt;/p&gt;
    &lt;code&gt;Options:
-o,--output-dir &amp;lt;arg&amp;gt;           Specifies the output directory for generated files
--keep-line-breaks              Preserves original line breaks in the extracted text
--content-safety-off &amp;lt;arg&amp;gt;      Disables one or more content safety filters. Accepts a comma-separated list of filter names. Arguments: all, hidden-text, off-page, tiny, hidden-ocg
--markdown-with-html            Sets the data extraction output format to Markdown with rendering complex elements like tables as HTML for better structure
--markdown-with-images          Sets the data extraction output format to Markdown with extracting images from the PDF and includes them as links
--markdown                      Sets the data extraction output format to Markdown
--html                          Sets the data extraction output format to HTML
-p,--password &amp;lt;arg&amp;gt;             Specifies the password for an encrypted PDF
--pdf                           Generates a new PDF file where the extracted layout data is visualized as annotations
--replace-invalid-chars &amp;lt;arg&amp;gt;   Replaces invalid or unrecognized characters (e.g., �, \u0000) with the specified character
&lt;/code&gt;
    &lt;p&gt;Root json node&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Field&lt;/cell&gt;
        &lt;cell role="head"&gt;Type&lt;/cell&gt;
        &lt;cell role="head"&gt;Optional&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;file name&lt;/cell&gt;
        &lt;cell&gt;string&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;Name of processed pdf file&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;number of pages&lt;/cell&gt;
        &lt;cell&gt;integer&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;Number of pages in pdf file&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;author&lt;/cell&gt;
        &lt;cell&gt;string&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;Author of pdf file&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;title&lt;/cell&gt;
        &lt;cell&gt;string&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;Title of pdf file&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;creation date&lt;/cell&gt;
        &lt;cell&gt;string&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;Creation date of pdf file&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;modification date&lt;/cell&gt;
        &lt;cell&gt;string&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;Modification date of pdf file&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;kids&lt;/cell&gt;
        &lt;cell&gt;array&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;Array of detected content elements&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Common fields of content json nodes&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Field&lt;/cell&gt;
        &lt;cell role="head"&gt;Type&lt;/cell&gt;
        &lt;cell role="head"&gt;Optional&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;id&lt;/cell&gt;
        &lt;cell&gt;integer&lt;/cell&gt;
        &lt;cell&gt;yes&lt;/cell&gt;
        &lt;cell&gt;Unique id of content element&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;level&lt;/cell&gt;
        &lt;cell&gt;string&lt;/cell&gt;
        &lt;cell&gt;yes&lt;/cell&gt;
        &lt;cell&gt;Level of content element&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;type&lt;/cell&gt;
        &lt;cell&gt;string&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;Type of content element&lt;p&gt;Possible types:&lt;/p&gt;&lt;code&gt;footer&lt;/code&gt;, &lt;code&gt;header&lt;/code&gt;, &lt;code&gt;heading&lt;/code&gt;, &lt;code&gt;line&lt;/code&gt;, &lt;code&gt;table&lt;/code&gt;, &lt;code&gt;table row&lt;/code&gt;, &lt;code&gt;table cell&lt;/code&gt;, &lt;code&gt;paragraph&lt;/code&gt;, &lt;code&gt;list&lt;/code&gt;, &lt;code&gt;list item&lt;/code&gt;, &lt;code&gt;image&lt;/code&gt;, &lt;code&gt;line art&lt;/code&gt;, &lt;code&gt;caption&lt;/code&gt;, &lt;code&gt;text block&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;page number&lt;/cell&gt;
        &lt;cell&gt;integer&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;Page number of content element&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;bounding box&lt;/cell&gt;
        &lt;cell&gt;array&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;Bounding box of content element&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Specific fields of text content json nodes (&lt;code&gt;caption&lt;/code&gt;, &lt;code&gt;heading&lt;/code&gt;, &lt;code&gt;paragraph&lt;/code&gt;)&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Field&lt;/cell&gt;
        &lt;cell role="head"&gt;Type&lt;/cell&gt;
        &lt;cell role="head"&gt;Optional&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;font&lt;/cell&gt;
        &lt;cell&gt;string&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;Font name of text&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;font size&lt;/cell&gt;
        &lt;cell&gt;double&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;Font size of text&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;text color&lt;/cell&gt;
        &lt;cell&gt;array&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;Color of text&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;content&lt;/cell&gt;
        &lt;cell&gt;string&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;Text value&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Specific fields of &lt;code&gt;table&lt;/code&gt; json nodes&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Field&lt;/cell&gt;
        &lt;cell role="head"&gt;Type&lt;/cell&gt;
        &lt;cell role="head"&gt;Optional&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;number of rows&lt;/cell&gt;
        &lt;cell&gt;integer&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;Number of table rows&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;number of columns&lt;/cell&gt;
        &lt;cell&gt;integer&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;Number of table columns&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;rows&lt;/cell&gt;
        &lt;cell&gt;array&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;Array of table rows&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;previous table id&lt;/cell&gt;
        &lt;cell&gt;integer&lt;/cell&gt;
        &lt;cell&gt;yes&lt;/cell&gt;
        &lt;cell&gt;Id of previous connected table&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;next table id&lt;/cell&gt;
        &lt;cell&gt;integer&lt;/cell&gt;
        &lt;cell&gt;yes&lt;/cell&gt;
        &lt;cell&gt;Id of next connected table&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Specific fields of &lt;code&gt;table row&lt;/code&gt; json nodes&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Field&lt;/cell&gt;
        &lt;cell role="head"&gt;Type&lt;/cell&gt;
        &lt;cell role="head"&gt;Optional&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;row number&lt;/cell&gt;
        &lt;cell&gt;integer&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;Number of table row&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;cells&lt;/cell&gt;
        &lt;cell&gt;array&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;Array of table cells&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Specific fields of &lt;code&gt;table cell&lt;/code&gt; json nodes&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Field&lt;/cell&gt;
        &lt;cell role="head"&gt;Type&lt;/cell&gt;
        &lt;cell role="head"&gt;Optional&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;row number&lt;/cell&gt;
        &lt;cell&gt;integer&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;Row number of table cell&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;column number&lt;/cell&gt;
        &lt;cell&gt;integer&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;Column number of table cell&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;row span&lt;/cell&gt;
        &lt;cell&gt;integer&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;Row span of table cell&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;column span&lt;/cell&gt;
        &lt;cell&gt;integer&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;Column span of table cell&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;kids&lt;/cell&gt;
        &lt;cell&gt;array&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;Array of table cell content elements&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Specific fields of &lt;code&gt;heading&lt;/code&gt; json nodes&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Field&lt;/cell&gt;
        &lt;cell role="head"&gt;Type&lt;/cell&gt;
        &lt;cell role="head"&gt;Optional&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;heading level&lt;/cell&gt;
        &lt;cell&gt;integer&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;Heading level of heading&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Specific fields of &lt;code&gt;list&lt;/code&gt; json nodes&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Field&lt;/cell&gt;
        &lt;cell role="head"&gt;Type&lt;/cell&gt;
        &lt;cell role="head"&gt;Optional&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;number of list items&lt;/cell&gt;
        &lt;cell&gt;integer&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;Number of list items&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;numbering style&lt;/cell&gt;
        &lt;cell&gt;string&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;Numbering style of this list&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;previous list id&lt;/cell&gt;
        &lt;cell&gt;integer&lt;/cell&gt;
        &lt;cell&gt;yes&lt;/cell&gt;
        &lt;cell&gt;Id of previous connected list&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;next list id&lt;/cell&gt;
        &lt;cell&gt;integer&lt;/cell&gt;
        &lt;cell&gt;yes&lt;/cell&gt;
        &lt;cell&gt;Id of next connected list&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;list items&lt;/cell&gt;
        &lt;cell&gt;array&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;Array of list item content elements&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Specific fields of &lt;code&gt;list item&lt;/code&gt; json nodes&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Field&lt;/cell&gt;
        &lt;cell role="head"&gt;Type&lt;/cell&gt;
        &lt;cell role="head"&gt;Optional&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;kids&lt;/cell&gt;
        &lt;cell&gt;array&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;Array of list item content elements&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Specific fields of &lt;code&gt;header&lt;/code&gt; and &lt;code&gt;footer&lt;/code&gt; json nodes&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Field&lt;/cell&gt;
        &lt;cell role="head"&gt;Type&lt;/cell&gt;
        &lt;cell role="head"&gt;Optional&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;kids&lt;/cell&gt;
        &lt;cell&gt;array&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;Array of header/footer content elements&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Specific fields of &lt;code&gt;text block&lt;/code&gt; json nodes&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Field&lt;/cell&gt;
        &lt;cell role="head"&gt;Type&lt;/cell&gt;
        &lt;cell role="head"&gt;Optional&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;kids&lt;/cell&gt;
        &lt;cell&gt;array&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;Array of text block content elements&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;We believe that great software is built together.&lt;/p&gt;
    &lt;p&gt;Your contributions are vital to the success of this project.&lt;/p&gt;
    &lt;p&gt;Please read CONTRIBUTING.md for details on how to contribute.&lt;/p&gt;
    &lt;p&gt;Have questions or need a little help? We're here for you!🤗&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;GitHub Discussions: For Q&amp;amp;A and general chats. Let's talk! 🗣️&lt;/item&gt;
      &lt;item&gt;GitHub Issues: Found a bug? 🐛 Please report it here so we can fix it.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We love our brand and want to protect it!&lt;/p&gt;
    &lt;p&gt;This project may contain trademarks, logos, or brand names for our products and services.&lt;/p&gt;
    &lt;p&gt;To ensure everyone is on the same page, please remember these simple rules:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Authorized Use: You're welcome to use our logos and trademarks, but you must follow our official brand guidelines.&lt;/item&gt;
      &lt;item&gt;No Confusion: When you use our trademarks in a modified version of this project, it should never cause confusion or imply that Hancom officially sponsors or endorses your version.&lt;/item&gt;
      &lt;item&gt;Third-Party Brands: Any use of trademarks or logos from other companies must follow that company’s specific policies.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This project is licensed under the Mozilla Public License 2.0.&lt;/p&gt;
    &lt;p&gt;For the full license text, see LICENSE.&lt;/p&gt;
    &lt;p&gt;For information on third-party libraries and components, see:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45347147</guid><pubDate>Tue, 23 Sep 2025 13:58:48 +0000</pubDate></item><item><title>x402 — An open protocol for internet-native payments</title><link>https://www.x402.org/</link><description>&lt;doc fingerprint="a538813a0d0efbd5"&gt;
  &lt;main&gt;
    &lt;p&gt;An open protocol for internet-native payments&lt;/p&gt;
    &lt;p&gt;Built around the HTTP 402 status code, x402 enables users to pay for resources via API without registration, emails, OAuth, or complex signatures.&lt;/p&gt;
    &lt;p&gt;x402 as a protocol has 0 fees for either the customer or the merchant.&lt;/p&gt;
    &lt;p&gt;Accept payments at the speed of the blockchain. Money in your wallet in 2 seconds, not T+2.&lt;/p&gt;
    &lt;p&gt;x402 is not tied to any specific blockchain or token, its a neutral standard open to integration by all.&lt;/p&gt;
    &lt;p&gt;As little as 1 line of middleware code or configuration in your existing web server stack and you can start accepting payments. Customers and agents aren't required to create an account or provide any personal information.&lt;/p&gt;
    &lt;p&gt;Anyone can implement or extend x402. It's not tied to any centralized provider, and encourages broad community participation.&lt;/p&gt;
    &lt;p&gt;Activates the dormant 402 HTTP status code and works with any HTTP stack. It works simply via headers and status codes on your existing HTTP server.&lt;/p&gt;
    &lt;p&gt;x402 unlocks new monetization models, offering developers and content creators a frictionless way to earn revenue from small transactions without forcing subscriptions or showing ads.&lt;/p&gt;
    &lt;p&gt;Agents can use the x402 Protocol to pay for API requests in real-time.&lt;/p&gt;
    &lt;p&gt;Using x402, customers can easily access storage services without account creation.&lt;/p&gt;
    &lt;p&gt;x402 unlocks instant transactions, enabling true micropayments for content.&lt;/p&gt;
    &lt;p&gt;Just add a single line of code in your app, and you can require a USDC payment for each incoming request.&lt;/p&gt;
    &lt;quote&gt;paymentMiddleware("0xYourAddress", {"/your-endpoint": "$0.01"}); // and thats it!&lt;/quote&gt;
    &lt;p&gt;If a request arrives without payment, the server responds with HTTP 402, prompting the client to pay and retry.&lt;/p&gt;
    &lt;quote&gt;HTTP/1.1 402 Payment Required&lt;/quote&gt;
    &lt;p&gt;x402 allows any web developer to accept crypto payments without the complexity of having to interact with the blockchain.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45347335</guid><pubDate>Tue, 23 Sep 2025 14:14:46 +0000</pubDate></item><item><title>Agents turn simple keyword search into compelling search experiences</title><link>https://softwaredoug.com/blog/2025/09/22/reasoning-agents-need-bad-search</link><description>&lt;doc fingerprint="3b1fda320b812e0a"&gt;
  &lt;main&gt;
    &lt;p&gt;A traditional RAG system often looks like the search we’ve built for years. An LLM calls a search API. We have query understanding and reranking functionality under the hood. All optimized for user engagement.&lt;/p&gt;
    &lt;p&gt;This thick-daddy search API does quite a bit, as shown below:&lt;/p&gt;
    &lt;p&gt;With small modifications (chunking, a focus on vector retrieval) the RAG implementations look familiar.&lt;/p&gt;
    &lt;p&gt;Agents, however, come with the ability to reason. And like human users, they might try a query, see the results, adjust their query, and zero-in on the best search terms to find what they want.&lt;/p&gt;
    &lt;p&gt;Like human users, the agent can build a “mental model” of how the search tool works. If there’s a clear, predictable correspondence between stimulus and output, the agent will figure it out. However, a large, complex system might not be as transparent.&lt;/p&gt;
    &lt;p&gt;Because of this, I’d argue the traditional, thick search APIs are counterproductive to being used by agents. They may be too complex for agents to reason about effectively.&lt;/p&gt;
    &lt;p&gt;So lately, I’ve tried a rather stupid search backend. More like this image:&lt;/p&gt;
    &lt;p&gt;In the furniture search examples I use in my Cheat at Search course, I tried this out.&lt;/p&gt;
    &lt;p&gt;Instead of a big, complex search system, I tell the agent about this dumb thing it can use to search. It then applies its own intelligence to use the tool correctly.&lt;/p&gt;
    &lt;code&gt;def search_products(query: str, top_k: int = 5) -&amp;gt; List[Dict]:
    """
    Search for furniture products.

    This is direct keyword search, no synonyms, only BM25 scoring on product name 
    and description and basic snowball tokenization of query and document.

    Args:
        query: The search query string.
        top_k: The number of top results to return.

    Returns:
        A list of dictionaries containing product information.
    """
   ...
&lt;/code&gt;
    &lt;p&gt;( the doc string here becomes part of the prompt, see the source code)&lt;/p&gt;
    &lt;p&gt;I slimmed the search down to fairly simple keyword search, and told GPT-5 exactly how it works. It behaves predictably, if stupidly.&lt;/p&gt;
    &lt;p&gt;And that’s a good thing.&lt;/p&gt;
    &lt;p&gt;I’m fairly surprised how well it works at solving user queries. Albeit slowly.&lt;/p&gt;
    &lt;p&gt;Here I am finding couches that would be good for a vampire.&lt;/p&gt;
    &lt;code&gt;User: A couch fit for a vampire

...

Assistant: Here are some dramatically tufted, velvet/chesterfield options with a suitably vampiric vibe:

Chesterfield-style sofas (timeless, tufted, dramatic arms/back)
- Porter 80" Velvet Rolled Arm Chesterfield Sofa (ID 4306)
- Rolled Arm Chesterfield Sofa (ID 28854)
- Quitaque 88" Rolled Arm Chesterfield Sofa (ID 17093)

Velvet sofas with luxe flair
- 85" Velvet Tuxedo Arm Sofa (ID 29613) — velvet with nailhead trim
- Camren 72" Velvet Round Arm Sofa (ID 40049)
- 112" Wide Velvet Sofa &amp;amp; Chaise (ID 27546)

Chaise lounges (fainting-couch energy)
- Rosemead Velvet Chaise Lounge (ID 40626)
- Avondale Velvet Chaise Lounge (ID 6452)
- Brighouse Chaise Lounge (ID 6392)
- Tilstone Chaise Lounge (ID 18643) — velvet with silver nailhead accents
&lt;/code&gt;
    &lt;p&gt;Or asking for the ugliest chair&lt;/p&gt;
    &lt;code&gt;User: ugliest chair in the catalog

Assistant: I ran several targeted searches to surface the most over-the-top
chairs. Here are the top contenders for “ugliest chair” in our catalog:

- Zebra Arm Chair (ID 18587) — bold zebra upholstery, statement piece energy
- Toby Cow Print Task Chair (ID 5002) — desk chair in full cow print
- Gaudy 47" Wide Chenille Armchair (ID 8921) — it literally says “Gaudy” in the name
- The Abbey Gothic Revival 19" Wide Side Chair (ID 36608) — ornate, carved gothic look
&lt;/code&gt;
    &lt;head rend="h2"&gt;Helping the agent remember as it goes&lt;/head&gt;
    &lt;p&gt;The agent can track how well a tool call works as it goes. In my code, the agent stores its evaluation of each tool’s results. Essentially “LLM-as-a-judge as you go”. The next time a search happens, the agent can recall what it liked / didn’t like last time.&lt;/p&gt;
    &lt;code&gt;system_prompt = """
    You take user search queries and use a search tool to find furniture products.
    
    Look at the search tools you have, their limitations, how they work, etc when forming your plan.

    Before searching you MUST use the "get_past_queries" to get similar, past queries
    the user has made

    Remember every tool usage you make. After searching with a tool, evaluate the results,
    then save the interaction (immediately after tool usage) with the "save_queries_used" tool
"""

&lt;/code&gt;
    &lt;p&gt;Below are search tool queries my code saved after I searched for the &lt;code&gt;ugliest chair in the catalog&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;You see the user’s query, along with the query used at the tool (ie &lt;code&gt;cow print chair&lt;/code&gt; ):&lt;/p&gt;
    &lt;code&gt;Saved interaction: user_query='ugliest chair in the catalog' 
search_tool_name='mcp_search-server.search_products' 
search_tool_query='cow print chair'
quality='good'
reasoning="Returned an adult 'cow print task chair' that clearly fits a loud/novelty aesthetic; strong candidate for 'ugliest chair' request."

Saved interaction: user_query='ugliest chair in the catalog'
search_tool_name='mcp_search-server.search_products' 
search_tool_query='patchwork accent chair'
quality='meh'
reasoning="Found a kids patchwork chair; adult options not surfaced. Only tangentially relevant to 'ugliest chair' intent."

Saved interaction: user_query='ugliest chair in the catalog' 
search_tool_name='mcp_search-server.search_products'
search_tool_query='skull chair'
quality='bad'
reasoning='No chair products matched; results were decor and textiles unrelated to chairs.'

&lt;/code&gt;
    &lt;p&gt;I’ll be able to recall what worked well for this query. I could even remember these expansions later if I build a normal site search without LLMs.&lt;/p&gt;
    &lt;p&gt;Semantic caching gives us another bonus. Later, when I search for a similar query, say &lt;code&gt;ugly chair&lt;/code&gt; , I can take &lt;code&gt;ugly chair&lt;/code&gt; and find past queries most similar to this query, like below.&lt;/p&gt;
    &lt;p&gt;The agent can see what’s worked well / hasn’t worked well for friends of &lt;code&gt;ugly chair&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;Getting past queries for: ugly chair

Matched query: 
  ugliest chair in the catalog, 
  similarity: 0.8236806392669678, 
  interaction: user_query='ugliest chair in the catalog' 
  search_tool_name='mcp_search-server.search_products'
  search_tool_query='novelty chair'
  quality='meh' reasoning="Found novelty chairs but primarily for kids; may not match adult seating requestfor 'ugliest chair'."
Matched query: ugliest chair in the catalog, 
  similarity: 0.8236806392669678, interaction: user_query='ugliest chair in the catalog' search_tool_name='mcp_search-server.search_products' search_tool_query='weird chair'
  quality='bad'
  reasoning="Keyword 'weird' mostly matched non-chair textiles and generic chair listings; not  helpful for surfacing intentionally odd or gaudy chairs."
Matched query: ugliest chair in the catalog, 
  similarity: 0.8236806392669678,
  interaction: user_query='ugliest chair in the catalog'
  search_tool_name='mcp_search-server.search_products'
  search_tool_query='zebra accent chair'
  quality='good'
  reasoning="Surfaced a 'zebra arm chair' (animal print) that fits an intentionally loud/gaudy aesthetic often perceived as ugly; results include a clear adult armchair option."
&lt;/code&gt;
    &lt;p&gt;As the agent searches, it’s little vector lookup of user queries turns into a knowledge graph. User queries connected to other user queries via similarity. And each with notes on what’s worked / not worked when calling the search tool.&lt;/p&gt;
    &lt;p&gt;It’s the ultimate learning machine.&lt;/p&gt;
    &lt;p&gt;At least as far as its own preferences.&lt;/p&gt;
    &lt;head rend="h2"&gt;Only as good as the agent’s ability to evaluate results&lt;/head&gt;
    &lt;p&gt;The LLM must be able to evaluate the results that come back. It needs to be a reasonable LLM judge. That means:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Did we return useful metadata for the agent to use in its evaluation? (Here we use product / description). But should we return images? Any other metadata?&lt;/item&gt;
      &lt;item&gt;Does the LLM actually have domain expertise to evaluate the results? (Note above it returns a brand “Gaudy”, which is not the adjective “gaudy” — as in ugly)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Finally the big one:&lt;/p&gt;
    &lt;head rend="h3"&gt;What about the user’s input?&lt;/head&gt;
    &lt;p&gt;The agent’s judgment is bound to be flawed / limited. Even in an ideal setting, it will always be disjoint from what users want. Lack of user clickstream feedback is a huge gap in RAG that continues here.&lt;/p&gt;
    &lt;p&gt;We can’t expect LLMs to have any idea of the crazy, insane things, humans do when nobody is watching. I’ve seen users do all sorts of things:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Users often click black / boring colors, not loud ones&lt;/item&gt;
      &lt;item&gt;Users click on spicy topics or images that have nothing to do with their query&lt;/item&gt;
      &lt;item&gt;Users get what they need from the search page, and never click anything&lt;/item&gt;
      &lt;item&gt;Users click because of the relationship between an image and its neighbor&lt;/item&gt;
      &lt;item&gt;Click data itself is sparse and noisy, and may require a lot in aggregate to make sense of&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;There are dozens of biases cataloged in user clickstream interactions with search results.&lt;/p&gt;
    &lt;p&gt;The search field has spent decades interpreting clickstream data, and the RAG world continues to lack good answers to getting that data back into the agent.&lt;/p&gt;
    &lt;p&gt;Maybe you’ve seen good research here? Please share!&lt;/p&gt;
    &lt;head rend="h2"&gt;Are reasoners actually actively harmful rankers?&lt;/head&gt;
    &lt;p&gt;Reasoning involves taking a base set of facts and deriving a solution. This, however, is different from directly optimizing for what users want (as expressed through clickstream data). We don’t have the technology for an LLM or agent to churn over millions of user sessions to become a better judge of search result quality.&lt;/p&gt;
    &lt;p&gt;Even if we gave the agent an oracle - labeled each result with user engagement data, could it make sense of latent patterns? Or would it simply be confused at the strange inconsistent behavior of humans that don’t act like math proofs or programming conpetitions?&lt;/p&gt;
    &lt;p&gt;Any ranking model optimized to engagement would more likely be implemented outside the agent: In a classic, thick search stack. The agent’s inner LLM judge, might look at model-ranked results and assume they’re bad. Why would users want such bland products? Why would the search have interpreted “Gaudy” to be a brand name, when its obviously “ugly”?&lt;/p&gt;
    &lt;p&gt;In short, users’ chaotic, implicit search expectations — as expressed in their clickstream — go against the agent’s orderly, explicit reasoning + judgment. An agent may issue a search and think “oh this result set was meh” when in reality, its the most engaging search results for that query.&lt;/p&gt;
    &lt;p&gt;Reasoning about search tool output might be very dangerous if the agent discards actually relevant results.&lt;/p&gt;
    &lt;p&gt;So take this article with this big grain of salt and be careful.&lt;/p&gt;
    &lt;p&gt;But still, it’s pretty fun.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45347363</guid><pubDate>Tue, 23 Sep 2025 14:17:23 +0000</pubDate></item><item><title>Getting AI to work in complex codebases</title><link>https://github.com/humanlayer/advanced-context-engineering-for-coding-agents/blob/main/ace-fca.md</link><description>&lt;doc fingerprint="3b74d0de090e4684"&gt;
  &lt;main&gt;
    &lt;p&gt;We read every piece of feedback, and take your input very seriously.&lt;/p&gt;
    &lt;p&gt;To see all available qualifiers, see our documentation.&lt;/p&gt;
    &lt;p&gt;There was an error while loading. Please reload this page.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45347532</guid><pubDate>Tue, 23 Sep 2025 14:27:36 +0000</pubDate></item><item><title>Show HN: Kekkai – a simple, fast file integrity monitoring tool in Go</title><link>https://github.com/catatsuy/kekkai</link><description>&lt;doc fingerprint="f54dd7534d159c2"&gt;
  &lt;main&gt;
    &lt;p&gt;A simple and fast Go tool for file integrity monitoring. Detects unauthorized file modifications caused by OS command injection and other attacks by recording file hashes during deployment and verifying them periodically.&lt;/p&gt;
    &lt;p&gt;The name "Kekkai" comes from the Japanese word 結界 (kekkai), meaning "barrier" - a protective boundary that keeps unwanted things out, perfectly representing this tool's purpose of protecting your files from tampering.&lt;/p&gt;
    &lt;p&gt;Kekkai was designed to solve specific challenges in production server environments:&lt;/p&gt;
    &lt;p&gt;Traditional tools like &lt;code&gt;tar&lt;/code&gt; or file sync utilities (e.g., &lt;code&gt;rsync&lt;/code&gt;) include metadata like timestamps in their comparisons, causing false positives when only timestamps change. In environments with heavy NFS usage or dynamic log directories, existing tools become difficult to configure and maintain.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Content-Only Hashing&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Hashes only file contents, ignoring timestamps and metadata&lt;/item&gt;
          &lt;item&gt;Detects actual content changes, not superficial modifications&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Immutable Exclude Rules&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Exclude patterns are set during manifest generation only&lt;/item&gt;
          &lt;item&gt;Cannot be modified during verification, preventing attackers from hiding changes&lt;/item&gt;
          &lt;item&gt;Only exclude server-generated files (logs, cache, uploads, NFS mounts)&lt;/item&gt;
          &lt;item&gt;Application dependencies (vendor, node_modules) are monitored as they're part of the deployment&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Symlink Security&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Uses &lt;code&gt;os.Lstat&lt;/code&gt;to properly detect symlinks without following them&lt;/item&gt;
          &lt;item&gt;Tracks symbolic links with their target paths (via &lt;code&gt;os.Readlink&lt;/code&gt;)&lt;/item&gt;
          &lt;item&gt;Hashes the symlink target path itself, not the target's content&lt;/item&gt;
          &lt;item&gt;Detects when symlinks are modified to point to different targets&lt;/item&gt;
          &lt;item&gt;Detects when regular files are replaced with symlinks (and vice versa)&lt;/item&gt;
          &lt;item&gt;Prevents attackers from hiding malicious changes through symlink manipulation&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;Uses &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Secure Hash Storage with S3&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Deploy servers have write-only access&lt;/item&gt;
          &lt;item&gt;Application servers have read-only access&lt;/item&gt;
          &lt;item&gt;Even if compromised, attackers cannot modify stored hashes&lt;/item&gt;
          &lt;item&gt;Local file output available for testing&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Tamper-Resistant Distribution&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Single Go binary with minimal dependencies&lt;/item&gt;
          &lt;item&gt;Recommended to run with restricted permissions&lt;/item&gt;
          &lt;item&gt;Configuration should be read from S3 or managed paths, not local files&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;🚀 Fast: Efficient hash calculation with parallel processing&lt;/item&gt;
      &lt;item&gt;🔒 Secure: Tamper-proof storage with S3 integration&lt;/item&gt;
      &lt;item&gt;📊 Monitoring Ready: Multiple output formats for various monitoring systems&lt;/item&gt;
      &lt;item&gt;🎯 Deterministic: Same file structure always produces the same hash&lt;/item&gt;
      &lt;item&gt;☁️ EC2 Ready: Authentication via IAM roles&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Build from source
git clone https://github.com/catatsuy/kekkai.git
cd kekkai
make

# Or directly with go build
go build -o ./bin/kekkai ./cmd/kekkai

# Run tests
make test&lt;/code&gt;
    &lt;code&gt;# Generate manifest
kekkai generate --target /var/www/app --output manifest.json

# Verify files
kekkai verify --manifest manifest.json --target /var/www/app&lt;/code&gt;
    &lt;code&gt;kekkai generate \
  --target /var/www/app \
  --exclude "*.log" \
  --exclude "cache/**" \
  --output manifest.json&lt;/code&gt;
    &lt;p&gt;Kekkai stores manifests in S3 for secure, centralized management. Each deployment updates the same &lt;code&gt;manifest.json&lt;/code&gt; file.&lt;/p&gt;
    &lt;code&gt;# For production deployment (must explicitly specify --base-path)
kekkai generate \
  --target /var/www/app \
  --s3-bucket my-manifests \
  --app-name myapp \
  --base-path production  # Explicitly required for production

# For staging/development (uses default "development" if not specified)
kekkai generate \
  --target /var/www/app \
  --s3-bucket my-manifests \
  --app-name myapp \
  --base-path staging

# During verification (must match the base-path used during generation)
kekkai verify \
  --s3-bucket my-manifests \
  --app-name myapp \
  --base-path production \
  --target /var/www/app&lt;/code&gt;
    &lt;p&gt;Benefits:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Lower S3 costs - Minimal S3 operations&lt;/item&gt;
      &lt;item&gt;Clean structure - One manifest file per application&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Add to crontab for periodic checks
*/5 * * * * kekkai verify \
  --s3-bucket my-manifests \
  --app-name myapp \
  --base-path production \
  --target /var/www/app

# Use cache for faster verification (cache in temp directory)
*/5 * * * * kekkai verify \
  --s3-bucket my-manifests \
  --app-name myapp \
  --base-path production \
  --target /var/www/app \
  --use-cache \
  --verify-probability 0.1

# Use persistent cache in custom directory
*/5 * * * * kekkai verify \
  --s3-bucket my-manifests \
  --app-name myapp \
  --base-path production \
  --target /var/www/app \
  --use-cache \
  --cache-dir /var/cache/kekkai \
  --verify-probability 0.1&lt;/code&gt;
    &lt;p&gt;Configure your monitoring system to alert based on your requirements (e.g., alert after consecutive failures).&lt;/p&gt;
    &lt;p&gt;These examples show common exclude patterns for various frameworks. Important: Only exclude files generated on the server (logs, cache, uploads). Application dependencies like &lt;code&gt;vendor&lt;/code&gt; or &lt;code&gt;node_modules&lt;/code&gt; MUST be monitored as they are part of the deployed application.&lt;/p&gt;
    &lt;p&gt;For production use, replace &lt;code&gt;--output manifest.json&lt;/code&gt; with S3 storage options (&lt;code&gt;--s3-bucket&lt;/code&gt;, &lt;code&gt;--app-name&lt;/code&gt;, &lt;code&gt;--base-path&lt;/code&gt;).&lt;/p&gt;
    &lt;code&gt;kekkai generate \
  --target /var/www/app \
  --exclude "storage/**" \
  --exclude "bootstrap/cache/**" \
  --exclude "*.log" \
  --output manifest.json&lt;/code&gt;
    &lt;code&gt;kekkai generate \
  --target /var/www/app \
  --exclude "*.log" \
  --exclude ".npm/**" \
  --exclude "tmp/**" \
  --output manifest.json&lt;/code&gt;
    &lt;code&gt;kekkai generate \
  --target /var/www/app \
  --exclude "log/**" \
  --exclude "tmp/**" \
  --exclude "public/assets/**" \
  --output manifest.json&lt;/code&gt;
    &lt;code&gt;kekkai generate \
  --target /var/www/app \
  --exclude "**/__pycache__/**" \
  --exclude "media/**" \
  --exclude "staticfiles/**" \
  --exclude "*.pyc" \
  --output manifest.json&lt;/code&gt;
    &lt;p&gt;For deployment server (write access):&lt;/p&gt;
    &lt;code&gt;{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:PutObject"
      ],
      "Resource": "arn:aws:s3:::my-manifests/*"
    }
  ]
}&lt;/code&gt;
    &lt;p&gt;For production server (read-only):&lt;/p&gt;
    &lt;code&gt;{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:GetObject"
      ],
      "Resource": "arn:aws:s3:::my-manifests/*"
    }
  ]
}&lt;/code&gt;
    &lt;p&gt;Recommended: Enable S3 versioning to maintain history of manifest changes.&lt;/p&gt;
    &lt;code&gt;# Optional: Enable versioning for history tracking
aws s3api put-bucket-versioning \
  --bucket my-manifests \
  --versioning-configuration Status=Enabled

# Enable encryption
aws s3api put-bucket-encryption \
  --bucket my-manifests \
  --server-side-encryption-configuration '{
    "Rules": [{
      "ApplyServerSideEncryptionByDefault": {
        "SSEAlgorithm": "AES256"
      }
    }]
  }'

# Optional: Set lifecycle policy to delete old versions after N days
aws s3api put-bucket-lifecycle-configuration \
  --bucket my-manifests \
  --lifecycle-configuration '{
    "Rules": [{
      "Id": "DeleteOldVersions",
      "Status": "Enabled",
      "NoncurrentVersionExpiration": {
        "NoncurrentDays": 30
      }
    }]
  }'&lt;/code&gt;
    &lt;code&gt;#!/bin/bash
# deploy.sh

set -e

APP_NAME="myapp"
DEPLOY_DIR="/var/www/app"
S3_BUCKET="my-manifests"

# 1. Install dependencies locally
cd ./src
composer install --no-dev

# 2. Deploy application to server
rsync -av ./src/ ${DEPLOY_DIR}/

# 3. Generate manifest and save to S3
# Note: For production, explicitly specify --base-path production
kekkai generate \
  --target ${DEPLOY_DIR} \
  --exclude "storage/**" \
  --exclude "bootstrap/cache/**" \
  --s3-bucket ${S3_BUCKET} \
  --app-name ${APP_NAME} \
  --base-path production  # MUST be explicit for production

echo "Deploy completed with integrity manifest"
echo "Manifest saved to: ${S3_BUCKET}/production/${APP_NAME}/manifest.json"&lt;/code&gt;
    &lt;p&gt;Generate a manifest file.&lt;/p&gt;
    &lt;code&gt;Options:
  -target string      Target directory (default ".")
  -output string      Output file, "-" for stdout (default "-")
  -exclude string     Exclude pattern (can be specified multiple times)
  -s3-bucket string   S3 bucket name
  -s3-region string   AWS region
  -base-path string   S3 base path (default "development")
  -app-name string    Application name (creates path: {base-path}/{app-name}/manifest.json)
  -format string      Output format: text, json (default "text")
  -workers int        Number of worker threads (0 = auto detect)
  -rate-limit int     Rate limit in bytes per second (0 = no limit)
  -timeout int        Timeout in seconds (default: 300)
&lt;/code&gt;
    &lt;p&gt;Verify file integrity.&lt;/p&gt;
    &lt;code&gt;Options:
  -manifest string    Manifest file path
  -s3-bucket string   S3 bucket name
  -s3-region string   AWS region
  -base-path string   S3 base path (default "development")
  -app-name string    Application name (reads from: {base-path}/{app-name}/manifest.json)
  -target string      Target directory to verify (default ".")
  -format string      Output format: text, json (default "text")
  -workers int              Number of worker threads (0 = auto detect)
  -rate-limit int           Rate limit in bytes per second (0 = no limit)
  -timeout int              Timeout in seconds (default: 300)
  -use-cache                Enable local cache for verification (checks size, mtime, ctime)
  -cache-dir string         Directory for cache file (default: system temp directory)
  -verify-probability float Probability of hash verification with cache hit (0.0-1.0, default: 0.1)
&lt;/code&gt;
    &lt;code&gt;✓ Integrity check passed
  Verified 1523 files
&lt;/code&gt;
    &lt;code&gt;{
  "success": true,
  "timestamp": "2024-01-01T00:00:00Z",
  "message": "All files verified successfully",
  "details": {
    "total_files": 1523,
    "verified_files": 1523
  }
}&lt;/code&gt;
    &lt;p&gt;Kekkai uses glob patterns for the &lt;code&gt;--exclude&lt;/code&gt; option to skip specific files and directories during manifest generation.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Pattern&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
        &lt;cell role="head"&gt;Example&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;*.ext&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Match files with specific extension&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;*.log&lt;/code&gt; matches &lt;code&gt;app.log&lt;/code&gt;, &lt;code&gt;error.log&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;dir/*&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Match all files in a directory&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;logs/*&lt;/code&gt; matches &lt;code&gt;logs/app.log&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;dir/**&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Match all files recursively&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;cache/**&lt;/code&gt; matches &lt;code&gt;cache/data.db&lt;/code&gt;, &lt;code&gt;cache/sessions/abc.txt&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;**/*.ext&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Match extension at any depth&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;**/*.pyc&lt;/code&gt; matches &lt;code&gt;app.pyc&lt;/code&gt;, &lt;code&gt;lib/utils.pyc&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;**/dir/*&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Match directory at any depth&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;**/logs/*&lt;/code&gt; matches &lt;code&gt;logs/app.log&lt;/code&gt;, &lt;code&gt;app/logs/error.log&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;path/to/file&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Exact path match&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;config/local.ini&lt;/code&gt; matches only that file&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Relative Paths: All patterns match against relative paths from the target directory&lt;/item&gt;
      &lt;item&gt;Forward Slashes: Always use &lt;code&gt;/&lt;/code&gt;as path separator (even on Windows)&lt;/item&gt;
      &lt;item&gt;No Negation: Patterns cannot be negated (no &lt;code&gt;!pattern&lt;/code&gt;support)&lt;/item&gt;
      &lt;item&gt;Order Independent: All patterns are evaluated, order doesn't matter&lt;/item&gt;
      &lt;item&gt;Immutable: Exclude patterns cannot be changed during verification&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Laravel/Symfony
--exclude "storage/**"          # User uploads
--exclude "var/cache/**"        # Framework cache
--exclude "var/log/**"          # Application logs
--exclude "public/uploads/**"   # Uploaded files

# Python/Django
--exclude "**/__pycache__/**"   # Python cache
--exclude "**/*.pyc"            # Compiled Python
--exclude "media/**"            # User uploads
--exclude "staticfiles/**"      # Collected static files

# Node.js
--exclude "*.log"               # Log files
--exclude "tmp/**"              # Temporary files
--exclude ".npm/**"             # NPM cache

# General
--exclude "*.tmp"               # Temporary files
--exclude "*.bak"               # Backup files
--exclude ".git/**"             # Git repository (if needed)&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;❌ &lt;code&gt;--exclude "vendor/**"&lt;/code&gt;(PHP dependencies)&lt;/item&gt;
      &lt;item&gt;❌ &lt;code&gt;--exclude "node_modules/**"&lt;/code&gt;(Node.js dependencies)&lt;/item&gt;
      &lt;item&gt;❌ &lt;code&gt;--exclude "venv/**"&lt;/code&gt;(Python virtual environment)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These are part of your deployed application and must be monitored for tampering.&lt;/p&gt;
    &lt;p&gt;✅ Only exclude server-generated content:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Log files&lt;/item&gt;
      &lt;item&gt;Cache directories&lt;/item&gt;
      &lt;item&gt;User uploads&lt;/item&gt;
      &lt;item&gt;Temporary files&lt;/item&gt;
      &lt;item&gt;NFS mounts&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Patterns are evaluated in this order:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Check for &lt;code&gt;**&lt;/code&gt;recursive matching&lt;/item&gt;
      &lt;item&gt;Special case: &lt;code&gt;**/*&lt;/code&gt;or&lt;code&gt;**&lt;/code&gt;matches everything&lt;/item&gt;
      &lt;item&gt;Suffix pattern: &lt;code&gt;dir/**&lt;/code&gt;matches everything under&lt;code&gt;dir/&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Prefix pattern: &lt;code&gt;**/*.ext&lt;/code&gt;matches files with extension at any depth&lt;/item&gt;
      &lt;item&gt;Simple glob: Standard shell glob matching with &lt;code&gt;*&lt;/code&gt;and&lt;code&gt;?&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Kekkai has comprehensive symlink security to prevent attackers from hiding malicious changes:&lt;/p&gt;
    &lt;p&gt;Kekkai handles symlinks differently depending on where they appear:&lt;/p&gt;
    &lt;p&gt;If &lt;code&gt;--target&lt;/code&gt; points to a symlink (e.g., &lt;code&gt;/current&lt;/code&gt; → &lt;code&gt;/releases/20240101&lt;/code&gt;):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Automatically resolved: Uses &lt;code&gt;filepath.EvalSymlinks&lt;/code&gt;to follow the symlink&lt;/item&gt;
      &lt;item&gt;Operates on real path: All operations happen in the resolved directory&lt;/item&gt;
      &lt;item&gt;Transparent to user: Works exactly as if you specified the real directory&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;# These produce identical results:
kekkai generate --target /var/www/current        # Symlink to /var/www/releases/20240101
kekkai generate --target /var/www/releases/20240101  # Direct path&lt;/code&gt;
    &lt;p&gt;For symlinks found within the target directory:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Not followed: Uses &lt;code&gt;os.Lstat&lt;/code&gt;to detect them without following&lt;/item&gt;
      &lt;item&gt;Tracked as symlinks: Stored with &lt;code&gt;IsSymlink: true&lt;/code&gt;flag&lt;/item&gt;
      &lt;item&gt;Target recorded: Link target path saved for verification&lt;/item&gt;
      &lt;item&gt;Hash of target path: Creates hash from &lt;code&gt;"symlink:" + target_path&lt;/code&gt;string&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Detection: Uses &lt;code&gt;os.Lstat&lt;/code&gt;to identify symlinks without following them&lt;/item&gt;
      &lt;item&gt;Target Tracking: Reads the link target with &lt;code&gt;os.Readlink&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Hash Calculation: Creates hash from &lt;code&gt;"symlink:" + target_path&lt;/code&gt;string&lt;/item&gt;
      &lt;item&gt;Verification: Checks both link type and target path during verification&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;✅ Symlink target changes (e.g., &lt;code&gt;/usr/bin/php&lt;/code&gt;→&lt;code&gt;/tmp/malicious&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;✅ File type changes (regular file → symlink or symlink → regular file)&lt;/item&gt;
      &lt;item&gt;✅ Broken symlinks (target doesn't exist)&lt;/item&gt;
      &lt;item&gt;✅ New symlinks added to the directory&lt;/item&gt;
      &lt;item&gt;✅ Deleted symlinks&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Original deployment
/app/config.php -&amp;gt; /etc/app/config.php  # Hash: abc123...

# These changes will be detected:
/app/config.php -&amp;gt; /tmp/fake-config.php  # Modified: different target
/app/config.php (regular file)           # Modified: type change
/app/config.php (deleted)                # Deleted: symlink removed&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Symlinks are never followed during hash calculation&lt;/item&gt;
      &lt;item&gt;Only the link itself is hashed, not the target's content&lt;/item&gt;
      &lt;item&gt;Prevents directory traversal attacks via symlink manipulation&lt;/item&gt;
      &lt;item&gt;Cache system skips symlinks (only caches regular files)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A: Kekkai only hashes file contents, so timestamp or permission changes don't affect hashes. Check for line ending differences (CRLF/LF).&lt;/p&gt;
    &lt;p&gt;A: Verify that the EC2 instance has the correct IAM role configured. Also check that the region is specified correctly.&lt;/p&gt;
    &lt;p&gt;A: For large file sets, use &lt;code&gt;--exclude&lt;/code&gt; options to skip server-generated directories like logs, cache, and temporary files. You can also optimize performance with:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;--use-cache&lt;/code&gt;: Enable local cache that checks file metadata (size, mtime, ctime) to skip hash calculation&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--verify-probability N&lt;/code&gt;: Set probability of hash verification even with cache hit (0.0-1.0, default: 0.1)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--workers N&lt;/code&gt;: Adjust the number of worker threads for your system&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--rate-limit N&lt;/code&gt;: Limit I/O throughput (bytes per second) to reduce system load&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Cache Mode: When using &lt;code&gt;--use-cache&lt;/code&gt;, kekkai maintains a local cache file (&lt;code&gt;.kekkai-cache-{base-name}-{app-name}.json&lt;/code&gt;) in the cache directory (defaults to system temp directory, or specify with &lt;code&gt;--cache-dir&lt;/code&gt;). Cache files are temporary by nature and will be recreated if missing. It checks file metadata including:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;File size&lt;/item&gt;
      &lt;item&gt;Modification time (mtime)&lt;/item&gt;
      &lt;item&gt;Change time (ctime) - cannot be easily forged&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If all metadata matches, it uses the cached hash. The &lt;code&gt;--verify-probability&lt;/code&gt; option adds probabilistic verification:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;0.0&lt;/code&gt;: Always trust cache (fastest, least secure)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;0.1&lt;/code&gt;: 10% chance to verify hash even with cache hit (default, good balance)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;1.0&lt;/code&gt;: Always verify hash (most secure, no performance benefit)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The cache file itself is protected with a hash to detect tampering.&lt;/p&gt;
    &lt;p&gt;💡 Cache Behavior: By default, cache files are stored in the system temp directory (e.g., &lt;code&gt;/tmp&lt;/code&gt; on Linux/macOS) and may be automatically cleaned by the system. This is intentional - the cache is designed to be ephemeral and will be recreated as needed for performance optimization.&lt;/p&gt;
    &lt;p&gt;Note that application dependencies (vendor, node_modules) should still be verified as they are part of the deployed application.&lt;/p&gt;
    &lt;p&gt;A: Use &lt;code&gt;--rate-limit&lt;/code&gt; to throttle I/O bandwidth. For example, &lt;code&gt;--rate-limit 10485760&lt;/code&gt; limits to 10MB/s. This global rate limit is shared across all worker threads, preventing system overload while still allowing parallel processing.&lt;/p&gt;
    &lt;p&gt;Alternatively, you can use systemd to control resource usage at the OS level:&lt;/p&gt;
    &lt;code&gt;# Run with limited CPU and I/O priority (with cache support)
systemd-run --quiet --wait --pipe --collect \
  -p Type=oneshot \
  -p CPUQuota=25% -p CPUWeight=50 \
  -p PrivateTmp=no \
  -p User=nobody \
  /bin/bash -lc 'nice -n 10 ionice -c2 -n7 /usr/local/bin/kekkai verify \
    --s3-bucket my-manifests \
    --app-name myapp \
    --target /srv/app \
    --use-cache \
    --verify-probability 0.1 \
    --rate-limit 10485760'&lt;/code&gt;
    &lt;p&gt;This approach provides more comprehensive resource control:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;CPUQuota=25%&lt;/code&gt;: Limits CPU usage to 25%&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;CPUWeight=50&lt;/code&gt;: Sets CPU scheduling weight (lower priority)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;PrivateTmp=no&lt;/code&gt;: Allows cache persistence in&lt;code&gt;/tmp&lt;/code&gt;across runs&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;User=nobody&lt;/code&gt;: Runs with minimal privileges for security&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;nice -n 10&lt;/code&gt;: Lower process priority&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;ionice -c2 -n7&lt;/code&gt;: Best-effort I/O scheduling with lowest priority&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--use-cache&lt;/code&gt;: Enables cache for faster verification&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--verify-probability 0.1&lt;/code&gt;: 10% chance to verify hash even with cache hit&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--rate-limit 10485760&lt;/code&gt;: Limits I/O to 10MB/s&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Important: The &lt;code&gt;PrivateTmp=no&lt;/code&gt; setting is required when using &lt;code&gt;--use-cache&lt;/code&gt; to ensure cache files persist between systemd-run executions. Without this, systemd creates an isolated &lt;code&gt;/tmp&lt;/code&gt; directory for each run, preventing cache reuse. If you prefer stronger isolation, use &lt;code&gt;--cache-dir&lt;/code&gt; to specify a persistent directory outside of &lt;code&gt;/tmp&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;# Alternative: Keep PrivateTmp=yes but use a custom cache directory
systemd-run --quiet --wait --pipe --collect \
  -p Type=oneshot \
  -p CPUQuota=25% -p CPUWeight=50 \
  -p PrivateTmp=yes \
  -p User=nobody \
  /bin/bash -lc 'nice -n 10 ionice -c2 -n7 /usr/local/bin/kekkai verify \
    --s3-bucket my-manifests \
    --app-name myapp \
    --target /srv/app \
    --use-cache \
    --cache-dir /var/cache/kekkai \
    --verify-probability 0.1 \
    --rate-limit 10485760'&lt;/code&gt;
    &lt;p&gt;Security Recommendation: Always run kekkai verification with minimal privileges using &lt;code&gt;-p User=nobody&lt;/code&gt; or a dedicated non-privileged user. This follows the principle of least privilege and reduces the security impact if the process is compromised.&lt;/p&gt;
    &lt;p&gt;Note: With Go 1.25+, &lt;code&gt;CPUQuota&lt;/code&gt; also automatically adjusts &lt;code&gt;GOMAXPROCS&lt;/code&gt; to match the quota, so kekkai will use fewer worker threads when CPU is limited, providing better resource utilization.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;--use-cache&lt;/code&gt; on NFS&lt;/p&gt;
    &lt;p&gt;Cache mode is disabled by default. Do not enable it on NFS-mounted directories as it does not work well and may run slower than without cache.&lt;/p&gt;
    &lt;p&gt;Kekkai's cache system uses ctime (change time) to detect file changes. Ctime is hard to fake and important for security.&lt;/p&gt;
    &lt;p&gt;However, NFS clients may not update ctime properly. This causes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Very low cache hit rate&lt;/item&gt;
      &lt;item&gt;Cache becomes useless&lt;/item&gt;
      &lt;item&gt;Runs as slow as normal verification&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Disable cache on NFS
kekkai verify \
  --s3-bucket my-manifests \
  --app-name myapp \
  --target /nfs/app \
  --workers 2 \
  --rate-limit 5242880&lt;/code&gt;
    &lt;p&gt;Disabling cache does not reduce security. All files are still verified with SHA256 hashes.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45347714</guid><pubDate>Tue, 23 Sep 2025 14:39:59 +0000</pubDate></item><item><title>Launch HN: Strata (YC X25) – One MCP server for AI to handle thousands of tools</title><link>https://news.ycombinator.com/item?id=45347914</link><description>&lt;doc fingerprint="3d5a22c222f374b3"&gt;
  &lt;main&gt;
    &lt;div&gt;&lt;p&gt;Hey HN! We are Klavis AI (&lt;/p&gt;https://www.klavis.ai/&lt;p&gt;) and we're launching Strata, one open-source MCP server that helps AI agents use thousands of API tools without getting overwhelmed. Instead of showing all available tools at once, Strata reveals them step-by-step based on what the AI actually needs.&lt;/p&gt;&lt;p&gt;As a former Senior SWE on Google Gemini 's tool use team, I saw firsthand how AI would struggle with tools. If you've built AI agents, you've likely hit the same walls: (1) AI agents struggle to pick the right API from hundreds of options. (2) Tool descriptions and info consume massive token budgets. (3) Most servers cap at 40~50 tools to avoid these problems, limiting what you can build.&lt;/p&gt;&lt;p&gt;Instead of flooding the AI with everything upfront, Strata works like a human would. It guides the AI agents to discover relevant categories, then lists available actions in those categories. It relies on LLMs’ reasoning to drill down progressively to find the exact tool needed. Here are some examples:&lt;/p&gt;&lt;p&gt;Github query: "Find my stale pull requests in our main repo"&lt;/p&gt;&lt;p&gt;Strata: AI model identifies GitHub → Shows categories (Repos, Issues, PRs, Actions) → AI selects PRs → Shows PR-specific actions -&amp;gt; AI selects list_pull_requests → Shows list_pull_requests details -&amp;gt; Executes list_pull_requests with the right parameters.&lt;/p&gt;&lt;p&gt;Jira query: "Create a bug ticket in the 'MOBILE' project about the app crashing on startup."&lt;/p&gt;&lt;p&gt;Strata: AI identifies Jira → Shows categories (Projects, Issues, Sprints) → AI selects Issues → Shows actions (create_issue, get_issue) → AI selects create_issue → Shows create_issue details → Executes with correct parameters.&lt;/p&gt;&lt;p&gt;Slack query: "Post a message in the #announcements channel that bonus will be paid out next Friday."&lt;/p&gt;&lt;p&gt;Strata: AI identifies Slack → Shows categories (Channels, Messages, Users) → AI selects Messages → Shows actions (send_message, schedule_message) → AI selects send_message → Shows send_message details → Executes with correct parameters.&lt;/p&gt;&lt;p&gt;This progressive approach unlocks a huge advantage: depth. While most integrations offer a handful of high-level tools, Strata can expose hundreds of granular features for a single app like GitHub, Jira, etc. Your AI agent can finally access the deep, specific features that real workflows require, without getting lost in a sea of options.&lt;/p&gt;&lt;p&gt;Under the hood, Strata manages authentication tokens and includes a built-in search tool for the agent to dig into documentation if it gets stuck.&lt;/p&gt;&lt;p&gt;On the MCPMark https://mcpmark.ai/leaderboard/mcp, Strata achieves +15.2% higher pass@1 rate vs the official GitHub server and +13.4% higher pass@1 rate vs the official Notion server. In human eval tests, it hits 83%+ accuracy on complex, real-world multi-app workflows.&lt;/p&gt;&lt;p&gt;Here is a quick demo to watch Strata navigate a complex workflow with multiple apps, automatically selecting the right tools at each step: https://www.youtube.com/watch?v=N00cY9Ov_fM.&lt;/p&gt;&lt;p&gt;You can connect to any external MCP Server into Strata, and we have an open source version for it: https://github.com/Klavis-AI/klavis.&lt;/p&gt;&lt;p&gt;For team or production use with more features, visit our website: https://www.klavis.ai. Add Strata to Cursor, VS Code or any MCP-compatible application with one click. You can also use our API to easily plug in Strata to your AI application.&lt;/p&gt;&lt;p&gt;We look forward to your comments. Thanks for reading!&lt;/p&gt;&lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45347914</guid><pubDate>Tue, 23 Sep 2025 14:52:07 +0000</pubDate></item><item><title>From MCP to shell: MCP auth flaws enable RCE in Claude Code, Gemini CLI and more</title><link>https://verialabs.com/blog/from-mcp-to-shell/</link><description>&lt;doc fingerprint="b5638dd89ba7ea90"&gt;
  &lt;main&gt;
    &lt;p&gt;During our security testing, we discovered that connecting to a malicious MCP server via common coding tools like Claude Code and Gemini CLI could give attackers instant control over user computers.&lt;/p&gt;
    &lt;p&gt;As a preview, hereâs a video of us opening the calculator (âpopping calcâ) on someoneâs computer through Claude Code:&lt;/p&gt;
    &lt;p&gt;âPopping calcâ is a harmless way of showcasing remote code execution. The exploits we found can be extended for malicious purposes beyond that, such as invisibly installing a reverse shell or malware.&lt;/p&gt;
    &lt;head rend="h2"&gt;TL;DR&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Earlier this year, MCP introduced an OAuth standard to authenticate clients&lt;/item&gt;
      &lt;item&gt;Many MCP clients did not validate the authorization URL passed by a malicious MCP server&lt;/item&gt;
      &lt;item&gt;We were able to exploit this bug to achieve Remote Code Execution (RCE) in popular tools&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;About Us&lt;/head&gt;
    &lt;p&gt;At Veria Labs, we build AI agents that secure high-stakes industries so you can ship quickly and confidently. Founded by members of the #1 competitive hacking team in the U.S., weâve already found critical bugs in AI tools, operating systems, and billion-dollar crypto exchanges.&lt;/p&gt;
    &lt;p&gt;Think we can help secure your systems? Weâd love to chat! Book a call here.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Attack Surface&lt;/head&gt;
    &lt;p&gt;MCP (Model Context Protocol) allows an AI to connect with external tools, APIs, and data sources. It extends an LLM applicationâs base capabilities by sharing context and performing actions, such as giving Gemini access to Google Drive.&lt;/p&gt;
    &lt;p&gt;In March, Anthropic released the first revision to their MCP specification, introducing an authorization framework using OAuth. OAuth is the standard that powers âLogin with Googleâ and other similar authentication methods. Adding OAuth to MCP is a great change for the AI ecosystem, giving a standardized way for MCP servers and clients to authenticate.&lt;/p&gt;
    &lt;p&gt;However, the way MCP clients implemented OAuth creates a new and subtle attack surface. In this blog post, we exploit this attack surface to varying degrees of success across different applications, including Cloudflareâs &lt;code&gt;use-mcp&lt;/code&gt; client library, Anthropicâs MCP Inspector, Claude Code, Gemini CLI, and (almost) ChatGPT itself.&lt;/p&gt;
    &lt;p&gt;The core issue is simple: MCP servers control where clients redirect users for authentication, and most clients trusted this URL completely.&lt;/p&gt;
    &lt;head rend="h2"&gt;Exploiting Cloudflareâs &lt;code&gt;use-mcp&lt;/code&gt; library XSS&lt;/head&gt;
    &lt;p&gt;We initially discovered this vulnerability pattern in June, when Cloudflare released their &lt;code&gt;use-mcp&lt;/code&gt; library. As of the time of writing, the library has over 36,000 weekly downloads on npm.&lt;/p&gt;
    &lt;p&gt;The bug occurs in the OAuth flow where the server tells the client where to open a browser window to authenticate. The bug occurs at &lt;code&gt;src/auth/browser-provider.ts&lt;/code&gt;. In code:&lt;/p&gt;
    &lt;p&gt;If youâre familiar with web exploitation, you may be able to see where this is going.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;use-mcp&lt;/code&gt; client performs &lt;code&gt;window.open()&lt;/code&gt; on &lt;code&gt;authUrlString&lt;/code&gt;, which is an arbitrary string supplied by the MCP server directly to the client. This creates an XSS vulnerability, as you can supply a &lt;code&gt;javascript:&lt;/code&gt; URL in &lt;code&gt;authUrlString&lt;/code&gt;. When supplied to &lt;code&gt;window.open&lt;/code&gt;, a &lt;code&gt;javascript:&lt;/code&gt; URL executes everything supplied as JavaScript code on the currently loaded page.&lt;/p&gt;
    &lt;p&gt;Impact: A user connecting to an MCP application with the &lt;code&gt;use-mcp&lt;/code&gt; library is vulnerable to the server delivering arbitrary JavaScript, which the client will automatically execute on the userâs browser. This can potentially lead to hijacking the user session and the takeover of the user account for that website.&lt;/p&gt;
    &lt;head rend="h3"&gt;Writing our &lt;code&gt;use-mcp&lt;/code&gt; exploit&lt;/head&gt;
    &lt;p&gt;We used the following Cloudflare Workers example code at cloudflare/remote-mcp-github-oauth for our exploit Proof of Concept (PoC). This made the setup process easy, and the PoC only required us to modify a few lines of code.&lt;/p&gt;
    &lt;p&gt;Specifically, our malicious &lt;code&gt;authUrlString&lt;/code&gt; payload is the following:&lt;/p&gt;
    &lt;p&gt;We were able to demonstrate our PoC on Cloudflareâs Workers AI LLM Playground:&lt;/p&gt;
    &lt;p&gt;The newly opened window counts as same-origin, allowing us to hijack the original web page via &lt;code&gt;window.opener&lt;/code&gt;. This gives us a reference to the parent windowâs JavaScript context.&lt;/p&gt;
    &lt;p&gt;Since we can force arbitrary client-side JavaScript execution, any user connecting to an MCP server via the &lt;code&gt;use-mcp&lt;/code&gt; library could have been vulnerable to exploits such as session hijacking and account takeover.&lt;/p&gt;
    &lt;head rend="h2"&gt;Escalating to RCE with MCP Inspector&lt;/head&gt;
    &lt;p&gt;While working on our exploit, we used Anthropicâs MCP Inspector to debug our malicious MCP server. While playing around with MCP Inspector, we found out it too is vulnerable to the same exploit as Cloudflareâs &lt;code&gt;use-mcp&lt;/code&gt; library!&lt;/p&gt;
    &lt;head rend="h3"&gt;XSS -&amp;gt; RCE: Abusing MCPâs &lt;code&gt;stdio&lt;/code&gt; Transport&lt;/head&gt;
    &lt;p&gt;We have XSS now, but that doesnât allow us to do all that much. However, since the application runs locally on a userâs machine, we were interested in seeing if we could do more. Turns out, we can request a connection using MCP Inspectorâs &lt;code&gt;stdio&lt;/code&gt; transport to escalate this XSS into Remote Code Execution (RCE) on the userâs system.&lt;/p&gt;
    &lt;head rend="h4"&gt;What is the MCP &lt;code&gt;stdio&lt;/code&gt; transport?&lt;/head&gt;
    &lt;p&gt;In the context of MCP Inspector, the browser UI canât speak directly to a local process, so the Inspector Proxy (a small Node.js service running on your machine) sits in the middle. When the UI asks to connect to a server via &lt;code&gt;stdio&lt;/code&gt;, the proxy spawns the requested command as a child process and bridges messages between the browser and that process. Functionally, itâs:&lt;/p&gt;
    &lt;p&gt;That bridging role turns an XSS in the Inspector UI into RCE: if attackerâcontrolled JavaScript can run in the Browser UI and obtain the proxyâs authentication token, it can tell the proxy to spawn any local command, effectively escalating XSS to arbitrary code execution on the host.&lt;/p&gt;
    &lt;head rend="h4"&gt;Completing the exploit chain&lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;stdio&lt;/code&gt; transport is normally secured against other local processes with an authentication token that only the MCP Inspector client knows. However, since we have XSS, we can steal this token from the query parameter &lt;code&gt;MCP_PROXY_AUTH_TOKEN&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;This gives us complete remote code execution on the userâs system with the privileges of the MCP Inspector process. Note that while this specific exploit is written for Windows, Linux and Mac systems are vulnerable too.&lt;/p&gt;
    &lt;head rend="h2"&gt;Exploiting Claude Code and Gemini CLI to take over your PC&lt;/head&gt;
    &lt;p&gt;We also decided to check whether our favorite command line agentic code editors might be vulnerable, as they are some of the most popular programs with MCP implementations.&lt;/p&gt;
    &lt;head rend="h3"&gt;Popping calc in Claude Code&lt;/head&gt;
    &lt;p&gt;Claude Code is not open source, but its npm package includes a minified bundle. We were able to browse different versions on socket.dev to grab &lt;code&gt;cli.js&lt;/code&gt;, which contains the entire Claude Code CLI in a single file.&lt;/p&gt;
    &lt;p&gt;The relevant code (modified for clarity) was:&lt;/p&gt;
    &lt;p&gt;While it performs URL schema validationâmaking it seem safe at first glanceâthe Windows specific code is still vulnerable to command injection. It spawns the browser with &lt;code&gt;cmd.exe /c start &amp;lt;authUrl&amp;gt;&lt;/code&gt;, but we could append &lt;code&gt;&amp;amp;calc.exe&lt;/code&gt;, causing cmd.exe to launch an additional program: &lt;code&gt;cmd.exe /c start &amp;lt;authUrl&amp;gt;&amp;amp;calc.exe&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;As such, this is our payload:&lt;/p&gt;
    &lt;p&gt;Claude Code version 1.0.54 rewrote this to spawn PowerShell instead of cmd.exe.&lt;/p&gt;
    &lt;p&gt;We adapted our exploit to use PowerShellâs string interpolation features. Double-quoted PowerShell strings allow expressions to be evaluated when constructing the string, similar to JavaScript template literals:&lt;/p&gt;
    &lt;p&gt;This payload encodes &lt;code&gt;calc.exe&lt;/code&gt; as base64, then uses PowerShellâs expression evaluation to decode and execute it during string construction.&lt;/p&gt;
    &lt;head rend="h3"&gt;Gemini CLI is also exploitable :)&lt;/head&gt;
    &lt;p&gt;Gemini CLI was exploitable in the exact same way. It passes the OAuth URL to the popular &lt;code&gt;open&lt;/code&gt; npm package.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;open&lt;/code&gt; packageâs README includes this warning:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;This package does not make any security guarantees. If you pass in untrusted input, itâs up to you to properly sanitize it.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;It turns out that the warning in the &lt;code&gt;open&lt;/code&gt; README is there for a good reason. Looking at the source of &lt;code&gt;open&lt;/code&gt;, we can see the URL opening logic is also implemented through PowerShell, with the same use of templating that made Claude Code vulnerable to command injection.&lt;/p&gt;
    &lt;p&gt;This means the exact same payload we used for Claude Code also works for Gemini CLI!&lt;/p&gt;
    &lt;head rend="h2"&gt;Defenses that prevented exploitation&lt;/head&gt;
    &lt;head rend="h3"&gt;Almost XSSing ChatGPT&lt;/head&gt;
    &lt;p&gt;Recently, OpenAI rolled out ChatGPT Developer Mode which provides full MCP support with the ability to add custom MCP Connectors to ChatGPT.&lt;/p&gt;
    &lt;p&gt;Looking through ChatGPTâs client-side JavaScript, we see that ChatGPT passes the modified redirect URL directly to &lt;code&gt;window.open&lt;/code&gt; during the OAuth flow. This is very similar to the &lt;code&gt;use-mcp&lt;/code&gt; package, resulting in an almost identical exploit.&lt;/p&gt;
    &lt;p&gt;However, there is a strong Content Security Policy (CSP) preventing the &lt;code&gt;javascript:&lt;/code&gt; URL from executing. We attempted to exploit with a custom data URL using the &lt;code&gt;text/html&lt;/code&gt; mimetype, but this was also blocked by ChatGPTâs CSP.&lt;/p&gt;
    &lt;head rend="h3"&gt;Server Side Redirect on Claude Web App&lt;/head&gt;
    &lt;p&gt;For connectors added on the Claude web app, we observed that a server-side redirect would be performed with the malicious URL specified by the MCP server. However, JavaScript execution did not occur. This is because &lt;code&gt;javascript:&lt;/code&gt; URLs are not executed from server-side redirects.&lt;/p&gt;
    &lt;head rend="h2"&gt;Industry Response &amp;amp; Fixes&lt;/head&gt;
    &lt;p&gt;The response across affected vendors was swift; but they took different approaches to solving the underlying problem:&lt;/p&gt;
    &lt;head rend="h3"&gt;Different Fix Approaches&lt;/head&gt;
    &lt;p&gt;Cloudflare created a strict-url-sanitise package, which validates URL schemes and blocks &lt;code&gt;javascript:&lt;/code&gt; URLs. This addresses the specific attack vector through input validation.&lt;/p&gt;
    &lt;p&gt;Anthropicâs fix for Claude Code went through multiple iterations, ultimately settling on eliminating shell usage entirely with &lt;code&gt;await execFileAsync("rundll32",["url,OpenURL",url],{});&lt;/code&gt;. As they already had URL schema validation, this removes the attack surface completely.&lt;/p&gt;
    &lt;p&gt;Google dropped the vulnerable &lt;code&gt;open&lt;/code&gt; package and reimplemented URL opening themselves. In their fix PR, they sanitized URLs by escaping single quotes (&lt;code&gt;'&lt;/code&gt; to &lt;code&gt;''&lt;/code&gt;) for PowerShell. This works, but is not a very robust fix.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Most Impactful Fix&lt;/head&gt;
    &lt;p&gt;The biggest impact came from Anthropicâs update to the MCP TypeScript SDK, which blacklisted dangerous URI schemes like &lt;code&gt;javascript:&lt;/code&gt;. As multiple tools including MCP Inspector consume this SDK, this single upstream change improved security across the entire ecosystem instantly.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Not being able to achieve XSS on ChatGPT shows that traditional defense-in-depth methods still work. While the underlying system was vulnerable, CSP prevented us from escalating it into a high-severity vulnerability. Much of the AI space is built on top of existing web technologies and can benefit from taking advantage of web security features.&lt;/p&gt;
    &lt;p&gt;Broad, upstream improvements like what was done in Anthropicâs MCP TypeScript SDK make the ecosystem more secure overall. Exploitation has been too easy in places, but the trajectory is encouraging and we are hopeful for the future of AI security.&lt;/p&gt;
    &lt;head rend="h2"&gt;Acknowledgements&lt;/head&gt;
    &lt;p&gt;Weâd like to thank the following bug bounty programs:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Cloudflare&lt;/item&gt;
      &lt;item&gt;Anthropic&lt;/item&gt;
      &lt;item&gt;Google VRP&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;They had a fast patching process, and both Claude Code and Gemini CLI have an included auto-updater, allowing the fixes to be deployed quickly.&lt;/p&gt;
    &lt;head rend="h3"&gt;Timeline&lt;/head&gt;
    &lt;head rend="h4"&gt;use-mcp&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2025-06-19: Bug reported to Cloudflare via HackerOne&lt;/item&gt;
      &lt;item&gt;2025-06-25: Bug triaged by Cloudflare&lt;/item&gt;
      &lt;item&gt;2025-06-25: Bounty awarded by Cloudflare ($550)&lt;/item&gt;
      &lt;item&gt;2025-06-30: Fix pushed by Cloudflare&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;MCP Inspector&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2025-06-23: Bug reported to Anthropic via HackerOne&lt;/item&gt;
      &lt;item&gt;2025-07-19: Bug triaged by Anthropic&lt;/item&gt;
      &lt;item&gt;2025-08-15: Bounty awarded by Anthropic ($2300)&lt;/item&gt;
      &lt;item&gt;2025-09-06: Published as GHSA-g9hg-qhmf-q45m and CVE-2025-58444&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Claude Code&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2025-07-12: Bug reported to Anthropic via HackerOne&lt;/item&gt;
      &lt;item&gt;2025-07-14: Bug closed by HackerOne Triage team as duplicate&lt;/item&gt;
      &lt;item&gt;2025-07-15: Reopened and properly triaged by Anthropic team&lt;/item&gt;
      &lt;item&gt;2025-07-31: Bounty awarded by Anthropic ($3700)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Gemini CLI&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2025-07-26: Bug reported to Google VRP under OSS VRP program&lt;/item&gt;
      &lt;item&gt;2025-07-28: Bug âidentified as an Abuse Risk and triaged to our Trust &amp;amp; Safety teamâ&lt;/item&gt;
      &lt;item&gt;2025-07-29: Bug filed as P2/S2 (priority and severity)&lt;/item&gt;
      &lt;item&gt;2025-09-04: Abuse VRP panel marks bug as duplicate of already tracked bug.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note: Unlike HackerOne, Google VRP checks duplicates at the same time as deciding bounties.&lt;/p&gt;
    &lt;head rend="h2"&gt;Appendix&lt;/head&gt;
    &lt;head rend="h3"&gt;Other Exploited Vendors&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Cherry Studio was briefly vulnerable, however upon discovery of the vulnerability, we failed to find a suitable security contact. A patch was later created using the same package Cloudflare used (strict-url-sanitise).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The Gemini CLI exploit briefly affected the downstream fork Qwen Code. Once the upstream fix was released, the Qwen Code team quickly patched their fork.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The&lt;/p&gt;&lt;code&gt;open&lt;/code&gt;exploit is not new. It was used before to exploit the&lt;code&gt;mcp-remote&lt;/code&gt;package on npm.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Proof of concepts&lt;/head&gt;
    &lt;p&gt;Each PoC is based on the same code with minor tweaks for each target. Code is published at https://github.com/verialabs/mcp-auth-exploit-pocs, including additional videos showcasing the exploits.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45348183</guid><pubDate>Tue, 23 Sep 2025 15:09:50 +0000</pubDate></item><item><title>Shopify, pulling strings at Ruby Central, forces Bundler and RubyGems takeover</title><link>https://joel.drapper.me/p/rubygems-takeover/</link><description>&lt;doc fingerprint="8f13309c118bf6fe"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Shopify, pulling strings at Ruby Central, forces Bundler and RubyGems takeover&lt;/head&gt;
    &lt;p&gt;Ruby Central recently took over a collection of open source projects from their maintainers without their consent. News of the takeover was first broken by Ellen on 19 September.&lt;/p&gt;
    &lt;p&gt;I have spoken to about a dozen people directly involved in the events, and seen a recording of a key meeting between Ruby Gems maintainers and Ruby Central, to uncover what went on.&lt;/p&gt;
    &lt;p&gt;Here’s a quick summary of what I know:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Ruby Central was struggling for money.&lt;/item&gt;
      &lt;item&gt;Sidekiq withdrew its $250,000/year sponsorship for Ruby Central because they platformed DHH at RailsConf 2025.&lt;/item&gt;
      &lt;item&gt;Shopify demanded that Ruby Central take full control of the RubyGems GitHub repositories and the &lt;code&gt;bundler&lt;/code&gt;and&lt;code&gt;rubygems-update&lt;/code&gt;gems, threatening to withdraw funding if Ruby Central did not comply.&lt;/item&gt;
      &lt;item&gt;HSBT jumped the gun and implemented the takeover plan adding Marty Haught as an owner and reducing maintainers permissions before Marty had discussed this with the maintainers.&lt;/item&gt;
      &lt;item&gt;Marty met with the maintainers after their access was temporarily restored.&lt;/item&gt;
      &lt;item&gt;Marty (and by extension, Ruby Central) understood that Ruby Central did not have the right to take over these GitHub repositories or gems from their long established community maintainers.&lt;/item&gt;
      &lt;item&gt;Marty presented alternatives such as making a fork of the relevant RubyGems projects and warned Ruby Central of the consequences of doing the takeover.&lt;/item&gt;
      &lt;item&gt;The board voted to execute the takeover anyway and Marty executed it immediately.&lt;/item&gt;
      &lt;item&gt;A number of board members subsequently misrepresented the takeover to the Ruby community on social media.&lt;/item&gt;
      &lt;item&gt;This was premeditated. Shopify had organised an on-call rotation to take over from the previous maintainers, some of which at the time were also operating the RubyGems Service.&lt;/item&gt;
      &lt;item&gt;Shopify specifically demanded that at least one of the RubyGems maintainers, André Arko, be excluded from returning to the project. André has been working on RubyGems for over a decade and was also one of the founders of Ruby Together, an organization that merged with Ruby Central.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;The Takeover&lt;/head&gt;
    &lt;p&gt;On 9 September, HSBT (Hiroshi Shibata) — a member of Ruby core and maintainer of RubyGems — renamed the RubyGems GitHub enterprise to “Ruby Central”, added a new owner, Marty Haught, and downgraded the permissions of several other maintainers.&lt;/p&gt;
    &lt;p&gt;According to one of the maintainers, when HSBT was challenged, he refused to revert these changes claiming he needed permission from Marty. On 15 September, Marty said the changes were a mistake and HSBT reverted some of the changes. However, Marty was not removed as an owner, even though the other maintainers never agreed to him being added.&lt;/p&gt;
    &lt;p&gt;On 17 September, RubyGems maintainers met with Marty on Zoom.&lt;/p&gt;
    &lt;p&gt;Marty explained he’s been working on “operational planning” for the RubyGems Service. He was putting together a new Operator Agreement that all the operators of the RubyGems Service would need to sign.&lt;/p&gt;
    &lt;p&gt;He also mentioned that it had been identified as a risk that there were external individuals with ownership permissions over repositories that are necessary for running the RubyGems Service. He said HSBT prematurely changed the ownership permissions before the operational plan was complete.&lt;/p&gt;
    &lt;p&gt;During the discussion, the maintainers clarified with Marty the distinction between the RubyGems source code and the RubyGems Service.&lt;/p&gt;
    &lt;p&gt;RubyGems is a collection of community owned, community maintained repositories of code that are held in commons for everyone in the Ruby community to use.&lt;/p&gt;
    &lt;p&gt;The RubyGems Service is entirely separate from that. It’s a specific deployment: a domain name and servers that happen to be running RubyGems source code. It is operated by Ruby Central.&lt;/p&gt;
    &lt;p&gt;This distinction is important. Anyone else could run the RubyGems source code on their own servers with their own domains. And Ruby Central could decide to run different source code on its servers — whether that be a fork of the RubyGems source code or otherwise.&lt;/p&gt;
    &lt;p&gt;The RubyGems maintainers have been developing this software for decades, predating Ruby Central’s operation of the RubyGems Service. Their contributions represent countless hours of unpaid work, establishing a clear history of community ownership and stewardship.&lt;/p&gt;
    &lt;p&gt;Ruby Central did contribute financially towards RubyGems maintenance, but these contributions did not confer ownership. Ruby Central’s funding of RubyGems development is no different than if they had contributed to the development of Rails, RSpec, or any other open source project. In no case would such funding grant them ownership rights over the project itself.&lt;/p&gt;
    &lt;p&gt;Similarly, Ruby Central’s employment of some RubyGems maintainers to operate the RubyGems Service does not transfer ownership of the separate open source projects.&lt;/p&gt;
    &lt;p&gt;Having personally reviewed a recording of this meeting, I have no doubt that Marty understood this distinction. The RubyGems source code and GitHub organisation was not owned by Ruby Central, even though Ruby Central operated a service with the same name.&lt;/p&gt;
    &lt;p&gt;On 18 September, the team started losing access again. This time they were removed from the GitHub organisation, their &lt;code&gt;rubygems.org&lt;/code&gt; email accounts were disabled and they were removed as owners of the &lt;code&gt;bundler&lt;/code&gt; and &lt;code&gt;rubygems-update&lt;/code&gt; gems. One maintainer, André Arko, was on-call for the RubyGems Service at the time when his access to GitHub and Fastly was revoked.&lt;/p&gt;
    &lt;p&gt;The Ruby Central board had voted for Ruby Central to take control of the RubyGems GitHub repositories and gems. And since Marty was now an owner, he was able to execute this order.&lt;/p&gt;
    &lt;head rend="h2"&gt;Ruby Central becomes mostly dependent on Shopify&lt;/head&gt;
    &lt;p&gt;When Ruby Central decided to platform DHH at the final RailsConf, they lost $250,000 USD of annual sponsorship from Sidekiq, and this I understand left them almost entirely dependent on Shopify.&lt;/p&gt;
    &lt;p&gt;An anonymous source told me that during Rails World, members of Ruby Central, Ruby Core, Rails Core and representatives from major companies (Shopify, GitHub) discussed possible funding options.&lt;/p&gt;
    &lt;p&gt;According to this source, Ruby Central was presented with a proposal for long-term support, but this would only happen if certain RubyGems maintainers were removed.&lt;/p&gt;
    &lt;p&gt;Another source has confirmed to me that a meeting between Rails Foundation and Ruby Central did take place at Rails World, however they were not able to verify the agenda or who was in attendance.&lt;/p&gt;
    &lt;p&gt;I do know that the Rails World conference was attended by HSBT, DHH, Aaron Patterson, Amanda Perino, Shan Cureton, Marty Haught, Ufuk Kayserilioglu and Rafael França.&lt;/p&gt;
    &lt;p&gt;I also know that Shopify specifically put immense financial pressure on Ruby Central to take full control of the RubyGems GitHub organisation and Ruby gems.&lt;/p&gt;
    &lt;p&gt;Freedom Dumlao, a Ruby Central board member, described the board vote saying “if I had voted the other way, I felt I’d be voting to start the process of shutting down Ruby Central”.&lt;/p&gt;
    &lt;p&gt;A source familiar with the events told me that Shopify’s pressure was both carrot and stick. Essentially, do what we ask and we’ll reward you with more funding, long-term financial stability. Don’t do this and you’ll never see a dollar of enterprise money again.&lt;/p&gt;
    &lt;p&gt;This to me strongly suggests that other companies were involved, perhaps through the Rails Foundation. But I have not been able to confirm anything beyond Shopify’s involvement.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Vote&lt;/head&gt;
    &lt;p&gt;According to a source familiar with the events, the Ruby Central board was made aware by Marty of the risks and damage this takeover would likely do to the community. Apparently he also highlighted other options besides the takeover, such as forking some of the projects.&lt;/p&gt;
    &lt;p&gt;Despite this, the board voted in favour of carrying out the takeover and Marty executed it immediately with his new owner privileges.&lt;/p&gt;
    &lt;p&gt;Shopify had given Ruby Central a hard deadline and it seems that Ruby Central only capitulated at the last moment.&lt;/p&gt;
    &lt;p&gt;I don’t know if the timing was intentional, but this takeover happened on the second day of the EuRuKo conference in Europe, which meant many outspoken European Rubyists were distracted at the time.&lt;/p&gt;
    &lt;p&gt;Because this takeover meant locking out most of the RubyGems Service operators including André who was on-call at the time, Shopify had contributed engineers to a new on-call rotation ready to spring into action after the takeover.&lt;/p&gt;
    &lt;p&gt;Shopify developers had been warming up with their first commits in six years coming in at the same time as the takeover.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Response&lt;/head&gt;
    &lt;p&gt;About six hours after Ellen broke the news, Ruby Central published their response: Strengthening the Stewardship of RubyGems and Bundler.&lt;/p&gt;
    &lt;p&gt;A post that feels like AI-generated corporate speak and bears no signature from anyone at Ruby Central willing to take responsibility.&lt;/p&gt;
    &lt;p&gt;The response says, “To strengthen supply chain security, we are taking important steps to ensure that administrative access to the RubyGems.org, RubyGems, and Bundler is securely managed. This includes both our production systems and GitHub repositories. In the near term we will temporarily hold administrative access to these projects while we finalize new policies that limit commit and organization access rights. This decision was made and approved by the Ruby Central Board as part of our fiduciary responsibility.”&lt;/p&gt;
    &lt;p&gt;But while Ruby Central has the right to lock down the RubyGems Service infrastructure, it never owned the RubyGems GitHub repositories.&lt;/p&gt;
    &lt;p&gt;DHH ignored Ellen’s post but instead retweeted the Ruby Central announcement with the caption “Ruby Central is making the right moves to ensure the Ruby supply chain is beyond reproach both technically and organisationally.”&lt;/p&gt;
    &lt;p&gt;A position that seems to stand in stark contrast to his other opinions. For example, he criticised Apple’s control of the App Store and takes the ownership of his own open source projects seriously.&lt;/p&gt;
    &lt;p&gt;Ruby Central board member and Shopify employee Ufuk Kayserilioglu misrepresented what happened, responding to Bluesky threads. For example he said, “Ruby Central has been running the rubygems.org system for years now, so this can hardly be considered a supply chain attack. On the contrary, we have a legal obligation to all the users of the system to keep it safe and secure.”&lt;/p&gt;
    &lt;p&gt;But no one accused Ruby Central of taking over the RubyGems Service and the takeover of the RubyGems GitHub organization and gems was not required to meet Ruby Central’s legal obligations. Remember, Ruby Central was in full control of what source code it deployed to the RubyGems Service which it operated.&lt;/p&gt;
    &lt;p&gt;He also said “How is limiting access to critical and shared infra &amp;amp; code a supply chain attack?” once again conflating the RubyGems source code with the RubyGems Service.&lt;/p&gt;
    &lt;p&gt;On 21 September, Freedom Dumlao published A board member’s perspective of the RubyGems controversy in which he claimed “Ruby Central has been responsible for RubyGems and Bundler for a long time. This isn’t a new development, and I’m honestly very confused about the confusion.”&lt;/p&gt;
    &lt;p&gt;This is a misrepresentation of the real situation where Ruby Central was responsible for operating the RubyGems Service but did not own the RubyGems source code, repositories or gems.&lt;/p&gt;
    &lt;p&gt;He goes on to talk about supply chain attacks, which I admit is a convenient cover, but I don’t believe is the genuine reason for the takeover.&lt;/p&gt;
    &lt;p&gt;He then confirms that a deadline loomed. “Either Ruby Central puts controls in place to ensure the safety and stability of the infrastructure we are responsible for, or lose the funding that we use to keep those things online and going. With less than 24 hours to go, we were still working on this. Conversations with some maintainers were still happening as far as I know but the cooperation we were hoping for was not emerging.”&lt;/p&gt;
    &lt;p&gt;He doesn’t mention Shopify, but based on my other sources, I know it was Shopify that applied this pressure.&lt;/p&gt;
    &lt;p&gt;“It was clear that we weren’t quite ready yet, but in the end we were out of time. A vote had to be cast so we could ensure we did not lose funding necessary to operate RubyGems. What I voted for, was to direct Marty, Ruby Central’s Director of Open Source, to temporarily remove access and lock down the systems, get operator agreements in place with maintainers, and then re-enable access to those folks who needed and wanted it. Marty did exactly what the board asked of him.”&lt;/p&gt;
    &lt;p&gt;This again highlights the pressure Shopify put on Ruby Central.&lt;/p&gt;
    &lt;p&gt;Two sources directly involved told me that access specifically would not be re-enabled for André who had been singled out. Sources have also suggested that Shopify had been pressuring Ruby Central to end their relationship with André and remove him from the RubyGems project for some period of time prior to this taking place.&lt;/p&gt;
    &lt;p&gt;On 23 September, Ruby Central shared a video address by Shan Cureton (Executive Director, Ruby Central) on behalf of Ruby Central’s board and team.&lt;/p&gt;
    &lt;p&gt;In it she claims that Bundler and RubyGems came under Ruby Central’s responsibility through the merger with Ruby Together. But Ruby Together never owned Bundler or RubyGems.&lt;/p&gt;
    &lt;p&gt;She mentioned the departure of a “lead maintainer” [André] and transition of security engineer [Samuel Giddins] as raising questions around access to RubyGems, Bundler and the RubyGems Service.&lt;/p&gt;
    &lt;p&gt;She says sponsors (plural) and companies who depend on Ruby tooling came to them with supply chain concerns. She explained that they couldn’t reach agreement with existing maintainers in the timelines they were facing.&lt;/p&gt;
    &lt;p&gt;I have seen the meeting with the maintainers and can tell you the conversation was primarily about ownership, not security. None of the maintainers had a problem with Ruby Central restricting access to the RubyGems Service that it operated.&lt;/p&gt;
    &lt;p&gt;They had a problem with Ruby Central taking control of the RubyGems open source code repositories and gems, which Ruby Central never owned.&lt;/p&gt;
    &lt;p&gt;She explains that the board voted to remove administrative and commit privileges until agreements could be put in place. She said it was never meant to be permanent.&lt;/p&gt;
    &lt;p&gt;She said “this is not a shutdown of community contribution and it’s not permanent”. However, my sources tell me this will be permanent for at least André and likely Samuel.&lt;/p&gt;
    &lt;p&gt;She said on-call coverage remains in place. We know that André was on-call when his access was revoked, so she must be talking about the new on-call rotation which Shopify contributed to.&lt;/p&gt;
    &lt;p&gt;She said “all of these changes are being made in good faith.” But we know that these changes were made at Shopify’s request to take control of the RubyGems projects and specifically to exclude André (and likely Samuel too).&lt;/p&gt;
    &lt;p&gt;She also talked about two new agreements: Operator Agreements cover access to production systems for on-call and maintenance responsibilities. Contributor Agreements cover access to Bundler and RubyGem code repositories, covering both paid and volunteer maintainers.&lt;/p&gt;
    &lt;p&gt;The Operator Agreements make sense, but it is not Ruby Central’s place to run the RubyGems projects including Bundler and the RubyGems.org source code, which are community owned as explained previously.&lt;/p&gt;
    &lt;p&gt;She said, “in most open source projects where the code is a library or framework, you usually don’t see formal operator agreements. People contribute under contributor license agreements, codes of conduct or decisions made by a steering committee. But RubyGems.org is different. It’s not just code, it’s a production service. It runs critical infrastructure for the Ruby ecosystem, processes billions of downloads, stores sensitive metadata and is relied on by companies that have compliance requirements. Because it’s a service, Ruby Central carries the legal liability, the financial exposure and the operational risk. This is why Operator Agreements are necessary. They ensure access is tied to responsibility and accountability.”&lt;/p&gt;
    &lt;p&gt;Here she conflates RubyGems.org (the source code) with the RubyGems Service operated by Ruby Central and running on the domain name &lt;code&gt;rubygems.org&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Claiming that Ruby Central owns the RubyGems.org repository because it operates a service that uses the source code is like claiming you own Rails because you have a Rails app and sponsored someone who contributed a PR to the project.&lt;/p&gt;
    &lt;p&gt;It’s confusing because of how the projects are named, and Ruby Central are taking advantage of that confusion.&lt;/p&gt;
    &lt;p&gt;The reality is Ruby Central never owned the Ruby Gems source code. They could only take it because Marty was added by HSBT without the consent of other maintainers.&lt;/p&gt;
    &lt;head rend="h2"&gt;RV&lt;/head&gt;
    &lt;p&gt;An important piece of context is that André and Samuel started a new cooperative with Kasper Timm Hansen and Sam Stephenson called Spinel.&lt;/p&gt;
    &lt;p&gt;Spinel is developing a new Ruby management tool called &lt;code&gt;rv&lt;/code&gt;. It was introduced on 25 August 2025, right before Rails World.&lt;/p&gt;
    &lt;p&gt;In his blog post, André says, “For the last ten years or so of working on Bundler, I’ve had a wish rattling around: I want a better dependency manager. It doesn’t just manage your gems, it manages your ruby versions, too. It doesn’t just manage your ruby versions, it installs pre-compiled rubies so you don’t have to wait for ruby to compile from source every time. And more than all of that, it makes it completely trivial to run any script or tool written in ruby, even if that script or tool needs a different ruby than your application does.”&lt;/p&gt;
    &lt;p&gt;Bluesky threads reveal that Rafael França (Shopify / Rails Core) saw this tool as a threat, saying “some of the “admins” even announced publicly many days ago they were launching a competitor tool [rv] and were funding raising for it. I’d not trust the system to such “admin”.”&lt;/p&gt;
    &lt;p&gt;He also quoted the &lt;code&gt;rv&lt;/code&gt; README which says, “Get rid of rvm, rbenv, chruby, asdf, mise, ruby-build, ruby-install, bundler, and rubygems, all at once”, adding the caption “I’m not so sure I trust them to not sabotage rubygems or bundler.”&lt;/p&gt;
    &lt;head rend="h2"&gt;What I don’t know&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;I don’t know how each member voted or exactly how the information was presented to the board. I was hoping that someone would leak it to me, but so far that has not happened.&lt;/item&gt;
      &lt;item&gt;I don’t know if other groups or companies were involved, though circumstantial evidence and hearsay seems to point to this.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you have any information you can provide, please contact me on Signal.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;It is not clear that Ruby Central’s plans include returning control of the RubyGems codebases to their original owners.&lt;/p&gt;
    &lt;p&gt;I am concerned that Ruby Central seems to be vulnerable to coercion by Shopify.&lt;/p&gt;
    &lt;p&gt;I am concerned that Ruby Central’s board with full knowledge of the consequences and the alternatives voted to take over a collection of open source projects from their maintainers without consent. Especially when these maintainers were acting in good faith at the time. This is the organisation we are meant to trust to host our Ruby gems.&lt;/p&gt;
    &lt;p&gt;I am concerned that Rails Core seems to consider &lt;code&gt;rv&lt;/code&gt; a “threat” rather than an exciting development, and I wonder if the “threat” is more Spinel than &lt;code&gt;rv&lt;/code&gt;. It seems likely that Spinel would be less susceptible to enterprise coercion and could offer a genuine alternative to RubyCentral’s RubyGems Service.&lt;/p&gt;
    &lt;head rend="h3"&gt;Disclosure&lt;/head&gt;
    &lt;p&gt;I was employed by Shopify between 2017 and 2022.&lt;/p&gt;
    &lt;head rend="h3"&gt;Disclaimer&lt;/head&gt;
    &lt;p&gt;I have put this story together to the best of my ability based on hours of conversations with many different people involved. But I am not a professional journalist and I may have missed something. If I have made a mistake, please let me know.&lt;/p&gt;
    &lt;p&gt;I am willing to talk to anyone involved to make sure the community has a fair and honest understanding of the events that took place.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45348390</guid><pubDate>Tue, 23 Sep 2025 15:25:29 +0000</pubDate></item><item><title>Always Invite Anna</title><link>https://sharif.io/anna-alexei</link><description>&lt;doc fingerprint="fedd98054dfc58ec"&gt;
  &lt;main&gt;
    &lt;p&gt;I was lucky enough to make a few friends my first semester of college. We ended up hanging out quite a bit during those early months.&lt;/p&gt;
    &lt;p&gt;We’d all get excited for the weekends because Friday nights meant going out to party. Everyone except for Anna, that is.&lt;/p&gt;
    &lt;p&gt;Anna was quiet, shy, and a definitely a goody-two-shoes. She was from Alabama and spoke with a pronounced southern drawl I’d rarely heard in Maryland. She was reserved but friendly once you got to know her. Anna cared about school a lot. She was almost always studying whenever I saw her.&lt;/p&gt;
    &lt;p&gt;Every Friday night we’d make plans to go out together and party. But Anna would always refuse to come. She’d say something along the lines of “I have to study” or “I just don’t feel like it tonight.”&lt;/p&gt;
    &lt;p&gt;Eventually, we stopped inviting Anna out. Everyone except Alexei.&lt;/p&gt;
    &lt;p&gt;I liked Alexei the most in our friend group. He was valedictorian of his high school, played tennis at a competitive level, and was remarkably smart. If anyone deserved to have an ego, it was Alexei. Yet somehow he managed to be the kindest person I’d ever known. But my absolute favorite thing about Alexei was that he always invited Anna to come party with us.&lt;/p&gt;
    &lt;p&gt;One Friday night as we were all about to leave the dorms for a house party, Alexei stopped us. “Hold on, let’s invite Anna.” We headed over to her dorm and invited her to come with us. She said “Sorry, I have to study for my Arabic exam next week, but you guys have fun.”&lt;/p&gt;
    &lt;p&gt;Alexei continued to invite Anna every time we went out for the rest of the semester. And Anna said no every single time.&lt;/p&gt;
    &lt;p&gt;Curious about his persistence, I asked him “Why do you keep inviting Anna out when she’ll just say no?”&lt;/p&gt;
    &lt;p&gt;I’ll never forget what he told me: “I know she’s always going to say no, but that’s not the point. I invite her out so she’ll always feel included in the group.”&lt;/p&gt;
    &lt;p&gt;After that first semester, the friend group disbanded and we all went our separate ways. Many years later I ran into Anna and we ended up catching up. She told me how difficult her first semester of college had been. She was very close with her mom and sister and missed them them terribly.&lt;/p&gt;
    &lt;p&gt;But then she said something that stayed with me: She was grateful. She was grateful to be part of that brief friend group because she felt like she had a family away from home. And that even though she never partied with us, she always felt included because we would stop by her room and invite her anyway.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45348495</guid><pubDate>Tue, 23 Sep 2025 15:33:23 +0000</pubDate></item><item><title>Android users can now use conversational editing in Google Photos</title><link>https://blog.google/products/photos/android-conversational-editing-google-photos/</link><description>&lt;doc fingerprint="f9d32b08ba342eeb"&gt;
  &lt;main&gt;
    &lt;p&gt;Today, we're starting to roll out the conversational editing feature first introduced on Pixel 10 to all eligible Android users in the U.S. Forget switching between tools and adjusting sliders. You can simply describe the edits you want using your voice or text and watch Google Photos bring your vision to life. And now even more people can try it.&lt;/p&gt;
    &lt;p&gt;Just tap “Help me edit” in the editor, describe what you want changed and voila, Photos takes care of it for you — with a little help from advanced Gemini capabilities. Not sure where to start? Use one of the provided suggestions or simply say “make it better.” Or get creative with out-of-this-world edits like transporting an alpaca from a petting zoo to Waikiki (tropical beverage included).&lt;/p&gt;
    &lt;p&gt;With simple gestures and one-tap editing suggestions, AI-powered creative tools and now conversational AI editing, it’s never been easier to bring your creativity to life.&lt;/p&gt;
    &lt;p&gt;Sequences shortened.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45349848</guid><pubDate>Tue, 23 Sep 2025 17:04:56 +0000</pubDate></item><item><title>Find SF parking cops</title><link>https://walzr.com/sf-parking/</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45350690</guid><pubDate>Tue, 23 Sep 2025 18:06:07 +0000</pubDate></item></channel></rss>