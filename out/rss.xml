<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Thu, 25 Sep 2025 09:37:46 +0000</lastBuildDate><item><title>Learning Persian with Anki, ChatGPT and YouTube</title><link>https://cjauvin.github.io/posts/learning-persian/</link><description>&lt;doc fingerprint="9290b055cdb636d2"&gt;
  &lt;main&gt;
    &lt;p&gt;I‚Äôve been learning Persian (Farsi) for a while now, and I‚Äôm using a bunch of tools for it. The central one is certainly Anki, a spaced repetition app to train memory. I‚Äôm creating my own never-ending deck of cards, with different types of content, for different purposes. The most frequent type of cards is grammar focused phrases (very rarely single words) coming sometimes from my own daily life, but also very often directly from videos of the Persian Learning YouTube channel, created by Majid, a very talented and nice Persian teacher, in my opinion.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs take an example, suppose there is this slide in one of Majid‚Äôs videos:&lt;/p&gt;
    &lt;p&gt;From this, I will extract three screenshots (with the MacOS screenshot tool). First, to create a card of type ‚Äúbasic‚Äù (one side). I use this type of card to exercise my reading, which is very difficult and remains stubbornly slow, even though I know the 32 letters of the Persian alphabet quite well by now. But the different ways of writing them (which varies by their position in the word) and the fact that the vowels are not present makes it an enduringly challenging task.&lt;/p&gt;
    &lt;p&gt;The next type of card I create with the two remaining screenshots is ‚Äúbasic and reversed‚Äù, which actually creates two cards (one for each direction), one with some romanized phrase, and the other with the English or French translation:&lt;/p&gt;
    &lt;p&gt;When I review these cards in my daily Anki routine, this is where ChatGPT enters into play. First I have set a ‚ÄúPersian‚Äù project with these instructions:&lt;/p&gt;
    &lt;p&gt;With this project, every time I have a doubt or don‚Äôt remember something in Anki, I just take a screenshot and paste it in the project:&lt;/p&gt;
    &lt;p&gt;With this, I have an instant refresher on any notion, in any context. Sometimes I need to do this over and over, before it gels into a deeper, more instant and visceral ‚Äúknowledge‚Äù.&lt;/p&gt;
    &lt;p&gt;The next set of techniques is also based on YouTube. I use a Chrome extension called Dual Subtitles (which only works of course with videos having actual dual sources of subtitles):&lt;/p&gt;
    &lt;p&gt;The dual subtitles serve a couple of purposes: first as a source of new Anki cards (I create the cards directly, again with screenshots in the clipboard).&lt;/p&gt;
    &lt;p&gt;I also use the Tweaks for YouTube extension, which allows me to get extra keyboard shortcuts, to go back and forward only 1 second, instead of the built-in 5 seconds.&lt;/p&gt;
    &lt;p&gt;With these YouTube extensions, I have developed this particular ‚Äútechnique‚Äù to improve my vocal understanding:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;I listen at 75% speed&lt;/item&gt;
      &lt;item&gt;I use the ‚Äúdual subtitles‚Äù browser extension to have both the Farsi and English subtitles at the same time (I set the Farsi one slightly bigger)&lt;/item&gt;
      &lt;item&gt;Every time a new sentence appears, I read it very quickly first in English (I pause if I need to), and then I listen carefully to the voice, to let the meaning and sound of Farsi infuse my mind (this part is very subtle but the most important: you must ‚Äúfeel‚Äù that you understand, and this feeling must cover even the words that you don‚Äôt know; because the meaning of the sentence is currently present and active in your mind, because you just read the English part, I believe that its mapping with the Farsi words that you then hear is particularly efficient, at least that‚Äôs my theory)&lt;/item&gt;
      &lt;item&gt;I also read the Farsi script, to improve my understanding, and disambiguate certain words for which it‚Äôs hard for me to hear what is exactly said&lt;/item&gt;
      &lt;item&gt;I repeat out loud what has been said also, which is quite important&lt;/item&gt;
      &lt;item&gt;Most importantly: I repeat this process (for a single video) over and over, in order to reach a stage where I genuinely understand what is said, in real-time, which is a very powerful and exhilarating feeling.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45359524</guid><pubDate>Wed, 24 Sep 2025 12:45:07 +0000</pubDate></item><item><title>How to Lead in a Room Full of Experts</title><link>https://idiallo.com/blog/how-to-lead-in-a-room-full-of-experts</link><description>&lt;doc fingerprint="f3751d93156404b7"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Here is a realization I made recently. I'm sitting in a room full of smart people. On one side are developers who understand the ins and outs of our microservice architecture. On the other are the front-end developers who can debug React in their sleep. In front of me is the product team that has memorized every possible user path that exists on our website. And then, there is me. The lead developer. I don't have the deepest expertise on any single technology.&lt;/p&gt;
      &lt;p&gt;So what exactly is my role when I'm surrounded by experts? Well, that's easy. I have all the answers.&lt;/p&gt;
      &lt;head rend="h2"&gt;Technical Leadership&lt;/head&gt;
      &lt;p&gt;OK. Technically, I don't have all the answers. But I know exactly where to find them and connect the pieces together. &lt;/p&gt;
      &lt;p&gt;When the backend team explains why a new authentication service would take three weeks to build, I'm not thinking about the OAuth flows or JWT token validation. Instead, I think about how I can communicate it to the product team who expects it done "sometime this week." When the product team requests a "simple" feature, I'm thinking about the 3 teams that need to be involved to update the necessary microservices.&lt;/p&gt;
      &lt;p&gt;Leadership in technical environments isn't about being the smartest person in the room. It's about being the most effective translator.&lt;/p&gt;
      &lt;head rend="h3"&gt;Leading is a Social Skill&lt;/head&gt;
      &lt;p&gt;I often get "eye rolls" when I say this to developers: You are not going to convince anyone with facts. In a room full of experts, your technical credibility gets you a seat at the table, but your social skills determine whether anything productive happens once you're there.&lt;/p&gt;
      &lt;p&gt;Where ideally you will provide documentation that everyone can read and understand, in reality, you need to talk to get people to understand. People can get animated when it comes to the tools they use. When the database team and the API team are talking past each other about response times, your role isn't to lay down the facts. Instead it's to read the room and find a way to address technical constraints and unclear requirements. It means knowing when to let a heated technical debate continue because it's productive, and when to intervene because it's become personal.&lt;/p&gt;
      &lt;head rend="h3"&gt;Leading is Remembering the Goal&lt;/head&gt;
      &lt;p&gt;When you are an expert in your field, you love to dive deep. It's what makes you experts. But someone needs to keep one eye on the forest while everyone else is examining the trees.&lt;/p&gt;
      &lt;p&gt;I've sat through countless meetings where engineers debated the merits of different caching strategies while the real issue was that we hadn't clearly defined what "fast enough" meant for the user experience. The technical discussion was fascinating, but it wasn't moving us toward shipping.&lt;/p&gt;
      &lt;p&gt;As a leader, your job isn't to have sophisticated technical opinions. It's to ask how this "discussion" can move us closer to solving our actual problem.&lt;/p&gt;
      &lt;p&gt;When you understand a problem, and you have a room full of experts, the solution often emerges from the discussion. But someone needs to clearly articulate what problem we're actually trying to solve.&lt;/p&gt;
      &lt;p&gt;When a product team says customers are reporting the app is too slow, that's not a clear problem. It's a symptom. It might be that users are not noticing when the shopping cart is loaded, or that maybe we have an event that is not being triggered at the right time. Or maybe the app feels sluggish during peak hours. Each of those problems has different solutions, different priorities, and different trade-offs. Each expert might be looking at the problem with their own lense, and may miss the real underlying problem.&lt;/p&gt;
      &lt;p&gt;Your role as a leader is to make sure the problem is translated in a way the team can clearly understand the problem.&lt;/p&gt;
      &lt;head rend="h3"&gt;Leading is Saying "I Don't Know"&lt;/head&gt;
      &lt;p&gt;By definition, leading is knowing the way forward. But in reality, in a room full of experts, pretending to know everything makes you look like an idiot.&lt;/p&gt;
      &lt;p&gt;Instead, "I don't know, but let's figure it out" becomes a superpower. It gives your experts permission to share uncertainty. It models intellectual humility. And it keeps the focus on moving forward rather than defending ego. It's also an opportunity to let your experts shine.&lt;/p&gt;
      &lt;p&gt;Nothing is more annoying than a lead who needs to be the smartest person in every conversation. Your database expert spent years learning how to optimize queries - let them be the hero when performance issues arise. Your security specialist knows threat models better than you, give them the floor when discussing architecture decisions.&lt;/p&gt;
      &lt;p&gt;Make room for some productive discussion. When two experts disagree about implementation approaches, your job isn't to pick the "right" answer. It's to help frame the decision in terms of trade-offs, timeline, and user impact.&lt;/p&gt;
      &lt;p&gt;Your value isn't in having all the expertise. It's in recognizing which expertise is needed when, and creating space for the right people to contribute their best work. &lt;/p&gt;
      &lt;head rend="h3"&gt;The Translation Challenge&lt;/head&gt;
      &lt;p&gt;There was this fun blog post I read recently about how non-developers read tutorials written by developers. What sounds natural to you, can be complete gibberish to someone else. As a lead, you constantly need to think about your audience. You need to learn multiple languages to communicate the same thing:&lt;/p&gt;
      &lt;p&gt;Developer language: "The authentication service has a dependency on the user service, and if we don't implement proper circuit breakers, we'll have cascading failures during high load."&lt;/p&gt;
      &lt;p&gt;Product language: "If our login system goes down, it could take the entire app with it. We need to build in some safeguards, which will add about a week to the timeline but prevent potential outages."&lt;/p&gt;
      &lt;p&gt;Executive language: "We're prioritizing system reliability over feature velocity for this sprint. This reduces risk of user-facing downtime that could impact revenue."&lt;/p&gt;
      &lt;p&gt;All three statements describe the same technical decision, but each is crafted for its audience. Your experts shouldn't have to learn product speak, and your product team shouldn't need to understand circuit breaker patterns. But someone needs to bridge that gap.&lt;/p&gt;
      &lt;head rend="h2"&gt;Beyond "Because, that's why!"&lt;/head&gt;
      &lt;p&gt;"I'm the lead, and we are going to do it this way." That's probably the worst way to make a decision. That might work in the short term, but it erodes trust and kills the collaborative culture that makes expert teams thrive.&lt;/p&gt;
      &lt;p&gt;Instead, treat your teams like adults and communicate the reason behind your decision:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;"We're choosing the more conservative approach because the cost of being wrong is high, and we can iterate later."&lt;/item&gt;
        &lt;item&gt;"I know this feels like extra work, but it aligns with our architectural goals and will save us time on the next three features."&lt;/item&gt;
        &lt;item&gt;"This isn't the most elegant solution, but it's the one we can ship confidently within our timeline."&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;The more comfortable you become with not being the expert, the more effective you become as a leader.&lt;/p&gt;
      &lt;p&gt;When you stop trying to out-expert the experts, you can focus on what expert teams actually need:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Clear problem definitions&lt;/item&gt;
        &lt;item&gt;Context for decision-making&lt;/item&gt;
        &lt;item&gt;Translation between different perspectives&lt;/item&gt;
        &lt;item&gt;Protection from unnecessary complexity&lt;/item&gt;
        &lt;item&gt;Space to do their best work&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Your role isn't to have all the answers. It's to make sure the right questions get asked, the right people get heard, and the right decisions get made for the right reasons.&lt;/p&gt;
      &lt;p&gt;Technical leadership in expert environments is less about command and control, and more about connection and context. You're not the conductor trying to play every instrument. You're the one helping the orchestra understand what song they're playing together.&lt;/p&gt;
      &lt;p&gt;That's a much more interesting challenge than trying to be the smartest person in the room.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45359604</guid><pubDate>Wed, 24 Sep 2025 12:52:52 +0000</pubDate></item><item><title>Smartphone Cameras Go Hyperspectral</title><link>https://spectrum.ieee.org/hyperspectral-imaging</link><description>&lt;doc fingerprint="77951f9c0a9da747"&gt;
  &lt;main&gt;
    &lt;p&gt;The human eye is mostly sensitive to only three bands of the electromagnetic spectrum‚Äîred, green, and blue (RGB)‚Äîin the visible range. In contrast, off-the-shelf smartphone camera sensors are potentially hyperspectral in nature, meaning that each pixel is sensitive to far more spectral bands. Now scientists have found a simple way for any conventional smartphone camera to serve as a hyperspectral sensor‚Äîby placing a card with a chart on it within its view. The new patent-pending technique may find applications in defense, security, medicine, forensics, agriculture, environmental monitoring, industrial quality control, and food and beverage quality analysis, the researchers add.&lt;/p&gt;
    &lt;p&gt;‚ÄúAt the heart of this work is a simple but powerful idea‚Äîa photo is never just an image,‚Äù says Semin Kwon, a postdoctoral research associate of biomedical engineering Purdue University in West Lafayette, Ind. ‚ÄúEvery photo carries hidden spectral information waiting to be uncovered. By extracting it, we can turn everyday photography into science.‚Äù&lt;/p&gt;
    &lt;p&gt;Using a smartphone camera and a spectral color chart, researchers can image the transmission spectrum of high-end whiskey, thus determining its authenticity. Semin Kwon/Purdue University&lt;/p&gt;
    &lt;p&gt;Every molecule has a unique spectral signature‚Äîthe degree to which it absorbs or reflects each wavelength of light. The extreme sensitivity to distinguishing color seen in scientific-grade hyperspectral sensors can help them identify chemicals based on their spectral signatures, for applications in a wide range of industries, such as medical diagnostics, distinguishing authentic versus counterfeit whiskey, monitoring air quality, and nondestructive analysis of pigments in artwork, says Young Kim, a professor of biomedical engineering at Purdue.&lt;/p&gt;
    &lt;p&gt;Previous research has pursued a number of different ways to recover spectral details from conventional smartphone RGB camera data. However, machine learning models developed for this purpose typically rely heavily on the task-specific data on which they are trained. This limits their generalizability and makes them susceptible to errors resulting from variations in lighting, image file formats, and more. Another possible avenue involved special hardware attachments, but these can prove expensive and bulky.&lt;/p&gt;
    &lt;p&gt;In the new study, the scientists designed a special color reference chart that can be printed on a card. They also developed an algorithm that can analyze smartphone pictures taken with this card and account for factors such as lighting conditions. This strategy can extract hyperspectral data from raw images with a sensitivity of 1.6 nanometers of difference in wavelength of visible light, comparable to scientific-grade spectrometers.&lt;/p&gt;
    &lt;p&gt;‚ÄúIn short, this technique could turn an ordinary smartphone into a pocket spectrometer,‚Äù Kim says.&lt;/p&gt;
    &lt;p&gt;The scientists are currently pursuing applications for their new technique in digital and mobile-health applications in both domestic and resource-limited settings. ‚ÄúWe are truly excited that this opens the door to making spectroscopy both affordable and accessible,‚Äù Kwon says.&lt;/p&gt;
    &lt;p&gt;The scientists recently detailed their findings in the journal IEEE Transactions on Image Processing.&lt;/p&gt;
    &lt;p&gt;Charles Q. Choi is a science reporter who contributes regularly to IEEE Spectrum. He has written for Scientific American, The New York Times, Wired, and Science, among others.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45360824</guid><pubDate>Wed, 24 Sep 2025 14:20:33 +0000</pubDate></item><item><title>Show HN: Dayflow ‚Äì A git log for your day</title><link>https://github.com/JerryZLiu/Dayflow</link><description>&lt;doc fingerprint="6ed6256e19be11b7"&gt;
  &lt;main&gt;
    &lt;p&gt;Turns your screen activity into a clean timeline with AI summaries and distraction highlights.&lt;/p&gt;
    &lt;p&gt;Quickstart ‚Ä¢ Why I built Dayflow ‚Ä¢ Features ‚Ä¢ How it works ‚Ä¢ Installation ‚Ä¢ Data &amp;amp; Privacy ‚Ä¢ Debug &amp;amp; Developer Tools ‚Ä¢ Auto‚Äëupdates ‚Ä¢ Contributing&lt;/p&gt;
    &lt;p&gt;Dayflow is a native macOS app (SwiftUI) that records your screen at 1 FPS, analyzes it every 15 minutes with AI, and generates a timeline of your activities with summaries. It's lightweight (25MB app size) and uses ~100MB of RAM and &amp;lt;1% cpu.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Privacy‚Äëminded by design: You choose your AI provider. Use Gemini (bring your own API key) or local models (Ollama / LM Studio). See Data &amp;amp; Privacy for details.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I built Dayflow after realizing that my calendar wasn't the source of truth for how I actually spent my time. My screen was. I wanted a calm, trustworthy timeline that let me see my workday without turning into yet another dashboard I had to maintain.&lt;/p&gt;
    &lt;p&gt;Dayflow stands for ownership and privacy by default. You control the data, you choose the AI provider, and you can keep everything local if that's what makes you comfortable. It's MIT licensed and fully open source because anything that watches your screen all day should be completely transparent about what it does with that information. The app should feel like a quiet assistant: respectful of your attention, honest about what it captures, and easy to shut off.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Automatic timeline of your day with concise summaries.&lt;/item&gt;
      &lt;item&gt;1 FPS recording - minimal CPU/storage impact.&lt;/item&gt;
      &lt;item&gt;15-minute analysis intervals for timely updates.&lt;/item&gt;
      &lt;item&gt;Watch timelapses of your day.&lt;/item&gt;
      &lt;item&gt;Auto storage cleanup - removes old recordings after 3 days.&lt;/item&gt;
      &lt;item&gt;Distraction highlights to see what pulled you off‚Äëtask.&lt;/item&gt;
      &lt;item&gt;Native UX built with SwiftUI.&lt;/item&gt;
      &lt;item&gt;Auto‚Äëupdates with Sparkle (daily check + background download).&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Infinitely customizable dashboard ‚Äî ask any question about your workday, pipe the answers into tiles you arrange yourself, and track trends over time.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Daily journal ‚Äî review the highlights Dayflow captured, reflect with guided prompts, and drop screenshots or notes alongside your generated timeline.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Capture ‚Äî Records screen at 1 FPS in 15-second chunks.&lt;/item&gt;
      &lt;item&gt;Analyze ‚Äî Every 15 minutes, sends recent footage to AI.&lt;/item&gt;
      &lt;item&gt;Generate ‚Äî AI creates timeline cards with activity summaries.&lt;/item&gt;
      &lt;item&gt;Display ‚Äî Shows your day as a visual timeline.&lt;/item&gt;
      &lt;item&gt;Cleanup ‚Äî Auto-deletes recordings older than 3 days.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The efficiency of your timeline generation depends on your chosen AI provider:&lt;/p&gt;
    &lt;code&gt;flowchart LR
    subgraph Gemini["Gemini Flow: 2 LLM Calls"]
        direction LR
        GV[Video] --&amp;gt; GU[Upload + Transcribe&amp;lt;br/&amp;gt;1 LLM call] --&amp;gt; GC[Generate Cards&amp;lt;br/&amp;gt;1 LLM call] --&amp;gt; GD[Done]
    end

    subgraph Local["Local Flow: 33+ LLM Calls"]
        direction LR
        LV[Video] --&amp;gt; LE[Extract 30 frames] --&amp;gt; LD[30 descriptions&amp;lt;br/&amp;gt;30 LLM calls] --&amp;gt; LM[Merge&amp;lt;br/&amp;gt;1 call] --&amp;gt; LT[Title&amp;lt;br/&amp;gt;1 call] --&amp;gt; LC[Merge Check&amp;lt;br/&amp;gt;1 call] --&amp;gt; LMC[Merge Cards&amp;lt;br/&amp;gt;1 call] --&amp;gt; LD2[Done]
    end

    %% Styling
    classDef geminiFlow fill:#e8f5e8,stroke:#4caf50,stroke-width:2px
    classDef localFlow fill:#fff8e1,stroke:#ff9800,stroke-width:2px
    classDef geminiStep fill:#4caf50,color:#fff
    classDef localStep fill:#ff9800,color:#fff
    classDef processing fill:#f5f5f5,stroke:#666
    classDef result fill:#e3f2fd,stroke:#1976d2

    class Gemini geminiFlow
    class Local localFlow
    class GU,GC geminiStep
    class LD,LM,LT,LC,LMC localStep
    class GV,LV,LE processing
    class GD,LD2 result
&lt;/code&gt;
    &lt;p&gt;Gemini leverages native video understanding for direct analysis, while Local models reconstruct understanding from individual frame descriptions - resulting in dramatically different processing complexity.&lt;/p&gt;
    &lt;p&gt;Download (end users)&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Grab the latest &lt;code&gt;Dayflow.dmg&lt;/code&gt;from GitHub Releases.&lt;/item&gt;
      &lt;item&gt;Open the app; grant Screen &amp;amp; System Audio Recording when prompted:&lt;lb/&gt;macOS ‚Üí System Settings ‚Üí Privacy &amp;amp; Security ‚Üí Screen &amp;amp; System Audio Recording ‚Üí enable Dayflow.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Build from source (developers)&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Install Xcode 15+ and open &lt;code&gt;Dayflow.xcodeproj&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Run the &lt;code&gt;Dayflow&lt;/code&gt;scheme on macOS 13+.&lt;/item&gt;
      &lt;item&gt;In your Run scheme, add your &lt;code&gt;GEMINI_API_KEY&lt;/code&gt;under Arguments &amp;gt; Environment Variables (if using Gemini).&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;macOS 13.0+&lt;/item&gt;
      &lt;item&gt;Xcode 15+&lt;/item&gt;
      &lt;item&gt;A Gemini API key (if using Gemini): https://ai.google.dev/gemini-api/docs/api-key&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Download &lt;code&gt;Dayflow.dmg&lt;/code&gt;and drag Dayflow into Applications.&lt;/item&gt;
      &lt;item&gt;Launch and grant the Screen &amp;amp; System Audio Recording permission.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;git clone https://github.com/JerryZLiu/Dayflow.git
cd Dayflow
open Dayflow.xcodeproj
# In Xcode: select the Dayflow target, configure signing if needed, then Run.&lt;/code&gt;
    &lt;p&gt;This section explains what Dayflow stores locally, what leaves your machine, and how provider choices affect privacy.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;App support folder: &lt;code&gt;~/Library/Application Support/Dayflow/&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Recordings (video chunks): &lt;code&gt;~/Library/Application Support/Dayflow/recordings/&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Local database: &lt;code&gt;~/Library/Application Support/Dayflow/chunks.sqlite&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Recording details: 1 FPS capture, analyzed every 15 minutes, 3-day retention&lt;/item&gt;
      &lt;item&gt;Purge / reset tip: Quit Dayflow. Then delete the entire &lt;code&gt;~/Library/Application Support/Dayflow/&lt;/code&gt;folder to remove recordings and analysis artifacts. Relaunch to start fresh.&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;p&gt;These paths are created by the app at first run. If you package Dayflow differently or run in a sandbox, paths may vary slightly.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Gemini (cloud, BYO key) ‚Äî Dayflow sends batch payloads to Google‚Äôs Gemini API for analysis.&lt;/item&gt;
      &lt;item&gt;Local models (Ollama / LM Studio) ‚Äî Processing stays on‚Äëdevice; Dayflow talks to a local server you run.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Short answer: There is a way to prevent Google from training on your data. If you enable Cloud Billing on at least one Gemini API project, Google treats all of your Gemini API and Google AI Studio usage under the ‚ÄúPaid Services‚Äù data‚Äëuse rules ‚Äî even when you‚Äôre using unpaid/free quota. Under Paid Services, Google does not use your prompts/responses to improve Google products/models. &lt;list rend="ul"&gt;&lt;item&gt;Terms: ‚ÄúWhen you activate a Cloud Billing account, all use of Gemini API and Google AI Studio is a ‚ÄòPaid Service‚Äô with respect to how Google Uses Your Data, even when using Services that are offered free of charge.‚Äù (Gemini API Additional Terms)&lt;/item&gt;&lt;item&gt;Abuse monitoring: even under Paid Services, Google logs prompts/responses for a limited period for policy enforcement and legal compliance. (Same Terms)&lt;/item&gt;&lt;item&gt;EEA/UK/Switzerland: the Paid‚Äëstyle data handling applies by default to all Services (including AI Studio and unpaid quota) even without billing. (Same Terms)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A couple useful nuances (from docs + forum clarifications):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;AI Studio is still free to use; enabling billing changes data handling, not whether Studio charges you. (Pricing page)&lt;/item&gt;
      &lt;item&gt;UI ‚ÄúPlan: Paid‚Äù check: In AI Studio ‚Üí API keys, you‚Äôll typically see ‚ÄúPlan: Paid‚Äù once billing is enabled on any linked project (UI may evolve).&lt;/item&gt;
      &lt;item&gt;Free workaround: ‚ÄúMake one project paid, keep using a free key elsewhere to get the best of both worlds.‚Äù The Terms imply account‚Äëlevel coverage once any billing account is activated, but the Apps nuance above may limit this in specific UI contexts. Treat this as an interpretation, not legal advice.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Privacy: With Ollama/LM Studio, prompts and model inference run on your machine. LM Studio documents full offline operation once models are downloaded.&lt;/item&gt;
      &lt;item&gt;Quality/latency: Local open models are improving but can underperform cloud models on complex summarization.&lt;/item&gt;
      &lt;item&gt;Power/battery: Local inference is GPU‚Äëheavy on Apple Silicon and will drain battery faster; prefer plugged‚Äëin sessions for long captures.&lt;/item&gt;
      &lt;item&gt;Future: We may explore fine‚Äëtuning or distilling a local model for better timeline summaries.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;References:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;LM Studio offline: https://lmstudio.ai/docs/app/offline&lt;/item&gt;
      &lt;item&gt;Ollama GPU acceleration (Metal on Apple): https://github.com/ollama/ollama/blob/main/docs/gpu.md&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To record your screen, Dayflow requires the Screen &amp;amp; System Audio Recording permission. Review or change later at:&lt;lb/&gt; System Settings ‚Üí Privacy &amp;amp; Security ‚Üí Screen &amp;amp; System Audio Recording.&lt;lb/&gt; Apple‚Äôs docs: https://support.apple.com/guide/mac-help/control-access-screen-system-audio-recording-mchld6aa7d23/mac&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;AI Provider &lt;list rend="ul"&gt;&lt;item&gt;Choose Gemini (set &lt;code&gt;GEMINI_API_KEY&lt;/code&gt;) or Local (Ollama/LM Studio endpoint).&lt;/item&gt;&lt;item&gt;For Gemini keys: https://ai.google.dev/gemini-api/docs/api-key&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Choose Gemini (set &lt;/item&gt;
      &lt;item&gt;Capture settings &lt;list rend="ul"&gt;&lt;item&gt;Start/stop capture from the main UI. Use Debug to verify batch contents.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Data locations &lt;list rend="ul"&gt;&lt;item&gt;See Data &amp;amp; Privacy for exact paths and a purge tip.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You can click the Dayflow icon in the menu bar and view the saved recordings&lt;/p&gt;
    &lt;p&gt;Dayflow integrates Sparkle via Swift Package Manager and shows the current version + a ‚ÄúCheck for updates‚Äù action. By default, the updater auto‚Äëchecks daily and auto‚Äëdownloads updates.&lt;/p&gt;
    &lt;code&gt;Dayflow/
‚îú‚îÄ Dayflow/                 # SwiftUI app sources (timeline UI, debug UI, capture &amp;amp; analysis pipeline)
‚îú‚îÄ docs/                    # Appcast and documentation assets (screenshots, videos)
‚îú‚îÄ scripts/                 # Release automation (DMG, notarization, appcast, Sparkle signing, one-button release)
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Screen capture is blank or fails&lt;lb/&gt;Check System Settings ‚Üí Privacy &amp;amp; Security ‚Üí Screen &amp;amp; System Audio Recording and ensure Dayflow is enabled.&lt;/item&gt;
      &lt;item&gt;API errors&lt;lb/&gt;Go into settings and verify your&lt;code&gt;GEMINI_API_KEY&lt;/code&gt;and network connectivity.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;V1 of the Dashboard (track answers to custom questions)&lt;/item&gt;
      &lt;item&gt;V1 of the daily journal&lt;/item&gt;
      &lt;item&gt;Fine tuning a small VLM&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;PRs welcome! If you plan a larger change, please open an issue first to discuss scope and approach.&lt;/p&gt;
    &lt;p&gt;Licensed under the MIT License. See LICENSE for the full text. Software is provided ‚ÄúAS IS‚Äù, without warranty of any kind.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Sparkle for battle‚Äëtested macOS updates.&lt;/item&gt;
      &lt;item&gt;Google AI Gemini API for analysis.&lt;/item&gt;
      &lt;item&gt;Ollama and LM Studio for local model support.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45361268</guid><pubDate>Wed, 24 Sep 2025 14:53:57 +0000</pubDate></item><item><title>How to be a leader when the vibes are off</title><link>https://chaoticgood.management/how-to-be-a-leader-when-the-vibes-are-off/</link><description>&lt;doc fingerprint="7a01915d36b1034c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;How to Be a Leader When the Vibes Are Off&lt;/head&gt;
    &lt;head rend="h2"&gt;...and the vibes are definitely off&lt;/head&gt;
    &lt;p&gt;It feels different in tech right now. We‚Äôre coming off a long era where optimism carried the industry. Something has curdled. AI hype, return-to-office mandates, and continued layoffs have shifted the mood. Managers are quicker to fire, existential dread has replaced the confidence that a tight job market for developers provided for decades. The vibes are for sure off.&lt;/p&gt;
    &lt;head rend="h3"&gt;What‚Äôs Changed?&lt;/head&gt;
    &lt;p&gt;(What follows are generalizations. If your company is escaping some or all of these, I applaud you. I‚Äôm sure there are exceptions.)&lt;/p&gt;
    &lt;p&gt;AI has injected some destabilization. ‚ÄúI don‚Äôt need junior devs when I can just pay $20/month for Cursor‚Äù has an effect on everyone even if this turns out to be silly down the road. I see lots of people worried that the aim of all of this is to ultimately have a robot do their entire job. Whether or not this is possible doesn‚Äôt mean people aren‚Äôt going to try. And it‚Äôs the trying that raises people‚Äôs anxiety. On top of that, we‚Äôve also got ‚ÄúAI Workslop‚Äù to contend with as well, which is making work harder for the diligent among us.&lt;/p&gt;
    &lt;p&gt;Return to Office feels like trust has been broken. Teams that continued to work well (or in some cases, better) after everyone in the industry went remote are now being told to come back to desks in offices. I‚Äôve even heard tales of this happening despite there not being enough office space for everyone, which seems very silly. Also, for the first time in my nearly 30-year career, I‚Äôve even heard of people being told they need to be ‚Äúat their desks at 9am‚Äù and ‚Äúexpected to stay until 5pm at a minimum.‚Äù Even before COVID-19 and the mass move to remote work, most companies were flexible on start and stop times. I almost never heard of set hours for software developers until recently. Rules like that scream ‚Äúwe don‚Äôt trust you unless we can see you,‚Äù even if that‚Äôs not really the reason for the mandates. (IMO there are benefits to working in the same location as your colleagues but ham-fisted, poorly thought out mandates are not the way to achieve them.)&lt;/p&gt;
    &lt;p&gt;Layoffs changed the market. For probably 20 years, job security wasn‚Äôt really a concern in the industry. Layoffs happened here and there and companies folded, but the demand was always strong and most people capable of writing code or managing people who write code could lose their job, spend the severance on a nice vacation, and return with the confidence that they‚Äôd be able to land a new gig in a couple of weeks, likely at higher pay. With the acknowledgement that this was a privilege not enjoyed by most of the working world, it is no longer true. The size and scope of layoffs over the last couple of years have injected more anxiety into the tech workforce.&lt;/p&gt;
    &lt;p&gt;C-Suite Energy has changed. Across the board, execs seem more efficiency-focused, financialized, and less mission-driven. The days of ‚Äútake care of the employees and the employees will take care of the business‚Äù feel like they‚Äôre in the rear-view mirror, and a new ‚Äúdo your job, or else!‚Äù mentality has taken its place.&lt;/p&gt;
    &lt;p&gt;You can‚Äôt change the macro forces that are driving these trends, but you can control how you show up for your team.&lt;/p&gt;
    &lt;head rend="h2"&gt;Wearing the ‚ÄòCompany Hat‚Äô vs. Chaotic-Good Leadership&lt;/head&gt;
    &lt;p&gt;My standard advice to anyone with a management role and anyone at the Staff+ level of individual contributor is that ‚Äúwearing the company hat‚Äù should be the default. You‚Äôre not always going to agree with the decisions that come down from the top. Even when you don‚Äôt agree with decisions the company leadership is making, part of your job is representing and facilitating those decisions with full alignment. When acting ‚Äúin public‚Äù (all-hands, department meetings, the #general channel), this is mandatory, as contradicting the bosses in a broad forum can kill the credibility you have the leadership across the wider team. It‚Äôs also a good way to get yourself fired.&lt;/p&gt;
    &lt;head rend="h3"&gt;Let them know you‚Äôre still on their side&lt;/head&gt;
    &lt;p&gt;But you know what also kills trust? Telling your team it‚Äôs sunny out when everyone can plainly see that it‚Äôs raining. Your team is made up of smart adults who can, at the very least, count the number of employees and the number of desks and calculate that ‚Äúeveryone in the office on Wednesday‚Äù isn‚Äôt going to work out well if the people outnumber the chairs. Telling them something else is going to make you look like an idiot toady in their eyes.&lt;/p&gt;
    &lt;p&gt;The right thing to do in this situation is to acknowledge that you see the situation the same way they do, but do it privately, within your immediate team only or in 1-1s. ‚ÄúYeah, this new policy sucks, I get it. It‚Äôs going to affect me in negative ways too.‚Äù It‚Äôs really important that you validate the emotions that all of these aspects are bringing up in people.&lt;/p&gt;
    &lt;head rend="h3"&gt;Don‚Äôt pretend you can fix it&lt;/head&gt;
    &lt;p&gt;You can promise to advocate for saner policies when the opportunity arises if your sphere of influence makes that possible, but don‚Äôt promise to make the problem go away if you can‚Äôt. Broken promises and poor do/say ratio performance will also kill your team‚Äôs faith in you, especially when it‚Äôs about things they really care about. And again, this is not a time for grandstanding. In public, you have to support the policies, but when you‚Äôre in private with your manager and your peers, that‚Äôs the time you can safely push for change.&lt;/p&gt;
    &lt;head rend="h3"&gt;Find small workarounds to make things livable&lt;/head&gt;
    &lt;p&gt;If you can provide some flexibility on seemingly inflexible policies, do it. If your management role includes enforcing the company‚Äôs rules, you can use some discretion about how strictly you want to enforce them. Personally, I would never want to ‚Äúrat out‚Äù a good performer who can‚Äôt get to their desk by 9am sharp because they have to drop off their kids or punish someone who bugged out early once to catch their favourite performer in concert one town over. Small acts demonstrating that you trust your team, even if the C-Suite doesn‚Äôt seem to trust the broader team the way they used to, can go a long way toward maintaining good morale within your group.&lt;/p&gt;
    &lt;p&gt;When things feel shaky in the broader org, people will look more to their direct leader for a sense of stability. The best thing you can do for them is provide it. Quiet honesty builds credibility and fosters loyalty.&lt;/p&gt;
    &lt;head rend="h2"&gt;This too shall pass&lt;/head&gt;
    &lt;p&gt;The industry is going through a period where a lot is changing all at once. We‚Äôve had a few of them before. Things will eventually settle down into a new normal. I‚Äôm not great at predictions, so I‚Äôll refrain from detailing what I think things will look like, but I don‚Äôt think it‚Äôll be entirely unfamiliar to those who were here before this latest inflection point. This is especially true if leaders who care and treat their staff like adults can stay grounded and stay true to their principles, even when that means performing small, quiet acts of rebellion.&lt;/p&gt;
    &lt;p&gt;You can‚Äôt fix the macro trends, but you can try to keep your corner of the tech world a place where people are glad to work.&lt;/p&gt;
    &lt;p&gt;"Off Kilter" by anujd89 is licensed under CC BY 2.0 .&lt;/p&gt;
    &lt;p&gt;Like this? Please feel free to share it on your favourite social media or link site! Share it with friends!&lt;/p&gt;
    &lt;p&gt;Hit subscribe to get new posts delivered to your inbox automatically.&lt;lb/&gt;Feedback? Get in touch!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45361394</guid><pubDate>Wed, 24 Sep 2025 15:03:59 +0000</pubDate></item><item><title>Python on the Edge: Fast, sandboxed, and powered by WebAssembly</title><link>https://wasmer.io/posts/python-on-the-edge-powered-by-webassembly</link><description>&lt;doc fingerprint="87ab2dfdccf438d4"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Python on the Edge: Fast, sandboxed, and powered by WebAssembly&lt;/head&gt;
    &lt;p&gt;We are excited to announce full Python support in Wasmer Edge (Beta)&lt;/p&gt;
    &lt;p&gt;Founder &amp;amp; CEO&lt;/p&gt;
    &lt;p&gt;With AI workloads on the rise, the demand for Python support on WebAssembly on the Edge has grown rapidly.&lt;/p&gt;
    &lt;p&gt;However, bringing Python to WebAssembly isn't trivial as it means supporting native modules like &lt;code&gt;numpy&lt;/code&gt;, &lt;code&gt;pandas&lt;/code&gt;, and &lt;code&gt;pydantic&lt;/code&gt;.&amp;#13;
While projects like &lt;code&gt;pyodide&lt;/code&gt; made strides in running Python in the browser via WebAssembly, their trade-offs don't fully fit server-side needs.&lt;/p&gt;
    &lt;p&gt;After months of hard work, today we're thrilled to announce full Python support in Wasmer Edge (Beta) powered by WebAssembly and WASIX.&lt;/p&gt;
    &lt;p&gt;Now you can run FastAPI, Streamlit, Django, LangChain, MCP servers and more directly on Wasmer and Wasmer Edge! To accomplish it we had to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Add support for dynamic linking (&lt;code&gt;dlopen&lt;/code&gt;/&lt;code&gt;dlsym&lt;/code&gt;) into WASIX&lt;/item&gt;
      &lt;item&gt;Add &lt;code&gt;libffi&lt;/code&gt;support (so Python libraries using&lt;code&gt;ctypes&lt;/code&gt;could be supported)&lt;/item&gt;
      &lt;item&gt;Polish Sockets and threading support in WASIX&lt;/item&gt;
      &lt;item&gt;Release our own Python Package Index with many of the most popular Python Native libraries compiled to WASIX&lt;/item&gt;
      &lt;item&gt;Create our own alternative to Heroku Buildpacks / Nixpacks / Railpack / Devbox to automatically detect a project type from its source code and deploy it (including running with Wasmer or deploying to Wasmer Edge!). Updates will be shared soon!&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;How fast is it?&lt;/head&gt;
    &lt;p&gt;This Python release is much faster than any of the other Python releases we did in the past.&lt;/p&gt;
    &lt;p&gt;It is fast. Insa‚Ä¶natively fast (it's even faster than our py2wasm project!)&lt;/p&gt;
    &lt;code&gt;$ wasmer run python/python@=0.2.0 --dir=. -- pystone.py&amp;#13;
Pystone(1.1) time for 50000 passes = 0.562538&amp;#13;
This machine benchmarks at 88882.9 pystones/second&amp;#13;
$ wasmer run python/python --dir=. -- pystone.py # Note: first run may take time&amp;#13;
Pystone(1.1) time for 50000 passes = 0.093556&amp;#13;
This machine benchmarks at 534439 pystones/second&amp;#13;
$ python3 pystone.py&amp;#13;
Pystone(1.1) time for 50000 passes = 0.0827736&amp;#13;
This machine benchmarks at 604057 pystones/second
&lt;/code&gt;
    &lt;p&gt;That's 6x faster, and nearly indistinguishable from native Python performance‚Ä¶ quite good, considering that your Python apps can now run fully sandboxed anywhere!&lt;/p&gt;
    &lt;p&gt;Note: the first time you run Python, it will take a few minutes to compile. We are working to improve this so no time will be spent on compilation locally.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;üöÄ Even faster performance coming soon: we are trialing an optimization technique that will boost Python performance in Wasm to 95% of native Python speed. This is already powering our PHP server in production. Result: Near-native Python performance, fully sandboxed. Stay tuned!&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;What it can run&lt;/head&gt;
    &lt;p&gt;Now, you can run any kind of Python API server, powered by &lt;code&gt;fastapi&lt;/code&gt;, &lt;code&gt;django&lt;/code&gt;, &lt;code&gt;flask&lt;/code&gt;, or &lt;code&gt;starlette&lt;/code&gt;, connected to a MySQL database automatically when needed (FastAPI template, Django template).&lt;/p&gt;
    &lt;p&gt;You can run &lt;code&gt;fastapi&lt;/code&gt; with websockets (example repo, demo).&lt;/p&gt;
    &lt;p&gt;You can run &lt;code&gt;mcp&lt;/code&gt; servers (deploy using our MCP template, demo).&lt;/p&gt;
    &lt;p&gt;You can run image processors like &lt;code&gt;pillow&lt;/code&gt;  (example repo, demo).&lt;/p&gt;
    &lt;p&gt;You can run &lt;code&gt;ffmpeg&lt;/code&gt; inside Python (example repo, demo).&lt;/p&gt;
    &lt;p&gt;You can run &lt;code&gt;streamlit&lt;/code&gt; and &lt;code&gt;langchain&lt;/code&gt; (deploy using our LangChain template, demo).&lt;/p&gt;
    &lt;p&gt;You can even run &lt;code&gt;pypandoc&lt;/code&gt;!  (example repo, demo).&lt;/p&gt;
    &lt;p&gt;Soon, we'll have full support for:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;curl_cffi&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;polars&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;code&gt;gevent&lt;/code&gt;/&lt;code&gt;greenlet&lt;/code&gt;(more on this soon!)&lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;Pytorch&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;Wasmer VS alternatives&lt;/head&gt;
    &lt;p&gt;Python on Wasmer Edge is just launching, but it's already worth asking: how does it stack up existing solutions?&lt;/p&gt;
    &lt;head rend="h3"&gt;Quick Comparison&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Feature / Platform&lt;/cell&gt;
        &lt;cell role="head"&gt;Wasmer Edge&lt;/cell&gt;
        &lt;cell role="head"&gt;Cloudflare&lt;/cell&gt;
        &lt;cell role="head"&gt;AWS Lambda&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Native modules (&lt;code&gt;numpy&lt;/code&gt;, &lt;code&gt;pandas&lt;/code&gt;, etc.)&lt;/cell&gt;
        &lt;cell&gt;‚úÖ Supported*&lt;/cell&gt;
        &lt;cell&gt;‚ùå Limited (no &lt;code&gt;libcurl&lt;/code&gt;)&lt;/cell&gt;
        &lt;cell&gt;‚úÖ Full support&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Multithreading &amp;amp; multiprocessing (&lt;code&gt;ffmpeg&lt;/code&gt;, &lt;code&gt;pandoc&lt;/code&gt;)&lt;/cell&gt;
        &lt;cell&gt;‚úÖ Yes&lt;/cell&gt;
        &lt;cell&gt;‚ùå No&lt;/cell&gt;
        &lt;cell&gt;‚úÖ Yes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;ASGI / WSGI frameworks (&lt;code&gt;uvicorn&lt;/code&gt;, &lt;code&gt;daphne&lt;/code&gt;)&lt;/cell&gt;
        &lt;cell&gt;‚úÖ Supported&lt;/cell&gt;
        &lt;cell&gt;‚ùå Patched / limited&lt;/cell&gt;
        &lt;cell&gt;‚ö†Ô∏è Needs wrappers&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;WebSockets (&lt;code&gt;streamlit&lt;/code&gt;)&lt;/cell&gt;
        &lt;cell&gt;‚úÖ Yes&lt;/cell&gt;
        &lt;cell&gt;‚ùå No&lt;/cell&gt;
        &lt;cell&gt;‚ùå No&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Raw sockets (&lt;code&gt;libcurl&lt;/code&gt;)&lt;/cell&gt;
        &lt;cell&gt;‚úÖ Supported&lt;/cell&gt;
        &lt;cell&gt;‚ùå JS &lt;code&gt;fetch&lt;/code&gt; only&lt;/cell&gt;
        &lt;cell&gt;‚úÖ Supported&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Multiple Python versions&lt;/cell&gt;
        &lt;cell&gt;‚úÖ In Roadmap (3.12, 3.14‚Ä¶)&lt;/cell&gt;
        &lt;cell&gt;‚ùå Tied to bundled runtime&lt;/cell&gt;
        &lt;cell&gt;‚úÖ Supported&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Cold starts&lt;/cell&gt;
        &lt;cell&gt;‚ö° Extremely fast&lt;/cell&gt;
        &lt;cell&gt;‚è≥ Medium (V8 isolates)&lt;/cell&gt;
        &lt;cell&gt;‚è≥ Slow&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Code changes required&lt;/cell&gt;
        &lt;cell&gt;‚úÖ None&lt;/cell&gt;
        &lt;cell&gt;‚ö†Ô∏è Some&lt;/cell&gt;
        &lt;cell&gt;‚ö†Ô∏è Wrappers&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Pricing&lt;/cell&gt;
        &lt;cell&gt;üí∞ Affordable&lt;/cell&gt;
        &lt;cell&gt;üí∞ Higher&lt;/cell&gt;
        &lt;cell&gt;üí∞ Higher&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Cloudflare Workers (Python) / Pyodide&lt;/head&gt;
    &lt;quote&gt;&lt;p&gt;‚ÑπÔ∏è Most of the demos that we showcased on this article, are not runnable inside of Cloudflare:&lt;/p&gt;&lt;code&gt;ffmpeg&lt;/code&gt;,&lt;code&gt;streamlit&lt;/code&gt;,&lt;code&gt;pypandoc&lt;/code&gt;.&lt;/quote&gt;
    &lt;p&gt;Cloudflare launched Python support ~18 months ago, by using Pyodide inside workerd, their JavaScript-based Workers runtime.&lt;/p&gt;
    &lt;p&gt;While great for browser-like environments, Pyodide has trade-offs that make it less suitable server-side. Here are the limitations when running Python in Cloudflare:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;‚ùå No support for &lt;code&gt;uvloop&lt;/code&gt;,&lt;code&gt;uvicorn&lt;/code&gt;, or similar event-native frameworks (JS event loop patches break compatibility with native).&lt;/item&gt;
      &lt;item&gt;‚ùå No pthreads or multiprocessing support, you can't call subprocesses like &lt;code&gt;ffmpeg&lt;/code&gt;or&lt;code&gt;pypandoc&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;‚ùå No raw HTTP client sockets (HTTP clients are patched to use JS &lt;code&gt;fetch&lt;/code&gt;, no&lt;code&gt;libcurl&lt;/code&gt;available).&lt;/item&gt;
      &lt;item&gt;‚ùå Limited to a bundled Python version and package set.&lt;/item&gt;
      &lt;item&gt;‚è≥ Cold starts slower due to V8 isolate warmup.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Why the limitations? Cloudflare relies on Pyodide: great in-browser execution, but server-side it implies no sockets, threads, or multiprocessing. The result: convenient for lightweight browser use, but might not be the best fit for real Python workloads on the server.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;In contrast, Wasmer Edge runs real Python on WASIX unmodified, so everything "just works", with near-native speed and fast cold starts.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;Amazon Lambda&lt;/head&gt;
    &lt;p&gt;AWS Lambda doesn't natively run unmodified Python apps:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;‚ùå You need adapters (such as https://github.com/slank/awsgi or https://github.com/Kludex/mangum) for running your WSGI sites.&lt;/item&gt;
      &lt;item&gt;‚ùå WebSockets are unsupported.&lt;/item&gt;
      &lt;item&gt;‚ö†Ô∏è Setup is complex, adapters are often unmaintained.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Why the limits? AWS Lambda requires you to use their HTTP lambda handler, which can cause incompatibility into your own HTTP servers. Also, because their lambda handlers are HTTP-based, there's no easy support for WebSockets.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;In contrast, Wasmer Edge supports any Python HTTP servers without requiring any code adaptation from your side.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;Why Wasmer Edge Stands Out&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Closer to native Python than Pyodide (no JS involvement at all).&lt;/item&gt;
      &lt;item&gt;Faster cold starts and more compatibility than Cloudflare's Workers.&lt;/item&gt;
      &lt;item&gt;More compatible than AWS Lambda (no wrappers/adapters).&lt;/item&gt;
      &lt;item&gt;More affordable across the board.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;üêç It's Showtime!&lt;/head&gt;
    &lt;p&gt;Python support in Wasmer and Wasmer Edge is already available and ready to use. We have set up many Python templates to help you get started in no time.&lt;/p&gt;
    &lt;p&gt;https://wasmer.io/templates?language=python&lt;/p&gt;
    &lt;p&gt;To make things even better, we are working on a MCP server for Wasmer, so you will be able to plug Wasmer into ChatGPT or Anthropic and have your websites deploying from your vibe-coded projects. Stay tuned!&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;‚ö†Ô∏è Python in Wasmer Edge is still in Beta, so expect some rough edges if your project doesn't work out of the box‚Ä¶ if you encounter any issues, please report them so we can work on enabling your workloads on Wasmer Edge.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Create your first MCP Server in Wasmer&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Go to https://wasmer.io/templates/mcp-chatgpt-starter?intent=at_vRxJIdtPCbKe&lt;/item&gt;
      &lt;item&gt;Connect your Github account&lt;/item&gt;
      &lt;item&gt;Create a git repo from the template&lt;/item&gt;
      &lt;item&gt;Deploy and enjoy!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note: source code available here: https://github.com/wasmer-examples/python-mcp-chatgpt-starter&lt;/p&gt;
    &lt;head rend="h2"&gt;Create your first Django app&lt;/head&gt;
    &lt;p&gt;We have set up a template for using Django + Uvicorn in Wasmer Edge.&lt;/p&gt;
    &lt;p&gt;You can start using it very easily, just click Deploy: https://wasmer.io/templates/django-starter?intent=at_WK0DIkt3CeKX&lt;/p&gt;
    &lt;p&gt;Deploying a Django app will create a MySQL DB for you in Wasmer Edge (Postgres support is coming soon), run migrations and prepare everything to run your website seamlessly.&lt;/p&gt;
    &lt;p&gt;Note: source code available here: https://github.com/wasmer-examples/django-wasmer-starter&lt;/p&gt;
    &lt;p&gt;Ready to deploy your first Python app on Wasmer Edge?&lt;/p&gt;
    &lt;p&gt;Here are the best places to begin:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;üöÄ Starter Templates ‚Üí Browse Python templates&lt;/item&gt;
      &lt;item&gt;üìñ Docs &amp;amp; Examples ‚Üí Wasmer GitHub&lt;/item&gt;
      &lt;item&gt;üí¨ Community Support ‚Üí Join our Discord&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;üëâ Deploy your first Python app now&lt;/p&gt;
    &lt;p&gt;With WebAssembly and Wasmer, Python is now portable, sandboxed, and running at near-native speeds. Ready for AI workloads, APIs, and anything you can imagine at the edge.&lt;lb/&gt; The sky is the limit ‚ù§Ô∏è.&lt;/p&gt;
    &lt;head rend="h5"&gt;About the Author&lt;/head&gt;
    &lt;p&gt;Syrus Akbary is an enterpreneur and programmer. Specifically known for his contributions to the field of WebAssembly. He is the Founder and CEO of Wasmer, an innovative company that focuses on creating developer tools and infrastructure for running Wasm&lt;/p&gt;
    &lt;p&gt;Founder &amp;amp; CEO&lt;/p&gt;
    &lt;p&gt;How fast is it?&lt;/p&gt;
    &lt;p&gt;What it can run&lt;/p&gt;
    &lt;p&gt;Wasmer VS alternatives&lt;/p&gt;
    &lt;p&gt;Quick Comparison&lt;/p&gt;
    &lt;p&gt;Cloudflare Workers (Python) / Pyodide&lt;/p&gt;
    &lt;p&gt;Amazon Lambda&lt;/p&gt;
    &lt;p&gt;Why Wasmer Edge Stands Out&lt;/p&gt;
    &lt;p&gt;üêç It's Showtime!&lt;/p&gt;
    &lt;p&gt;Create your first MCP Server in Wasmer&lt;/p&gt;
    &lt;p&gt;Create your first Django app&lt;/p&gt;
    &lt;p&gt;Deploy your first Python site in seconds with our managed cloud solution.&lt;/p&gt;
    &lt;head rend="h5"&gt;Read more&lt;/head&gt;
    &lt;p&gt;wasmerwasmer edgerustprojectsedgeweb scraper&lt;/p&gt;
    &lt;head rend="h6"&gt;Build a Web Scraper in Rust and Deploy to Wasmer Edge&lt;/head&gt;
    &lt;p&gt;RudraAugust 14, 2023&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45362023</guid><pubDate>Wed, 24 Sep 2025 15:48:36 +0000</pubDate></item><item><title>SedonaDB: A new geospatial DataFrame library written in Rust</title><link>https://sedona.apache.org/latest/blog/2025/09/24/introducing-sedonadb-a-single-node-analytical-database-engine-with-geospatial-as-a-first-class-citizen/</link><description>&lt;doc fingerprint="3bdb989c4036eb8a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Introducing SedonaDB: A single-node analytical database engine with geospatial as a first-class citizen&lt;/head&gt;
    &lt;p&gt;The Apache Sedona community is excited to announce the initial release of SedonaDB! √∞&lt;/p&gt;
    &lt;p&gt;SedonaDB is the first open-source, single-node analytical database engine that treats spatial data as a first-class citizen. It is developed as a subproject of Apache Sedona.&lt;/p&gt;
    &lt;p&gt;Apache Sedona powers large-scale geospatial processing on distributed engines like Spark (SedonaSpark), Flink (SedonaFlink), and Snowflake (SedonaSnow). SedonaDB extends the Sedona ecosystem with a single-node engine optimized for small-to-medium data analytics, delivering the simplicity and speed that distributed systems often cannot.&lt;/p&gt;
    &lt;head rend="h2"&gt;√∞¬§ What is SedonaDB¬∂&lt;/head&gt;
    &lt;p&gt;Written in Rust, SedonaDB is lightweight, blazing fast, and spatial-native. Out of the box, it provides:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;√∞¬∫√Ø¬∏ Full support for spatial types, joins, CRS (coordinate reference systems), and functions on top of industry-standard query operations.&lt;/item&gt;
      &lt;item&gt;√¢¬° Query optimizations, indexing, and data pruning features under the hood that make spatial operations just work with high performance.&lt;/item&gt;
      &lt;item&gt;√∞ Pythonic and SQL interfaces familiar to developers, plus APIs for R and Rust.&lt;/item&gt;
      &lt;item&gt;√¢√Ø¬∏ Flexibility to run in single-machine environments on local files or data lakes.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;SedonaDB utilizes Apache Arrow and Apache DataFusion, providing everything you need from a modern, vectorized query engine. What sets it apart is the ability to process spatial workloads natively, without extensions or plugins. Installation is straightforward, and SedonaDB integrates easily into both local development and cloud pipelines, offering a consistent experience across environments.&lt;/p&gt;
    &lt;p&gt;The initial release of SedonaDB provides a comprehensive suite of geometric vector operations and seamlessly integrates with GeoArrow, GeoParquet, and GeoPandas. Future versions will support all popular spatial functions, including functions for raster data.&lt;/p&gt;
    &lt;head rend="h2"&gt;√∞ SedonaDB quickstart example¬∂&lt;/head&gt;
    &lt;p&gt;Start by installing SedonaDB:&lt;/p&gt;
    &lt;code&gt;pip install "apache-sedona[db]"
&lt;/code&gt;
    &lt;p&gt;Now instantiate the connection:&lt;/p&gt;
    &lt;code&gt;import sedona.db

sd = sedona.db.connect()
&lt;/code&gt;
    &lt;p&gt;Let's perform a spatial join using SedonaDB.&lt;/p&gt;
    &lt;p&gt;Suppose you have a &lt;code&gt;cities&lt;/code&gt; table with latitude and longitude points representing the center of each city, and a &lt;code&gt;countries&lt;/code&gt; table with a column containing a polygon of the country's geographic boundaries.&lt;/p&gt;
    &lt;p&gt;Here are a few rows from the &lt;code&gt;cities&lt;/code&gt; table:&lt;/p&gt;
    &lt;code&gt;√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¨√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢
√¢     name     √¢            geometry           √¢
√¢   utf8view   √¢      geometry &amp;lt;epsg:4326&amp;gt;     √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬™√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬°
√¢ Vatican City √¢ POINT(12.4533865 41.9032822)  √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬§
√¢ San Marino   √¢ POINT(12.4417702 43.9360958)  √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬§
√¢ Vaduz        √¢ POINT(9.5166695 47.1337238)   √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬§
&lt;/code&gt;
    &lt;p&gt;And here are a few rows from the countries table:&lt;/p&gt;
    &lt;code&gt;√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¨√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¨√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢
√¢             name            √¢   continent   √¢                      geometry                      √¢
√¢           utf8view          √¢    utf8view   √¢                geometry &amp;lt;epsg:4326&amp;gt;                √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬™√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬™√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬°
√¢ Fiji                        √¢ Oceania       √¢ MULTIPOLYGON(((180 -16.067132663642447,180 -16.55√¢¬¶ √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬§
√¢ United Republic of Tanzania √¢ Africa        √¢ POLYGON((33.90371119710453 -0.9500000000000001,34√¢¬¶ √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬§
√¢ Western Sahara              √¢ Africa        √¢ POLYGON((-8.665589565454809 27.656425889592356,-8√¢¬¶ √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬§
&lt;/code&gt;
    &lt;p&gt;Here√¢s how to perform a spatial join to compute the country of each city:&lt;/p&gt;
    &lt;code&gt;sd.sql(
    """
select
    cities.name as city_name,
    countries.name as country_name,
    continent
from cities
join countries
where ST_Intersects(cities.geometry, countries.geometry)
"""
).show(3)
&lt;/code&gt;
    &lt;p&gt;The code utilizes &lt;code&gt;ST_Intersects&lt;/code&gt; to determine if a city is contained within a given country.&lt;/p&gt;
    &lt;p&gt;Here's the result of the query:&lt;/p&gt;
    &lt;code&gt;√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¨√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¨√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢
√¢   city_name   √¢         country_name        √¢ continent √¢
√¢    utf8view   √¢           utf8view          √¢  utf8view √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬™√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬™√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬°
√¢ Suva          √¢ Fiji                        √¢ Oceania   √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬§
√¢ Dodoma        √¢ United Republic of Tanzania √¢ Africa    √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬§
√¢ Dar es Salaam √¢ United Republic of Tanzania √¢ Africa    √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¥√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¥√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢
&lt;/code&gt;
    &lt;p&gt;The example above performs a point-in-polygon join, mapping city locations (points) to the countries they fall within (polygons). SedonaDB executes these joins efficiently by leveraging spatial indices where beneficial and dynamically adapting join strategies at runtime using input data samples. While many general-purpose engines struggle with the performance of such operations, SedonaDB is purpose-built for spatial workloads and delivers consistently fast results.&lt;/p&gt;
    &lt;head rend="h2"&gt;√∞ Apache Sedona SpatialBench¬∂&lt;/head&gt;
    &lt;p&gt;To test our work on SedonaDB, we also needed to develop a mechanism to evaluate its performance and speed. This led us to develop Apache Sedona SpatialBench, a benchmark for assessing geospatial SQL analytics query performance across database systems.&lt;/p&gt;
    &lt;p&gt;Let's compare the performance of SedonaDB vs. GeoPandas and DuckDB Spatial for some representative spatial queries as defined in SpatialBench.&lt;/p&gt;
    &lt;p&gt;Here are the results from SpatialBench v0.1 for Queries 1√¢12 at scale factor 1 (SF1) and scale factor 10 (SF10).&lt;/p&gt;
    &lt;p&gt;SedonaDB demonstrates balanced performance across all query types and scales effectively to SF 10. DuckDB excels at spatial filters and some geometric operations but faces challenges with complex joins and KNN queries. GeoPandas, while popular in the Python ecosystem, requires manual optimization and parallelization to handle larger datasets effectively. An in-depth performance analysis can be found in the SpatialBench website.&lt;/p&gt;
    &lt;p&gt;Here√¢s an example of the SpatialBench Query #8 that works for SedonaDB and DuckDB:&lt;/p&gt;
    &lt;code&gt;SELECT b.b_buildingkey, b.b_name, COUNT(*) AS nearby_pickup_count
FROM trip t JOIN building b ON ST_DWithin(ST_GeomFromWKB(t.t_pickuploc), ST_GeomFromWKB(b.b_boundary), 0.0045) -- ~500m
GROUP BY b.b_buildingkey, b.b_name
ORDER BY nearby_pickup_count DESC
&lt;/code&gt;
    &lt;p&gt;This query intentionally performs a distance-based spatial join between points and polygons, followed by an aggregation of the results.&lt;/p&gt;
    &lt;p&gt;Here's what the query returns:&lt;/p&gt;
    &lt;code&gt;√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¨√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¨√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢
√¢ b_buildingkey √¢  b_name  √¢ nearby_pickup_count √¢
√¢     int64     √¢ utf8view √¢        int64        √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬™√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬™√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬°
√¢          3779 √¢ linen    √¢                  42 √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬§
√¢         19135 √¢ misty    √¢                  36 √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬§
√¢          4416 √¢ sienna   √¢                  26 √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¥√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¥√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢
&lt;/code&gt;
    &lt;p&gt;Here√¢s the equivalent GeoPandas code:&lt;/p&gt;
    &lt;code&gt;trips_df = pd.read_parquet(data_paths["trip"])
trips_df["pickup_geom"] = gpd.GeoSeries.from_wkb(
    trips_df["t_pickuploc"], crs="EPSG:4326"
)
pickups_gdf = gpd.GeoDataFrame(trips_df, geometry="pickup_geom", crs="EPSG:4326")

buildings_df = pd.read_parquet(data_paths["building"])
buildings_df["boundary_geom"] = gpd.GeoSeries.from_wkb(
    buildings_df["b_boundary"], crs="EPSG:4326"
)
buildings_gdf = gpd.GeoDataFrame(
    buildings_df, geometry="boundary_geom", crs="EPSG:4326"
)

threshold = 0.0045  # degrees (~500m)
result = (
    buildings_gdf.sjoin(pickups_gdf, predicate="dwithin", distance=threshold)
    .groupby(["b_buildingkey", "b_name"], as_index=False)
    .size()
    .rename(columns={"size": "nearby_pickup_count"})
    .sort_values(["nearby_pickup_count", "b_buildingkey"], ascending=[False, True])
    .reset_index(drop=True)
)
&lt;/code&gt;
    &lt;head rend="h2"&gt;√∞¬∫√Ø¬∏ SedonaDB CRS management¬∂&lt;/head&gt;
    &lt;p&gt;SedonaDB manages the CRS when reading/writing files, as well as in DataFrames, making your pipelines safer and saving you from manual work.&lt;/p&gt;
    &lt;p&gt;Let's compute the number of buildings in the state of Vermont to highlight the CRS management features embedded in SedonaDB.&lt;/p&gt;
    &lt;p&gt;Start by reading in a FlatGeobuf file that uses the EPSG 32618 CRS with GeoPandas and then convert it to a SedonaDB DataFrame:&lt;/p&gt;
    &lt;code&gt;import geopandas as gpd

path = "https://raw.githubusercontent.com/geoarrow/geoarrow-data/v0.2.0/example-crs/files/example-crs_vermont-utm.fgb"
gdf = gpd.read_file(path)
vermont = sd.create_data_frame(gdf)
&lt;/code&gt;
    &lt;p&gt;Let√¢s check the schema of the &lt;code&gt;vermont&lt;/code&gt; DataFrame:&lt;/p&gt;
    &lt;code&gt;vermont.schema

SedonaSchema with 1 field:
  geometry: wkb &amp;lt;epsg:32618&amp;gt;
&lt;/code&gt;
    &lt;p&gt;We can see that the &lt;code&gt;vermont&lt;/code&gt; DataFrame maintains the CRS that√¢s specified in the FlatGeobuf file.  SedonaDB doesn√¢t have a native FlatGeobuf reader yet, but it√¢s easy to use the GeoPandas FlatGeobuf reader and then convert it to a SedonaDB DataFrame with a single line of code.&lt;/p&gt;
    &lt;p&gt;Now read a GeoParquet file into a SedonaDB DataFrame.&lt;/p&gt;
    &lt;code&gt;buildings = sd.read_parquet(
    "https://github.com/geoarrow/geoarrow-data/releases/download/v0.2.0/microsoft-buildings_point_geo.parquet"
)
&lt;/code&gt;
    &lt;p&gt;Check the schema of the DataFrame:&lt;/p&gt;
    &lt;code&gt;buildings.schema

SedonaSchema with 1 field:
  geometry: geometry &amp;lt;ogc:crs84&amp;gt;
&lt;/code&gt;
    &lt;p&gt;Let√¢s expose these two tables as views and run a spatial join to see how many buildings are in Vermont:&lt;/p&gt;
    &lt;code&gt;buildings.to_view("buildings", overwrite=True)
vermont.to_view("vermont", overwrite=True)

sd.sql(
    """
select count(*) from buildings
join vermont
where ST_Intersects(buildings.geometry, vermont.geometry)
"""
).show()
&lt;/code&gt;
    &lt;p&gt;This command correctly errors out because the tables have different CRSs. For safety, SedonaDB errors out rather than give you the wrong answer! Here's the error message that's easy to debug:&lt;/p&gt;
    &lt;code&gt;SedonaError: type_coercion
caused by
Error during planning: Mismatched CRS arguments: ogc:crs84 vs epsg:32618
Use ST_Transform() or ST_SetSRID() to ensure arguments are compatible.
&lt;/code&gt;
    &lt;p&gt;Let√¢s rewrite the spatial join to convert the &lt;code&gt;vermont&lt;/code&gt; CRS to EPSG:4326, so it√¢s compatible with the &lt;code&gt;buildings&lt;/code&gt; CRS.&lt;/p&gt;
    &lt;code&gt;sd.sql(
    """
select count(*) from buildings
join vermont
where ST_Intersects(buildings.geometry, ST_Transform(vermont.geometry, 'EPSG:4326'))
"""
).show()
&lt;/code&gt;
    &lt;p&gt;We now get the correct result!&lt;/p&gt;
    &lt;code&gt;√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢
√¢ count(*) √¢
√¢   int64  √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬°
√¢   361856 √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢
&lt;/code&gt;
    &lt;p&gt;SedonaDB tracks the CRS when reading/writing files, converting to/from GeoPandas DataFrames, or when performing DataFrame operations, so your spatial computations run safely and correctly!&lt;/p&gt;
    &lt;head rend="h2"&gt;√∞¬Ø Realistic example with SedonaDB¬∂&lt;/head&gt;
    &lt;p&gt;Let's now turn our attention to a KNN join, which is a more complex spatial operation.&lt;/p&gt;
    &lt;p&gt;Suppose you're analyzing ride-sharing data and want to identify which buildings are most commonly near pickup points, helping understand the relationship between trip origins and nearby landmarks, businesses, or residential structures that might influence ride demand patterns.&lt;/p&gt;
    &lt;p&gt;This query finds the five closest buildings to each trip pickup location using spatial nearest neighbor analysis. For every trip, it identifies the five buildings that are geographically closest to where the passenger was picked up and calculates the exact distance to each of those buildings.&lt;/p&gt;
    &lt;p&gt;Here√¢s the query:&lt;/p&gt;
    &lt;code&gt;WITH trip_with_geom AS (
    SELECT t_tripkey, t_pickuploc, ST_GeomFromWKB(t_pickuploc) as pickup_geom
    FROM trip
),
building_with_geom AS (
    SELECT b_buildingkey, b_name, b_boundary, ST_GeomFromWKB(b_boundary) as boundary_geom
    FROM building
)
SELECT
    t.t_tripkey,
    t.t_pickuploc,
    b.b_buildingkey,
    b.b_name AS building_name,
    ST_Distance(t.pickup_geom, b.boundary_geom) AS distance_to_building
FROM trip_with_geom t JOIN building_with_geom b
ON ST_KNN(t.pickup_geom, b.boundary_geom, 5, FALSE)
ORDER BY distance_to_building ASC, b.b_buildingkey ASC
&lt;/code&gt;
    &lt;p&gt;Here are the results of the query:&lt;/p&gt;
    &lt;code&gt;√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¨√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¨√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¨√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¨√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢
√¢ t_tripkey √¢          t_pickuploc          √¢ b_buildingkey √¢ building_name √¢ distance_to_building √¢
√¢   int64   √¢             binary            √¢     int64     √¢      utf8     √¢        float64       √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬™√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬™√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬™√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬™√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬°
√¢   5854027 √¢ 01010000001afa27b85825504001√¢¬¶ √¢            79 √¢ gainsboro     √¢                  0.0 √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬§
√¢   3326828 √¢ 01010000001bfcc5b8b7a95d4083√¢¬¶ √¢           466 √¢ deep          √¢                  0.0 √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬§
√¢   1239844 √¢ 0101000000ce471770d6ce2a40f9√¢¬¶ √¢           618 √¢ ivory         √¢                  0.0 √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¥√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¥√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¥√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¥√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢
&lt;/code&gt;
    &lt;p&gt;This is one of the queries from SpatialBench.&lt;/p&gt;
    &lt;head rend="h2"&gt;√∞¬¶ Why SedonaDB was built in Rust¬∂&lt;/head&gt;
    &lt;p&gt;SedonaDB is built in Rust, a high-performance, memory-safe language that offers fine-grained memory management and a mature ecosystem of data libraries. It takes full advantage of this ecosystem by integrating with projects such as Apache DataFusion, GeoArrow, and georust/geo.&lt;/p&gt;
    &lt;p&gt;While Spark provides extension points that let SedonaSpark optimize spatial queries in distributed settings, DataFusion offers stable APIs for pruning, spatial operators, and optimizer rules on a single node. This enabled us to embed deep spatial awareness into the engine while preserving full non-spatial functionality. Thanks to the DataFusion project and community, the experience was both possible and enjoyable.&lt;/p&gt;
    &lt;head rend="h2"&gt;√¢√Ø¬∏ Why SedonaDB and SedonaSpark are Both Needed¬∂&lt;/head&gt;
    &lt;p&gt;SedonaSpark is well-suited for large-scale geospatial workloads or environments where Spark is already part of your production stack. For instance, joining a 100 GB vector dataset with a large raster dataset. For smaller datasets, however, Spark's distributed architecture can introduce unnecessary overhead, making it slower to run locally, harder to install, and more difficult to tune.&lt;/p&gt;
    &lt;p&gt;SedonaDB is better for smaller datasets and when running computations locally. The SedonaDB spatial functions are compatible with the SedonaSpark functions, so SQL chunks that work for one engine will usually work for the other. Over time, we will ensure that both project APIs are fully interoperable. Here's an example of a chunk to analyze the Overture buildings table that works for both engines.&lt;/p&gt;
    &lt;code&gt;nyc_bbox_wkt = (
    "POLYGON((-74.2591 40.4774, -74.2591 40.9176, -73.7004 40.9176, -73.7004 40.4774, -74.2591 40.4774))"
)

sd.sql(f"""
SELECT
    id,
    height,
    num_floors,
    roof_shape,
    ST_Centroid(geometry) as centroid
FROM
    buildings
WHERE
    is_underground = FALSE
    AND height IS NOT NULL
    AND height &amp;gt; 20
    AND ST_Intersects(geometry, ST_SetSRID(ST_GeomFromText('{nyc_bbox_wkt}'), 4326))
LIMIT 5;
&lt;/code&gt;
    &lt;head rend="h2"&gt;√∞ Next steps¬∂&lt;/head&gt;
    &lt;p&gt;While SedonaDB is well-tested and provides a core set of features that can perform numerous spatial analyses, it remains an early-stage project with multiple opportunities for new features.&lt;/p&gt;
    &lt;p&gt;Many more ST functions are required. Some are relatively straightforward, but others are complex.&lt;/p&gt;
    &lt;p&gt;The community will add built-in support for other spatial file formats, such as GeoPackage and GeoJSON, to SedonaDB. You can read data in these formats into GeoPandas DataFrames and convert them to SedonaDB DataFrames in the meantime.&lt;/p&gt;
    &lt;p&gt;Raster support is also on the roadmap, which is a complex undertaking, so it's an excellent opportunity to contribute if you're interested in solving challenging problems with Rust.&lt;/p&gt;
    &lt;p&gt;Refer to the SedonaDB v0.2 milestone for more details on the specific tasks outlined for the next release. Additionally, feel free to create issues, comment on the Discord, or start GitHub discussions to brainstorm new features.&lt;/p&gt;
    &lt;head rend="h2"&gt;√∞¬§ Join the community¬∂&lt;/head&gt;
    &lt;p&gt;The Apache Sedona community has an active Discord community, monthly user meetings, and regular contributor meetings.&lt;/p&gt;
    &lt;p&gt;SedonaDB welcomes contributions from the community. Feel free to request to take ownership of an issue, and we will be happy to assign it to you. You're also welcome to join the contributor meetings, and the other active contributors will be glad to help you get your pull request over the finish line!&lt;/p&gt;
    &lt;p&gt;Info&lt;/p&gt;
    &lt;p&gt;We√¢re celebrating the launch of SedonaDB &amp;amp; SpatialBench with a special Apache Sedona Community Office Hour!&lt;/p&gt;
    &lt;p&gt;√∞ October 7, 2025&lt;/p&gt;
    &lt;p&gt;√¢¬∞ 8√¢9 AM Pacific Time&lt;/p&gt;
    &lt;p&gt;√∞ Online&lt;/p&gt;
    &lt;p&gt;√∞ Sign up here&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45362206</guid><pubDate>Wed, 24 Sep 2025 16:00:45 +0000</pubDate></item><item><title>New bacteria, and two potential antibiotics, discovered in soil</title><link>https://www.rockefeller.edu/news/38239-hundreds-of-new-bacteria-and-two-potential-antibiotics-found-in-soil/</link><description>&lt;doc fingerprint="7c5460762d250ab9"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Hundreds of new bacteria, and two potential antibiotics, found in soil&lt;/head&gt;
    &lt;p&gt;Most bacteria cannot be cultured in the lab‚Äîand that‚Äôs been bad news for medicine. Many of our frontline antibiotics originated from microbes, yet as antibiotic resistance spreads and drug pipelines run dry, the soil beneath our feet has a vast hidden reservoir of untapped lifesaving compounds.&lt;/p&gt;
    &lt;p&gt;Now, researchers have developed a way to access this microbial goldmine. Their approach, published in Nature Biotechnology, circumvents the need to grow bacteria in the lab by extracting very large DNA fragments directly from soil to piece together the genomes of previously hidden microbes, and then mines resulting genomes for bioactive molecules.&lt;/p&gt;
    &lt;p&gt;From a single forest sample, the team generated hundreds of complete bacterial genomes never seen before, as well as two new antibiotic leads. The findings offer a scalable way to scour unculturable bacteria for new drug leads‚Äîand expose the vast, uncharted microbial frontier that shapes our environment.&lt;/p&gt;
    &lt;p&gt;‚ÄúWe finally have the technology to see the microbial world that have been previously inaccessible to humans,‚Äù says Sean F. Brady, head of the Laboratory of Genetically Encoded Small Molecules at Rockefeller. ‚ÄúAnd we‚Äôre not just seeing this information; we‚Äôre already turning it into potentially useful antibiotics. This is just the tip of the spear.‚Äù&lt;/p&gt;
    &lt;p&gt;Microbial dark matter&lt;/p&gt;
    &lt;p&gt;When hunting for bacteria, soil is an obvious choice. It‚Äôs the largest, most biodiverse reservoir of bacteria on the planet‚Äîa single teaspoon of it may contain thousands of different species. Many important therapeutics, including most of our antibiotic arsenal, were discovered in the tiny fraction of soil bacteria that can be grown in the laboratory. And soil is dirt cheap.&lt;/p&gt;
    &lt;p&gt;Yet we know very little about the millions of microbes packed into the earth. Scientists suspect that these hidden bacteria hold not only an untapped reservoir of new therapeutics, but clues as to how microbes shape climate, agriculture, and the larger environment that we live in. ‚ÄúAll over the world there‚Äôs this hidden ecosystem of microbes that could have dramatic effects on our lives,‚Äù Brady adds. ‚ÄúWe wanted to finally see them.‚Äù&lt;/p&gt;
    &lt;p&gt;Getting that glimpse involved weaving together several approaches. First, the team optimized a method for isolating large, high-quality DNA fragments directly from soil. Pairing this advance with emerging long-read nanopore sequencing allowed Jan Burian, a postdoctoral associate in the Brady lab, to produce continuous stretches of DNA that were tens of thousands of base pairs long‚Äî200 times longer than any previously existing technology could manage. Soil DNA contains a huge number of different bacteria; without such large DNA sequences to work with, resolving that complex genetic puzzle into complete and contiguous genomes for disparate bacteria proved exceedingly difficult.&lt;/p&gt;
    &lt;p&gt;‚ÄúIt‚Äôs easier to assemble a whole genome out of bigger pieces of DNA, rather than the millions of tiny snippets that were available before,‚Äù Brady says. ‚ÄúAnd that makes a dramatic difference in your confidence in your results.‚Äù&lt;/p&gt;
    &lt;p&gt;Unique small molecules, like antibiotics, that bacteria produce are called ‚Äúnatural products‚Äù. To convert the newly uncovered sequences into bioactive molecules, the team applied a synthetic bioinformatic natural products (synBNP) approach. They bioinformatically predicted the chemical structures of natural products directly from the genome data and then chemically synthesized them in the lab. With the synBNP approach, Brady and colleagues managed to turn the genetic blueprints from uncultured bacteria into actual molecules‚Äîincluding two potent antibiotics.&lt;/p&gt;
    &lt;p&gt;Brady describes the method, which is scalable and can be adapted to virtually any metagenomic space beyond soil, as a three-step strategy that could kick off a new era of microbiology: ‚ÄúIsolate big DNA, sequence it, and computationally convert it into something useful.‚Äù&lt;/p&gt;
    &lt;p&gt;Two new drug candidates, and counting&lt;/p&gt;
    &lt;p&gt;Applied to their single forest soil sample, the team‚Äôs approach produced 2.5 terabase-pairs of sequence data‚Äîthe deepest long-read exploration of a single soil sample to date. Their analysis uncovered hundreds of complete contiguous bacterial genomes, more than 99 percent of which were entirely new to science and identified members from 16 major branches of the bacterial family tree.&lt;/p&gt;
    &lt;p&gt;The two lead compounds discovered could translate into potent antibiotics. One, called erutacidin, disrupts bacterial membranes through an uncommon interaction with the lipid cardiolipin and is effective against even the most challenging drug-resistant bacteria. The other, trigintamicin, acts on a protein-unfolding motor known as ClpX, a rare antibacterial target.&lt;/p&gt;
    &lt;p&gt;Brady emphasizes that these discoveries are only the beginning. The study demonstrates that previously inaccessible microbial genomes can now be decoded and mined for bioactive molecules at scale without culturing the organisms. Unlocking the genetic potential of microbial dark matter may also provide new insights into the hidden microbial networks that sustain ecosystems.&lt;/p&gt;
    &lt;p&gt;‚ÄúWe‚Äôre mainly interested in small molecules as therapeutics, but there are applications beyond medicine,‚Äù Burian says. ‚ÄúStudying culturable bacteria led to advances that helped shape the modern world and finally seeing and accessing the uncultured majority will drive a new generation of discovery.‚Äù&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45362254</guid><pubDate>Wed, 24 Sep 2025 16:03:42 +0000</pubDate></item><item><title>Terence Tao: The role of small organizations in society has shrunk significantly</title><link>https://mathstodon.xyz/@tao/115259943398316677</link><description>&lt;doc fingerprint="f8eb8f2f2d953eed"&gt;
  &lt;main&gt;
    &lt;p&gt;To use the Mastodon web application, please enable JavaScript. Alternatively, try one of the native apps for Mastodon for your platform.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45362697</guid><pubDate>Wed, 24 Sep 2025 16:32:24 +0000</pubDate></item><item><title>Launch HN: Flywheel (YC S25) ‚Äì Waymo for Excavators</title><link>https://news.ycombinator.com/item?id=45362914</link><description>&lt;doc fingerprint="2c7449d6851a3652"&gt;
  &lt;main&gt;
    &lt;div&gt;&lt;p&gt;Hey HN, We're Jash and Mahimana, cofounders of Flywheel AI (&lt;/p&gt;https://useflywheel.ai&lt;p&gt;). We‚Äôre building a remote teleop and autonomous stack for excavators.&lt;/p&gt;&lt;p&gt;Here's a video: https://www.youtube.com/watch?v=zCNmNm3lQGk.&lt;/p&gt;&lt;p&gt;Interfacing with existing excavators for enabling remote teleop (or autonomy) is hard. Unlike cars which use drive-by-wire technology, most of the millions of excavators are fully hydraulic machines. The joysticks are connected to a pilot hydraulic circuit, which proportionally moves the cylinders in the main hydraulic circuit which ultimately moves the excavator joints. This means excavators mostly do not have an electronic component to control the joints. We solve this by mechanically actuating the joysticks and pedals inside the excavators.&lt;/p&gt;&lt;p&gt;We do this with retrofits which work on any excavator model/make, enabling us to augment existing machines. By enabling remote teleoperation, we are able to increase site safety, productivity and also cost efficiency.&lt;/p&gt;&lt;p&gt;Teleoperation by the operators enables us to prepare training data for autonomy. In robotics, training data comprises observation and action. While images and videos are abundant on the internet, egocentric (PoV) observation and action data is extremely scarce, and it is this scarcity that is holding back scaling robot learning policies.&lt;/p&gt;&lt;p&gt;Flywheel solves this by preparing the training data coming from our remote teleop-enabled excavators which we have already deployed. And we do this with very minimal hardware setup and resources.&lt;/p&gt;&lt;p&gt;During our time in YC, we did 25-30 iterations of sensor stack and placement permutations/combinations, and model hyperparams variations. We called this ‚Äúevolution of the physical form of our retrofit‚Äù. Eventually, we landed on our current evolution and have successfully been able to train some levels of autonomy with only a few hours of training data.&lt;/p&gt;&lt;p&gt;The big takeaway was how much more important data is than optimizing hyperparams of the model. So today, we‚Äôre open sourcing 100hrs of excavator dataset that we collected using Flywheel systems on real construction sites. This is in partnership with Frodobots.ai.&lt;/p&gt;&lt;p&gt;Dataset: https://huggingface.co/datasets/FlywheelAI/excavator-dataset&lt;/p&gt;&lt;p&gt;Machine/retrofit details:&lt;/p&gt;&lt;quote&gt;&lt;code&gt;  Volvo EC380 (38 ton excavator)
  4xcamera (25fps)
  25 hz expert operator‚Äôs action data
&lt;/code&gt;&lt;/quote&gt;&lt;p&gt; The dataset contains observation data from 4 cameras and operator's expert action data which can be used to train imitation learning models to run an excavator autonomously for the workflows in those demonstrations, like digging and dumping. We were able to train a small autonomy model for bucket pick and place on Kubota U17 from just 6-7 hours of data collected during YC.&lt;/p&gt;&lt;p&gt;We‚Äôre just getting started. We have good amounts of variations in daylight, weather, tasks, and would be adding more hours of data and also converting to lerobot format soon. We‚Äôre doing this so people like you and me can try out training models on real world data which is very, very hard to get.&lt;/p&gt;&lt;p&gt;So please checkout the dataset here and feel free to download and use however you like. We would love for people to do things with it! I‚Äôll be around in the thread and look forward to comments and feedback from the community!&lt;/p&gt;&lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45362914</guid><pubDate>Wed, 24 Sep 2025 16:48:27 +0000</pubDate></item><item><title>How fast is Go? Simulating particles on a smart TV</title><link>https://dgerrells.com/blog/how-fast-is-go-simulating-millions-of-particles-on-a-smart-tv</link><description>&lt;doc fingerprint="66bcdd582b200d99"&gt;
  &lt;main&gt;
    &lt;p&gt;The challenge, simulate millions of particles in golang, multi-player enabled, cpu only, smart tv compatible.&lt;/p&gt;
    &lt;p&gt;Let's go, pun very much intended.&lt;/p&gt;
    &lt;p&gt;So, during my day job I had to write a ws server which merged multiple upstream ws servers into a single ws server. (Don't ask) I was lost and not even the power of claude, gemini, and cursor could save me. The vibes were simply not enough to get the project done. I had to learn the real real stuff.&lt;/p&gt;
    &lt;p&gt;To learn the real stuff I decided to write a particle simulation and see how many particles I could get golang to push. I had already done this in the lands of javascript, rust, and swift. But given Go doesn't support simd, I knew it wouldn't be a fair fight against the other languages and more importantly, it would be boring let alone helping my day job.&lt;/p&gt;
    &lt;p&gt;I figured to give go the best shake I needed to up the difficulty by adding multi-player to the simulation. After all, go is best known as a snappy productive backend language. Is it snappy and productive enough for simulating a million particles and syncing it across hundreds of clients?&lt;/p&gt;
    &lt;p&gt;Only one way to find out.&lt;/p&gt;
    &lt;p&gt;There is a rule and an important one. No client simulation allowed only server. The client should be a simple web page. It should work anywhere browser runs.&lt;/p&gt;
    &lt;p&gt;Determinism. This is a key word in computer science and it is here too. If you start with the same initial state and apply the same input, you will always get the same result. Predictable and reproducible.&lt;/p&gt;
    &lt;p&gt;Many multi-player games use determinism to effectively decouple the relationship of bigger game states from the data the server must sync across clients.&lt;/p&gt;
    &lt;p&gt;Real-time strategy games are a poster child for this. Rather than send the positions of thousands of units, projectiles, unit health, etc to clients, you would send only player input data which the client can use to derive the game state at a given point in time. Add a little bit of client side prediction, rollback, and the like and you get a smooth game experience, usually.&lt;/p&gt;
    &lt;p&gt;But this requires the client to be able to simulate the game and not all clients are fast. How can this run everywhere a browser does if I want to simulate millions of particles?&lt;/p&gt;
    &lt;p&gt;I know what the ol'gafferongames would say right now. Even if I didn't use determinism I'd still need to simulate SOMETHING on the client. If I want to send millions of particle positions to the client, surely I'd need to send at least the positions right? I could derive the velocities and quantize the crap out of everything too. I am sure there are other tricks I don't remember from his excellent blog series on game state networking code.&lt;/p&gt;
    &lt;p&gt;I still call that cheating too as the client has to simulate something and it may not scale to millions of particles.&lt;/p&gt;
    &lt;p&gt;I have another idea. I can decouple the simulation size much like determinism does by taking a hint from graphics programming.&lt;/p&gt;
    &lt;p&gt;Way back when in the days of Doom 3 when Mr. Carmack was in his prime, games would calculate lighting based on building shadow geometry from polygons. This was done in Doom 3 and looked amazing. However, as you increased polygon count, the cost of shadows would go up.&lt;/p&gt;
    &lt;p&gt;The industry then figured out how to decouple polygon count from lighting by using a graphics buffer and deferred shading. The details are heavy but the important part is that the cost of lighting is no longer proportional to number of polygons in the scene. Instead, it is based on a ‚Äúfixed‚Äù g-buffer size. The buffer is proportional to the render resolution.&lt;/p&gt;
    &lt;p&gt;This is why 4k is so expensive to render and the games industry has stoked on AI upscaling and frame interpolation. Fewer pixels means faster rendering. Geometry is making a come back with slick virtualization too but I digress. The important part is that deferred shading decouples polygons count from lighting.&lt;/p&gt;
    &lt;p&gt;Well, what if, I went full SSR and did all the rendering on the server sending the simulation frames to the clients? All a client needs to do is just, play a video onto an html canvas element. I would still need to send the input to the server but the size of the simulation wouldn't matter. The cost would be fixed to the resolution of the client.&lt;/p&gt;
    &lt;p&gt;I could have a billion particles and the data the client needs would remain the same. Now, if there were only a few thousand particles it wouldn't be worth the trade off.&lt;/p&gt;
    &lt;p&gt;How do I know it is worth it though?&lt;/p&gt;
    &lt;p&gt;I want to simulate 1 million particles at HD resolution so 1920x1080 pixels. There are 4 bytes per pixel rgba but I only need rgb. Actually, I only need one byte per pixel since that is how the previous particle simulations in the other languages worked. That means I am sending 2,073,600 bytes per frame or a little over 2mb. Oof. That is 120mb/s at 60 fps.&lt;/p&gt;
    &lt;p&gt;If I were to compress the frames they could drop down to a fraction of that. H264 or H265 (if I pay) could cut that down to 260kb per frame. I could also send only 24 frames/s making it pretty close to streaming regular video content.&lt;/p&gt;
    &lt;p&gt;Note were are talking full bytes here not bits.&lt;/p&gt;
    &lt;p&gt;If I were to send the particle data instead, each particle is made up of an x, y, dx, and dy. They are floating point numbers at 4 bytes each so a particle takes up 16 bytes. I could, derive the dx and dy so I will say 8 bytes per particle. That makes it 8,000,000 bytes or 8mb. 4x as big as the raw frame buffer. I am sure with a bit of quantization that could be brought down more and I wouldn't need to send the data 60/s but what if I want 10 million particles? The frame buffer starts to look more appealing.&lt;/p&gt;
    &lt;p&gt;The other thing that is nice here is that by sending the frames, the complexity drops. I don't need to worry about prediction or interpolation on the client. This wouldn't be great for a twitch shooter but they also don't sync millions of data points either.&lt;/p&gt;
    &lt;p&gt;No lossy compression. A particle simulation like this gets destroyed by lossy compression due to how dense the information is. There is little repitition in the data so HEVC or other lossy codecs will wreck it. Lossy compression is out of the question. It HAS to be lossless.&lt;/p&gt;
    &lt;p&gt;For example, look at this image, see all that noise? That doesn't compress well especially in motion.&lt;/p&gt;
    &lt;p&gt;Additionally, compression isn't free. If I want to scale to hundreds of clients, I cannot spend all my time compressing data. This is an important consideration.&lt;/p&gt;
    &lt;p&gt;I am going to use TCP via websockets. For low latency realtime apps UDP would be better as it allows unordered messages preventing head of line blocking. However, it is significantly more complicated and not well supported on the web. TCP's guaranteed ordering of messages will cause some lag spikes but alas it is the best the web has to offer. QUIC is a thing however it only helps the blocking when there are multiple connections which this won't have.&lt;/p&gt;
    &lt;p&gt;TCP websockets it is. Onwards!&lt;/p&gt;
    &lt;p&gt;My end game is to have a configurable play area where each client has its own view into the world. The best starting point would be to have a single view that is sent to all the clients.&lt;/p&gt;
    &lt;p&gt;The simulation itself is pretty simple. A particle struct distributing updates to many go routines. I knew that I couldn't write to a buffer while sending it to a client so I setup double buffering of the frames. A &lt;code&gt;ticker&lt;/code&gt; is the way to get a game loop going though I am not a fan of it.&lt;/p&gt;
    &lt;p&gt;Here is the gist.&lt;/p&gt;
    &lt;code&gt;type Particle struct {
  x  float32
  y  float32
  dx float32
  dy float32
}

type Input struct {
  X           float32
  Y           float32
  IsTouchDown bool
}

type SimState struct {
  dt     float32
  width  uint32
  height uint32
}

// setup variables

func startSim() {
  // setup code

  for range ticker.C {
    // metadata code

    wg.Add(numThreads)
    // no locking, that is fine.
    numClients := len(clients)
    const friction = 0.99

    for i := 0; i &amp;lt; numThreads; i++ {
      go func(threadID int) {
        defer wg.Done()

        startIndex := threadID * particlesPerThread
        endIndex := startIndex + particlesPerThread

        if threadID == numThreads-1 {
          endIndex = particleCount
        }

        for p := startIndex; p &amp;lt; endIndex; p++ {
          for i := 0; i &amp;lt; numClients; i++ {
            input := inputs[i]
            if input.IsTouchDown {
              // apply gravity
            }
          }

          particles[p].x += particles[p].dx
          particles[p].y += particles[p].dy
          particles[p].dx *= friction
          particles[p].dy *= friction

          // bounce if outside bounds
        }
      }(i)
    }

    // wait for them to complete
    wg.Wait()

    framebuffer := getWriteBuffer()
    copy(framebuffer, bytes.Repeat([]byte{0}, len(framebuffer)))
    for _, p := range particles {
      // build frame buffer
    }
    swapBuffers()

    go func(data []byte) {
      // Non-blocking send: if the channel is full, the oldest frame is dropped
      select {
      case fameChannel &amp;lt;- framebuffer:
      default:
        // Channel is full, drop the frame
      }
    }(framebuffer)
  }
}
&lt;/code&gt;
    &lt;p&gt;A few notes. The simulation is basic. Particles can be pulled around by players slowing down by some friction value.&lt;/p&gt;
    &lt;p&gt;I want to avoid locking as much as possible. This means I am going to try and create a fixed amount of memory based the max number of clients I can handle then use the clients index as a fast way to access their respective data, in this case input data.&lt;/p&gt;
    &lt;p&gt;I also want to avoid writing a frame buffer per client as I know that writing to the frame buffer is not cache friendly and should only be done once. As a matter of fact, building the frame buffer is the most expensive part. I learned this from previous languages. Cache locality and all that.&lt;/p&gt;
    &lt;p&gt;One thing I learned about go is that it can be verbose. Not java levels but it is pretty lengthy.&lt;/p&gt;
    &lt;p&gt;I will skip the boiler plate code.&lt;/p&gt;
    &lt;code&gt;func main() {
  go startSim()

  http.HandleFunc("/ws", wsHandler)
  // for webpage
  http.Handle("/", http.FileServer(http.Dir("./public")))

  log.Println("Server started on :8080")
  if err := http.ListenAndServe(":8080", nil); err != nil {
    log.Fatal("ListenAndServe:", err)
  }
}
&lt;/code&gt;
    &lt;p&gt;Initially, each websocket spins up their own ticker writing the latest frame buffer to the client. This is ugly and has sync issues but as a first version it is fine.&lt;/p&gt;
    &lt;code&gt;func wsHandler(w http.ResponseWriter, r *http.Request) {
  // boiler plate

  clientsMu.Lock()
  // safely add client
  clientsMu.Unlock()

  defer func() {
    // safely clean up
  }()

  go func() {
    // wait for messages
    var input Input
    err := binary.Read(bytes.NewReader(message), binary.LittleEndian, &amp;amp;input)

    // maybe later we lock or figure out way to not need a lock in a hot path
    // this is fine as only chance of bad data is if someone connect or drops whilst updating input
    idx := findClientIndex(conn)
    if idx != -1 &amp;amp;&amp;amp; idx &amp;lt; maxClients {
      inputs[idx] = input
    }
  }()

  ticker := time.NewTicker(time.Second / 30)
  for range ticker.C {
    data := getReadBuffer()
    if err := conn.WriteMessage(websocket.BinaryMessage, data); err != nil {
      log.Println("Write failed:", err)
      break
    }
  }
}
&lt;/code&gt;
    &lt;p&gt;The client index is pretty silly but I really don't want to lock the hot path here.&lt;/p&gt;
    &lt;p&gt;I'll skip over the client javascript/html. It isn't that interesting, setup a web socket, write data to a canvas element using &lt;code&gt;setPixels&lt;/code&gt; the usually.&lt;/p&gt;
    &lt;p&gt;So, how well does it work? Pretty well. But it is not fast, and is writing a shit load of data. It also sometimes flickers when the tickers touch the frame buffer at the same time the simulation thread does.&lt;/p&gt;
    &lt;p&gt;Still, a good starting point.&lt;/p&gt;
    &lt;p&gt;I used &lt;code&gt;pprof&lt;/code&gt; to run some samples to see where things are slow. I noticed that time was spent creating go routines. While light they are not free. The idea is to have go routines listening in on channels. I refactored the simulation to wait for &lt;code&gt;SimJob&lt;/code&gt; requests to come in.&lt;/p&gt;
    &lt;code&gt;type SimJob struct {
  startIndex int
  endIndex   int
  simState   SimState
  inputs     [maxClients]Input
  numClients int
}

// other code

jobs := make(chan SimJob, numThreads)
var wg sync.WaitGroup
for i := 0; i &amp;lt; numThreads; i++ {
  go worker(jobs, &amp;amp;wg)
}

// in sim loop
for i := 0; i &amp;lt; numThreads; i++ {
  startIndex := i * particlesPerThread
  endIndex := startIndex + particlesPerThread
  if i == numThreads-1 {
    endIndex = particleCount
  }
  jobs &amp;lt;- SimJob{startIndex, endIndex, simState, &amp;amp;inputs, numClients, &amp;amp;framebuffer}
}
&lt;/code&gt;
    &lt;p&gt;I also reused frame buffers with a &lt;code&gt;pool&lt;/code&gt; and then fixed the syncing by pushing to a channel which another thread would listen on. This thread would then write the new frame out to all the clients before adding it back to the pool. At first I used a simple for loop but that made it write as fast as the slowest client so instead I had it prep the frame and then push it to ANOTHER set of channels per client. Threads on each of those channels would then actually write data to the client.&lt;/p&gt;
    &lt;code&gt;func broadcastFrames(ch &amp;lt;-chan *Frame, pool *sync.Pool) {
  for {
    frame := &amp;lt;-ch
    // setup data
    clientsMu.Lock()
    for i, conn := range clients {
      // send to other channel
      select {
      case clientSendChannelMap[conn] &amp;lt;- dataToSend:
      default:
        log.Printf("Client %d's channel is full, dropping frame. Requesting full frame.", i)
      }
    }
    clientsMu.Unlock()
    pool.Put(frame)
  }
}

// other code
func writePump(conn *websocket.Conn) {
  var channel = clientSendChannelMap[conn]
  for {
    message, ok := &amp;lt;-channel
    if !ok {
      return
    }

    if err := conn.WriteMessage(websocket.BinaryMessage, message); err != nil {
      log.Printf("Write to client failed: %v", err)
      return
    }
  }
}
&lt;/code&gt;
    &lt;p&gt;I pump when a client connects and stop when they disconnect. This works pretty well and fixes the sync issue.&lt;/p&gt;
    &lt;p&gt;I experimented with compression a bit using the standard &lt;code&gt;flate&lt;/code&gt; lib. It worked but was pretty slow. I then experimented with every kind of encoding I could think of to reduce the data transferred.&lt;/p&gt;
    &lt;p&gt;I tried a bit mask where I would sent 0/1 flags per pixel followed by the in order pixel data that changed. This was fast per client but ended up being larger than the raw frame data when particles started moving around.&lt;/p&gt;
    &lt;p&gt;I tried run length encoding, variable length encoding, and delta encoding all combined in different ways.&lt;/p&gt;
    &lt;p&gt;There are some gotchas I ran into For RLE to work you need a sentinel byte to know if there is a run of data. This drops the available space per pixel. It could be possible to encode multiple pixels at a time but that didn't work that well. VLE is just run length using varints to drop the size down for small numbers this worked and did help reduce size by almost 8% on average.&lt;/p&gt;
    &lt;p&gt;Delta encoding was the most fun. The way I tried it was to delta encode between frames. That way passing the delta into RLE would massively compress the data. The issue is that particle counts can go up or down per pixel. This required &lt;code&gt;zigzag&lt;/code&gt; encoding but there was an issue.&lt;/p&gt;
    &lt;p&gt;If each pixel is one byte, and the byte can go from 0 to 255 in a single frame, then the size of the deltas would need to go from -255 to +255 which doesn't fit in a byte. The &lt;code&gt;zigzag&lt;/code&gt; encoding worked and I figured I could just drop down to 7-bits per pixel.&lt;/p&gt;
    &lt;p&gt;I read about zigzag encoding from a microsoft paper on optimized compression for depth data from multiple connect sensors. I don't remember the name. To save you some time, just ask AI about it.&lt;/p&gt;
    &lt;code&gt;func CreateDeltaBuffer(oldBuffer, newBuffer []byte) []byte {
  if len(oldBuffer) != len(newBuffer) {
    return newBuffer
  }
  deltaBuffer := make([]byte, len(newBuffer))

  for i := 0; i &amp;lt; len(newBuffer); i++ {
    diff := int16(newBuffer[i]) - int16(oldBuffer[i])
    deltaBuffer[i] = zigZagEncode(int8(diff))
  }

  return deltaBuffer
}
&lt;/code&gt;
    &lt;p&gt;And it works. Unless the whole frame was moving around, the size was ~30% smaller than the raw frame data. If nothing moved, it would send only a few bytes! But again, there were problems.&lt;/p&gt;
    &lt;p&gt;The code is complicated. I had to track if I was sending delta frames or full frames, and if a client dropped a frame, I'd need to send a full frame again. This would cause a brief flash on the screen. I could add frame counting logic so the client could drop out of sync deltas but the code was already a bit much.&lt;/p&gt;
    &lt;code&gt;func broadcastFrames(ch &amp;lt;-chan *Frame, pool *sync.Pool) {
  for {
    frame := &amp;lt;-ch
    fullFrameBuffer.Reset()
    fullFrameBuffer.WriteByte(OpCodeFullFrame)
    fullFrameBuffer.Write(frame.FullBuffer)
    fullFrameBytes := fullFrameBuffer.Bytes()

    deltaFrameBuffer.Reset()
    deltaFrameBuffer.WriteByte(OpCodeDeltaFrame)
    deltaFrameBuffer.Write(frame.Delta)
    deltaFrameBytes := deltaFrameBuffer.Bytes()

    clientsMu.Lock()
    for i, conn := range clients {

      var dataToSend []byte
      if clientFullFrameRequestFlags[i] {
        dataToSend = fullFrameBytes
        clientFullFrameRequestFlags[i] = false
      } else {
        dataToSend = deltaFrameBytes
      }

      select {
      case clientSendChannelMap[conn] &amp;lt;- dataToSend:
      default:
        log.Printf("Client %d's channel is full, dropping frame. Requesting full frame.", i)
        clientFullFrameRequestFlags[i] = true
      }
    }
    clientsMu.Unlock()
    pool.Put(frame)
  }
}
&lt;/code&gt;
    &lt;p&gt;It does work though.&lt;/p&gt;
    &lt;p&gt;However, I want each client to have their own ‚Äúview‚Äù into a bigger frame where the can pan around the world fighting for particles. For this to work, with the current setup, I'd need to track delta frames per client and figure out how to handle dropped delta frames better.&lt;/p&gt;
    &lt;p&gt;Back to the drawing board.&lt;/p&gt;
    &lt;p&gt;Stupid simple, is simple stupid.&lt;/p&gt;
    &lt;p&gt;I am going to have 1 bit per pixel. That means only a single ‚Äúluminance‚Äù value. In the previous video I was only effectively using a few bits on the client having only about 8 ‚Äúluminance‚Äù values. Bet you thought it was more right?&lt;/p&gt;
    &lt;p&gt;This will simplify everything. The only hard part is packing and unpacking the data. I also figured I could implement client camera state too. I waffled a bit around how to store this state and update it.&lt;/p&gt;
    &lt;p&gt;Does the client sent their camera position directly? What if the window resizes? What if the client sends a negative camera size? What if they send an infinite one? How do I prevent blocking?&lt;/p&gt;
    &lt;p&gt;I ended up copying what I did for the input data.&lt;/p&gt;
    &lt;code&gt;type ClientCam struct {
  X      float32
  Y      float32
  Width  int32
  Height int32
}
var (
  inputs     [maxClients]Input
  cameras    [maxClients]ClientCam
)
&lt;/code&gt;
    &lt;p&gt;I would have the client send the camera data with the existing input message.&lt;/p&gt;
    &lt;code&gt;  for {
    mt, message, err := conn.ReadMessage()
    if err != nil {
      log.Println("Read failed:", err)
      return
    }
    if mt == websocket.BinaryMessage {
      reader := bytes.NewReader(message)
      var touchInput Input
      // interpret x and y as offsets.
      var camInput ClientCam
      // read input
      errInput := binary.Read(reader, binary.LittleEndian, &amp;amp;touchInput)
      errCam := binary.Read(reader, binary.LittleEndian, &amp;amp;camInput)
      // set client input/camera state
      // handle errors in the input, bounds, negatives etc.
    }
  }
&lt;/code&gt;
    &lt;p&gt;For sending frame data, at first, I naively packed the buffer and then unpacked it again based on the region a client was rendering.&lt;/p&gt;
    &lt;code&gt;// in simulation thread
for _, p := range particles {
  x := int(p.x)
  y := int(p.y)
  if x &amp;gt;= 0 &amp;amp;&amp;amp; x &amp;lt; int(simState.width) &amp;amp;&amp;amp; y &amp;gt;= 0 &amp;amp;&amp;amp; y &amp;lt; int(simState.height) {
    idx := (y*int(simState.width) + x)
    if idx &amp;lt; int(simState.width*simState.height) {
      byteIndex := idx / 8
      bitOffset := idx % 8
      if byteIndex &amp;lt; len(framebuffer) {
        framebuffer[byteIndex] |= (1 &amp;lt;&amp;lt; bitOffset)
      }
    }
  }
}

// in frame send channel before sending to client 
// get client camera info
for row := int32(0); row &amp;lt; height; row++ {
  for col := int32(0); col &amp;lt; width; col++ {
    mainFrameIndex := ((y+row)*int32(simState.width) + (x + col))
    dataToSendIndex := (row*width + col)

    if mainFrameIndex/8 &amp;lt; int32(len(frameBuffer)) &amp;amp;&amp;amp; dataToSendIndex/8 &amp;lt; int32(len(dataToSend)) {
      isSet := (frameBuffer[mainFrameIndex/8] &amp;gt;&amp;gt; (mainFrameIndex % 8)) &amp;amp; 1
      if isSet == 1 {
        dataToSend[dataToSendIndex/8] |= (1 &amp;lt;&amp;lt; (dataToSendIndex % 8))
      }
    }
  }
}
&lt;/code&gt;
    &lt;p&gt;This makes HD only a few hundred kb/frame.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;pprof&lt;/code&gt; has a niffy way of downloading a simple of the running up you can then load and look at with their cli tool.&lt;/p&gt;
    &lt;p&gt;I checked a sample using &lt;code&gt;pprof&lt;/code&gt; and noticed the sim was slow when it was building the frame buffer so I moved the frame building to the simulation workers. This is possible now because each thread will only mark a bit as on if there is a particle there. Which means I don't need to lock anything and can let the last writer win.&lt;/p&gt;
    &lt;p&gt;Before&lt;/p&gt;
    &lt;code&gt;Showing top 10 nodes out of 35
      flat  flat%   sum%        cum   cum%
    16.75s 52.54% 52.54%     17.05s 53.48%  main.worker
     6.42s 20.14% 72.68%      6.49s 20.36%  main.broadcastFrames
&lt;/code&gt;
    &lt;p&gt;And after&lt;/p&gt;
    &lt;code&gt;Showing top 10 nodes out of 36
      flat  flat%   sum%        cum   cum%
    13.01s 46.46% 46.46%     13.28s 47.43%  main.worker
     6.98s 24.93% 71.39%      7.08s 25.29%  main.broadcastFrames
&lt;/code&gt;
    &lt;p&gt;The simulation is a bit faster but notice that the broadcast frames is slow. You know that packing I am doing? Ya, that ended up being very very slow. 25% of the time is spent broadcasting frames to a few clients due to all the bit opps packing and unpacking.&lt;/p&gt;
    &lt;p&gt;A simple fix is to use full bytes even if the count is only ever 0-1 and then using a lookup map to skip the bit opps like so.&lt;/p&gt;
    &lt;code&gt;var uint64ToByteLUT = make(map[uint64]byte)

func init() {
  var byteSlice = make([]byte, 8)
  for i := 0; i &amp;lt; 256; i++ {
    for bit := 0; bit &amp;lt; 8; bit++ {
      if (i&amp;gt;&amp;gt;bit)&amp;amp;1 == 1 {
        byteSlice[bit] = 1
      } else {
        byteSlice[bit] = 0
      }
    }
    // trust me bro
    uint64ToByteLUT[BytesToUint64Unsafe(byteSlice)] = byte(i)
  }
}

for row := int32(0); row &amp;lt; height; row++ {
  yOffset := (y + row) * int32(simState.width)
  for col := int32(0); col &amp;lt; width; col += 8 {
    fullBufferIndex := yOffset + (x + col)
    chunk := frameBuffer[fullBufferIndex : fullBufferIndex+8]
    key := BytesToUint64Unsafe(chunk)

    packedByte, _ := uint64ToByteLUT[key]

    if outputByteIndex &amp;lt; int32(len(dataToSend)) {
      dataToSend[outputByteIndex] = packedByte
      outputByteIndex++
    }
  }
}
&lt;/code&gt;
    &lt;p&gt;Now slicing the frame region for a client is almost free. The idea is pretty simple, interpret 8 bytes at a time as a uint64 which maps to the corresponding single byte with the right bits set. I did something similar on the client too to make unpacking faster. The extra memory here is pretty small and worth it to me.&lt;/p&gt;
    &lt;p&gt;Not bad. I am sure more can be done but the main sim is taking up the majority of the time.&lt;/p&gt;
    &lt;code&gt;Showing top 10 nodes out of 49
      flat  flat%   sum%        cum   cum%
    15.62s 58.85% 58.85%     15.83s 59.65%  main.worker
     4.66s 17.56% 76.41%      4.66s 17.56%  runtime.pthread_cond_wait
     2.96s 11.15% 87.57%      2.96s 11.15%  runtime.pthread_cond_signal
     0.90s  3.39% 90.96%      0.90s  3.39%  runtime.kevent
     0.47s  1.77% 92.73%      0.80s  3.01%  runtime.mapaccess2_fast64
     0.36s  1.36% 94.08%      1.17s  4.41%  main.broadcastFrames
     0.20s  0.75% 94.84%      0.20s  0.75%  runtime.asyncPreempt
     0.17s  0.64% 95.48%      0.17s  0.64%  runtime.madvise
     0.17s  0.64% 96.12%      0.17s  0.64%  runtime.pthread_kill
     0.17s  0.64% 96.76%      0.17s  0.64%  runtime.usleep
&lt;/code&gt;
    &lt;p&gt;On the topic of memory, with millions of particles the server barely breaks over 100mb. Not rust good but node would be well over a few gigs and I cannot imagine python would be much lighter.&lt;/p&gt;
    &lt;p&gt;I waffled about again for well over a day trying to get some go assembly to work for those sweet simd gains but it wasn't to be. There are some libraries I tried too but none of them did better than what I already had. There are some interesting things with go's compiler around how to optimize out dereferences and bounds checking which did give me a 30% boost.&lt;/p&gt;
    &lt;p&gt;For example, before I was updating the particles like so&lt;/p&gt;
    &lt;code&gt;for i := job.startIndex; i &amp;lt; job.endIndex; i++ {
  particles[i].x += particles[i].dx
  // etc
&lt;/code&gt;
    &lt;p&gt;But by doing this&lt;/p&gt;
    &lt;code&gt;for i := job.startIndex; i &amp;lt; job.endIndex; i++ {
  p := &amp;amp;particles[i]
  p.x += p.dx
&lt;/code&gt;
    &lt;p&gt;Big boost. It makes sense. Bounds checking was happening way too often. I did a few more micro optimizations which helped but nothing significant. I did try SoA too and that actually did worse than AoS. Go figure.&lt;/p&gt;
    &lt;p&gt;I tried to use a fast inverse square root function but for some reason the go version of it didn't work at all which is weird because js lands had no issue. Oh well.&lt;/p&gt;
    &lt;p&gt;The previous profile samples were taken simulating a dozen or so clients with a few million particles. If I bump that up to 20m particles, I get this.&lt;/p&gt;
    &lt;code&gt;Showing top 10 nodes out of 21
      flat  flat%   sum%        cum   cum%
   113.16s 93.06% 93.06%    115.22s 94.75%  main.worker
     2.27s  1.87% 94.93%      2.27s  1.87%  runtime.pthread_cond_wait
     2.04s  1.68% 96.60%      2.04s  1.68%  runtime.asyncPreempt
     1.23s  1.01% 97.62%      1.23s  1.01%  runtime.pthread_cond_signal
     0.63s  0.52% 98.13%      1.56s  1.28%  runtime.mapaccess2_fast64
     0.30s  0.25% 98.38%      1.92s  1.58%  main.broadcastFrames
&lt;/code&gt;
    &lt;p&gt;The simulation is the clear bottle neck and without simd, I am not sure how much better I can get it.&lt;/p&gt;
    &lt;p&gt;It works though. Time to deploy it to the cloud.&lt;/p&gt;
    &lt;p&gt;I was going to use a digital ocean docklet but it would cost more than an Equinox gym membership for anything faster than a toaster. I ended up spending ~$8/month at netcup for a 16 gig 10 core arm vm Manassas, Virginia with a ping of over 300ms. Fantastic. Not sure why the DNS on their panel says germany though.&lt;/p&gt;
    &lt;p&gt;I opened a few ports under root like the psychopath I and gave it a spin.&lt;/p&gt;
    &lt;p&gt;Pretty slick. More clients barely impacts cpu usage. But will it scale to 1k clients? I am not sure.&lt;/p&gt;
    &lt;p&gt;The netcup vps has a 2.5 Gbit pipe. At a 1920x1080 resolution with 1 bit per pixel at a rate of 30 frames/s down the pipe, that should theoretically support well over 300 clients. Now, if these were mobile devices with a fraction of that resolution say 390x844 or so, (we are not talking native), it may actually scale.&lt;/p&gt;
    &lt;p&gt;It is worth noting it does suffer from head of line blocking at times especially as the resolution gets cranked up. Clients with a resolution larger than the max particle grid are ignored so your 5k display won't work.&lt;/p&gt;
    &lt;p&gt;I did a bit of polishing on the client, edge scrolling, panning, mobile support, etc but this post isn't about javascript it is about golang.&lt;/p&gt;
    &lt;p&gt;Go is able to simulate 2.5 million particles at 60fps while sending data at 30 fps to in theory over 300 clients maybe even a thousand. And because this happens on the server it even runs on a smart tv. Don't believe me? Just visit, howfastisgo.dev and see for yourself or watch the video below.&lt;/p&gt;
    &lt;p&gt;While go certainly isn't a compute workhorse, that is pretty impressive. Anywhere a browser runs this works! Go is just THAT fast.&lt;/p&gt;
    &lt;p&gt;This is just so cursed. I love it.&lt;/p&gt;
    &lt;p&gt;Until next time.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45364450</guid><pubDate>Wed, 24 Sep 2025 18:48:39 +0000</pubDate></item><item><title>SonyShell ‚Äì An effort to ‚ÄúSSH into my Sony DSLR‚Äù</title><link>https://github.com/goudvuur/sonyshell</link><description>&lt;doc fingerprint="34960c72d72e66ed"&gt;
  &lt;main&gt;
    &lt;p&gt;A Linux-only helper built on Sony‚Äôs official Camera Remote SDK. It connects to a Sony A6700 camera over Wi-Fi/Ethernet, listens for new photos, downloads them automatically, and can optionally run a script on each downloaded file.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Auto-connect via enumeration or direct IP/MAC.&lt;/item&gt;
      &lt;item&gt;Watches for new capture events and fetches the newest files.&lt;/item&gt;
      &lt;item&gt;Saves into a chosen directory with unique filenames.&lt;/item&gt;
      &lt;item&gt;Post-download hook: run any executable/script with the saved file path as argument.&lt;/item&gt;
      &lt;item&gt;Keepalive mode: auto-retry on startup failure or after disconnects.&lt;/item&gt;
      &lt;item&gt;Cleaned, Linux-only code (no Windows ifdefs, simpler logging).&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;./sony-remote --dir /photos [options]&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;--dir &amp;lt;path&amp;gt;&lt;/code&gt;: Directory to save files (required in most real setups).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--ip &amp;lt;addr&amp;gt;&lt;/code&gt;: Connect directly by IPv4 (e.g.&lt;code&gt;192.168.10.184&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--mac &amp;lt;hex:mac&amp;gt;&lt;/code&gt;: Optional MAC (e.g.&lt;code&gt;10:32:2c:2a:1a:6d&lt;/code&gt;) for direct IP.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--cmd &amp;lt;path&amp;gt;&lt;/code&gt;: Executable/script to run after each download, invoked as&lt;code&gt;cmd /photos/DSC01234.JPG&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--keepalive &amp;lt;ms&amp;gt;&lt;/code&gt;: Retry interval when offline or after disconnect.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-v&lt;/code&gt;,&lt;code&gt;--verbose&lt;/code&gt;: Verbose property-change logging.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Enumerate + keep retrying every 2s, run a hook after each file:&lt;/p&gt;
    &lt;code&gt;./sony-remote --dir /photos --keepalive 2000 --cmd /usr/local/bin/ingest-photo&lt;/code&gt;
    &lt;p&gt;Direct IP connect, verbose logs, retry every 3s:&lt;/p&gt;
    &lt;code&gt;./sony-remote --ip 192.168.10.184 --mac 10:32:2c:2a:1a:6d --dir /photos -v --keepalive 3000&lt;/code&gt;
    &lt;p&gt;Requires Linux, g++, and the Sony Camera Remote SDK.&lt;/p&gt;
    &lt;p&gt;See INSTALL.md&lt;/p&gt;
    &lt;p&gt;or (untested)&lt;/p&gt;
    &lt;code&gt;g++ -std=c++17 sony-a6700-remote-cleaned.cpp \
    -I/path/to/CrSDK/include \
    -L/path/to/CrSDK/lib -lCameraRemoteSDK \
    -lpthread -o sony-remote&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Connect to the camera (via IP or enumeration). Stores/reuses SDK fingerprint under &lt;code&gt;~/.cache/sonshell/&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Wait for notifications: when the camera signals new contents, spawn a download thread.&lt;/item&gt;
      &lt;item&gt;Download newest files to &lt;code&gt;--dir&lt;/code&gt;. Safe naming ensures no overwrite (&lt;code&gt;file_1.jpg&lt;/code&gt;, etc.).&lt;/item&gt;
      &lt;item&gt;Hook: if &lt;code&gt;--cmd&lt;/code&gt;is set, fork/exec the script with the saved path.&lt;/item&gt;
      &lt;item&gt;Reconnect on errors/disconnects if &lt;code&gt;--keepalive&lt;/code&gt;is set.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Built on/for Ubuntu 24.04&lt;/item&gt;
      &lt;item&gt;It uses Sony's official Camera Remote SDK (not included here).&lt;/item&gt;
      &lt;item&gt;See DOCS.md for a deep dive into the internals.&lt;/item&gt;
      &lt;item&gt;I leaned heavily on ChatGPT while creating this, so please don't mind the mess! ;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Sony Camera Remote SDK: https://support.d-imaging.sony.co.jp/app/sdk/en/index.html&lt;/item&gt;
      &lt;item&gt;See LICENSE for licensing details.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45365878</guid><pubDate>Wed, 24 Sep 2025 21:00:00 +0000</pubDate></item><item><title>Everything that's wrong with Google Search in one image</title><link>https://bitbytebit.substack.com/p/everything-thats-wrong-with-google</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45366566</guid><pubDate>Wed, 24 Sep 2025 22:11:48 +0000</pubDate></item><item><title>Helium Browser</title><link>https://helium.computer/</link><description>&lt;doc fingerprint="170e82509195b1e4"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Internet without interruptions&lt;/head&gt;
    &lt;p&gt;Best privacy and unbiased ad-blocking by default. Handy features like native !bangs and split view. No adware, no bloat, no noise. People-first and fully open source.&lt;/p&gt;
    &lt;head rend="h1"&gt;Best privacy by default, not as a hidden option&lt;/head&gt;
    &lt;p&gt;Helium blocks ads, trackers, fingerprinting, third-party cookies, cryptominers, and phishing websites by default thanks to preinstalled uBlock Origin. No extra steps are needed, and there are no biased exceptions √¢ unlike other browsers. &lt;lb/&gt; The browser itself doesn't have any ads, trackers, or analytics. Helium also doesn't make any web requests without your explicit consent, it makes zero web requests on first launch. &lt;lb/&gt; Not enough? Increase privacy even further with ungoogled-chromium flags or uBlock Origin filters. You're finally at the steering wheel of your privacy on the Internet √¢ not in a toy car, but in a real race car. &lt;lb/&gt; We will always stand by our promise of the best privacy and will never prioritize profit over people, unlike big corporations.&lt;/p&gt;
    &lt;head rend="h2"&gt;Respectful by design&lt;/head&gt;
    &lt;p&gt;Helium doesn't annoy you with anything and never will. It doesn't do anything without your consent: no unprovoked tabs about updates or sponsors, no persistent popups telling you about features you don't care about, no weird restarts. &lt;lb/&gt; Nothing interrupts you, jumps in your face, or breaks your flow. Everything just makes sense. You're in full control.&lt;/p&gt;
    &lt;head rend="h2"&gt;Fast, efficient, and light&lt;/head&gt;
    &lt;p&gt;Helium is based on Chromium, the fastest and most optimized browser yet. Helium builds on this base to improve performance and save even more energy. You will notice a difference after using Helium for a day. It doesn't slow down over time. &lt;lb/&gt; All bloat is removed: Helium is one of the lightest modern browsers available.&lt;/p&gt;
    &lt;head rend="h2"&gt;Powerful when you need it&lt;/head&gt;
    &lt;p&gt;Open pages side-by-side with split view to get even more things done at once. Quickly copy page links with √¢+Shift+C and share your discoveries with ease. Install any web apps and use them as standalone desktop apps without duplicating Chromium.&lt;/p&gt;
    &lt;head rend="h2"&gt;Designed to get out of your way&lt;/head&gt;
    &lt;p&gt;Helium's interface is compact and minimalistic, but it doesn't compromise on beauty or functionality. More web content fits on the screen at once, and the browser interface doesn't get in your way. You can hide everything extra from the toolbar if it annoys you. &lt;lb/&gt; Helium is built with attention to detail. Nothing jiggles or flickers abnormally. Your actions aren't throttled or stopped by lag. Everything's fast, smooth, and simple. Comfort and simplicity are among our top priorities.&lt;/p&gt;
    &lt;head rend="h2"&gt;Works with all Chromium extensions, privately&lt;/head&gt;
    &lt;p&gt;All Chromium extensions are supported and work right away, by default, including all MV2 extensions. We'll keep support for MV2 extensions for as long as possible. &lt;lb/&gt; Helium anonymizes all internal requests to the Chrome Web Store via Helium services. Thanks to this, Google can't track your extension downloads or target ads using this data. No other browser does this.&lt;/p&gt;
    &lt;head rend="h2"&gt;Free and fully open-source&lt;/head&gt;
    &lt;p&gt;All parts of the Helium browser are open source, including online services. You can self-host Helium services and use your own instance in your browser. &lt;lb/&gt; Everything is available on GitHub. No exceptions.&lt;/p&gt;
    &lt;head rend="h2"&gt;Always safe and sound&lt;/head&gt;
    &lt;p&gt;We release new Chromium updates (such as security patches) as soon as possible. Your browser will always be safe and up to date. &lt;lb/&gt; Helium updates itself automatically on macOS, with auto-updating options available on Linux and Windows. &lt;lb/&gt; All builds are available on GitHub, and you can even make one yourself. The choice is yours!&lt;/p&gt;
    &lt;head rend="h2"&gt;Best security practices for everyone, by default&lt;/head&gt;
    &lt;p&gt;Helium enforces HTTPS on all websites and warns you when a website doesn't support it. Passkeys just work. &lt;lb/&gt; There's no built-in password manager. Passwords should be separate from a web browser to be truly secure and immutable. &lt;lb/&gt; There's also no cloud-based history/data sync. You should be the only one with access to your browsing data, not some conglomerate.&lt;/p&gt;
    &lt;head rend="h1"&gt;Browse the Internet faster with !bangs&lt;/head&gt;
    &lt;p&gt;Skip the search engine and go directly to the website you want. Choose from over 13,000 bangs that make the Internet a breeze to browse, such as !w for Wikipedia, !gh for GitHub, and !wa for Wolfram Alpha. &lt;lb/&gt; Want to chat with AI? Just add !chatgpt or any other AI provider name at the start of your query. Helium will start a new chat for you without sending your prompt anywhere else. &lt;lb/&gt; Helium bangs are the fastest and most private implementation of bangs yet. They work offline, directly in your browser. &lt;lb/&gt; Not sure which bang to use? Check out the full list of bangs!&lt;/p&gt;
    &lt;head rend="h1"&gt;The web browser made for people, with love&lt;/head&gt;
    &lt;p&gt;We're making a web browser that we enjoy using ourselves. Helium's main goal is to provide an honest, comfortable, privacy-respecting, and non-invasive browsing experience.&lt;/p&gt;
    &lt;head rend="h2"&gt;Perfect for developers&lt;/head&gt;
    &lt;p&gt;Helium is based on Chromium and doesn't break any web APIs or standards, despite the focus on privacy. DevTools have been cleaned up and no longer nag you with anything. There's nothing that gets in your way of creating the Internet of the future.&lt;/p&gt;
    &lt;head rend="h2"&gt;Perfect for everyone on the go&lt;/head&gt;
    &lt;p&gt;Helium's efficiency makes it handy for everyone with their laptop on the go. Split view and quick link copying make it easier than ever to get things done faster. Helium loads pages faster and saves data by blocking ads and other crap.&lt;/p&gt;
    &lt;head rend="h1"&gt;Ready to try Helium?&lt;/head&gt;
    &lt;p&gt;It's never too late to get your internet life back on the right track. Helium can transfer your most important stuff from other browsers in one click. We hope you'll love it!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45366867</guid><pubDate>Wed, 24 Sep 2025 22:51:16 +0000</pubDate></item><item><title>Do YC after you graduate: Early decision for students</title><link>https://www.ycombinator.com/early-decision</link><description>&lt;doc fingerprint="b61d01d108cf1634"&gt;
  &lt;main&gt;
    &lt;p&gt;Apply now, do YC after you graduate. For students who want to finish school before doing YC. Get funded the moment you're accepted.&lt;/p&gt;
    &lt;p&gt;Sneha and Anushka, founders of Spur (S24), applied in Fall 2023 for the S24 batch using Early Decision. This allowed them to graduate in May 2024 and then do YC. They've since raised $4.5M from top investors for their AI-powered QA testing tools.&lt;/p&gt;
    &lt;p&gt;Early Decision lets you apply to YC while you're still in school and reserve your spot in a future batch. For example, you apply in Fall of this year, for a spot in the summer batch of the following year. You submit the same YC application as if you were applying for the upcoming batch. If you're accepted, we'll fund you immediately and hold your place for after you graduate.&lt;/p&gt;
    &lt;p&gt;This program is designed for students who want to finish their degree before starting a company. If you're considering working on your own startup after graduation, Early Decision makes it easy to lock in your spot.&lt;/p&gt;
    &lt;p&gt;Even if you're not completely sure yet if you want to do a startup, you should still apply. There is no downside.&lt;/p&gt;
    &lt;p&gt;Also, if you're not in your final year, you can still apply for Early Decision. You'll be able to finish the school year you're currently in, and then either join a later batch or decide to drop out and start sooner.&lt;/p&gt;
    &lt;p&gt;The most common path is students applying in the fall of their final year and joining the summer batch after graduating in Spring. But you can apply for any batch in the future within reason. The application and interview process is the same as if you were applying for the upcoming batch. Once you're accepted, YC funds you right away and confirms your future batch.&lt;/p&gt;
    &lt;p&gt;When you fill out your YC application, you'll see a question asking which batch you want to apply for. Simply select "A batch after Winter 2026" to indicate you're applying for Early Decision, and tell us which batch you'd like to be considered for.&lt;/p&gt;
    &lt;p&gt;The batch preference question in the YC application&lt;/p&gt;
    &lt;p&gt;Many students want to finish their degree or complete more of their education before starting a company. Also we know that many students spend a lot of time in Fall or during their final year applying for jobs or internships. Early Decision gives students another option: apply to YC and bet on yourself.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45367046</guid><pubDate>Wed, 24 Sep 2025 23:12:38 +0000</pubDate></item><item><title>How did sports betting become legal in the US?</title><link>https://shreyashariharan.substack.com/p/how-did-sports-betting-become-legal</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45367086</guid><pubDate>Wed, 24 Sep 2025 23:15:38 +0000</pubDate></item><item><title>The "Wage Level" Mirage: H-1B proposal could help outsourcers and hurt US talent</title><link>https://ifp.org/the-wage-level-mirage/</link><description>&lt;doc fingerprint="3e79a14d386f9fef"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;This morning, the Department of Homeland Security proposed replacing the H-1B lottery with a weighted system that favors workers certified at higher Department of Labor (DOL) ‚ÄúWage Levels.‚Äù1 On the surface, this seems like a merit-based reform: higher wages should mean higher skills. In reality, DOL‚Äôs Wage Levels are very different from actual wages. The Wage Level framework was never designed to compare wages across occupations because it measures relative seniority within a job category, not actual pay. There are many workers paid at the highest DOL Wage Level but making below the median American wage, while some of the lowest DOL Wage Level are among the best-paid in the economy.&lt;/p&gt;
    &lt;p&gt;Using recently released FOIA data on all H-1B registrations and petitions filed from FY2021‚Äì2024, linked with Department of Labor data showing those individual workers‚Äô Wage Levels, I show that the proposed Wage Level system would not prioritize the highest-skilled or best-paid workers. Instead, it would have three surprising consequences:&lt;/p&gt;
    &lt;p&gt;1. A windfall for outsourcers. Large IT outsourcing firms (Wipro, Infosys, Tata, Cognizant, etc.) would actually gain under the rule, receiving 8% more visas, because they systematically register mid-career workers classified at higher Wage Levels despite paying comparatively low salaries. By contrast, a system based on actual pay would reduce visas for outsourcers by over 60%.&lt;/p&gt;
    &lt;p&gt;2. A setback for US-educated talent. International students graduating from American universities earn higher salaries on average than other H-1B workers, but because they are early-career, they are overwhelmingly classified at the lowest Wage Levels. The proposed rule would cut visas to F-1 graduates by 7%. A compensation-ranking would increase their share by 7%.&lt;/p&gt;
    &lt;p&gt;3. Minimal skill gains. DHS justifies the rule as raising skill levels, but the effect is trivial: the weighted lottery would lift median H-1B salaries by just 3% (from $92,000 to $95,000). By contrast, a compensation-based system would raise the median by a 52% jump to $140,000. It would also vastly increase the share of PhDs selected by a whopping 148%.&lt;/p&gt;
    &lt;p&gt;The Wage Level system was created to enforce minimum pay rules, but is ill-equipped to rank workers. Using it as a prioritization tool produces perverse results: rewarding outsourcing firms and older workers in lower-skill occupations while penalizing genuinely high-wage, high-skill talent, especially students trained in the US.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Wage Level Mirage&lt;/head&gt;
    &lt;p&gt;DOL‚Äôs Wage Level framework was designed as a compliance mechanism to enforce the Immigration and Nationality Act‚Äôs (INA) requirement that H-1B workers be paid at or above the ‚Äúprevailing wage‚Äù for their occupation and geographical area. To operationalize this, DOL uses the Occupational Employment and Wage Statistics (OEWS) survey to distinguish between four Wage Levels, intended to approximate the relative seniority of the offered position within a given occupation. The four Wage Levels are Level I (‚Äúentry‚Äù), Level II (‚Äúqualified‚Äù), Level III (‚Äúexperienced‚Äù), and Level IV (‚Äúfully competent‚Äù).&lt;/p&gt;
    &lt;p&gt;Currently, there are two random lotteries conducted each year: first an unreserved lottery for all registered H-1B petitions including those for beneficiaries who have earned a US Master‚Äôs or above, and then a reserved lottery only for US Master‚Äôs and above graduates.&lt;/p&gt;
    &lt;p&gt;DHS‚Äôs proposed rule would offer workers at OEWS Wage Level IV four chances in the lottery, workers at Wage Level III three chances, workers at Wage Level II two chances, and workers at Wage Level I one chance.&lt;/p&gt;
    &lt;p&gt;Importantly, Wage Levels are not the same as actual wages but are intended to correspond to seniority within an occupation and area. While Wage Levels are correlated with actual salaries, it is only a loose proxy, and there is significant overlap between the actual salaries paid by employers for workers at different Wage Levels.&lt;/p&gt;
    &lt;p&gt;A Wage Level IV job is not necessarily a high-wage job. In fact, the data show many Level IV certifications for salaries far below the median American wage, while some Level II jobs are among the best-paid in the economy. The ranking system would thus favor companies sponsoring older workers with longer seniority, even in lower-skill jobs, over genuinely high-wage, high-skill roles.&lt;/p&gt;
    &lt;p&gt;Consider some real petitions filed between FY2021-2024:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;An otolaryngology surgeon certified at Level I with a salary of $300,000.&lt;/item&gt;
      &lt;item&gt;A PhD technical staff member at Open AI certified at Level II with a salary of $280,000.&lt;/item&gt;
      &lt;item&gt;A CEO/CTO of a manufacturing startup certified at Level II with a salary of $300,000.&lt;/item&gt;
      &lt;item&gt;An acupuncturist certified at Wage Level III with an annual salary of $41,600.&lt;/item&gt;
      &lt;item&gt;A landscape architect certified at Wage Level IV with a salary of $36,090.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These are not representative examples of the typical worker at each Wage Level, but they are illustrative examples showing how widely Wage Levels diverge from real salaries. Under the proposed rule, the landscape architect and the acupuncturist mentioned above would be ranked ahead of the AI researcher, surgeon, and startup executive and would have more chances in the lottery. Under a compensation-based ranking, the landscape architect and acupuncturist would not get visas, while the others would.&lt;/p&gt;
    &lt;p&gt;This Wage Level system was never intended to serve as a proxy for relative merit, social value, or national interest. It was designed to be a tool for wage protection, not a ranking system.&lt;/p&gt;
    &lt;head rend="h2"&gt;DOL Wage Levels versus Compensation-ranking&lt;/head&gt;
    &lt;p&gt;Given the divergence between actual compensation and Wage Levels that can occur, we must turn to actual data on the distribution of Wage Levels and salaries to understand the outcomes of different mechanisms to award H-1Bs.&lt;/p&gt;
    &lt;p&gt;To measure the effects of alternative selection mechanisms, I turn to data obtained through a lawsuit filed by Bloomberg against DHS under the Freedom of Information Act.2 These data include detailed information on all H-1B lottery registrations made from FY2021‚Äì2024, along with data from petitions filed by lottery winners, including the jobs, employers, and compensation of H-1B filers. The data also include a unique code identifying the corresponding LCA associated with an H-1B petition, where the employer certified the Wage Level of the H-1B. DOL public disclosure data allowed me to link the FOIA data obtained by Bloomberg with the corresponding LCAs that actually resulted in H-1B registrations and filings.3 &lt;lb/&gt;Given this detailed information on a random sample of the registrant pool, I can simulate how H-1Bs would have been awarded in FY2021‚Äì2024 had the proposed weighted lottery been in effect.4 I also simulate what a pure compensation-based ranking would have done. I do not think a pure compensation-based ranking is ideal (I have elsewhere recommended adjusting compensation by geography and age 5), but it is a straightforward baseline of comparison. &lt;lb/&gt;As we‚Äôll see, compensation-ranking would have the opposite effect from using Wage Levels on important criteria, including student retention and visas awarded to outsourcers.&lt;/p&gt;
    &lt;p&gt;A boon to the large outsourcing companies&lt;/p&gt;
    &lt;p&gt;Large IT outsourcing companies, like the so-called WITCH companies (Wipro, Infosys, TCS, Cognizant, and HCL) are particularly well positioned to benefit from any prioritization based on Wage Levels. Their business model relies on flooding the lottery with registrations for mid-career staff in routine roles, who often qualify for higher wage levels despite the relatively low actual pay.&lt;/p&gt;
    &lt;p&gt;To identify large outsourcers, I focus on firms that register for more than 2,000 H-1Bs in any year with a clear outsourcing model. The most prominent include Wipro Limited, Infosys, Tech Mahindra Americas, Tata Consultancy Services, Capgemini, Cognizant Technology, and HCL America.&lt;/p&gt;
    &lt;p&gt;The results are striking. Across FY2021‚Äì2024, the proposed Wage Level-weighted lottery would have actually increased the large outsourcers‚Äô success rate. Across FY2021‚ÄìFY2024, large outsourcers would have received about 8% more visas under the proposed system than under the status quo.&lt;/p&gt;
    &lt;p&gt;The mechanism is straightforward. Data show that for large outsourcers, Wage Levels are disproportionately concentrated in Level II and Level III certifications. Most other employers, by contrast, hire far more at Level I than the outsourcers do. Because the proposed rule assigns more lottery entries to Level II and Level III workers, large outsourcers gain an advantage despite offering systematically lower wages.&lt;/p&gt;
    &lt;p&gt;By contrast, a compensation-based ranking would cut the outsourcers‚Äô share substantially, awarding fewer visas to these firms in every year of the data. On average, compensation-ranking would reduce H-1Bs to outsourcers by more than 60%.&lt;/p&gt;
    &lt;p&gt;Analogous results are apparent for other H-1B-dependent employers, companies with a large share of their workforce on H-1Bs. H-1B-dependent firms are those that employ at least eight H-1B employees if they have under 26 total employees, at least 13 H-1B employees if they have 26-50 employees, or at least 15% of all employees if they have more than 50 employees. The proposed rule would increase their H-1Bs by 4%, while compensation-ranking would decrease it by 54%.&lt;/p&gt;
    &lt;p&gt;A blow to talent retention&lt;/p&gt;
    &lt;p&gt;One of the clearest signals of whether the H-1B system supports American competitiveness is how it treats international students educated in the United States, given that American institutions of higher education are the on-ramp for most skilled talent in the United States. Each year, tens of thousands of foreign students graduate from American universities, often in STEM fields, and seek to transition from F-1 student status to H-1B employment.&lt;/p&gt;
    &lt;p&gt;The proposed rule would disadvantage international students educated in the United States. Graduates on F-1 visas, by definition, are early-career. Their positions are classified at Level I or Level II, even when the salaries are six-figure offers in cutting-edge industries. The FOIA data confirm this pattern: F-1 students entering the H-1B process earned higher salaries on average than non-F-1 workers, but they were far more likely to be placed at the lowest Wage Levels.&lt;/p&gt;
    &lt;p&gt;The result is that under the proposed Wage Level-based rule, the United States would lose more of the very talent pipeline it has already invested in training. The proposed rule would cut the number of H-1Bs to F-1 students each year by about 7%, while compensation-ranking would raise the number by about 7%.&lt;/p&gt;
    &lt;p&gt;The fact that compensation-ranking would increase the share of H-1Bs going to students may be counterintuitive, but it is borne out consistently in the data and can be explained by the fact that compensation-ranking has two countervailing effects: 1) they are disadvantaged relative to later career workers in the fields they are entering, but 2) they are advantaged relative to everyone in lower-paying occupations. Given that international students tend to disproportionately enter higher-paying occupations, they are often paid more than other H-1B workers, even in early career stages. As we will see in the next section, one dynamic helping them under compensation-ranking is less competition for visas from H-1B-dependent employers and large outsourcing companies.&lt;/p&gt;
    &lt;p&gt;A modest increase in wages.&lt;/p&gt;
    &lt;p&gt;A central rationale for reforming the H-1B allocation system is to better target scarce visas toward the most skilled workers. Yet the choice of metric matters enormously for what ‚Äúskill‚Äù means in practice. &lt;lb/&gt;Under the current lottery, the median salary for new H-1B petitions in each year from FY2021‚ÄìFY2024 averaged about $92,000. DHS‚Äôs proposed Wage Level-weighted lottery would raise that figure only modestly, to $95,000, a gain of just 3%. By contrast, a compensation-ranking system based on actual salaries would shift the composition of recipients dramatically, raising median wages to $140,000, a 52% increase. Those figures represent the average effect, but the effect is smaller in FY2021 and rose through FY2024. In FY2024, the median salary under compensation-ranking would have been $159,000, compared to only $98,000 under the proposed rule. &lt;/p&gt;
    &lt;p&gt;The results look even more stark at the lower end of the wage distribution. Under the proposed rule, there would still be H-1Bs going to workers paid less than the median US salary. In 2024, the 10th percentile salary under the weighted lottery would be just $70,000. Under compensation-ranking, not a single H-1B would go to any workers making less than $130,000.&lt;lb/&gt;The contrast is also apparent when looking at educational attainment. PhDs represent a relatively small share of the overall applicant pool, but they are often concentrated in frontier industries where skill and expertise are most critical. The weighted lottery would increase the number of PhDs selected by 22% compared to the status quo. Compensation-ranking, however, would raise the number of PhDs awarded H-1Bs by 148%, far more than the proposed system.&lt;/p&gt;
    &lt;p&gt;These results reflect the fundamental problem with using Wage Levels as a proxy for skill. Wage Levels reward seniority within an occupation, but they are explicitly designed to level the playing field between low-skilled and high-skilled occupations. The result is a middling increase in the skill levels of H-1B recipients.&lt;/p&gt;
    &lt;head rend="h2"&gt;Gaming, Manipulation, and Changing Behavior&lt;/head&gt;
    &lt;p&gt;The methodology in the previous section assumes that the pool of talent wouldn‚Äôt change, and that employers would continue to register for the lottery in similar ways as they have historically. But different policy environments will create different incentives that will change employer behavior.&lt;/p&gt;
    &lt;p&gt;First, other proposed policy changes may shift what the pool of registrants looks like. The $100,000 H-1B fee does not appear to apply to workers changing status from within the United States, which gives multinational companies, including outsourcing companies, the incentive to bring people on L visas and transfer them to H-1Bs to avoid the fee. This may mean that outsourcers won‚Äôt be significantly discouraged from continuing to register if they can avoid H-1B fees, and may benefit since their access to Ls is a competitive advantage against non-multinational companies. Meanwhile, proposed changes limiting international students‚Äô ability to stay after they graduate may shrink the potential pool of student talent. All of these changes add uncertainty to the magnitudes of the estimates in this report, although the basic mechanisms remain unchanged.&lt;/p&gt;
    &lt;p&gt;Second, explicitly prioritizing Wage Levels will encourage employers to find ways to game them. For example, employers can find ways to classify their workers in occupational categories with lower prevailing wages. For example, outsourcing companies can identify workers as computer programmers rather than software developers to get higher Wage Levels for the same salary. Given the artificiality of occupational classifications and Wage Levels, the proposal is open to much more manipulation and unintended consequences than compensation-ranking, which is much more difficult to game.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;The proposed wage-level prioritization rule would invert the intended logic of the H-1B program. Instead of channeling visas toward the valuable roles, it would reward seniority, benefitting outsourcing firms while hurting America‚Äôs talent pipeline by offering fewer visas to graduates of American schools.&lt;/p&gt;
    &lt;p&gt;The biggest winners would be firms that already dominate the program by volume but pay less than market rates. The losers would be international students completing programs at the nation‚Äôs colleges, and the companies and sectors on the front lines of America‚Äôs technological and economic competition.&lt;/p&gt;
    &lt;p&gt;Relying on DOL‚Äôs Wage Levels will not make the H-1B much more merit-based. Policymakers should instead prioritize actual compensation offers so the H-1B system can fulfill its potential as a genuine high-skilled visa program that advances America‚Äôs economic and strategic goals. In Part II, I will dive more deeply into the outcomes of compensation-ranking and some further alternatives.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Department of Homeland Security, "Weighted Selection Process for Registrants and Petitioners Seeking To File Cap-Subject H-1B Petitions," 8 CFR Part 214, RIN 1615-AD01.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;USCIS, US H-1B Visa Lottery and Petition Data FY 2021 - FY 2024, obtained by Bloomberg. https://github.com/BloombergGraphics/2024-h1b-immigration-data&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;DOL, Performance Data. https://www.dol.gov/agencies/eta/foreign-labor/performance&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Because the US Master‚Äôs and up lottery and the general lottery are random draws from the US Master‚Äôs and up and general populations respectively (this is only the case because the unreserved lottery is now held first), we have a true random sample of each of these populations. In each of 1,000 simulations, I draw from the registrant pool each year enough from both pools to match the expected likelihood of winning that attained in that year.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Jeremy Neufeld, Talent Recruitment Roulette: Replacing the H-1B Lottery, IFP (January 17, 2025). https://ifp.org/h1b/&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45368058</guid><pubDate>Thu, 25 Sep 2025 01:05:48 +0000</pubDate></item><item><title>Knotty: A domain-specific language for knitting patterns</title><link>https://t0mpr1c3.github.io/knotty/index.html</link><description>&lt;doc fingerprint="a5ebd7a76a7f3314"&gt;
  &lt;main&gt;
    &lt;p&gt;‚ñº Knotty 1 Introduction 2 How to Make a New Pattern 3 Input and Output 4 Code Examples 5 Reference On this page: Knotty 8.11 contents ‚Üê prev up next ‚Üí Knotty Tom Price &amp;lt; t0mpr1c3@gmail.com &amp;gt; ( require knotty ) package: knotty-lib A domain-specific language for knitting patterns. contents ‚Üê prev up next ‚Üí&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45369768</guid><pubDate>Thu, 25 Sep 2025 06:13:32 +0000</pubDate></item><item><title>The all-in-one PC: Raspberry Pi 500 on sale now at $200</title><link>https://www.raspberrypi.com/news/the-ultimate-all-in-one-pc-raspberry-pi-500-plus-on-sale-now-at-200/</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45370304</guid><pubDate>Thu, 25 Sep 2025 07:52:28 +0000</pubDate></item><item><title>Perhaps my last post ‚Äì we'll see (2016)</title><link>http://itila.blogspot.com/2016/04/perhaps-my-last-post-well-see.html</link><description>&lt;doc fingerprint="a72c4c940fbfd90c"&gt;
  &lt;main&gt;
    &lt;p&gt;I'd like my posts to have an ending, so I'm going to make this my final one - maybe.&lt;/p&gt;
    &lt;p&gt;While the doctors haven't expressed an opinion, I think it's possible I haven't got long to go, because I've lost 15 kg, and last Friday's CT scan showed that I've got secondaries on the go in my bones (as we already anticipated from the high ALP levels measured over the past weeks); my platelet count is very low, so they suspect that my bone marrow may be having trouble with cancer cells. On Monday they propose to take a bone marrow sample to find out what's going on. My extreme breathlessness continues - lying still in bed is fine, but getting out of bed onto the commode and back feels afterwards rather like a marathon. Maybe I'll pull through, but let's tentatively wrap up my blog-posts now.&lt;/p&gt;
    &lt;p&gt;There's lots I could write, but the way I'd like to stop is by pointing you to the writings of someone else. Max Edwards wrote a piece for the Guardian about his own cancer, and much of what he writes resonates for me. He was a remarkably eloquent writer.&lt;/p&gt;
    &lt;p&gt;Thanks for reading!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45370306</guid><pubDate>Thu, 25 Sep 2025 07:52:55 +0000</pubDate></item></channel></rss>