<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Fri, 24 Oct 2025 04:11:56 +0000</lastBuildDate><item><title>PyTorch Monarch</title><link>https://pytorch.org/blog/introducing-pytorch-monarch/</link><description>&lt;doc fingerprint="bddc819de3a94aa8"&gt;
  &lt;main&gt;
    &lt;p&gt;We now live in a world where ML workflows (pre-training, post training, etc) are heterogeneous, must contend with hardware failures, are increasingly asynchronous and highly dynamic. Traditionally, PyTorch has relied on an HPC-style multi-controller model, where multiple copies of the same script are launched across different machines, each running its own instance of the application (often referred to as SPMD). ML workflows are becoming more complex: pre-training might combine advanced parallelism with asynchrony and partial failure; while RL models used in post-training require a high degree of dynamism with complex feedback loops. While the logic of these workflows may be relatively straightforward, they are notoriously difficult to implement well in a multi-controller system, where each node must decide how to act based on only a local view of the workflow’s state.&lt;/p&gt;
    &lt;p&gt;We believe that the long-term sustainable way to address this is through a single controller programming model, in which a single script orchestrates all distributed resources, making them feel almost local. This architectural shift simplifies distributed programming—your code looks and feels like a single-machine Python program, but can scale across thousands of GPUs. You can directly use Pythonic constructs—classes, functions, loops, tasks, futures—to express complex distributed algorithms.&lt;/p&gt;
    &lt;p&gt;We’re excited to introduce Monarch, a distributed programming framework that brings the simplicity of single-machine PyTorch to entire clusters.&lt;/p&gt;
    &lt;p&gt;Monarch lets you program distributed systems the way you’d program a single machine, hiding the complexity of distributed computing:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Program clusters like arrays. Monarch organizes hosts, processes, and actors into scalable meshes that you can manipulate directly. You can operate on entire meshes (or slices of them) with simple APIs—Monarch handles the distribution and vectorization automatically, so you can think in terms of what you want to compute, not where the code runs.&lt;/item&gt;
      &lt;item&gt;Progressive fault handling. With Monarch, you write your code as if nothing fails. When something does fail, Monarch fails fast by default—stopping the whole program, just like an uncaught exception in a simple local script. Later, you can progressively add fine-grained fault handling exactly where you need it, catching and recovering from failures just like you’d catch exceptions.&lt;/item&gt;
      &lt;item&gt;Separate control from data. Monarch splits control plane (messaging) from data plane (RDMA transfers), enabling direct GPU-to-GPU memory transfers across your cluster. Monarch lets you send commands through one path, while moving data through another, optimized for what each does best.&lt;/item&gt;
      &lt;item&gt;Distributed tensors that feel local. Monarch integrates seamlessly with PyTorch to provide tensors that are sharded across clusters of GPUs. Monarch tensor operations look local but are executed across distributed large clusters, with Monarch handling the complexity of coordinating across thousands of GPUs.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Programming Model&lt;/head&gt;
    &lt;head rend="h4"&gt;Key APIs: Process and Actor Meshes&lt;/head&gt;
    &lt;p&gt;Monarch organizes resources into multidimensional arrays, or meshes. A process mesh is an array of processes spread across many hosts; an actor mesh is an array of actors, each running inside a separate process. Like array programming in NumPy or PyTorch, meshes make it simple to dispatch operations efficiently across large systems.&lt;/p&gt;
    &lt;p&gt;At launch, Monarch supports process meshes over GPU clusters—typically one process per GPU—onto which you can spawn actors into actor meshes. For local development, the same meshes can also run on a local development server.&lt;/p&gt;
    &lt;head rend="h4"&gt;Advanced APIs: Tensor Engine and RDMA Buffer&lt;/head&gt;
    &lt;p&gt;Monarch’s tensor engine brings distributed tensors to process meshes. It lets you write PyTorch programs as if the entire cluster of GPUs were attached to the machine running the script. For bulk data movement, Monarch also provides an RDMA buffer API, enabling direct, high-throughput transfers between processes on supported NICs.&lt;/p&gt;
    &lt;p&gt;Extensive details and more detailed examples can be found on our GitHub page.&lt;/p&gt;
    &lt;head rend="h4"&gt;A Simple Example&lt;/head&gt;
    &lt;p&gt;Monarch code imperatively describes how to create processes and actors using a simple Python API:&lt;/p&gt;
    &lt;quote&gt;from monarch.actor import Actor, endpoint, this_host procs = this_host().spawn_procs({"gpus": 8}) # define an actor that has one method class Example(Actor): @endpoint def say_hello(self, txt): return f"hello {txt}" # spawn the actors actors = procs.spawn("actors", Example) # have them say hello hello_future = actors.say_hello.call("world") # print out the results print(hello_future.get())&lt;/quote&gt;
    &lt;p&gt;In the above example, we define an Actor called “Example” that is deployed on 8 GPUs on the local host. The controller then invokes this example across the host and waits for their response. The actors can expose a variety of interfaces.&lt;/p&gt;
    &lt;head rend="h4"&gt;Slicing Meshes&lt;/head&gt;
    &lt;p&gt;We express broadcasted communication by organizing actors into a Mesh – a multidimensional container with named dimensions. For instance, a cluster might have dimensions {“hosts”: 32, “gpus”: 8}. Dimension names are normally things like “hosts”, indexing across the hosts in a cluster, or “gpus”, indexing across things in a machine.&lt;/p&gt;
    &lt;quote&gt;from monarch.actor import Actor, endpoint, this_host procs = this_host().spawn_procs({"gpus": 8}) # define an actor that has two methods class Example(Actor): @endpoint def say_hello(self, txt): return f"hello {txt}" @endpoint def say_bye(self, txt): return f"goodbye {txt}" # spawn the actors actors = procs.spawn("actors", Example) # have half of them say hello hello_fut = actors.slice(gpus=slice(0,4)).say_hello.call("world") # the other half say good bye bye_fut = actors.slice(gpus=slice(4,8)).say_bye.call("world") print(hello_fut.get()) print(bye_fut.get())&lt;/quote&gt;
    &lt;head rend="h4"&gt;Fault Recovery&lt;/head&gt;
    &lt;p&gt;Users can express distributed programs that can error through pythonic try, except blocks. Complex fault detection and fault recovery schemes can be built on top of these primitives. The following showcases handling a simple runtime Exception in a remote actor.&lt;/p&gt;
    &lt;quote&gt;from monarch.actor import Actor, endpoint, this_host procs = this_host().spawn_procs({"gpus": 8}) class Example(Actor): @endpoint def say_hello(self, txt): return f"hello {txt}" @endpoint def say_bye(self, txt): raise Exception("saying bye is hard") actors = procs.spawn("actors", Example) hello_fut = actors.slice(gpus=slice(0,4)).say_hello.call("world") bye_fut = actors.slice(gpus=slice(4,8)).say_bye.call("world") try: print(hello_fut.get()) except: print("couldn't say hello") try: print(bye_fut.get()) except Exception: print("got an exception saying bye")&lt;/quote&gt;
    &lt;p&gt;See “Case Study 2: Fault Tolerance in Large Scale Pre Training” for a more realistic use case.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Monarch Backend&lt;/head&gt;
    &lt;p&gt;Monarch is split into a Python-based frontend, and a backend implemented in Rust. Python is the lingua franca of machine learning, and our Python frontend APIs allow users to seamlessly integrate with existing code and libraries (like PyTorch!), and to use Monarch with interactive computing tools like Jupyter notebooks. Our Rust-based backend facilitates our performance, scale, and robustness — we amply use Rust’s fearless concurrency in Monarch’s implementation.&lt;/p&gt;
    &lt;head rend="h3"&gt;Hyperactor and hyperactor_mesh&lt;/head&gt;
    &lt;p&gt;At the bottom of the stack is a Rust-based actor framework called hyperactor. Hyperactor is a low-level distributed actor system, focused on performant message passing and robust supervision. hyperactor_mesh is built on top of hyperactor, and combines its various components into an efficient “vectorized” actor implementation. Hyperactor_mesh is oriented towards providing actor operations cheaply over large meshes of actors.&lt;/p&gt;
    &lt;p&gt;Monarch’s core Python APIs, in turn, are fairly thin wrappers around hyperactor_mesh.&lt;/p&gt;
    &lt;head rend="h3"&gt;Scalable messaging&lt;/head&gt;
    &lt;p&gt;Everything in Monarch relies on scalable messaging: the core APIs supporting casting messages to large meshes of actors. Hyperactor achieves this through two mechanisms: multicast trees and multipart messaging.&lt;/p&gt;
    &lt;p&gt;First, in order to support multicasting, Hyperactor sets up multicast trees to distribute messages. When a message is cast, it is first sent to some initial nodes, which then forward copies of the message to a set of its children, and so on, until the message has been fully distributed throughout the mesh. This lets us avoid single-host bottlenecks, effectively using the whole mesh as a distributed cluster for message forwarding.&lt;/p&gt;
    &lt;p&gt;Second, we ensure that the control plane is never in the critical path of data delivery. For example, we use multipart messaging to avoid copying, to enable sharing data across high-fanout sends (such as those that occur in our multicast trees), and materialize into efficient, vectorized writes managed by the OS.&lt;/p&gt;
    &lt;head rend="h2"&gt;Case Studies&lt;/head&gt;
    &lt;p&gt;We believe that this general purpose API and its native integration with PyTorch will unlock the next generation of AI applications at scale and the more complex orchestration requirements that they present.&lt;/p&gt;
    &lt;head rend="h3"&gt;Case Study 1: Reinforcement Learning&lt;/head&gt;
    &lt;p&gt;Reinforcement learning has been critical to the current generation of frontier models. RL enables models to do deep research, perform tasks in an environment and solve challenging problems such as math and code. For a deeper dive, we recommend this post for a deeper dive into the topics.&lt;/p&gt;
    &lt;p&gt;In order to train a reasoning model (see figure below), generator processes produce prompts from the reasoning model specializing in a specific domain (say, programming code generation). The generator uses these prompts (an incomplete coding problem statement) to derive a set of solutions or trajectories (executable code in this example) often interacting with the world through tools (compiler) and environments. Reward pipelines evaluate these solutions and come up with scores. These scores and rewards are used to train the same model whose weights are then transferred back to the systems that generated the prompt responses.&lt;/p&gt;
    &lt;p&gt;This constitutes a single training loop! As illustrated in the figure below, this is effectively a real-time pipeline of a number of heterogeneous computations within a training loop that may have to be orchestrated and scaled individually.&lt;/p&gt;
    &lt;p&gt;When implementing the RL example above in Monarch, each component — generator, trainer, inference engine, reward pipeline — might be represented by a mesh: a mesh of generators, a mesh of trainers, a mesh of inference nodes, a mesh of reward pipelines. (The figure above shows a simplistic example with only two meshes: generator and trainer).&lt;/p&gt;
    &lt;p&gt;The training script then uses these meshes to orchestrate the overall flow of the job: telling the generator mesh to start working from a new batch of prompts, passing the data to the training mesh when they are done, and updating the inference mesh when a new model snapshot is ready. The orchestrator is written as an ordinary Python program, calling methods on meshes and passing data between them. Because Monarch supports remote memory transfers (RDMA) natively, the actual data is transferred directly between members of meshes (just like you might copy a tensor from one GPU to another), enabling efficient and scalable workflows.&lt;/p&gt;
    &lt;head rend="h4"&gt;VERL&lt;/head&gt;
    &lt;p&gt;Volcano Engine Reinforcement Learning (VERL) is a widely used Reinforcement Learning framework in the industry today.&lt;/p&gt;
    &lt;p&gt;We integrated Monarch with VERL as a proof-of-concept and post-trained the Qwen-2.5-7B math model using GRPO on a curated math dataset and evaluated it on the AIME 2024 benchmark. We trained for 500+ training steps on H200 GPUs using Megatron-LM scaling progressively from 16, 64, 1024 to 2048 GPUs. The runs were stable and yielded good numerical parity with existing options, demonstrating that Monarch can orchestrate existing RL frameworks.&lt;/p&gt;
    &lt;p&gt;We are actively working on open-sourcing this integration to allow future users to evaluate Monarch as an option in their VERL jobs.&lt;/p&gt;
    &lt;head rend="h4"&gt;TorchForge&lt;/head&gt;
    &lt;p&gt;TorchForge represents a different approach: a pytorch native RL framework designed from the ground up with Monarch primitives.&lt;/p&gt;
    &lt;p&gt;TorchForge’s goal is to let researchers express RL algorithms as naturally as pseudocode, while Monarch handles the distributed complexity underneath. The result is code that looks like this:&lt;/p&gt;
    &lt;quote&gt;async def continuous_rollouts(): while True: prompt, target = await dataloader.sample.route() response = await policy.generate.route(prompt) reward = await reward.evaluate_response.route(prompt, response.text, target) await replay_buffer.add.route(Episode(...))&lt;/quote&gt;
    &lt;p&gt;No distributed coordination code, no retry logic, just RL written in Python.&lt;/p&gt;
    &lt;head rend="h4"&gt;Building on Monarch: Services and TorchStore&lt;/head&gt;
    &lt;p&gt;This clean API is possible because torchforge builds two key abstractions on Monarch’s primitives:&lt;/p&gt;
    &lt;p&gt;“Services” wrap Monarch ActorMeshes with RL-specific patterns. They leverage Monarch’s fault tolerance, resource allocation, and mailbox system, while adding patterns like load-balanced routing (`.route()`), parallel broadcasts (`.fanout()`), and sticky sessions for stateful operations.&lt;/p&gt;
    &lt;quote&gt;# A service is a managed group of ActorMeshes with routing primitives policy = PolicyActor.options( procs=8, with_gpus=True, num_replicas=16 # Creates 16 replicas, each with 8 GPUs ).as_service() # Services provide RL-friendly adverbs built on Monarch actors response = await policy.generate.route(prompt) # Load-balanced routing await policy.update_weights.fanout(version) # Parallel broadcast&lt;/quote&gt;
    &lt;p&gt;TorchStore is a distributed key-value store for PyTorch tensors that handles weight synchronization between training and inference. Built on Monarch’s RDMA primitives and single-controller design, it provides simple DTensor APIs while efficiently resharding weights on the fly – critical for off-policy RL where training and inference use different layouts.&lt;/p&gt;
    &lt;p&gt;These abstractions demonstrate Monarch’s composability: torchforge uses Monarch’s primitives (actors, RDMA, fault tolerance) as building blocks to create RL-specific infrastructure. The resulting framework handles coordination complexity at the infrastructure layer, letting researchers focus on algorithms.&lt;/p&gt;
    &lt;p&gt;For detailed examples of Forge’s APIs, component integration, and design philosophy, see our torchforge blog post.&lt;/p&gt;
    &lt;head rend="h3"&gt;Case Study 2: Fault Tolerance in Large Scale Pre Training&lt;/head&gt;
    &lt;p&gt;Hardware and software failures are common and frequent at scale. For example, in our Llama3 training runs we experienced 419 interruptions across a 54 day training window for a 16k GPU training job. This averages to about one failure every 3 hours. If we project this out to 10s of thousands of GPUs, this represents a failure once every hour or more frequently. Restarting the entire job for each of these failures will reduce the effective training time.&lt;/p&gt;
    &lt;p&gt;A solution is to use methods to further leverage distributed training through methods to make the numerics of the model more tolerant of having the various groups run more asynchronously. For example, TorchFT, released from PyTorch, provides a way to withstand failures of GPUs and allow the training to continue. One strategy is to use Hybrid Sharded Data Parallelism that combines fault tolerant DDP with FSDP v2 and PP. On failure we use torchcomms to gracefully handle errors and UI training on the next batch without downtime. This isolates failures to a single “replica group” and we can continue training with a subset of the original job.&lt;/p&gt;
    &lt;p&gt;Monarch integrates with TorchFT. Monarch centralizes the control plane into a single-controller model. Monarch uses its fault detection primitives to detect failures, and upon detection, can spin up new logical replica groups (Monarch Meshes) to join training once initialized. TorchFT’s Lighthouse server acts as a Monarch actor. Monarch provides configurable recovery strategies based on failure type. On faults, the controller first attempts fast, process‑level restarts within the existing allocation and only escalates to job reallocation when necessary, while TorchFT keeps healthy replicas stepping so progress continues during recovery.&lt;/p&gt;
    &lt;p&gt;We ran this code on a 30 node (240 H100s) Coreweave cluster, using the SLURM scheduler to train Qwen3-32B using torchtitan and TorchFT. We injected 100 injected failures every 3 minutes across multiple failure modes (segfaults, process kills, NCCL abort, host eviction, GIL deadlock). Monarch allows for configurable recovery strategies based on failure type — we observed this to be 60% faster by avoiding unnecessary job rescheduling (relative to full SLURM job restarts). We see 90s avg recovery for process failures and 2.5min avg recovery machine failures. For more details, see the README.&lt;/p&gt;
    &lt;head rend="h3"&gt;Case Study 3: Interactive Debugging with a Large GPU cluster&lt;/head&gt;
    &lt;p&gt;The actor framework is not just limited to large scale orchestration of complex jobs. It enables the ability to seamlessly debug complex, multi-GPU computations interactively. This capability represents a fundamental shift from traditional batch-oriented debugging to real-time, exploratory problem-solving that matches the scale and complexity of contemporary AI systems.&lt;/p&gt;
    &lt;p&gt;Traditional debugging workflows break down when confronted with the realities of modern ML systems. A model that trains perfectly on a single GPU may exhibit subtle race conditions, deadlocks, memory fragmentation, or communication bottlenecks when scaled across dozens of accelerators.&lt;/p&gt;
    &lt;p&gt;Monarch provides an interactive developer experience. With a local jupyter notebook, a user can drive a cluster as a Monarch mesh.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Persistent distributed compute allows very fast iteration without submitting new jobs&lt;/item&gt;
      &lt;item&gt;Workspace sync_workspace API quickly syncs local conda environment code to Mesh nodes.&lt;/item&gt;
      &lt;item&gt;Monarch provides a mesh-native, distributed debugger&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;See a jupyter tutorial at pytorch.org&lt;/p&gt;
    &lt;head rend="h3"&gt;Monarch + Lightning AI Notebook&lt;/head&gt;
    &lt;p&gt;See Monarch in action as we launch a 256-GPU training job from a single Studio notebook, powered by TorchTitan. Experience seamless scaling, persistent resources, and interactive debugging — all in one single Notebook. The figure above shows this schema. Also, see our Monarch-Lightning blog post. In this example, the traditional SPMD TorchTitan workload is encapsulated as an Actor within Monarch, allowing users to pre-train large language models (such as Llama-3 and Llama-4) interactively from a Studio Notebook.&lt;/p&gt;
    &lt;p&gt;Monarch enables you to reserve and maintain compute resources directly from your local Studio Notebook in lightning. Even if your notebook session is interrupted or code disconnects, your cluster allocation remains active through Multi-Machine Training (MMT). This persistence of the process allocator allows you to iterate, experiment, and resume work seamlessly, reducing manual intervention and making the notebook a reliable control center for distributed training tasks.&lt;/p&gt;
    &lt;p&gt;Using Monarch’s Actor model, you can define and launch the Titan Trainer as an Actor on a process mesh, scaling your training jobs to hundreds of GPUs – all from within the Studio notebook. Monarch handles the orchestration, code and file sharing, and log collection, so you can reconfigure and relaunch jobs quickly. Logs and metrics are available directly in the notebook, as well as through external tools like Litlogger and WandB, making it easy to monitor and manage large-scale training.&lt;/p&gt;
    &lt;p&gt;Monarch brings interactive debugging to distributed training. You can set Python breakpoints in your Actor code, inspect running processes, and attach to specific actors for real-time troubleshooting – all from the notebook interface. After training, you can modify configurations or define new actors and relaunch jobs on the same resources without waiting for new allocations. This dynamic workflow accelerates experimentation and provides deep insight into your distributed training runs.&lt;/p&gt;
    &lt;p&gt;The code snippet in the Monarch-Lightning blog post shows the sample Lightning studio notebook for Monarch to pre-train the Llama-3.1 – 8B model using TorchTitan on 256 GPUs.&lt;/p&gt;
    &lt;head rend="h3"&gt;Try Monarch Today: Build, Scale, and Debug Distributed AI Workflows with Ease&lt;/head&gt;
    &lt;p&gt;Monarch is available now on GitHub—ready for you to explore, build with, and contribute to. Dive into the Monarch repo to get started, explore the documentation for deeper technical details, and try out our interactive Jupyter notebook to see Monarch in action. For an end-to-end example of launching large-scale training directly from your notebook, check out the Lightning.ai integration. Whether you’re orchestrating massive training runs, experimenting with reinforcement learning, or interactively debugging distributed systems, Monarch gives you the tools to do it all—simply and at scale.&lt;/p&gt;
    &lt;head rend="h3"&gt;Acknowledgments&lt;/head&gt;
    &lt;p&gt;Thank you to the whole Monarch team for making this work possible. Also, a special thanks to our Top Contributors on GitHub!&lt;/p&gt;
    &lt;p&gt;Ahmad Sharif, Allen Wang, Alireza Shamsoshoara, Amir Afzali, Amr Mahdi, Andrew Gallagher, Benji Pelletier, Carole-Jean Wu, Chris Gottbrath, Colin Taylor, Davide Italiano, Dennis van der Staay, Eliot Hedeman, Gayathri Aiyer, Gregory Chanan, Hamid Shojanazeri, James Perng, James Sun, Jana van Greunen, Jayasi Mehar, Joe Spisak, John William Humphreys, Jun Li, Kai Li, Keyan Pishdadian, Kiuk Chung, Lucas Pasqualin, Marius Eriksen, Marko Radmilac, Mathew Oldham, Matthew Zhang, Michael Suo, Matthias Reso, Osama Abuelsorour, Pablo Ruiz Fischer Bennetts, Peng Zhang, Rajesh Nishtala, Riley Dulin, Rithesh Baradi, Robert Rusch, Sam Lurye, Samuel Hsia, Shayne Fletcher, Tao Lin, Thomas Wang, Victoria Dudin, Vidhya Venkat, Vladimir Ivanov, Zachary DeVito&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45680237</guid><pubDate>Thu, 23 Oct 2025 10:15:12 +0000</pubDate></item><item><title>US probes Waymo robotaxis over school bus safety</title><link>https://www.yahoo.com/news/articles/us-investigates-waymo-robotaxis-over-102015308.html</link><description>&lt;doc fingerprint="fd2fc04a0886f8cd"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;US probes Alphabet unit Waymo robotaxis over school bus safety&lt;/head&gt;
    &lt;p&gt;By David Shepardson and Akash Sriram&lt;/p&gt;
    &lt;p&gt;WASHINGTON (Reuters) -The National Highway Traffic Safety Administration said on Monday it has opened a preliminary investigation into about 2,000 Waymo self-driving vehicles after reports that the company's robotaxis may have failed to follow traffic safety laws around a stopped school bus.&lt;/p&gt;
    &lt;p&gt;The probe of the Alphabet unit is the latest scrutiny of self-driving systems by regulators reviewing how driverless technologies interact with pedestrians, cyclists and other road users.&lt;/p&gt;
    &lt;p&gt;NHTSA opened the investigation after a recent media report aired video of an incident in Georgia in which a Waymo did not remain stationary when approaching a school bus with its red lights flashing and stop arm deployed.&lt;/p&gt;
    &lt;p&gt;The report said the Waymo vehicle initially stopped then maneuvered around the bus, passing the extended stop arm while students were disembarking.&lt;/p&gt;
    &lt;p&gt;Waymo's automated driving system surpassed 100 million miles of driving in July and is logging 2 million miles per week, the agency said. "Based on NHTSA’s engagement with Waymo on this incident and the accumulation of operational miles, the likelihood of other prior similar incidents is high," the agency said.&lt;/p&gt;
    &lt;p&gt;A Waymo spokesperson said the company has “already developed and implemented improvements related to stopping for school buses and will land additional software updates in our next software release.”&lt;/p&gt;
    &lt;p&gt;The company added "driving safely around children has always been one of Waymo's highest priorities." The company said Waymo "approached the school bus from an angle where the flashing lights and stop sign were not visible and drove slowly around the front of the bus before driving past it, keeping a safe distance from children."&lt;/p&gt;
    &lt;p&gt;NHTSA said the vehicle involved was equipped with Waymo's fifth-generation Automated Driving System and was operating without a human safety driver at the time of the incident.&lt;/p&gt;
    &lt;p&gt;Waymo has said its robotaxi fleet numbers more than 1,500 vehicles operating across major U.S. cities, including Phoenix, Los Angeles, San Francisco and Austin.&lt;/p&gt;
    &lt;p&gt;In July, NHTSA closed a 14-month investigation into a series of minor collisions "with clearly visible objects that a competent driver would be expected to avoid" and "unexpected behavior" from Waymo self-driving vehicles, after 22 reports about its robotaxis driving in ways that potentially violated traffic safety laws. The probe ended after two recalls.&lt;/p&gt;
    &lt;p&gt;(Reporting by Akash Sriram in Bengaluru and David Shepardson in Washington; Editing by Tasim Zahid)&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45681147</guid><pubDate>Thu, 23 Oct 2025 12:40:57 +0000</pubDate></item><item><title>Show HN: Git for LLMs – A context management interface</title><link>https://twigg.ai</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=45682776</guid><pubDate>Thu, 23 Oct 2025 15:12:57 +0000</pubDate></item><item><title>Trump pardons convicted Binance founder</title><link>https://www.wsj.com/finance/currencies/trump-pardons-convicted-binance-founder-7509bd63</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45683152</guid><pubDate>Thu, 23 Oct 2025 15:41:48 +0000</pubDate></item><item><title>Claude Memory</title><link>https://www.anthropic.com/news/memory</link><description>&lt;doc fingerprint="777befbbffb2c0ea"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Bringing memory to Claude&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Update&lt;p&gt;Expanding to Pro and Max plans&lt;/p&gt;&lt;p&gt;Oct 23, 2025&lt;/p&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Whether you’re iterating on a strategy proposal, debugging an issue, or managing multiple projects, Claude picks up right where you left off. Like Team and Enterprise users, you get project-scoped memory (each project has its own separate memory), full control to view and edit what Claude remembers, and incognito chat for conversations that don’t save to memory.&lt;/p&gt;
    &lt;p&gt;Before this rollout, we ran extensive safety testing across sensitive wellbeing-related topics and edge cases—including whether memory could reinforce harmful patterns in conversations, lead to over-accommodation, and enable attempts to bypass our safeguards. Through this testing, we identified areas where Claude's responses needed refinement and made targeted adjustments to how memory functions. These iterations helped us build and improve the memory feature in a way that allows Claude to provide helpful and safe responses to users.&lt;/p&gt;
    &lt;p&gt;To get started, enable memory in Settings.&lt;/p&gt;
    &lt;p&gt;Today, we’re introducing memory to the Claude app, where Claude remembers you and your team’s projects and preferences, eliminating the need to re-explain context and keeping complex work moving forward.&lt;/p&gt;
    &lt;p&gt;Memory is fully optional, with granular user controls that help you manage what Claude remembers. We’re also introducing Incognito chats that don’t appear in your conversation history or save to memory.&lt;/p&gt;
    &lt;p&gt;Memory is rolling out to Team and Enterprise plan users starting today. Enterprise admins can choose whether to disable memory for their organization at any time. Incognito chat is available to all Claude users.&lt;/p&gt;
    &lt;head rend="h2"&gt;Memory built for work&lt;/head&gt;
    &lt;p&gt;With memory, Claude focuses on learning your professional context and work patterns to maximize productivity. It remembers your team’s processes, client needs, project details, and priorities. Sales teams keep client context across deals, product teams maintain specifications across sprints, and executives track initiatives without constantly rebuilding context.&lt;/p&gt;
    &lt;p&gt;If you use projects, Claude creates a separate memory for each project. This ensures that your product launch planning stays separate from client work, and confidential discussions remain separate from general operations. These project boundaries help you and your teams manage complex, concurrent initiatives without mixing unrelated details, serving as a safety guardrail that keeps sensitive conversations contained.&lt;/p&gt;
    &lt;p&gt;Claude uses a memory summary to capture all its memories in one place for you to view and edit. In your settings, you can see exactly what Claude remembers from your conversations, and update the summary at any time by chatting with Claude. Based on what you tell Claude to focus on or to ignore, Claude will adjust the memories it references.&lt;/p&gt;
    &lt;head rend="h2"&gt;Incognito chat&lt;/head&gt;
    &lt;p&gt;Sometimes you need Claude’s help without using or adding to memory. Incognito chat gives you a clean slate for conversations that you don’t want to preserve in memory. It is perfect for sensitive brainstorming, confidential strategy discussions, or when you simply want a fresh conversation without context from previous chats. Your regular memory and conversation history remain untouched. If you’re using memory on a Team or Enterprise plan, your standard data retention settings apply.&lt;/p&gt;
    &lt;head rend="h2"&gt;Starting with teams at work&lt;/head&gt;
    &lt;p&gt;Memory introduces new safety considerations and we've designed the feature to be useful in work settings, while avoiding sensitive conversations and topics. We're also taking a thoughtful phased approach to ensure these powerful capabilities are deployed responsibly, and will continue to evaluate and test how memory works across the different ways people use Claude before expanding availability.&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting started&lt;/head&gt;
    &lt;p&gt;To see memory in action, enable the feature in Settings, and let Claude generate memory with your past chats at initial set-up. Ask Claude questions like “what were we working on last week?” to see what Claude remembers across your existing chats and connected tools. If you would like to bring your memory details over from a different AI tool or export your memory from Claude for backup or migration, you can follow these instructions.&lt;/p&gt;
    &lt;p&gt;Great work builds over time. With memory, each conversation with Claude improves the next.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45684134</guid><pubDate>Thu, 23 Oct 2025 16:56:07 +0000</pubDate></item><item><title>New updates and more access to Google Earth AI</title><link>https://blog.google/technology/research/new-updates-and-more-access-to-google-earth-ai/</link><description>&lt;doc fingerprint="3412e0dad1b67589"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;New updates and more access to Google Earth AI&lt;/head&gt;
    &lt;p&gt;When disasters strike, Google products like Search and Maps help billions of people make critical decisions to stay safe. Our flood forecasting information — now covering more than two billion people — provides life-saving forecasts before the most significant river floods. It's helped organizations like World Vision get drinking water and food to communities when they need it most. And during the devastating 2025 California wildfires, we provided crisis alerts with information from local authorities to 15 million people across Los Angeles while showing them where to find shelter in Google Maps. This is all made possible by our geospatial AI models, not only for floods and wildfires, but cyclones, air quality and many more.&lt;/p&gt;
    &lt;p&gt;We recently introduced Google Earth AI, bringing together these geospatial models to help tackle the planet's most critical needs. Earth AI is built on decades modeling the world, combined with state of the art predictive models and Gemini’s advanced reasoning, letting enterprises, cities and nonprofits achieve deeper understanding in minutes — efforts that previously required complex analytics and years of research.&lt;/p&gt;
    &lt;p&gt;Today, we’re advancing Earth AI’s innovations and capabilities, and expanding access around the globe. Here’s how:&lt;/p&gt;
    &lt;head rend="h2"&gt;Connecting the dots with Geospatial Reasoning&lt;/head&gt;
    &lt;p&gt;To solve a complex problem, you need to see the whole picture, not just one piece of it. That’s the idea behind Geospatial Reasoning, a framework powered by Gemini that now lets AI automatically connect different Earth AI models — like weather forecasts, population maps and satellite imagery — to answer complex questions.&lt;/p&gt;
    &lt;p&gt;Instead of just seeing where a storm might hit, our latest research demonstrates that analysts can use Geospatial Reasoning to identify which communities are most vulnerable and what infrastructure is at risk, all at once. For example, Geospatial Reasoning empowers the nonprofit GiveDirectly to respond to disasters by combining flood and population density information, helping them identify who needs direct aid most.&lt;/p&gt;
    &lt;p&gt;Sign up for consideration to become a Trusted Tester for Geospatial Reasoning. Social impact organizations can learn about future support for nonprofits through Google.org and our non-commercial access programs.&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting deeper insights in Google Earth&lt;/head&gt;
    &lt;p&gt;Gemini capabilities in Google Earth enable analysts to quickly understand information about the world just by asking questions. Now, we’re bringing new Earth AI models to Gemini capabilities in Google Earth, letting users instantly find objects and discover patterns from satellite imagery.&lt;/p&gt;
    &lt;p&gt;For example, a water company can now spot where a river has dried up — which can help communities predict the risk of dust storms during a drought — and notify people in advance. Or, analysts can quickly identify where harmful algae is blooming in order to monitor drinking water supply, giving authorities time to issue warnings or shut down water utilities.&lt;/p&gt;
    &lt;p&gt;This experimental capability will be available in the U.S. in the coming weeks to Google Earth Professional and Professional Advanced users. And starting today, Google AI Pro and Ultra subscribers in the U.S. can access Gemini capabilities in Google Earth with higher limits.&lt;/p&gt;
    &lt;p&gt;Finding algae blooms within Google Earth imagery.&lt;/p&gt;
    &lt;head rend="h2"&gt;Bringing Earth AI to Google Cloud&lt;/head&gt;
    &lt;p&gt;We’re making Earth AI Imagery, Population and Environment models directly available to Trusted Testers on Google Cloud. Businesses can now use their own data alongside our models and datasets, like Imagery Insights, to tackle specific challenges from environmental monitoring to disaster response.&lt;/p&gt;
    &lt;head rend="h2"&gt;Earth AI in Action&lt;/head&gt;
    &lt;p&gt;We’ve begun piloting Earth AI with thousands of organizations, including:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The World Health Organization Regional Office for Africa (WHO AFRO) is using Earth AI’s Population and Environment models, along with their own datasets, to understand and predict which areas in the Democratic Republic of Congo are at risk for cholera outbreaks. This information enables experts to help manage water, sanitation and vaccinations.&lt;/item&gt;
      &lt;item&gt;Satellite imagery providers like Planet and Airbus are using Earth AI models to analyze the billions of pixels they capture daily. Planet looks at historical satellite imagery to help customers map deforestation. Airbus uses Earth AI to help customers detect where vegetation is encroaching on power lines, so they can prevent outages.&lt;/item&gt;
      &lt;item&gt;Bellwether, a moonshot at Alphabet's X, is using Earth AI to provide hurricane predictions insights for global insurance broker McGill and Partners. This enables McGill's clients to pay claims faster so homeowners can start rebuilding sooner.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These updates will make it easier to take action with Earth AI. We're working with partners to train Earth AI models to reason about the physical world as fluently as LLMs reason about the digital one so that we can support those building solutions for their communities, solving problems in public health, crisis response and beyond.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45684155</guid><pubDate>Thu, 23 Oct 2025 16:58:04 +0000</pubDate></item><item><title>OpenAI acquires Sky.app</title><link>https://openai.com/index/openai-acquires-software-applications-incorporated</link><description>&lt;doc fingerprint="313411143f63e33e"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;OpenAI acquires Software Applications Incorporated, maker of Sky&lt;/head&gt;
    &lt;p&gt;Enhancing how people use AI on their computers.&lt;/p&gt;
    &lt;p&gt;AI progress isn’t only about advancing intelligence—it’s about unlocking it through interfaces that understand context, adapt to your intent, and work seamlessly. That’s why we’re excited to share that OpenAI has acquired Software Applications Incorporated, makers of Sky.&lt;/p&gt;
    &lt;p&gt;Sky is a powerful natural language interface for the Mac. With Sky, AI works alongside you, whether you’re writing, planning, coding, or managing your day. Sky understands what’s on your screen and can take action using your apps.&lt;/p&gt;
    &lt;p&gt;We will bring Sky’s deep macOS integration and product craft into ChatGPT, and all members of the team will join OpenAI.&lt;/p&gt;
    &lt;p&gt;“We’re building a future where ChatGPT doesn’t just respond to your prompts, it helps you get things done. Sky’s deep integration with the Mac accelerates our vision of bringing AI directly into the tools people use every day.” —Nick Turley, VP &amp;amp; Head of ChatGPT&lt;/p&gt;
    &lt;p&gt;“We’ve always wanted computers to be more empowering, customizable, and intuitive. With LLMs, we can finally put the pieces together. That’s why we built Sky, an AI experience that floats over your desktop to help you think and create. We’re thrilled to join OpenAI to bring that vision to hundreds of millions of people.” —Ari Weinstein, Co-Founder and CEO, Software Applications Incorporated&lt;/p&gt;
    &lt;p&gt;Stay tuned for more updates as we get to work integrating Sky’s capabilities.&lt;/p&gt;
    &lt;p&gt;Disclosure: An investment fund associated with Sam Altman held a passive investment in Software Applications Incorporated. This acquisition was led by Nick Turley and Fidji Simo and approved by the independent Transaction and Audit Committees of OpenAI’s board of directors.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45684236</guid><pubDate>Thu, 23 Oct 2025 17:04:17 +0000</pubDate></item><item><title>Armed police swarm student after AI mistakes bag of Doritos for a weapon</title><link>https://www.dexerto.com/entertainment/armed-police-swarm-student-after-ai-mistakes-bag-of-doritos-for-a-weapon-3273512/</link><description>&lt;doc fingerprint="ed14b9150981b13f"&gt;
  &lt;main&gt;
    &lt;p&gt;Concerns over AI surveillance in schools are intensifying after armed officers swarmed a 16-year-old student outside Kenwood High School in Baltimore when an AI gun detection system falsely flagged a Doritos bag as a firearm.&lt;/p&gt;
    &lt;p&gt;Taki Allen was hanging out with friends after football practice on October 20 when multiple police cars suddenly pulled up.&lt;/p&gt;
    &lt;p&gt;“It was like eight cop cars that came pulling up for us,” Allen told WBAL-TV 11 News. “They started walking toward me with guns, talking about ‘Get on the ground,’ and I was like, ‘What?’”&lt;/p&gt;
    &lt;p&gt;“They made me get on my knees, put my hands behind my back, and cuff me. Then they searched me and found nothing,” he said.&lt;/p&gt;
    &lt;p&gt;Allen was handcuffed at gunpoint. Police later showed him the AI-captured image that triggered the alert. The crumpled Doritos bag in his pocket had been mistaken for a gun.&lt;/p&gt;
    &lt;p&gt;“It was mainly like, am I gonna die? Are they going to kill me? “They showed me the picture, said that looks like a gun, I said, ‘no, it’s chips.’”&lt;/p&gt;
    &lt;head rend="h2"&gt;Student afraid to return to school after AI sends police after him&lt;/head&gt;
    &lt;p&gt;The AI system behind the incident is part of Omnilert’s gun detection technology, introduced in Baltimore County Public Schools last year. It scans existing surveillance footage and alerts police in real time when it detects what it believes to be a weapon.&lt;/p&gt;
    &lt;p&gt;Omnilert later admitted the incident was a “false positive” but claimed the system “functioned as intended,” saying its purpose is to “prioritize safety and awareness through rapid human verification.”&lt;/p&gt;
    &lt;p&gt;Baltimore County Public Schools echoed the company’s statement in a letter to parents, offering counseling services to students impacted by the incident.&lt;/p&gt;
    &lt;head rend="h2"&gt;Related&lt;/head&gt;
    &lt;p&gt;“We understand how upsetting this was for the individual that was searched as well as the other students who witnessed the incident,” the principal wrote. “Our counselors will provide direct support to the students who were involved.”&lt;/p&gt;
    &lt;p&gt;Allen says no one from the school has reached out to him personally.&lt;/p&gt;
    &lt;p&gt;“They didn’t apologize. They just told me it was protocol,” he said. “I was expecting at least somebody to talk to me about it.”&lt;/p&gt;
    &lt;p&gt;The teen now says he no longer feels safe going to school.&lt;/p&gt;
    &lt;p&gt;“If I eat another bag of chips or drink something, I feel like they’re going to come again,” Allen said.&lt;/p&gt;
    &lt;p&gt;The case has sparked fresh debate over the reliability of AI surveillance tools and their real-world consequences, especially in schools.&lt;/p&gt;
    &lt;p&gt;This incident comes as more institutions implement AI technology. Earlier this month, Major General William ‘Hank’ Taylor, one of the top officers in the US Army, admitted to using ChatGPT to make key military decisions.&lt;/p&gt;
    &lt;p&gt;Meanwhile, the UK introduced strict age verification measures for mature content, requiring users to pass a facial scan to prove they’re over 18. This has left some adults unable to access content, such as Britain’s most tattooed man, who said the age check system told him to “remove his face” because it interpreted his tattoos as a mask.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45684934</guid><pubDate>Thu, 23 Oct 2025 18:09:37 +0000</pubDate></item><item><title>Can “second life” EV batteries work as grid-scale energy storage?</title><link>https://www.volts.wtf/p/can-second-life-ev-batteries-work</link><description>&lt;doc fingerprint="677ad37521942734"&gt;
  &lt;main&gt;
    &lt;p&gt;Redwood Materials has long dominated EV battery recycling, but what if they could drain every last drop of energy from those batteries before recycling them? I talk with the company’s CTO, Colin Campbell, about Redwood Energy, a new division doing just that by deploying used batteries as grid-scale storage at a massive scale. This isn’t just a side project; it’s a plan to turn a massive wave of incoming used batteries into a key resource for the grid.&lt;/p&gt;
    &lt;p&gt;(PDF transcript)&lt;lb/&gt;(Active transcript)&lt;/p&gt;
    &lt;head rend="h4"&gt;Text transcript:&lt;/head&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;Hi everybody, this is Volts for October 22, 2025, “Can ‘second life’ EV batteries work as grid-scale energy storage?” I’m your host, David Roberts. Redwood Materials started recycling lithium-ion batteries before it was cool, before it was profitable, and before there were very many lithium-ion batteries to speak of. In the five years since, it has grown and scaled to capture the overwhelming majority of the booming automotive-battery recycling market in North America.&lt;/p&gt;
    &lt;p&gt;Earlier this year, it spun off a new division called Redwood Energy. The idea is simple: it is going to hook the EV batteries it receives up to large arrays that serve as grid-scale energy storage. That way, it can drain every last bit of useful life out of them before it recycles them.&lt;/p&gt;
    &lt;p&gt;So-called “second life” batteries have been discussed for many, many years, but this is the first time they have been deployed at appreciable scale. Redwood has built an off-grid facility where 20 megawatts of solar panels are powering 63 megawatt-hours of second life batteries that feed into two one-megawatt data centers.&lt;/p&gt;
    &lt;p&gt;Today, I am going to chat with the company’s CTO, Colin Campbell, about why Redwood hatched this initiative, how the costs of second life storage compare with the rest of the storage market, and how big he thinks this side of the business could ultimately get.&lt;/p&gt;
    &lt;p&gt;All right then, with no further ado, Colin Campbell, welcome to Volts. Thank you so much for coming.&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;David, thanks so much for having me. Pleasure to be here with you.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;Before we jump in, let’s back up a little bit — I should have probably mentioned in the intro you are, in certain circles, semi-famous. Colin Campbell worked at Tesla for many, many years — 17 years — one of their lead engineers. So a big role in designing and growing Tesla. So maybe you could just say briefly what it is that pulled you over to this side of things?&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;Yeah, I was really excited when I joined Redwood to take some of the same philosophies and approaches and really, how should we say it, sort of the ethical impetus that brought me to Tesla and to deploy it in another part of the world. So instead of in consumer devices, cars, in industrial and industry, in things like the metals markets, novel material manufacturing at scale in the US. These are things that I think we all feel have withered a little bit over the last 20, 30 years in the US, and there’s a tremendous opportunity to apply some of the same lessons that we used at Tesla to those industries.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;Was it 2019 that Redwood got underway?&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;Yes, that’s right.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;And when Redwood started, there were not that many EV batteries around. So you guys have been recycling all kinds. Basically just taking all lithium-ion batteries. Is that right?&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;Yeah. If you’ve got a lithium-ion battery, we will recycle it for you. So we’ll take your AirPods, your toothbrush, we’ll take that laptop, cell phone, all the way up to EVs.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;When did automotive batteries become the majority of your input by volume?&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;That is a good question.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;Was it recent or was that early on?&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;I would say the transition to EV batteries dominating what we received, it’s been in the last year or 18 months.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;So the front edge of a very large wave of batteries has begun to arrive?&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;Yeah, the wave is out there, it’s coming. The waters have finally started to arrive at the beach here.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;So then comes this idea — and I have to think that someone at Redwood has had this idea in the back of their minds for a while — this idea of you’re getting all these batteries and then you’re just sucking the life out of them before you recycle them. It seems quite logical, but it raises all kinds of questions. So the very first question, I think the most important, because people have been discussing second life batteries for a long time, and always the barrier that keeps people away from it is batteries are very heterogeneous.&lt;/p&gt;
    &lt;p&gt;They come in. There are lots of different chemistries, there are lots of different wear and tear, states of wear and tear, how much life they have in them. You just get a huge variety of batteries. Making all those batteries work together has been the trick. And so when I heard this announcement from Redwood, I was like, “Oh my God, how’d they solve that problem?” And I went and looked and all it says on the site is, “Hey, we solved that problem.” So I’m very curious. Like, I thought that was an extremely difficult problem, but apparently you just hook all these different batteries up to this little box, this little box that you’re calling the universal translator, and voila, they all work together. So can you tell us a little bit about how you did that?&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;Simple as that. Yes. No, you’re absolutely right. And the idea of a second life battery in a second life energy storage system has been out there for a long time. I think you nailed one piece of it, which was that we weren’t really seeing those batteries come back for, originally, recycling, and now for second life in appreciable volume until really recently. We know it’s inevitable that they’ll come back, but it just hadn’t started to happen yet. And the other thing that’s made this practical — and more than practical, a good business for us — is a couple of realizations we had.&lt;/p&gt;
    &lt;p&gt;So one is that electric vehicle batteries are incredibly robust, and grid-scale energy storage is a relatively pastoral life for those batteries — it’s really like putting the old horse out to pasture. So what that means is you don’t have to do much to them. They’re mechanically made for salt spray and vibration and being underwater, and they’re electrically made for fast charging, for merging onto the freeway, for high performance.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;They’re very high-end.&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;Yes, exactly.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;I don’t know that people understand, one, how high end they are, or two, how big they are — capacity-wise, they’re huge. And then there’s three, sort of how big they are physically. I mean, one of the things that sort of haunted me as I was reading about this, and this is for years, when I think automotive battery, my brain wants to think about the size and shape of the normal —&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;12-volt lead-acid guy.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;12-volt battery, that square. And I was like, “Oh, those little square boxes. Well, those will be easy to stack.” But of course, an actual EV battery is this big, unwieldy, odd-shaped thing that is also heavy. You have to carry them around with forklifts.&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;Yep. So because they’re so awkward and big, if you’re going to repurpose them and have a profitable business doing that, you really can’t spend a bunch of labor on it. So the important thing for us was, don’t disassemble it, don’t mess with it, really use it as it is.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;That’s key here. You’re not getting at the cells, you’re not tearing it down and using the cells, you’re just — it’s just the battery itself. You’re just plugging the battery itself, laying it on the ground and plugging it in.&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;Yeah, there’s a little bit more magic to that, but that’s the core of it — do as little as possible, make it really, really simple to keep it cheap and resilient and flexible for the, what you said, the wide variety of packs that we get.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;And we should say another barrier to making this business work was the incredible logistics involved in gathering batteries, which, of course, you guys are already doing.&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;Yeah, it’s an underappreciated challenge of the recycling business, which is just gathering all the batteries. It’s an incredibly diverse and heterogeneous set of companies, set of individuals, set of utilities, and municipal agencies that collect them. And our commercial team has just crushed it, really, in making it so that all those batteries come to us.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;What’s the percentage now?&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;It’s north of 70 or 80%.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;So is that 80% of the batteries that get recycled or do you think it’s 80% of all lithium-ion car batteries?&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;It is of the lithium-ion batteries. So more than just car batteries that get recycled in the US.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;So you’re the bulk of the recycling business. But you didn’t actually answer the core of the question, which is — I mean, I’m sure the actual answer is some sort of software gobbledygook that I wouldn’t understand — but what is it that made it possible for you, you guys, to figure this software out when other people have failed?&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;Yeah. So there are two parts, like we said: One is this mechanical simplicity and the other is what you referred to earlier, this box that we’re calling a universal translator. So what is that? That is something that our really strong in-house power electronics team built. It is a way to make sure every one of these unique snowflakes of a pack gets individual care and feeding. So it’s a way to control the voltage, control the power for each pack, depending on its own unique state of health, its own unique state of charge. Also to talk the language of any pack.&lt;/p&gt;
    &lt;p&gt;So if you’ve got 50 different pack manufacturers, they all talk 50 different languages, the content is kind of similar —&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;I was curious about that, are you sort of able to generalize at all or are you really sort of having, “Well, here’s NMC language. We got to program that in.” Is there any sort of commonality among batteries or is it really just bespoke, you got to program every single individual one in there?&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;It’s pretty bespoke. The content is the same. All battery packs want to talk about is cell voltages, cell temperatures, and pack currents. But the languages are pretty different.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;Interesting.&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;So that’s been a real effort and a real successful effort by our software team to abstract that away and make it so that at the high level we just have a battery.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;So I’m curious just how agnostic it is. Can you say with confidence that any EV battery on the market, if you wear it out in your car, you can plug it into this thing? Is that categorical?&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;It’s never, never 100%. But we’ve put a lot of effort into making sure we’re in the high 90s. So this universal translator that we built, it’s ready for battery packs anywhere from 200 to 900 volts, for example. So we know more modern cars are higher voltage, semi trucks are higher voltage, Cybertrucks are higher voltage. And then also we’re agnostic to chemistry, so LFP, high nickel, we can take it all.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;That is just wild. So congrats to your software team. It actually raises the question, which is, I’m wondering, like, how modular is this? It seems like the box, the software is the key thing. So if I, for whatever reason, had one of these boxes and I’m like, I don’t know, I run an automotive garage or something and I just happen to have three or four spare EV batteries in my backyard, could I hook them together and treat them like one battery, just using this thing? Like how, how small can it get, I guess is the question?&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;Yeah, you hypothetically could. I’d never thought about it, to be honest. But there’s no reason you couldn’t make a 200 kilowatt-hour site using this same piece of technology.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;So it’ll scale down as far as you would ever want?&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;It will. I’ll say that’s not the question we most commonly get. Usually people want to know how big can you go and how fast can you go?&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;Yeah, yeah, yeah, well, we’ll get there. But you know, I love me some distributed energy, so I’d love to think about who could do this on site, basically. So you get the batteries and you assess them presumably to see if they’re suitable for this. Sort of like, what’s the criteria and how many batteries make it through?&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;Yeah, the assessment, I think it’s less of an intense torture test than most people imagine. And the reason for that is you don’t need that much cycle life out of a used battery in order for it to make economic sense to put it into service. So you don’t really need perfect understanding of its state of health. You just need to know it’s good enough. And so that’s something — we’ve built some specialized hardware to do that. It’s less than a minute per battery.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;Oh, so you get it, you plug it in, you get a thumbs-up or thumbs-down.&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;Red light, green light.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;And how many batteries get sort of rejected versus accepted, do you know?&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;We do know. I’ll have to say we’re constitutionally sort of incapable of rejecting things. So first pass, it’s high 90s that we’re accepting.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;Ah, so most batteries then will have some juice left.&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;Most batteries are good enough, and that doesn’t mean they’re great, but they’re really good enough to deploy. And then we’re always looking at that fraction that got rejected and just saying, “How can we squeeze a little bit more utility out of those?”&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;Maybe this comparison you don’t have in the top of your head, but relatively speaking, for a given battery, say an NMC battery, what is the value of this remaining capacity you’re getting out of them versus the value of the metals that you recover out of them?&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;Yeah, well, obviously it varies and we’re learning, but equal to or greater and maybe several times greater.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;Oh, interesting. So this is not a marginal bit of value creation. Then you’re getting, you’re 2x-ing the value you get out of these batteries by doing this?&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;Something like that. Which is like, “Oh my goodness, look at this utility that we were rushing into the recycler. Let’s go and take a detour and capture it.”&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;Right, right. So tell me just a little bit about, from an engineering perspective, how you design a system when you know that these batteries, let’s say relative to a typical grid-scale battery installation, you’re going to be mucking with these a lot more. You’re going to be adding them and removing them more frequently than a typical battery installation. How do you design around that to try to keep that cost — because I’ll just say up front, like when I first heard about this, and I’m sure a lot of people say this, like my first thought is just like, “That just sounds like a lot of logistics, like a lot of people, a lot of forklifts, a lot of space, just a lot of logistics.”&lt;/p&gt;
    &lt;p&gt;So it must be clear to you that you’re getting more value out of it than the logistics. But how are you trying to minimize the logistics of being this active — taking things in and out?&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;Yeah, well, the short answer, and maybe not that helpful, is you design for it. So you know that you’re going to have to swap packs more often than in a brand new energy storage site with new cells. And so you provision for that in your design. So, yeah, there’s some small amount of forklift traffic. You make sure it’s really simple to access packs, that it’s really simple for an operator to connect and disconnect packs, to lift them, to move them. And I think one piece of this that is related to that is that these sites are slightly lower energy density than maybe a brand new purpose-built energy storage site. And that really eases service.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;How do you mean? Like, what do you mean safety-wise?&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;No, no, just in terms of the megawatt-hours per acre. They’re a little more spread out, and part of that is service and planning for service.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;So you just have to literally physically lay them on the ground. You have to space them out enough that you can get to them, basically?&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;Yeah, it’s actually — I think of it less as a requirement of second life energy storage and it’s more one about like what is the value of a really dense energy storage site? For some sites it’s really important: if you’re in a city, if you’re close to some infrastructure where land is expensive, it’s really important. But in many, many sites it’s less important and you would actually get a lower cost dollars per kilowatt-hour installed with less energy density. And that was one important realization.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;Because of lower labor costs, because of easier — it’s just easier to use, easier to update.&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;Just simpler. The closer you pack cells together, the more careful you have to be because they do contain energy.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;Well, let’s talk about safety then. Because I was going to get to that later, but I’m assuming that, like, because these are spaced out so widely and they’re just sort of individually attached, it’s kind of difficult for me to imagine any sort of large-scale fire or any danger really. Is there safety? What is the, what are the safety considerations here?&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;So top of the list for us, of course, designing this thing is safety. And our guiding light has always been passive safety. So if you do have a fire for whatever reason, that it stops on its own, that no fire response is needed, no sprinklers are needed, you’re absolutely welcome to. We encourage people to do that, but it’s not necessary. So it’s a passive feature of the system.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;So this individual battery that’s hooked up could theoretically catch on fire, burn up, and be done — and that’s it. And who cares, basically.&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;That’s exactly right.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;Has that happened? Have you had any thermal runaways yet? I’m always curious what the actual percentage of that happening is.&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;No, we haven’t. It’s really rare. I mean, it’s impactful and it’s important when it does. And I think Moss Landing in particular really sticks in everyone’s imagination.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;It really does. That’s why I have to ask this question anytime I talk to a battery company.&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;Yeah. And that’s real. That was a real — that was a bad outcome. But we are designed to avoid anything like that.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;How long have you had enough batteries come through yet that you’re able to generalize about kind of what you’re getting out of them? Like, what is the average lifespan of one of these?&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;Yeah, we really don’t know, to be frank. But what we do know is that it’s long enough to be a good business, that if we deploy them for a year, for 500 cycles, something like that, that makes sense. And we know that that’s well within reach.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;But presumably this little box is recording lots of information about these. So you are gathering tons of data, presumably tons of data about the performance of these used batteries?&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;Yeah, absolutely. I think we’re going to be the clearinghouse for what actually happens at the end of life for EV packs. But it takes a long time. They’re pretty robust. So even after you have rightfully retired one from your car for being slow and having a short range, there are many, many, many years of capacity remaining at that thing.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;80% is like the typical industry — that’s where you sort of cut off the EV battery usefulness. When it gets down to 80%?&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;Yeah, that’s a norm. Depends on the customer. It’s really their choice.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;Sure, but 80% is a lot.&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;Exactly.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;You sort of already answered this, but I’ll ask it anyway: So right now you’re going to places where land is not at a particular premium. And so there’s no real reason not to spread out, as you say. You might even get a little better unit economics spreading out more. I wonder, do you expect all future second life grid-scale storage to be in places where land is not a premium? Because what I’m wondering is, are there engineering ways to get them closer together? Could you ever figure out how to stack them? Like, they are very spread out.&lt;/p&gt;
    &lt;p&gt;And like the first thing that came to mind was, if this becomes really valuable and really common, somebody is going to figure out how to squish them closer together. Is that something you think about at all?&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;We do think about it and we in fact have already done it. So our architecture is flexible, so we really work with our customer to understand the value of density to them. And if it’s high, then we can provide a much denser product.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;You can do it. Is there stacking? Can you stack them at all? Go vertical at all? Because this is the thing, they just spread out horizontally. And I can’t help thinking, if you could just go up a couple of stories, you could fit so much more.&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;Yeah. There are a bunch of different ways to do it, and it just starts to get a little more complicated. You’ve got to build the tower for them to live in and figure out how to get them in and out. It’s all possible. It all makes sense in certain circumstances, but not all. So we work with the people who are buying these to figure out what makes sense.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;Is the idea here that you will be the developer and the owner of the battery in all cases, or are you selling, will someone else own and operate them? What’s the business model exactly?&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;I mean, this industry, there’s probably every business model under the sun available, and we’re open to pretty much all of them. The one thing that is important to us is that we recycle the packs. That’s part of our mission. It’s part of what we think is important to do in North America, and we intend to do that.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;But that sort of suggests that the model that makes the most sense is for you to just be sort of having them and owning them and running them the whole time. Or is the idea that you could ship a bunch of these to a separate site, they set up one of these, and then they ship them back to your facility when they’re done with them, but they own them while they’re in the battery?&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;There’s — I’m not trying to be cute — you could imagine a million different ways to structure that sort of contractual relationship. And in the end, the batteries, we still would like to receive them for recycling.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;I guess it’s more like the physics or logistics of it. I’m sort of wondering about. Because doing this right next to the plant where they’re recycled makes just a ton of sense. Obviously, there’s very little logistics, but once you have one of these set up in a spot that is remote from the recycling facility, then the fact that packs are coming in and out frequently becomes a little bit more of an issue. Because then you have a sort of a supply chain between you and a remote facility. That doesn’t daunt you at all?&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;You have to think about it. But this is not like rail cars and rail cars of packs going back and forth, even from a giant energy storage site. It’s like a few. And at the beginning, while we are really pioneering this and doing it for the first time — I think it makes total sense for us to do it ourselves. And we will. But I don’t think that’s necessary long term.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;So one of the claims your CEO J. B. Straubel makes in his sort of introductory video is that this is going to be cheaper than grid-scale storage with new lithium-ion batteries. And I guess the calculation there is — on one hand, your business model works as is. So you have the batteries from the perspective of this new division basically for free. Like they’re already there. You’re already gathering them. So in a sense, even if they’re lower quality than new batteries, they’re just sitting there. So, in a sense they’re cheaper that way.&lt;/p&gt;
    &lt;p&gt;But on the other hand, you have all this logistics, you have all these forklifts and trucks and setting up and the space and site and everything else. So I’m just wondering how the costs balance out. Is there an easy comparison between a new second life battery and a brand new battery? Like a cost comparison?&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;Yeah, I think as you pointed out, the cost of energy is low compared to a brand new battery because it’s degraded, it’s been out in the world, it’s not a fresh battery. The balance of system, we’re actually making really important strides there too. So the power electronics, the mechanical assembly, there’s a lot of cost in these sites around what people call the EPC cost — engineering, procurement, construction — civil works, getting the batteries plugged in, wiring the whole thing up. And we spent and are spending a lot of time optimizing sort of the rest of the cost stack.&lt;/p&gt;
    &lt;p&gt;So not just leaning on the fact that our energy is relatively low cost because it is second life energy, but really pushing hard on power electronics. We have that whole team in-house. Push down the dollars per watt of power electronics, make it a really, really simple, really, really robust thing to deploy. And that’s underappreciated about what we’re up to — it’s not just that the batteries are cheaper, the whole system is simpler.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;Well, that’s, I mean, it’s the whole system that first jumped up when I started reading about this. Like obviously that’s where, like, you found an incredibly cheap source of batteries, i.e. a big pile of batteries you already had in your backyard. But like, it’s all, all the costs are the logistics and the balance of system stuff. So you think you have running room there to bring those costs down substantially?&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;Absolutely.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;But you can claim that you are beating a new lithium-ion grid-scale battery storage system on the sort of per kilowatt-hour basis already?&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;Yes, we can, and that’s installed over life. So there’s a little bit of pack replacement that has to happen.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;Do you have a number? What are you pitching to people when you’re trying to sell this?&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;I do have a number, but it’s enough to make it a really good, great business, I think, is what I will say here.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;Okay, but like, sort of legendarily, a new lithium-ion battery grid-scale storage — they sort of will do four hours, some of them will do eight hours. Some people now are pushing the economics because batteries are getting cheaper. They’re pushing the economics out to maybe 12 hours at the outer edge. If your unit costs are that much cheaper, how — what kind of durations do you think you can get given your cheaper system?&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;You’ve actually nailed it there. So because energy is cheaper, the balance of plant, which we are making great progress on, like power electronics, the cost of power starts to become more important. And so we have a real advantage for eight-hour and beyond energy storage. We can play and compete and win at 2-hour and 4-hour on total installed cost over life. But it starts to look really compelling at the longer durations.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;Oh, interesting. Can you do 24 hours? The Holy Grail. Could you make something baseload out of this?&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;Absolutely. The original modeling that we did for this was around a 24-hour system. Turning renewables into baseload. Looks really good.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;I’ve been told by many very confident-sounding people, Colin, that that’s impossible. So you say you did it. So do you envision the next plant you build and the next one after that — do you envision hooking these up to the grid and having them sort of make money off providing services to the grid, arbitrage, frequency, et cetera, et cetera? Or do you primarily envision doing what you’ve done with this first installation, which is going off-grid? You got solar panels, you got batteries, you got data centers. It’s all hooked up together, it’s all self-contained and running on its own, and does not need one of these very difficult-to-get grid connections.&lt;/p&gt;
    &lt;p&gt;Which model do you think is going to dominate?&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;We are going to do all of the above. We can and we are doing sites for grid services. Just plug a battery into the grid and help support the grid by participating in the market. We have other “microgrids.” They’re not so micro anymore. They’re many, many, many megawatts.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;Go ahead and brag. This one you built? Yeah, it’s Nevada. I forget — it’s the biggest microgrid in America? You are claiming North America.&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;It’s the largest microgrid in North America and it’s the largest second-energy storage site in the world. So that’s like you said at the top, it’s a 12-megawatt AC, 63-megawatt-hour grid supporting about 2 or 3 megawatts of data centers and run by solar. So all the energy comes from another 12 megawatts of solar.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;And the grid does not touch it. There’s no transmission grid connection at all?&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;That’s exactly right.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;What a beautiful thing. I’ve also been confidently told that those things don’t work economically. Exciting that it is working. And you’ve probably answered this already. But in terms of grid services, you hook one of these things up to the grid, there’s nothing that new batteries can do that these batteries can’t do, right? In terms of frequency regulation and all this kind of stuff. Grid forming, whatever. The kind of things that the batteries and inverters can do.&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;Virtual inertia, all of that. I mean, one of the things that is very topical right now is the data center market, of course, like a giant increase in the demand for power.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;Indeed.&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;And there’s just this beautiful symbiosis between energy storage and data centers. Sort of no matter what your primary source of power is. So much of it is the, there’s already huge racks of batteries and uninterruptible power supplies inside the data center. Already these big diesel generators. We can make both of those things obsolete.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;So you now have two little data centers running. I mean, the data center company technology that you’re using at this site is also pretty interesting on its own merits. But they’re building basically what they call modular data centers. And I’ve been wondering when someone’s going to do this because everybody, it’s kind of the same arc nuclear went on at first. Everybody’s like economies of scale. Economies of scale. Bigger, bigger, bigger, bigger, bigger, big, big, big. And you’re getting these data centers now that are like small cities, like gigawatt-plus data centers. And I have part of this podcast premise to hunt great faith in modularity and repeatability and manufacturability competing against those economies of scale.&lt;/p&gt;
    &lt;p&gt;So this company is making data centers at this site in Nevada, is making these basically like containerized, modularized data centers. Like a 1-megawatt data center with all the power, electronics, cooling, etc., contained in the box, basically.&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;That’s our partner Crusoe, we love them, they’re super smart and they’re doing exactly what you’re talking about, which is: how do we drive down the cost of a data center? We mass-manufacture it.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;Mass manufacture and distribute so it’ll be distributed just like everything else. And then you can wedge a little bit of data centering here and a little bit of data centering there rather than having to look for a mega site. But anyway, that’s probably a whole separate pod on its own. But yeah, but again, so there’s — so you could, in terms of building these sort of like off-grid, self-contained solar plus battery plus use — whoever’s in that use, that could be any manufacturing, be a factory, be anything?&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;Could be anyone who needs power. So data center is obviously top of the list right now. You can go set up a data center quite quickly and run it off solar and storage while you wait for your grid interconnection. If you’re planning to put a natural gas turbine, while you wait for that turbine to show up, those are actually — can be at the power levels we’re talking about, they can take a long time.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;Yes.&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;So we can help sort of blaze the trail. And then there’s really a tremendous amount of utility and, like I said, symbiosis between data centers and energy storage.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;Well, just speed too, right? Like the top — you know this — the top thing hyperscalers are saying, the top thing data centers want is speed to market. They hardly care about the cost, they just want to get hooked up, time to power. And I’m guessing, tell me if I’m wrong, but I’m guessing you can build one of these things, a solar field and a battery field attached to a manufacturing facility, faster than any large-scale centralized power plant. Is that —&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;That is turning out to be true.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;How fast? Do you have a number yet? You’ve only built one.&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;I mean it obviously depends so much on scale, but 12 months, 18 months.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;How long did it take to build the one you built?&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;I mean, the one we built from empty dirt pad to fully commissioned was like four months.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;What?! A four-month timeline for a fully operational 24/7 power supply.&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;This is where the simplicity of install is so important.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;Yeah, so again, you’re pushing away at that, the balance of system, the speed, etc. But again, already four months is just wildly faster than anything else available. Have you learned anything — I mean, this thing, this one you’ve got running has only been running for two, three months. It’s relatively new, so I’m sure there’s lots more to learn. But have you learned anything about the performance of these things in the field? Like, are you getting data that is updating any of your priors? Is anything surprising you about what’s actually happening out there?&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;So far, no surprises. So we’re confirming a lot of the things that we had modeled out. We have a really excellent physics-based modeling team. So I’ll give you one example. A lot of EV batteries are liquid cooled. So if you are driving around in your electric car, your cells are cooled by fluid that runs through a radiator. And we don’t use that system because of the complexity that’s associated with pumps and pipes and fluids and all of those things. And we now know for a fact that that is absolutely fine in the middle of the desert in the summer.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;So most of the overheating probably comes from being contained in a hot car over hot pavement. In that environment, they just won’t overheat if you’re sitting them out in the open.&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;It’s really about the rate that we are pushing power into them.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;Oh, I see.&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;So if we’re discharging, say at the fastest, over two hours, that’s pretty chill for a battery compared to showing up at a supercharger and trying to fill it in 20 minutes.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;Again, this is out to pasture again. These are old horses that are being treated gently.&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;So it’s working exactly as we expected it to.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;And you’re not worried at all that in the fullness of time, because batteries, new batteries are legendarily rapidly falling in cost, you’re not worried at all that at some point that process is going to undercut your economics. Do you think you’re going to stay ahead of that?&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;Yeah, well, I think a used battery is always going to be cheaper than a new one. I’m pretty confident in that. And the other piece of it is what I talked about a little bit earlier. We’re not sitting still on the non-battery parts. So we’re really, we have a great team, power electronics, mechanical engineering, software, all of that pushing down the cost of all the things that aren’t the battery.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;Is there a future where you include other kinds of non-automotive batteries in this system? Like, is there another kind of battery that has enough juice in it to make it worth the squeeze?&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;I think so. I mean, we’re pretty agnostic to battery type. So it really comes down to sort of acquisition cost, in the end.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;I’m just sort of assuming that like a toothbrush battery is just not going to be worth getting the human to walk out there and plug it in.&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;Let me backpedal. I don’t think ever that you’ll see like a matrix-like field of AirPods or toothbrushes.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;So what’s the general cutoff? Like a weed whacker battery, like how big before...?&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;I don’t know, but I bet somewhere in the like 10 to 14 kilowatt-hour range, somewhere in there it’ll start to make sense. So maybe light hybrids, but not weed whackers.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;And not toothbrushes. So in terms of the logistics of gathering the batteries, I’m assuming that’s all already up and running. That’s what you’ve been doing for the last few years. Is your ability to do this with the batteries going to change that side of things — how you gather them? I know there’s some sort of partnership with GM, it’s a little vague on the website, exactly what that consists of. What is that?&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;Well, I think we’ve really put more effort into exactly those kinds of partnerships with automakers who sometimes have the packs at their end of life and need a place to send them.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;Do they, though? Why would they, like, why would GM end up with used GM batteries?&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;Well, sometimes. So some, some packs —&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;I’m so curious, what happens to a battery — like if my battery runs out, I go to a — I don’t know, a mechanic or whatever and get a new one. I have no idea what happens to it. Tell us just a little bit about what that looks like?&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;Everything under the sun happens to it. So if it’s a dealer, then that’s one, like, pretty consistent path. If it’s a shop, your local EV service shop, it’s actually been a challenge for them. And it’s one thing that, well before this energy storage business started, that we’ve been working hard to make as easy as possible. So we actually have sort of a — I don’t know what you want to call it, like a reverse Amazon — to make it super easy to send big batteries like this to us to figure out what they’re worth and get them on their way.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;So they mail them to you, basically, they ship them to you.&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;Yes.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;Interesting. And that — I don’t know, that just seems so expensive when I think about it. Again, so much logistics. It’s amazing how much logistics is involved in your business model.&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;Logistics is a real thing. It’s also remarkable how efficient it is to move mass around. Transportation costs for a battery are not a giant piece of the concern. It’s really the complexity of just like there’s a million places these end up — junkyards, people’s houses, dealerships, everything you can imagine.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;What is the GM partnership, the newer GM partnership? It caught my eye, and the reason I ask about it is because I thought it would just be them getting you their old batteries. But it also says something in the press release about them maybe sending you new batteries also to be included in these power plants. So I was wondering about that — if there would ever be any reason for new batteries to wander into this milieu.&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;Yeah, well, maybe a couple things. GM’s been tremendously collaborative with us as we embark on the second life project. And we appreciate that. It’s very helpful to have their support as we do it. And then not specific to GM, but you do have to ask yourself the question “Why do these batteries have to be used?” And if we can do a good enough job on the balance of system costs, then they don’t. That’s not our focus right now, but it’s something that definitely is on my mind.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;Interesting. So theoretically, you could end up being just a very competitive grid storage company that is agnostic to the source of batteries?&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;Could be.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;Someday, maybe.&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;The thing that we’re getting good at right now, and it’s still — it’s still early days, I want to emphasize that for us — is being uniquely capable of integrating all of these zillion different battery types. So this, like, heterogeneity is something that we’re really good at.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;Which raises a question I’ve had about Redwood forever. Recycling really forever. Now that you are big enough to sort of throw your weight around a little bit, you’re a big player in this market. Have you initiated any kind of talks with battery designers and manufacturers about trying to get batteries to be designed more for recyclability on the front end? And now that I think about it too, that would probably — like, the more sort of uniform they get and the easier to recycle it gets, I feel like that would also have the effect of making them easier to do this with, to integrate into these things with. Are you at all trying to influence the design of batteries on the front end?&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;That’s a great question. And I confronted that a little bit in my prior life where I was designing EV battery packs.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;I think Tesla’s actually famous for designing batteries that are very not that. They’re very integrated into the car. They’re very — not particularly recyclable.&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;I’ll say a couple things about that. One is, when you’re doing things the way that we need to do them to make an impact in the world at industrial scale, having packs be disassembled by hand or even with automation, the sort of design for recyclability, that’s pretty expensive. And so I think the recycling processes that win are going to be something that is at least philosophically like backing up a truck full of packs and like dumping them into a machine.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;Are you physically disassembling all the batteries now? Before you recycle them in the recycling process.&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;Right now I try not to. I try to send them into energy storage.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;You hire people to do that.&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;But no, our processes don’t require a great deal of disassembly because every pack is different, and I think every pack is rightfully different in order to make a great car or a great truck or whatever it is, it’s hard to make those things.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;So you think standardization is going to happen on the recycling end, not on the design end, basically?&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;I think so. And I think the processes for separating out all of the metals that are contained in a pack are going to work fine, regardless of how the pack was built.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;Yeah, yeah, I need to do another pod just on the recycling stuff. By way of wrapping up, let’s look out a little bit into the future. When I brought this up online, a lot of people’s first reaction was, “Well, that seems like kind of a boutique distraction from the core business that Redwood’s in.” And the more I looked into it, I was like, “Well, there’s like two gazillion batteries on the way to recycling.” Again, we’re just at the front end of this. They’re just sort of trickling in now, I guess you’d say, in what is going to become a flood.&lt;/p&gt;
    &lt;p&gt;So you could see this growing and expanding quite a bit. I just wonder how you see this division relative to the recycling division. I mean, you sort of touched on this earlier. Do you think this will be, in the fullness of time, a comparably sized part of the business or even bigger than the recycling part?&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;Absolutely. There’s plenty of batteries out there in the world that are going to need to be reused. Like you — you hit on it. It’s like, what is North American EV pack production per year today? It’s 100 gigawatt-hours, 150 gigawatt-hours per year. And then the entire storage, I think, deployed last year in the US was — was half of that. So, the EV market is, for the moment, quite big compared to the storage market. They’re both growing, but —&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;EVs are growing faster. I mean, I’m assuming EVs are always going to dwarf stationary storage.&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;Storage is catching up. I think it’s about a quarter of the cells in the US that go into storage.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;Really? That’s bigger than I would have guessed.&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;But the point is there’s plenty of used packs. We know that it’s inevitable they’re going to come back. So that’s to the like, is it a boutique distraction? No, I think it’s a real durable business. And to the question, is it a distraction? We are still quite committed to the metals business. I think that’s still so exciting and so important to bring back to North America.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;And this does seem like, to me at least, this seems just to fit right in. Like in a sense, it’s almost like a version of recycling. You’re sort of recycling power. You’re recycling the power before you recycle the metals.&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;I was trying to think about how to say this.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;But in the same way, you’re preventing new stuff from having to be made. I mean, in a sense, in both cases, you’re preventing the necessity for new stuff.&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;Yeah, I mean, the need for new stuff is gigantic. And so in order to fuel a complete electrification of the economy, there’s still a lot of new stuff we’re going to need, but we can in the short term blunt that a little bit and make it cheaper by extracting really every last drop of utility, whether it be energy storage capacity, whether it be the metals out of old things. And then if you fast forward 100 years, like it’ll all be circular.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;So this is, especially in these depressing current times. These are among, among the places I go, my happy places is thinking about this 100-year time horizon and total circularity. And it’s interesting to think how this fits into that picture, basically. You can imagine a world of full circularity vis-à-vis metals, right, where we no longer are having to dig up any new metals, basically. We’ve reached a steady state. We’re perfectly recycling all the metals that come through.&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;You can. The thing I want to emphasize is that that also just makes economic sense. It is a cheaper way to provide nickel and lithium in addition to being sort of lower carbon and all of the other things that circularity provides.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;Yes. Less grim labor implications, all that kind of stuff as well. But I was sort of thinking about how this fits into that. Like if you’re getting all the world’s batteries and recycling them and you’re sucking all this capacity out of them before you recycle them, you’re also like, that’s a big, potentially a big chunk of the energy storage we need. I don’t know if anybody’s tried to do this kind of math, like 20 years out or whatever, 50 years out. Like what, what sort of — like how much of the stationary storage market in the US do you think could ultimately be served by second life batteries?&lt;/p&gt;
    &lt;p&gt;This is all napkin math, of course.&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;It’s a good question. So today we can serve like 10, 15% of the market.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;Really? Already with the batteries you’re receiving today?&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;Yeah, so we’re receiving something like 5 gigawatt-hours a year. And I think the US put 50 into service this year.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;5 gigawatt-hours a year. And this plant you built, we should say, this off-grid plant, it’s like 63, not, it’s not even 100 megawatt-hours. So to really scale this up to start matching the input, that’s just going to be a lot of land, isn’t it? You’re not worried at all about the land?&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;I mean, this is like, this is a version of “Am I worried about the land use of solar?” And no, turns out you get a lot of, a lot of energy density coming from the sun. A lot of power density, sorry. And if you look at the size of — we can even look at our pilot plant in Nevada. If you look at the size of the battery compared to the size of the solar array, it’s 1/10 the size.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;1/10?!&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;So, and that’s, we could do better. We could do 1/20 or 1/30 if we wanted to.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;Ah, so you can shrink down the size of the battery thing. Can’t really shrink down the size of the solar.&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;Yeah, exactly. So, it’s not that big.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;So for the off-grid applications, it would be the generation. That would be the land, the real land issue, not so much the batteries. What’s next? We pull our view out from 100 years out to say five. What’s next? What’s the next plant? Do you imagine being sort of the owner and developer of the next few of these as you kind of prove them out and show they can work?&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;What’s next is much bigger projects. So we have several of those lined up already in the 5 to 10 and bigger x the one we put in Nevada. So substantially bigger than our pilot site in Nevada. And so we’re really focused on executing those and delivering those.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;And are those in Nevada?&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;They’re all over the country.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;Really? I think of the vast west desert as where you find these big swaths of cheap land. Are you building — you have your eye on the east coast, the northeast, congested areas too?&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;We have projects — I’m sort of like doing the Rolodex in my head here — not quite everywhere in the country, but not just in the desert west, to your point.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;Interesting. And I’m sure some of this is confidential, but for the projects in your pipeline, are they all off-grid, attached to solar in some sort of manufacturing facility, or are some of them going to be grid-connected?&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;Some of them will be grid connected. Like if you can imagine a place where somebody has put a utility-scale battery, we have a project like that. We have grid-connected grid arbitrage. We have behind the meter, front of the meter. We have paired with manufacturing, paired with renewables.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;Any energy storage plays, basically.&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;Exactly. Same energy storage as any other kind.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;As I think about the logistics, new things keep coming to mind. But in terms of your battery gathering, how many sites do you have where you’re gathering batteries? Because wherever you build one of these, you are establishing basically an ongoing relationship to some source of these batteries, right? A supply chain. You’re going to have to have batteries continually shipped between where they’re gathered and where you have this plant. How many of those battery gathering sites are there?&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;As this becomes a national project for us, we need sort of regional warehousing.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;Is it all in the one Nevada plant at this point?&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;It’s not. We also have a big site that we’re developing and actually just opened our first recycling plant there in South Carolina. So that’s a strategic spot for us on the east coast, near a port, near a lot of industry. And that’s another logical place for us to inventory this stuff. But I imagine, in the fullness of time, we will put them where it makes sense to support these customers.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;I just keep trying to wrap my head around the shift that you are about to undergo. Like right now you say you got 5 gigawatt-hours a year of batteries incoming.&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;Gigawatt-hours.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;Yep, gigawatt-hours, sorry. I’m assuming that the line is going up and to the right.&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;Yes, it is.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;How fast per year? Like, what’s next year? How steep is that line? I mean, I’m assuming it’s just going to get real steep relatively soon. But I don’t know if you’ve actually charted it out and plotted it out.&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;Yeah, it’s a little hard to predict, but we’ve seen 50% year-on-year growth at the beginning of this wave. And you can look back at the beginning of EV manufacturing in 2012 in volume. What was the ramp rate? And we’re going to see something like that.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;Interesting. So we’re going to go from like 5, 10. Like having 100 GWh of batteries coming through is not crazy. Not out of the...&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;Yeah, I mean, you can almost just predict it. We’re making 100 gigawatt-hours today, so in 10 years we’ll be seeing those come back ready for second life.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;And so then I just, at that level of volume, getting more power out of them will in no way be a small thing. I think this is what J. B. Straubel was kind of trying to get at in his big talk — this may look like a side thing today, but it’s going to be enormously important. If you’ve got 100 gigawatt-hours coming through, that’s a lot of power to just be throwing away. So it’s just going to be a lot more significant in five years.&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;It really will. So we are laying the groundwork for this. This team knows how to build things at scale. We have done it before. The business and the power electronics, the mechanical bits that we’re designing for energy storage, they’re all designed for mass manufacture, for simple assembly in the field. Because we know that this is coming — tens and hundreds of gigawatts.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;Well, I love to see someone getting ahead of the puck. As I look around these days at US politics, it’s nice to see someone anticipating what’s going to happen and getting ready before it happens. Just a beautiful thing to witness.&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;Thank you. We’re having fun. We love finding something valuable that hasn’t been extracted and going after it. And this is just another example of it.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;Yeah, it’s just a bunch of puzzles. I mean, all this second life stuff is just an enormous logistical puzzle to solve. And solving puzzles is fun.&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;We really enjoy it.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;Colin, thanks so much for coming on and talking us through this. Really interesting. I think it’s a lot bigger of a deal than it sort of appears on the surface. So it was fun to — fun to dig in a little bit with you. Thanks for coming on.&lt;/p&gt;
    &lt;p&gt;Colin Campbell&lt;/p&gt;
    &lt;p&gt;So fun to be here. Great questions. Really enjoyed it.&lt;/p&gt;
    &lt;p&gt;David Roberts&lt;/p&gt;
    &lt;p&gt;Thank you for listening to Volts. It takes a village to make this podcast work. Shout out, especially, to my super producer, Kyle McDonald, who makes me and my guests sound smart every week. And it is all supported entirely by listeners like you. So, if you value conversations like this, please consider joining our community of paid subscribers at volts.wtf. Or, leaving a nice review, or telling a friend about Volts. Or all three. Thanks so much, and I’ll see you next time.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45685007</guid><pubDate>Thu, 23 Oct 2025 18:15:45 +0000</pubDate></item><item><title>Show HN: OpenSnowcat – A fork of Snowplow to keep open analytics alive</title><link>https://opensnowcat.io/</link><description>&lt;doc fingerprint="1f1f8b543076746"&gt;
  &lt;main&gt;
    &lt;p&gt;OpenSnowcat is an open-source fork of Snowplow under Apache 2.0 License, &lt;lb/&gt;fully compatible with Snowplow and Segment SDKs.&lt;/p&gt;
    &lt;p&gt;Free and Open-Source Forever&lt;/p&gt;
    &lt;p&gt;Provide existing and new users with a widely-accepted license that companies can trust and that won't change in the future.&lt;/p&gt;
    &lt;p&gt;Integrated &amp;amp; Cost-Efficient&lt;/p&gt;
    &lt;p&gt;Integrate with cloud services that simplify managing the platform and running cost-efficiently and reliably at scale.&lt;/p&gt;
    &lt;p&gt;Secure, Stable over Time&lt;/p&gt;
    &lt;p&gt;We value and prioritize security, stability, and business continuity. Maintain the components with the latest security patches and packages.&lt;/p&gt;
    &lt;p&gt;Backwards-compatible &amp;amp; Portable&lt;/p&gt;
    &lt;p&gt;Compatible with Snowplow and other SDKs so that existing implementations can continue to drive value and seamless portability from and to Snowplow.&lt;/p&gt;
    &lt;p&gt;Easier to Install &amp;amp; Maintain&lt;/p&gt;
    &lt;p&gt;Streamlined setup, consistent performance, and effortless management, empowering teams to focus on driving value rather than handling infrastructure complexities.&lt;/p&gt;
    &lt;p&gt;Scalable and High-Performing&lt;/p&gt;
    &lt;p&gt;Designed to handle large-scale real-time workloads with ease, ensuring optimal performance, minimal latency, and the ability to scale dynamically as your business grows.&lt;/p&gt;
    &lt;p&gt;No spam, strictly product updates, once a month or less.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45685793</guid><pubDate>Thu, 23 Oct 2025 19:24:04 +0000</pubDate></item><item><title>Zram Performance Analysis</title><link>https://notes.xeome.dev/notes/Zram</link><description>&lt;doc fingerprint="f2e918dba5296b1d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Zram Performance Analysis&lt;/head&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;Zram is a kernel module that utilizes a compressed virtual memory block device allowing for efficient memory management. In this document we will analyze the performance of various compression algorithms used in Zram and their impact on the system. We will also discuss the effects of different page-cluster values on the system’s latencies and throughput.&lt;/p&gt;
    &lt;head rend="h2"&gt;Compression Algorithm Comparison&lt;/head&gt;
    &lt;p&gt;The following table compares the performance of different compression algorithms used in Zram, in terms of compression time, data size, compressed size, total size, and compression ratio.&lt;/p&gt;
    &lt;p&gt;Data from Linux Reviews:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="6"&gt;
        &lt;cell role="head"&gt;Algorithm&lt;/cell&gt;
        &lt;cell role="head"&gt;Cp time&lt;/cell&gt;
        &lt;cell role="head"&gt;Data&lt;/cell&gt;
        &lt;cell role="head"&gt;Compressed&lt;/cell&gt;
        &lt;cell role="head"&gt;Total&lt;/cell&gt;
        &lt;cell role="head"&gt;Ratio&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;lzo&lt;/cell&gt;
        &lt;cell&gt;4.571s&lt;/cell&gt;
        &lt;cell&gt;1.1G&lt;/cell&gt;
        &lt;cell&gt;387.8M&lt;/cell&gt;
        &lt;cell&gt;409.8M&lt;/cell&gt;
        &lt;cell&gt;2.689&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;lzo-rle&lt;/cell&gt;
        &lt;cell&gt;4.471s&lt;/cell&gt;
        &lt;cell&gt;1.1G&lt;/cell&gt;
        &lt;cell&gt;388M&lt;/cell&gt;
        &lt;cell&gt;410M&lt;/cell&gt;
        &lt;cell&gt;2.682&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;lz4&lt;/cell&gt;
        &lt;cell&gt;4.467s&lt;/cell&gt;
        &lt;cell&gt;1.1G&lt;/cell&gt;
        &lt;cell&gt;403.4M&lt;/cell&gt;
        &lt;cell&gt;426.4M&lt;/cell&gt;
        &lt;cell&gt;2.582&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;lz4hc&lt;/cell&gt;
        &lt;cell&gt;14.584s&lt;/cell&gt;
        &lt;cell&gt;1.1G&lt;/cell&gt;
        &lt;cell&gt;362.8M&lt;/cell&gt;
        &lt;cell&gt;383.2M&lt;/cell&gt;
        &lt;cell&gt;2.872&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;842&lt;/cell&gt;
        &lt;cell&gt;22.574s&lt;/cell&gt;
        &lt;cell&gt;1.1G&lt;/cell&gt;
        &lt;cell&gt;538.6M&lt;/cell&gt;
        &lt;cell&gt;570.5M&lt;/cell&gt;
        &lt;cell&gt;1.929&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;zstd&lt;/cell&gt;
        &lt;cell&gt;7.897s&lt;/cell&gt;
        &lt;cell&gt;1.1G&lt;/cell&gt;
        &lt;cell&gt;285.3M&lt;/cell&gt;
        &lt;cell&gt;298.8M&lt;/cell&gt;
        &lt;cell&gt;3.961&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Data from u/VenditatioDelendaEst:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="7"&gt;
        &lt;cell role="head"&gt;algo&lt;/cell&gt;
        &lt;cell role="head"&gt;page-cluster&lt;/cell&gt;
        &lt;cell role="head"&gt;MiB/s&lt;/cell&gt;
        &lt;cell role="head"&gt;IOPS&lt;/cell&gt;
        &lt;cell role="head"&gt;Mean Latency (ns)&lt;/cell&gt;
        &lt;cell role="head"&gt;99% Latency (ns)&lt;/cell&gt;
        &lt;cell role="head"&gt;comp_ratio&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;lzo&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;5821&lt;/cell&gt;
        &lt;cell&gt;1490274&lt;/cell&gt;
        &lt;cell&gt;2428&lt;/cell&gt;
        &lt;cell&gt;7456&lt;/cell&gt;
        &lt;cell&gt;2.77&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;lzo&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;6668&lt;/cell&gt;
        &lt;cell&gt;853514&lt;/cell&gt;
        &lt;cell&gt;4436&lt;/cell&gt;
        &lt;cell&gt;11968&lt;/cell&gt;
        &lt;cell&gt;2.77&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;lzo&lt;/cell&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;7193&lt;/cell&gt;
        &lt;cell&gt;460352&lt;/cell&gt;
        &lt;cell&gt;8438&lt;/cell&gt;
        &lt;cell&gt;21120&lt;/cell&gt;
        &lt;cell&gt;2.77&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;lzo&lt;/cell&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;7496&lt;/cell&gt;
        &lt;cell&gt;239875&lt;/cell&gt;
        &lt;cell&gt;16426&lt;/cell&gt;
        &lt;cell&gt;39168&lt;/cell&gt;
        &lt;cell&gt;2.77&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;lzo-rle&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;6264&lt;/cell&gt;
        &lt;cell&gt;1603776&lt;/cell&gt;
        &lt;cell&gt;2235&lt;/cell&gt;
        &lt;cell&gt;6304&lt;/cell&gt;
        &lt;cell&gt;2.74&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;lzo-rle&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;7270&lt;/cell&gt;
        &lt;cell&gt;930642&lt;/cell&gt;
        &lt;cell&gt;4045&lt;/cell&gt;
        &lt;cell&gt;10560&lt;/cell&gt;
        &lt;cell&gt;2.74&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;lzo-rle&lt;/cell&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;7832&lt;/cell&gt;
        &lt;cell&gt;501248&lt;/cell&gt;
        &lt;cell&gt;7710&lt;/cell&gt;
        &lt;cell&gt;19584&lt;/cell&gt;
        &lt;cell&gt;2.74&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;lzo-rle&lt;/cell&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;8248&lt;/cell&gt;
        &lt;cell&gt;263963&lt;/cell&gt;
        &lt;cell&gt;14897&lt;/cell&gt;
        &lt;cell&gt;37120&lt;/cell&gt;
        &lt;cell&gt;2.74&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;lz4&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;7943&lt;/cell&gt;
        &lt;cell&gt;2033515&lt;/cell&gt;
        &lt;cell&gt;1708&lt;/cell&gt;
        &lt;cell&gt;3600&lt;/cell&gt;
        &lt;cell&gt;2.63&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;lz4&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;9628&lt;/cell&gt;
        &lt;cell&gt;1232494&lt;/cell&gt;
        &lt;cell&gt;2990&lt;/cell&gt;
        &lt;cell&gt;6304&lt;/cell&gt;
        &lt;cell&gt;2.63&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;lz4&lt;/cell&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;10756&lt;/cell&gt;
        &lt;cell&gt;688430&lt;/cell&gt;
        &lt;cell&gt;5560&lt;/cell&gt;
        &lt;cell&gt;11456&lt;/cell&gt;
        &lt;cell&gt;2.63&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;lz4&lt;/cell&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;11434&lt;/cell&gt;
        &lt;cell&gt;365893&lt;/cell&gt;
        &lt;cell&gt;10674&lt;/cell&gt;
        &lt;cell&gt;21376&lt;/cell&gt;
        &lt;cell&gt;2.63&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;zstd&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;2612&lt;/cell&gt;
        &lt;cell&gt;668715&lt;/cell&gt;
        &lt;cell&gt;5714&lt;/cell&gt;
        &lt;cell&gt;13120&lt;/cell&gt;
        &lt;cell&gt;3.37&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;zstd&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;2816&lt;/cell&gt;
        &lt;cell&gt;360533&lt;/cell&gt;
        &lt;cell&gt;10847&lt;/cell&gt;
        &lt;cell&gt;24960&lt;/cell&gt;
        &lt;cell&gt;3.37&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;zstd&lt;/cell&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;2931&lt;/cell&gt;
        &lt;cell&gt;187608&lt;/cell&gt;
        &lt;cell&gt;21073&lt;/cell&gt;
        &lt;cell&gt;48896&lt;/cell&gt;
        &lt;cell&gt;3.37&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;zstd&lt;/cell&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;3005&lt;/cell&gt;
        &lt;cell&gt;96181&lt;/cell&gt;
        &lt;cell&gt;41343&lt;/cell&gt;
        &lt;cell&gt;95744&lt;/cell&gt;
        &lt;cell&gt;3.37&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Data from my raspberry pi 4, 2gb model:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="7"&gt;
        &lt;cell role="head"&gt;algo&lt;/cell&gt;
        &lt;cell role="head"&gt;page-cluster&lt;/cell&gt;
        &lt;cell role="head"&gt;MiB/s&lt;/cell&gt;
        &lt;cell role="head"&gt;IOPS&lt;/cell&gt;
        &lt;cell role="head"&gt;Mean Latency (ns)&lt;/cell&gt;
        &lt;cell role="head"&gt;99% Latency (ns)&lt;/cell&gt;
        &lt;cell role="head"&gt;comp_ratio&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;lzo&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;1275.19&lt;/cell&gt;
        &lt;cell&gt;326448.93&lt;/cell&gt;
        &lt;cell&gt;9965.14&lt;/cell&gt;
        &lt;cell&gt;18816.00&lt;/cell&gt;
        &lt;cell&gt;1.62&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;lzo&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;1892.08&lt;/cell&gt;
        &lt;cell&gt;242186.68&lt;/cell&gt;
        &lt;cell&gt;14178.77&lt;/cell&gt;
        &lt;cell&gt;31104.00&lt;/cell&gt;
        &lt;cell&gt;1.62&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;lzo&lt;/cell&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;2451.65&lt;/cell&gt;
        &lt;cell&gt;156905.52&lt;/cell&gt;
        &lt;cell&gt;23083.55&lt;/cell&gt;
        &lt;cell&gt;56064.00&lt;/cell&gt;
        &lt;cell&gt;1.62&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;lzo&lt;/cell&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;2786.33&lt;/cell&gt;
        &lt;cell&gt;89162.46&lt;/cell&gt;
        &lt;cell&gt;42224.49&lt;/cell&gt;
        &lt;cell&gt;107008.00&lt;/cell&gt;
        &lt;cell&gt;1.62&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;lzo-rle&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;1271.53&lt;/cell&gt;
        &lt;cell&gt;325511.42&lt;/cell&gt;
        &lt;cell&gt;9997.72&lt;/cell&gt;
        &lt;cell&gt;20096.00&lt;/cell&gt;
        &lt;cell&gt;1.62&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;lzo-rle&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;1842.69&lt;/cell&gt;
        &lt;cell&gt;235863.95&lt;/cell&gt;
        &lt;cell&gt;14627.23&lt;/cell&gt;
        &lt;cell&gt;34048.00&lt;/cell&gt;
        &lt;cell&gt;1.62&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;lzo-rle&lt;/cell&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;2404.35&lt;/cell&gt;
        &lt;cell&gt;153878.65&lt;/cell&gt;
        &lt;cell&gt;23592.19&lt;/cell&gt;
        &lt;cell&gt;60160.00&lt;/cell&gt;
        &lt;cell&gt;1.62&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;lzo-rle&lt;/cell&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;2766.61&lt;/cell&gt;
        &lt;cell&gt;88531.46&lt;/cell&gt;
        &lt;cell&gt;42579.14&lt;/cell&gt;
        &lt;cell&gt;114176.00&lt;/cell&gt;
        &lt;cell&gt;1.62&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;lz4&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;1329.87&lt;/cell&gt;
        &lt;cell&gt;340447.83&lt;/cell&gt;
        &lt;cell&gt;9421.35&lt;/cell&gt;
        &lt;cell&gt;15936.00&lt;/cell&gt;
        &lt;cell&gt;1.59&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;lz4&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;2004.43&lt;/cell&gt;
        &lt;cell&gt;256567.19&lt;/cell&gt;
        &lt;cell&gt;13238.78&lt;/cell&gt;
        &lt;cell&gt;25216.00&lt;/cell&gt;
        &lt;cell&gt;1.59&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;lz4&lt;/cell&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;2687.75&lt;/cell&gt;
        &lt;cell&gt;172015.93&lt;/cell&gt;
        &lt;cell&gt;20807.00&lt;/cell&gt;
        &lt;cell&gt;43264.00&lt;/cell&gt;
        &lt;cell&gt;1.59&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;lz4&lt;/cell&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;3157.29&lt;/cell&gt;
        &lt;cell&gt;101033.42&lt;/cell&gt;
        &lt;cell&gt;36901.36&lt;/cell&gt;
        &lt;cell&gt;80384.00&lt;/cell&gt;
        &lt;cell&gt;1.59&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;zstd&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;818.88&lt;/cell&gt;
        &lt;cell&gt;209633.97&lt;/cell&gt;
        &lt;cell&gt;16672.13&lt;/cell&gt;
        &lt;cell&gt;38656.00&lt;/cell&gt;
        &lt;cell&gt;1.97&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;zstd&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;1069.07&lt;/cell&gt;
        &lt;cell&gt;136840.50&lt;/cell&gt;
        &lt;cell&gt;26777.05&lt;/cell&gt;
        &lt;cell&gt;69120.00&lt;/cell&gt;
        &lt;cell&gt;1.97&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;zstd&lt;/cell&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;1286.17&lt;/cell&gt;
        &lt;cell&gt;82314.84&lt;/cell&gt;
        &lt;cell&gt;46059.39&lt;/cell&gt;
        &lt;cell&gt;127488.00&lt;/cell&gt;
        &lt;cell&gt;1.97&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;zstd&lt;/cell&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;1427.75&lt;/cell&gt;
        &lt;cell&gt;45688.14&lt;/cell&gt;
        &lt;cell&gt;84876.56&lt;/cell&gt;
        &lt;cell&gt;246784.00&lt;/cell&gt;
        &lt;cell&gt;1.97&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The table presents the performance metrics of different compression algorithms, including LZO, LZO-RLE, LZ4, and ZSTD. The metrics include throughput, compression ratio, and latency, which are important factors to consider for selecting the optimal compression algorithm. We used a weighted sum to evaluate the performance of each algorithm and page cluster combination, with weights of 0.4 for latency, 0.4 for compression ratio, and 0.2 for throughput. The results show that LZ4 with page cluster 0 achieved the highest weighted sum, indicating that it is the optimal choice for this dataset. Overall, this evaluation provides valuable insights for selecting the most suitable compression algorithm for data storage and processing, balancing between compression ratio, throughput, and latency.&lt;/p&gt;
    &lt;p&gt;Code used to calculate weighed sums:&lt;/p&gt;
    &lt;p&gt;Data from me:&lt;/p&gt;
    &lt;p&gt;Compiling memory intensive code (vtm ). Test was done on raspberry pi 4b, 2gb ram.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;algo&lt;/cell&gt;
        &lt;cell role="head"&gt;time&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;lz4&lt;/cell&gt;
        &lt;cell&gt;433.63s&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;zstd&lt;/cell&gt;
        &lt;cell&gt;459.34s&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Page-cluster Values and Latency&lt;/head&gt;
    &lt;p&gt;The page-cluster value controls the number of pages that are read in from swap in a single attempt, similar to the page cache readahead. The consecutive pages are not based on virtual or physical addresses, but consecutive on swap space, meaning they were swapped out together.&lt;/p&gt;
    &lt;p&gt;The page-cluster value is a logarithmic value. Setting it to zero means one page, setting it to one means two pages, setting it to two means four pages, etc. A value of zero disables swap readahead completely.&lt;/p&gt;
    &lt;p&gt;The default value is 3 (8 pages at a time). However, tuning this value to a different value may provide small benefits if the workload is swap-intensive. Lower values mean lower latencies for initial faults, but at the same time, extra faults and I/O delays for following faults if they would have been part of that consecutive pages readahead would have brought in.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;In the analysis of Zram performance, it was determined that the zstd algorithm provides the highest compression ratio while still maintaining acceptable speeds. The high compression ratio allows more of the working set to fit in uncompressed memory, reducing the need for swap and ultimately improving performance.&lt;/p&gt;
    &lt;p&gt;For daily use (non latency sensitive), it is recommended to use zstd with &lt;code&gt;page-cluster=0&lt;/code&gt; as the majority of swapped data is likely stale (old browser tabs). However, systems that require constant swapping may benefit from using the lz4 algorithm due to its higher throughput and lower latency.&lt;/p&gt;
    &lt;p&gt;It is important to note that the decompression of zstd is slow and results in a lack of throughput gain from readahead. Therefore, &lt;code&gt;page-cluster=0&lt;/code&gt; should be used for zstd. This is the default setting on ChromeOS and seems to be standard practice on Android.&lt;/p&gt;
    &lt;p&gt;The default &lt;code&gt;page-cluster&lt;/code&gt; value is set to 3, which is better suited for physical swap. This value dates back to 2005, when the kernel switched to git, and may have been used in a time before the widespread use of SSDs. It is recommended to consider the specific requirements of the system and workload when configuring Zram.&lt;/p&gt;
    &lt;head rend="h1"&gt;Sources and See Also&lt;/head&gt;
    &lt;p&gt;https://docs.kernel.org/admin-guide/sysctl/vm.html&lt;/p&gt;
    &lt;p&gt;https://www.reddit.com/r/Fedora/comments/mzun99/new_zram_tuning_benchmarks/&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45686280</guid><pubDate>Thu, 23 Oct 2025 19:58:12 +0000</pubDate></item><item><title>When is it better to think without words?</title><link>https://www.henrikkarlsson.xyz/p/wordless-thought</link><description>&lt;doc fingerprint="7519513caab092d0"&gt;
  &lt;main&gt;
    &lt;head rend="h6"&gt;Portrait of a Man with Glasses I, Francis Bacon, 1963&lt;/head&gt;
    &lt;p&gt;This essay can be read as a complement to last year’s “How to think in writing.”&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Thoughts die the moment they are embodied in words.&lt;/p&gt;&lt;lb/&gt;—Schopenhauer&lt;/quote&gt;
    &lt;head rend="h4"&gt;1.&lt;/head&gt;
    &lt;p&gt;In the 1940s, when the French mathematician Jacques Hadamard asked good mathematicians how they came up with solutions to hard problems, they nearly universally answered that they didn’t think in words; neither did they think in images or equations. Rather, what passed through the mathematicians as they struggled with problems were such things as vibrations in their hands, nonsense words in their ears, or blurry shapes in their heads.1&lt;/p&gt;
    &lt;p&gt;Hadamard, who had the same types of experiences, wrote in The Psychology of Invention in the Mathematical Field that this mode of thinking was distinct from daydreaming, and that most people, though they often think wordlessly, have never experienced the kind of processing that the mathematicians did.&lt;/p&gt;
    &lt;p&gt;When I read this, in December 2024, all sorts of questions arose in me. First of all, what does it even mean? Do they not think in words and equations at all? And secondly, how do I square this with my personal experience, which is that whenever I write what I think about a subject, it always turns out that my thoughts do not hold up on paper? No matter how confident I am in my thoughts, they reveal themselves on the page as little but logical holes, contradictions, and non sequiturs.&lt;/p&gt;
    &lt;p&gt;I recognize myself when Paul Graham writes:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The reason I’ve spent so long establishing [that writing helps you refine your thinking] is that it leads to another [point] that many people will find shocking. If writing down your ideas always makes them more precise and more complete, then no one who hasn’t written about a topic has fully formed ideas about it. And someone who never writes has no fully formed ideas about anything nontrivial.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;How come Hadamard’s colleagues are able to have productive thoughts, working in their heads, without words, sometimes, for days on end?&lt;/p&gt;
    &lt;head rend="h2"&gt;Tense subconscious processing&lt;/head&gt;
    &lt;p&gt;Hadamard’s book is most famous for its detailed discussion of what Henri Poincaré called the “sudden illumination”—the moment when the solution to a problem emerges “in the shower” unexpectedly after a long period of unconscious incubation.&lt;/p&gt;
    &lt;p&gt;The hypothesis here is that if you work hard on a problem, you soak your subconscious with it. Wrestling with a problem helps you build a mental model of what you know and what you don’t—providing the subconscious with building blocks to work with. (You can’t have genuine intuition and inspiration in areas where you lack knowledge.) Then, once you drop the problem from conscious thought and go take care of the dishes or something, the subconscious begins a silent and parallelized search, trying many, many alternatives (in a somewhat random fashion), until something snaps in place. When this happens, the solution bubbles back up to the conscious mind, as if out of nowhere, making you freeze mid-motion with a stack of dirty plates in your hands.&lt;/p&gt;
    &lt;p&gt;This is a very useful thing to know about the mind, because it means you can steer your subconscious towards the particular problems you want it to work on. By priming yourself with important problems before doing the dishes or going for walks or sleeping, you make sure your mental resources are used on what matters for you, instead of, for example, the open loops in a Netflix series you watched before bed. It is free labor.&lt;/p&gt;
    &lt;p&gt;But—this is not what Hadamard is talking about when he describes the wordless thought of the mathematicians and researchers he has surveyed. Instead, what they seem to be doing is something similar to this subconscious, parallelized search, except they do it in a “tensely” focused way.&lt;/p&gt;
    &lt;p&gt;The impression I get is that Hadamard loads a question into his mind (either in a non-verbal way, or by reading a mathematical problem that has been written by himself or someone else), and then he holds the problem effortfully centered in his mind. Effortfully, but wordlessly, and without clear visualizations. Describing the mental image that filled his mind while working on a problem concerning infinite series for his thesis, Hadamard writes that his mind was occupied by an image of a ribbon which was thicker in certain places (corresponding to possibly important terms). He also saw something that looked like equations, but as if seen from a distance, without glasses on: he was unable to make out what they said.&lt;/p&gt;
    &lt;p&gt;I’m not sure what is going on here. But here’s a speculation. As I understand it, when one part of our brain is working, it often inhibits another—if you put words to distressing feelings, for example, the language-oriented parts of your brain inhibit the amygdala, which reduces the emotional distress. Similarly, when you are focused on a task at hand, the executive control network of your brain will tend to inhibit the default mode network which is responsible for mind wandering. (This might explain why illuminations tend to occur mainly in the shower, when the executive control networks downregulate and the mind is allowed to wander.)&lt;/p&gt;
    &lt;p&gt;Here’s my speculation: perhaps Hadamard and the other great mathematicians are able to enter into a modality of thought where they are able to keep both the default mode network and the executive control network on at the same time. Perhaps this allows them to do a sort of subconscious, in-the-shower-type processing, while still maintaining enough conscious focus to ensure the thoughts don’t drift away from the problem and its constraints.&lt;/p&gt;
    &lt;p&gt;When I look into this, I notice that there is research indicating that when doing certain types of creative work, the default mode network and the executive control network are, indeed, active at the same time, which they usually aren’t. Individuals who are experienced in a creative field seem to have the capacity to keep the default mode network turned on, allowing them to generate many permutations of ideas, while steering it with the executive control network, ensuring their parallelized mindwandering is constrained by the facts of the problem. I suspect we are all capable of this to some extent, but doing it to the extent Hadamard’s subjects did is akin to a ballerina spinning on her toes: a mental posture that requires serious practice to develop the necessary muscles and coordination.&lt;/p&gt;
    &lt;p&gt;I’m not well-versed in neuroscience enough to know if I’m interpreting this right; I’m just speculating.&lt;/p&gt;
    &lt;p&gt;But what we do know is that Hadamard, as he worked, would pace up and down his room with what “witnesses to [his] daily life and work” called his “inside” look. (Others, like Poincaré and Helmholtz, seem to have sat at their desks, staring into nowhere.) And this type of deep, consciously-blurry concentration could go on for a long time: Hadamard mentions that he only stopped walking if he needed to write down a proof (reluctantly). An acquaintance writes that a friend of his shared an office with one of the best now living physicists; this physicist’s work habit was to come into the office in the morning and then stare into the wall for 8 hours before going home. Imagine holding a productive thought for that long without writing any steps down and, presumably, without even compressing things into words inside your head!&lt;/p&gt;
    &lt;head rend="h2"&gt;The interplay between writing and non-linguistic thinking&lt;/head&gt;
    &lt;p&gt;Hadamard writes that he sometimes used algebraic signs when dealing with easy calculations, but adds that, “whenever the matter looks more difficult, they become too heavy a baggage for me.”&lt;/p&gt;
    &lt;p&gt;Why are words too heavy?&lt;/p&gt;
    &lt;p&gt;Reasoning from my experience, I suspect it is because words are laborious. When we put words to a thought, we have to compress something that is like a web in our mind, filled with connections and associations going in all directions, turning that web into a sequential string of words; we have to compress what is high-dimensional into something low-dimensional. This has all sorts of advantages, which I will return to, but the point I want to emphasize here is that compression is effortful. It takes intense concentration to find the right words (rather than the sloppy ones that first come to mind), and then to put them in the proper order. As James Joyce said to his friend when he was asked why he looked so gloomy, “I’ve only written seven words today…” “But why then are you in despair—seven words is a lot for you!” “I don’t know in which order to put them…”&lt;/p&gt;
    &lt;p&gt;If we can avoid the compression step, and do the manipulations directly in the high-dimensional, non-linguistic, conceptual space, we can move much faster.2&lt;/p&gt;
    &lt;p&gt;But this is a big if. Most people, myself included, have too weak mental models to do this kind of processing for complex problems, and so, our thoughts are riddled with contradictions and holes that we often don’t notice unless we try to write them down. We can move faster in wordless thought, but we’re moving at random. If, however, you have deep expertise in an area, like the mathematicians, it is possible to let go of the language compression and do a much faster search. M, who started his career as a physicist, tells me that when he was 13 and read that Einstein thought without words, he felt disappointed since his mind didn’t work like that; then, “a decade and many thousands of hours of mathematics and physics later,” he reread the passage and recognized himself almost completely. I guess this was because the labor of learning mathematics, done largely through reading and writing his way through complex ideas and problems, had given him deep enough mental models to make words somewhat superfluous.&lt;/p&gt;
    &lt;p&gt;But even then, as Hadamard notes, writing is a necessary step of the process. The insights arrived at wordlessly need to be submitted to the rigor of mathematical notation and logic, to test their validity. It is a sort of feedback mechanism: unless the intuition holds up on the page, it is a false intuition.&lt;/p&gt;
    &lt;p&gt;The written results also work as relay results. By writing something down and making sure it is solid, we can offload that thought from working memory and instead use it as a building block for the next step of the thought. Or, to use a metaphor by the mathematician William Hamilton, deep thinking is like building a tunnel through a sandbank:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;In this operation, it is impossible to succeed unless every foot, nay, almost every inch in our progress be secured by an arch of masonry before we attempt the excavation of another. Now, language is to the mind precisely what the arch is to the tunnel. The power of thinking and the power of excavation are not dependent on the words in the one case, on the mason-work in the other; but without these subsidiaries, neither process could be carried on beyond its rudimentary commencement.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;So writing—and reading, seriously, the writings of others—is a way to collect stepping stones: ideas that have been stabilized enough that they can carry us as we walk deeper into the thought space.&lt;/p&gt;
    &lt;p&gt;But this stabilization of meaning can go wrong, too, if we stabilize ideas that aren’t ready to be stabilized yet. When writing, there are all sorts of details that need to be specified for our paragraphs to make sense, and if we don’t know what should go into a sentence, it is all too easy to fill in the uncertain parts with guesses. At least my brain has the most miraculous autocomplete function and supplies me with credible endings to any sentence I start—often credible nonsense. But when the nonsense is there on the page, next to thoughts I’ve settled through hard work, it looks respectable! It often takes considerable work to realize I’ve fooled myself.&lt;/p&gt;
    &lt;p&gt;This was another reason Hadamard’s subjects gave for why they were reluctant to use words: they were afraid of the false precision writing forces onto thinking. They were afraid of premature precision and the confusion it breeds. By thinking in blurry images, or tensions of the hands, or sounds, they could keep their thoughts accurately vague in the areas where there was still uncertainty. They wrote down on paper, as settled, only mostly what was actually known. If you are disciplined, you can write in such a way that you avoid false precision.&lt;/p&gt;
    &lt;p&gt;To sum up: the relationship between verbal thinking and deep wordless concentration is complex.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Non-verbal, blurry thinking is faster and can search in a broader way, but it is more error-prone than verbal thought.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Good writing tends to come from an attempt to capture in words something you understand wordlessly, rather than moving ideas around on the page; but, paradoxically, a generative subconscious is usually one that has been trained by writing and deep reading, which provides the subconscious with relay results and other mental structures necessary for deep thought.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Writing forces precision, which can fool us into locking in details we have no reason to lock in, but written notes (or drawings) are a necessary aid when thinking long chains of thoughts.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Over the last nine months, as I’ve been thinking about this topic, I’ve become more mindful about when words hinder and when they help. I notice that I spend more time in wordless thoughts than I used to. But I’m also more deliberate about using writing to structure my brain so it feeds me better thoughts.&lt;/p&gt;
    &lt;p&gt;As always, a big thank you to the paying subscribers who fund the work on the public essays. I couldn’t do this without you! I also want to thank Johanna Karlsson and Michael Nielsen for discussion about the topic. Esha Rana helped me with the final edit.&lt;/p&gt;
    &lt;p&gt;I can think of examples of mathematicians and physicist for whom this is not true. The first one who comes to mind is Richard Feynman, who said in an interview:&lt;/p&gt;
    &lt;head rend="h5"&gt;Feynman:&lt;/head&gt;
    &lt;p&gt;I actually did the work on the paper.&lt;/p&gt;
    &lt;head rend="h5"&gt;Weiner:&lt;/head&gt;
    &lt;p&gt;That s right. It wasn’t a record of what you had done but it is the work.&lt;/p&gt;
    &lt;head rend="h5"&gt;Feynman:&lt;/head&gt;
    &lt;p&gt;It’s the doing it — it’s the scrap paper.&lt;/p&gt;
    &lt;head rend="h5"&gt;Weiner:&lt;/head&gt;
    &lt;p&gt;Well, the work was done in your head but the record of it is still here.&lt;/p&gt;
    &lt;head rend="h5"&gt;Feynman:&lt;/head&gt;
    &lt;p&gt;No, it’s not a record, not really, it’s working. You have to work on paper and this is the paper. OK?&lt;/p&gt;
    &lt;p&gt;A more technical way of saying this is that our (non-verbal) thoughts seem to behave as vectors; when a cluster of neurons fire together, that pattern is like an address pointing toward a point in a high dimensional space. But when we convert our thoughts to words, we convert that vector into a scalar. I’m not sure if this is true, but here is a paper laying out the argument for why it might be.&lt;/p&gt;
    &lt;p&gt;The discussion of vectors and dimension reduction also has an interesting parallel to an ongoing discussion in AI research. When a large language model calculates what to output, the “thinking” happens in high dimensional space where vectors are passed from layer to layer. At the final layer, that high dimensional representation is collapsed into a token—the written output. When this happens, enormous amounts of information is lost: the residual stream contains over a thousand times more information than gets encoded into the token! That lends some support to the idea that non-verbal (partly unconscious) thinking might be more information rich in humans, too.&lt;/p&gt;
    &lt;p&gt;In reasoning models, where the LLM is encouraged to think for longer, what happens is that this written output—this collapsed thought—is fed back into the model as input, so it can keep thinking about it. It is as if a person were to lose all of their memories and thoughts every few seconds and could only rely on whatever conclusions they had written on a slip of paper; this seems, potentially, like a limited way of thinking. To come around this problem—if it is a problem—one idea that is being explored is to feed the entire vector back into the model as a chain of thought, instead of the tokens on the scratch pad. This would be something like letting the models think in a non-verbal mental space, akin to what Hadamard described—thinking in the latent space, rather than on the scratch pad.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45687441</guid><pubDate>Thu, 23 Oct 2025 21:26:57 +0000</pubDate></item><item><title>/dev/null is an ACID compliant database</title><link>https://jyu.dev/blog/why-dev-null-is-an-acid-compliant-database/</link><description>&lt;doc fingerprint="8812dd1940f64a11"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Atomicity&lt;/head&gt;
    &lt;p&gt;Operations are "all or nothing."&lt;/p&gt;
    &lt;p&gt;Anything you write to &lt;code&gt;/dev/null&lt;/code&gt; disappears entirely. There's no partial write problem: it’s either written (and discarded) or not written at all. ✅&lt;/p&gt;
    &lt;head rend="h2"&gt;Consistency&lt;/head&gt;
    &lt;p&gt;The system transitions from one valid state to another.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;/dev/null&lt;/code&gt; always stays in a consistent state (empty). No matter what you write, the invariant "file contains nothing" always holds. ✅&lt;/p&gt;
    &lt;head rend="h2"&gt;Isolation&lt;/head&gt;
    &lt;p&gt;Concurrent transactions don’t interfere with each other.&lt;/p&gt;
    &lt;p&gt;Multiple processes can write to &lt;code&gt;/dev/null&lt;/code&gt; at the same time, and their outputs never conflict, because nothing is ever stored. ✅&lt;/p&gt;
    &lt;head rend="h2"&gt;Durability&lt;/head&gt;
    &lt;p&gt;Once a transaction is committed, it remains so, even after crashes.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;/dev/null&lt;/code&gt; "durably" commits your data into nothingness. After a crash or reboot, it still contains exactly what it always has: nothing. ✅&lt;/p&gt;
    &lt;p&gt;There is only 1 small problem though, it only comes with 0b of free storage. For more space, you will have to contact entreprise sales, which is actually just me!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45687458</guid><pubDate>Thu, 23 Oct 2025 21:28:02 +0000</pubDate></item><item><title>How memory maps (mmap) deliver faster file access in Go</title><link>https://info.varnish-software.com/blog/how-memory-maps-mmap-deliver-25x-faster-file-access-in-go</link><description>&lt;doc fingerprint="f2fbd59d36b2b0c8"&gt;
  &lt;main&gt;
    &lt;p&gt;One of the slowest things you can do in an application is making system calls. They're slow because you do have to enter the kernel, which is quite expensive. What should you do when you need to do a lot of disk I/O but you care about performance? One solution is to use memory maps.&lt;lb/&gt;Memory maps are a modern Unix mechanism where you can take a file and make it part of the virtual memory. In Unix context, modern means that it was introduced in the 1980s or later. You have a file, containing data, you mmap it and you'll get a pointer to where this resides. Now, instead of seeking and reading, you just read from this pointer, adjusting the offset to get to the right data.&lt;/p&gt;
    &lt;head rend="h2"&gt;Performance&lt;/head&gt;
    &lt;p&gt;To show what kind of performance you can get using memory maps, I've written a little Go library that allows you to read from a file using a memory map or a ReaderAt. ReaderAt will do a pread(), which is a seek/read combo, while mmap will just read from the memory map.&lt;/p&gt;
    &lt;p&gt;This almost feels like magic. Initially, when we launched Varnish Cache back in 2006, this was one of the features that made Varnish Cache very fast when delivering content. Varnish Cache would use memory maps to deliver content at blistering speeds.&lt;lb/&gt;Also, since you can operate with pointers into memory that is allocated by the memory map, you'll reduce memory pressure as well as raw latency.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Downside of Memory Maps&lt;/head&gt;
    &lt;p&gt;The downside of memory maps is that you really can't write to the memory map. The reason is due to the way virtual memory works. When you're writing to a part of virtual memory that isn't mapped into physical memory, the CPU will generate a page fault. On a modern computer, the CPU is responsible for tracking what virtual memory pages are mapped onto what physical memory. Since you're writing to a page that isn't mapped, the CPU needs help.&lt;lb/&gt;So, when the page fault occurs, the OS will 1) allocate a new memory page, 2) read the contents of the file at the correct offset, 3) write this to the new memory page. Then control is returned to the application. The application will now overwrite the virtual memory page with new data.&lt;lb/&gt;Can we stop and appreciate how extremely inefficient this is? I think it is fairly safe to say that writing through a memory map is never a good idea when considering performance. At least if there is any risk, the file isn't mapped up in physical memory.&lt;lb/&gt;Let me illustrate this with a few more benchmarks.&lt;/p&gt;
    &lt;p&gt;As you can see, whether or not the pages are in cache is crucial for performance. WriterAt, which uses the pwrite call, is a much more predictable bet.&lt;lb/&gt;Still, writing through memory maps, was what Varnish Cache did initially. It somehow got away with it, but mostly because the competition was pretty bad.&lt;lb/&gt;This is why Varnish Cache got the malloc backend and why Varnish Enterprise got the various Massive Storage Engines. The malloc backend resolved the problem by just allocating system memory through the malloc system call, and the Massive Storage Engine uses io_uring, which is so new that support for it is still somewhat limited.&lt;/p&gt;
    &lt;head rend="h2"&gt;Using Memory Maps to Solve Real-world Performance Problems&lt;/head&gt;
    &lt;p&gt;The last couple of weeks I've been working on an HTTP-backed filesystem. This is part of our AI Storage Acceleration solution, geared towards high performance computing environments. In this filesystem we needed a way to transfer folder data over HTTP. A folder is really just a listing of files, symbolic links and directories. The naive approach would be just to use JSON encoding, but JSON is notorious for being slow.&lt;lb/&gt;Our priority is performance. We made a benchmarking suite, comparing various databases with each other. CDB was overall the fastest. Looking at the numbers, we'd still see that CDB would spend something like 1200ns on a database lookup that was entirely in the page cache. This seems very slow to me. After all, everything should be in memory and spending 1200ns reading memory sounds at least 100x too slow. I started looking into the CDB implementation I was using. It was the above ReaderAt implementation. So, most of the time is likely spent waiting for the operating system.&lt;lb/&gt;Some hours later, I was able to replace the seek/read with a memory map. This resulted in a 25x improvement in performance. Again, it feels like magic. Unlike the original file stevedore in Varnish Cache, this performance improvement has no downside.&lt;lb/&gt;Benchmarks: https://github.com/perbu/mmaps-in-go &lt;lb/&gt;CDB64 files with memory maps: https://github.com/perbu/cdb &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45687796</guid><pubDate>Thu, 23 Oct 2025 21:56:07 +0000</pubDate></item><item><title>Apple loses UK App Store monopoly case, penalty might near $2B</title><link>https://9to5mac.com/2025/10/23/apple-loses-uk-app-store-monopoly-case-penalty-might-near-2-billion/</link><description>&lt;doc fingerprint="a6ccef2653173374"&gt;
  &lt;main&gt;
    &lt;p&gt;A landmark case in the UK concerning Apple’s App Store practices has just been decided, with a London tribunal ruling against the company in a move that could cost Apple up to $2 billion.&lt;/p&gt;
    &lt;head rend="h2"&gt;London tribunal rules that Apple overcharged app developers for years with unfair commissions&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;Apple abused its dominant position by charging app developers unfair commissions, a London tribunal ruled on Thursday, in a blow which could leave the U.S. tech company on the hook for hundreds of millions of pounds in damages.&lt;/p&gt;
      &lt;p&gt;The Competition Appeal Tribunal (CAT) ruled against Apple after a trial of the lawsuit, which was brought on behalf of millions of iPhone and iPad users in the United Kingdom.&lt;/p&gt;
      &lt;p&gt;The CAT ruled that Apple had abused its dominant position from October 2015 until the end of 2020 by shutting out competition in the app distribution market and by “charging excessive and unfair prices” as commission to developers. […]&lt;/p&gt;
      &lt;p&gt;The case had been valued at around 1.5 billion pounds ($2 billion) by those who brought it. A hearing next month will decide how damages are calculated and Apple’s application for permission to appeal.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Apple has already said it will appeal the ruling, which the company said “takes a flawed view of the thriving and competitive app economy”.&lt;/p&gt;
    &lt;p&gt;The estimated $2 billion in damages that might be enforced is expected to combine several different factors.&lt;/p&gt;
    &lt;p&gt;Per Reuters:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;“developers were overcharged by the difference between a 17.5% commission for app purchases and the commission Apple charged, which Kent’s lawyers said was usually 30%.”&lt;/item&gt;
      &lt;item&gt;“The CAT also ruled that app developers passed on 50% of the overcharge to consumers.”&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We will keep you posted on any further developments with the case, including Apple’s appeal and the actual damages when they are calculated next month.&lt;/p&gt;
    &lt;p&gt;For previous coverage of this lawsuit, here’s the original 2023 news story.&lt;/p&gt;
    &lt;p&gt;What are your takeaways from the ruling against Apple for its App Store practices in the UK? Let us know in the comments.&lt;/p&gt;
    &lt;head rend="h3"&gt;Best iPhone accessories&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;AirPods Pro 3&lt;/item&gt;
      &lt;item&gt;AirTag 4-pack (now only $65, down from $99)&lt;/item&gt;
      &lt;item&gt;MagSafe Car Mount for iPhone&lt;/item&gt;
      &lt;item&gt;10-year AirTag battery case 2-pack&lt;/item&gt;
      &lt;item&gt;100W USB-C fast charging power adapter&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;FTC: We use income earning auto affiliate links. More.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45688006</guid><pubDate>Thu, 23 Oct 2025 22:11:23 +0000</pubDate></item><item><title>AI discovers a 5x faster MoE load balancing algorithm than human experts</title><link>https://adrs-ucb.notion.site/moe-load-balancing</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=45688236</guid><pubDate>Thu, 23 Oct 2025 22:35:22 +0000</pubDate></item><item><title>Introduction to the concept of likelihood and its applications (2018)</title><link>https://journals.sagepub.com/doi/10.1177/2515245917744314</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45688443</guid><pubDate>Thu, 23 Oct 2025 22:52:15 +0000</pubDate></item><item><title>React Flow, open source libraries for node-based UIs with React or Svelte</title><link>https://github.com/xyflow/xyflow</link><description>&lt;doc fingerprint="ce0797600b4bffda"&gt;
  &lt;main&gt;
    &lt;p&gt;Powerful open source libraries for building node-based UIs with React or Svelte. Ready out-of-the-box and infinitely customizable.&lt;/p&gt;
    &lt;p&gt;The xyflow repository is the home of four packages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;React Flow 12 &lt;code&gt;@xyflow/react&lt;/code&gt;packages/react&lt;/item&gt;
      &lt;item&gt;React Flow 11 &lt;code&gt;reactflow&lt;/code&gt;v11 branch&lt;/item&gt;
      &lt;item&gt;Svelte Flow &lt;code&gt;@xyflow/svelte&lt;/code&gt;packages/svelte&lt;/item&gt;
      &lt;item&gt;Shared helper library &lt;code&gt;@xyflow/system&lt;/code&gt;packages/system&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Are you using React Flow or Svelte Flow for a personal project? Great! No sponsorship needed, you can support us by reporting any bugs you find, sending us screenshots of your projects, and starring us on Github 🌟&lt;/p&gt;
    &lt;p&gt;Are you using React Flow or Svelte Flow at your organization and making money from it? Awesome! We rely on your support to keep our libraries developed and maintained under an MIT License, just how we like it. For React Flow you can do that on the React Flow Pro website and for both of our libraries you can do it through Github Sponsors.&lt;/p&gt;
    &lt;p&gt;The best way to get started is to check out the React Flow or Svelte Flow learn section. However if you want to get a sneak peek of how to install and use the libraries you can see it here:&lt;/p&gt;
    &lt;head&gt;React Flow basic usage&lt;/head&gt;
    &lt;code&gt;npm install @xyflow/react&lt;/code&gt;
    &lt;code&gt;import { useCallback } from 'react';
import {
ReactFlow,
MiniMap,
Controls,
Background,
useNodesState,
useEdgesState,
addEdge,
} from '@xyflow/react';

import '@xyflow/react/dist/style.css';

const initialNodes = [
{ id: '1', position: { x: 0, y: 0 }, data: { label: '1' } },
{ id: '2', position: { x: 0, y: 100 }, data: { label: '2' } },
];

const initialEdges = [{ id: 'e1-2', source: '1', target: '2' }];

function Flow() {
const [nodes, setNodes, onNodesChange] = useNodesState(initialNodes);
const [edges, setEdges, onEdgesChange] = useEdgesState(initialEdges);

const onConnect = useCallback((params) =&amp;gt; setEdges((eds) =&amp;gt; addEdge(params, eds)), [setEdges]);

return (
  &amp;lt;ReactFlow
    nodes={nodes}
    edges={edges}
    onNodesChange={onNodesChange}
    onEdgesChange={onEdgesChange}
    onConnect={onConnect}
  &amp;gt;
    &amp;lt;MiniMap /&amp;gt;
    &amp;lt;Controls /&amp;gt;
    &amp;lt;Background /&amp;gt;
  &amp;lt;/ReactFlow&amp;gt;
);
}

export default Flow;&lt;/code&gt;
    &lt;head&gt;Svelte Flow basic usage&lt;/head&gt;
    &lt;code&gt;npm install @xyflow/svelte&lt;/code&gt;
    &lt;code&gt;&amp;lt;script lang="ts"&amp;gt;
import { writable } from 'svelte/store';
import {
  SvelteFlow,
  Controls,
  Background,
  BackgroundVariant,
  MiniMap,
} from '@xyflow/svelte';

import '@xyflow/svelte/dist/style.css'

const nodes = writable([
  {
    id: '1',
    type: 'input',
    data: { label: 'Input Node' },
    position: { x: 0, y: 0 }
  },
  {
    id: '2',
    type: 'custom',
    data: { label: 'Node' },
    position: { x: 0, y: 150 }
  }
]);

const edges = writable([
  {
    id: '1-2',
    type: 'default',
    source: '1',
    target: '2',
    label: 'Edge Text'
  }
]);
&amp;lt;/script&amp;gt;

&amp;lt;SvelteFlow
{nodes}
{edges}
fitView
on:nodeclick={(event) =&amp;gt; console.log('on node click', event)}
&amp;gt;
&amp;lt;Controls /&amp;gt;
&amp;lt;Background variant={BackgroundVariant.Dots} /&amp;gt;
&amp;lt;MiniMap /&amp;gt;
&amp;lt;/SvelteFlow&amp;gt;&lt;/code&gt;
    &lt;p&gt;For releasing packages we are using changesets in combination with the changeset Github action. The rough idea is:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;create PRs for new features, updates and fixes (with a changeset if relevant for changelog)&lt;/item&gt;
      &lt;item&gt;merge into main&lt;/item&gt;
      &lt;item&gt;changset creates a PR that bumps all packages based on the changesets&lt;/item&gt;
      &lt;item&gt;merge changeset PR if you want to release to Github and npm&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Built by xyflow&lt;/head&gt;
    &lt;p&gt;React Flow and Svelte Flow are maintained by the xyflow team. If you need help or want to talk to us about a collaboration, reach out through our contact form or by joining our Discord Server.&lt;/p&gt;
    &lt;p&gt;React Flow and Svelte Flow are MIT licensed.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45688836</guid><pubDate>Thu, 23 Oct 2025 23:33:28 +0000</pubDate></item><item><title>Computer Science Courses That Don't Exist, but Should (2015)</title><link>https://prog21.dadgum.com/210.html</link><description>&lt;doc fingerprint="b44d70d748170ab9"&gt;
  &lt;main&gt;
    &lt;p&gt;Discover how to create and use variables that aren't inside of an object hierarchy. Learn about "functions," which are like methods but more generally useful. Prerequisite: Any course that used the term "abstract base class."&lt;/p&gt;
    &lt;p&gt;CSCI 3300: Classical Software Studies&lt;lb/&gt; Discuss and dissect historically significant products, including VisiCalc, AppleWorks, Robot Odyssey, Zork, and MacPaint. Emphases are on user interface and creativity fostered by hardware limitations.&lt;/p&gt;
    &lt;p&gt;CSCI 4020: Writing Fast Code in Slow Languages&lt;lb/&gt; Analyze performance at a high level, writing interpreted Python that matches or beats typical C++ code while being less fragile and more fun to work with.&lt;/p&gt;
    &lt;p&gt;CSCI 2170: User Experience of Command Line Tools&lt;lb/&gt; An introduction to UX principles as applied to command line programs designed as class projects. Core focus is on output relevance, readability, and minimization. UNIX "ls" tool is a case study in excessive command line switches.&lt;/p&gt;
    &lt;p&gt;PSYC 4410: Obsessions of the Programmer Mind&lt;lb/&gt; Identify and understand tangential topics that software developers frequently fixate on: code formatting, taxonomy, type systems, splitting projects into too many files. Includes detailed study of knee-jerk criticism when exposed to unfamiliar systems.&lt;/p&gt;
    &lt;p&gt;permalink September 10, 2015&lt;/p&gt;
    &lt;p&gt;I'm James Hague, a recovering programmer who has been designing video games since the 1980s. Programming Without Being Obsessed With Programming and Organizational Skills Beat Algorithmic Wizardry are good starting points. For the older stuff, try the 2012 Retrospective.&lt;/p&gt;
    &lt;p&gt;Where are the comments?&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45690045</guid><pubDate>Fri, 24 Oct 2025 02:22:07 +0000</pubDate></item><item><title>Roc Camera</title><link>https://roc.camera/</link><description>&lt;doc fingerprint="18a41a343b25502d"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Photography used to have magic&lt;/head&gt;
    &lt;head rend="h2"&gt;Photographyusedtohavemagic&lt;/head&gt;
    &lt;p&gt;There was a time when cameras captured magic. A photo wasn't just an image; it was a snapshot of life. People lined up, waited their turn, and treasured the results. Photos told stories, a reflection of reality, a physical artifact of our lives.&lt;/p&gt;
    &lt;head rend="h2"&gt;AI has blurred the line&lt;/head&gt;
    &lt;head rend="h2"&gt;AIhasblurredtheline&lt;/head&gt;
    &lt;p&gt;In the past two decades, we've seen an explosion in the way we take, share, and create images. Smartphones and social media has capturing and sharing images easier. Generative AI has made it possible to create any image we can imagine.&lt;/p&gt;
    &lt;head rend="h2"&gt;Lost sight of what is real&lt;/head&gt;
    &lt;head rend="h2"&gt;Lostsightofwhatisreal&lt;/head&gt;
    &lt;p&gt;We've started to lose sight of what is real in an endless sea of copies, AI-generated noise, and altered realities. We're losing our ability to find our bearings in the digital world.&lt;/p&gt;
    &lt;head rend="h2"&gt;It's time for Roc Camera&lt;/head&gt;
    &lt;head rend="h2"&gt;It'stimeforRocCamera&lt;/head&gt;
    &lt;p&gt;By combining sensors, an on-device zero-knowledge proofs, and a tamper-proof environment for attesting sensor data, We've built Roc Camera to capture verifiably real moments.&lt;/p&gt;
    &lt;head rend="h3"&gt;Camera Components:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;â¢ 4-inch IPS LCD Screen 720x720 with Capacitive Touch&lt;/item&gt;
      &lt;item&gt;â¢ 16MP Sony IMX519 CMOS with 122Â° FOV lens&lt;/item&gt;
      &lt;item&gt;â¢ Raspberry Pi 4 4GB RAM ARM Cortex-A72 1.5 Ghz&lt;/item&gt;
      &lt;item&gt;â¢ LiPo 4000mAh Battery&lt;/item&gt;
      &lt;item&gt;â¢ Uninterruptible Power Supply Board&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Capture&lt;/head&gt;
    &lt;p&gt;Capture a photo that only this Camera can uniquely take&lt;/p&gt;
    &lt;head rend="h3"&gt;Prove&lt;/head&gt;
    &lt;p&gt;Creates a Zero Knowledge (ZK) Proof of the camera sensor data and other metadatas&lt;/p&gt;
    &lt;head rend="h3"&gt;Verify&lt;/head&gt;
    &lt;p&gt;Verify that the photo is real by checking the ZK proof via the Roc Photo SDK&lt;/p&gt;
    &lt;head rend="h2"&gt; Capture &lt;lb/&gt;verifiably&lt;lb/&gt; real&lt;lb/&gt; moments &lt;/head&gt;
    &lt;p&gt;In Beta (Batch 2 is open)&lt;/p&gt;
    &lt;p&gt;Current Lead time: 2~3 weeks&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45690251</guid><pubDate>Fri, 24 Oct 2025 02:54:29 +0000</pubDate></item></channel></rss>