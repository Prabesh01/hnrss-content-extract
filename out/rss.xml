<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Tue, 04 Nov 2025 22:39:13 +0000</lastBuildDate><item><title>Things you can do with diodes</title><link>https://lcamtuf.substack.com/p/things-you-can-do-with-diodes</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45805900</guid><pubDate>Mon, 03 Nov 2025 23:49:03 +0000</pubDate></item><item><title>You can't cURL a Border</title><link>https://drobinin.com/posts/you-cant-curl-a-border/</link><description>&lt;doc fingerprint="7f44c81abfa3ce57"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;You can't cURL a Border&lt;/head&gt;
    &lt;p&gt;An error fare to Iceland pops up. It's cheap enough to feel like a typo and most likely will be gone in minutes. I'm moderately obsessed with ways to travel on budget, so I keep an eye on these.&lt;lb/&gt; Before I click Buy, I need to know (fast!) if it actually works for me: would I need a visa, are there any odd passport requirements, can I quickly sort out the driving permit, would it affect my Schengen 90/180 window, break UK presence tests or accidentally prevent a tax residency I am chasing.&lt;/p&gt;
    &lt;p&gt;It isn’t one check, it’s a stack of small unfriendly ones, and takes around 20 minutes to process. Some bits are fun, like hunting for a seat upgrade, but mostly it’s counting midnights and expiry dates so a cheap weekend doesn’t become an expensive lesson.&lt;/p&gt;
    &lt;p&gt;I've been doing this dance for a decade now. In 2015 I made a spreadsheet for a US visa application that wanted ten years of travel history, down to the day. The spreadsheet grew: UK work visa, Indefinite Leave to Remain and citizenship applications, Canadian work permits. Any government form that asked "where have you been?" got its answer from the same battered CSV. It worked well enough, in the sense that I was never detained.&lt;/p&gt;
    &lt;p&gt;It also made me think that this was a solvable problem I was solving badly. I built a ledger to answer “where was I on 15 March 2023?” Instead, I ran simulations to check, “if I book this, what breaks later?”&lt;/p&gt;
    &lt;p&gt;The only question is whether the computer can answer all of faster than I do, and leave December, the next border control, and the end of the tax year blissfully uneventful.&lt;/p&gt;
    &lt;head rend="h2"&gt;Does this trip compile? ¶&lt;/head&gt;
    &lt;p&gt;That twenty-minute panic before buying a flight comes from one basic problem: none of the systems that judge you will tell you your state.&lt;/p&gt;
    &lt;p&gt;Schengen is running one check. The UK is running another. Tax residency is running a third. Your passport is running its own quiet clock in the background. None of them explain themselves, and none of them agree on what “a day” even is.&lt;/p&gt;
    &lt;p&gt;Schengen cares about presence across rolling windows. The UK counts how many midnights you were physically in the country in a tax year that, for historical reasons [1], starts on 6 April out of all options. Some countries track how many days you’ve spent in certain places and change the medical paperwork they expect from you once you cross a threshold. Meanwhile your passport might fail you with its expiry date, validity rules that may apply on arrival or departure depending on routing, and a finite number of blank facing pages that some countries require.&lt;/p&gt;
    &lt;p&gt;None of that is easily exposed. The officer at the desk can see it but you can’t. That's parsing the State—both kinds. The government's view of you, and the state machine that tracks it.&lt;/p&gt;
    &lt;head rend="h3"&gt;The bureaucratic edge cases ¶&lt;/head&gt;
    &lt;p&gt;The rules aren't just complex—they're occasionally specific in ways that make you regret leaving the house in the first place.&lt;/p&gt;
    &lt;p&gt;To apply for British citizenship, you need to prove you were physically in the UK on your application date but five years ago. Not approximately five years, not that week—that exact day when you press "submit" on the form minus five years. Miss it by 24 hours and your application is reject after months of waiting, and you have to pay a hefty fee to re-apply.&lt;/p&gt;
    &lt;p&gt;Transiting through a UK airport? Leaving the terminal doesn't count as presence unless you do something "unrelated to your travel"—buy a sausage roll at Greggs, see a play in West End, meet a friend. The guidance doesn't even specify a minimum spend.&lt;/p&gt;
    &lt;p&gt;Morocco runs on UTC+1 most of the year but switches to UTC during Ramadan to shorten the fasting day. Which means "days spent in Morocco" depends on your timezone database version and whether you remembered to update it.&lt;/p&gt;
    &lt;p&gt;It would be alright with a single source of truth, but all these facts are scattered across (semi)official websites and PDFs, and you're supposed to figure it out yourself.&lt;/p&gt;
    &lt;p&gt;So the job isn't "log trips" (I already did that for ten years in a spreadsheet). The job is: given what I've already done and what I'm about to do, does this plan quietly break anything, and if so, where, and by how much.&lt;/p&gt;
    &lt;p&gt;"You're at 56 days because Amsterdam contributed 12, Prague 3, Barcelona 10, Iceland would add a month, and February doesn't count anymore" is something I can trust, argue with, or fix. "You're fine" isn't.&lt;/p&gt;
    &lt;p&gt;That's where this stops being a spreadsheet and starts being a linter. Apparently I want the compiler warning before I press Buy.&lt;/p&gt;
    &lt;head rend="h2"&gt;Counting the right midnight ¶&lt;/head&gt;
    &lt;p&gt;If I want a compiler warning I trust, the compiler has to agree with the officer about what a day is.&lt;/p&gt;
    &lt;p&gt;For the past five years I've been working on an app for people with epilepsy, managing timezones, medication reminders, and edge cases. We juggled multiple sources of truth and multiple storage styles (some records in UTC, some in local time with timezones stored separately—historical reasons, obviously).&lt;/p&gt;
    &lt;p&gt;This time, I tried to learn from that: facts are stored as instants, reasoning happens in local days of the jurisdiction that cares.&lt;/p&gt;
    &lt;p&gt;Take this routing: depart Dublin morning of November the 17th, brief Newark layover, a longer one in Mexico City, 23-hour Heathrow stop, then Tenerife. Ask five immigration systems "how many tax residency days?" and you get five answers:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Ireland: zero (under 30 days/year threshold).&lt;/item&gt;
      &lt;item&gt;US: zero (foreign-to-foreign transit under 24 hours).&lt;/item&gt;
      &lt;item&gt;Mexico: two (you cross midnight twice).&lt;/item&gt;
      &lt;item&gt;UK: zero (even though you cross midnight once), unless you went landside for non-travel reasons, then one.&lt;/item&gt;
      &lt;item&gt;Schengen: one (entry day counts, exit day will count too, even if both are only for 15 minutes).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Each stop has same or similar conditions, but different state machines are asking different questions. I pin the timezone database version that produced each result, and when rules or clocks shift, I recompute so I could show both answers if needed. Yesterday should stay reproducible even when tomorrow disagrees.[2]&lt;/p&gt;
    &lt;head rend="h2"&gt;Parsing the State ¶&lt;/head&gt;
    &lt;p&gt;In other words, the linter is meant to answer the same question in various disguises: "what happens if I do this?"&lt;/p&gt;
    &lt;p&gt;Can I book Christmas in the Alps with three summer weekends planned in Europe? Does it matter if I leave UK before the tax year ends? What passport should I travel on? Does anything expire between booking and boarding?&lt;/p&gt;
    &lt;p&gt;Every question has the same shape: simulate forward, find what breaks, decide if you care. The goal isn't to convince border officers—it's to not make mistakes they'd catch. Trips get assembled from whatever I can verify later: geotagged photos, background location, manual entries. A resolver turns that into "present on this local day" and keeps track of why.&lt;/p&gt;
    &lt;p&gt;I don't hardcode rules, I ship interpretations instead: each jurisdiction has a small versioned blob that says what counts, how the window is measured, where that reading came from.&lt;/p&gt;
    &lt;p&gt;The paperwork gets the same treatment because documents are state machines too. A passport isn’t just means of identity; it has constraints and timers. Some requirements, like six months validity are legacy and usually exist to keep deportations possible without issuing emergency documents, but still need to be checked. [3]&lt;/p&gt;
    &lt;p&gt;Before I buy anything the linter should tell me that I don't have the correct flavour of IDP (and man, getting those in Scotland since they delegated it from Post Offices to corner shops is tough), that a Dubai connection flips a “valid on arrival” buffer into “invalid on departure”. Quiet warnings, early enough to change dates or renew the right booklet, and clear enough that I won't have to improvise at a counter.&lt;/p&gt;
    &lt;p&gt;If the world changes—new examples, revised guidance, a delayed system finally launches—I don’t rewrite history. I version the assumption, keep both answers recorded, and move on.&lt;/p&gt;
    &lt;p&gt;Keeping all those rules up-to-date is hard, so rather than maintaining rules for every country, I parse a few databases, then let users configure their own tracking goals. A user emailed about Cyprus's fast-tracked tax residency scheme; another pointed out I'd missed a few countries entirely. The app gets better as people use it, which feels more honest than pretending to be a global authority on 195 countries' immigration rules.&lt;/p&gt;
    &lt;head rend="h2"&gt;Making things public ¶&lt;/head&gt;
    &lt;p&gt;The app is local by default. Calculations happen on-device, and if you're in airplane mode, it still works. Network is always the bottleneck, and I'm the person who spent a weekend reverse-engineering gym entry to save 44 seconds. I'm not adding server round-trips when you're standing at a gate.&lt;/p&gt;
    &lt;p&gt;Being local also means no liability. Personal immigration history is exactly the kind of data governments might want. Keeping it off my servers means nobody can demand I hand it over. Some friends asked about cloud sync: I keep saying no. Not because sync is hard—it is, though surely Claude Code can do that for me [4]—but because the moment you add a server you add retention policies, jurisdiction questions, and a magnet for legal demands. If you want it on another device, export a file and move it yourself like the ancestors did it.&lt;/p&gt;
    &lt;p&gt;The first version just counted Schengen days, then I added the UK's midnight arithmetic because I needed it for my own calculations. Then documents with their expiry rules because I was tired of manually looking them up. Then the what-if layer because adding and deleting trips to see impact felt like manually diffing state. Then visa requirements and IDP rules: none of this was planned, it accumulated from use, the same way my fermentation tracker grew from "can I eat this?" to HACCP compliance documents.&lt;/p&gt;
    &lt;p&gt;I shipped it because keeping it private felt unfinished, and because I'd like fewer people spending twenty minutes researching whether a £62 return flight will cause problems six months later.&lt;/p&gt;
    &lt;p&gt;That Iceland error fare? I bought it. The app told me I wouldn't need an IDP, that the trip wouldn't push me over any Schengen threshold, that I'd leave with 34 days of margin in my 90/180 window, and—importantly—that booking it would mean I'd stop being a UK tax resident given my upcoming Canada move. Useful things to know before clicking purchase. The officer at Keflavík looked at his screen, agreed with his systems, and waved me through.&lt;/p&gt;
    &lt;p&gt;I called the app Residency and you can get it here. No subscriptions, costs less than an airport martini, and you'll likely regret it less a few hours later.[5]&lt;/p&gt;
    &lt;p&gt;You can't &lt;code&gt;cURL&lt;/code&gt; a border. But you can track your own state carefully enough that when the governments know the answer, so do you.&lt;/p&gt;
    &lt;p&gt;Working on problems where rules are complex and official documentation is &lt;del&gt;garbage&lt;/del&gt; contradictory? I build systems that handle state when the state won't tell you your state. work@drobinin.com&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Until 1752, England’s New Year began on 25 March, aka Lady Day. The calendar reform shaved 11 days off, nudging the tax year to 5 April, and skipping a leap day in 1800 pushed it to 6 April. Two centuries later it stays there probably because changing it would be expensive and annoying. ↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Apparently time zones shift all the time for a variety of reasons, from political to religious. ↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I couldn't find any legit reasons for keeping the "six-month rule" around but it seems like it's still occasionally checked, sometimes even during boarding. ↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;These stunts are performed by trained professionals, don't try this at home. ↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I have nothing against airport bars, but hear me out: vermouth should be kept in the fridge at all times, and glasses need to be chilled. ↩︎&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45806263</guid><pubDate>Tue, 04 Nov 2025 00:37:14 +0000</pubDate></item><item><title>When stick figures fought</title><link>https://animationobsessive.substack.com/p/when-stick-figures-fought</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45806348</guid><pubDate>Tue, 04 Nov 2025 00:48:56 +0000</pubDate></item><item><title>My Truck Desk</title><link>https://www.theparisreview.org/blog/2025/10/29/truck-desk/</link><description>&lt;doc fingerprint="6f48b816f30396dc"&gt;
  &lt;main&gt;
    &lt;p&gt;Photograph courtesy of Bud Smith.&lt;/p&gt;
    &lt;p&gt;After eight glorious weeks of freedom, I got rehired.&lt;/p&gt;
    &lt;p&gt;First thing I did was walk over to the machine shop to look for my F-150. The oil stain was there but the truck wasn’t. It wasn’t in the rock lot where the bulldozers parked either.&lt;/p&gt;
    &lt;p&gt;Who would have stooped so low as to co-opt that piece of shit? It had no heat and no air-conditioning. The radio bubbled static. Door handles were missing. Floorboards, fenders, and frame all rusted and rotted. It certainly hadn’t been what could be called roadworthy. And, my God, the smell.&lt;/p&gt;
    &lt;p&gt;I went into the machine shop. One of the welders lifted his hood and told me the bad news—they’d had to move the truck for a rebar delivery and the engine on that old thing finally blew, so the truck got dragged to the scrapyard.&lt;/p&gt;
    &lt;p&gt;In a dusty corner, I saw a pile of salvaged tools from the truck. I took some wrenches and my tape measure but didn’t see what I was really looking for—my Truck Desk®. Oh well.&lt;/p&gt;
    &lt;p&gt;I caught a ride out to the unit with the foreman and the rest of the crew. Our goal for the day was to unbolt components from a heat exchanger and fly them off with a crane. Once the exchanger was apart and inspected, we’d begin our real repairs.&lt;/p&gt;
    &lt;p&gt;The morning went well. The mornings always go well. Everybody knows what they’re doing. We’re professionals, equals. Same pay. Same benefits. All working together toward retirement. We look out for each other. Whoever has the hardest task in this crew today could be the foreman tomorrow, and vice versa. Nobody wants to be the boss, so our bosses are the best kind.&lt;/p&gt;
    &lt;p&gt;At first break we packed into our truck and drove shoulder-to-shoulder back to the trailer compound for coffee. During the five-minute drive, I couldn’t help but think how good I’d had it when I had the luxury of using that piece of shit F-150.&lt;/p&gt;
    &lt;p&gt;See, the truck nobody else wanted had been my office. I’d built a portable desk inside it. My truck desk, I called it. A couple of planks screwed together, our union sticker slapped on, the whole deal sealed with shellac. I’d built the desk so it slid into the bottom of the steering wheel and sat across the armrests. I used to hang back at the job and sneak in some creative work while the rest of the crew went to break. My desk—which I’d taken far too long to build and perfect through many prototypes—had been stowed behind the driver’s seat when the truck was hauled off by the wrecker.&lt;/p&gt;
    &lt;p&gt;Back at the break trailer, I took my old seat and joined in on the jokes, insults, tall tales. That trailer was, to me, the best place for storytelling in the world—but, as always, it was too loud, too raucous, too fun to do any writing or reading, which is all I ever want to do on break. At lunch, I retreated into the relative quiet of the machine shop. I sat down by the drill press and took out my cell phone and started writing. Just like I used to do.&lt;/p&gt;
    &lt;p&gt;For nearly two decades I’ve worked off and on at this petrochemical plant as a mechanic and welder. The union dispatched me here: When it gets slow, I get laid off; when work picks up, I boomerang back. And the whole time, I’ve written stories and parts of my novels during breaks—fifteen minutes for coffee and then half an hour for lunch. I’ve also made use of the heaven-sent delays brought on by lightning, severe rainstorms, evacuations, permitting problems, equipment issues, and so on. I’m thankful for each and every delay that happens on this construction site, and, believe me, there are many.&lt;/p&gt;
    &lt;p&gt;Most artists I know are like this. Finding time to make art while working another job, or taking care of loved ones. They improvise. They get better. They get worse. They get better again.&lt;/p&gt;
    &lt;p&gt;Really it mostly comes down to that first thing: finding time. When I talk to people who want to find more time, I repeat something an old-timer said to me early on: “You’ve gotta make your own conditions.”&lt;/p&gt;
    &lt;p&gt;What does that mean? Well. Is it raining? You can either stand out in the rain and get wet, or you can find a coil of tie-wire and hang up tarps for a hooch.&lt;/p&gt;
    &lt;p&gt;There’s another expression I like, which goes: “Let your wallet be your guide.” I try to remember that every time I feel the urge to quit my job and never return.&lt;/p&gt;
    &lt;p&gt;So ever since cell phones got smart, I’ve sat somewhere quiet, semi-on-the-clock, texting myself poems, paragraphs that became stories and novels, and things about my life, or I should say just life, like this thing you’re reading right now.&lt;/p&gt;
    &lt;p&gt;Writing on my cell phone, pecking away, was good enough for many years, but then after a rightfully humbling decade of manual labor, I started having irrational fantasies about convenience and comfort.&lt;/p&gt;
    &lt;p&gt;Of course I have a desk in my apartment, but I couldn’t help myself. Somehow I’d gotten seduced by the prospect of attaining my very own cubicle amid this massive junkyard full of toxic waste.&lt;/p&gt;
    &lt;p&gt;One day I walked into the payroll trailer where the secretaries and site manager sat. There wasn’t an explicit sign that said NO CONTRACTORS ALLOWED, but it was an unspoken rule. The trailer had a few unused old cubicles tucked to the side. I sat down in one and happily pecked away with my thumbs. Every break for a week I went in and worked on my writing. After a few days I started to feel like I should hang pictures of my mom and dad and my wife inside it. But I didn’t dare.&lt;/p&gt;
    &lt;p&gt;Then things really heated up. I brought in a Bluetooth keyboard and wrote a whole story that day on my breaks. There was no going back. My heart soared. I thought I should adopt a brown dog with a bandanna around his neck just so I could thumbtack his picture to the cubicle wall. I hadn’t interacted with any of the office staff, but they’d seen me. They’d followed my oily bootprints down the hallway and begun to leer. Who is this diesel-stinking contractor? He’s probably the one who’s been eating Janelle’s Oreos. He raided the mango-kiwi yogurt from the fridge. He glommed all the sporks. I knew my cubicle dreams were over the morning I found the site manager waiting in “my” cubicle.&lt;/p&gt;
    &lt;p&gt;“What are you doing here?” he asked.&lt;/p&gt;
    &lt;p&gt;In all my years working at that place, I’d never seen the site manager out on the site. I’m not sure he knew what it was or where it was. You went to him to order tools; he was the one who said no. I’d only ever seen him at a urinal or buying bacon and eggs off the lunch truck. But if I had ever seen him out on the site, it would have never occurred to me to ask him what he was doing there. He was wearing a blue polo shirt and khakis, and I was in his world—and he was asking.&lt;/p&gt;
    &lt;p&gt;“Office work,” I said.&lt;/p&gt;
    &lt;p&gt;“What kind, exactly?”&lt;/p&gt;
    &lt;p&gt;How can you explain literary fiction to a site manager?&lt;/p&gt;
    &lt;p&gt;“Little bit of everything,” I said.&lt;/p&gt;
    &lt;p&gt;I started writing in the machine shop again. It wasn’t the same. Once I’d been infected by the cubicle virus, there was no going back. Out of scrap lumber I gathered from various dumpsters, I built a proper desk for myself in the northeast corner of the shop. That desk was a huge leap forward in possibility and productivity. In the evenings, if I wrote something by hand or on my typewriter at home, I could now use my time at work to retype it at my shop desk.&lt;/p&gt;
    &lt;p&gt;The shop desk was not ideal. Some days I arrived to find someone had disassembled a small motor on top of it, gaskets and hardware spread out on newspaper. Other times I found pneumatic guns taken apart, or electrical devices with wiring splayed in a colorful tangle, or—fair enough—important blueprints laid out the entire length of the desk.&lt;/p&gt;
    &lt;p&gt;Right around this time I first saw the F-150. One of the workers had abandoned it by the shop. I put a battery in. That lasted one shift. Then I took an alternator out of another junk truck and, lo and behold, I had my own four wheels. The fan belt screamed. The engine smoked. The brakes worked when they wanted to. It was mine that whole dangerous year.&lt;/p&gt;
    &lt;p&gt;Then, one day, my luck changed.&lt;/p&gt;
    &lt;p&gt;A crate full of chain falls got delivered. It was a glorious crate, made of sanded spruce. I unscrewed some of the planking and built my first Truck Desk prototype.&lt;/p&gt;
    &lt;p&gt;It was made of three boards cut at twenty-four inches. Light and compact. Sealed with shellac. It slid into the bottom of the steering wheel, one side supported by a curved rebar I welded into a nut that fit exactly in a recess on the driver’s door. The center console supported the other side of the desk. I kept it stored behind the seat. Whenever break time came and the crew drove back to the trailer compound, I stayed parked on the unit and got at least ten extra minutes to write.&lt;/p&gt;
    &lt;p&gt;Now that I had my Truck Desk, that vehicle was my very own rolling cubicle.&lt;/p&gt;
    &lt;p&gt;Having that truck reminded me of when I lived on 173rd Street in New York City. Back then I used to drive around endlessly looking for street parking. I would see men and women sitting in their cars. They weren’t leaving, though; they were reading a book or a magazine, smoking cigarettes, playing Sudoku, scribbling love letters. They were the wisest men and women in the entire city, using their vehicles as a kind of office down on the street, a sanctuary where they could do their real work.&lt;/p&gt;
    &lt;p&gt;After the F-150 was scrapped, I never got a replacement truck. I never found that first Truck Desk either, even when I called the scrapyard.&lt;/p&gt;
    &lt;p&gt;What I did do, though, was go over to the carpenter’s side of the shop and cut a scaffold plank at twenty-nine inches. This simple plank fits across the armrests of whatever Chevy or Ford pickup the crew has that day. This dramatic redesign of Truck Desk into Truck Plank® took all of ten seconds. I didn’t bother with the sticker or shellac.&lt;/p&gt;
    &lt;p&gt;The years on the job have rolled on. Now editors send me Word documents with comments and questions and tracked changes. I bring my backpack to work with my laptop inside.&lt;/p&gt;
    &lt;p&gt;Every morning, when I find out what crew I’m in, I bring that plank with me. I stick it on the dashboard and climb into the driver’s seat. I drive us all out to the job and at break time I take them to the trailer. I clean my hands with pumice wipes and sit alone in whoever’s truck it is that day, pulling the plank off the dashboard and setting it across the armrests. Within a minute or so, I’ve got the laptop out and I’m working. If somebody from the crew is still in the back seat, bandanna over their eyes, snoozing, I do my best to keep extra quiet. And if they begin to snore, I don’t let that bother me at all.&lt;/p&gt;
    &lt;p&gt;Bud Smith is the author of the novel Teenager and the story collection Double Bird, among other books. Mighty, a novel, is forthcoming from Knopf in spring 2027. His story “Skyhawks” appears in the new Fall issue of The Paris Review.&lt;/p&gt;
    &lt;p&gt;Last / Next Article&lt;/p&gt;
    &lt;p&gt;Share&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45806903</guid><pubDate>Tue, 04 Nov 2025 02:37:01 +0000</pubDate></item><item><title>Tell HN: X is opening any tweet link in a webview whether you press it or not</title><link>https://news.ycombinator.com/item?id=45807775</link><description>&lt;doc fingerprint="d49d99132ed61bc5"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Just saw the CEO of Substack celebrating traffic from X/Twitter shooting up thinking they stopped suppressing tweets with links[0]. Actually, this traffic is because now any time you open a tweet with a link, the in-app webview loads in the background, and displays when you press the link.&lt;/p&gt;
      &lt;p&gt;I run an ecom store that gets a lot of its customers from Twitter. I was also shocked to see my traffic double or triple overnight and thought the algorithm had blessed me and my business. Soon realized what was actually happening. Thought other traffic-monitors might appreciate this explanation.&lt;/p&gt;
      &lt;p&gt;Meanwhile Nikita Bier is pretending they never suppressed tweets with links to begin with, offering the alternative explanation: "a common complaint is that posts with links tend to get lower reach. This is because the web browser covers the post and people forget to Like or Reply. So X doesn't get a clear signal whether the content is any good"[1]. A bit of a rewriting of history since Elon and his mom both tweeted about how it wasn't fair to use his platform to promote other links/platforms, even banning people who shared profiles of other social networks (including Paul Graham for a period). They suppressed all links shortly after.&lt;/p&gt;
      &lt;p&gt;[0] https://x.com/cjgbest/status/1985464687350485092&lt;/p&gt;
      &lt;p&gt;[1] https://x.com/nikitabier/status/1979994223224209709&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45807775</guid><pubDate>Tue, 04 Nov 2025 05:53:02 +0000</pubDate></item><item><title>Bloom filters are good for search that does not scale</title><link>https://notpeerreviewed.com/blog/bloom-filters/</link><description>&lt;doc fingerprint="7b8d83e54decc003"&gt;
  &lt;main&gt;
    &lt;p&gt;A great blog post from 2013 describes using bloom filters to build a space-efficient full text search index for small numbers of documents. The algorithm is simple: Per document, create a bloom filter of all its words. To query, simply check each document's bloom filter for the query terms.&lt;/p&gt;
    &lt;p&gt;With a query time complexity of O(number-of-documents), we can forget about using this on big corpuses, right? In this blog post I propose a way of scaling the technique to large document corpuses (e.g. the web) and discuss why that is a bad idea.&lt;/p&gt;
    &lt;p&gt;Fun fact: There is a nice implementation of this exact algorithm that is still used in the wild. But let's get into it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why even try this?&lt;/head&gt;
    &lt;p&gt;The bloom filter index's big selling point is its small size. It allows static websites with dozens of pages to ship a full text search index to the client that is as small as a small image. An equivalent inverted index, which is the traditional textbook approach for keyword-based full text search, would be multiple times bigger.&lt;/p&gt;
    &lt;p&gt;But index size is not only relevant on small blog websites. If we could scale this technique to larger document corpuses and achive similar space savings, that would be huge!&lt;/p&gt;
    &lt;p&gt;The main thing that seems to stand in our way is query performance. Instead of always checking every document's bloom filter, we will try to construct an index that only checks a small subset of filters, but still finds all matching documents.&lt;/p&gt;
    &lt;head rend="h2"&gt;Some Ideas that don't work at all&lt;/head&gt;
    &lt;p&gt;Look, I brainstormed a bunch of ideas for how to improve the bloom filter based index. I will quickly go over two of them, because identifying and discarding ideas that will not work is an important part of science and engineering.&lt;/p&gt;
    &lt;head rend="h3"&gt;Sort the filters&lt;/head&gt;
    &lt;p&gt;If we sort the filters by some metric, for example by the most to least significant bits, then we can use a binary search algorithm or something like that, right? - Wrong.&lt;/p&gt;
    &lt;p&gt;We can construct a simple counter example to show that this does not work. Here the query is matched by the first and the last filter in a sorted list of filters.&lt;/p&gt;
    &lt;head rend="h3"&gt;Tree of filters&lt;/head&gt;
    &lt;p&gt;Plain sorting does not work, but what if we structure our big set of filters into a tree? Imagine it sort of like this, but much bigger.&lt;/p&gt;
    &lt;p&gt;At each branch node, we construct an aggregate filter that encodes all documents that are reachable from the branch. Aggregate filters are constructed by a simple bitwise or of the other filters like this.&lt;/p&gt;
    &lt;p&gt;When we get a query, we first check it against the top level branch filters. If a filter does not match e.g. the filter for document 6-10, we can discard that entire branch of the tree for this query.&lt;/p&gt;
    &lt;p&gt;Ideally we would like to search as few branches of the tree as possible to improve performance. How many branches we do need to search, depends heavily on the partitioning of the documents. Intuitively, we can think of it like this: branch A should contain all documents that contain the words "dog", "cat", "bird". Branch B should contain documents with "car", "bus", "plane". The fewer branches each word is contained in the better.&lt;/p&gt;
    &lt;p&gt;Here comes the problem: What if there is a document that says "I took my cat on the bus today"?&lt;lb/&gt; This breaks our assumtion above, and suddenly for the query "bus" we need to search both branches. You can imagine this happening for almost every word in the dicionary across all branches, because language is complex and lets us say so many different things in many different contexts.&lt;/p&gt;
    &lt;p&gt;Or in other words: Text documents are high-dimensional.&lt;lb/&gt; I recommend reading about the curse of dimensionality for an intuition of what issues this implies. Here it means that it is basically impossible to cluster text documents into disjunct subsets without significant overlap. For our seach index that means that even when using this tree, we would still need to search almost every document for every query.&lt;/p&gt;
    &lt;head rend="h2"&gt;Inverted Index of Bloom Filters&lt;/head&gt;
    &lt;p&gt;The problem with our previous tree-based idea is that there is so much overlap between text documents. But I know one book that only contains every word exactly once: The dictionary. We can construct a search tree of the entire dictionary, again based on bloom filters. Each leaf represents a set of words. At each leaf we keep a list of pointers to every document's filter that contains one of those words.&lt;/p&gt;
    &lt;p&gt;Maybe not so incidentally, this looks a lot like an inverted index. And it works! For any query term we can walk the tree to the leaf that contains the query term and then we match only against the filters at that leaf. Instead of a hash table, as in the inverted index, our index uses a tree for the dictionary, but fundamentally it does a similar thing.&lt;/p&gt;
    &lt;p&gt;The big difference is that the tree can be smaller than the hash table. Remember, size is the main reason to attempt this at all. Not only is there no empty space in a tree, but we also encode all the words in our bloom filters instead of storing them outright. Modern bloom filters (actually called Xor filters) require about ten bits per element [1], much less than the 8 bits per character required to store a full word.&lt;/p&gt;
    &lt;p&gt;As an aside, bloom filters are indeed already used in full text search for large-ish datasets, but in the form of skip-indexes. In a skip index, a bloom filter is used to quickly check if a large chunk of data contains a value (e.g. a word) at all. That way, a database can avoid reading chunks of data that do not contain any records for a given query. Until very recently this technique was used by the Clickhouse OLAP system for full text search [2]. It has been superseeded by a proper inverted index in 2025.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why all of this is still a bad idea&lt;/head&gt;
    &lt;p&gt;We did it! We have a working idea for a bloom filter based search index that works for large document corpuses. The query time complexity is not as good as for an inverted index, but it is logarithmic with the number of documents. That is good enough if you ask me. So why do I write that it is still a bad idea?&lt;/p&gt;
    &lt;p&gt;Let's think about what allows the bloom filter based index to be small again. Instead of storing the entire dictionary in our index, we use bloom filters that require about ten bits per word. Ten bits per word. Ten bits per every word. Not unique word. Every word in our document corpus (except duplicates in the same document). To make our math exceedingly simple, let's say the english dictionary has about 500 Thousand unique words and every document contains 1000 distinct words. At ten bits per entry for a bloom filter, that makes each document's filter about 1.25kb. Assume that words are on average ten characters long, then the dictionary will require 5mb for the text alone. We can assume another 4mb for the inverted index's hash table to get a lower bound of 9mb for the inverted index. Both indexes require similar amounts of space for document ids and pointers, so relative to the inverted index, our bloom filter index grows by 1.25kb per document. Divide 9mb by 1.25kb and you find out that at only 7200 documents the inverted index becomes more space efficient than the bloom filter index. Of course the real numbers will be different and we are ignoring some things here, but the trend will stay the same.&lt;/p&gt;
    &lt;p&gt;What is going on here is that while an inverted index must store every word in the dictionary exactly once, sharing the space when a word is reused, bloom filters do not share space amongst each other. Every document's bloom filter must encode all words in the document from scratch. If a word is contained in thousands of documents, that requires much more space than simply storing the word in plain text.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusions&lt;/head&gt;
    &lt;p&gt;When you have a small number of documents relative to the size of your dictionary, bloom filters can indeed achieve a much smaller full text search index than is possible traditionally.&lt;/p&gt;
    &lt;p&gt;Bloom filters are space efficient when compressing a large dictionary into a small number of filters. As more filters share the same dictionary, this efficiency decreases. Intuitively this is because bloom filters cannot share information amongst each other. Each filter must encode its entire dictionary from scratch. An inverted index does not do this. It only stores the dictionary once and shares it for all documents, so it gets more space efficient with the number of documents.&lt;/p&gt;
    &lt;p&gt;More generally, there is no synergy between bloom filters. Each filter on its own is efficient, but as a whole system, a different approach might be more efficient. We can transfer this insight to other problem domains as well. For example, imagine a content moderation system on a social media platform that allows blocking individual accounts. If we have one global blocklist on our platform, a bloom filter can be an efficient (though maybe not ideal) implementation of this. But allow every user to create their own blocklist and a different design will be more scaleable.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45808998</guid><pubDate>Tue, 04 Nov 2025 09:25:31 +0000</pubDate></item><item><title>What is a manifold?</title><link>https://www.quantamagazine.org/what-is-a-manifold-20251103/</link><description>&lt;doc fingerprint="ba91984004c1afe9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;What Is a Manifold?&lt;/head&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;Standing in the middle of a field, we can easily forget that we live on a round planet. We’re so small in comparison to the Earth that from our point of view, it looks flat.&lt;/p&gt;
    &lt;p&gt;The world is full of such shapes — ones that look flat to an ant living on them, even though they might have a more complicated global structure. Mathematicians call these shapes manifolds. Introduced by Bernhard Riemann in the mid-19th century, manifolds transformed how mathematicians think about space. It was no longer just a physical setting for other mathematical objects, but rather an abstract, well-defined object worth studying in its own right.&lt;/p&gt;
    &lt;p&gt;This new perspective allowed mathematicians to rigorously explore higher-dimensional spaces — leading to the birth of modern topology, a field dedicated to the study of mathematical spaces like manifolds. Manifolds have also come to occupy a central role in fields such as geometry, dynamical systems, data analysis and physics.&lt;/p&gt;
    &lt;p&gt;Today, they give mathematicians a common vocabulary for solving all sorts of problems. They’re as fundamental to mathematics as the alphabet is to language. “If I know Cyrillic, do I know Russian?” said Fabrizio Bianchi, a mathematician at the University of Pisa in Italy. “No. But try to learn Russian without learning Cyrillic.”&lt;/p&gt;
    &lt;p&gt;So what are manifolds, and what kind of vocabulary do they provide?&lt;/p&gt;
    &lt;head rend="h2"&gt;Ideas Taking Shape&lt;/head&gt;
    &lt;p&gt;For millennia, geometry meant the study of objects in Euclidean space, the flat space we see around us. “Until the 1800s, ‘space’ meant ‘physical space,’” said José Ferreirós, a philosopher of science at the University of Seville in Spain — the analogue of a line in one dimension, or a flat plane in two dimensions.&lt;/p&gt;
    &lt;p&gt;In Euclidean space, things behave as expected: The shortest distance between any two points is a straight line. A triangle’s angles add up to 180 degrees. The tools of calculus are reliable and well defined.&lt;/p&gt;
    &lt;p&gt;But by the early 19th century, some mathematicians had started exploring other kinds of geometric spaces — ones that aren’t flat but rather curved like a sphere or saddle. In these spaces, parallel lines might eventually intersect. A triangle’s angles might add up to more or less than 180 degrees. And doing calculus can become a lot less straightforward.&lt;/p&gt;
    &lt;p&gt;The mathematical community struggled to accept (or even understand) this shift in geometric thinking.&lt;/p&gt;
    &lt;p&gt;But some mathematicians wanted to push these ideas even further. One of them was Bernhard Riemann, a shy young man who had originally planned to study theology — his father was a pastor — before being drawn to mathematics. In 1849, he decided to pursue his doctorate under the tutelage of Carl Friedrich Gauss, who had been studying the intrinsic properties of curves and surfaces, independent of the space surrounding them.&lt;/p&gt;
    &lt;p&gt;In 1854, Riemann was required to deliver a lecture to secure a teaching position at the University of Göttingen. His assigned topic: the foundations of geometry. On June 10, despite a fear of public speaking, he described a new theory in which he generalized Gauss’ ideas about the geometry of surfaces to an arbitrary number of dimensions (and even to infinite dimensions).&lt;/p&gt;
    &lt;p&gt;Gauss was immediately impressed with the lecture, which involved not just math but also philosophy and physics. But most mathematicians found Riemann’s ideas too vague and abstract to be of much use. “Many scientists and philosophers were saying, ‘This is nonsense,’” Ferreirós said. And so, for decades, the work was largely ignored. Riemann’s lecture didn’t appear in print until 1868, two years after his death.&lt;/p&gt;
    &lt;p&gt;But by the end of the 19th century, mathematical greats like Henri Poincaré had recognized the importance of Riemann’s ideas. And in 1915, Albert Einstein used them in his general theory of relativity, bringing them out of the realm of philosophical abstraction and into the real world. By the middle of the 20th century, they had become a mathematical staple.&lt;/p&gt;
    &lt;p&gt;Riemann had introduced a concept that could encompass all possible geometries, in any number of dimensions. A concept that would change how mathematicians view space.&lt;/p&gt;
    &lt;p&gt;A manifold.&lt;/p&gt;
    &lt;head rend="h2"&gt;Charted Territory&lt;/head&gt;
    &lt;p&gt;The term “manifold” comes from Riemann’s Mannigfaltigkeit, which is German for “variety” or “multiplicity.”&lt;/p&gt;
    &lt;p&gt;A manifold is a space that looks Euclidean when you zoom in on any one of its points. For instance, a circle is a one-dimensional manifold. Zoom in anywhere on it, and it will look like a straight line. An ant living on the circle will never know that it’s actually round. But zoom in on a figure eight, right at the point where it crosses itself, and it will never look like a straight line. The ant will realize at that intersection point that it’s not in a Euclidean space. A figure eight is therefore not a manifold.&lt;/p&gt;
    &lt;p&gt;Similarly, in two dimensions, the surface of the Earth is a manifold; zoom in far enough anywhere on it, and it’ll look like a flat 2D plane. But the surface of a double cone — a shape consisting of two cones connected at their tips — is not a manifold.&lt;/p&gt;
    &lt;p&gt;Manifolds address a problem that mathematicians would otherwise have to deal with: A shape’s properties can change depending on the nature and dimension of the space it lives in (and how it sits in that space). For instance, lay a piece of string on a table, and connect its ends without lifting it. You’ll get a simple loop. Now hold the string in the air and tie its ends together. By considering the string in three dimensions, you can pass it over and under itself before you connect the ends, creating all sorts of knots beyond the simple loop. They all represent the same one-dimensional manifold — the looped string — but they have different properties when considered in two versus three dimensions.&lt;/p&gt;
    &lt;p&gt;Mathematicians avoid such ambiguities by focusing on the manifold’s intrinsic properties. The defining property of manifolds — that at any point, they look Euclidean — is immensely helpful on that front. Because it’s possible to think about any small patch of the manifold in terms of Euclidean space, mathematicians can use traditional calculus techniques to, say, compute its area or volume, or describe movement on it.&lt;/p&gt;
    &lt;p&gt;To do this, mathematicians divide a given manifold into several overlapping patches and represent each with a “chart” — a set of some number of coordinates (equal to the manifold’s dimension) that tell you where you are on the manifold. Crucially, you also need to write down rules that describe how the coordinates of overlapping charts relate to one another. The collection of all these charts is called an atlas.&lt;/p&gt;
    &lt;p&gt;You can then use this atlas — whose charts translate smaller regions of your potentially complicated manifold into familiar Euclidean space — to measure and explore the manifold one patch at a time. If you want to understand how a function behaves on a manifold, or get a sense of its global structure, you can break the problem up into pieces, solve each piece on a different chart, in Euclidean space, and then stitch together the results from all the charts in the atlas to get the full answer you’re seeking.&lt;/p&gt;
    &lt;p&gt;Today, this approach is ubiquitous throughout math and physics.&lt;/p&gt;
    &lt;head rend="h2"&gt;Manifold Uses&lt;/head&gt;
    &lt;p&gt;Manifolds are crucial to our understanding of the universe, for one. In his general theory of relativity, Einstein described space-time as a four-dimensional manifold, and gravity as that manifold’s curvature. And the three-dimensional space we see around us is also a manifold — one that, as manifolds do, appears Euclidean to those of us living within it, even though we’re still trying to figure out its global shape.&lt;/p&gt;
    &lt;p&gt;Even in cases where manifolds don’t seem to be present, mathematicians and physicists try to rewrite their problems in the language of manifolds to make use of their helpful properties. “So much of physics comes down to understanding geometry,” said Jonathan Sorce, a theoretical physicist at Princeton University. “And often in surprising ways.”&lt;/p&gt;
    &lt;p&gt;Consider a double pendulum, which consists of one pendulum hanging from the end of another. Small changes in the double pendulum’s initial conditions lead it to carve out very different trajectories through space, making its behavior hard to predict and understand. But if you represent the configuration of the pendulum with just two angles (one describing the position of each of its arms), then the space of all possible configurations looks like a doughnut, or torus — a manifold. Each point on this torus represents one possible state of the pendulum; paths on the torus represent the trajectories the pendulum might follow through space. This allows researchers to translate their physical questions about the pendulum into geometric ones, making them more intuitive and easier to solve. This is also how they study the movements of fluids, robots, quantum particles and more.&lt;/p&gt;
    &lt;p&gt;Similarly, mathematicians often view the solutions to complicated algebraic equations as a manifold to better understand their properties. And they analyze high-dimensional datasets — such as those recording the activity of thousands of neurons in the brain — by looking at how those data points might sit on a lower-dimensional manifold.&lt;/p&gt;
    &lt;p&gt;Asking how scientists use manifolds is akin to asking how they use numbers, Sorce said. “They are at the foundation of everything.”&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45809193</guid><pubDate>Tue, 04 Nov 2025 09:58:14 +0000</pubDate></item><item><title>Chaining FFmpeg with a Browser Agent</title><link>https://100x.bot/a/chaining-ffmpeg-with-browser-agent</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=45810430</guid><pubDate>Tue, 04 Nov 2025 12:52:54 +0000</pubDate></item><item><title>Show HN: A CSS-Only Terrain Generator</title><link>https://terra.layoutit.com</link><description>&lt;doc fingerprint="109996e321a537ae"&gt;
  &lt;main&gt;
    &lt;p&gt;CSS Terrain Generator Regenerate Restart Undo Redo Import Export Heightmap CSS VOX TXT PNG Copy Embed Open Codepen Download Code move raise lower about world size ✕ ✕ landmass coverage small medium large terrain type pampas hilly alpinist biome temperate arctic desert camera settings rotate x 45° tilt y 60° zoom 50% pan x 0px lift y 0px animate reset to defaults minimap heightmap matrix v0.0.1 Regenerate&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45811093</guid><pubDate>Tue, 04 Nov 2025 13:58:35 +0000</pubDate></item><item><title>Optimizing Datalog for the GPU</title><link>https://danglingpointers.substack.com/p/optimizing-datalog-for-the-gpu</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45811447</guid><pubDate>Tue, 04 Nov 2025 14:31:27 +0000</pubDate></item><item><title>How devtools map minified JS code back to your TypeScript source code</title><link>https://www.polarsignals.com/blog/posts/2025/11/04/javascript-source-maps-internals</link><description>&lt;doc fingerprint="2e607229161f0532"&gt;
  &lt;main&gt;
    &lt;p&gt;Source maps are the main piece in the jigsaw puzzle of mapping symbols and locations from "built" JavaScript files back to the original source code. When you debug minified JavaScript in your browser's DevTools and see the original source with proper variable names and formatting, you're witnessing source maps in action.&lt;/p&gt;
    &lt;p&gt;For example, when your browser encounters an error at &lt;code&gt;bundle.min.js:1:27698&lt;/code&gt;, the source map translates this to &lt;code&gt;src/index.ts:73:16&lt;/code&gt;, revealing exactly where the issue occurred in your original TypeScript code:&lt;/p&gt;
    &lt;p&gt;But how does this actually work under the hood? In this post, we'll take a deep dive into the internals of source maps, exploring their format, encoding mechanisms, and how devtools use them to bridge the gap between production code and developer-friendly sources.&lt;/p&gt;
    &lt;head rend="h2"&gt;The TypeScript Build Pipeline&lt;/head&gt;
    &lt;p&gt;Modern JavaScript builds typically involve three main stages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Transpilation: TypeScript → JavaScript&lt;/item&gt;
      &lt;item&gt;Bundling: Combining modules into a single file&lt;/item&gt;
      &lt;item&gt;Minification: Compressing code for production&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;At each stage, source maps preserve the connection back to the original code.&lt;/p&gt;
    &lt;head rend="h3"&gt;Stage 0: Source TS files&lt;/head&gt;
    &lt;p&gt;The original TypeScript source files with full type annotations.&lt;/p&gt;
    &lt;head rend="h4"&gt;Source Files&lt;/head&gt;
    &lt;quote&gt;1export function add(a: number, b: number): number {2 return a + b;3}&lt;/quote&gt;
    &lt;quote&gt;1import { add } from './add';23export function computeFibonacci(n: number): number {4 if (n &amp;lt;= 1) return n;5 return add(computeFibonacci(n - 1), computeFibonacci(n - 2));6}&lt;/quote&gt;
    &lt;quote&gt;1import { computeFibonacci } from './fibonacci';23const result = computeFibonacci(10);4console.log(`Fibonacci(10) = ${result}`);&lt;/quote&gt;
    &lt;p&gt;No source map at this stage&lt;/p&gt;
    &lt;head rend="h2"&gt;The Source Map File Format&lt;/head&gt;
    &lt;p&gt;Source maps use JSON format, typically with a &lt;code&gt;.js.map&lt;/code&gt; extension. Let's examine a source map structure from our &lt;code&gt;add.js.map&lt;/code&gt; file:&lt;/p&gt;
    &lt;code&gt;{
  "version": 3,
  "file": "add.js",
  "sourceRoot": "",
  "sources": ["add.ts"],
  "names": ["add", "a", "b"],
  "mappings": "AAAA,OAAO,SAAS,IAAI,CAAC,EAAE;EACrB,OAAO,IAAI;AACb"
}
&lt;/code&gt;
    &lt;head rend="h5"&gt;Fields Breakdown:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;version&lt;/code&gt;: Indicates the source map version (currently always&lt;code&gt;3&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;file&lt;/code&gt;: The generated file name this source map corresponds to.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;sourceRoot&lt;/code&gt;: Optional prefix for all source URLs. Useful when sources are hosted elsewhere.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;sources&lt;/code&gt;: Array of original source file paths from which the generated file was built.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;sourcesContent&lt;/code&gt;: Optional array containing the actual source code. This allows DevTools to display sources even if the original files aren't accessible. Usually disabled in production builds.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;names&lt;/code&gt;: Array of original identifiers (variable names, function names, etc.) that appear in the source. Referenced by the mappings.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;mappings&lt;/code&gt;: The compressed mapping data. This is the heart of the source map and uses VLQ encoding. More on this below.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Understanding the Mappings: VLQ Encoding&lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;mappings&lt;/code&gt; field is where the real magic happens. It contains the actual position mappings between every token in the generated JavaScript file and its corresponding location in the original source files.&lt;/p&gt;
    &lt;p&gt;Essentially, it answers the question: "For this character at line X, column Y in the minified file, where was it originally located?"&lt;/p&gt;
    &lt;p&gt;This mapping data tracks:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The file path and name of the original source file&lt;/item&gt;
      &lt;item&gt;The exact line and column in the source file&lt;/item&gt;
      &lt;item&gt;The original variable/function name (if renamed during minification)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But instead of storing this as a massive JSON array of positions, which would be larger than the minified code itself, source maps use a highly compressed format. Here's what the encoded string looks like:&lt;/p&gt;
    &lt;code&gt;"AAAA,OAAO,SAAS,IAAI,CAAC,EAAE;EACrB,OAAO,IAAI;AACb"
&lt;/code&gt;
    &lt;p&gt;To keep file sizes manageable, mappings use Variable Length Quantity (VLQ) encoding with Base64 characters. Let's break this down.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Mapping Structure&lt;/head&gt;
    &lt;p&gt;The mappings string is a series of segments separated by commas and semicolons:&lt;/p&gt;
    &lt;code&gt;"segment,segment,segment;segment,segment;segment"
&lt;/code&gt;
    &lt;p&gt;We'll see significance of commas and semicolons shortly, but first, what is a "segment"?&lt;/p&gt;
    &lt;p&gt;Each segment represents a mapping from a position in the generated file to a position in the source file. Segments come in three flavors:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;1 value: This referenced column doesn't map to any source (e.g., webpack-generated code)&lt;/p&gt;
        &lt;code&gt;[generatedColumn]&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;4 values: This is the most common case, mapping a position in the generated file to a position in the source file:&lt;/p&gt;
        &lt;code&gt;[generatedColumn, sourceFileIndex, sourceLine, sourceColumn]&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;5 values: Same as 4, plus a reference to the original name of the variable/function:&lt;/p&gt;
        &lt;code&gt;[generatedColumn, sourceFileIndex, sourceLine, sourceColumn, nameIndex]&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The most common case is 4 values (basic position mapping). The 5th value is only added when a variable or function was renamed during minification.&lt;/p&gt;
    &lt;p&gt;But wait, notice that segments only contain the column in the generated file, not the line number. How does the decoder know which line a segment belongs to?&lt;/p&gt;
    &lt;p&gt;The answer lies in the structure: semicolons act as line breaks. The position of segments between semicolons determines their line number in the generated file.&lt;/p&gt;
    &lt;p&gt;This is why empty lines in the generated file still need semicolons, they maintain the line count even with no mappings.&lt;/p&gt;
    &lt;p&gt;Let's see how this works with a real example:&lt;/p&gt;
    &lt;p&gt;Notice how the decoded values give relative positions, each value represents the difference from the previous position, not absolute coordinates. This is crucial: instead of encoding large column numbers like 27698 in minified files, source maps only store small deltas like +7 or +15, making the encoded strings much more compact.&lt;/p&gt;
    &lt;p&gt;Now that we understand the mapping structure, let's see how these numbers actually get transformed into the Base64 alphabet characters we see in the mappings string.&lt;/p&gt;
    &lt;head rend="h3"&gt;How VLQ Encoding Works&lt;/head&gt;
    &lt;p&gt;VLQ (Variable Length Quantity) encoding is an efficient way to represent numbers using as few bytes as possible. It's perfect for source maps because most position differences are small numbers.&lt;/p&gt;
    &lt;p&gt;The encoding process has three main steps:&lt;/p&gt;
    &lt;p&gt;1. Encode the sign bit&lt;/p&gt;
    &lt;p&gt;Since we need to handle both positive and negative differences (code can move backward), VLQ uses the least significant bit (LSB) to encode the sign:&lt;/p&gt;
    &lt;code&gt;Positive number: LSB = 0
Negative number: LSB = 1

Examples:
 5 → binary: 101 → with sign bit: 1010 (LSB=0 for positive)
-5 → binary: 101 → with sign bit: 1011 (LSB=1 for negative)
&lt;/code&gt;
    &lt;p&gt;2. Split into 5-bit groups&lt;/p&gt;
    &lt;p&gt;Each Base64 character can represent 6 bits, but we need 1 bit as a "continuation" flag to indicate if more characters follow. This leaves 5 bits for data:&lt;/p&gt;
    &lt;code&gt;[continuation bit][5 data bits]
       ↑              ↑
   1 = more coming    actual value bits
   0 = last character
&lt;/code&gt;
    &lt;p&gt;3. Convert to Base64&lt;/p&gt;
    &lt;p&gt;Map each 6-bit value to a Base64 character:&lt;/p&gt;
    &lt;code&gt;A=0, B=1, C=2... Z=25, a=26, b=27... z=51, 0=52, 1=53... 9=61, +=62, /=63
&lt;/code&gt;
    &lt;head rend="h6"&gt;Example&lt;/head&gt;
    &lt;p&gt;Lets go through the steps to encode the number 7:&lt;/p&gt;
    &lt;p&gt;That's why in our mapping example, the value 7 is encoded as 'O'!&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;I hope this deep dive into JavaScript source maps has shed light on how they function under the hood and adds to your appreciation for the amount of position data they efficiently encode.&lt;/p&gt;
    &lt;p&gt;P.S. Stay tuned: source maps support is coming to parca-agent and Polar Signals Cloud, bringing the same debugging magic to your performance profiling workflow!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45812000</guid><pubDate>Tue, 04 Nov 2025 15:21:16 +0000</pubDate></item><item><title>This Day in 1988, the Morris worm infected 10% of the Internet within 24 hours</title><link>https://www.tomshardware.com/tech-industry/cyber-security/on-this-day-in-1988-the-morris-worm-slithered-out-and-sparked-a-new-era-in-cybersecurity-10-percent-of-the-internet-was-infected-within-24-hours</link><description>&lt;doc fingerprint="ff40223f0be08a6f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;37 years ago this week, the Morris worm infected 10% of the Internet within 24 hours — worm slithered out and sparked a new era in cybersecurity&lt;/head&gt;
    &lt;p&gt;The Internet contracted worms a year before the World Wide Web was even a thing.&lt;/p&gt;
    &lt;p&gt;This week in 1988, Cornell graduate student Robert Tappan Morris unleashed his eponymous worm upon the Internet. The wave of infections grew to 10% of the entire Internet within 24 hours, causing astronomically expensive damage for the time. However, the pioneering Morris worm malware wasn’t made with malice, says an FBI retrospective on the “programming error.” It was designed to gauge the size of the Internet, resulting in a classic case of unintended consequences.&lt;/p&gt;
    &lt;head rend="h2"&gt;Morris worm dissection&lt;/head&gt;
    &lt;p&gt;Known to be something of a prankster, Morris must have felt some foreboding about releasing his ‘innocent’ program into the wild. Evidence of this comes from his release method. “He released it by hacking into an MIT computer from his Cornell terminal in Ithaca, New York,” according to the FBI.&lt;/p&gt;
    &lt;p&gt;The Morris worm was written in C and targeted BSD UNIX systems, like VAX and Sun-3 machines. Specifically, the FBI writes, it “exploited a backdoor in the Internet’s electronic mail system and a bug in the ‘finger’ program that identified network users.” In contrast to computer viruses, the worm Morris had devised had no need of a host program, but could self-replicate and spread autonomously.&lt;/p&gt;
    &lt;p&gt;Thankfully, the Morris worm wasn’t written to cause damage to files. Due to those unintended consequences, though, it precipitated massive slowdowns, and messaging delays and system crashes were common symptoms. It became a computer news sensation in the worst possible way. Just to get rid of the worm in a timely fashion, some institutions ended up wiping complete systems and unplugging networks for as long as a week.&lt;/p&gt;
    &lt;p&gt;Among the Morris worm's casualties were prestigious institutions such as Berkeley, Harvard, Princeton, Stanford, Johns Hopkins, NASA, and the Lawrence Livermore National Laboratory.&lt;/p&gt;
    &lt;head rend="h2"&gt;Whodunit?&lt;/head&gt;
    &lt;p&gt;Experts worked hard to find a fix, and while they did so, the question of who was behind the worm came to the fore. Understandably, whoever created and unleashed this worm needed to feel some consequences, and thus, the FBI was brought in.&lt;/p&gt;
    &lt;p&gt;Apparently, Morris sought to anonymously explain and apologize for the worm, but an inadvertent slip of his initials by a friend landed Morris in it.&lt;/p&gt;
    &lt;p&gt;Get Tom's Hardware's best news and in-depth reviews, straight to your inbox.&lt;/p&gt;
    &lt;p&gt;FBI interviews and computer file analysis would subsequently confirm Morris was the culprit. He was indicted under the rather freshly inked Computer Fraud and Abuse Act of 1986. After a court appearance for his misdemeanors in 1989, Morris ended up not with jail time, but with a fine, probation, and 400 hours of community service to complete.&lt;/p&gt;
    &lt;head rend="h2"&gt;Computer worms have been around longer than the World Wide Web&lt;/head&gt;
    &lt;p&gt;Back in November 1988, the Internet bore little resemblance to what it is today. For example, the World Wide Web (WWW) wasn’t even a thing. Though the WWW would soon form the core experience for the first tide of surfers in the 90s.&lt;/p&gt;
    &lt;p&gt;At the time, the Internet’s backbone was the NSFNET, the recent successor to ARPANET. Its purpose was mostly to expand the prior backbone’s reach beyond military and defense institutions, and it more broadly embraced academia. While we are here, it is worth mentioning that NSFNET was decommissioned in 1995, and succeeded by the commercial Internet, which emerged in the 1990s off the back of private ISPs and commercial backbones.&lt;/p&gt;
    &lt;p&gt;So, when we talk about 10% of the Internet being paralyzed by the Morris Worm, contemporary estimates are that about 6,000 of the approximately 60,000 connected systems were infected and impacted. Moreover, when we highlighted the potentially massive costs of this first worm propagating, estimates range from $100,000 to millions of dollars.&lt;/p&gt;
    &lt;p&gt;Computer worms have remained a scary phenomenon in recent times. For example, we reported on the first-generation AI worm, the Morris II generative AI worm, last year.&lt;/p&gt;
    &lt;p&gt;Follow Tom's Hardware on Google News, or add us as a preferred source, to get our latest news, analysis, &amp;amp; reviews in your feeds.&lt;/p&gt;
    &lt;p&gt;Mark Tyson is a news editor at Tom's Hardware. He enjoys covering the full breadth of PC tech; from business and semiconductor design to products approaching the edge of reason.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;header&gt;sb5k&lt;/header&gt;I was working at DEC when the worm slithered its way across the Internet, as part of an engineering team. I also helped manage our Ultrix systems; our IT department knew VMS only.Reply&lt;lb/&gt;I don't remember which CPU was in our systems, but the worm was not able to run on our systems, but I did find it dropped in them.&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;Gaston404&lt;/header&gt;I completely disagree with the tone of the article. Depicting this as an accident without consequences and limited effect is simply incorrect.Reply&lt;lb/&gt;Back then as a part time job I managed some of the traffic routing through Washington DC. Mail relays were shutdown and backed up queues were spooked off to tape. By today’s standards the volume of traffic may seem trivial but when many of these links ran at 56kbps or less. It was a mess. The main way administrators communicated with each other was email. This also affected collaboration between University researchers and access to the NSF super computer centers.&lt;lb/&gt;At the time rumors maintained that Morris used exploits that he learned from his father who had a consulting agreement with the NSA. So if this is true there is a certain level of non-originality.&lt;lb/&gt;On one hand stronger persecution may have reduced follow on internet crime. On the other hand the fragility demonstrated by this crime, resulted in the creation of procedures to deal with outages. If anything the naive sense of trusted collaboration that pervaded the Internet started to fade.&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;derekullo&lt;/header&gt;In 9 years, Tiktok has infected over 90% of the internet!Reply&lt;lb/&gt;Much slower but also much more insidious!&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;DS426&lt;/header&gt;Reply&lt;quote/&gt;The next big social media craze is probably just around the corner. I shutter to think how ludicrous it will be.derekullo said:In 9 years, Tiktok has infected over 90% of the internet!&lt;lb/&gt;Much slower but also much more insidious!&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45812024</guid><pubDate>Tue, 04 Nov 2025 15:23:14 +0000</pubDate></item><item><title>Pg_lake: Postgres with Iceberg and data lake access</title><link>https://github.com/Snowflake-Labs/pg_lake</link><description>&lt;doc fingerprint="fb9ba072642955ea"&gt;
  &lt;main&gt;
    &lt;p&gt;&lt;code&gt;pg_lake&lt;/code&gt; integrates Iceberg and data lake files into Postgres. With the &lt;code&gt;pg_lake&lt;/code&gt; extensions, you can use Postgres as a stand-alone lakehouse system that supports transactions and fast queries on Iceberg tables, and can directly work with raw data files in object stores like S3.&lt;/p&gt;
    &lt;p&gt;At a high level, &lt;code&gt;pg_lake&lt;/code&gt; lets you:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Create and modify Iceberg tables directly from PostgreSQL, with full transactional guarantees and query them from other engines&lt;/item&gt;
      &lt;item&gt;Query and import data from Parquet, CSV, JSON, and Iceberg files stored in S3 or other compatible object stores&lt;/item&gt;
      &lt;item&gt;Export query results back to S3 in Parquet, CSV, or JSON formats using COPY commands&lt;/item&gt;
      &lt;item&gt;Read geospatial formats supported by GDAL, such as GeoJSON and Shapefiles&lt;/item&gt;
      &lt;item&gt;Use compression transparently with .gz and .zst&lt;/item&gt;
      &lt;item&gt;Use the built-in map type for semi-structured or key–value data&lt;/item&gt;
      &lt;item&gt;Combine heap, Iceberg, and external Parquet/CSV/JSON files in the same SQL queries and modifications — all with full transactional guarantees and no SQL limitations&lt;/item&gt;
      &lt;item&gt;Infer table columns and types from external data sources such as Iceberg, Parquet, JSON, and CSV files&lt;/item&gt;
      &lt;item&gt;Leverage DuckDB’s query engine underneath for fast execution without leaving Postgres&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;There are two ways to set up &lt;code&gt;pg_lake&lt;/code&gt;:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Using Docker, for an easy, ready-to-run test environment.&lt;/item&gt;
      &lt;item&gt;Building from source, for a manual setup or development use.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Both approaches include the PostgreSQL extensions, the &lt;code&gt;pgduck_server&lt;/code&gt; application and setting up S3-compatible storage.&lt;/p&gt;
    &lt;p&gt;Follow the Docker README to set up and run &lt;code&gt;pg_lake&lt;/code&gt; with Docker.&lt;/p&gt;
    &lt;p&gt;Once you’ve built and installed the required components, you can initialize &lt;code&gt;pg_lake&lt;/code&gt; inside Postgres.&lt;/p&gt;
    &lt;p&gt;Create all required extensions at once using &lt;code&gt;CASCADE&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;CREATE EXTENSION pg_lake CASCADE;
NOTICE:  installing required extension "pg_lake_table"
NOTICE:  installing required extension "pg_lake_engine"
NOTICE:  installing required extension "pg_extension_base"
NOTICE:  installing required extension "pg_lake_iceberg"
NOTICE:  installing required extension "pg_lake_copy"
CREATE EXTENSION&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;pgduck_server&lt;/code&gt; is a standalone process that implements the Postgres wire-protocol (locally), and underneath uses &lt;code&gt;DuckDB&lt;/code&gt; to execute queries.&lt;/p&gt;
    &lt;p&gt;When you run &lt;code&gt;pgduck_server&lt;/code&gt; it starts listening to port &lt;code&gt;5332&lt;/code&gt; on unix domain socket:&lt;/p&gt;
    &lt;code&gt;pgduck_server
LOG pgduck_server is listening on unix_socket_directory: /tmp with port 5332, max_clients allowed 10000
&lt;/code&gt;
    &lt;p&gt;As &lt;code&gt;pgduck_server&lt;/code&gt; implements Postgres wire protocol, you can access it via &lt;code&gt;psql&lt;/code&gt; on port &lt;code&gt;5332&lt;/code&gt; and host &lt;code&gt;/tmp&lt;/code&gt; and run commands via DuckDB.&lt;/p&gt;
    &lt;p&gt;For example, you can get the DuckDB version:&lt;/p&gt;
    &lt;code&gt;psql -p 5332 -h /tmp

select version() as duckdb_version; 
duckdb_version 
---------------- 
v1.3.2 (1 row)&lt;/code&gt;
    &lt;p&gt;You can also provide some additional settings while starting the server, to see all:&lt;/p&gt;
    &lt;code&gt;pgduck_server --help
&lt;/code&gt;
    &lt;p&gt;There are some important settings that should be adjusted, especially on production systems:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;--memory_limit&lt;/code&gt;: Optionally specify the maximum memory of pgduck_server similar to DuckDB's memory_limit, the default is 80 percent of the system memory&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--init_file_path &amp;lt;path&amp;gt;&lt;/code&gt;: Execute all statements in this file on start-up&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--cache_dir&lt;/code&gt;: Specify the directory to use to cache remote files (from S3)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;&lt;code&gt;pgduck_server&lt;/code&gt; relies on the DuckDB secrets manager for credentials and it follows the credentials chain by default for AWS and GCP. Make sure your cloud credentials are configured properly — for example, by setting them in ~/.aws/credentials.&lt;/p&gt;
    &lt;p&gt;Once you set up the credential chain, you should set the &lt;code&gt;pg_lake_iceberg.default_location_prefix&lt;/code&gt;. This is the location where Iceberg tables are stored:&lt;/p&gt;
    &lt;code&gt;SET pg_lake_iceberg.default_location_prefix TO 's3://testbucketpglake';&lt;/code&gt;
    &lt;p&gt;You can also set the credentials on &lt;code&gt;pgduck_server&lt;/code&gt; for local development with &lt;code&gt;minio&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;You can create Iceberg tables by adding &lt;code&gt;USING iceberg&lt;/code&gt; to your &lt;code&gt;CREATE TABLE&lt;/code&gt; statements.&lt;/p&gt;
    &lt;code&gt;CREATE TABLE iceberg_test USING iceberg 
      AS SELECT 
            i as key, 'val_'|| i  as val
         FROM 
            generate_series(0,99)i;&lt;/code&gt;
    &lt;p&gt;Then, query it:&lt;/p&gt;
    &lt;code&gt;SELECT count(*) FROM iceberg_test;
 count 
-------
   100
(1 row)&lt;/code&gt;
    &lt;p&gt;You can then see the Iceberg metadata location:&lt;/p&gt;
    &lt;code&gt;SELECT table_name, metadata_location FROM iceberg_tables;


    table_name     |                                                metadata_location
-------------------+--------------------------------------------------------------------------------------------------------------------
 iceberg_test      | s3://testbucketpglake/postgres/public/test/435029/metadata/00001-f0c6e20a-fd1c-4645-87c9-c0c64b92992b.metadata.json&lt;/code&gt;
    &lt;p&gt;You can import or export data directly using &lt;code&gt;COPY&lt;/code&gt; in Parquet, CSV, or newline-delimited JSON formats.  The format is automatically inferred from the file extension, or you can specify it explicitly with &lt;code&gt;COPY&lt;/code&gt; options like &lt;code&gt;WITH (format 'csv', compression 'gzip')&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;-- Copy data from Postgres to S3 with format parquet
-- Read from any data source, including iceberg tables, heap tables or any query results
COPY (SELECT * FROM iceberg_test) TO 's3://testbucketpglake/parquet_data/iceberg_test.parquet';

-- Copy back from S3 to any table in Postgres
-- This example copies into an iceberg table, but could be heap table as well
COPY iceberg_test FROM 's3://testbucketpglake/parquet_data/iceberg_test.parquet';&lt;/code&gt;
    &lt;p&gt;You can create a foreign table directly from a file or set of files without having to specify column names or types.&lt;/p&gt;
    &lt;code&gt;-- use the files under the path, can use * for all files
CREATE FOREIGN TABLE parquet_table() 
SERVER pg_lake 
OPTIONS (path 's3://testbucketpglake/parquet_data/*.parquet');

-- note that we infer the columns from the file
\d parquet_table
              Foreign table "public.parquet_table"
 Column |  Type   | Collation | Nullable | Default | FDW options 
--------+---------+-----------+----------+---------+-------------
 key    | integer |           |          |         | 
 val    | text    |           |          |         | 
Server: pg_lake
FDW options: (path 's3://testbucketpglake/parquet_data/*.parquet')

-- and, query it
select count(*) from parquet_table;
 count 
-------
   100
(1 row)
&lt;/code&gt;
    &lt;p&gt;A &lt;code&gt;pg_lake&lt;/code&gt; instance consists of two main components: PostgreSQL with the pg_lake extensions and pgduck_server.&lt;/p&gt;
    &lt;p&gt;Users connect to PostgreSQL to run SQL queries, and the &lt;code&gt;pg_lake&lt;/code&gt; extensions integrate with Postgres’s hooks to handle query planning, transaction boundaries, and overall orchestration of execution.&lt;/p&gt;
    &lt;p&gt;Behind the scenes, parts of query execution are delegated to DuckDB through pgduck_server, a separate multi-threaded process that implements the PostgreSQL wire protocol (locally). This process runs DuckDB together with our duckdb_pglake extension, which adds PostgreSQL-compatible functions and behavior.&lt;/p&gt;
    &lt;p&gt;Users typically don’t need to be aware of &lt;code&gt;pgduck_server&lt;/code&gt;; it operates transparently to improve performance. When appropriate, &lt;code&gt;pg_lake&lt;/code&gt; delegates scanning of the data and the computation to DuckDB’s highly parallel, column-oriented execution engine.&lt;/p&gt;
    &lt;p&gt;This separation also avoids the threading and memory-safety limitations that would arise from embedding DuckDB directly inside the Postgres process, which is designed around process isolation rather than multi-threaded execution. Moreover, it lets us interact with the query engine directly by connecting to it using standard Postgres clients.&lt;/p&gt;
    &lt;p&gt;The team behind pg_lake has a lot of experience building Postgres extensions (e.g. Citus, pg_cron, pg_documentdb). Over time, we’ve learned that large, monolithic PostgreSQL extensions are harder to evolve and maintain.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;pg_lake&lt;/code&gt; follows a modular design built around a set of interoperating components — mostly implemented as PostgreSQL extensions, others as supporting services or libraries.&lt;lb/&gt; Each part focuses on a well-defined layer, such as table and metadata management, catalog and object store integration, query execution, or data format handling. This approach makes it easier to extend, test, and evolve the system, while keeping it familiar to anyone with a PostgreSQL background.&lt;/p&gt;
    &lt;p&gt;The current set of components are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;pg_lake_iceberg: a PostgreSQL extension that implements the Iceberg specification&lt;/item&gt;
      &lt;item&gt;pg_lake_table: a PostgreSQL extension that implements a foreign data wrapper to query files in object storage&lt;/item&gt;
      &lt;item&gt;pg_lake_copy: a PostgreSQL extension that implements COPY to/from your data lake&lt;/item&gt;
      &lt;item&gt;pg_lake_engine: a common module for different pg_lake extensions&lt;/item&gt;
      &lt;item&gt;pg_extension_base: A foundational building block for other extensions&lt;/item&gt;
      &lt;item&gt;pg_extension_updater: An extension for updating all extensions on start-up. See README.md.&lt;/item&gt;
      &lt;item&gt;pg_lake_benchmark: a PostgreSQL extension that performs various benchmarks on lake tables. See README.md.&lt;/item&gt;
      &lt;item&gt;pg_map: A generic map type generator&lt;/item&gt;
      &lt;item&gt;pgduck_server: a stand-alone server that loads DuckDB into the same server machine and exposes DuckDB via the PostgreSQL protocol&lt;/item&gt;
      &lt;item&gt;duckdb_pglake: a DuckDB extension that adds missing PostgreSQL functions to DuckDB&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;&lt;code&gt;pg_lake&lt;/code&gt; development started in early 2024 at Crunchy Data with the goal of bringing Iceberg to PostgreSQL. The first few months were focused on building a robust integration of an external query engine (DuckDB). To get to market early, we made the query/import/export features available to Crunchy Bridge customers as Crunchy Bridge for Analytics.&lt;/p&gt;
    &lt;p&gt;Next, we started building a comprehensive implementation of the Iceberg (v2) protocol with support for transactions and almost all PostgreSQL features. In November 2024, we relaunched Crunchy Bridge for Analytics as Crunchy Data Warehouse available on Crunchy Bridge and on-premises.&lt;/p&gt;
    &lt;p&gt;In June 2025, Crunchy Data was acquired by Snowflake. Following the acquisition, Snowflake decided to open source the project as &lt;code&gt;pg_lake&lt;/code&gt; in November 2025. The initial version is 3.0 because of the two prior generations. If you’re currently a Crunchy Data Warehouse user there will be an automatic upgrade path, though some names will change.&lt;/p&gt;
    &lt;p&gt;Full project documentation can be found in the docs directory.&lt;/p&gt;
    &lt;p&gt;Copyright (c) Snowflake Inc. All rights reserved. Licensed under the Apache 2.0 license.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;pg_lake&lt;/code&gt; is dependent on third-party projects Apache Avro and DuckDB. During build, &lt;code&gt;pg_lake&lt;/code&gt; applies patches to Avro and certain DuckDB extensions in order to provide the &lt;code&gt;pg_lake&lt;/code&gt; functionality. The source code associated with the Avro and DuckDB extensions is downloaded from the applicable upstream repos and the source code associated with those projects remains under the original licenses. If you are packaging or redistributing packages that include &lt;code&gt;pg_lake&lt;/code&gt;, please note that you should review those upstream license terms.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45812606</guid><pubDate>Tue, 04 Nov 2025 16:12:27 +0000</pubDate></item><item><title>Launch HN: Plexe (YC X25) – Build production-grade ML models from prompts</title><link>https://www.plexe.ai/</link><description>&lt;doc fingerprint="d1d50ecbdfaae78"&gt;
  &lt;main&gt;
    &lt;p&gt;AI Data Scientist&lt;/p&gt;
    &lt;p&gt;AI Data Scientist&lt;/p&gt;
    &lt;p&gt;AI Data Scientist&lt;/p&gt;
    &lt;p&gt;Your Agentic ML Engineering&lt;/p&gt;
    &lt;p&gt;Team&lt;/p&gt;
    &lt;p&gt;Turn your data into engineered AI solutions.&lt;/p&gt;
    &lt;p&gt;Turn your data into engineered AI solutions.&lt;/p&gt;
    &lt;p&gt;Turn your data into engineered AI solutions.&lt;/p&gt;
    &lt;p&gt;Turn your raw data into engineered AI solutions.&lt;/p&gt;
    &lt;p&gt;Custom ML Models&lt;/p&gt;
    &lt;p&gt;Data Dashboards&lt;/p&gt;
    &lt;p&gt;API Endpoints&lt;/p&gt;
    &lt;p&gt;Batch Jobs&lt;/p&gt;
    &lt;p&gt;File Upload&lt;/p&gt;
    &lt;p&gt;Database Connectors&lt;/p&gt;
    &lt;p&gt;Custom ML Models&lt;/p&gt;
    &lt;p&gt;Data Dashboards&lt;/p&gt;
    &lt;p&gt;API Endpoints&lt;/p&gt;
    &lt;p&gt;Batch Jobs&lt;/p&gt;
    &lt;p&gt;File Upload&lt;/p&gt;
    &lt;p&gt;Database Connectors&lt;/p&gt;
    &lt;p&gt;Custom ML Models&lt;/p&gt;
    &lt;p&gt;Data Dashboards&lt;/p&gt;
    &lt;p&gt;API Endpoints&lt;/p&gt;
    &lt;p&gt;Batch Jobs&lt;/p&gt;
    &lt;p&gt;File Upload&lt;/p&gt;
    &lt;p&gt;Database Connectors&lt;/p&gt;
    &lt;p&gt;Custom ML Models&lt;/p&gt;
    &lt;p&gt;Data Dashboards&lt;/p&gt;
    &lt;p&gt;API Endpoints&lt;/p&gt;
    &lt;p&gt;Batch Jobs&lt;/p&gt;
    &lt;p&gt;File Upload&lt;/p&gt;
    &lt;p&gt;Database Connectors&lt;/p&gt;
    &lt;p&gt;How It Works&lt;/p&gt;
    &lt;p&gt;From Prompt to Production: The Plexe Workflow&lt;/p&gt;
    &lt;p&gt;Get Instant, Actionable Data Insights&lt;/p&gt;
    &lt;p&gt;Simply connect your data, Plexe checks quality, and spots the patterns that matter most.&lt;/p&gt;
    &lt;p&gt;Create Model&lt;/p&gt;
    &lt;p&gt;Build a custom AI model for your specific needs in a few simple steps&lt;/p&gt;
    &lt;p&gt;Give me quick insights on ecommerce fraud dataset&lt;/p&gt;
    &lt;p&gt;Quick Insights on E-commerce Fraud Dataset&lt;/p&gt;
    &lt;p&gt;ð Key Findings:&lt;/p&gt;
    &lt;p&gt;Data Quality: â Excellent - No missing values across all fields&lt;/p&gt;
    &lt;p&gt;Fraud Rate: ð Very low at -1% (mean of 0.01025), which is typical for fraud datasets&lt;/p&gt;
    &lt;p&gt;ð Notable Patterns&lt;/p&gt;
    &lt;p&gt;Transaction Amounts:&lt;/p&gt;
    &lt;p&gt;Average: $90.59 but with high variation (std: $116.23)&lt;/p&gt;
    &lt;p&gt;Ask anything about models or datasets...&lt;/p&gt;
    &lt;p&gt;Turn Ideas into Deployable ML Models&lt;/p&gt;
    &lt;p&gt;Tell Plexe what you want to achieve, in plain language and it will build a production-ready model thatâs engineered for your exact business challenge.&lt;/p&gt;
    &lt;p&gt;Create Model&lt;/p&gt;
    &lt;p&gt;Build a custom AI model for your specific needs in a few simple steps&lt;/p&gt;
    &lt;p&gt;Describe your modelâs purpose&lt;/p&gt;
    &lt;p&gt;Explain what you want your model to do in detail. Be specific about what you want to predict and what data it should use.&lt;/p&gt;
    &lt;p&gt;Model Intent&lt;/p&gt;
    &lt;p&gt;Build me a product recommendations model for my ecommerce website&lt;/p&gt;
    &lt;p&gt;Model Name&lt;/p&gt;
    &lt;p&gt;build-product-recommendations&lt;/p&gt;
    &lt;p&gt;Generate&lt;/p&gt;
    &lt;p&gt;A unique identifier for your model. Use lowercase letters, numbers, and hyphens only.&lt;/p&gt;
    &lt;p&gt;Full Transparency, Built In&lt;/p&gt;
    &lt;p&gt;We believe you should always know what your AI is doing and why. Plexe gives you clear performance metrics, training details, and easy-to-read explanations so you can trust every prediction your model makes.&lt;/p&gt;
    &lt;p&gt;Funding Prediction Model&lt;/p&gt;
    &lt;p&gt;completed&lt;/p&gt;
    &lt;p&gt;Retrain Model&lt;/p&gt;
    &lt;p&gt;Download Model&lt;/p&gt;
    &lt;p&gt;Performance&lt;/p&gt;
    &lt;p&gt;Overview&lt;/p&gt;
    &lt;p&gt;Technical Details&lt;/p&gt;
    &lt;p&gt;API Usage&lt;/p&gt;
    &lt;p&gt;Model Performance&lt;/p&gt;
    &lt;p&gt;Training performance, metrics and behavior insights.&lt;/p&gt;
    &lt;p&gt;Training Performance&lt;/p&gt;
    &lt;p&gt;Mean Absolute Error&lt;/p&gt;
    &lt;p&gt;0.2083&lt;/p&gt;
    &lt;p&gt;Training Details&lt;/p&gt;
    &lt;p&gt;Preprocessing&lt;/p&gt;
    &lt;p&gt;One-hot encoding for categorical variables proj_a, proj_b, funder and quarter.&lt;/p&gt;
    &lt;p&gt;Spotlight&lt;/p&gt;
    &lt;p&gt;Spotlight&lt;/p&gt;
    &lt;p&gt;As Seen On&lt;/p&gt;
    &lt;p&gt;As Seen On&lt;/p&gt;
    &lt;p&gt;As Seen On&lt;/p&gt;
    &lt;p&gt;Read what the media is saying about us&lt;/p&gt;
    &lt;p&gt;Read what the media is saying about us&lt;/p&gt;
    &lt;p&gt;Read what the media is saying about us&lt;/p&gt;
    &lt;p&gt;Featured in BIâs 10 Most Exciting AI Startups from YC Spring 2025&lt;/p&gt;
    &lt;p&gt;Featured in BIâs 10 Most Exciting AI Startups from YC Spring 2025&lt;/p&gt;
    &lt;p&gt;Plexe AI Redefines Credit Underwriting With Real-Time ML Models&lt;/p&gt;
    &lt;p&gt;Read More&lt;/p&gt;
    &lt;p&gt;Read More&lt;/p&gt;
    &lt;p&gt;Read More&lt;/p&gt;
    &lt;p&gt;Plexe Launches to Bring Custom AI Models to Every Business&lt;/p&gt;
    &lt;p&gt;Plexe Launches to Bring Custom AI Models to Every Business&lt;/p&gt;
    &lt;p&gt;Plexe Launches to Bring Custom AI Models to Every Business&lt;/p&gt;
    &lt;p&gt;Read More&lt;/p&gt;
    &lt;p&gt;Read More&lt;/p&gt;
    &lt;p&gt;Read More&lt;/p&gt;
    &lt;p&gt;Plexe featured in European Startups at Y Combinator&lt;/p&gt;
    &lt;p&gt;Plexe featured in European Startups at Y Combinator&lt;/p&gt;
    &lt;p&gt;Plexe featured in European Startups at Y Combinator&lt;/p&gt;
    &lt;p&gt;Read More&lt;/p&gt;
    &lt;p&gt;Read More&lt;/p&gt;
    &lt;p&gt;Read More&lt;/p&gt;
    &lt;p&gt;Solutions&lt;/p&gt;
    &lt;p&gt;Solutions&lt;/p&gt;
    &lt;p&gt;What Plexe Can Build For You&lt;/p&gt;
    &lt;p&gt;What Plexe Can Build For You&lt;/p&gt;
    &lt;p&gt;What Plexe Can Build For You&lt;/p&gt;
    &lt;p&gt;Tailored ML solutions for your industry, deployed instantly.&lt;/p&gt;
    &lt;p&gt;Tailored ML solutions for your industry, deployed instantly.&lt;/p&gt;
    &lt;p&gt;Tailored ML solutions for your industry, deployed instantly.&lt;/p&gt;
    &lt;p&gt;Select your industry:&lt;/p&gt;
    &lt;p&gt;Finance &amp;amp; Banking&lt;/p&gt;
    &lt;p&gt;E-commerce&lt;/p&gt;
    &lt;p&gt;Logistics&lt;/p&gt;
    &lt;p&gt;Cybersecurity&lt;/p&gt;
    &lt;p&gt;Stop fraud before it drains your revenue.&lt;/p&gt;
    &lt;p&gt;Protect your customers and your bottom line with AI that spots suspicious activity before it becomes a problem. &lt;/p&gt;
    &lt;p&gt;Lend with confidence.&lt;/p&gt;
    &lt;p&gt;Make smarter credit decisions by accurately understanding whoâs truly creditworthy.&lt;/p&gt;
    &lt;p&gt;Keep your best customers from leaving.&lt;/p&gt;
    &lt;p&gt;Identify early signs of churn so you can act before valuable relationships are lost.&lt;/p&gt;
    &lt;p&gt;Select your industry:&lt;/p&gt;
    &lt;p&gt;Finance &amp;amp; Banking&lt;/p&gt;
    &lt;p&gt;E-commerce&lt;/p&gt;
    &lt;p&gt;Logistics&lt;/p&gt;
    &lt;p&gt;Cybersecurity&lt;/p&gt;
    &lt;p&gt;Stop fraud before it drains your revenue.&lt;/p&gt;
    &lt;p&gt;Protect your customers and your bottom line with AI that spots suspicious activity before it becomes a problem. &lt;/p&gt;
    &lt;p&gt;Lend with confidence.&lt;/p&gt;
    &lt;p&gt;Make smarter credit decisions by accurately understanding whoâs truly creditworthy.&lt;/p&gt;
    &lt;p&gt;Keep your best customers from leaving.&lt;/p&gt;
    &lt;p&gt;Identify early signs of churn so you can act before valuable relationships are lost.&lt;/p&gt;
    &lt;p&gt;Select your industry:&lt;/p&gt;
    &lt;p&gt;Finance &amp;amp; Banking&lt;/p&gt;
    &lt;p&gt;E-commerce&lt;/p&gt;
    &lt;p&gt;Logistics&lt;/p&gt;
    &lt;p&gt;Cybersecurity&lt;/p&gt;
    &lt;p&gt;Stop fraud before it drains your revenue.&lt;/p&gt;
    &lt;p&gt;Protect your customers and your bottom line with AI that spots suspicious activity before it becomes a problem. &lt;/p&gt;
    &lt;p&gt;Lend with confidence.&lt;/p&gt;
    &lt;p&gt;Make smarter credit decisions by accurately understanding whoâs truly creditworthy.&lt;/p&gt;
    &lt;p&gt;Keep your best customers from leaving.&lt;/p&gt;
    &lt;p&gt;Identify early signs of churn so you can act before valuable relationships are lost.&lt;/p&gt;
    &lt;p&gt;Select your industry:&lt;/p&gt;
    &lt;p&gt;Finance &amp;amp; Banking&lt;/p&gt;
    &lt;p&gt;E-commerce&lt;/p&gt;
    &lt;p&gt;Logistics&lt;/p&gt;
    &lt;p&gt;Cybersecurity&lt;/p&gt;
    &lt;p&gt;Stop fraud before it drains your revenue.&lt;/p&gt;
    &lt;p&gt;Protect your customers and your bottom line with AI that spots suspicious activity before it becomes a problem. &lt;/p&gt;
    &lt;p&gt;Lend with confidence.&lt;/p&gt;
    &lt;p&gt;Make smarter credit decisions by accurately understanding whoâs truly creditworthy.&lt;/p&gt;
    &lt;p&gt;Keep your best customers from leaving.&lt;/p&gt;
    &lt;p&gt;Identify early signs of churn so you can act before valuable relationships are lost.&lt;/p&gt;
    &lt;p&gt;FAQ&lt;/p&gt;
    &lt;p&gt;FAQ&lt;/p&gt;
    &lt;p&gt;Questions? Weâve Got Answers.&lt;/p&gt;
    &lt;p&gt;Questions? Weâve Got Answers.&lt;/p&gt;
    &lt;p&gt;Questions? Weâve Got Answers.&lt;/p&gt;
    &lt;p&gt;Everything you need to know about using Plexe, from building your first model to deploying at scale.&lt;/p&gt;
    &lt;p&gt;Everything you need to know about using Plexe, from building your first model to deploying at scale.&lt;/p&gt;
    &lt;p&gt;Everything you need to know about using Plexe, from building your first model to deploying at scale.&lt;/p&gt;
    &lt;p&gt;Who owns the models?&lt;/p&gt;
    &lt;p&gt;Where can I use Plexe?&lt;/p&gt;
    &lt;p&gt;Do you have a free version?&lt;/p&gt;
    &lt;p&gt;Can I use Plexe without my own data?&lt;/p&gt;
    &lt;p&gt;How secure is my data?&lt;/p&gt;
    &lt;p&gt;Can Plexe integrate with my existing tools?&lt;/p&gt;
    &lt;p&gt;Do you offer annual or enterprise pricing?&lt;/p&gt;
    &lt;p&gt;Who owns the models?&lt;/p&gt;
    &lt;p&gt;Where can I use Plexe?&lt;/p&gt;
    &lt;p&gt;Do you have a free version?&lt;/p&gt;
    &lt;p&gt;Can I use Plexe without my own data?&lt;/p&gt;
    &lt;p&gt;How secure is my data?&lt;/p&gt;
    &lt;p&gt;Can Plexe integrate with my existing tools?&lt;/p&gt;
    &lt;p&gt;Do you offer annual or enterprise pricing?&lt;/p&gt;
    &lt;p&gt;Who owns the models?&lt;/p&gt;
    &lt;p&gt;Where can I use Plexe?&lt;/p&gt;
    &lt;p&gt;Do you have a free version?&lt;/p&gt;
    &lt;p&gt;Can I use Plexe without my own data?&lt;/p&gt;
    &lt;p&gt;How secure is my data?&lt;/p&gt;
    &lt;p&gt;Can Plexe integrate with my existing tools?&lt;/p&gt;
    &lt;p&gt;Do you offer annual or enterprise pricing?&lt;/p&gt;
    &lt;p&gt;Who owns the models?&lt;/p&gt;
    &lt;p&gt;Where can I use Plexe?&lt;/p&gt;
    &lt;p&gt;Do you have a free version?&lt;/p&gt;
    &lt;p&gt;Can I use Plexe without my own data?&lt;/p&gt;
    &lt;p&gt;How secure is my data?&lt;/p&gt;
    &lt;p&gt;Can Plexe integrate with my existing tools?&lt;/p&gt;
    &lt;p&gt;Do you offer annual or enterprise pricing?&lt;/p&gt;
    &lt;p&gt;Letâs Build Something Incredible Together.&lt;/p&gt;
    &lt;p&gt;Whether youâre starting from scratch or scaling to millions of users, Plexe is your AI engineering team, ready to turn your ideas into real solutions.&lt;/p&gt;
    &lt;p&gt;Whether youâre starting from scratch or scaling to millions of users, Plexe is your AI engineering team, ready to turn your data into your competitive advantage.&lt;/p&gt;
    &lt;p&gt;Letâs Build Something Incredible Together.&lt;/p&gt;
    &lt;p&gt;Whether youâre starting from scratch or scaling to millions of users, Plexe is your AI engineering team, ready to turn your ideas into real solutions.&lt;/p&gt;
    &lt;p&gt;Letâs Build Something Incredible Together.&lt;/p&gt;
    &lt;p&gt;Whether youâre starting from scratch or scaling to millions of users, Plexe is your AI engineering team, ready to turn your ideas into real solutions.&lt;/p&gt;
    &lt;p&gt;Â© 2025 Plexe Ltd. All rights reserved.&lt;/p&gt;
    &lt;p&gt;Â© 2025 Plexe Ltd. All rights reserved.&lt;/p&gt;
    &lt;p&gt;Â© 2025 Plexe Ltd. All rights reserved.&lt;/p&gt;
    &lt;p&gt;Â© 2025 Plexe Ltd. All rights reserved.&lt;/p&gt;
    &lt;p&gt;How It Works&lt;/p&gt;
    &lt;p&gt;From Prompt to Production: The Plexe Workflow&lt;/p&gt;
    &lt;p&gt;From Prompt to Production: The Plexe Workflow&lt;/p&gt;
    &lt;p&gt;Get Instant, Actionable Data Insights&lt;/p&gt;
    &lt;p&gt;Simply connect your data, Plexe checks quality, and spots the patterns that matter most. Youâll see whatâs working, whatâs not, and where the real opportunities are hiding. No code, no setup, no fuss.&lt;/p&gt;
    &lt;p&gt;Create Model&lt;/p&gt;
    &lt;p&gt;Build a custom AI model for your specific needs in a few simple steps&lt;/p&gt;
    &lt;p&gt;Give me quick insights on ecommerce fraud dataset&lt;/p&gt;
    &lt;p&gt;Quick Insights on E-commerce Fraud Dataset&lt;/p&gt;
    &lt;p&gt;ð Key Findings:&lt;/p&gt;
    &lt;p&gt;Data Quality: â Excellent - No missing values across all fields&lt;/p&gt;
    &lt;p&gt;Fraud Rate: ð Very low at -1% (mean of 0.01025), which is typical for fraud datasets&lt;/p&gt;
    &lt;p&gt;ð Notable Patterns&lt;/p&gt;
    &lt;p&gt;Transaction Amounts:&lt;/p&gt;
    &lt;p&gt;Average: $90.59 but with high variation (std: $116.23)&lt;/p&gt;
    &lt;p&gt;Ask anything about models or datasets...&lt;/p&gt;
    &lt;p&gt;Turn Ideas into Deployable ML Models&lt;/p&gt;
    &lt;p&gt;Tell Plexe what you want to achieve, in plain language and weâll build a production-ready model thatâs engineered for your exact business challenge. Whether itâs predicting churn or fraud detection, youâll go from idea to working AI in hours, not months.&lt;/p&gt;
    &lt;p&gt;Create Model&lt;/p&gt;
    &lt;p&gt;Build a custom AI model for your specific needs in a few simple steps&lt;/p&gt;
    &lt;p&gt;Describe your modelâs purpose&lt;/p&gt;
    &lt;p&gt;Explain what you want your model to do in detail. Be specific about what you want to predict and what data it should use.&lt;/p&gt;
    &lt;p&gt;Model Intent&lt;/p&gt;
    &lt;p&gt;Build me a product recommendations model for my ecommerce website&lt;/p&gt;
    &lt;p&gt;Model Name&lt;/p&gt;
    &lt;p&gt;build-product-recommendations&lt;/p&gt;
    &lt;p&gt;Generate&lt;/p&gt;
    &lt;p&gt;A unique identifier for your model. Use lowercase letters, numbers, and hyphens only.&lt;/p&gt;
    &lt;p&gt;Full Transparency, Built In&lt;/p&gt;
    &lt;p&gt;We believe you should always know what your AI is doing and why. Plexe gives you clear performance metrics, training details, and easy-to-read explanations so you can trust every prediction your model makes.&lt;/p&gt;
    &lt;p&gt;Funding Prediction Model&lt;/p&gt;
    &lt;p&gt;completed&lt;/p&gt;
    &lt;p&gt;Retrain Model&lt;/p&gt;
    &lt;p&gt;Download Model&lt;/p&gt;
    &lt;p&gt;Performance&lt;/p&gt;
    &lt;p&gt;Overview&lt;/p&gt;
    &lt;p&gt;Technical Details&lt;/p&gt;
    &lt;p&gt;API Usage&lt;/p&gt;
    &lt;p&gt;Model Performance&lt;/p&gt;
    &lt;p&gt;Training performance, metrics and behavior insights.&lt;/p&gt;
    &lt;p&gt;Training Performance&lt;/p&gt;
    &lt;p&gt;Mean Absolute Error&lt;/p&gt;
    &lt;p&gt;0.2083&lt;/p&gt;
    &lt;p&gt;Training Details&lt;/p&gt;
    &lt;p&gt;Preprocessing&lt;/p&gt;
    &lt;p&gt;One-hot encoding for categorical variables proj_a, proj_b, funder and quarter.&lt;/p&gt;
    &lt;p&gt;How It Works&lt;/p&gt;
    &lt;p&gt;From Prompt to Production: The Plexe Workflow&lt;/p&gt;
    &lt;p&gt;How It Works&lt;/p&gt;
    &lt;p&gt;From Prompt to Production: The Plexe Workflow&lt;/p&gt;
    &lt;p&gt;Get Instant, Actionable Data Insights&lt;/p&gt;
    &lt;p&gt;Simply connect your data, Plexe checks quality, and spots the patterns that matter most. Youâll see whatâs working, whatâs not, and where the real opportunities are hiding. No code, no setup, no fuss.&lt;/p&gt;
    &lt;p&gt;Create Model&lt;/p&gt;
    &lt;p&gt;Build a custom AI model for your specific needs in a few simple steps&lt;/p&gt;
    &lt;p&gt;Give me quick insights on ecommerce fraud dataset&lt;/p&gt;
    &lt;p&gt;Quick Insights on E-commerce Fraud Dataset&lt;/p&gt;
    &lt;p&gt;ð Key Findings:&lt;/p&gt;
    &lt;p&gt;Data Quality: â Excellent - No missing values across all fields&lt;/p&gt;
    &lt;p&gt;Fraud Rate: ð Very low at -1% (mean of 0.01025), which is typical for fraud datasets&lt;/p&gt;
    &lt;p&gt;ð Notable Patterns&lt;/p&gt;
    &lt;p&gt;Transaction Amounts:&lt;/p&gt;
    &lt;p&gt;Average: $90.59 but with high variation (std: $116.23)&lt;/p&gt;
    &lt;p&gt;Ask anything about models or datasets...&lt;/p&gt;
    &lt;p&gt;Turn Ideas into Deployable ML Models&lt;/p&gt;
    &lt;p&gt;Tell Plexe what you want to achieve, in plain language and weâll build a production-ready model thatâs engineered for your exact business challenge. Whether itâs predicting churn or fraud detection, youâll go from idea to working AI in hours, not months.&lt;/p&gt;
    &lt;p&gt;Create Model&lt;/p&gt;
    &lt;p&gt;Build a custom AI model for your specific needs in a few simple steps&lt;/p&gt;
    &lt;p&gt;Describe your modelâs purpose&lt;/p&gt;
    &lt;p&gt;Explain what you want your model to do in detail. Be specific about what you want to predict and what data it should use.&lt;/p&gt;
    &lt;p&gt;Model Intent&lt;/p&gt;
    &lt;p&gt;Build me a product recommendations model for my ecommerce website&lt;/p&gt;
    &lt;p&gt;Model Name&lt;/p&gt;
    &lt;p&gt;build-product-recommendations&lt;/p&gt;
    &lt;p&gt;Generate&lt;/p&gt;
    &lt;p&gt;A unique identifier for your model. Use lowercase letters, numbers, and hyphens only.&lt;/p&gt;
    &lt;p&gt;Full Transparency, Built In&lt;/p&gt;
    &lt;p&gt;We believe you should always know what your AI is doing and why. Plexe gives you clear performance metrics, training details, and easy-to-read explanations so you can trust every prediction your model makes.&lt;/p&gt;
    &lt;p&gt;Funding Prediction Model&lt;/p&gt;
    &lt;p&gt;completed&lt;/p&gt;
    &lt;p&gt;Retrain Model&lt;/p&gt;
    &lt;p&gt;Download Model&lt;/p&gt;
    &lt;p&gt;Performance&lt;/p&gt;
    &lt;p&gt;Overview&lt;/p&gt;
    &lt;p&gt;Technical Details&lt;/p&gt;
    &lt;p&gt;API Usage&lt;/p&gt;
    &lt;p&gt;Model Performance&lt;/p&gt;
    &lt;p&gt;Training performance, metrics and behavior insights.&lt;/p&gt;
    &lt;p&gt;Training Performance&lt;/p&gt;
    &lt;p&gt;Mean Absolute Error&lt;/p&gt;
    &lt;p&gt;0.2083&lt;/p&gt;
    &lt;p&gt;Training Details&lt;/p&gt;
    &lt;p&gt;Preprocessing&lt;/p&gt;
    &lt;p&gt;One-hot encoding for categorical variables proj_a, proj_b, funder and quarter.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45813310</guid><pubDate>Tue, 04 Nov 2025 17:07:47 +0000</pubDate></item><item><title>NoLongerEvil-Thermostat – Nest Generation 1 and 2 Firmware</title><link>https://github.com/codykociemba/NoLongerEvil-Thermostat</link><description>&lt;doc fingerprint="1eabf0080969b3e8"&gt;
  &lt;main&gt;
    &lt;quote&gt;&lt;g-emoji&gt;⚠️&lt;/g-emoji&gt;WARNING: EXPERIMENTAL SOFTWARE&lt;p&gt;This project is currently in the experimental/testing phase. Do NOT use this firmware on any thermostat that is critical for your heating or cooling needs. Flashing this firmware may brick your device or cause unexpected behavior. Only proceed if you have a backup thermostat or can afford to have your device non-functional during testing.&lt;/p&gt;&lt;/quote&gt;
    &lt;p&gt;This directory contains the tools and firmware needed to flash custom firmware to Nest Thermostat devices using the OMAP DFU (Device Firmware Update) interface.&lt;/p&gt;
    &lt;p&gt;This firmware loader uses the OMAP bootloader interface to flash custom bootloader and kernel images to Nest Thermostat devices. The device must be put into DFU mode to accept new firmware.&lt;/p&gt;
    &lt;p&gt;Important: After flashing this firmware, your device will no longer contact Nest/Google servers. It will operate independently and connect to the NoLongerEvil platform instead, giving you complete control over your thermostat.&lt;/p&gt;
    &lt;p&gt;The custom firmware flashes the device with modified bootloader and kernel components that redirect all network traffic from the original Nest/Google servers to a server we specify. This server hosts a reverse-engineered replica of their API, allowing the thermostat to function independently while giving you complete control over your device data and settings.&lt;/p&gt;
    &lt;p&gt;By intercepting the communication layer, the thermostat believes it's communicating with the official Nest infrastructure, but instead connects to the NoLongerEvil platform. This approach ensures full compatibility with the device's existing software while breaking free from Google's cloud dependency.&lt;/p&gt;
    &lt;code&gt;git clone --recurse-submodules https://github.com/codykociemba/NoLongerEvil-Thermostat.git
cd NoLongerEvil-Thermostat&lt;/code&gt;
    &lt;p&gt;Before building, you'll need to install some required packages:&lt;/p&gt;
    &lt;code&gt;sudo apt-get update
sudo apt-get install build-essential libusb-1.0-0-dev gcc&lt;/code&gt;
    &lt;p&gt;First, install Xcode Command Line Tools:&lt;/p&gt;
    &lt;code&gt;xcode-select --install&lt;/code&gt;
    &lt;p&gt;Then install libusb using Homebrew (the build script will attempt to install this automatically if missing):&lt;/p&gt;
    &lt;code&gt;# Install Homebrew if you don't have it
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"

# Install libusb
brew install libusb&lt;/code&gt;
    &lt;code&gt;chmod +x build.sh
./build.sh&lt;/code&gt;
    &lt;p&gt;The build script will automatically detect your operating system (Linux, macOS, or Windows) and build the appropriate binary.&lt;/p&gt;
    &lt;p&gt;IMPORTANT: You must start the installer script BEFORE rebooting the device.&lt;/p&gt;
    &lt;code&gt;chmod +x install.sh
./install.sh&lt;/code&gt;
    &lt;code&gt;chmod +x install.sh
./install.sh&lt;/code&gt;
    &lt;p&gt;Note for macOS: You may need to grant USB permissions. If you encounter permission issues, check System Preferences → Security &amp;amp; Privacy.&lt;/p&gt;
    &lt;p&gt;The script will wait for the device to enter DFU mode.&lt;/p&gt;
    &lt;p&gt;Follow these steps carefully:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Charge the device - Ensure your Nest Thermostat is properly charged (at least 50% battery recommended)&lt;/item&gt;
      &lt;item&gt;Remove from wall - Remove the Nest from its back plate/wall mount&lt;/item&gt;
      &lt;item&gt;Connect via USB - Plug the Nest into your computer using a micro USB cable&lt;/item&gt;
      &lt;item&gt;Wait for the installer - Make sure the &lt;code&gt;install.sh&lt;/code&gt;script is running and waiting&lt;/item&gt;
      &lt;item&gt;Reboot the device - Press and hold down on the display for 10-15 seconds until the device reboots&lt;/item&gt;
      &lt;item&gt;DFU mode active - Once it reboots, the device will enter DFU mode and the installer script will recognize it and begin flashing&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The firmware installer will automatically detect the device and flash the custom bootloader (x-load, u-boot) and kernel (uImage).&lt;/p&gt;
    &lt;p&gt;After the firmware is flashed successfully, you should see our logo on the device screen:&lt;/p&gt;
    &lt;p&gt;Important:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Keep the device plugged in via USB&lt;/item&gt;
      &lt;item&gt;Wait for the device to complete its boot sequence (this may take 3-4 minutes)&lt;/item&gt;
      &lt;item&gt;Do not disconnect or power off the device during this time&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Once the device has fully rebooted:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Visit https://nolongerevil.com in your web browser&lt;/item&gt;
      &lt;item&gt;Register an account (or sign in if you already have one)&lt;/item&gt;
      &lt;item&gt;Navigate to your Dashboard&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You will see a "No devices" screen that prompts you for an entry code.&lt;/p&gt;
    &lt;p&gt;To link your Nest device to your NoLongerEvil account:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;On your Nest device, navigate to: Settings → Nest App → Get Entry Code&lt;/item&gt;
      &lt;item&gt;The device will display a unique entry code&lt;/item&gt;
      &lt;item&gt;Enter this code on the NoLongerEvil dashboard&lt;/item&gt;
      &lt;item&gt;Your device is now linked and ready to use!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The firmware installation process installs three components:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;x-load.bin - First-stage bootloader (X-Loader for OMAP)&lt;/item&gt;
      &lt;item&gt;u-boot.bin - Second-stage bootloader (Das U-Boot) loaded at address 0x80100000&lt;/item&gt;
      &lt;item&gt;uImage - Linux kernel image loaded at address 0x80A00000&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;After flashing, the device jumps to execution at 0x80100000 (u-boot).&lt;/p&gt;
    &lt;p&gt;This tool provides low-level access to the device's boot process. Use responsibly:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Only use on devices you own&lt;/item&gt;
      &lt;item&gt;Improper firmware can brick your device (Don't sue me bro)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This project builds upon the excellent work of several security researchers and developers:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;grant-h / ajb142 - omap_loader, the USB bootloader tool used to flash OMAP devices&lt;/item&gt;
      &lt;item&gt;exploiteers (GTVHacker) - Original research and development of the Nest DFU attack, which demonstrated the ability to flash custom firmware to Nest devices gen 1 &amp;amp; gen 2&lt;/item&gt;
      &lt;item&gt;FULU and all bounty backers - For funding the Nest Learning Thermostat Gen 1/2 bounty and supporting the right-to-repair movement&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Without their groundbreaking research, open-source contributions, and advocacy for device ownership rights, this work would not be possible. Thank you!&lt;/p&gt;
    &lt;p&gt;We are committed to transparency and the right-to-repair movement. The firmware images and backend API server code will be open sourced soon, allowing the community to audit, improve, and self-host their own infrastructure.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45813343</guid><pubDate>Tue, 04 Nov 2025 17:10:35 +0000</pubDate></item><item><title>Codemaps: Understand Code, Before You Vibe It</title><link>https://cognition.ai/blog/codemaps</link><description>&lt;doc fingerprint="3fc096052cb3ae5f"&gt;
  &lt;main&gt;
    &lt;quote&gt;“Your code is your understanding of the problem you’re exploring. So it’s only when you have your code in your head that you really understand the problem.” — Paul Graham&lt;/quote&gt;
    &lt;p&gt;Software development only becomes engineering with understanding. Your ability to reason through your most challenging coding tasks is constrained by your mental model of how things work — in other words, how quickly and how well you onboard to any codebase for solving any problem. However most AI vibe coding tools are aimed at relieving you of that burden by reading → thinking → writing the code for you, increasing the separation from you and your code. This is fine for low value, commodity tasks, but absolutely unacceptable for the hard, sensitive, and high value work that defines real engineering.&lt;/p&gt;
    &lt;p&gt;We all need more AI that turns your brain ON, not OFF.&lt;/p&gt;
    &lt;p&gt;Today we are announcing Windsurf Codemaps, which are first-of-its-kind AI-annotated structured maps of your code, powered by SWE-1.5 and Claude Sonnet 4.5. Building on our popular work from DeepWiki and Ask Devin, Codemaps is the next step in hyper-contextualized codebase understanding, grounded in precise code navigation.&lt;/p&gt;
    &lt;p&gt;Every engineering task — debugging, refactors, new features — starts with understanding. Great engineers aren’t just good at writing code; they’re good at reading it, building mental models that span files, layers, and systems.&lt;/p&gt;
    &lt;p&gt;But modern codebases are sprawling: hundreds of files, multiple services, dense abstractions. Based on own experience and deep conversations with our customers across the Fortune 500, even top engineers spend much of their deep-work time finding and remembering what matters.&lt;/p&gt;
    &lt;p&gt;It’s a huge tax on productivity:&lt;/p&gt;
    &lt;p&gt;This is the frontier that AI coding tools haven’t yet solved. Onboarding isn’t even a onetime cost, you pay it every time you switch context and codebases. The faster and better you understand your codebase, the faster and better you’ll be able to fix it yourself, or prompt agents to do it.&lt;/p&gt;
    &lt;p&gt;Until today, the standard approach by Copilot, Claude Code, Codex, and even Windsurf Cascade, was to have you ask questions of a generalist agent with access to your code in a typical chat experience. But those solutions don’t solve focused onboarding and strongly grounded navigation to onboard, debug, and better context engineer for your codebase.&lt;/p&gt;
    &lt;p&gt;At Cognition, we’ve been investing far more deeply in understanding:&lt;/p&gt;
    &lt;p&gt;Codemaps is our next investment in tooling that makes engineers the best versions of themselves.&lt;/p&gt;
    &lt;p&gt;When you first open Codemaps (click the new maps icon or Cmd+Shift+C in Windsurf) with a codebase opened in Windsurf, you can enter in a prompt for the task you are trying to do, or take one of the automatic suggestions. You can choose a Fast (SWE-1.5) or Smart (Sonnet 4.5) model to generate your Codemap. Every Codemap is snapshots your code and respects ZDR.&lt;/p&gt;
    &lt;p&gt;Based on our demos to customers, you will experience Codemaps best on your own codebase and asking a question about how or where some functionality works. In our dogfooding, we find particular effectiveness tracing through client-server problems or a data pipeline or debugging auth/security issues:&lt;/p&gt;
    &lt;p&gt;If all you wanted was to quickly jump through grouped and nested parts of your code that related to your question, this is already an improvement compared to asking the same question in Cascade, where answers are not as densely linked to the exact lines of code.&lt;/p&gt;
    &lt;p&gt;You can also toggle over to a visually drawn Codemap, which performs the same functions when you click on individual nodes: they send you to the exact part of the codebase you clicked on.&lt;/p&gt;
    &lt;p&gt;However, if you want a little more context, then you can hit “See more” in any section to expand our “trace guide” that gives a more descriptive explanation of what groups the discovered lines together.&lt;/p&gt;
    &lt;p&gt;Finally, inside Cascade you can also reference a codemap for the agent with &lt;code&gt;@{codemap}&lt;/code&gt; (all of it, or a particular subsection) in your prompt to provide more specific context and dramatically improve the performance of your agent for your task.&lt;/p&gt;
    &lt;p&gt;We feel that the popular usage of “vibe coding” has strayed far from the original intent, into a blanket endorsement of plowing through any and all AI generated code slop. If you look at the difference between the most productive vs the problematic AI-assisted coders, the productive ones can surf the vibes of code that they understand well, whereas people get into trouble when the code they generate and maintain starts to outstrip their ability to understand it.&lt;/p&gt;
    &lt;p&gt;To understand is to be accountable. As AI takes on more of the easy work, the hard problems left to humans are the ones that demand real comprehension: debugging complex systems, refactoring legacy code, making architecture decisions. In this new era, the engineer’s role shifts from authoring to accountability — you might not write every line, but you’re still responsible for what ships. That accountability depends on understanding what the AI produced, why it changed, and whether it’s safe. Codemaps closes that gap by giving both the human and the AI a shared picture of the system: how it’s structured, how data flows, where dependencies live. Codemaps is our latest Fast Agent, but as we discussed in the Semi-Async Valley of Death, our goal isn't just about speed, it is to help your human engineers stay in flow, stay on top of their code, and to move faster and more confidently on the hardest problems, never shipping slop that they don't understand.&lt;/p&gt;
    &lt;p&gt;Augment engineers for high value work, relieve them of low value work. The other local minima that the coding agent industry has gotten stuck in is in the general messaging of replacing engineers for low value work and not having any solutions for the hardest tasks apart from “pls ultrathink high, no mistakes”, which only gives autonomy to the agent, at the expense of the engineer. The long history of human-machine collaboration teaches us that we can always do more with the synergy rather than humans-alone or AI-alone. Our view is that the AI product that engineers will love most is the one that makes them better at their job, not the one that tries to replace them with a sloppy facsimile of themselves.&lt;/p&gt;
    &lt;p&gt;With Codemaps, we are now exposing to humans some of the indexing and analysis we do inside of our coding agents. These artifacts are sharable today across teams for learning and discussion, but we have yet to benchmark how much better they can make our coding agents like Devin and Cascade in solving challenging tasks on their own. We also see opportunities for connecting and annotating codemaps, as well as defining an open &lt;code&gt;.codemap&lt;/code&gt; protocol that can be used by other code agents and custom tooling built by you. Complementing our Fast Context feature, this is an advancement in human-readable automatic context engineering.&lt;/p&gt;
    &lt;p&gt;You can try Codemaps on the latest versions of Windsurf, or DeepWiki!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45813767</guid><pubDate>Tue, 04 Nov 2025 17:47:09 +0000</pubDate></item><item><title>AI and Copyright: Expanding Copyright Hurts Everyone–Here's What to Do Instead</title><link>https://www.eff.org/deeplinks/2025/02/ai-and-copyright-expanding-copyright-hurts-everyone-heres-what-do-instead</link><description>&lt;doc fingerprint="3d709b75b97585a6"&gt;
  &lt;main&gt;
    &lt;p&gt;You shouldn't need a permission slip to read a webpage–whether you do it with your own eyes, or use software to help. AI is a category of general-purpose tools with myriad beneficial uses. Requiring developers to license the materials needed to create this technology threatens the development of more innovative and inclusive AI models, as well as important uses of AI as a tool for expression and scientific research.&lt;/p&gt;
    &lt;head rend="h3"&gt;Threats to Socially Valuable Research and Innovation&lt;/head&gt;
    &lt;p&gt;Requiring researchers to license fair uses of AI training data could make socially valuable research based on machine learning (ML) and even text and data mining (TDM) prohibitively complicated and expensive, if not impossible. Researchers have relied on fair use to conduct TDM research for a decade, leading to important advancements in myriad fields. However, licensing the vast quantity of works that high-quality TDM research requires is frequently cost-prohibitive and practically infeasible.&lt;/p&gt;
    &lt;p&gt;Fair use protects ML and TDM research for good reason. Without fair use, copyright would hinder important scientific advancements that benefit all of us. Empirical studies back this up: research using TDM methodologies are more common in countries that protect TDM research from copyright control; in countries that don’t, copyright restrictions stymie beneficial research. It’s easy to see why: it would be impossible to identify and negotiate with millions of different copyright owners to analyze, say, text from the internet.&lt;/p&gt;
    &lt;p&gt;The stakes are high, because ML is critical to helping us interpret the world around us. It's being used by researchers to understand everything from space nebulae to the proteins in our bodies. When the task requires crunching a huge amount of data, such as the data generated by the world’s telescopes, ML helps rapidly sift through the information to identify features of potential interest to researchers. For example, scientists are using AlphaFold, a deep learning tool, to understand biological processes and develop drugs that target disease-causing malfunctions in those processes. The developers released an open-source version of AlphaFold, making it available to researchers around the world. Other developers have already iterated upon AlphaFold to build transformative new tools.&lt;/p&gt;
    &lt;head rend="h3"&gt;Threats to Competition&lt;/head&gt;
    &lt;p&gt;Requiring AI developers to get authorization from rightsholders before training models on copyrighted works would limit competition to companies that have their own trove of training data, or the means to strike a deal with such a company. This would result in all the usual harms of limited competition—higher costs, worse service, and heightened security risks—as well as reducing the variety of expression used to train such tools and the expression allowed to users seeking to express themselves with the aid of AI. As the Federal Trade Commission recently explained, if a handful of companies control AI training data, “they may be able to leverage their control to dampen or distort competition in generative AI markets” and “wield outsized influence over a significant swath of economic activity.”&lt;/p&gt;
    &lt;p&gt;Legacy gatekeepers have already used copyright to stifle access to information and the creation of new tools for understanding it. Consider, for example, Thomson Reuters v. Ross Intelligence, widely considered to be the first lawsuit over AI training rights ever filed. Ross Intelligence sought to disrupt the legal research duopoly of Westlaw and LexisNexis by offering a new AI-based system. The startup attempted to license the right to train its model on Westlaw’s summaries of public domain judicial opinions and its method for organizing cases. Westlaw refused to grant the license and sued its tiny rival for copyright infringement. Ultimately, the lawsuit forced the startup out of business, eliminating a would-be competitor that might have helped increase access to the law.&lt;/p&gt;
    &lt;p&gt;Similarly, shortly after Getty Images—a billion-dollar stock images company that owns hundreds of millions of images—filed a copyright lawsuit asking the court to order the “destruction” of Stable Diffusion over purported copyright violations in the training process, Getty introduced its own AI image generator trained on its own library of images.&lt;/p&gt;
    &lt;p&gt;Requiring developers to license AI training materials benefits tech monopolists as well. For giant tech companies that can afford to pay, pricey licensing deals offer a way to lock in their dominant positions in the generative AI market by creating prohibitive barriers to entry. To develop a “foundation model” that can be used to build generative AI systems like ChatGPT and Stable Diffusion, developers need to “train” the model on billions or even trillions of works, often copied from the open internet without permission from copyright holders. There’s no feasible way to identify all of those rightsholders—let alone execute deals with each of them. Even if these deals were possible, licensing that much content at the prices developers are currently paying would be prohibitively expensive for most would-be competitors.&lt;/p&gt;
    &lt;p&gt;We should not assume that the same companies who built this world can fix the problems they helped create; if we want AI models that don’t replicate existing social and political biases, we need to make it possible for new players to build them.&lt;/p&gt;
    &lt;p&gt;Nor is pro-monopoly regulation through copyright likely to provide any meaningful economic support for vulnerable artists and creators. Notwithstanding the highly publicized demands of musicians, authors, actors, and other creative professionals, imposing a licensing requirement is unlikely to protect the jobs or incomes of the underpaid working artists that media and entertainment behemoths have exploited for decades. Because of the imbalance in bargaining power between creators and publishing gatekeepers, trying to help creators by giving them new rights under copyright law is, as EFF Special Advisor Cory Doctorow has written, like trying to help a bullied kid by giving them more lunch money for the bully to take.&lt;/p&gt;
    &lt;p&gt;Entertainment companies’ historical practices bear out this concern. For example, in the late-2000’s to mid-2010’s, music publishers and recording companies struck multimillion-dollar direct licensing deals with music streaming companies and video sharing platforms. Google reportedly paid more than $400 million to a single music label, and Spotify gave the major record labels a combined 18 percent ownership interest in its now-$100 billion company. Yet music labels and publishers frequently fail to share these payments with artists, and artists rarely benefit from these equity arrangements. There is no reason to believe that the same companies will treat their artists more fairly once they control AI.&lt;/p&gt;
    &lt;head rend="h3"&gt;Threats to Free Expression&lt;/head&gt;
    &lt;p&gt;Generative AI tools like text and image generators are powerful engines of expression. Creating content—particularly images and videos—is time intensive. It frequently requires tools and skills that many internet users lack. Generative AI significantly expedites content creation and reduces the need for artistic ability and expensive photographic or video technology. This facilitates the creation of art that simply would not have existed and allows people to express themselves in ways they couldn’t without AI.&lt;/p&gt;
    &lt;p&gt;Some art forms historically practiced within the African American community—such as hip hop and collage—have a rich tradition of remixing to create new artworks that can be more than the sum of their parts. As professor and digital artist Nettrice Gaskins has explained, generative AI is a valuable tool for creating these kinds of art. Limiting the works that may be used to train AI would limit its utility as an artistic tool, and compound the harm that copyright law has already inflicted on historically Black art forms.&lt;/p&gt;
    &lt;p&gt;Generative AI has the power to democratize speech and content creation, much like the internet has. Before the internet, a small number of large publishers controlled the channels of speech distribution, controlling which material reached audiences’ ears. The internet changed that by allowing anyone with a laptop and Wi-Fi connection to reach billions of people around the world. Generative AI magnifies those benefits by enabling ordinary internet users to tell stories and express opinions by allowing them to generate text in a matter of seconds and easily create graphics, images, animation, and videos that, just a few years ago, only the most sophisticated studios had the capability to produce. Legacy gatekeepers want to expand copyright so they can reverse this progress. Don’t let them: everyone deserves the right to use technology to express themselves, and AI is no exception.&lt;/p&gt;
    &lt;head rend="h3"&gt;Threats to Fair Use&lt;/head&gt;
    &lt;p&gt;In all of these situations, fair use—the ability to use copyrighted material without permission or payment in certain circumstances—often provides the best counter to restrictions imposed by rightsholders. But, as we explained in the first post in this series, fair use is under attack by the copyright creep. Publishers’ recent attempts to impose a new licensing regime for AI training rights—despite lacking any recognized legal right to control AI training—threatens to undermine the public’s fair use rights.&lt;/p&gt;
    &lt;p&gt;By undermining fair use, the AI copyright creep makes all these other dangers more acute. Fair use is often what researchers and educators rely on to make their academic assessments and to gather data. Fair use allows competitors to build on existing work to offer better alternatives. And fair use lets anyone comment on, or criticize, copyrighted material.&lt;/p&gt;
    &lt;p&gt;When gatekeepers make the argument against fair use and in favor of expansive copyright—in court, to lawmakers, and to the public—they are looking to cement their own power, and undermine ours.&lt;/p&gt;
    &lt;head rend="h3"&gt;A Better Way Forward&lt;/head&gt;
    &lt;p&gt;AI also threatens real harms that demand real solutions.&lt;/p&gt;
    &lt;p&gt;Many creators and white-collar professionals increasingly believe that generative AI threatens their jobs. Many people also worry that it enables serious forms of abuse, such as AI-generated nonconsensual intimate imagery, including of children. Privacy concerns abound, as does consternation over misinformation and disinformation. And it’s already harming the environment.&lt;/p&gt;
    &lt;p&gt;Expanding copyright will not mitigate these harms, and we shouldn’t forfeit free speech and innovation to chase snake oil “solutions” that won’t work.&lt;/p&gt;
    &lt;p&gt;We need solutions that address the roots of these problems, like inadequate protections for labor rights and personal privacy. Targeted, issue-specific policies are far more likely to succeed in resolving the problems society faces. Take competition, for example. Proponents of copyright expansion argue that treating AI development like the fair use that it is would only enrich a handful of tech behemoths. But imposing onerous new copyright licensing requirements to train models would lock in the market advantages enjoyed by Big Tech and Big Media—the only companies that own large content libraries or can afford to license enough material to build a deep learning model—profiting entrenched incumbents at the public’s expense. What neither Big Tech nor Big Media will say is that stronger antitrust rules and enforcement would be a much better solution.&lt;/p&gt;
    &lt;p&gt;What’s more, looking beyond copyright future-proofs the protections. Stronger environmental protections, comprehensive privacy laws, worker protections, and media literacy will create an ecosystem where we will have defenses against any new technology that might cause harm in those areas, not just generative AI.&lt;/p&gt;
    &lt;p&gt;Expanding copyright, on the other hand, threatens socially beneficial uses of AI—for example, to conduct scientific research and generate new creative expression—without meaningfully addressing the harms.&lt;/p&gt;
    &lt;p&gt;This post is part of our AI and Copyright series. For more information about the state of play in this evolving area, see our first post.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45816013</guid><pubDate>Tue, 04 Nov 2025 21:19:15 +0000</pubDate></item><item><title>Send this article to your friend who still thinks the cloud is a good idea</title><link>https://rameerez.com/send-this-article-to-your-friend-who-still-thinks-the-cloud-is-a-good-idea/</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45816041</guid><pubDate>Tue, 04 Nov 2025 21:22:15 +0000</pubDate></item><item><title>CPUs and GPUs to Become More Expensive After TSMC Price Hike in 2026</title><link>https://www.guru3d.com/story/cpus-and-gpus-to-become-more-expensive-after-tsmc-price-hike-in-2026/</link><description>&lt;doc fingerprint="2ef0d9142a987f3f"&gt;
  &lt;main&gt;
    &lt;p&gt;A big part of the story is AI. Demand for chips used in data centers and AI training has exploded, and TSMC now produces more than 80% of all AI chips worldwide. That puts the company in a strong position to adjust prices as its production lines run at full capacity. Right now, advanced nodes generate about 74% of TSMC’s total income — with 5 nm responsible for 37% and 3 nm for 23%. Once 2 nm production ramps up in 2026, that figure will exceed 75%. The company says the higher prices aren’t just about profits; they’re also intended to keep TSMC ahead technologically and fund research into future manufacturing processes like 1.4 nm.&lt;/p&gt;
    &lt;p&gt;To make room for this focus, TSMC is shifting engineers and equipment away from older nodes like 6 nm and 7 nm. That could leave some customers in automotive or industrial markets—who still depend on mature process technologies—with fewer manufacturing options or higher costs. The company’s leadership says this isn’t a one-time adjustment but part of a multi-year price realignment that could continue through 2030. Given that most modern CPUs and GPUs use TSMC’s advanced nodes, the impact will almost certainly reach consumers. Hardware built after 2026—especially AI accelerators, gaming GPUs, and next-generation desktop processors—will likely cost more. Competitors such as Intel and Samsung are working to close the gap with their own 2 nm technologies, but TSMC remains the market leader by a wide margin. Its ability to command higher prices without losing business highlights just how critical it has become to the global semiconductor ecosystem.&lt;/p&gt;
    &lt;p&gt;Source: digitimes&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45816217</guid><pubDate>Tue, 04 Nov 2025 21:42:29 +0000</pubDate></item><item><title>US gives local police a face-scanning app similar to one used by ICE agents</title><link>https://arstechnica.com/tech-policy/2025/11/us-gives-local-police-a-face-scanning-app-similar-to-one-used-by-ice-agents/</link><description>&lt;doc fingerprint="1cc80d5051bf4319"&gt;
  &lt;main&gt;
    &lt;p&gt;US Customs and Border Protection (CBP) launched a face-scanning app for local law enforcement agencies that assist the federal government with immigration-enforcement operations. The Mobile Identify app was released on the Google Play store on October 30.&lt;/p&gt;
    &lt;p&gt;“This app facilitates functions authorized by Section 287(g) of the Immigration and Nationality Act (INA),” a US law that lets Immigration and Customs Enforcement (ICE) delegate immigration-officer duties to state and local law enforcement, according to the Mobile Identify app’s description on the Google Play store. “Through a formal agreement, or Memorandum of Agreement (MOA), with DHS [Department of Homeland Security], participating agencies like your Sheriff’s Department can have designated officers who are trained, certified, and authorized to perform certain immigration enforcement functions, helping to identify and process individuals who may be in the country unlawfully. This tool is built to streamline those responsibilities securely and efficiently, directly in the field.”&lt;/p&gt;
    &lt;p&gt;A screenshot of the app on the Google Play listing shows it requires camera access “to take photos of subjects.” More information on how it works was reported today by 404 Media. “A source with knowledge of the app told 404 Media the app doesn’t return names after a face search. Instead it tells users to contact ICE and provides a reference number, or to not detain the person depending on the result,” the news report said.&lt;/p&gt;
    &lt;p&gt;404 Media said it downloaded the app and decompiled its code, finding that “multiple parts of the app’s code make clear references to scanning faces. One package is called ‘facescanner.’ Other parts mention ‘FacePresence’ and ‘No facial image found.'” The app is not available on iPhone as of now.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45816260</guid><pubDate>Tue, 04 Nov 2025 21:48:26 +0000</pubDate></item></channel></rss>