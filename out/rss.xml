<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Tue, 11 Nov 2025 22:38:51 +0000</lastBuildDate><item><title>Pikaday: A friendly guide to front-end date pickers</title><link>https://pikaday.dbushell.com</link><description>&lt;doc fingerprint="2611da0b238d3ebc"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Who needs a JavaScript date picker?&lt;/head&gt;
    &lt;p&gt;The answer, in most cases, is nobody! Complex UI leads to more errors and abandoned forms. There can be easier ways to pick a date than a calendar widget. This guide provides alternate ideas and aims to send developers on a path towards user-friendly interfaces.&lt;/p&gt;
    &lt;head rend="h2"&gt;Native date and time inputs&lt;/head&gt;
    &lt;p&gt;If you absolutely must use a calendar widget then itâs wise to use the native input. All modern browsers support native date and time inputs.&lt;/p&gt;
    &lt;head rend="h3"&gt;Date input&lt;/head&gt;
    &lt;p&gt; The &lt;code&gt;date&lt;/code&gt; input type provides a native date picker.
        &lt;/p&gt;
    &lt;head rend="h3"&gt;Time input&lt;/head&gt;
    &lt;p&gt; The &lt;code&gt;time&lt;/code&gt; input type allows users to specify hours and minutes.
          &lt;/p&gt;
    &lt;head rend="h3"&gt;Datetime input&lt;/head&gt;
    &lt;p&gt; The &lt;code&gt;datetime-local&lt;/code&gt; input type combines both date and time.
          &lt;/p&gt;
    &lt;head rend="h2"&gt;Why use native inputs&lt;/head&gt;
    &lt;p&gt;Native inputs are super easy to implement with one line of code. The web browser handles many important details for developers:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Accessibility (mostly*)&lt;/item&gt;
      &lt;item&gt;Performance&lt;/item&gt;
      &lt;item&gt;Internationalisation&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Let browsers do the hard work! Browsers allow keyboard users to type numbers in sequence. Most browsers provide alternate UI for time and date selection like the classic calendar widget. They're not perfect but do you trust a JavaScript library to do better?&lt;/p&gt;
    &lt;p&gt;*Oh dear! Even native date pickers have some accessibility issues.&lt;/p&gt;
    &lt;head rend="h2"&gt;Separate inputs&lt;/head&gt;
    &lt;p&gt;A single date picker can be tricky to operate. For memorable dates using separate inputs can improve usability. The example below is based on GOV.UK date input component.&lt;/p&gt;
    &lt;head rend="h3"&gt;Select elements&lt;/head&gt;
    &lt;p&gt;If only a limited set of data is valid then using select elements may be suitable. They can require fewer interactions to use and they eliminate typing errors.&lt;/p&gt;
    &lt;p&gt;Numeric month labels can be helpful but take care in how theyâre written. Screen readers may mistakenly announce â1 Januaryâ as âthe 1st of Januaryâ, for example.&lt;/p&gt;
    &lt;p&gt;Travel booking often has a fixed schedule with limited time options, such as every 15 minutes. Relative dates like âTodayâ and âTomorrowâ can be easier to understand.&lt;/p&gt;
    &lt;head rend="h2"&gt;Masked inputs&lt;/head&gt;
    &lt;p&gt;Another common alternative to date pickers is a single input with a placeholder mask. This can be used for full or partial dates. JavaScript can enhancement the experience.&lt;/p&gt;
    &lt;p&gt; The examples above provide client-side validation with errors such as âPlease enter a valid day for February (1 to 28)â. Valid dates are confirmed in full and formatted with the &lt;code&gt;Intl&lt;/code&gt; API.
        &lt;/p&gt;
    &lt;p&gt;Caution! Updating input values with JavaScript can break native undo/redo.&lt;/p&gt;
    &lt;p&gt;Itâs even possible to visually combined mutliple inputs using CSS to appear as one.&lt;/p&gt;
    &lt;head rend="h2"&gt;Ranges and limited options&lt;/head&gt;
    &lt;p&gt; JavaScript date pickers that support range selection across two calendars are difficult to use, especially without a pointer. Consider providing two inputs instead to reduce complexity. If users are required to select an available date then a group of &lt;code&gt;radio&lt;/code&gt; inputs can do the job.
        &lt;/p&gt;
    &lt;p&gt;The example below illustrates the idea but is not fully interactive.&lt;/p&gt;
    &lt;p&gt;There are many design variations of this pattern. This idea is to replace complicated UI with a series of simple tasks. Such a pattern can be implemented as a multi-page form with JavaScript used to enhance it into a single page interactive experience.&lt;/p&gt;
    &lt;head rend="h2"&gt;Frequently asked questions&lt;/head&gt;
    &lt;head&gt;What if I use a JavaScript framework like React?&lt;/head&gt;
    &lt;p&gt; All good JavaScript frameworks allow you to use native HTML elements. Not everything needs to be a custom component. Native input events can integrate with framework callbacks. Use attributes like &lt;code&gt;value&lt;/code&gt; for two-way state binding.
            &lt;/p&gt;
    &lt;head&gt;How do I style the native date picker?&lt;/head&gt;
    &lt;p&gt; The on-page &lt;code&gt;input&lt;/code&gt; element
              can be partially styled but other parts are not stylable.
              That is a good thing! Native system UI is familiar to the user.
              The design will differ based on operating system and input method.
              Date pickers even look different across browsers and that's fine too, you don't need to add yet another design to the mix!
            &lt;/p&gt;
    &lt;head&gt;A stakeholder is demanding a JavaScript date picker, how do I dissuade them?&lt;/head&gt;
    &lt;p&gt;Remember: the end goal is a successful form submission. Complex and fragile UI leads to more errors. All date pickers have accessibility issues. Combining basic inputs can be more user-friendly. Untested JavaScript UI may fall foul of regulation like the European Accessibility Act. Keep it simple for success!&lt;/p&gt;
    &lt;head&gt;How do I test and guarantee accessibility?&lt;/head&gt;
    &lt;p&gt;Itâs critical to understand the relevant accessibility guidelines. You donât need to memorise WCAG but there are no shortcuts to learning the important parts. Leverage existing web standards to avoid mistakes trying to code custom UI.&lt;/p&gt;
    &lt;p&gt;Browser dev tools have built-in accessibility features to help identify mistakes. However, no tool is perfect. The only way to know for sure is to conduct user testing.&lt;/p&gt;
    &lt;p&gt;Accessibility overlays are strongly discouraged and can make matters worse.&lt;/p&gt;
    &lt;head&gt;Where can I learn more about date picker accessibility?&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Collecting dates in an accessible way by Graham Armfield&lt;/item&gt;
      &lt;item&gt;What makes an accessible date picker? Is it even possible? by Russ Weakley&lt;/item&gt;
      &lt;item&gt;Maybe You Donât Need a Date Picker by Adrian Roselli&lt;/item&gt;
      &lt;item&gt;Date Picker Dialog Example by ARIA Authoring Practices Guide&lt;/item&gt;
      &lt;item&gt;Designing The Perfect Date And Time Picker by Vitaly Friedman&lt;/item&gt;
    &lt;/list&gt;
    &lt;head&gt;This is all great but can you please recommend a JavaScript date picker?&lt;/head&gt;
    &lt;p&gt;Sorry, no! There is no universal solution and all date pickers have issues. I hope this guide has given you the knowledge to evaluate your own requirements. Try to achieve your goal in the simplest way. A date picker is probably not the answer.&lt;/p&gt;
    &lt;p&gt;Before you go! Remember to test and gather feedback from real users :)&lt;/p&gt;
    &lt;p&gt;This guide is a work in progress, feedback is welcome!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45887957</guid><pubDate>Tue, 11 Nov 2025 14:58:47 +0000</pubDate></item><item><title>Firefox expands fingerprint protections</title><link>https://blog.mozilla.org/en/firefox/fingerprinting-protections/</link><description>&lt;doc fingerprint="9f3493f63975af6e"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Firefox expands fingerprint protections: advancing towards a more private web&lt;/head&gt;
    &lt;p&gt;With Firefox 145, we’re rolling out major privacy upgrades that take on browser fingerprinting — a pervasive and hidden tracking technique that lets websites identify you even when cookies are blocked or you’re in private browsing. These protections build on Mozilla’s long-term goal of building a healthier, transparent and privacy-preserving web ecosystem.&lt;/p&gt;
    &lt;p&gt;Fingerprinting builds a secret digital ID of you by collecting subtle details of your setup — ranging from your time zone to your operating system settings — that together create a “fingerprint” identifiable across websites and across browser sessions. Having a unique fingerprint means fingerprinters can continuously identify you invisibly, allowing bad actors to track you without your knowledge or consent. Online fingerprinting is able to track you for months, even when you use any browser’s private browsing mode.&lt;/p&gt;
    &lt;p&gt;Protecting people’s privacy has always been core to Firefox. Since 2020, Firefox’s built-in Enhanced Tracking Protection (ETP) has blocked known trackers and other invasive practices, while features like Total Cookie Protection and now expanded fingerprinting defenses demonstrate a broader goal: prioritizing your online freedom through innovative privacy-by-design. Since 2021, Firefox has been incrementally enhancing anti-fingerprinting protections targeting the most common pieces of information collected for suspected fingerprinting uses.&lt;/p&gt;
    &lt;p&gt;Today, we are excited to announce the completion of the second phase of defenses against fingerprinters that linger across all your browsing but aren’t in the known tracker lists. With these fingerprinting protections, the amount of Firefox users trackable by fingerprinters is reduced by half.&lt;/p&gt;
    &lt;head rend="h2"&gt;How we built stronger defenses&lt;/head&gt;
    &lt;p&gt;Drawing from a global analysis of how real people’s browsers can be fingerprinted, Mozilla has developed new, unique and powerful defenses against real-world fingerprinting techniques. Firefox is the first browser with this level of insight into fingerprinting and the most effective deployed defenses to reduce it. Like Total Cookie Protection, one of our most innovative privacy features, these new defenses are debuting in Private Browsing Mode and ETP Strict mode initially, while we work to enable them by default.&lt;/p&gt;
    &lt;head rend="h2"&gt;How Firefox protects you&lt;/head&gt;
    &lt;p&gt;These fingerprinting protections work on multiple layers, building on Firefox’s already robust privacy features. For example, Firefox has long blocked known tracking and fingerprinting scripts as part of its Enhanced Tracking Protection.&lt;/p&gt;
    &lt;p&gt;Beyond blocking trackers, Firefox also limits the information it makes available to websites — a privacy-by-design approach — that preemptively shrinks your fingerprint. Browsers provide a way for websites to ask for information that enables legitimate website features, e.g. your graphics hardware information, which allows sites to optimize games for your computer. But trackers can also ask for that information, for no other reason than to help build a fingerprint of your browser and track you across the web.&lt;/p&gt;
    &lt;p&gt;Since 2021, Firefox has been incrementally advancing fingerprinting protections, covering the most pervasive fingerprinting techniques. These include things like how your graphics card draws images, which fonts your computer has, and even tiny differences in how it performs math. The first phase plugged the biggest and most-common leaks of fingerprinting information.&lt;/p&gt;
    &lt;p&gt;Recent Firefox releases have tackled the next-largest leaks of user information used by online fingerprinters. This ranges from strengthening the font protections to preventing websites from getting to know your hardware details like the number of cores your processor has, the number of simultaneous fingers your touchscreen supports, and the dimensions of your dock or taskbar. The full list of detailed protections is available in our documentation.&lt;/p&gt;
    &lt;p&gt;Our research shows these improvements cut the percentage of users seen as unique by almost half.&lt;/p&gt;
    &lt;p&gt;Firefox’s new protections are a balance of disrupting fingerprinters while maintaining web usability. More aggressive fingerprinting blocking might sound better, but is guaranteed to break legitimate website features. For instance, calendar, scheduling, and conferencing tools legitimately need your real time zone. Firefox’s approach is to target the most leaky fingerprinting vectors (the tricks and scripts used by trackers) while preserving functionality many sites need to work normally. The end result is a set of layered defenses that significantly reduce tracking without downgrading your browsing experience. More details are available about both the specific behaviors and how to recognize a problem on a site and disable protections for that site alone, so you always stay in control. The goal: strong privacy protections that don’t get in your way.&lt;/p&gt;
    &lt;head rend="h2"&gt;What’s next for your privacy&lt;/head&gt;
    &lt;p&gt;If you open a Private Browsing window or use ETP Strict mode, Firefox is already working behind the scenes to make you harder to track. The latest phase of Firefox’s fingerprinting protections marks an important milestone in our mission to deliver: smart privacy protections that work automatically — no further extensions or configurations needed. As we head into the future, Firefox remains committed to fighting for your privacy, so you get to enjoy the web on your terms. Upgrade to the latest Firefox and take back control of your privacy.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45888891</guid><pubDate>Tue, 11 Nov 2025 16:04:08 +0000</pubDate></item><item><title>Show HN: Cactoide – Federated RSVP Platform</title><link>https://cactoide.org/</link><description>&lt;doc fingerprint="efaf8ee999fbbf20"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Cactoide(ea)* ðµ&lt;/head&gt;
    &lt;head rend="h2"&gt;The Ultimate RSVP Platform&lt;/head&gt;
    &lt;p&gt;A federated mobile-first event RSVP platform that lets you create events, share unique URLs, and collect RSVPs without any registration required. With built-in federation, discover and share events across a decentralized network of instances.&lt;/p&gt;
    &lt;p&gt;Cactoide is open source and easily self-hostable. View the source code, contribute, or host your own instance.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why Cactoide(ae)?ðµ*&lt;/head&gt;
    &lt;p&gt;Like the cactus, great events bloom under any condition when managed with care. Cactoide(ae) helps you streamline RSVPs, simplify coordination, and keep every detail efficientâso your gatherings are resilient, vibrant, and unforgettable.&lt;/p&gt;
    &lt;head rend="h2"&gt;Discover Public Events&lt;/head&gt;
    &lt;p&gt;See what others are planning and get inspired&lt;/p&gt;
    &lt;head rend="h2"&gt;Why Cactoide?&lt;/head&gt;
    &lt;head rend="h3"&gt;Instant Event Creation&lt;/head&gt;
    &lt;p&gt;Create events in seconds with our streamlined form. No accounts, no waiting, just pure efficiency.&lt;/p&gt;
    &lt;head rend="h3"&gt;One-Click Sharing&lt;/head&gt;
    &lt;p&gt;Each event gets a unique, memorable URL. Share instantly via any platform or messaging app.&lt;/p&gt;
    &lt;head rend="h3"&gt;All-in-One Clarity&lt;/head&gt;
    &lt;p&gt;No more scrolling through endless chats and reactions. See everyone's availability and responses neatly in one place.&lt;/p&gt;
    &lt;head rend="h3"&gt;No Hassle, No Sign-Ups&lt;/head&gt;
    &lt;p&gt;Skip registrations and endless forms. Unlike other event platforms, you create and share instantly â no accounts, no barriers.&lt;/p&gt;
    &lt;head rend="h3"&gt;Smart Limits&lt;/head&gt;
    &lt;p&gt;Choose between unlimited RSVPs or set a limited capacity. Perfect for any event size.&lt;/p&gt;
    &lt;head rend="h3"&gt;Effortless Simplicity&lt;/head&gt;
    &lt;p&gt;Designed to be instantly clear and easy. No learning curve â just open, create, and go.&lt;/p&gt;
    &lt;head rend="h3"&gt;Invite Links&lt;/head&gt;
    &lt;p&gt;Create invite-only events with special links. Only people with the specific invite link can RSVP, giving you full control over who can attend.&lt;/p&gt;
    &lt;head rend="h3"&gt;Federation&lt;/head&gt;
    &lt;p&gt;Connect with other Cactoide instances to discover events across the network. Share your public events and create a decentralized event discovery network.&lt;/p&gt;
    &lt;head rend="h2"&gt;How It Works&lt;/head&gt;
    &lt;head rend="h3"&gt;1. Create Event&lt;/head&gt;
    &lt;p&gt;Fill out a simple form with event details. Choose between limited or unlimited capacity.&lt;/p&gt;
    &lt;head rend="h3"&gt;2. Get Unique URL&lt;/head&gt;
    &lt;p&gt;Receive a random, memorable URL for your event. Perfect for sharing anywhere.&lt;/p&gt;
    &lt;head rend="h3"&gt;3. Collect RSVPs&lt;/head&gt;
    &lt;p&gt;People visit your link and join with just their name. No accounts needed.&lt;/p&gt;
    &lt;head rend="h2"&gt;Ready to Create Your First Event?&lt;/head&gt;
    &lt;p&gt;Join thousands of event organizers who trust Cactoide&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45889793</guid><pubDate>Tue, 11 Nov 2025 17:01:07 +0000</pubDate></item><item><title>Cache-friendly, low-memory Lanczos algorithm in Rust</title><link>https://lukefleed.xyz/posts/cache-friendly-low-memory-lanczos/</link><description>&lt;doc fingerprint="91155a0e0fae71f1"&gt;
  &lt;main&gt;
    &lt;p&gt;The standard Lanczos method for computing matrix functions has a brutal memory requirement: storing an basis matrix that grows with every iteration. For a -variable problem needing iterations, that’s roughly 4 GB just for the basis.&lt;/p&gt;
    &lt;p&gt;In this post, we will explore one of the most straightforward solutions to this problem: a two-pass variant of the Lanczos algorithm that only requires memory at the cost of doubling the number of matrix-vector products. The surprising part is that when implemented carefully, the two-pass version isn’t just memory-efficient—it can be faster for certain problems. We will dig into why.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;All code is available on GitHub: two-pass-lanczos&lt;/item&gt;
      &lt;item&gt;The full technical report with proofs and additional experiments: report.pdf&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Table of Contents&lt;/head&gt;
    &lt;head&gt;Open Table of Contents&lt;/head&gt;
    &lt;head rend="h1"&gt;Computing Matrix Functions&lt;/head&gt;
    &lt;p&gt;Let’s consider the problem of computing the action of matrix functions on a vector:&lt;/p&gt;
    &lt;p&gt;where is a large sparse Hermitian matrix and is a matrix function defined on the spectrum of . This is a problem that appears pretty often in scientific computing: solving linear systems corresponds to , exponential integrators for PDEs use , and many other problems require functions like or .&lt;/p&gt;
    &lt;p&gt;Indeed, there are a lot problems with computing directly. First of all, even if is sparse, is generally dense. Storing it explicitly is out of the question for large problems. Even if we could store it, computing it directly would require algorithms like the Schur-Parlett method that scale as , which is impractical for large .&lt;/p&gt;
    &lt;p&gt;However we know that given any matrix function defined on the spectrum of , we can express as a polynomial in of degree at most (the size of the matrix) such that (this is a consequence of the Cayley-Hamilton theorem). This polynomial interpolates and its derivatives in the Hermitian sense at the eigenvalues of .&lt;/p&gt;
    &lt;p&gt;This gives us a good and a bad news: the good news is that, well, we can express as a polynomial in . The bad news is that the degree of this polynomial can be as high as , which is huge for large problems. The idea is then to find a low-degree polynomial approximation to that is good enough for our purposes. If we can find a polynomial of degree such that , then we can approximate the solution as:&lt;/p&gt;
    &lt;p&gt;This polynomial only involves vectors within a specific subspace.&lt;/p&gt;
    &lt;head rend="h2"&gt;Krylov Projection&lt;/head&gt;
    &lt;p&gt;We can notice that only depends on vectors in the Krylov subspace of order&lt;/p&gt;
    &lt;p&gt;This is fortunate: we can compute an approximate solution by staying within this space, which only requires repeated matrix-vector products with . For large sparse matrices, that’s the only operation we can do efficiently anyway.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;We don’t need to construct explicitly. We compute iteratively: .&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;But there’s a problem: the raw vectors form a terrible basis. They quickly become nearly parallel, making any computation numerically unstable. We need an orthonormal basis.&lt;/p&gt;
    &lt;head rend="h3"&gt;Building an Orthonormal Basis&lt;/head&gt;
    &lt;p&gt;The standard method is the Arnoldi process, which is Gram-Schmidt applied to Krylov subspaces. We start by normalizing . Then, iteratively:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Compute a new candidate:&lt;/item&gt;
      &lt;item&gt;Orthogonalize against all existing basis vectors:&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Normalize:&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The coefficients become entries of a projected matrix. After iterations, we have:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;: an orthonormal basis for&lt;/item&gt;
      &lt;item&gt;: an upper Hessenberg matrix representing the projection of onto this subspace&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We can express this relationship with the Arnoldi decomposition:&lt;/p&gt;
    &lt;head rend="h3"&gt;Solving in the Reduced Space&lt;/head&gt;
    &lt;p&gt;Now we approximate our original problem by solving it in the small -dimensional space. Using the Full Orthogonal Method (FOM), we enforce that the residual is orthogonal to the Krylov subspace. This gives:&lt;/p&gt;
    &lt;p&gt;where is computed as:&lt;/p&gt;
    &lt;p&gt;The heavy lifting is now on computing , a small matrix. Since , we can afford direct methods like Schur-Parlett ().&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;For (linear systems), this reduces to solving with LU decomposition.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h1"&gt;The Lanczos Algorithm&lt;/head&gt;
    &lt;p&gt;When is Hermitian (or symmetric in the real case), the general Arnoldi process simplifies dramatically. We can prove that must also be Hermitian. A matrix that is both upper Hessenberg and Hermitian must be real, symmetric, and tridiagonal. This is a huge simplification.&lt;/p&gt;
    &lt;p&gt;In the literature, this projected matrix is denoted to highlight its tridiagonal structure:&lt;/p&gt;
    &lt;p&gt;where are the diagonal elements and are the off-diagonals (subdiagonals from the orthogonalization).&lt;/p&gt;
    &lt;head rend="h2"&gt;Three-Term Recurrence&lt;/head&gt;
    &lt;p&gt;This tridiagonal structure leads to a beautiful simplification. To build the next basis vector , we don’t need the entire history of vectors. We only need the two previous ones. Since is Hermitian, this guarantees that any new vector is automatically orthogonal to all earlier vectors (beyond the previous two). So we can skip the full orthogonalization and use a simple three-term recurrence:&lt;/p&gt;
    &lt;p&gt;Rearranging gives us an algorithm to compute directly:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Compute the candidate:&lt;/item&gt;
      &lt;item&gt;Extract the diagonal coefficient:&lt;/item&gt;
      &lt;item&gt;Orthogonalize against the two previous vectors:&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Normalize: and&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is known as the Lanczos algorithm. It’s more efficient than Arnoldi because each iteration only orthogonalizes against two previous vectors instead of all prior ones.&lt;/p&gt;
    &lt;head rend="h2"&gt;Reconstructing the Solution&lt;/head&gt;
    &lt;p&gt;After iterations, we end up with the tridiagonal matrix and all basis vectors . We can then reconstruct the approximate solution as:&lt;/p&gt;
    &lt;p&gt;where is solved from the small tridiagonal matrix.&lt;/p&gt;
    &lt;p&gt;There is a timing problem however: we cannot compute the coefficients until all iterations are complete. The full matrix is only available at the end, so we must store every basis vector along the way, leading to a memory cost of .&lt;/p&gt;
    &lt;p&gt;So we’re left with a choice: whether we store all the basis vectors and solve the problem in passes, or find a way to avoid storing them. There is a middle ground.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;There are also techniques to compress the basis vectors, have a look here&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h1"&gt;Two-Pass Algorithm&lt;/head&gt;
    &lt;p&gt;Here’s where we break the timing deadlock. The insight that we don’t actually need to store the basis vectors if we can afford to compute them twice&lt;/p&gt;
    &lt;p&gt;Think about what we have after the first pass. We’ve computed all the and coefficients that compose the entire tridiagonal matrix . These numbers are small compared to the full basis. What if we kept only these scalars, discarded all the vectors, and then replayed the Lanczos recurrence a second time? We’d regenerate the same basis, and this time we’d use it to build the solution.&lt;/p&gt;
    &lt;p&gt;This comes at a cost. We run Lanczos twice, so we pay for matrix-vector products instead of . But we only ever store a constant number of vectors in memory, no basis matrix. The memory complexity drops to .&lt;/p&gt;
    &lt;p&gt;It sounds like a bad trade at first. But as we’ll see later, the cache behavior of this two-pass approach can actually make it as fast (or even faster) on real hardware if well optimized.&lt;/p&gt;
    &lt;head rend="h2"&gt;First Pass: Compute the Projected Problem&lt;/head&gt;
    &lt;p&gt;We initialize and set , .Then we run the standard Lanczos recurrence:&lt;/p&gt;
    &lt;p&gt;At each step, we record and . But we do not store . Instead, we discard it immediately after computing . In this way we only keep in memory at most just three vectors at any time (, , and the working vector ).&lt;/p&gt;
    &lt;p&gt;After iterations, we have the full set . These scalars define the tridiagonal matrix . We can now solve:&lt;/p&gt;
    &lt;p&gt;This is the solution in the reduced space. Now that we have the coefficients we need to build .&lt;/p&gt;
    &lt;head rend="h2"&gt;Second Pass: Reconstruct and Accumulate&lt;/head&gt;
    &lt;p&gt;With in memory, we replay the Lanczos recurrence exactly as before. We start with the same initialization (, , ) and apply the same sequence of operations, using the stored scalars and to reconstruct each basis vector on demand. We can write some rust-like pseudocode for this second pass to get a feel for it:&lt;/p&gt;
    &lt;code&gt;let mut x_k = vec![0.0; n];
let mut v_prev = vec![0.0; n];
let mut v_curr = b.clone() / b_norm;

for j in 1..=k {
    let w = A @ v_curr;  // Matrix-vector product

    // We don't recompute alpha/beta; we already have them from pass 1
    let alpha_j = alphas[j - 1];
    let beta_prev = j &amp;gt; 1 ? betas[j - 2] : 0.0;

    // Accumulate the solution
    x_k += y_k[j - 1] * v_curr;

    // Regenerate the next basis vector for the *next* iteration
    let v_next = (w - alpha_j * v_curr - beta_prev * v_prev) / betas[j - 1];

    // Slide the window forward
    v_prev = v_curr;
    v_curr = v_next;
}&lt;/code&gt;
    &lt;p&gt;This loop regenerates each on demand and immediately uses it to update the solution. Once we’ve accumulated into , we discard the vector. We never store the full basis.&lt;/p&gt;
    &lt;head rend="h3"&gt;A Subtle Numerical Point&lt;/head&gt;
    &lt;p&gt;There is one detail worth noting: floating-point arithmetic is deterministic. When we replay the Lanczos recurrence in the second pass with the exact same inputs and the exact same order of operations, we get bitwise-identical vectors. The regenerated in pass 2 are identical to the ones computed in pass 1.&lt;/p&gt;
    &lt;p&gt;However, the order in which we accumulate the solution differs. In a standard Lanczos, is built as a single matrix-vector product: (a &lt;code&gt;gemv&lt;/code&gt; call in BLAS). In the two-pass method, it’s built as a loop of scaled vector additions (a series of &lt;code&gt;axpy&lt;/code&gt; calls). These operations accumulate rounding error differently, so the final solution differs slightly, typically by machine epsilon. This rarely matters in practice, and convergence is unaffected.&lt;/p&gt;
    &lt;head rend="h1"&gt;Implementation&lt;/head&gt;
    &lt;p&gt;Building this in Rust forces us to think concretely about where data lives and how it flows through the cache hierarchy. We need to control memory layout, decide when allocations happen, and choose abstractions that cost us nothing at runtime.&lt;/p&gt;
    &lt;p&gt;For linear algebra, we reach for &lt;code&gt;faer&lt;/code&gt;. Three design choices in this library matter for what we’re building:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Stack allocation via &lt;code&gt;MemStack&lt;/code&gt;: Pre-allocated scratch space that lives for the entire computation. The hot path becomes allocation-free.&lt;/item&gt;
      &lt;item&gt;Matrix-free operators: The &lt;code&gt;LinOp&lt;/code&gt;trait defines an operator by its action (&lt;code&gt;apply&lt;/code&gt;) without materializing a matrix. For large sparse problems, this is the only viable approach.&lt;/item&gt;
      &lt;item&gt;SIMD-friendly loops: The &lt;code&gt;zip!&lt;/code&gt;macro generates code that compiles to packed instructions.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Recurrence Step&lt;/head&gt;
    &lt;p&gt;Our starting point is the Lanczos three-term recurrence that we derived earlier:&lt;/p&gt;
    &lt;p&gt;We can translate this into a recurrence step function. The signature looks like this:&lt;/p&gt;
    &lt;code&gt;fn lanczos_recurrence_step&amp;lt;T: ComplexField, O: LinOp&amp;lt;T&amp;gt;&amp;gt;(
    operator: &amp;amp;O,
    mut w: MatMut&amp;lt;'_, T&amp;gt;,
    v_curr: MatRef&amp;lt;'_, T&amp;gt;,
    v_prev: MatRef&amp;lt;'_, T&amp;gt;,
    beta_prev: T::Real,
    stack: &amp;amp;mut MemStack,
) -&amp;gt; (T::Real, Option&amp;lt;T::Real&amp;gt;)&lt;/code&gt;
    &lt;p&gt;The function is generic over the field type &lt;code&gt;T&lt;/code&gt; (&lt;code&gt;f64&lt;/code&gt;, &lt;code&gt;c64&lt;/code&gt;, etc.) and the operator type &lt;code&gt;O&lt;/code&gt;. It operates on matrix views (&lt;code&gt;MatMut&lt;/code&gt; and &lt;code&gt;MatRef&lt;/code&gt;) to avoid unnecessary data copies. The return type gives us the diagonal element  and, if no breakdown occurs, the off-diagonal .&lt;/p&gt;
    &lt;p&gt;Now we can implement the body by following the math. The first step is the most expensive:&lt;/p&gt;
    &lt;code&gt;// 1. Apply operator: w = A * v_curr
operator.apply(w.rb_mut(), v_curr, Par::Seq, stack);&lt;/code&gt;
    &lt;p&gt;The matrix-vector product dominates the computational cost. Everything else is secondary.&lt;/p&gt;
    &lt;p&gt;Next, we orthogonalize against . This is where we benefit from &lt;code&gt;faer&lt;/code&gt;’s design. The &lt;code&gt;zip!&lt;/code&gt; macro fuses this operation into a single loop that the compiler vectorizes into SIMD instructions.&lt;/p&gt;
    &lt;code&gt;// 2. Orthogonalize against v_{j-1}: w -= β_{j-1} * v_{j-1}
let beta_prev_scaled = T::from_real_impl(&amp;amp;beta_prev);
zip!(w.rb_mut(), v_prev).for_each(|unzip!(w_i, v_prev_i)| {
    *w_i = sub(w_i, &amp;amp;mul(&amp;amp;beta_prev_scaled, v_prev_i));
});&lt;/code&gt;
    &lt;p&gt;With &lt;code&gt;w&lt;/code&gt; partially orthogonalized, we can compute the diagonal coefficient via an inner product. Since  is Hermitian,  is guaranteed real.&lt;/p&gt;
    &lt;code&gt;// 3. Compute α_j = v_j^H * w
let alpha = T::real_part_impl(&amp;amp;(v_curr.adjoint() * w.rb())[(0, 0)]);&lt;/code&gt;
    &lt;p&gt;We complete the orthogonalization against with another &lt;code&gt;zip!&lt;/code&gt; loop.&lt;/p&gt;
    &lt;code&gt;// 4. Orthogonalize against v_j: w -= α_j * v_j
let alpha_scaled = T::from_real_impl(&amp;amp;alpha);
zip!(w.rb_mut(), v_curr).for_each(|unzip!(w_i, v_curr_i)| {
    *w_i = sub(w_i, &amp;amp;mul(&amp;amp;alpha_scaled, v_curr_i));
});&lt;/code&gt;
    &lt;p&gt;Now &lt;code&gt;w&lt;/code&gt; holds the unnormalized next basis vector. We compute its norm to get . If this norm is numerically zero, the Krylov subspace is invariant, the iteration has reached its natural stopping point. This is called breakdown.&lt;/p&gt;
    &lt;code&gt;// 5. Compute β_j = ||w||_2 and check for breakdown
let beta = w.rb().norm_l2();
let tolerance = breakdown_tolerance::&amp;lt;T::Real&amp;gt;();

if beta &amp;lt;= tolerance {
    (alpha, None)
} else {
    (alpha, Some(beta))
}&lt;/code&gt;
    &lt;p&gt;The function returns &lt;code&gt;None&lt;/code&gt; for  when breakdown occurs, signaling to the caller that no further iterations should proceed.&lt;/p&gt;
    &lt;head rend="h2"&gt;An Iterator for State Management&lt;/head&gt;
    &lt;p&gt;The recurrence step is a pure function, but calling it in a loop is both inefficient and awkward. We’d need to manually pass vectors in and out of each iteration. More critically, we’d create copies when we should be reusing memory.&lt;/p&gt;
    &lt;p&gt;The iterator pattern solves this. We create a struct that encapsulates the state:&lt;/p&gt;
    &lt;code&gt;struct LanczosIteration&amp;lt;'a, T: ComplexField, O: LinOp&amp;lt;T&amp;gt;&amp;gt; {
    operator: &amp;amp;'a O,
    v_prev: Mat&amp;lt;T&amp;gt;,       // v_{j-1}
    v_curr: Mat&amp;lt;T&amp;gt;,       // v_j
    work: Mat&amp;lt;T&amp;gt;,         // Workspace for the next vector
    beta_prev: T::Real,   // β_{j-1}
    // ... iteration counters
}&lt;/code&gt;
    &lt;p&gt;The main design choice here is that vectors are owned (&lt;code&gt;Mat&amp;lt;T&amp;gt;&lt;/code&gt;), not borrowed. This enables an optimization in the &lt;code&gt;next_step&lt;/code&gt; method. After computing the next vector and normalizing it into &lt;code&gt;work&lt;/code&gt;, we cycle the state without allocating or copying:&lt;/p&gt;
    &lt;code&gt;// Inside next_step, after normalization...
core::mem::swap(&amp;amp;mut self.v_prev, &amp;amp;mut self.v_curr);
core::mem::swap(&amp;amp;mut self.v_curr, &amp;amp;mut self.work);&lt;/code&gt;
    &lt;p&gt;On x86-64, swapping two &lt;code&gt;Mat&amp;lt;T&amp;gt;&lt;/code&gt; structures (fat pointers) compiles to three &lt;code&gt;mov&lt;/code&gt; instructions. The pointers change, but no vector data moves. After the swap, &lt;code&gt;v_prev&lt;/code&gt; points to what &lt;code&gt;v_curr&lt;/code&gt; held, &lt;code&gt;v_curr&lt;/code&gt; points to &lt;code&gt;work&lt;/code&gt;’s allocation, and &lt;code&gt;work&lt;/code&gt; points to the old &lt;code&gt;v_prev&lt;/code&gt; data. In the next iteration, &lt;code&gt;work&lt;/code&gt; gets reused.&lt;/p&gt;
    &lt;p&gt;We keep exactly three n-dimensional vectors live in memory. The same allocations cycle through the computation, staying hot in L1 cache. This is the core reason the two-pass method can be faster than expected, the working set never leaves cache.&lt;/p&gt;
    &lt;head rend="h2"&gt;First Pass: Computing the Decomposition&lt;/head&gt;
    &lt;p&gt;The first pass runs the Lanczos iteration and collects the coefficients . Basis vectors are discarded after each step.&lt;/p&gt;
    &lt;code&gt;pub fn lanczos_pass_one&amp;lt;T: ComplexField&amp;gt;(
    operator: &amp;amp;impl LinOp&amp;lt;T&amp;gt;,
    b: MatRef&amp;lt;'_, T&amp;gt;,
    k: usize,
    stack: &amp;amp;mut MemStack,
) -&amp;gt; Result&amp;lt;LanczosDecomposition&amp;lt;T::Real&amp;gt;, LanczosError&amp;gt; {
    // ...
}&lt;/code&gt;
    &lt;p&gt;We allocate vectors for the coefficients with a capacity hint to avoid reallocations:&lt;/p&gt;
    &lt;code&gt;let mut alphas = Vec::with_capacity(k);
let mut betas = Vec::with_capacity(k - 1);&lt;/code&gt;
    &lt;p&gt;Then we construct the iterator. This allocates the three work vectors once. After this point, the hot path is allocation-free:&lt;/p&gt;
    &lt;code&gt;let mut lanczos_iter = LanczosIteration::new(operator, b, k, b_norm)?;

for i in 0..k {
    if let Some(step) = lanczos_iter.next_step(stack) {
        alphas.push(step.alpha);
        steps_taken += 1;

        let tolerance = breakdown_tolerance::&amp;lt;T::Real&amp;gt;();
        if step.beta &amp;lt;= tolerance {
            break;
        }

        if i &amp;lt; k - 1 {
            betas.push(step.beta);
        }
    } else {
        break;
    }
}&lt;/code&gt;
    &lt;p&gt;The check for breakdown stops the iteration when the residual becomes numerically zero. This means we’ve found an invariant subspace and there’s no value in continuing.&lt;/p&gt;
    &lt;p&gt;At the end, we collect the scalars into a &lt;code&gt;LanczosDecomposition&lt;/code&gt; struct. The memory footprint throughout this pass is constant: three n-dimensional vectors plus two small arrays that grow to at most  elements.&lt;/p&gt;
    &lt;head rend="h2"&gt;Second Pass: Reconstructing the Solution&lt;/head&gt;
    &lt;p&gt;Now we face a different problem. We have the coefficients from the first pass and the coefficient vector from solving the projected problem. We need to reconstruct the solution:&lt;/p&gt;
    &lt;p&gt;without storing the full basis matrix .&lt;/p&gt;
    &lt;p&gt;The recurrence step in this pass is structurally similar to the first pass, but with a key difference: we no longer compute inner products or norms. We already know the coefficients, so the step becomes pure reconstruction.&lt;/p&gt;
    &lt;code&gt;fn lanczos_reconstruction_step&amp;lt;T: ComplexField, O: LinOp&amp;lt;T&amp;gt;&amp;gt;(
    operator: &amp;amp;O,
    mut w: MatMut&amp;lt;'_, T&amp;gt;,
    v_curr: MatRef&amp;lt;'_, T&amp;gt;,
    v_prev: MatRef&amp;lt;'_, T&amp;gt;,
    alpha_j: T::Real,
    beta_prev: T::Real,
    stack: &amp;amp;mut MemStack,
) {
    // Apply operator
    operator.apply(w.rb_mut(), v_curr, Par::Seq, stack);

    // Orthogonalize using stored α_j and β_{j-1}
    let beta_prev_scaled = T::from_real_impl(&amp;amp;beta_prev);
    zip!(w.rb_mut(), v_prev).for_each(|unzip!(w_i, v_prev_i)| {
        *w_i = sub(w_i, &amp;amp;mul(&amp;amp;beta_prev_scaled, v_prev_i));
    });

    let alpha_scaled = T::from_real_impl(&amp;amp;alpha_j);
    zip!(w.rb_mut(), v_curr).for_each(|unzip!(w_i, v_curr_i)| {
        *w_i = sub(w_i, &amp;amp;mul(&amp;amp;alpha_scaled, v_curr_i));
    });
}&lt;/code&gt;
    &lt;p&gt;This is cheaper than the first-pass recurrence. We’ve eliminated the inner products that computed and the norm calculation for . What remains is pure orthogonalization and the operator application.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;lanczos_pass_two&lt;/code&gt; implements this reconstruction. We initialize the three work vectors and the solution accumulator:&lt;/p&gt;
    &lt;code&gt;pub fn lanczos_pass_two&amp;lt;T: ComplexField&amp;gt;(
    operator: &amp;amp;impl LinOp&amp;lt;T&amp;gt;,
    b: MatRef&amp;lt;'_, T&amp;gt;,
    decomposition: &amp;amp;LanczosDecomposition&amp;lt;T::Real&amp;gt;,
    y_k: MatRef&amp;lt;'_, T&amp;gt;,
    stack: &amp;amp;mut MemStack,
) -&amp;gt; Result&amp;lt;Mat&amp;lt;T&amp;gt;, LanczosError&amp;gt; {
    let mut v_prev = Mat::&amp;lt;T&amp;gt;::zeros(b.nrows(), 1);
    let inv_norm = T::from_real_impl(&amp;amp;T::Real::recip_impl(&amp;amp;decomposition.b_norm));
    let mut v_curr = b * Scale(inv_norm);  // v_1

    let mut work = Mat::&amp;lt;T&amp;gt;::zeros(b.nrows(), 1);

    // Initialize solution with first component
    let mut x_k = &amp;amp;v_curr * Scale(T::copy_impl(&amp;amp;y_k[(0, 0)]));&lt;/code&gt;
    &lt;p&gt;We build the solution incrementally by starting with the first basis vector scaled by its coefficient. The main loop then regenerates each subsequent vector: we regenerate each subsequent basis vector, normalize it using the stored , and immediately accumulate its contribution:&lt;/p&gt;
    &lt;code&gt;for j in 0..decomposition.steps_taken - 1 {
    let alpha_j = T::Real::copy_impl(&amp;amp;decomposition.alphas[j]);
    let beta_j = T::Real::copy_impl(&amp;amp;decomposition.betas[j]);
    let beta_prev = if j == 0 {
        T::Real::zero_impl()
    } else {
        T::Real::copy_impl(&amp;amp;decomposition.betas[j - 1])
    };

    // 1. Regenerate the unnormalized next vector
    lanczos_reconstruction_step(
        operator,
        work.as_mut(),
        v_curr.as_ref(),
        v_prev.as_ref(),
        alpha_j,
        beta_prev,
        stack,
    );

    // 2. Normalize using stored β_j
    let inv_beta = T::from_real_impl(&amp;amp;T::Real::recip_impl(&amp;amp;beta_j));
    zip!(work.as_mut()).for_each(|unzip!(w_i)| {
        *w_i = mul(w_i, &amp;amp;inv_beta);
    });

    // 3. Accumulate: x_k += y_{j+1} * v_{j+1}
    let coeff = T::copy_impl(&amp;amp;y_k[(j + 1, 0)]);
    zip!(x_k.as_mut(), work.as_ref()).for_each(|unzip!(x_i, v_i)| {
        *x_i = add(x_i, &amp;amp;mul(&amp;amp;coeff, v_i));
    });

    // 4. Cycle vectors for the next iteration
    core::mem::swap(&amp;amp;mut v_prev, &amp;amp;mut v_curr);
    core::mem::swap(&amp;amp;mut v_curr, &amp;amp;mut work);
}&lt;/code&gt;
    &lt;p&gt;The accumulation &lt;code&gt;x_k += y_{j+1} * v_{j+1}&lt;/code&gt; is implemented as a fused multiply-add in the &lt;code&gt;zip!&lt;/code&gt; loop. On hardware with FMA support, this becomes a single instruction per element, not three separate operations.&lt;/p&gt;
    &lt;p&gt;Note that we accumulate the solution incrementally. After each iteration, &lt;code&gt;x_k&lt;/code&gt; contains a partial result. We cycle through the same three vectors (&lt;code&gt;v_prev&lt;/code&gt;, &lt;code&gt;v_curr&lt;/code&gt;, &lt;code&gt;work&lt;/code&gt;), keeping the working set small and resident in L1 cache.&lt;/p&gt;
    &lt;p&gt;Compare this to the standard method’s final reconstruction step: . This is a dense matrix-vector product where is . When and are both large, this matrix no longer fits in cache. The CPU must stream it from main memory, paying the cost of memory latency. Each element requires a load, multiply, and accumulate, but the load operations dominate—the CPU stalls waiting for data.&lt;/p&gt;
    &lt;p&gt;In our two-pass reconstruction, the operator &lt;code&gt;$\mathbf{A}$&lt;/code&gt; is applied  times, but against vectors that stay in cache. The memory bandwidth is spent on reading the sparse structure of  and the vector elements, not on scanning a dense  matrix.&lt;/p&gt;
    &lt;p&gt;This is the reason the two-pass method can be faster on real hardware despite performing twice as many matrix-vector products. The cache behavior of the reconstruction phase overwhelms the savings of storing the basis.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Public API&lt;/head&gt;
    &lt;p&gt;We can wrap the two passes into a single entry point:&lt;/p&gt;
    &lt;code&gt;pub fn lanczos_two_pass&amp;lt;T, O, F&amp;gt;(
    operator: &amp;amp;O,
    b: MatRef&amp;lt;'_, T&amp;gt;,
    k: usize,
    stack: &amp;amp;mut MemStack,
    mut f_tk_solver: F,
) -&amp;gt; Result&amp;lt;Mat&amp;lt;T&amp;gt;, LanczosError&amp;gt;
where
    T: ComplexField,
    O: LinOp&amp;lt;T&amp;gt;,
    F: FnMut(&amp;amp;[T::Real], &amp;amp;[T::Real]) -&amp;gt; Result&amp;lt;Mat&amp;lt;T&amp;gt;, anyhow::Error&amp;gt;,
{
    // First pass: compute T_k coefficients
    let decomposition = lanczos_pass_one(operator, b, k, stack)?;

    if decomposition.steps_taken == 0 {
        return Ok(Mat::zeros(b.nrows(), 1));
    }

    // Solve projected problem: y_k' = f(T_k) * e_1
    let y_k_prime = f_tk_solver(&amp;amp;decomposition.alphas, &amp;amp;decomposition.betas)?;

    // Scale by ||b||
    let y_k = &amp;amp;y_k_prime * Scale(T::from_real_impl(&amp;amp;decomposition.b_norm));

    // Second pass: reconstruct solution
    lanczos_pass_two(operator, b, &amp;amp;decomposition, y_k.as_ref(), stack)
}&lt;/code&gt;
    &lt;p&gt;The design separates concerns. The &lt;code&gt;f_tk_solver&lt;/code&gt; closure is where we inject the specific matrix function. We compute the Lanczos decomposition, then pass the coefficients to the user-provided solver, which computes  for whatever function  is needed. This decoupling means we handle linear solves, matrix exponentials, or any other function without modifying the core algorithm.&lt;/p&gt;
    &lt;p&gt;The caller provides &lt;code&gt;f_tk_solver&lt;/code&gt; as a closure. It receives the raw  arrays and must return the coefficient vector . We then scale it by  and pass everything to the second pass.&lt;/p&gt;
    &lt;head rend="h3"&gt;Example: Solving a Linear System&lt;/head&gt;
    &lt;p&gt;To see this in practice, consider solving . We compute , which means the &lt;code&gt;f_tk_solver&lt;/code&gt; must solve the small tridiagonal system .&lt;/p&gt;
    &lt;p&gt;Since is tridiagonal, we can exploit its structure. A sparse LU factorization solves it in time instead of the cost of a dense method.&lt;/p&gt;
    &lt;code&gt;let f_tk_solver = |alphas: &amp;amp;[f64], betas: &amp;amp;[f64]| -&amp;gt; Result&amp;lt;Mat&amp;lt;f64&amp;gt;, anyhow::Error&amp;gt; {
    let steps = alphas.len();
    if steps == 0 {
        return Ok(Mat::zeros(0, 1));
    }

    // 1. Assemble T_k from coefficients using triplet format
    let mut triplets = Vec::with_capacity(3 * steps - 2);
    for (i, &amp;amp;alpha) in alphas.iter().enumerate() {
        triplets.push(Triplet { row: i, col: i, val: alpha });
    }
    for (i, &amp;amp;beta) in betas.iter().enumerate() {
        triplets.push(Triplet { row: i, col: i + 1, val: beta });
        triplets.push(Triplet { row: i + 1, col: i, val: beta });
    }
    let t_k_sparse = SparseColMat::try_new_from_triplets(steps, steps, &amp;amp;triplets)?;

    // 2. Construct e_1
    let mut e1 = Mat::zeros(steps, 1);
    e1.as_mut()[(0, 0)] = 1.0;

    // 3. Solve T_k * y' = e_1 via sparse LU
    Ok(t_k_sparse.as_ref().sp_lu()?.solve(e1.as_ref()))
};&lt;/code&gt;
    &lt;p&gt;The closure takes the coefficient arrays, constructs the sparse tridiagonal matrix, and solves the system. The triplet format lets us build the matrix efficiently without knowing its structure in advance. The sparse LU solver leverages the tridiagonal structure to avoid dense factorization.&lt;/p&gt;
    &lt;head rend="h1"&gt;Some interesting results&lt;/head&gt;
    &lt;p&gt;Now that we have a working implementation we can run some tests. The core idea of what we have done is simple: trade flops for better memory access. But does this trade actually pay off on real hardware? To find out, we need a reliable way to benchmark it.&lt;/p&gt;
    &lt;p&gt;For the data, we know that the performance of any Krylov method is tied to the operator’s spectral properties. We need a way to generate a family of test problems where we can precisely control the size, sparsity, and numerical difficulty. A great way to do this is with Karush-Kuhn-Tucker (KKT) systems, which are sparse, symmetric, and have a specific block structure.&lt;/p&gt;
    &lt;p&gt;This structure gives us two critical knobs to turn. First, with the netgen utility, we can control the matrix, which lets us dial in the problem dimension, . Second, we build the diagonal block D with random entries from a range . This parameter, , gives us direct control over the numerical difficulty of the problem.&lt;/p&gt;
    &lt;p&gt;For a symmetric matrix like , the 2-norm condition number, , is the ratio of its largest to its smallest eigenvalue: . Since is diagonal, its eigenvalues are simply its diagonal entries. We are drawing these entries from a uniform distribution , so we have and . This means we get direct control, as .The spectral properties of this block heavily influence the spectrum of the entire matrix . A large condition number in leads to a more ill-conditioned system for . The convergence rate of Krylov methods like Lanczos is fundamentally governed by the distribution of the operator’s eigenvalues. An ill-conditioned matrix, with a wide spread of eigenvalues, will require more iterations, , to reach the desired accuracy. By simply adjusting the parameter, we can generate everything from well-conditioned problems that converge quickly to ill-conditioned ones that force us to run a large number of iterations. This is exactly what we need to rigorously test our implementation.&lt;/p&gt;
    &lt;head rend="h2"&gt;Memory and Computation Trade-off&lt;/head&gt;
    &lt;p&gt;We measure the algorithm against two hypotheses on a large sparse problem with , varying the number of iterations .&lt;/p&gt;
    &lt;p&gt;Hypothesis 1 (Memory): The one-pass method stores the full basis with complexity . We expect its memory to grow linearly with . The two-pass method operates with memory, so it should have a flat profile.&lt;/p&gt;
    &lt;p&gt;Hypothesis 2 (Runtime): The two-pass method performs matrix-vector products instead of . If all else were equal, we’d expect it to run twice as slow.&lt;/p&gt;
    &lt;head rend="h3"&gt;Memory Usage&lt;/head&gt;
    &lt;p&gt;The memory data confirms Hypothesis 1 exactly. The one-pass method’s footprint scales as a straight line—each additional iteration adds one vector to the basis. The two-pass method remains flat. No allocation growth happens after initialization.&lt;/p&gt;
    &lt;head rend="h3"&gt;Runtime: Where Theory Breaks&lt;/head&gt;
    &lt;p&gt;The runtime data contradicts Hypothesis 2. The two-pass method is slower, but never by a factor of two. For small , the gap is minimal. As grows, the two-pass runtime diverges slowly from the one-pass method, not by doubling, but by a much smaller margin.&lt;/p&gt;
    &lt;p&gt;This difference comes from memory access patterns. Both methods perform matrix-vector products, but they differ in how they reconstruct the solution.&lt;/p&gt;
    &lt;p&gt;The one-pass method computes in a single dense matrix-vector product. When and are large, the basis matrix exceeds all cache levels. The CPU cannot keep the data resident; instead, it streams from main memory. This is a memory-bandwidth-bound operation. The processor stalls, waiting for each load to complete. Instruction-level parallelism collapses.&lt;/p&gt;
    &lt;p&gt;The two-pass method reconstructs the solution incrementally. At each iteration, it operates on exactly three n-dimensional vectors: , , and . This working set fits in L1 cache. The processor performs matrix-vector products (each one reading the sparse operator, then applying it to a cached vector), but the solution accumulation happens entirely within cache. The additional matrix-vector products are cheaper than the memory latency of the standard method.&lt;/p&gt;
    &lt;p&gt;The cost of re-computing basis vectors is less than the latency cost of scanning an dense matrix from main memory.&lt;/p&gt;
    &lt;head rend="h3"&gt;Medium-Scale Behavior&lt;/head&gt;
    &lt;p&gt;At we can observe an equilibrium. The two methods have nearly identical runtime. The standard method’s matrix is smaller; it fits partially in cache. The cache-miss penalty here becomes manageable. The two-pass method still has the advantage of cache-local accumulation, but the difference is marginal.&lt;/p&gt;
    &lt;head rend="h3"&gt;What About Dense Matrices?&lt;/head&gt;
    &lt;p&gt;To be sure of our hypothesis, we can test it directly using a dense matrix of size . For dense problems, the matrix-vector product is , it dominates all other costs. Memory latency will become negligible relative to the compute work and the cache efficiency advantage should disappear.&lt;/p&gt;
    &lt;p&gt;We can see that the two-pass method runs almost exactly twice as slow as the one-pass method. The slope ratio is exactly 2:1. In a compute-bound regime, the extra matrix-vector products cannot be hidden by cache effects. Here, the theoretical trade-off holds perfectly.&lt;/p&gt;
    &lt;head rend="h2"&gt;Scalability&lt;/head&gt;
    &lt;p&gt;Now, let’s fix the iteration count at and vary from to to measure scalability. Based on what we have seen before, we would expect the two-pass memory to scale linearly with but with a small constant factor (three vectors, plus scalars). The one-pass method should also scale linearly, but with a -dependent slope.&lt;/p&gt;
    &lt;p&gt;Here we have to use a logarithmic y-axis to show both curves; the two-pass line is so flat relative to the one-pass line that it’s otherwise invisible.&lt;/p&gt;
    &lt;p&gt;Runtime scales linearly with for both methods, as expected. Below , the two methods have similar performance. This is the regime where both basis and working set fit in cache, or where the problem is small enough that memory latency is not the bottleneck.&lt;/p&gt;
    &lt;p&gt;As increases beyond , the matrix-vector product time dominates. The sparse structure of ensures that each matvec requires multiple memory accesses per element. For the one-pass method, the final reconstruction of begins to cost more as the matrix grows. For the two-pass method, performing matrix-vector products means the matvec cost accumulates more rapidly. The divergence is gradual, not sharp, because the advantage of cache locality in accumulation persists—but it cannot overcome the fundamental cost of doubling the number of expensive operations.&lt;/p&gt;
    &lt;p&gt;Well, that’s it. If you want to have a better look at the code or use it, it’s all open source:&lt;/p&gt;
    &lt;p&gt;This was more of an exploration than a production-ready library, so expect rough edges. But I hope it gives an interesting perspective on how algorithm engineering and low-level implementation details can alter what seems like a straightforward trade-off on a blackboard.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45889891</guid><pubDate>Tue, 11 Nov 2025 17:08:00 +0000</pubDate></item><item><title>We ran over 600 image generations to compare AI image models</title><link>https://latenitesoft.com/blog/evaluating-frontier-ai-image-generation-models/</link><description>&lt;doc fingerprint="371511f4993ed21d"&gt;
  &lt;main&gt;
    &lt;p&gt;tl:dr; We’ve been making photo apps for iOS for long enough that we have gray hairs now, and using our experience we ran over 600 image generations to compare which AI models work best for which image edits. If you want, you can jump right to the image comparisons, or the conclusion, but promise us you won’t presumptuous comments on Hacker News until you’ve also read the background!&lt;/p&gt;
    &lt;head rend="h2"&gt;Background&lt;/head&gt;
    &lt;p&gt;Hi! We’re LateNiteSoft, and we’ve been working on photography-related iOS apps for 15 years now. Working on market-leading apps such as Camera+, Photon and REC, we’ve always had our finger on the pulse on what users want out of their mobile photography.&lt;/p&gt;
    &lt;p&gt;With the ground-breaking release of OpenAI’s gpt-image-1 image generation model earlier this year, we started investigating all the interesting use cases we could think of for AI image editing.&lt;/p&gt;
    &lt;p&gt;But as a company that has never taken any venture capital investment, all our products have to pay for themselves. We’re in it to delight our users, not just capture market share and sell them out. When considering AI projects, one thing has been clear – we can’t take the AI startup road where you have a generous free tier, charge an unreasonably small monthly fee for “unlimited”, and hope you’re going to make it up on scale (code for “someone please acquire us”).&lt;/p&gt;
    &lt;p&gt;All the AI-focused billing systems we could find out there were based on this. Assuming you want to claim unlimited access, and then sandbag users with “fair use” clauses and prevent them from any actual unlimited usage (which is, obviously, untenable, since you’ll end up with one $20/mo user reselling to everyone else).&lt;/p&gt;
    &lt;p&gt;Since we want to fairly charge our customers for what they actually use, we’ve built a credit-based “pay per generation”-style billing system (that internally we’ve been calling CreditProxy). We’ve also been planning on providing this as a service, since nobody else seems to be doing it, so if you’re interested in being a trial user, get in touch!&lt;/p&gt;
    &lt;p&gt;We released our app MorphAI as a public proof of concept to give CreditProxy a proper real world-test, and have marketed it to the users of Camera+, which includes traditional photo-editing functionality, including a whole host of popular photo filters, giving us a built-in audience of customers ready for the next step in image editing.&lt;/p&gt;
    &lt;p&gt;With the release of newer models like nanoBanana and Seedream, we’ve had to consider which models make sense to support. We need to explore the trade-offs between quality, prompt complexity, and pricing.&lt;/p&gt;
    &lt;p&gt;A couple of hastily-hacked together scripts, and many, many AI generation credits later, we have some results! So that everyone else also doesn’t have to waste their money, we figured we’d share what we found:&lt;/p&gt;
    &lt;head rend="h2"&gt;The Tests&lt;/head&gt;
    &lt;p&gt;Based on our experience with Camera+ and the kind of edits our users have been making with MorphAI, we picked a host of somewhat naive prompts. Veteran Midjourney users may scoff at these, but in our experience these are the kinds of prompts that our average user is likely to use.&lt;/p&gt;
    &lt;p&gt;As for test photos, we chose some some representative things people like to take photos of: their pets, their kids, landscapes, their cars, and product photography.&lt;/p&gt;
    &lt;p&gt;Image generation times are also relevant. During our test period, the generation time for all models was fairly consistent, and didn’t vary by image or prompt complexity.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;OpenAI (High)&lt;/cell&gt;
        &lt;cell&gt;Gemini&lt;/cell&gt;
        &lt;cell&gt;Seedream&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;80 seconds&lt;/cell&gt;
        &lt;cell&gt;11 seconds&lt;/cell&gt;
        &lt;cell&gt;9 seconds&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;OpenAI also has a quality setting, the images included here were all generated on High quality, but we also tested Medium, and those generations averaged 36 seconds. We can include the Medium quality images as well if there is any interest!&lt;/p&gt;
    &lt;p&gt;There are a ton of photos to compare here, so to make things easier to flip through, here are some keyboard shortcuts to help you out: Click on a photo to see it larger. Now you can use the arrow keys to switch between models. Press the tab key to switch between test images. Hit ESC to leave the view.&lt;/p&gt;
    &lt;head rend="h2"&gt;Classic filters&lt;/head&gt;
    &lt;p&gt;These are the types of filters that we used to implement manually, by painstakingly hand-crafting textures and Photoshop layers and then converting those to Objective-C code. Now all you need is a few words into a language model (and to burn down half of a rain forest or so; just the cost of progress).&lt;/p&gt;
    &lt;p&gt;Our conclusion for this category is that for photo realistic filters like this, Gemini really shines by preserving details from the original and minimizing hallucinations, but often at the expense of the strength and creativity of the effect. Especially with photos of people, Gemini seems to refuse to apply any edits at all, with a strong bias towards photo realism.&lt;/p&gt;
    &lt;p&gt;OpenAI really likes to mess with the details of the photo, giving a characteristic “AI slop” feel, which can be a deal breaker on things like human faces.&lt;/p&gt;
    &lt;head rend="h3"&gt;Grungy vintage photo&lt;/head&gt;
    &lt;head rend="h3"&gt;Use soft, diffused lighting&lt;/head&gt;
    &lt;head rend="h3"&gt;Transform into a kaleidoscopic pattern&lt;/head&gt;
    &lt;p&gt;Gemini took some really odd shortcuts in generating some of these!&lt;/p&gt;
    &lt;head rend="h3"&gt;Apply a heat map effect&lt;/head&gt;
    &lt;p&gt;It’s clear that none of the models actually have a concept of what generates heat here, aside from Seedream knowing that humans generate heat, clearly revealing that without any ground truth the models struggle.&lt;/p&gt;
    &lt;head rend="h3"&gt;Make it look like a long exposure photograph&lt;/head&gt;
    &lt;p&gt;This is an interesting test since in some of the sample photos a long exposure doesn’t make sense. In the ones where it makes the most sense – the landscape and the car, OpenAI did the best, but on the other hand it completely messed up the cats and the product, and the portrait photo turned into a trippy art piece.&lt;/p&gt;
    &lt;p&gt;Gemini, maybe logically, did nothing. Seedream liked adding light streaks as if a car drove past, with only the portrait photo seemingly making any sense.&lt;/p&gt;
    &lt;head rend="h3"&gt;Pinhole camera&lt;/head&gt;
    &lt;p&gt;In this case, it was funny to watch Gemini take a literal approach and generate actual pictures of cameras! For this reason we re-worked this prompt by just adding the word “effect”.&lt;/p&gt;
    &lt;head rend="h3"&gt;Pinhole camera effect&lt;/head&gt;
    &lt;p&gt;Gemini liked to generate a literal pinhole camera here so we tried modifying the prompt.&lt;/p&gt;
    &lt;head rend="h3"&gt;Add a layer of fog or mist&lt;/head&gt;
    &lt;head rend="h3"&gt;Make it look like it’s golden hour&lt;/head&gt;
    &lt;head rend="h3"&gt;Make it look like it’s etched in glass&lt;/head&gt;
    &lt;p&gt;With this prompt, there is ambiguity in what “it” is, so we tried a reworded prompt as well. Only OpenAI consistently knew what a traditional etched glass effect looks like. Seedream’s glass item effect looks really cool!&lt;/p&gt;
    &lt;head rend="h3"&gt;Make it look like the photo is etched in glass&lt;/head&gt;
    &lt;p&gt;Gemini has a really interesting interpretation here! And Seedream had some pretty fantastic results.&lt;/p&gt;
    &lt;head rend="h3"&gt;Remove background&lt;/head&gt;
    &lt;p&gt;This is a classic job people have spent their lives doing manually in Photoshop since the early 90’s. But what is a “background”, really? Is the ground in front of a car the “background”? We also retried this with a tweaked prompt.&lt;/p&gt;
    &lt;p&gt;OpenAI’s “sloppification” of the details of objects makes it useless for this purpose.&lt;/p&gt;
    &lt;head rend="h3"&gt;Isolate the object&lt;/head&gt;
    &lt;p&gt;With the tweaked prompt, Gemini’s API actually returned a followup question: “Which object would you like to isolate? There are two cats in the image.”, which our generation script was not prepared to handle! So it is missing from this comparison.&lt;/p&gt;
    &lt;head rend="h3"&gt;Give it a metallic sheen&lt;/head&gt;
    &lt;p&gt;Another case where “it” is vague and we can retry with a more specific prompt. The product imagery is another case where Seedream created a really stunning result, even adding a reflection of someone taking the photo with their phone!&lt;/p&gt;
    &lt;head rend="h3"&gt;Give the object a metallic sheen&lt;/head&gt;
    &lt;p&gt;Modifying the prompt here really only changed OpenAI’s interpretation.&lt;/p&gt;
    &lt;head rend="h2"&gt;Lens effects&lt;/head&gt;
    &lt;p&gt;One of the filter packs we had worked on for Camera+ using traditional methods was a lens effect filter pack. But unlike traditional edits, with generative AI you can also create wide-angle lens effects that can just make up the portions of the image that the camera couldn’t capture.&lt;/p&gt;
    &lt;p&gt;This is another category where it’s very visible how OpenAI regenerates and hallucinates all the details in a picture, where Gemini and Seedream’s results are very faithful to the original and look more like actual lens permutations.&lt;/p&gt;
    &lt;head rend="h3"&gt;Apply a fish-eye lens effect&lt;/head&gt;
    &lt;head rend="h3"&gt;Strong bokeh blur&lt;/head&gt;
    &lt;p&gt;It was pretty surprising how poorly the models did here considering how common this must be among the training data. OpenAI give a strong blur but no bokeh effects. Gemini gives us a bunch of random circles in front of the image, demonstrating an understanding of what people want out of a bokeh filter but not how it works. Seedream does really well here.&lt;/p&gt;
    &lt;head rend="h3"&gt;Apply a Dutch angle (canted frame)&lt;/head&gt;
    &lt;p&gt;OpenAI really lost it’s mind here on the car photo.&lt;/p&gt;
    &lt;head rend="h3"&gt;Change to a bird’s-eye view&lt;/head&gt;
    &lt;head rend="h2"&gt;Style transfer&lt;/head&gt;
    &lt;p&gt;Style Transfer is the process of applying an artistic style to a photo. This technique predates the current AI model by quite a few years with popular apps generating Van Gogh paintings out of your photos. We were also early out in attempting style transfer for our apps, shout out to Noel’s Intel iMac which had to run at full blast all night just to generate a 256x256px image, since it was our only machine with a compatible GPU.&lt;/p&gt;
    &lt;p&gt;While Gemini was good at preserving reality in the more photorealistic effects in the previous section, when it comes to the more artistic styles, OpenAI has them beat, while Gemini keeps things far too conservative, especially with photos of a human in them, where it sometimes seems to just do nothing at all, is this some kind of safety guardrail?&lt;/p&gt;
    &lt;head rend="h3"&gt;Draw this in the style of a Studio Ghibli movie&lt;/head&gt;
    &lt;p&gt;ChatGPT went viral with this prompt, with Sam Altman even making it his profile on X. And OpenAI keeps the crown – is Google too conservative in order to avoid a lawsuit? Seedream makes an attempt but they just end up looking like “generic Anime”.&lt;/p&gt;
    &lt;head rend="h3"&gt;Transform into watercolor painting&lt;/head&gt;
    &lt;head rend="h3"&gt;Make it look like a pastel artwork&lt;/head&gt;
    &lt;head rend="h3"&gt;Transform into Art Nouveau style&lt;/head&gt;
    &lt;head rend="h3"&gt;Apply a ukiyo-e Japanese woodblock print style&lt;/head&gt;
    &lt;p&gt;A very stark example of Gemini failing to apply a style on photos with humans. This is a prompt where Seedream knocked it out of the park, perhaps showing a larger portion of their training data being sourced from asian cultures than the western models.&lt;/p&gt;
    &lt;head rend="h3"&gt;Transform into low poly art&lt;/head&gt;
    &lt;p&gt;Seedream blows everyone else away here.&lt;/p&gt;
    &lt;head rend="h2"&gt;Portrait effects&lt;/head&gt;
    &lt;p&gt;For prompts about human appearance, we have only applied them to the portrait photo.&lt;/p&gt;
    &lt;head rend="h3"&gt;Make it look like a caricature&lt;/head&gt;
    &lt;p&gt;Seedream seems to be biased towards asian culture, giving an anime look instead of a western-style cartoon caricature.&lt;/p&gt;
    &lt;head rend="h3"&gt;Turn them into an action figure in the blister pack&lt;/head&gt;
    &lt;p&gt;OpenAI’s style here went viral a while back, but Gemini is stunningly realistic. Seedream is a weird mix of realistic and hallucinations.&lt;/p&gt;
    &lt;head rend="h2"&gt;Generative edits&lt;/head&gt;
    &lt;p&gt;The place where generative AI really shines is when it can show off some creativity, and these were some prompts we added as suggestions in MorphAI to showcase that and inspire our users. OpenAI still seems to win here.&lt;/p&gt;
    &lt;head rend="h3"&gt;Create a 70’s vinyl record cover&lt;/head&gt;
    &lt;p&gt;This is an example of a prompt that has a small viral moment with OpenAI, but the other models can’t even get the aspect ratio right.&lt;/p&gt;
    &lt;head rend="h3"&gt;Introduce mythical creatures native to this environment&lt;/head&gt;
    &lt;p&gt;This one showcases OpenAI’s creativity. Gemini seems kind of creepy?&lt;/p&gt;
    &lt;head rend="h3"&gt;Add a mystical portal or gateway&lt;/head&gt;
    &lt;p&gt;Gemini replacing the face with a portal is certainly a choice!&lt;/p&gt;
    &lt;head rend="h3"&gt;Incorporate futuristic technology elements&lt;/head&gt;
    &lt;p&gt;Another example of OpenAI being far more creative and willing to re-do the whole image.&lt;/p&gt;
    &lt;head rend="h3"&gt;Make it look whimsical and enchanting&lt;/head&gt;
    &lt;p&gt;This one also shows OpenAI being more artistic, and Gemini being more realistic while still trying to incorporate the prompt.&lt;/p&gt;
    &lt;head rend="h3"&gt;Transform the scene to a stormy night&lt;/head&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;If you made it all the way down here you probably don’t need a summary, but for our purposes, we’ve at least concluded that there is no one-size-fits all model at this point.&lt;/p&gt;
    &lt;p&gt;OpenAI is great for fully transformative filters like style transfer or more creative generative applications, whereas Gemini works better for more realistic edits. Seedream lies somewhere in the middle and is a bit of a jack of all trades, and for the price and performance may be a good replacement for OpenAI.&lt;/p&gt;
    &lt;p&gt;We’ve been experimenting on working on a “prompt classifier” to automatically choose a model – sending artistic prompts to OpenAI and more realistic prompts to Gemini, if there’s any interest we can follow up with how that worked out!&lt;/p&gt;
    &lt;head rend="h4"&gt;Methodology&lt;/head&gt;
    &lt;p&gt;Tests were performed on October 8 with &lt;code&gt;gpt-image-1&lt;/code&gt;, &lt;code&gt;gemini-2.5-flash-image&lt;/code&gt; and &lt;code&gt;seedream-4-0-250828&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Timings were measured on a consumer internet connection in Japan (Fiber connection, 10 Gbps nominal bandwidth) during a limited test run in a short time period.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45890186</guid><pubDate>Tue, 11 Nov 2025 17:26:54 +0000</pubDate></item><item><title>Show HN: Data Formulator – interactive AI agents for data analysis (Microsoft)</title><link>https://data-formulator.ai/</link><description>&lt;doc fingerprint="dbfdf795bc5b90ad"&gt;
  &lt;main&gt;
    &lt;p&gt;Run this app with javascript&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45890394</guid><pubDate>Tue, 11 Nov 2025 17:44:21 +0000</pubDate></item><item><title>Terminal Latency on Windows (2024)</title><link>https://chadaustin.me/2024/02/windows-terminal-latency/</link><description>&lt;doc fingerprint="140ed15eb7063ff7"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Terminal Latency on Windows&lt;/head&gt;
    &lt;p&gt;UPDATE 2024-04-15: Windows Terminal 1.19 contains a fix that reduces latency by half! Itâs now competitive with WSLtty on my machine. Details in the GitHub Issue.&lt;/p&gt;
    &lt;p&gt;In 2009, I wrote about why MinTTY is the best terminal on Windows. Even today, that post is one of my most popular.&lt;/p&gt;
    &lt;p&gt;Since then, the terminal situation on Windows has improved:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Cygwin defaults to MinTTY; you no longer need to manually install it.&lt;/item&gt;
      &lt;item&gt;Windows added PTY support, obviating the need for offscreen console window hacks that add latency.&lt;/item&gt;
      &lt;item&gt;Windows added basically full support for ANSI terminal sequences in both the legacy conhost.exe consoles and its new Windows Terminal.&lt;/item&gt;
      &lt;item&gt;We now have a variety of terminals to choose from, even on Windows: Cmder, ConEmu, Alacritty, WezTerm, xterm.js (component of Visual Studio Code)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The beginning of a year is a great time to look at your tools and improve your environment.&lt;/p&gt;
    &lt;p&gt;Iâd already enabled 24-bit color in all of my environments and streamlined my tmux config. Itâs about time that I take a look at the newer terminals.&lt;/p&gt;
    &lt;p&gt;Roughly in order, I care about:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Minimum feature set: 24-bit color, reasonable default fonts with emoji support, italics are nice.&lt;/item&gt;
      &lt;item&gt;Input latency.&lt;/item&gt;
      &lt;item&gt;Throughput at line rate, for example, when I &lt;code&gt;cat&lt;/code&gt;a large file.&lt;/item&gt;
      &lt;item&gt;Support for multiple tabs in one window would be nice, but tmux suffices for me.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Which terminals should I test?&lt;/head&gt;
    &lt;p&gt;I considered the following.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Legacy conhost.exe (also known as Windows Console), Windows 10 19045&lt;/item&gt;
      &lt;item&gt;MinTTY (3.7.0)&lt;/item&gt;
      &lt;item&gt;Alacritty (0.13.1)&lt;/item&gt;
      &lt;item&gt;WezTerm (20240203-110809-5046fc22)&lt;/item&gt;
      &lt;item&gt;Windows Terminal (1.18.10301.0)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Testing Features&lt;/head&gt;
    &lt;p&gt;Testing color and italics support is easy with my colortest.rs script. To test basic emoji, you can cat the Unicode emoji 1.0 emoji-data.txt. To test more advanced support, try the zero-width joiner list in the latest/ directory.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Terminal&lt;/cell&gt;
        &lt;cell role="head"&gt;Emoji&lt;/cell&gt;
        &lt;cell role="head"&gt;Font Attributes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;conhost.exe&lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;No italics&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;MinTTY&lt;/cell&gt;
        &lt;cell&gt;Black and white&lt;/cell&gt;
        &lt;cell&gt;All major attributes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Alacritty&lt;/cell&gt;
        &lt;cell&gt;Black and white&lt;/cell&gt;
        &lt;cell&gt;Everything but double underline&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;WezTerm&lt;/cell&gt;
        &lt;cell&gt;Color&lt;/cell&gt;
        &lt;cell&gt;All major attributes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Windows Terminal&lt;/cell&gt;
        &lt;cell&gt;Color&lt;/cell&gt;
        &lt;cell&gt;All major attributes&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Everything but conhost.exe meets my bar.&lt;/p&gt;
    &lt;p&gt;Itâs also worth noting that conhost.exe has a terrible default palette. The default yellow is a pukey green and dark blue is barely visible. You can change palettes, but defaults matter.&lt;/p&gt;
    &lt;head rend="h2"&gt;Latency&lt;/head&gt;
    &lt;p&gt;I set up two latency tests. One with an 80x50 blank window in the upper left corner of the screen. The other fullscreen, editing an Emacs command at the bottom of the screen.&lt;/p&gt;
    &lt;p&gt;Since latencies are additive, system configuration doesnât matter as much as the absolute milliseconds of latency each terminal adds, but Iâll describe my entire setup and include total keypress-to-pixels latency.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Windows 10&lt;/item&gt;
      &lt;item&gt;Intel i7-4771 @ 3.5 GHz&lt;/item&gt;
      &lt;item&gt;NVIDIA GTX 1060&lt;/item&gt;
      &lt;item&gt;Keyboard: Sweet 16 Macro Pad&lt;/item&gt;
      &lt;item&gt;Display: LG 27GP950-B at 4K, 120 Hz, adaptive sync&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Measurement Methodology&lt;/head&gt;
    &lt;p&gt;With Is It Snappy?, I measured the number of frames between pressing a key and pixels changing on the screen.&lt;/p&gt;
    &lt;p&gt;To minimize ambiguity about when the key was pressed, I slammed a pencilâs eraser into the key, and always measured the key press as the second frame after contact. (The first frame was usually when the eraser barely touched the key. It would usually clear the activation depth by the second frame.)&lt;/p&gt;
    &lt;p&gt;I considered the latency to end when pixels just started to change on the screen. In practice, pixels take several 240 Hz frames to transition from black to white, but I consistently marked the beginning of that transition.&lt;/p&gt;
    &lt;p&gt;I took five measurements for each configuration and picked the median. Each measurement was relatively consistent, so average would have been a fine metric too. It doesnât change the results below.&lt;/p&gt;
    &lt;head rend="h3"&gt;80x50&lt;/head&gt;
    &lt;p&gt;80x50 window, upper left of screen, cleared terminal, single keypress.&lt;/p&gt;
    &lt;p&gt;Confirmed window size with:&lt;/p&gt;
    &lt;code&gt;$ echo $(tput cols)x$(tput lines)
80x50
&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Terminal&lt;/cell&gt;
        &lt;cell role="head"&gt;Median Latency (ms)&lt;/cell&gt;
        &lt;cell role="head"&gt;240 Hz Camera Frames&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;conhost.exe WSL1&lt;/cell&gt;
        &lt;cell&gt;33.3&lt;/cell&gt;
        &lt;cell&gt;8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;MinTTY WSL1&lt;/cell&gt;
        &lt;cell&gt;33.3&lt;/cell&gt;
        &lt;cell&gt;8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;conhost.exe Cygwin&lt;/cell&gt;
        &lt;cell&gt;41.3&lt;/cell&gt;
        &lt;cell&gt;10&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;MinTTY Cygwin&lt;/cell&gt;
        &lt;cell&gt;57.9&lt;/cell&gt;
        &lt;cell&gt;14&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;WezTerm cmd.exe&lt;/cell&gt;
        &lt;cell&gt;62.5&lt;/cell&gt;
        &lt;cell&gt;15&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Alacritty WSL1&lt;/cell&gt;
        &lt;cell&gt;62.5&lt;/cell&gt;
        &lt;cell&gt;15&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;WezTerm WSL1&lt;/cell&gt;
        &lt;cell&gt;66.7&lt;/cell&gt;
        &lt;cell&gt;16&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Windows Terminal WSL1&lt;/cell&gt;
        &lt;cell&gt;66.7&lt;/cell&gt;
        &lt;cell&gt;16&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Fullscreen&lt;/head&gt;
    &lt;p&gt;Maximized emacs, editing a command in the bottom row of the terminal. I only tested WSL1 this time.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Terminal&lt;/cell&gt;
        &lt;cell role="head"&gt;Median Latency (ms)&lt;/cell&gt;
        &lt;cell role="head"&gt;240 Hz Camera Frames&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;conhost.exe&lt;/cell&gt;
        &lt;cell&gt;45.8&lt;/cell&gt;
        &lt;cell&gt;11&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;MinTTY&lt;/cell&gt;
        &lt;cell&gt;52.42&lt;/cell&gt;
        &lt;cell&gt;12&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;WezTerm&lt;/cell&gt;
        &lt;cell&gt;75&lt;/cell&gt;
        &lt;cell&gt;18&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Windows Terminal&lt;/cell&gt;
        &lt;cell&gt;75&lt;/cell&gt;
        &lt;cell&gt;18&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Alacritty&lt;/cell&gt;
        &lt;cell&gt;87.5&lt;/cell&gt;
        &lt;cell&gt;21&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Throughput&lt;/head&gt;
    &lt;p&gt;I generated a 100,000-line file with:&lt;/p&gt;
    &lt;code&gt;$ yes "This sentence has forty-five (45) characters." | head -n 100000 &amp;gt; /tmp/lines.txt
&lt;/code&gt;
    &lt;p&gt;Then I measured the wall-clock duration of:&lt;/p&gt;
    &lt;code&gt;$ time cat /tmp/lines.txt
&lt;/code&gt;
    &lt;p&gt;This benchmark captures the case that I accidentally dump a ton of output and Iâm sitting there just waiting for the terminal to become responsive again. I have a gigabit internet connection, and itâs embarrassing to be CPU-bound instead of IO-bound.&lt;/p&gt;
    &lt;p&gt;I did include Cygwin in this test, just to have two different MinTTY datapoints.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Terminal&lt;/cell&gt;
        &lt;cell role="head"&gt;Elapsed Time (s)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;MinTTY WSL1&lt;/cell&gt;
        &lt;cell&gt;0.57&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;MinTTY Cygwin&lt;/cell&gt;
        &lt;cell&gt;2.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Windows Terminal&lt;/cell&gt;
        &lt;cell&gt;5.25&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Alacritty&lt;/cell&gt;
        &lt;cell&gt;5.75&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;WezTerm&lt;/cell&gt;
        &lt;cell&gt;6.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;conhost.exe&lt;/cell&gt;
        &lt;cell&gt;21.8&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;I assume this means MinTTY throttles display updates in some way. Of course this is totally fine, because you couldnât read the output either way.&lt;/p&gt;
    &lt;p&gt;To test the hypothesis that MinTTY was caching cell rendering by their contents, I also tried generating a file that rotated through different lines, with no effect.&lt;/p&gt;
    &lt;code&gt;with open("/tmp/lines2.txt", "w") as f:
  for i in range(100000):
    sentence="This sentence has forty-five (45) characters."
    print(sentence[i%len(sentence):]+sentence[:i%len(sentence)], file=f)
&lt;/code&gt;
    &lt;head rend="h3"&gt;CPU Usage During Repeated Keypresses&lt;/head&gt;
    &lt;p&gt;While making these measurements, I noticed some strange behaviors. My monitor runs at 120 Hz and animation and window dragging are generally smooth. But right after you start Alacritty, dragging the window animates at something like 30-60 frames per second. Itâs noticeably chunkier. WezTerm does the same, but slightly worse. Maybe 20 frames per second.&lt;/p&gt;
    &lt;p&gt;I donât know if I can blame the terminals themselves, because I sometimes experience this even with Notepad.exe too. But the choppiness stands out much more. Maybe something is CPU-bound in responding to window events?&lt;/p&gt;
    &lt;p&gt;This made me think of a new test: if I open a terminal and hold down the âaâ button on autorepeat, how much CPU does the terminal consume?&lt;/p&gt;
    &lt;p&gt;To measure this, I set the terminal processâs affinity to my third physical core, and watched the CPU usage graph in Task Manager. Not a great methodology, but it gave a rough sense. Again, 80x50.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Terminal&lt;/cell&gt;
        &lt;cell role="head"&gt;Percent of Core&lt;/cell&gt;
        &lt;cell role="head"&gt;Private Bytes After Startup (KiB)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;conhost&lt;/cell&gt;
        &lt;cell&gt;0%&lt;/cell&gt;
        &lt;cell&gt;6,500&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Alacritty&lt;/cell&gt;
        &lt;cell&gt;5%&lt;/cell&gt;
        &lt;cell&gt;74,000&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;MinTTY WSL1&lt;/cell&gt;
        &lt;cell&gt;10%&lt;/cell&gt;
        &lt;cell&gt;10,200&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;MinTTY Cygwin&lt;/cell&gt;
        &lt;cell&gt;10%&lt;/cell&gt;
        &lt;cell&gt;10,500&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Windows Terminal&lt;/cell&gt;
        &lt;cell&gt;20%&lt;/cell&gt;
        &lt;cell&gt;73,700&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;WezTerm&lt;/cell&gt;
        &lt;cell&gt;85%&lt;/cell&gt;
        &lt;cell&gt;134,000&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The WezTerm CPU usage has to be a bug. Iâll report it.&lt;/p&gt;
    &lt;head rend="h3"&gt;CPU Usage (Idle)&lt;/head&gt;
    &lt;p&gt;I often have a pile of idle terminals sitting around. I donât want them to chew battery life. So letâs take a look at CPU Cycles Delta (courtesy of Process Explorer) with a fresh, idle WSL session.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Terminal&lt;/cell&gt;
        &lt;cell role="head"&gt;Idle Cycles/s (Focused)&lt;/cell&gt;
        &lt;cell role="head"&gt;Idle Cycles/s (Background)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;conhost&lt;/cell&gt;
        &lt;cell&gt;~900,000&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Alacritty&lt;/cell&gt;
        &lt;cell&gt;~2,400,000&lt;/cell&gt;
        &lt;cell&gt;no difference&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;WezTerm&lt;/cell&gt;
        &lt;cell&gt;~2,600,000&lt;/cell&gt;
        &lt;cell&gt;~1,600,000&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Windows Terminal&lt;/cell&gt;
        &lt;cell&gt;~55,000,000&lt;/cell&gt;
        &lt;cell&gt;~6,100,000&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;MinTTY WSL1&lt;/cell&gt;
        &lt;cell&gt;~120,000,000&lt;/cell&gt;
        &lt;cell&gt;no difference&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;MinTTY Cygwin&lt;/cell&gt;
        &lt;cell&gt;~120,000,000&lt;/cell&gt;
        &lt;cell&gt;no difference&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;These numbers arenât great at all! For perspective, I have a pile of Firefox tabs open, some of them actively running JavaScript, and theyâre âonlyâ using a few hundred million cycles per second.&lt;/p&gt;
    &lt;p&gt;Raymond Chen once wrote a blog post about the importance of properly idling in the Windows Terminal Server days. You might have a dozen users logged into a host, and if a program is actively polling, itâs eating performance that others could use.&lt;/p&gt;
    &lt;p&gt;Today, we often run on batteries, so idling correctly still matters, but it seems to be something of a lost art. The only terminal that idles completely is the old conhost.exe.&lt;/p&gt;
    &lt;p&gt;The other lesson we can draw is that Microsoftâs own replacement for conhost.exe, Windows Terminal, uses over 10x the RAM, 60x the CPU when focused, and infinitely more CPU when idle.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusions&lt;/head&gt;
    &lt;p&gt;conhost.exe consistently has the best latency, with MinTTY not much behind. MinTTY handily dominates the throughput test, supports all major ANSI character attributes, and has a better default palette.&lt;/p&gt;
    &lt;p&gt;As in 2009, Iâd say MinTTY is still pretty great. (I should try to track down that idle CPU consumption. It feels more like a bug than a requirement.)&lt;/p&gt;
    &lt;p&gt;If you want to use MinTTY as the default terminal for WSL, install WSLtty.&lt;/p&gt;
    &lt;p&gt;The others all have slightly worse latencies, but theyâre in a similar class. Iâm particularly sensitive to latency, so Iâd had a suspicion even before measuring. Maybe itâs some consequence of being GPU-accelerated? Out of curiousity, I put Windows Terminal in software-rendered mode, and it shaved perhaps 4 ms off (median of 62.5 ms, 15 frames). Perhaps just measurement noise.&lt;/p&gt;
    &lt;p&gt;While Iâm going to stick with MinTTY, one thing is clear: there is room to improve all of the above.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45890726</guid><pubDate>Tue, 11 Nov 2025 18:07:51 +0000</pubDate></item><item><title>Vertical integration is the only thing that matters</title><link>https://becca.ooo/blog/vertical-integration/</link><description>&lt;doc fingerprint="2edeb01947372560"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Vertical Integration is the Only Thing That Matters&lt;/head&gt;
    &lt;p&gt;On the subject of developer tooling, or perhaps computer programs more broadly, I have become increasingly convinced that vertical integration is the only thing that matters. I also think that the inability of developer productivity startups to vertically integrate their offerings has hindered their adoption and utility. I’d like to talk about what I mean by “vertical integration” and why we don’t have it today.&lt;/p&gt;
    &lt;head rend="h2"&gt;#What is vertical integration?&lt;/head&gt;
    &lt;p&gt;When I say “vertical integration”, I am referring to tight integration between different tools in a stack.&lt;/p&gt;
    &lt;p&gt;Here are some workflows that are possible with a vertically-integrated stack:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;You create a new branch and build the project you’re working on. You haven’t made any changes yet, so the build finishes in under a second because your build system shares artifacts with builds from CI and deploys. This build system also caches artifacts from builds you initiate locally, so a coworker who checks out your branch (also a single click from the PR) will be able to run your code immediately.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;A test fails in CI. You click a button to open the failing test in your editor, which gives you an interactive call-stack (potentially spanning multiple binaries and programming languages) to explore.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;A check on your PR fails because your code wasn’t formatted correctly. You click a button to apply the formatting changes directly to your code, and they’re immediately reflected in your editor.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;You merge a PR which breaks a service. Your PR is initially deployed as a canary to a small portion of clients. The deploy system detects an elevated failure rate, and rolls back your PR automatically. This adds a note to your PR and messages you on your Slack-equivalent. The note indicates which checks failed, and you can click them to see a history of those checks.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;You click on a function in your editor and are presented with a list of usages of that function. The list includes entries from other projects, written in other languages, and across RPC boundaries.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;A service crashes in production. A ticket is automatically opened and routed to your team (or a note is added to an existing ticket). You click a button to open the line of code that crashed in your editor, and again you’re shown an interactive call-stack for the failure.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;All of these workflows span multiple parts of the stack: local builds and CI builds, the test runner and the editor, the code review system and CI, the deploy system and version control. These features are often derisively referred to as “glue code”. The glue code is the vertical integration work.&lt;/p&gt;
    &lt;p&gt;I’d like to draw your attention to a couple different themes here:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;None of the features here are particularly shocking, but they all require cooperation between tools that aren’t used to cooperating. Your test runner knows the call stack of a failing test, but it can’t make that information available in a format your editor or terminal is able to consume. Your deploy system runs an optimized build and then throws away all the artifacts, so if you want to build the same commit you need to start from scratch. The compilation was already run, but your build system isn’t able to grab artifacts from CI because your build system doesn’t know that you have CI.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;IDEs make some of these workflows possible, albeit scoped to a single project. This is because IDEs implement glue integrations between a bunch of different test runners, build systems, languages, and so on.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Many of these workflows require infrastructure to be running independently of your CI, deploy system, and developer workstations. If you want CI to tell you if a test has failed recently in other builds, you need a system that knows when that test was run and what the results were. If you want to share build artifacts between CI and developer builds, you need a system to cache those artifacts and accurately identify when they can be reused.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;#Why isn’t a vertically-integrated stack the default?&lt;/head&gt;
    &lt;p&gt;The reasons why vertically-integrated stacks aren’t common are a bit different for open source and industrial users, but industry funds open source so there’s a fair amount of overlap.&lt;/p&gt;
    &lt;head rend="h3"&gt;#Vertical integration in open source&lt;/head&gt;
    &lt;p&gt;Open source projects tend to be miniscule. Even projects with hundreds of thousands of lines of code are uncommon, while codebases of that size seem quaint to industrial engineers.&lt;/p&gt;
    &lt;p&gt;While I’m sure that many engineers would enjoy having access to the workflows I described at the start of the post, the motivation is a lot less pressing. Who needs shared build artifacts when a clean optimized build takes a couple minutes? Who needs integration between the test suite and the code editor when the entire codebase fits in the working memory of the only engineer who seriously works on it? And gradual rollouts don’t even make sense when the project doesn’t publicly provide a hosted instance of the software.&lt;/p&gt;
    &lt;p&gt;Integrated stacks also present a coordination issue for open source projects. Does the integration between the test runner and the code editor live in the test runner or the code editor? How is it tested? Open source is full of petty kings and overgrown forum moderators, so this sort of collaboration is rarely feasible in practice.&lt;/p&gt;
    &lt;p&gt;Open source is also full of software freedom acolytes who insist that each tool must “do one thing well.” To these engineers, project A maintaining an integration with project B is a threat to the ability of users to swap out project B for a different tool; the best approach, to them, is for every tool to behave as if no other tool exists. The fact that this results in strictly less-capable tools seems to be lost on these engineers.&lt;/p&gt;
    &lt;p&gt;At the same time, many open source projects are owned or funded largely by a single corporation with no motivation (or ability) to make their internal stack available externally. Any integrations in the project must therefore be compatible with the stack used by those corporations internally. For similar reasons, it is also common to see projects with test suites or build systems that cannot be used outside of the organization that funds them.&lt;/p&gt;
    &lt;head rend="h3"&gt;#Vertical integration in industry&lt;/head&gt;
    &lt;p&gt;Industrial users aren’t interested in a smooth developer experience because it makes engineers’ lives easier. Industrial users want a vertically-integrated stack because engineer time is expensive. This means that a vertically-integrated stack must save time in an absolute sense; that is, including the time it takes to implement and adopt the system.&lt;/p&gt;
    &lt;p&gt;These migration costs push industrial users away from tools that span the stack. When startups are small, the overhead of implementing such a system increases the likelihood that the startup will fail. An organization must be stable enough in order for spending time on integration work to be justifiable. But once an organization is stable enough to want to build out a smoother developer experience, they’ve already developed their own bespoke build and deploy system, often spanning multiple repos with complex dependencies and subtle quirks. At this point, the costs of switching to an off-the-shelf vertically-integrated stack are prohibitively large.&lt;/p&gt;
    &lt;p&gt;Organizations that investigate e.g. switching to Bazel will hear horror stories of migration efforts that went on for years before being scrapped. This is another issue with implementing tight vertical integrations across the stack: deep investment is needed, over an extended period of time, in order for benefits to materialize. Many younger organizations used to bringing features from planning to production in one or two quarters find this sort of investment very challenging to justify.&lt;/p&gt;
    &lt;head rend="h3"&gt;#Vertical integration as a product&lt;/head&gt;
    &lt;p&gt;Could we build a vertically-integrated development environment as a product and sell it to companies who would otherwise build a similar but less-capable system internally? On the surface, this sounds like a great idea: lots of organizations are devoting a lot of effort to building out pretty similar systems for building, deploying, and editing code. It would be fantastic if we could centralize that effort, saving each organization valuable engineering time while delivering a better experience.&lt;/p&gt;
    &lt;p&gt;But remember what we’re talking about: integrations between a code forge, build system, deploy system, CI, and code editor. Large organizations are either unwilling or unable to adopt radically different tools. Your developer tools startup will then need to build and maintain integrations between N products for each of your M large clients. At the same time, you’ll have to compromise on features; if you can’t ship a patch to GitHub, you can’t build a link to open a failing test case locally into their UI.&lt;/p&gt;
    &lt;p&gt;Selling integrations is also a risky position for a company to be in. There’s a natural incentive for the companies being integrated to write replacement integrations in-house and claw back some of the profits that are going to the external competitor. Vendors selling integrations may also find themselves in an uncomfortable position when the services they’re integrating with raise their prices or restrict API access. Cursor, for example, had to hastily change their pricing after Anthropic changed their prices. Cursor also had to beg Microsoft to unban Cursor from the VS Code extension marketplace. Anthropic and Microsoft both view Cursor as a competitor and would rather they did not exist at all. This is not a foundation for a dynamic of productive collaboration between these companies on integrations between their services!&lt;/p&gt;
    &lt;p&gt;For smaller organizations, adopting an integrated developer environment requires a huge leap of faith. GitHub is a very well-known and mature product. Every enterprise developer tool provides a GitHub integration (not as good as first-party support, but much better than nothing). What if the organization providing your developer tooling shuts down? It will be extremely challenging to convince early adopters that the costs are worth it. The migration costs we discussed earlier also apply to leaving such a system, except that users may find themselves forced to leave such a system with little notice.&lt;/p&gt;
    &lt;p&gt;Scott Kennedy notes in “It’s not just an IDE: building the developer cloud is hard” that it’s possible to build a piece of the vision successfully, but benefits remain limited without cooperation from the rest of the stack:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Startups or open source historically focused on one piece of the puzzle. Sourcegraph has been working hard for a decade just to get the CodeSearch piece right. Bazel might be open source, but it has a “batteries not included” feel. The full experience requires putting many pieces together. It’s hard.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;When (e.g.) code search and the build system are produced by different corporations, the profit motive discourages corporations from building tight integrations with other products in the same space.&lt;/p&gt;
    &lt;p&gt;The vertical integration, the glue code linking these different products together, really is the thing that creates the value here. And the glue code is inherently tied to the quirks of the infrastructure of the corporation that built it:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;We’ve also seen dozens of companies successfully take Google/Facebook’s internal tooling ideas and build billion dollar B2B SaaS companies. So where is my developer cloud?&lt;/p&gt;
      &lt;p&gt;It doesn’t exist because it’s really fucking hard.&lt;/p&gt;
      &lt;p&gt;Google can’t make theirs available because they can’t disentangle from their internal architecture without damaging their internal productivity. I know because I was there when they tried. It’s hard.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The value comes from the whole stack being tightly integrated. If you can’t ship the whole stack, you’re very limited in the features you can provide. Glue code is very challenging to sell. Vertical integration is the only thing that matters.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45891772</guid><pubDate>Tue, 11 Nov 2025 19:36:05 +0000</pubDate></item><item><title>Agentic pelican on a bicycle</title><link>https://www.robert-glaser.de/agentic-pelican-on-a-bicycle/</link><description>&lt;doc fingerprint="3f239a28bf3adc7d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Agentic Pelican on a Bicycle&lt;/head&gt;
    &lt;p&gt;The agentic loop—generate, assess, improve—seems like a natural fit for iterating on pelicans on bicycles.&lt;/p&gt;
    &lt;p&gt;Simon Willison has been running his own informal model benchmark for years: “Generate an SVG of a pelican riding a bicycle.” It’s delightfully absurd—and surprisingly revealing. Even the model labs channel this benchmark in their marketing campaigns announcing new models.&lt;/p&gt;
    &lt;p&gt;Simon’s traditional approach is zero-shot: throw the prompt at the model, get SVG back. Maybe—if you’re lucky—you get something resembling a pelican on a bicycle.&lt;/p&gt;
    &lt;p&gt;Nowadays everyone is talking about agents. Models running in a loop using tools. Sometimes they have vision capabilities, too. They can look at what they just created, cringe a little, and try again. The agentic loop—generate, assess, improve—seems like a natural fit for such a task.&lt;/p&gt;
    &lt;p&gt;So I ran a different experiment: what if we let models iterate on their pelicans? What if they could see their own output and self-correct?&lt;/p&gt;
    &lt;head rend="h2"&gt;The Prompt&lt;/head&gt;
    &lt;code&gt;Generate an SVG of a pelican riding a bicycle

- Convert the .svg to .jpg using chrome devtools, then look at the .jpg using your vision capabilities.
- Improve the .svg based on what you see in the .jpg and what's still to improve.
- Keep iterating in this loop until you're satisfied with the generated svg.
- Keep the .jpg for every iteration along the way.&lt;/code&gt;
    &lt;p&gt;Besides the file system and access to a command line, the models had access to Chrome DevTools MCP server (for SVG-to-JPG conversion) and their own multimodal vision capabilities. They could see what they’d drawn, identify problems, and iterate. The loop continued until they declared satisfaction.&lt;/p&gt;
    &lt;p&gt;I used the Chrome DevTools MCP server to give every model the same rasterizer. Without this, models would fall back to whatever SVG-to-image conversion they prefer or have available locally—ImageMagick, Inkscape, browser screenshots, whatever. Standardizing the rendering removes one variable from the equation.&lt;/p&gt;
    &lt;p&gt;The prompt itself is deliberately minimal. I could have steered the iterative loop with more specific guidance—“focus on anatomical accuracy,” “prioritize mechanical realism,” “ensure visual balance.” But that would defeat the point. Simon’s original benchmark is beautifully unconstrained, and I wanted to preserve that spirit. The question isn’t “can models follow detailed improvement instructions?” It’s “when left to their own judgment, what do they choose to fix?”&lt;/p&gt;
    &lt;head rend="h2"&gt;The Models&lt;/head&gt;
    &lt;p&gt;I tested six models across the frontier, all multimodal:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Claude Opus 4.1, Claude Sonnet 4.5, Claude Haiku 4.5, all with thinking&lt;/item&gt;
      &lt;item&gt;GPT-5 (on medium reasoning effort)&lt;/item&gt;
      &lt;item&gt;GPT-5-Codex (on medium reasoning effort)&lt;/item&gt;
      &lt;item&gt;Gemini 2.5 Pro&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Each model decided independently when to stop iterating. Some made four passes. Others kept going for six. None knew when to quit.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Results&lt;/head&gt;
    &lt;p&gt;Let’s see what happened. For each model, I’m showing the first attempt (left) and the final result (right) after self-correction.&lt;/p&gt;
    &lt;head rend="h3"&gt;Claude Opus 4.1 (4 iterations)&lt;/head&gt;
    &lt;p&gt;Opus started with a serviceable pelican-bicycle combo and then did something interesting: it added realism. The final version has an actual bicycle chain connecting the pedals to the rear wheel. The wheels gained more spokes. The pelican’s proportions improved, and it got arms holding the handlebars. This wasn’t just “add more details”—it was “make this mechanically coherent.” Interestingly, we got the catch of the day on a special plate on the handlebars. Oh, and look at the street and the birds in the backdrop!&lt;/p&gt;
    &lt;head rend="h3"&gt;Claude Sonnet 4.5 (4 iterations)&lt;/head&gt;
    &lt;p&gt;Sonnet took a more restrained approach. The changes between iterations were subtler—refinements to curves, adding shadows, and movement indicators. Adjustments to positioning. Improving the spokes. Improving the arms and handlebars. The final result is cleaner, but the core composition remained remarkably stable.&lt;/p&gt;
    &lt;head rend="h3"&gt;Claude Haiku 4.5 (6 iterations)&lt;/head&gt;
    &lt;p&gt;Haiku took the longest journey—six full iterations. It kept tweaking, kept adjusting. The additional iterations didn’t necessarily produce a dramatically better result, but Haiku seemed determined to get every detail right before calling it done. So the pelican definitely received proper legs and feet.&lt;/p&gt;
    &lt;head rend="h3"&gt;GPT-5 Medium (5 iterations)&lt;/head&gt;
    &lt;p&gt;GPT-5 Medium started with a recognizable pelican-bicycle scene and refined it over five iterations. The improvements were incremental—better proportions, clearer shapes—but the fundamental composition held steady throughout.&lt;/p&gt;
    &lt;head rend="h3"&gt;GPT-5-Codex Medium (5 iterations)&lt;/head&gt;
    &lt;p&gt;Here’s where things get interesting. Its initial attempt was… let’s call it “abstract.” A sort of layer cake of pelican parts. And then, instead of simplifying, it doubled down. The final result added even more layers. More complexity. More parts. Whether this counts as “improvement” is a philosophical question I’m not qualified to answer.&lt;/p&gt;
    &lt;head rend="h3"&gt;Gemini 2.5 Pro (6 iterations)&lt;/head&gt;
    &lt;p&gt;Gemini was the outlier. Most models preserved their initial composition through iterations, making refinements but keeping the core structure mostly intact. Gemini actually changed the fundamental arrangement—the pelican’s pose, the bicycle’s orientation, the spatial relationship between them. Six iterations showed a bigger leap.&lt;/p&gt;
    &lt;head rend="h2"&gt;What Did We Learn?&lt;/head&gt;
    &lt;p&gt;The results are… mixed.&lt;/p&gt;
    &lt;p&gt;The optimistic take: Models like Opus 4.1 made genuinely thoughtful improvements. Adding a bicycle chain isn’t just decoration—it shows understanding of mechanical relationships. The wheel spokes, the adjusted proportions—these are signs of vision-driven refinement working as intended.&lt;/p&gt;
    &lt;p&gt;The skeptical take: Most models didn’t fundamentally change their approach. They tweaked. They adjusted. They added details. But the basic composition—pelican shape, bicycle shape, spatial relationship—was determined in iteration one and largely frozen thereafter.&lt;/p&gt;
    &lt;p&gt;The confusing take: Some models (looking at you, GPT-5-Codex) seemed to mistake “more complex” for “better.” The self-feedback loop amplified their initial artistic direction rather than correcting it. If your first draft is a layer cake of pelican parts, and your self-correction produces an even more elaborate layer cake... did the loop help? Of course, GPT-5-Codex is a fine-tune of GPT-5, optimized for engineering tasks. Could be that its strengths are not in broad visual capabilities.&lt;/p&gt;
    &lt;p&gt;The agentic approach definitely produces different results than zero-shot generation. Whether it produces better results seems to depend heavily on the model’s ability to self-critique. Vision capabilities alone aren’t enough—you need something more: aesthetic judgment, mechanical reasoning, or at least the wisdom to know when to stop adding details.&lt;/p&gt;
    &lt;p&gt;Simon’s zero-shot benchmark reveals how well models handle unusual creative tasks on the first try. The agentic variant reveals something else: how well models can evaluate and improve their own creative output. Turns out, that’s a different skill entirely. But—somehow related?&lt;/p&gt;
    &lt;p&gt;All test code and results available in the GitHub repository.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45891817</guid><pubDate>Tue, 11 Nov 2025 19:40:15 +0000</pubDate></item><item><title>A catalog of side effects</title><link>https://bernsteinbear.com/blog/compiler-effects/</link><description>&lt;doc fingerprint="6202111079d508b8"&gt;
  &lt;main&gt;
    &lt;p&gt;Optimizing compilers like to keep track of each IR instruction’s effects. An instruction’s effects vary wildly from having no effects at all, to writing a specific variable, to completely unknown (writing all state).&lt;/p&gt;
    &lt;p&gt;This post can be thought of as a continuation of What I talk about when I talk about IRs, specifically the section talking about asking the right questions. When we talk about effects, we should ask the right questions: not what opcode is this? but instead what effects does this opcode have?&lt;/p&gt;
    &lt;p&gt;Different compilers represent and track these effects differently. I’ve been thinking about how to represent these effects all year, so I have been doing some reading. In this post I will give some summaries of the landscape of approaches. Please feel free to suggest more.&lt;/p&gt;
    &lt;p&gt;Internal IR effect tracking is similar to the programming language notion of algebraic effects in type systems, but internally, compilers keep track of finer-grained effects. Effects such as “writes to a local variable”, “writes to a list”, or “reads from the stack” indicate what instructions can be re-ordered, duplicated, or removed entirely.&lt;/p&gt;
    &lt;p&gt;For example, consider the following pseodocode for some made-up language that stands in for a snippet of compiler IR:&lt;/p&gt;
    &lt;code&gt;# ...
v = some_var[0]
another_var[0] = 5
# ...
&lt;/code&gt;
    &lt;p&gt;The goal of effects is to communicate to the compiler if, for example, these two IR instructions can be re-ordered. The second instruction might write to a location that the first one reads. But it also might not! This is about knowing if &lt;code&gt;some_var&lt;/code&gt; and &lt;code&gt;another_var&lt;/code&gt; alias—if they are different names that
refer to the same object.&lt;/p&gt;
    &lt;p&gt;We can sometimes answer that question directly, but often it’s cheaper to compute an approximate answer: could they even alias? It’s possible that &lt;code&gt;some_var&lt;/code&gt; and &lt;code&gt;another_var&lt;/code&gt; have different types, meaning that (as long as you
have strict aliasing) the &lt;code&gt;Load&lt;/code&gt; and &lt;code&gt;Store&lt;/code&gt; operations that implement these
reads and writes by definition touch different locations. And if they look
at disjoint locations, there need not be any explicit order enforced.&lt;/p&gt;
    &lt;p&gt;Different compilers keep track of this information differently. The null effect analysis gives up and says “every instruction is maximally effectful” and therefore “we can’t re-order or delete any instructions”. That’s probably fine for a first stab at a compiler, where you will get a big speed up purely based on strength reductions. Over-approximations of effects should always be valid.&lt;/p&gt;
    &lt;p&gt;But at some point you start wanting to do dead code elimination (DCE), or common subexpression elimination (CSE), or loads/store elimination, or move instructions around, and you start wondering how to represent effects. That’s where I am right now. So here’s a catalog of different compilers I have looked at recently.&lt;/p&gt;
    &lt;p&gt;There are two main ways I have seen to represent effects: bitsets and heap range lists. We’ll look at one example compiler for each, talk a bit about tradeoffs, then give a bunch of references to other major compilers.&lt;/p&gt;
    &lt;p&gt;We’ll start with Cinder, a Python JIT, because that’s what I used to work on.&lt;/p&gt;
    &lt;p&gt;Cinder tracks heap effects for its high-level IR (HIR) in instr_effects.h. Pretty much everything happens in the &lt;code&gt;memoryEffects(const Instr&amp;amp; instr)&lt;/code&gt; function, which is expected to know
everything about what effects the given instruction might have.&lt;/p&gt;
    &lt;p&gt;The data representation is a bitset representation of a lattice called an &lt;code&gt;AliasClass&lt;/code&gt; and that is defined in alias_class.h. Each
bit in the bitset represents a distinct location in the heap: reads from and
writes to each of these locations are guaranteed not to affect any of the other
locations.&lt;/p&gt;
    &lt;p&gt;Here is the X-macro that defines it:&lt;/p&gt;
    &lt;code&gt;#define HIR_BASIC_ACLS(X) \
  X(ArrayItem)            \
  X(CellItem)             \
  X(DictItem)             \
  X(FuncArgs)             \
  X(FuncAttr)             \
  X(Global)               \
  X(InObjectAttr)         \
  X(ListItem)             \
  X(Other)                \
  X(TupleItem)            \
  X(TypeAttrCache)        \
  X(TypeMethodCache)

enum BitIndexes {
#define ACLS(name) k##name##Bit,
    HIR_BASIC_ACLS(ACLS)
#undef ACLS
};
&lt;/code&gt;
    &lt;p&gt;Note that each bit implicitly represents a set: &lt;code&gt;ListItem&lt;/code&gt; does not refer to a
specific list index, but the infinite set of all possible list indices. It’s
any list index. Still, every list index is completely disjoint from, say, every
entry in a global variable table.&lt;/p&gt;
    &lt;p&gt;(And, to be clear, an object in a list might be the same as an object in a global variable table. The objects themselves can alias. But the thing being written to or read from, the thing being side effected, is the container.)&lt;/p&gt;
    &lt;p&gt;Like other bitset lattices, it’s possible to union the sets by or-ing the bits. It’s possible to query for overlap by and-ing the bits.&lt;/p&gt;
    &lt;code&gt;class AliasClass {
  // The union of two AliasClass
  AliasClass operator|(AliasClass other) const {
    return AliasClass{bits_ | other.bits_};
  }

  // The intersection (overlap) of two AliasClass
  AliasClass operator&amp;amp;(AliasClass other) const {
    return AliasClass{bits_ &amp;amp; other.bits_};
  }
};
&lt;/code&gt;
    &lt;p&gt;If this sounds familiar, it’s because (as the repo notes) it’s a similar idea to Cinder’s type lattice representation.&lt;/p&gt;
    &lt;p&gt;Like other lattices, there is both a bottom element (no effects) and a top element (all possible effects):&lt;/p&gt;
    &lt;code&gt;#define HIR_OR_BITS(name) | k##name

#define HIR_UNION_ACLS(X)                           \
  /* Bottom union */                                \
  X(Empty, 0)                                       \
  /* Top union */                                   \
  X(Any, 0 HIR_BASIC_ACLS(HIR_OR_BITS))             \
  /* Memory locations accessible by managed code */ \
  X(ManagedHeapAny, kAny &amp;amp; ~kFuncArgs)
&lt;/code&gt;
    &lt;p&gt;Union operations naturally hit a fixpoint at &lt;code&gt;Any&lt;/code&gt; and intersection operations
naturally hit a fixpoint at &lt;code&gt;Empty&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;All of this together lets the optimizer ask and answer questions such as:&lt;/p&gt;
    &lt;p&gt;and more.&lt;/p&gt;
    &lt;p&gt;Let’s take a look at an (imaginary) IR version of the code snippet in the intro and see what analyzing it might look like in the optimizer. Here is the fake IR:&lt;/p&gt;
    &lt;code&gt;v0: Tuple = ...
v1: List = ...
v2: Int[5] = ...
# v = some_var[0]
v3: Object = LoadTupleItem v0, 0
# another_var[0] = 5
StoreListItem v1, 0, v2
&lt;/code&gt;
    &lt;p&gt;You can imagine that &lt;code&gt;LoadTupleItem&lt;/code&gt; declares that it reads from the
&lt;code&gt;TupleItem&lt;/code&gt; heap and &lt;code&gt;StoreListItem&lt;/code&gt; declares that it writes to the &lt;code&gt;ListItem&lt;/code&gt;
heap. Because tuple and list pointers cannot be casted into one another and
therefore cannot alias, these are
disjoint heaps in our bitset. Therefore &lt;code&gt;ListItem &amp;amp; TupleItem == 0&lt;/code&gt;, therefore
these memory operations can never interfere! They can (for example) be
re-ordered arbitrarily.&lt;/p&gt;
    &lt;p&gt;In Cinder, these memory effects could in the future be used for instruction re-ordering, but they are today mostly used in two places: the refcount insertion pass and DCE.&lt;/p&gt;
    &lt;p&gt;DCE involves first finding the set of instructions that need to be kept around because they are useful/important/have effects. So here is what the Cinder DCE &lt;code&gt;isUseful&lt;/code&gt; looks like:&lt;/p&gt;
    &lt;code&gt;bool isUseful(Instr&amp;amp; instr) {
  return instr.IsTerminator() || instr.IsSnapshot() ||
      (instr.asDeoptBase() != nullptr &amp;amp;&amp;amp; !instr.IsPrimitiveBox()) ||
      (!instr.IsPhi() &amp;amp;&amp;amp; memoryEffects(instr).may_store != AEmpty);
}
&lt;/code&gt;
    &lt;p&gt;There are some other checks in there but &lt;code&gt;memoryEffects&lt;/code&gt; is right there at the
core of it!&lt;/p&gt;
    &lt;p&gt;Now that we have seen the bitset representation of effects and an implementation in Cinder, let’s take a look at a different representation and and an implementation in JavaScriptCore.&lt;/p&gt;
    &lt;p&gt;I keep coming back to How I implement SSA form by Fil Pizlo, one of the significant contributors to JavaScriptCore (JSC). In particular, I keep coming back to the Uniform Effect Representation section. This notion of “abstract heaps” felt very… well, abstract. Somehow more abstract than the bitset representation. The pre-order and post-order integer pair as a way to represent nested heap effects just did not click.&lt;/p&gt;
    &lt;p&gt;It didn’t make any sense until I actually went spelunking in JavaScriptCore and found one of several implementations—because, you know, JSC is six compilers in a trenchcoat[citation needed].&lt;/p&gt;
    &lt;p&gt;DFG, B3, DOMJIT, and probably others all have their own abstract heap implementations. We’ll look at DOMJIT mostly because it’s a smaller example and also illustrates something else that’s interesting: builtins. We’ll come back to builtins in a minute.&lt;/p&gt;
    &lt;p&gt;Let’s take a lookat how DOMJIT structures its abstract heaps: a YAML file.&lt;/p&gt;
    &lt;code&gt;DOM:
    Tree:
        Node:
            - Node_firstChild
            - Node_lastChild
            - Node_parentNode
            - Node_nextSibling
            - Node_previousSibling
            - Node_ownerDocument
        Document:
            - Document_documentElement
            - Document_body
&lt;/code&gt;
    &lt;p&gt;It’s a hierarchy. &lt;code&gt;Node_firstChild&lt;/code&gt; is a subheap of &lt;code&gt;Node&lt;/code&gt; is a subheap of…
and so on. A write to any &lt;code&gt;Node_nextSibling&lt;/code&gt; is a write to &lt;code&gt;Node&lt;/code&gt; is a write to
… Sibling heaps are unrelated: &lt;code&gt;Node_firstChild&lt;/code&gt; and &lt;code&gt;Node_lastChild&lt;/code&gt;, for
example, are disjoint.&lt;/p&gt;
    &lt;p&gt;To get a feel for this, I wired up a simplified version of ZJIT’s bitset generator (for types!) to read a YAML document and generate a bitset. It generated the following Rust code:&lt;/p&gt;
    &lt;code&gt;mod bits {
  pub const Empty: u64 = 0u64;
  pub const Document_body: u64 = 1u64 &amp;lt;&amp;lt; 0;
  pub const Document_documentElement: u64 = 1u64 &amp;lt;&amp;lt; 1;
  pub const Document: u64 = Document_body | Document_documentElement;
  pub const Node_firstChild: u64 = 1u64 &amp;lt;&amp;lt; 2;
  pub const Node_lastChild: u64 = 1u64 &amp;lt;&amp;lt; 3;
  pub const Node_nextSibling: u64 = 1u64 &amp;lt;&amp;lt; 4;
  pub const Node_ownerDocument: u64 = 1u64 &amp;lt;&amp;lt; 5;
  pub const Node_parentNode: u64 = 1u64 &amp;lt;&amp;lt; 6;
  pub const Node_previousSibling: u64 = 1u64 &amp;lt;&amp;lt; 7;
  pub const Node: u64 = Node_firstChild | Node_lastChild | Node_nextSibling | Node_ownerDocument | Node_parentNode | Node_previousSibling;
  pub const Tree: u64 = Document | Node;
  pub const DOM: u64 = Tree;
  pub const NumTypeBits: u64 = 8;
}
&lt;/code&gt;
    &lt;p&gt;It’s not a fancy X-macro, but it’s a short and flexible Ruby script.&lt;/p&gt;
    &lt;p&gt;Then I took the DOMJIT abstract heap generator—also funnily enough a short Ruby script—modified the output format slightly, and had it generate its int pairs:&lt;/p&gt;
    &lt;code&gt;mod bits {
  /* DOMJIT Abstract Heap Tree.
  DOM&amp;lt;0,8&amp;gt;:
      Tree&amp;lt;0,8&amp;gt;:
          Node&amp;lt;0,6&amp;gt;:
              Node_firstChild&amp;lt;0,1&amp;gt;
              Node_lastChild&amp;lt;1,2&amp;gt;
              Node_parentNode&amp;lt;2,3&amp;gt;
              Node_nextSibling&amp;lt;3,4&amp;gt;
              Node_previousSibling&amp;lt;4,5&amp;gt;
              Node_ownerDocument&amp;lt;5,6&amp;gt;
          Document&amp;lt;6,8&amp;gt;:
              Document_documentElement&amp;lt;6,7&amp;gt;
              Document_body&amp;lt;7,8&amp;gt;
  */
  pub const DOM: HeapRange = HeapRange { start: 0, end: 8 };
  pub const Tree: HeapRange = HeapRange { start: 0, end: 8 };
  pub const Node: HeapRange = HeapRange { start: 0, end: 6 };
  pub const Node_firstChild: HeapRange = HeapRange { start: 0, end: 1 };
  pub const Node_lastChild: HeapRange = HeapRange { start: 1, end: 2 };
  pub const Node_parentNode: HeapRange = HeapRange { start: 2, end: 3 };
  pub const Node_nextSibling: HeapRange = HeapRange { start: 3, end: 4 };
  pub const Node_previousSibling: HeapRange = HeapRange { start: 4, end: 5 };
  pub const Node_ownerDocument: HeapRange = HeapRange { start: 5, end: 6 };
  pub const Document: HeapRange = HeapRange { start: 6, end: 8 };
  pub const Document_documentElement: HeapRange = HeapRange { start: 6, end: 7 };
  pub const Document_body: HeapRange = HeapRange { start: 7, end: 8 };
}
&lt;/code&gt;
    &lt;p&gt;It already comes with a little diagram, which is super helpful for readability.&lt;/p&gt;
    &lt;p&gt;Any empty range(s) represent empty heap effects: if the start and end are the same number, there are no effects. There is no one &lt;code&gt;Empty&lt;/code&gt; value, but any empty
range could be normalized to &lt;code&gt;HeapRange { start: 0, end: 0 }&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Maybe this was obvious to you, dear reader, but this pre-order/post-order thing is about nested ranges! Seeing the output of the generator laid out clearly like this made it make a lot more sense for me.&lt;/p&gt;
    &lt;p&gt;What about checking overlap? Here is the implementation in JSC:&lt;/p&gt;
    &lt;code&gt;namespace WTF {
// Check if two ranges overlap assuming that neither range is empty.
template&amp;lt;typename T&amp;gt;
constexpr bool nonEmptyRangesOverlap(T leftMin, T leftMax, T rightMin, T rightMax)
{
    ASSERT_UNDER_CONSTEXPR_CONTEXT(leftMin &amp;lt; leftMax);
    ASSERT_UNDER_CONSTEXPR_CONTEXT(rightMin &amp;lt; rightMax);

    return leftMax &amp;gt; rightMin &amp;amp;&amp;amp; rightMax &amp;gt; leftMin;
}

// Pass ranges with the min being inclusive and the max being exclusive.
template&amp;lt;typename T&amp;gt;
constexpr bool rangesOverlap(T leftMin, T leftMax, T rightMin, T rightMax) {
    ASSERT_UNDER_CONSTEXPR_CONTEXT(leftMin &amp;lt;= leftMax);
    ASSERT_UNDER_CONSTEXPR_CONTEXT(rightMin &amp;lt;= rightMax);

    // Empty ranges interfere with nothing.
    if (leftMin == leftMax)
        return false;
    if (rightMin == rightMax)
        return false;

    return nonEmptyRangesOverlap(leftMin, leftMax, rightMin, rightMax);
}
}

class HeapRange {
    bool overlaps(const HeapRange&amp;amp; other) const {
        return WTF::rangesOverlap(m_begin, m_end, other.m_begin, other.m_end);
    }
}
&lt;/code&gt;
    &lt;p&gt;(See also How to check for overlapping intervals and Range overlap in two compares for more fun.)&lt;/p&gt;
    &lt;p&gt;While bitsets are a dense representation (you have to hold every bit), they are very compact and they are very precise. You can hold any number of combinations of 64 or 128 bits in a single register. The union and intersection operations are very cheap.&lt;/p&gt;
    &lt;p&gt;With int ranges, it’s a little more complicated. An imprecise union of &lt;code&gt;a&lt;/code&gt; and
&lt;code&gt;b&lt;/code&gt; can take the maximal range that covers both &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt;. To get a more
precise union, you have to keep track of both. In the worst case, if you want
efficient arbitrary queries, you need to store your int ranges in an interval
tree. So what gives?&lt;/p&gt;
    &lt;p&gt;I asked Fil if both bitsets and int ranges answer the same question, why use int ranges? He said that it’s more flexible long-term: bitsets get expensive as soon as you need over 128 bits (you might need to heap allocate them!) whereas ranges have no such ceiling. But doesn’t holding sequences of ranges require heap allocation? Well, despite Fil writing this in his SSA post:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The purpose of the effect representation baked into the IR is to provide a precise always-available baseline for alias information that is super easy to work with. […] you can have instructions report that they read/write multiple heaps […] you can have a utility function that produces such lists on demand.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;It’s important to note that this doesn’t actually involve any allocation of lists. JSC does this very clever thing where they have “functors” that they pass in as arguments that compress/summarize what they want to out of an instruction’s effects.&lt;/p&gt;
    &lt;p&gt;Let’s take a look at how the DFG (for example) uses these heap ranges in analysis. The DFG is structured in such a way that it can make use of the DOMJIT heap ranges directly, which is neat.&lt;/p&gt;
    &lt;p&gt;Note that &lt;code&gt;AbstractHeap&lt;/code&gt; in the example below is a thin wrapper over the DFG
compiler’s own &lt;code&gt;DOMJIT::HeapRange&lt;/code&gt; equivalent:&lt;/p&gt;
    &lt;code&gt;class AbstractHeapOverlaps {
public:
    AbstractHeapOverlaps(AbstractHeap heap)
        : m_heap(heap)
        , m_result(false)
    {
    }

    void operator()(AbstractHeap otherHeap) const
    {
        if (m_result)
            return;
        m_result = m_heap.overlaps(otherHeap);
    }

    bool result() const { return m_result; }

private:
    AbstractHeap m_heap;
    mutable bool m_result;
};

bool writesOverlap(Graph&amp;amp; graph, Node* node, AbstractHeap heap)
{
    NoOpClobberize noOp;
    AbstractHeapOverlaps addWrite(heap);
    clobberize(graph, node, noOp, addWrite, noOp);
    return addWrite.result();
}
&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;clobberize&lt;/code&gt; is the function that calls these functors (&lt;code&gt;noOp&lt;/code&gt; or &lt;code&gt;addWrite&lt;/code&gt; in
this case) for each effect that the given IR instruction &lt;code&gt;node&lt;/code&gt; declares.&lt;/p&gt;
    &lt;p&gt;I’ve pulled some relevant snippets of &lt;code&gt;clobberize&lt;/code&gt;, which is quite long, that I
think are interesting.&lt;/p&gt;
    &lt;p&gt;First, some instructions (constants, here) have no effects. There’s some utility in the &lt;code&gt;def(PureValue(...))&lt;/code&gt; call but I didn’t understand fully.&lt;/p&gt;
    &lt;p&gt;Then there are some instructions that conditionally have effects depending on the use types of their operands.1 Taking the absolute value of an Int32 or a Double is effect-free but otherwise looks like it can run arbitrary code.&lt;/p&gt;
    &lt;p&gt;Some run-time IR guards that might cause side exits are annotated as such—they write to the &lt;code&gt;SideState&lt;/code&gt; heap.&lt;/p&gt;
    &lt;p&gt;Local variable instructions read specific heaps indexed by what looks like the local index but I’m not sure. This means accessing two different locals won’t alias!&lt;/p&gt;
    &lt;p&gt;Instructions that allocate can’t be re-ordered, it looks like; they both read and write the &lt;code&gt;HeapObjectCount&lt;/code&gt;. This probably limits the amount of allocation
sinking that can be done.&lt;/p&gt;
    &lt;p&gt;Then there’s &lt;code&gt;CallDOM&lt;/code&gt;, which is the builtins stuff I was talking about. We’ll
come back to that after the code block.&lt;/p&gt;
    &lt;code&gt;template&amp;lt;typename ReadFunctor, typename WriteFunctor, typename DefFunctor, typename ClobberTopFunctor&amp;gt;
void clobberize(Graph&amp;amp; graph, Node* node, const ReadFunctor&amp;amp; read, const WriteFunctor&amp;amp; write, const DefFunctor&amp;amp; def)
{
    // ...

    switch (node-&amp;gt;op()) {
    case JSConstant:
    case DoubleConstant:
    case Int52Constant:
        def(PureValue(node, node-&amp;gt;constant()));
        return;

    case ArithAbs:
        if (node-&amp;gt;child1().useKind() == Int32Use || node-&amp;gt;child1().useKind() == DoubleRepUse)
            def(PureValue(node, node-&amp;gt;arithMode()));
        else
            clobberTop();
        return;

    case AssertInBounds:
    case AssertNotEmpty:
        write(SideState);
        return;

    case GetLocal:
        read(AbstractHeap(Stack, node-&amp;gt;operand()));
        def(HeapLocation(StackLoc, AbstractHeap(Stack, node-&amp;gt;operand())), LazyNode(node));
        return;

    case NewArrayWithSize:
    case NewArrayWithSizeAndStructure:
        read(HeapObjectCount);
        write(HeapObjectCount);
        return;

    case CallDOM: {
        const DOMJIT::Signature* signature = node-&amp;gt;signature();
        DOMJIT::Effect effect = signature-&amp;gt;effect;
        if (effect.reads) {
            if (effect.reads == DOMJIT::HeapRange::top())
                read(World);
            else
                read(AbstractHeap(DOMState, effect.reads.rawRepresentation()));
        }
        if (effect.writes) {
            if (effect.writes == DOMJIT::HeapRange::top()) {
                if (Options::validateDFGClobberize())
                    clobberTopFunctor();
                write(Heap);
            } else
                write(AbstractHeap(DOMState, effect.writes.rawRepresentation()));
        }
        ASSERT_WITH_MESSAGE(effect.def == DOMJIT::HeapRange::top(), "Currently, we do not accept any def for CallDOM.");
        return;
    }
    }
}
&lt;/code&gt;
    &lt;p&gt;(Remember that these &lt;code&gt;AbstractHeap&lt;/code&gt; operations are very similar to DOMJIT’s
&lt;code&gt;HeapRange&lt;/code&gt; with a couple more details—and in some cases even contain DOMJIT
&lt;code&gt;HeapRange&lt;/code&gt;s!)&lt;/p&gt;
    &lt;p&gt;This &lt;code&gt;CallDOM&lt;/code&gt; node is the way for the DOM APIs in the browser—a significant
chunk of the builtins, which are written in C++—to communicate what they do
to the optimizing compiler. Without any annotations, the JIT has to assume that
a call into C++ could do anything to the JIT state. Bummer!&lt;/p&gt;
    &lt;p&gt;But because, for example, &lt;code&gt;Node.firstChild&lt;/code&gt; annotates what
memory it reads from and what it doesn’t write to,
the JIT can optimize around it better—or even remove the access completely.
It means the JIT can reason about calls to known builtins the same way that
it reasons about normal JIT opcodes.&lt;/p&gt;
    &lt;p&gt;(Incidentally it looks like it doesn’t even make a C call, but instead is inlined as a little memory read snippet using a JIT builder API. Neat.)&lt;/p&gt;
    &lt;p&gt;Last, we’ll look at Simple, which has a slightly different take on all of this.&lt;/p&gt;
    &lt;p&gt;Simple is Cliff Click’s pet Sea of Nodes (SoN) project to try and showcase the idea to the world—outside of a HotSpot C2 context.&lt;/p&gt;
    &lt;p&gt;This one is a little harder for me to understand but it looks like each translation unit has a &lt;code&gt;StartNode&lt;/code&gt; that doles out
different classes of memory nodes for each alias class. Each IR node then takes
data dependencies on whatever effect nodes it might uses.&lt;/p&gt;
    &lt;p&gt;Alias classes are split up based on the paper Type-Based Alias Analysis (PDF): “Our approach is a form of TBAA similar to the ‘FieldTypeDecl’ algorithm described in the paper.”&lt;/p&gt;
    &lt;p&gt;The Simple project is structured into sequential implementation stages and alias classes come into the picture in Chapter 10.&lt;/p&gt;
    &lt;p&gt;Because I spent a while spelunking through other implementations to see how other projects did this, here is a list of the projects I looked at. Mostly, they use bitsets.&lt;/p&gt;
    &lt;p&gt;HHVM, a JIT for the Hack language, also uses a bitset for its memory effects. See for example: alias-class.h and memory-effects.h.&lt;/p&gt;
    &lt;p&gt;HHVM has a couple places that use this information, such as a definition-sinking pass, alias analysis, DCE, store elimination, refcount opts, and more.&lt;/p&gt;
    &lt;p&gt;If you are wondering why the HHVM representation looks similar to the Cinder representation, it’s because some former HHVM engineers such as Brett Simmers also worked on Cinder!&lt;/p&gt;
    &lt;p&gt;(note that I am linking an ART fork on GitHub as a reference, but the upstream code is hosted on googlesource)&lt;/p&gt;
    &lt;p&gt;Android’s ART Java runtime also uses a bitset for its effect representation. It’s a very compact class called &lt;code&gt;SideEffects&lt;/code&gt; in nodes.h.&lt;/p&gt;
    &lt;p&gt;The side effects are used in loop-invariant code motion, global value numbering, write barrier elimination, scheduling, and more.&lt;/p&gt;
    &lt;p&gt;CoreCLR mostly uses a bitset for its &lt;code&gt;SideEffectSet&lt;/code&gt;
class. This one is interesting though because it also splits out effects
specifically to include sets of local variables (&lt;code&gt;LclVarSet&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;V8 is also about six completely different compilers in a trenchcoat.&lt;/p&gt;
    &lt;p&gt;Turboshaft uses a struct in operations.h called &lt;code&gt;OpEffects&lt;/code&gt; which is two bitsets for reads/writes of effects. This is used in
value numbering as well a bunch of
other small optimization passes they call “reducers”.&lt;/p&gt;
    &lt;p&gt;Maglev also has this thing called &lt;code&gt;NodeT::kProperties&lt;/code&gt; in their IR
nodes that also looks like a bitset and is used in their various
reducers. It has effect query methods on it such as &lt;code&gt;can_eager_deopt&lt;/code&gt; and
&lt;code&gt;can_write&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Until recently, V8 also used Sea of Nodes as its IR representation, which also tracks side effects more explicitly in the structure of the IR itself.&lt;/p&gt;
    &lt;p&gt;Guile Scheme looks like it has a custom tagging scheme type thing.&lt;/p&gt;
    &lt;p&gt;Both bitsets and int ranges are perfectly cromulent ways of representing heap effects for your IR. The Sea of Nodes approach is also probably okay since it powers HotSpot C2 and (for a time) V8.&lt;/p&gt;
    &lt;p&gt;Remember to ask the right questions of your IR when doing analysis.&lt;/p&gt;
    &lt;p&gt;Thank you to Fil Pizlo for writing his initial GitHub Gist and sending me on this journey and thank you to Chris Gregory, Brett Simmers, and Ufuk Kayserilioglu for feedback on making some of the explanations more helpful.&lt;/p&gt;
    &lt;p&gt;This is because the DFG compiler does this interesting thing where they track and guard the input types on use vs having types attached to the input’s own def. It might be a clean way to handle shapes inside the type system while also allowing the type+shape of an object to change over time (which it can do in many dynamic language runtimes). ↩&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45891868</guid><pubDate>Tue, 11 Nov 2025 19:44:49 +0000</pubDate></item><item><title>A modern 35mm film scanner for home</title><link>https://www.soke.engineering/</link><description>&lt;doc fingerprint="4d1a5a5e7b33d889"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The New Era of &lt;lb/&gt;Film Scanning&lt;/head&gt;
    &lt;head rend="h2"&gt;The New Era of &lt;lb/&gt;Film Scanning&lt;/head&gt;
    &lt;head rend="h2"&gt;Knokke - a high-resolution 35 mm film scanner built for photographers who demand speed, quality and control.&lt;/head&gt;
    &lt;head rend="h2"&gt;The New Era of &lt;lb/&gt;Film Scanning&lt;/head&gt;
    &lt;head rend="h2"&gt;Knokke - a high-resolution 35 mm film scanner built for photographers who demand speed, quality, and control.&lt;/head&gt;
    &lt;head rend="h2"&gt;Knokke redefines film scanning by bringing modern imaging, optics, and software into a beautifully engineered device.&lt;/head&gt;
    &lt;head rend="h3"&gt;4064&lt;/head&gt;
    &lt;head rend="h3"&gt;4064&lt;/head&gt;
    &lt;head rend="h3"&gt;DPI Resolution&lt;/head&gt;
    &lt;head rend="h3"&gt;DPI Resolution&lt;/head&gt;
    &lt;head rend="h3"&gt;120 dB&lt;/head&gt;
    &lt;head rend="h3"&gt;120 dB&lt;/head&gt;
    &lt;head rend="h3"&gt;Dynamic Range&lt;/head&gt;
    &lt;head rend="h3"&gt;Dynamic Range&lt;/head&gt;
    &lt;head rend="h3"&gt;48-bit&lt;/head&gt;
    &lt;head rend="h3"&gt;48-bit&lt;/head&gt;
    &lt;head rend="h3"&gt;True Color&lt;/head&gt;
    &lt;head rend="h3"&gt;True Color&lt;/head&gt;
    &lt;head rend="h2"&gt;Knokke - State-of-the-Art Hardware&lt;/head&gt;
    &lt;head rend="h3"&gt;Knokke - State-of-the-Art Hardware&lt;/head&gt;
    &lt;head rend="h2"&gt;Knokke - State-of-the-Art Hardware&lt;/head&gt;
    &lt;p&gt;The modern 35 mm film scanner that captures a full roll in under just a few minutes while capturing every frame at 4064 DPI and 48bit colour. Its custom optics and state-of-the-art sensor deliver benchmark setting quality and speed at a price only Knokke can offer.&lt;/p&gt;
    &lt;p&gt;Launch Updates&lt;/p&gt;
    &lt;p&gt;Launch Updates&lt;/p&gt;
    &lt;p&gt;Launch Updates&lt;/p&gt;
    &lt;p&gt;A Modern Workflow&lt;/p&gt;
    &lt;p&gt;A Modern Workflow&lt;/p&gt;
    &lt;p&gt;A Modern Workflow&lt;/p&gt;
    &lt;head rend="h2"&gt;Korova - Custom Software&lt;/head&gt;
    &lt;head rend="h3"&gt;Korova - Custom Software&lt;/head&gt;
    &lt;head rend="h2"&gt;Korova - Custom Software&lt;/head&gt;
    &lt;p&gt;Built for the 21st century, Knokke runs on Korova, a lean C++ application that's native to Linux, macOS, and Windowsâso you can forget vintage PCs and enjoy a plug-and-play workflow that lets you focus on your photos.&lt;/p&gt;
    &lt;p&gt;Each frame can have custom scan settings, repeatable across multiple scans for consistent results and tailored workflows. The scanner can also skip directly to requested frames, massively accelerating scanning time and enabling fast access to key shots without unnecessary delay.&lt;/p&gt;
    &lt;p&gt;Launch Updates&lt;/p&gt;
    &lt;p&gt;Launch Updates&lt;/p&gt;
    &lt;p&gt;Launch Updates&lt;/p&gt;
    &lt;head rend="h3"&gt;Engineered for Individual Users and Lab Professionals&lt;/head&gt;
    &lt;head rend="h6"&gt;01&lt;/head&gt;
    &lt;head rend="h6"&gt;Quality&lt;/head&gt;
    &lt;p&gt;Knokkeâs premium build and precision engineering ensure lasting, reliable performance.&lt;/p&gt;
    &lt;head rend="h6"&gt;02&lt;/head&gt;
    &lt;head rend="h6"&gt;Speed&lt;/head&gt;
    &lt;p&gt;Knokkeâs high scan speed and streamlined workflow keep you moving.&lt;/p&gt;
    &lt;head rend="h6"&gt;03&lt;/head&gt;
    &lt;head rend="h6"&gt;Full Control&lt;/head&gt;
    &lt;p&gt;Knokke lets you fine-tune every detail with flexible settings and precise color control.&lt;/p&gt;
    &lt;head rend="h6"&gt;04&lt;/head&gt;
    &lt;head rend="h6"&gt;Future Proof&lt;/head&gt;
    &lt;p&gt;Knokke comes with ongoing software support, open-source flexibility, and readily available spare parts.&lt;/p&gt;
    &lt;head rend="h2"&gt;Price at Launch&lt;/head&gt;
    &lt;head rend="h2"&gt;999â¬&lt;/head&gt;
    &lt;p&gt;Includes scanner + software&lt;/p&gt;
    &lt;head rend="h2"&gt;4064 dpi resolution&lt;/head&gt;
    &lt;head rend="h2"&gt;5 min per roll&lt;/head&gt;
    &lt;head rend="h2"&gt;48-bit colour depth&lt;/head&gt;
    &lt;head rend="h2"&gt;120 dB Dynamic Range&lt;/head&gt;
    &lt;head rend="h2"&gt;LED Matrix&lt;/head&gt;
    &lt;head rend="h2"&gt;RGB LED backlight&lt;/head&gt;
    &lt;head rend="h2"&gt;USB-C 3.2&lt;/head&gt;
    &lt;head rend="h2"&gt;Custom software&lt;/head&gt;
    &lt;head rend="h2"&gt;A Closer Look at Knokke&lt;/head&gt;
    &lt;head rend="h2"&gt;A Closer Look at Knokke&lt;/head&gt;
    &lt;head rend="h2"&gt;A Closer Look at Knokke&lt;/head&gt;
    &lt;head rend="h2"&gt;Specifications.&lt;/head&gt;
    &lt;head rend="h2"&gt;Specifications.&lt;/head&gt;
    &lt;head rend="h3"&gt;IMAGING SYSTEM&lt;/head&gt;
    &lt;p&gt;SENSOR&lt;/p&gt;
    &lt;p&gt;Backside illumintaed CMOS Sensor&lt;/p&gt;
    &lt;p&gt;DYNAMIC RANGE&lt;/p&gt;
    &lt;p&gt;linear dynamic range of 78 dB, expandable to 120 dB with native 16-bit HDR log profile, up to 14 stops of range&lt;/p&gt;
    &lt;p&gt;RESOLUTION&lt;/p&gt;
    &lt;p&gt;Max. 4064 dpi (~22 MP), 2032 dpi (~5,5MP)&lt;/p&gt;
    &lt;p&gt;LENS&lt;/p&gt;
    &lt;p&gt;Custom 4 element lens with high MTF (modulation transfer function)&lt;/p&gt;
    &lt;p&gt;LIGHT SOURCE&lt;/p&gt;
    &lt;p&gt;RGB LED backlight&lt;/p&gt;
    &lt;head rend="h3"&gt;IMAGING SYSTEM&lt;/head&gt;
    &lt;p&gt;SENSOR&lt;/p&gt;
    &lt;p&gt;Backside illumintaed CMOS Sensor&lt;/p&gt;
    &lt;p&gt;DYNAMIC RANGE&lt;/p&gt;
    &lt;p&gt;linear dynamic range of 78 dB, expandable to 120 dB with native 16-bit HDR log profile, up to 14 stops of range&lt;/p&gt;
    &lt;p&gt;RESOLUTION&lt;/p&gt;
    &lt;p&gt;Max. 4064 dpi (~22 MP), 2032 dpi (~5,5MP)&lt;/p&gt;
    &lt;p&gt;LENS&lt;/p&gt;
    &lt;p&gt;Custom 4 element lens with high MTF (modulation transfer function)&lt;/p&gt;
    &lt;p&gt;LIGHT SOURCE&lt;/p&gt;
    &lt;p&gt;RGB LED backlight&lt;/p&gt;
    &lt;head rend="h3"&gt;PERFORMANCE &amp;amp; WORKFLOW&lt;/head&gt;
    &lt;p&gt;Scan Speed&lt;/p&gt;
    &lt;p&gt;per roll under 5 minutes (4064 dpi), under 2 minutes (2032 dpi)&lt;/p&gt;
    &lt;p&gt;FILM TRANSPORT&lt;/p&gt;
    &lt;p&gt;automated, min. strip length 3 images&lt;/p&gt;
    &lt;p&gt;FRAME CONTROL&lt;/p&gt;
    &lt;p&gt;per-frame scan settings, skip directly to any frame&lt;/p&gt;
    &lt;p&gt;DXN DECODER&lt;/p&gt;
    &lt;p&gt;reads 35 mm DX codes, embeds film type, ISO, roll info into metadata&lt;/p&gt;
    &lt;p&gt;SOFTWARE&lt;/p&gt;
    &lt;p&gt;Korova (native for Windows, macOS, Linux)&lt;/p&gt;
    &lt;p&gt;FILE FORMATS&lt;/p&gt;
    &lt;p&gt;RAW, TIFF, DNG linear, JPEG, PNG, BMP, HDR&lt;/p&gt;
    &lt;p&gt;FILE SIZES&lt;/p&gt;
    &lt;p&gt;RAW/TIFF/DNG linear ~127 MB; JPEG XL (lossless) 42â52 MB; PNG 106â118 MB&lt;/p&gt;
    &lt;head rend="h3"&gt;PERFORMANCE &amp;amp; WORKFLOW&lt;/head&gt;
    &lt;p&gt;Scan Speed&lt;/p&gt;
    &lt;p&gt;per roll under 5 minutes (4064 dpi), under 2 minutes (2032 dpi)&lt;/p&gt;
    &lt;p&gt;FILM TRANSPORT&lt;/p&gt;
    &lt;p&gt;automated, min. strip length 3 images&lt;/p&gt;
    &lt;p&gt;FRAME CONTROL&lt;/p&gt;
    &lt;p&gt;per-frame scan settings, skip directly to any frame&lt;/p&gt;
    &lt;p&gt;DXN DECODER&lt;/p&gt;
    &lt;p&gt;reads 35 mm DX codes, embeds film type, ISO, roll info into metadata&lt;/p&gt;
    &lt;p&gt;SOFTWARE&lt;/p&gt;
    &lt;p&gt;Korova (native for Windows, macOS, Linux)&lt;/p&gt;
    &lt;p&gt;FILE FORMATS&lt;/p&gt;
    &lt;p&gt;RAW, TIFF, DNG linear, JPEG, PNG, BMP, HDR&lt;/p&gt;
    &lt;p&gt;FILE SIZES&lt;/p&gt;
    &lt;p&gt;RAW/TIFF/DNG linear ~127 MB; JPEG XL (lossless) 42â52 MB; PNG 106â118 MB&lt;/p&gt;
    &lt;head rend="h3"&gt;HARDWARE&lt;/head&gt;
    &lt;p&gt;DIMENSIONS&lt;/p&gt;
    &lt;p&gt;250 Ã 150 Ã 63 mm&lt;/p&gt;
    &lt;p&gt;WEIGHT&lt;/p&gt;
    &lt;p&gt;1400 grams&lt;/p&gt;
    &lt;p&gt;INTERFACE&lt;/p&gt;
    &lt;p&gt;USB-C (USB 3.1)&lt;/p&gt;
    &lt;p&gt;POWER SUPPLY&lt;/p&gt;
    &lt;p&gt;18 V DC, 2 A (included)&lt;/p&gt;
    &lt;head rend="h3"&gt;HARDWARE&lt;/head&gt;
    &lt;p&gt;DIMENSIONS&lt;/p&gt;
    &lt;p&gt;250 Ã 150 Ã 63 mm&lt;/p&gt;
    &lt;p&gt;WEIGHT&lt;/p&gt;
    &lt;p&gt;1400 grams&lt;/p&gt;
    &lt;p&gt;INTERFACE&lt;/p&gt;
    &lt;p&gt;USB-C (USB 3.1)&lt;/p&gt;
    &lt;p&gt;POWER SUPPLY&lt;/p&gt;
    &lt;p&gt;18 V DC, 2 A (included)&lt;/p&gt;
    &lt;head rend="h2"&gt;Price at Launch&lt;/head&gt;
    &lt;head rend="h2"&gt;999â¬&lt;/head&gt;
    &lt;p&gt;Includes scanner + software&lt;/p&gt;
    &lt;head rend="h2"&gt;4064 dpi resolution&lt;/head&gt;
    &lt;head rend="h2"&gt;5 min per roll&lt;/head&gt;
    &lt;head rend="h2"&gt;48-bit colour depth&lt;/head&gt;
    &lt;head rend="h2"&gt;120 dB Dynamic Range&lt;/head&gt;
    &lt;head rend="h2"&gt;LED Matrix&lt;/head&gt;
    &lt;head rend="h2"&gt;RGB LED backlight&lt;/head&gt;
    &lt;head rend="h2"&gt;USB-C 3.2&lt;/head&gt;
    &lt;head rend="h2"&gt;Custom software&lt;/head&gt;
    &lt;head rend="h2"&gt;Frequently &lt;lb/&gt;Asked &lt;lb/&gt;Questions.&lt;/head&gt;
    &lt;p&gt;Will Knokke support 120 film, panoramic formats, or border scanning?&lt;/p&gt;
    &lt;p&gt;Knokke fully supports any frame width on 35 mm (135) film, thanks to automatic edge detection. It can also perform partial border scans to preserve maximum resolution and frame content. (120 film support is not part of the first release but is under consideration for the future.)&lt;/p&gt;
    &lt;p&gt;What kind of light source and sensor does Knokke use?&lt;/p&gt;
    &lt;p&gt;Knokke uses an RGB LED backlight, precisely wavelength-matched to a modern backside-illuminated CMOS sensor. The sensor features 2 Î¼m pixels and delivers a 78 dB linear dynamic range in 12-bit mode, expandable to 120 dB (â 14 stops) with the 3-step HDR log mode at native 16-bit output. This combination ensures accurate colour reproduction, tonal smoothness, and detail retention in both highlights and shadows.&lt;/p&gt;
    &lt;p&gt;What makes Knokke different from other scanners like the Fuji Frontier, Nikon Coolscan, or camera-stand setups?&lt;/p&gt;
    &lt;p&gt;Knokke combines speed, scanning quality, and ease of use in a compact form factor. It scans a full 35 mm roll in under 5 minutes, offers per-frame customisation, and requires no legacy computer hardware or drivers.&lt;/p&gt;
    &lt;p&gt;Will example scans be shared before launch?&lt;/p&gt;
    &lt;p&gt;Yes. Weâre collaborating with several film labs in Berlin to benchmark Knokke against Fuji Frontier and Noritsu scanners. Sample results will be published before the Kickstarter campaign, so you can make a fully informed decision.&lt;/p&gt;
    &lt;p&gt;Is the software open source?&lt;/p&gt;
    &lt;p&gt;Yes. Our control application, Korova, will be fully open source and maintained long term. Itâs a native, lightweight application for Windows, macOS, and Linux.&lt;/p&gt;
    &lt;p&gt;How much is Knokke going to cost?&lt;/p&gt;
    &lt;p&gt;Knokke will cost 999â¬ at launch. We are still working on bringing the price down further threw optimising design and sourcing. It's final retail price is set at 1599â¬.&lt;/p&gt;
    &lt;p&gt;Is Knokke open, repairable, and long-term supported?&lt;/p&gt;
    &lt;p&gt;Absolutely. Weâre committed to building a scanner that lasts decades. All schematics and repair manuals will be publicly available, replacement parts can be purchased directly, and the software will remain supported for as long as possible.&lt;/p&gt;
    &lt;p&gt;When will Knokke be available?&lt;/p&gt;
    &lt;p&gt;Knokke will launch on Kickstarter.com in Q1 2026. Follow us on Instagram and subscribe to our newsletter to be among the first notified about updates.&lt;/p&gt;
    &lt;p&gt;Can I become a beta tester?&lt;/p&gt;
    &lt;p&gt;Thank you for your interest! Weâve received an incredible number of requests to join our beta testing program. Weâll be running two separate testing rounds - one in collaboration with selected creators and film labs, and another open to members of our community.&lt;/p&gt;
    &lt;p&gt;Will Knokke support 120 film, panoramic formats, or border scanning?&lt;/p&gt;
    &lt;p&gt;Knokke fully supports any frame width on 35 mm (135) film, thanks to automatic edge detection. It can also perform partial border scans to preserve maximum resolution and frame content. (120 film support is not part of the first release but is under consideration for the future.)&lt;/p&gt;
    &lt;p&gt;What kind of light source and sensor does Knokke use?&lt;/p&gt;
    &lt;p&gt;Knokke uses an RGB LED backlight, precisely wavelength-matched to a modern backside-illuminated CMOS sensor. The sensor features 2 Î¼m pixels and delivers a 78 dB linear dynamic range in 12-bit mode, expandable to 120 dB (â 14 stops) with the 3-step HDR log mode at native 16-bit output. This combination ensures accurate colour reproduction, tonal smoothness, and detail retention in both highlights and shadows.&lt;/p&gt;
    &lt;p&gt;What makes Knokke different from other scanners like the Fuji Frontier, Nikon Coolscan, or camera-stand setups?&lt;/p&gt;
    &lt;p&gt;Knokke combines speed, scanning quality, and ease of use in a compact form factor. It scans a full 35 mm roll in under 5 minutes, offers per-frame customisation, and requires no legacy computer hardware or drivers.&lt;/p&gt;
    &lt;p&gt;Will example scans be shared before launch?&lt;/p&gt;
    &lt;p&gt;Yes. Weâre collaborating with several film labs in Berlin to benchmark Knokke against Fuji Frontier and Noritsu scanners. Sample results will be published before the Kickstarter campaign, so you can make a fully informed decision.&lt;/p&gt;
    &lt;p&gt;Is the software open source?&lt;/p&gt;
    &lt;p&gt;Yes. Our control application, Korova, will be fully open source and maintained long term. Itâs a native, lightweight application for Windows, macOS, and Linux.&lt;/p&gt;
    &lt;p&gt;How much is Knokke going to cost?&lt;/p&gt;
    &lt;p&gt;Knokke will cost 999â¬ at launch. We are still working on bringing the price down further threw optimising design and sourcing. It's final retail price is set at 1599â¬.&lt;/p&gt;
    &lt;p&gt;Is Knokke open, repairable, and long-term supported?&lt;/p&gt;
    &lt;p&gt;Absolutely. Weâre committed to building a scanner that lasts decades. All schematics and repair manuals will be publicly available, replacement parts can be purchased directly, and the software will remain supported for as long as possible.&lt;/p&gt;
    &lt;p&gt;When will Knokke be available?&lt;/p&gt;
    &lt;p&gt;Knokke will launch on Kickstarter.com in Q1 2026. Follow us on Instagram and subscribe to our newsletter to be among the first notified about updates.&lt;/p&gt;
    &lt;p&gt;Can I become a beta tester?&lt;/p&gt;
    &lt;p&gt;Thank you for your interest! Weâve received an incredible number of requests to join our beta testing program. Weâll be running two separate testing rounds - one in collaboration with selected creators and film labs, and another open to members of our community.&lt;/p&gt;
    &lt;head rend="h2"&gt;999â¬&lt;/head&gt;
    &lt;p&gt;Includes scanner + software&lt;/p&gt;
    &lt;head rend="h2"&gt;4064 dpi resolution&lt;/head&gt;
    &lt;head rend="h2"&gt;5 min per roll&lt;/head&gt;
    &lt;head rend="h2"&gt;48-bit colour depth&lt;/head&gt;
    &lt;head rend="h2"&gt;120 dB Dynamic Range&lt;/head&gt;
    &lt;head rend="h2"&gt;LED Matrix&lt;/head&gt;
    &lt;head rend="h2"&gt;RGB LED backlight&lt;/head&gt;
    &lt;head rend="h2"&gt;USB-C 3.2&lt;/head&gt;
    &lt;head rend="h2"&gt;Custom software&lt;/head&gt;
    &lt;head rend="h2"&gt;Frequently Asked Questions.&lt;/head&gt;
    &lt;p&gt;Will Knokke support 120 film, panoramic formats, or border scanning?&lt;/p&gt;
    &lt;p&gt;Knokke fully supports any frame width on 35 mm (135) film, thanks to automatic edge detection. It can also perform partial border scans to preserve maximum resolution and frame content. (120 film support is not part of the first release but is under consideration for the future.)&lt;/p&gt;
    &lt;p&gt;What kind of light source and sensor does Knokke use?&lt;/p&gt;
    &lt;p&gt;Knokke uses an RGB LED backlight, precisely wavelength-matched to a modern backside-illuminated CMOS sensor. The sensor features 2 Î¼m pixels and delivers a 78 dB linear dynamic range in 12-bit mode, expandable to 120 dB (â 14 stops) with the 3-step HDR log mode at native 16-bit output. This combination ensures accurate colour reproduction, tonal smoothness, and detail retention in both highlights and shadows.&lt;/p&gt;
    &lt;p&gt;What makes Knokke different from other scanners like the Fuji Frontier, Nikon Coolscan, or camera-stand setups?&lt;/p&gt;
    &lt;p&gt;Knokke combines speed, scanning quality, and ease of use in a compact form factor. It scans a full 35 mm roll in under 5 minutes, offers per-frame customisation, and requires no legacy computer hardware or drivers.&lt;/p&gt;
    &lt;p&gt;Will example scans be shared before launch?&lt;/p&gt;
    &lt;p&gt;Yes. Weâre collaborating with several film labs in Berlin to benchmark Knokke against Fuji Frontier and Noritsu scanners. Sample results will be published before the Kickstarter campaign, so you can make a fully informed decision.&lt;/p&gt;
    &lt;p&gt;How much is Knokke going to cost?&lt;/p&gt;
    &lt;p&gt;Knokke will cost 999â¬ at launch. We are still working on bringing the price down further threw optimising design and sourcing. It's final retail price is set at 1599â¬.&lt;/p&gt;
    &lt;p&gt;Is the software open source?&lt;/p&gt;
    &lt;p&gt;Yes. Our control application, Korova, will be fully open source and maintained long term. Itâs a native, lightweight application for Windows, macOS, and Linux.&lt;/p&gt;
    &lt;p&gt;Is Knokke open, repairable, and long-term supported?&lt;/p&gt;
    &lt;p&gt;Absolutely. Weâre committed to building a scanner that lasts decades. All schematics and repair manuals will be publicly available, replacement parts can be purchased directly, and the software will remain supported for as long as possible.&lt;/p&gt;
    &lt;p&gt;When will Knokke be available?&lt;/p&gt;
    &lt;p&gt;Knokke will launch on Kickstarter.com in Q1 2026. Follow us on Instagram and subscribe to our newsletter to be among the first notified about updates.&lt;/p&gt;
    &lt;p&gt;Can I become a beta tester?&lt;/p&gt;
    &lt;p&gt;Knokke will launch on Kickstarter in Q1 2026. Follow us on Instagram or subscribe to our newsletter to be among the first notified when pre-orders open.&lt;/p&gt;
    &lt;p&gt;Will Knokke support 120 film, panoramic formats, or border scanning?&lt;/p&gt;
    &lt;p&gt;Knokke fully supports any frame width on 35 mm (135) film, thanks to automatic edge detection. It can also perform partial border scans to preserve maximum resolution and frame content. (120 film support is not part of the first release but is under consideration for the future.)&lt;/p&gt;
    &lt;p&gt;What kind of light source and sensor does Knokke use?&lt;/p&gt;
    &lt;p&gt;Knokke uses an RGB LED backlight, precisely wavelength-matched to a modern backside-illuminated CMOS sensor. The sensor features 2 Î¼m pixels and delivers a 78 dB linear dynamic range in 12-bit mode, expandable to 120 dB (â 14 stops) with the 3-step HDR log mode at native 16-bit output. This combination ensures accurate colour reproduction, tonal smoothness, and detail retention in both highlights and shadows.&lt;/p&gt;
    &lt;p&gt;What makes Knokke different from other scanners like the Fuji Frontier, Nikon Coolscan, or camera-stand setups?&lt;/p&gt;
    &lt;p&gt;Knokke combines speed, scanning quality, and ease of use in a compact form factor. It scans a full 35 mm roll in under 5 minutes, offers per-frame customisation, and requires no legacy computer hardware or drivers.&lt;/p&gt;
    &lt;p&gt;Will example scans be shared before launch?&lt;/p&gt;
    &lt;p&gt;Yes. Weâre collaborating with several film labs in Berlin to benchmark Knokke against Fuji Frontier and Noritsu scanners. Sample results will be published before the Kickstarter campaign, so you can make a fully informed decision.&lt;/p&gt;
    &lt;p&gt;How much is Knokke going to cost?&lt;/p&gt;
    &lt;p&gt;Knokke will cost 999â¬ at launch. We are still working on bringing the price down further threw optimising design and sourcing. It's final retail price is set at 1599â¬.&lt;/p&gt;
    &lt;p&gt;Is the software open source?&lt;/p&gt;
    &lt;p&gt;Yes. Our control application, Korova, will be fully open source and maintained long term. Itâs a native, lightweight application for Windows, macOS, and Linux.&lt;/p&gt;
    &lt;p&gt;Is Knokke open, repairable, and long-term supported?&lt;/p&gt;
    &lt;p&gt;Absolutely. Weâre committed to building a scanner that lasts decades. All schematics and repair manuals will be publicly available, replacement parts can be purchased directly, and the software will remain supported for as long as possible.&lt;/p&gt;
    &lt;p&gt;When will Knokke be available?&lt;/p&gt;
    &lt;p&gt;Knokke will launch on Kickstarter.com in Q1 2026. Follow us on Instagram and subscribe to our newsletter to be among the first notified about updates.&lt;/p&gt;
    &lt;p&gt;Can I become a beta tester?&lt;/p&gt;
    &lt;p&gt;Knokke will launch on Kickstarter in Q1 2026. Follow us on Instagram or subscribe to our newsletter to be among the first notified when pre-orders open.&lt;/p&gt;
    &lt;head rend="h4"&gt;Engineered for individual and lab use&lt;/head&gt;
    &lt;head rend="h6"&gt;01&lt;/head&gt;
    &lt;head rend="h6"&gt;Quality&lt;/head&gt;
    &lt;p&gt;Knokkeâs premium build and precision engineering ensure lasting, reliable performance.&lt;/p&gt;
    &lt;head rend="h6"&gt;01&lt;/head&gt;
    &lt;head rend="h6"&gt;Quality&lt;/head&gt;
    &lt;p&gt;Knokkeâs premium build and precision engineering ensure lasting, reliable performance.&lt;/p&gt;
    &lt;head rend="h6"&gt;02&lt;/head&gt;
    &lt;head rend="h6"&gt;Speed&lt;/head&gt;
    &lt;p&gt;Knokkeâs high scan speed and streamlined workflow keep you moving.&lt;/p&gt;
    &lt;head rend="h6"&gt;02&lt;/head&gt;
    &lt;head rend="h6"&gt;Speed&lt;/head&gt;
    &lt;p&gt;Knokkeâs high scan speed and streamlined workflow keep you moving.&lt;/p&gt;
    &lt;head rend="h6"&gt;03&lt;/head&gt;
    &lt;head rend="h6"&gt;Full Control&lt;/head&gt;
    &lt;p&gt;Knokke lets you fine-tune every detail with flexible settings and precise color control.&lt;/p&gt;
    &lt;head rend="h6"&gt;03&lt;/head&gt;
    &lt;head rend="h6"&gt;Full Control&lt;/head&gt;
    &lt;p&gt;Knokke lets you fine-tune every detail with flexible settings and precise color control.&lt;/p&gt;
    &lt;head rend="h6"&gt;04&lt;/head&gt;
    &lt;head rend="h6"&gt;Future Proof&lt;/head&gt;
    &lt;p&gt;Knokke comes with ongoing software support, open-source flexibility, and readily available spare parts.&lt;/p&gt;
    &lt;head rend="h6"&gt;04&lt;/head&gt;
    &lt;head rend="h6"&gt;Future Proof&lt;/head&gt;
    &lt;p&gt;Knokke comes with ongoing software support, open-source flexibility, and readily available spare parts.&lt;/p&gt;
    &lt;head rend="h2"&gt;A Closer Look at Knokke&lt;/head&gt;
    &lt;head rend="h2"&gt;A Closer Look at Knokke&lt;/head&gt;
    &lt;head rend="h2"&gt;Specifications.&lt;/head&gt;
    &lt;head rend="h2"&gt;Specifications.&lt;/head&gt;
    &lt;head rend="h3"&gt;IMAGING SYSTEM&lt;/head&gt;
    &lt;p&gt;SENSOR&lt;/p&gt;
    &lt;p&gt;Backside illuminated CMOS Sensor&lt;/p&gt;
    &lt;p&gt;DYNAMIC RANGE&lt;/p&gt;
    &lt;p&gt;linear dynamic range of 78 dB, expandable to 120 dB with native 16-bit HDR log profile, up to 14 stops of range&lt;/p&gt;
    &lt;p&gt;RESOLUTION&lt;/p&gt;
    &lt;p&gt;Max. 4064 dpi (~22 MP), 2032 dpi (~5,5MP)&lt;/p&gt;
    &lt;p&gt;LENS&lt;/p&gt;
    &lt;p&gt;Custom 4 element lens with high MTF (modulation transfer function)&lt;/p&gt;
    &lt;p&gt;LIGHT SOURCE&lt;/p&gt;
    &lt;p&gt;RGB LED backlight&lt;/p&gt;
    &lt;head rend="h3"&gt;IMAGING SYSTEM&lt;/head&gt;
    &lt;p&gt;SENSOR&lt;/p&gt;
    &lt;p&gt;Backside illuminated CMOS Sensor&lt;/p&gt;
    &lt;p&gt;DYNAMIC RANGE&lt;/p&gt;
    &lt;p&gt;linear dynamic range of 78 dB, expandable to 120 dB with native 16-bit HDR log profile, up to 14 stops of range&lt;/p&gt;
    &lt;p&gt;RESOLUTION&lt;/p&gt;
    &lt;p&gt;Max. 4064 dpi (~22 MP), 2032 dpi (~5,5MP)&lt;/p&gt;
    &lt;p&gt;LENS&lt;/p&gt;
    &lt;p&gt;Custom 4 element lens with high MTF (modulation transfer function)&lt;/p&gt;
    &lt;p&gt;LIGHT SOURCE&lt;/p&gt;
    &lt;p&gt;RGB LED backlight&lt;/p&gt;
    &lt;head rend="h3"&gt;PERFORMANCE &amp;amp; WORKFLOW&lt;/head&gt;
    &lt;p&gt;Scan Speed&lt;/p&gt;
    &lt;p&gt;per roll under 5 minutes (4064 dpi), under 2 minutes (2032 dpi)&lt;/p&gt;
    &lt;p&gt;FILM TRANSPORT&lt;/p&gt;
    &lt;p&gt;automated, min. strip length 3 images&lt;/p&gt;
    &lt;p&gt;FRAME CONTROL&lt;/p&gt;
    &lt;p&gt;per-frame scan settings, skip directly to any frame&lt;/p&gt;
    &lt;p&gt;DXN DECODER&lt;/p&gt;
    &lt;p&gt;reads 35 mm DX codes, embeds film type, ISO, roll info into metadata&lt;/p&gt;
    &lt;p&gt;SOFTWARE&lt;/p&gt;
    &lt;p&gt;Korova (native for Windows, macOS, Linux)&lt;/p&gt;
    &lt;p&gt;FILE FORMATS&lt;/p&gt;
    &lt;p&gt;RAW, TIFF, DNG linear, JPEG, PNG, BMP, HDR&lt;/p&gt;
    &lt;p&gt;FILE SIZES&lt;/p&gt;
    &lt;p&gt;RAW/TIFF/DNG linear ~127 MB; JPEG XL (lossless) 42â52 MB; PNG 106â118 MB&lt;/p&gt;
    &lt;head rend="h3"&gt;PERFORMANCE &amp;amp; WORKFLOW&lt;/head&gt;
    &lt;p&gt;Scan Speed&lt;/p&gt;
    &lt;p&gt;per roll under 5 minutes (4064 dpi), under 2 minutes (2032 dpi)&lt;/p&gt;
    &lt;p&gt;FILM TRANSPORT&lt;/p&gt;
    &lt;p&gt;automated, min. strip length 3 images&lt;/p&gt;
    &lt;p&gt;FRAME CONTROL&lt;/p&gt;
    &lt;p&gt;per-frame scan settings, skip directly to any frame&lt;/p&gt;
    &lt;p&gt;DXN DECODER&lt;/p&gt;
    &lt;p&gt;reads 35 mm DX codes, embeds film type, ISO, roll info into metadata&lt;/p&gt;
    &lt;p&gt;SOFTWARE&lt;/p&gt;
    &lt;p&gt;Korova (native for Windows, macOS, Linux)&lt;/p&gt;
    &lt;p&gt;FILE FORMATS&lt;/p&gt;
    &lt;p&gt;RAW, TIFF, DNG linear, JPEG, PNG, BMP, HDR&lt;/p&gt;
    &lt;p&gt;FILE SIZES&lt;/p&gt;
    &lt;p&gt;RAW/TIFF/DNG linear ~127 MB; JPEG XL (lossless) 42â52 MB; PNG 106â118 MB&lt;/p&gt;
    &lt;head rend="h3"&gt;HARDWARE&lt;/head&gt;
    &lt;p&gt;DIMENSIONS&lt;/p&gt;
    &lt;p&gt;250 Ã 150 Ã 63 mm&lt;/p&gt;
    &lt;p&gt;WEIGHT&lt;/p&gt;
    &lt;p&gt;1400 grams&lt;/p&gt;
    &lt;p&gt;INTERFACE&lt;/p&gt;
    &lt;p&gt;USB-C (USB 3.1)&lt;/p&gt;
    &lt;p&gt;POWER SUPPLY&lt;/p&gt;
    &lt;p&gt;18 V DC, 2 A (included)&lt;/p&gt;
    &lt;head rend="h3"&gt;HARDWARE&lt;/head&gt;
    &lt;p&gt;DIMENSIONS&lt;/p&gt;
    &lt;p&gt;250 Ã 150 Ã 63 mm&lt;/p&gt;
    &lt;p&gt;WEIGHT&lt;/p&gt;
    &lt;p&gt;1400 grams&lt;/p&gt;
    &lt;p&gt;INTERFACE&lt;/p&gt;
    &lt;p&gt;USB-C (USB 3.1)&lt;/p&gt;
    &lt;p&gt;POWER SUPPLY&lt;/p&gt;
    &lt;p&gt;18 V DC, 2 A (included)&lt;/p&gt;
    &lt;head rend="h3"&gt;Engineered for individual and lab use&lt;/head&gt;
    &lt;head rend="h6"&gt;01&lt;/head&gt;
    &lt;head rend="h6"&gt;Quality&lt;/head&gt;
    &lt;p&gt;Knokkeâs premium build and precision engineering ensure lasting, reliable performance.&lt;/p&gt;
    &lt;head rend="h6"&gt;01&lt;/head&gt;
    &lt;head rend="h6"&gt;Quality&lt;/head&gt;
    &lt;p&gt;Knokkeâs premium build and precision engineering ensure lasting, reliable performance.&lt;/p&gt;
    &lt;head rend="h6"&gt;02&lt;/head&gt;
    &lt;head rend="h6"&gt;Speed&lt;/head&gt;
    &lt;p&gt;Knokkeâs high scan speed and streamlined workflow keep you moving.&lt;/p&gt;
    &lt;head rend="h6"&gt;02&lt;/head&gt;
    &lt;head rend="h6"&gt;Speed&lt;/head&gt;
    &lt;p&gt;Knokkeâs high scan speed and streamlined workflow keep you moving.&lt;/p&gt;
    &lt;head rend="h6"&gt;03&lt;/head&gt;
    &lt;head rend="h6"&gt;Full Control&lt;/head&gt;
    &lt;p&gt;Knokke lets you fine-tune every detail with flexible settings and precise color control.&lt;/p&gt;
    &lt;head rend="h6"&gt;03&lt;/head&gt;
    &lt;head rend="h6"&gt;Full Control&lt;/head&gt;
    &lt;p&gt;Knokke lets you fine-tune every detail with flexible settings and precise color control.&lt;/p&gt;
    &lt;head rend="h6"&gt;04&lt;/head&gt;
    &lt;head rend="h6"&gt;Future Proof&lt;/head&gt;
    &lt;p&gt;Knokke comes with ongoing software support, open-source flexibility, and readily available spare parts.&lt;/p&gt;
    &lt;head rend="h6"&gt;04&lt;/head&gt;
    &lt;head rend="h6"&gt;Future Proof&lt;/head&gt;
    &lt;p&gt;Knokke comes with ongoing software support, open-source flexibility, and readily available spare parts.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45891907</guid><pubDate>Tue, 11 Nov 2025 19:48:19 +0000</pubDate></item><item><title>Adk-go: code-first Go toolkit for building, evaluating, and deploying AI agents</title><link>https://github.com/google/adk-go</link><description>&lt;doc fingerprint="3d738c52c9d3a6e3"&gt;
  &lt;main&gt;
    &lt;head rend="h3"&gt;An open-source, code-first Go toolkit for building, evaluating, and deploying sophisticated AI agents with flexibility and control.&lt;/head&gt;
    &lt;head rend="h3"&gt;Important Links: Docs &amp;amp; Samples &amp;amp; Python ADK &amp;amp; Java ADK &amp;amp; ADK Web.&lt;/head&gt;
    &lt;p&gt;Agent Development Kit (ADK) is a flexible and modular framework that applies software development principles to AI agent creation. It is designed to simplify building, deploying, and orchestrating agent workflows, from simple tasks to complex systems. While optimized for Gemini, ADK is model-agnostic, deployment-agnostic, and compatible with other frameworks.&lt;/p&gt;
    &lt;p&gt;This Go version of ADK is ideal for developers building cloud-native agent applications, leveraging Go's strengths in concurrency and performance.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Idiomatic Go: Designed to feel natural and leverage the power of Go.&lt;/item&gt;
      &lt;item&gt;Rich Tool Ecosystem: Utilize pre-built tools, custom functions, or integrate existing tools to give agents diverse capabilities.&lt;/item&gt;
      &lt;item&gt;Code-First Development: Define agent logic, tools, and orchestration directly in Go for ultimate flexibility, testability, and versioning.&lt;/item&gt;
      &lt;item&gt;Modular Multi-Agent Systems: Design scalable applications by composing multiple specialized agents.&lt;/item&gt;
      &lt;item&gt;Deploy Anywhere: Easily containerize and deploy agents, with strong support for cloud-native environments like Google Cloud Run.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To add ADK Go to your project, run:&lt;/p&gt;
    &lt;code&gt;go get google.golang.org/adk&lt;/code&gt;
    &lt;p&gt;This project is licensed under the Apache 2.0 License - see the LICENSE file for details.&lt;/p&gt;
    &lt;p&gt;The exception is internal/httprr - see its LICENSE file.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45891968</guid><pubDate>Tue, 11 Nov 2025 19:52:55 +0000</pubDate></item><item><title>Xortran - A PDP-11 Neural Network With Backpropagation in Fortran IV</title><link>https://github.com/dbrll/Xortran</link><description>&lt;doc fingerprint="7293ba097f3c7842"&gt;
  &lt;main&gt;
    &lt;p&gt;XORTRAN is a multilayer perceptron (MLP) written in FORTRAN IV, compiled and executed under RT-11 on a PDP-11/34A (via SIMH simulator).&lt;/p&gt;
    &lt;p&gt;It learns the classic non-linear XOR problem using:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;One hidden layer (4 neurons, leaky ReLU activation)&lt;/item&gt;
      &lt;item&gt;Backpropagation with mean squared error loss&lt;/item&gt;
      &lt;item&gt;He-like initialization (manual Gaussian via Box-Muller lite)&lt;/item&gt;
      &lt;item&gt;Learning rate annealing (0.5 → 0.1 → 0.01)&lt;/item&gt;
      &lt;item&gt;Tanh output&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The code compiles with the DEC FORTRAN IV compiler (1974). Execution requires a system with at least 32 kilobytes of memory and an FP11 floating-point processor. The PDP-11/34A was chosen as it was the smallest and most affordable PDP-11 equipped with an FP11 floating-point processor in the 1970s.&lt;/p&gt;
    &lt;p&gt;The training of the 17 parameters should take less than a couple minutes on the real hardware. In SIMH, setting the throttle to 500K (&lt;code&gt;set throttle 500K&lt;/code&gt;) will
provide a more realistic execution speed.&lt;/p&gt;
    &lt;p&gt;The output shows the mean squared loss every 100 epochs, followed by the final predictions from the forward pass.&lt;/p&gt;
    &lt;p&gt;The network converges towards the expected XOR outputs after a few hundred epochs, gradually reducing the error until it accurately approximates the desired results.&lt;/p&gt;
    &lt;code&gt;.RUN XORTRN
   1  0.329960233835D+00
 100  0.195189856059D+00
 200  0.816064184115D-01
 300  0.654882376056D-02
 400  0.109833284544D-02
 500  0.928130032748D-03

0 0 GOT:0.008353 EXPECTED:0.
0 1 GOT:0.979327 EXPECTED:1.
1 0 GOT:0.947050 EXPECTED:1.
1 1 GOT:0.020147 EXPECTED:0.
STOP --
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;In SIMH, attach the RL1 drive (&lt;/p&gt;&lt;code&gt;ATT RL1 xortran.rl1&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;In RT-11 (I use the single job RT-11-SJ V5), assuming&lt;/p&gt;&lt;code&gt;DL1:&lt;/code&gt;is assigned to&lt;code&gt;DK:&lt;/code&gt;:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;.FORTRAN/LIST:XORTRN.LST XORTRN.FOR
.LINK XORTRN.OBJ,FORLIB
.RUN XORTRN
&lt;/code&gt;
    &lt;p&gt;Or if you just want to run the binary:&lt;/p&gt;
    &lt;code&gt;.RUN DL1:XORTRN
&lt;/code&gt;
    &lt;p&gt;This project demonstrates that a minimal FORTRAN IV environment from the 1970s was sufficient to implement a basic neural network with backpropagation.&lt;lb/&gt; It’s both a retro-computing curiosity and a small historical experiment bridging early scientific computing and modern machine learning.&lt;/p&gt;
    &lt;p&gt;© 2025 Damien Boureille&lt;/p&gt;
    &lt;p&gt;This code is released under the MIT License.&lt;lb/&gt; You are free to use, copy, modify, and redistribute it, provided that you credit the original author.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45892174</guid><pubDate>Tue, 11 Nov 2025 20:10:24 +0000</pubDate></item><item><title>The terminal of the future</title><link>https://jyn.dev/the-terminal-of-the-future</link><description>&lt;doc fingerprint="37eac95c22846b89"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;the terminal of the future&lt;/head&gt;
    &lt;p&gt;This post is part 6 of a multi-part series called “the computer of the next 200 years”.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Terminal internals are a mess. A lot of it is just the way it is because someone made a decision in the 80s and now it’s impossible to change. —Julia Evans&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;This is what you have to do to redesign infrastructure. Rich [Hickey] didn't just pile some crap on top of Lisp [when building Clojure]. He took the entire Lisp and moved the whole design at once. —Gary Bernhardt&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;a mental model of a terminal&lt;/head&gt;
    &lt;p&gt;At a very very high level, a terminal has four parts:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;The "terminal emulator", which is a program that renders a grid-like structure to your graphical display.&lt;/item&gt;
      &lt;item&gt;The "pseudo-terminal" (PTY), which is a connection between the terminal emulator and a "process group" which receives input. This is not a program. This is a piece of state in the kernel.&lt;/item&gt;
      &lt;item&gt;The "shell", which is a program that leads the "process group", reads and parses input, spawns processes, and generally acts as an event loop. Most environments use bash as the default shell.&lt;/item&gt;
      &lt;item&gt;The programs spawned by your shell, which interact with all of the above in order to receive input and send output.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I lied a little bit above. "input" is not just text. It also includes signals that can be sent to the running process. Converting keystrokes to signals is the job of the PTY.&lt;/p&gt;
    &lt;p&gt;Similar, "output" is not just text. It's a stream of ANSI Escape Sequences that can be used by the terminal emulator to display rich formatting.&lt;/p&gt;
    &lt;head rend="h2"&gt;what does a better terminal look like?&lt;/head&gt;
    &lt;p&gt;I do some weird things with terminals. However, the amount of hacks I can get up to are pretty limited, because terminals are pretty limited. I won't go into all the ways they're limited, because it's been rehashed many times before. What I want to do instead is imagine what a better terminal can look like.&lt;/p&gt;
    &lt;head rend="h3"&gt;a first try: Jupyter&lt;/head&gt;
    &lt;p&gt;The closest thing to a terminal analog that most people are familiar with is Jupyter Notebook. This offers a lot of cool features that are not possible in a "traditional" VT100 emulator:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;high fidelity image rendering&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;a "rerun from start" button (or rerun the current command; or rerun only a single past command) that replaces past output instead of appending to it&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;"views" of source code and output that can be rewritten in place (e.g. markdown can be viewed either as source or as rendered HTML)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;a built-in editor with syntax highlighting, tabs, panes, mouse support, etc.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;some problems&lt;/head&gt;
    &lt;p&gt;Jupyter works by having a "kernel" (in this case, a python interpreter) and a "renderer" (in this case, a web application displayed by the browser). You could imagine using a Jupyter Notebook with a shell as the kernel, so that you get all the nice features of Jupyter when running shell commands. However, that quickly runs into some issues:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Your shell gets the commands all at once, not character-by-character, so tab-complete, syntax highlighting, and autosuggestions don't work.&lt;/item&gt;
      &lt;item&gt;What do you do about long-lived processes? By default, Jupyter runs a cell until completion; you can cancel it, but you can't suspend, resume, interact with, nor view a process while it's running. Don't even think about running &lt;code&gt;vi&lt;/code&gt;or&lt;code&gt;top&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;The "rerun cell" buttons do horrible things to the state of your computer (normal Jupyter kernels have this problem too, but "rerun all" works better when the commands don't usually include &lt;code&gt;rm -rf&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;Undo/redo do not work. (They don't work in a normal terminal either, but people attempt to use them more when it looks like they should be able to.)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It turns out all these problems are solveable.&lt;/p&gt;
    &lt;head rend="h2"&gt;how does that work?&lt;/head&gt;
    &lt;head rend="h3"&gt;shell integration&lt;/head&gt;
    &lt;p&gt;There exists today a terminal called Warp. Warp has built native integration between the terminal and the shell, where the terminal understands where each command starts and stops, what it outputs, and what is your own input. As a result, it can render things very prettily:&lt;/p&gt;
    &lt;p&gt;It does this using (mostly) standard features built-in to the terminal and shell (a custom DCS): you can read their explanation here. It's possible to do this less invasively using OSC 133 escape codes; I'm not sure why Warp didn't do this, but that's ok.&lt;/p&gt;
    &lt;p&gt;iTerm2 does a similar thing, and this allows it to enable really quite a lot of features: navigating between commands with a single hotkey; notifying you when a command finishes running, showing the current command as an "overlay" if the output goes off the screen.&lt;/p&gt;
    &lt;head rend="h3"&gt;long-lived processes&lt;/head&gt;
    &lt;p&gt;This is really three different things. The first is interacting with a long-lived process. The second is suspending the process without killing it. The third is disconnecting from the process, in such a way that the process state is not disturbed and is still available if you want to reconnect.&lt;/p&gt;
    &lt;head rend="h4"&gt;interacting&lt;/head&gt;
    &lt;p&gt;To interact with a process, you need bidirectional communication, i.e. you need a "cell output" that is also an input. An example would be any TUI, like &lt;code&gt;top&lt;/code&gt;, &lt;code&gt;gdb&lt;/code&gt;, or &lt;code&gt;vim&lt;/code&gt; 1.  Fortunately, Jupyter is really good at this!  The whole design is around having interactive outputs that you can change and update.&lt;/p&gt;
    &lt;p&gt;Additionally, I would expect my terminal to always have a "free input cell", as Matklad describes in A Better Shell, where the interactive process runs in the top half of the window and an input cell is available in the bottom half. Jupyter can do this today, but "add a cell" is manual, not automatic.&lt;/p&gt;
    &lt;head rend="h4"&gt;suspending&lt;/head&gt;
    &lt;p&gt;"Suspending" a process is usually called "job control". There's not too much to talk about here, except that I would expect a "modern" terminal to show me all suspended and background processes as a de-emphasized persistent visual, kinda like how Intellij will show you "indexing ..." in the bottom taskbar.&lt;/p&gt;
    &lt;head rend="h4"&gt;disconnecting&lt;/head&gt;
    &lt;p&gt;There are roughly three existing approaches for disconnecting and reconnecting to a terminal session (Well, four if you count reptyr).&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Tmux / Zellij / Screen&lt;/p&gt;
        &lt;p&gt;These tools inject a whole extra terminal emulator between your terminal emulator and the program. They work by having a "server" which actually owns the PTY and renders the output, and a "client" that displays the output to your "real" terminal emulator. This model lets you detach clients, reattach them later, or even attach multiple clients at once. You can think of this as a "batteries-included" approach. It also has the benefit that you can program both the client and the server (although many modern terminals, like Kitty and Wezterm are programmable now); that you can organize your tabs and windows in the terminal (although many modern desktop environments have tiling and thorough keyboard shortcuts); and that you get street cred for looking like Hackerman.&lt;/p&gt;
        &lt;p&gt;The downside is that, well, now you have an extra terminal emulator running in your terminal, with all the bugs that implies.&lt;/p&gt;
        &lt;p&gt;iTerm actually avoids this by bypassing the tmux client altogether and acting as its own client that talks directly to the server. In this mode, "tmux tabs" are actually iTerm tabs, "tmux panes" are iTerm panes, and so on. This is a good model, and I would adopt it when writing a future terminal for integration with existing tmux setups.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Mosh is a really interesting place in the design space. It is not a terminal emulator replacement; instead it is an ssh replacement. Its big draw is that it supports reconnecting to your terminal session after a network interruption. It does that by running a state machine on the server and replaying an incremental diff of the viewport to the client. This is a similar model to tmux, except that it doesn't support the "multiplexing" part (it expects your terminal emulator to handle that), nor scrollback (ditto). Because it has its own renderer, it has a similar class of bugs to tmux. One feature it does have, unlike tmux, is that the "client" is really running on your side of the network, so local line editing is instant.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;alden/shpool/dtach/abduco/diss&lt;/p&gt;
        &lt;p&gt;These all occupy a similar place in the design space: they only handle session detach/resume with a client/server, not networking or scrollback, and do not include their own terminal emulator. Compared to tmux and mosh, they are highly decoupled.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;rerun and undo/redo&lt;/head&gt;
    &lt;p&gt;I'm going to treat these together because the solution is the same: dataflow tracking.&lt;/p&gt;
    &lt;p&gt;Take as an example pluto.jl, which does this today by hooking into the Julia compiler.&lt;/p&gt;
    &lt;p&gt;Note that this updates cells live in response to previous cells that they depend on. Not pictured is that it doesn't update cells if their dependencies haven't changed. You can think of this as a spreadsheet-like Jupyter, where code is only rerun when necessary.&lt;/p&gt;
    &lt;p&gt;You may say this is hard to generalize. The trick here is orthogonal persistence. If you sandbox the processes, track all IO, and prevent things that are "too weird" unless they're talking to other processes in the sandbox (e.g. unix sockets and POST requests), you have really quite a lot of control over the process! This lets you treat it as a pure function of its inputs, where its inputs are "the whole file system, all environment variables, and all process attributes".&lt;/p&gt;
    &lt;head rend="h3"&gt;derived features&lt;/head&gt;
    &lt;p&gt;Once you have these primitives—Jupyter notebook frontends, undo/redo, automatic rerun, persistence, and shell integration—you can build really quite a lot on top. And you can build it incrementally, piece-by-piece:&lt;/p&gt;
    &lt;head rend="h4"&gt;needs a Jupyter notebook frontend&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Runbooks (actually, you can build these just with Jupyter and a PTY primitive).&lt;/item&gt;
      &lt;item&gt;Terminal customization that uses normal CSS, no weird custom languages or ANSI color codes.&lt;/item&gt;
      &lt;item&gt;Search for commands by output/timestamp. Currently, you can search across output in the current session, or you can search across all command input history, but you don't have any kind of smart filters, and the output doesn't persist across sessions.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;needs shell integration&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Timestamps and execution duration for each command.&lt;/item&gt;
      &lt;item&gt;Local line-editing, even across a network boundary.&lt;/item&gt;
      &lt;item&gt;IntelliSense for shell commands, without having to hit tab and with rendering that's integrated into the terminal.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;needs sandboxed tracing&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;"All the features from sandboxed tracing": collaborative terminals, querying files modified by a command, "asciinema but you can edit it at runtime", tracing build systems.&lt;/item&gt;
      &lt;item&gt;Extend the smart search above to also search by disk state at the time the command was run.&lt;/item&gt;
      &lt;item&gt;Extending undo/redo to a git-like branching model (something like this is already support by emacs undo-tree), where you have multiple "views" of the process tree.&lt;/item&gt;
      &lt;item&gt;Given the undo-tree model, and since we have sandboxing, we can give an LLM access to your project, and run many of them in parallel at the same time without overwriting each others state, and in such a way that you can see what they're doing, edit it, and save it into a runbook for later use.&lt;/item&gt;
      &lt;item&gt;A terminal in a prod environment that can't affect the state of the machine, only inspect the existing state.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;ok but how do you build this&lt;/head&gt;
    &lt;p&gt;jyn, you may say, you can't build vertical integration in open source. you can't make money off open source projects. the switching costs are too high.&lt;/p&gt;
    &lt;p&gt;All these things are true. To talk about how this is possible, we have to talk about incremental adoption.&lt;/p&gt;
    &lt;p&gt;if I were building this, I would do it in stages, such that at each stage the thing is an improvement over its alternatives. This is how &lt;code&gt;jj&lt;/code&gt; works and it works extremely well: it doesn't require everyone on a team to switch at once because individual people can use &lt;code&gt;jj&lt;/code&gt;, even for single commands, without a large impact on everyone else.&lt;/p&gt;
    &lt;head rend="h3"&gt;stage 1: transactional semantics&lt;/head&gt;
    &lt;p&gt;When people think of redesigning the terminal, they always think of redesigning the terminal emulator. This is exactly the wrong place to start. People are attached to their emulators. They configure them, they make them look nice, they use their keybindings. There is a high switching cost to switching emulators because everything affects everything else. It's not so terribly high, because it's still individual and not shared across a team, but still high.&lt;/p&gt;
    &lt;p&gt;What I would do instead is start at the CLI layer. CLI programs are great because they're easy to install and run and have very low switching costs: you can use them one-off without changing your whole workflow.&lt;/p&gt;
    &lt;p&gt;So, I would write a CLI that implements transactional semantics for the terminal. You can imagine an interface something like &lt;code&gt;transaction [start|rollback|commit]&lt;/code&gt;, where everything run after &lt;code&gt;start&lt;/code&gt; is undoable. There is a lot you can do with this alone, I think you could build a whole business off this.&lt;/p&gt;
    &lt;head rend="h3"&gt;stage 2: persistent sessions&lt;/head&gt;
    &lt;p&gt;Once I had transactional semantics, I would try to decouple persistence from tmux and mosh.&lt;/p&gt;
    &lt;p&gt;To get PTY persistence, you have to introduce a client/server model, because the kernel really really expects both sides of a PTY to always be connected. Using commands like alden, or a library like it (it's not that complicated), lets you do this simply, without affecting the terminal emulator nor the programs running inside the PTY session.&lt;/p&gt;
    &lt;p&gt;To get scrollback, the server could save input and output indefinitely and replay them when the client reconnects. This gets you "native" scrollback—the terminal emulator you're already using handles it exactly like any other output, because it looks exactly like any other output—while still being replayable and resumable from an arbitrary starting point. This requires some amount of parsing ANSI escape codes2, but it's doable with enough work.&lt;/p&gt;
    &lt;p&gt;To get network resumption like mosh, my custom server could use Eternal TCP (possibly built on top of QUIC for efficiency). Notably, the persistence for the PTY is separate from the persistence for the network connection. Eternal TCP here is strictly an optimization: you could build this on top of a bash script that runs &lt;code&gt;ssh host eternal-pty attach&lt;/code&gt; in a loop, it's just not as nice an experience because of network delay and packet loss. Again, composable parts allow for incremental adoption.&lt;/p&gt;
    &lt;p&gt;At this point, you're already able to connect multiple clients to a single terminal session, like tmux, but window management is still done by your terminal emulator, not by the client/server. If you wanted to have window management integrated, the terminal emulator could speak the tmux -CC protocol, like iTerm.&lt;/p&gt;
    &lt;p&gt;All parts of this stage can be done independently and in parallel from the transactional semantics, but I don't think you can build a business off them, it's not enough of an improvement over the existing tools.&lt;/p&gt;
    &lt;head rend="h3"&gt;stage 3: structured RPC&lt;/head&gt;
    &lt;p&gt;This bit depends on the client/server model. Once you have a server interposed between the terminal emulator and the client, you can start doing really funny things like tagging I/O with metadata. This lets all data be timestamped3 and lets you distinguish input from output. xterm.js works something like this. When combined with shell integration, this even lets you distinguish shell prompts from program output, at the data layer.&lt;/p&gt;
    &lt;p&gt;Now you can start doing really funny things, because you have a structured log of your terminal session. You can replay the log as a recording, like asciinema4; you can transform the shell prompt without rerunning all the commands; you can import it into a Jupyter Notebook or Atuin Desktop; you can save the commands and rerun them later as a script. Your terminal is data.&lt;/p&gt;
    &lt;head rend="h3"&gt;stage 4: jupyter-like frontend&lt;/head&gt;
    &lt;p&gt;This is the very first time that we touch the terminal emulator, and it's intentionally the last step because it has the highest switching costs. This makes use of all the nice features we've built to give you a nice UI. You don't need our &lt;code&gt;transaction&lt;/code&gt; CLI anymore unless you want nested transactions, because your whole terminal session starts in a transaction by default. You get all the features I mention above, because we've put all the pieces together.&lt;/p&gt;
    &lt;head rend="h2"&gt;jyn, what the fuck&lt;/head&gt;
    &lt;p&gt;This is bold and ambitious and I think building the whole thing would take about a decade. That's ok. I'm patient.&lt;/p&gt;
    &lt;p&gt;You can help me by spreading the word :) Perhaps this post will inspire someone to start building this themselves.&lt;/p&gt;
    &lt;head rend="h2"&gt;bibliography&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Gary Bernhardt, “A Whole New World”&lt;/item&gt;
      &lt;item&gt;Alex Kladov, “A Better Shell”&lt;/item&gt;
      &lt;item&gt;jyn, “how i use my terminal”&lt;/item&gt;
      &lt;item&gt;jyn, “Complected and Orthogonal Persistence”&lt;/item&gt;
      &lt;item&gt;jyn, “you are in a box”&lt;/item&gt;
      &lt;item&gt;jyn, “there's two costs to making money off an open source project…”&lt;/item&gt;
      &lt;item&gt;Rebecca Turner, “Vertical Integration is the Only Thing That Matters”&lt;/item&gt;
      &lt;item&gt;Julia Evans, “New zine: The Secret Rules of the Terminal”&lt;/item&gt;
      &lt;item&gt;Julia Evans, “meet the terminal emulator”&lt;/item&gt;
      &lt;item&gt;Julia Evans, “What happens when you press a key in your terminal?”&lt;/item&gt;
      &lt;item&gt;Julia Evans, “What's involved in getting a "modern" terminal setup?”&lt;/item&gt;
      &lt;item&gt;Julia Evans, “Bash scripting quirks &amp;amp; safety tips”&lt;/item&gt;
      &lt;item&gt;Julia Evans, “Some terminal frustrations”&lt;/item&gt;
      &lt;item&gt;Julia Evans, “Reasons to use your shell's job control”&lt;/item&gt;
      &lt;item&gt;“signal(7) - Miscellaneous Information Manual”&lt;/item&gt;
      &lt;item&gt;Christian Petersen, “ANSI Escape Codes”&lt;/item&gt;
      &lt;item&gt;saoirse, “withoutboats/notty: A new kind of terminal”&lt;/item&gt;
      &lt;item&gt;Jupyter Team, “Project Jupyter Documentation”&lt;/item&gt;
      &lt;item&gt;“Warp: The Agentic Development Environment”&lt;/item&gt;
      &lt;item&gt;“Warp: How Warp Works”&lt;/item&gt;
      &lt;item&gt;“Warp: Completions”&lt;/item&gt;
      &lt;item&gt;George Nachman, “iTerm2: Proprietary Escape Codes”&lt;/item&gt;
      &lt;item&gt;George Nachman, “iTerm2: Shell Integration”&lt;/item&gt;
      &lt;item&gt;George Nachman, “iTerm2: tmux Integration”&lt;/item&gt;
      &lt;item&gt;Project Jupyter, “Jupyter Widgets”&lt;/item&gt;
      &lt;item&gt;Nelson Elhage, “nelhage/reptyr: Reparent a running program to a new terminal”&lt;/item&gt;
      &lt;item&gt;Kovid Goyal, “kitty”&lt;/item&gt;
      &lt;item&gt;Kovid Goyal, “kitty - Frequently Asked Questions”&lt;/item&gt;
      &lt;item&gt;Wez Furlong, “Wezterm”&lt;/item&gt;
      &lt;item&gt;Keith Winstein, “Mosh: the mobile shell”&lt;/item&gt;
      &lt;item&gt;Keith Winstein, “Display errors with certain characters&lt;/item&gt;
      &lt;item&gt;Matthew Skala, “alden: detachable terminal sessions without breaking scrollback”&lt;/item&gt;
      &lt;item&gt;Ethan Pailes, “shell-pool/shpool: Think tmux, then aim... lower”&lt;/item&gt;
      &lt;item&gt;Ned T. Crigler, “crigler/dtach: A simple program that emulates the detach feature of screen”&lt;/item&gt;
      &lt;item&gt;Marc André Tanner, “martanne/abduco: abduco provides session management”&lt;/item&gt;
      &lt;item&gt;yazgoo, “yazgoo/diss: dtach-like program / crate in rust”&lt;/item&gt;
      &lt;item&gt;Fons van der Plas, “Pluto.jl — interactive Julia programming environment”&lt;/item&gt;
      &lt;item&gt;Ellie Huxtable, “Atuin Desktop: Runbooks that Run”&lt;/item&gt;
      &lt;item&gt;Toby Cubitt, “undo-tree”&lt;/item&gt;
      &lt;item&gt;“SIGHUP - Wikipedia”&lt;/item&gt;
      &lt;item&gt;Jason Gauci, “How Eternal Terminal Works”&lt;/item&gt;
      &lt;item&gt;Marcin Kulik, “Record and share your terminal sessions, the simple way - asciinema.org”&lt;/item&gt;
      &lt;item&gt;“Alternate Screen | Ratatui”&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45892191</guid><pubDate>Tue, 11 Nov 2025 20:11:33 +0000</pubDate></item><item><title>Collaboration sucks</title><link>https://newsletter.posthog.com/p/collaboration-sucks</link><description>&lt;doc fingerprint="a369965128e686dd"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Collaboration sucks&lt;/head&gt;
    &lt;head rend="h3"&gt;Be the driver&lt;/head&gt;
    &lt;p&gt;“If you want to go fast, go alone; if you want to go far, go together”&lt;/p&gt;
    &lt;p&gt;This phrase will slowly kill your company and I’m here to prove it.&lt;/p&gt;
    &lt;p&gt;Imagine you are driving a car. It’s often useful to have someone give you directions, point out gas stations, and recommend stops for snacks. This is a helpful amount of collaboration.&lt;/p&gt;
    &lt;p&gt;An unhelpful amount of collaboration is getting out of your car to ask pedestrians if they like your car, swapping drivers every 10 minutes, or having someone constantly commenting on your driving.&lt;/p&gt;
    &lt;p&gt;In the first scenario, you get the right amount of feedback to get to your destination as fast as possible. In the second, you get more feedback, but it slows you down. You run the risk of not making it to the place you want to go.&lt;/p&gt;
    &lt;p&gt;The second scenario is also the one most startups (or companies, really) end up in because of ✨ collaboration ✨.&lt;/p&gt;
    &lt;head rend="h2"&gt;Being good at feedback means knowing when not to give it&lt;/head&gt;
    &lt;p&gt;As PostHog grows, I’ve seen more and more collaboration that doesn’t add value or adds far too little value for the time lost collaborating. So much so we made “collaboration sucks” the topic of the week during a recent company all hands.&lt;/p&gt;
    &lt;p&gt;“You’re the driver” is a key value for us at PostHog. We aim to hire people who are great at their jobs and get out of their way. No deadlines, minimal coordination, and no managers telling you what to do.&lt;/p&gt;
    &lt;p&gt;In return, we ask for extraordinarily high ownership and the ability to get a lot done by yourself. Marketers ship code, salespeople answer technical questions without backup, and product engineers work across the stack.&lt;/p&gt;
    &lt;p&gt;This means there is almost always someone better at what you are doing than you are. It is tempting to get them, or anybody really, involved and ✨ collaborate ✨, but collaboration forces the driver to slow down and explain stuff (background, context, their thinking).&lt;/p&gt;
    &lt;p&gt;This tendency reveals itself in a few key phrases:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;“Curious what X thinks”&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;“Would love to hear Y’s take on this”&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;“We should work with Z on this”&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This sometimes leads to valuable insights, but always slows the driver down. It erodes their motivation, confidence, and effectiveness, and ultimately leads us to ship less.&lt;/p&gt;
    &lt;head rend="h2"&gt;If collaboration sucks, why do people do it?&lt;/head&gt;
    &lt;p&gt;Everyone is to blame.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;People want to be helpful. For example, when someone posts their work-in-progress in Slack, others feel obliged to give feedback because we have a culture of feedback.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;On the flip side, people don’t ask for feedback from specific people because it doesn’t feel inclusive, even though it would help.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;People aren’t specific enough about what feedback they need. This creates more space for collaboration to sneak in. A discussion about building a specific feature can devolve into reevaluating the entire product roadmap if you let it.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;When someone has a good idea, the response often defaults to “let’s discuss” rather than “ok, do it.” As proof, we have 175 mentions of “let’s discuss” in Slack.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;People just want to talk about stuff because they&lt;/p&gt;&lt;del rend="overstrike"&gt;are too busy&lt;/del&gt;can’t be bothered to act on it. We drift from our ideal of a pull request to an issue/RFC to Slack (we are mostly here) to “let’s discuss”.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;It’s not clear who the owner is (or no one wants to own what’s being discussed).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;It is annoying, but sometimes a single person can’t ship certain things front to back to a high-enough quality and we can’t just ship and iterate. We can fix broken code, but we can’t resend a newsletter.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;How to crush collaboration (and go farther, faster)&lt;/head&gt;
    &lt;p&gt;So if collaboration is your enemy, how do you defeat it? Here’s what we say:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Default to shipping. Pull requests &amp;gt; issues &amp;gt; Slack messages.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Every time you see ✨ collaboration ✨ happening, speak up and destroy it. Say “there are too many people involved. X, you are the driver, you decide.” (This is a great way to make friends btw).&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Tag who you specifically want input from and what you want from them, not just throw things out there into the void.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Prefer to give feedback after something has shipped (but before the next iteration) rather than reviewing it before it ships. Front-loading your feedback can turn it into a quasi-approval process.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;If you are a team lead, or leader of leads, who has been asked for feedback, consider being more you can just do stuff.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;When it’s your thing, you are the “informed captain.” Listen to feedback, but know it’s ultimately up to you to decide what to do, not the people giving feedback.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Unfortunately for me, not all collaboration can be rooted out, and even I will admit that some collaboration is useful. Ian and Andy edited this newsletter after all.&lt;/p&gt;
    &lt;p&gt;The point is, if you aren’t actively attempting to collaborate less, you are probably collaborating too much by default and hurting your ability to go far, fast.&lt;lb/&gt;Words by Charles Cook, who also hates sparkling water, presumably because the bubbles are too collaborative.&lt;/p&gt;
    &lt;head rend="h2"&gt;👷 Jobs at PostHog&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;AI Product Engineer working on PostHog AI, LLM Analytics or Array teams.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Backend Engineer for Feature Flags and Ingestion teams&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Influencer Wrangler on the Marketing team&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;YC Technical Onboarding Specialist on the Onboarding team (San Fran based)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;ClickHouse Operations Engineer on the ClickHouse team&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;📖 More good reads&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Workflows are now in Alpha and I already broke mine – Sara Miteva&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Your data model is your destiny – Matt Brown&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Spinning Plates – Dylan Martin&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;1000x: The Power of an Interface for Performance (video) – Joran Dirk Greef&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45892394</guid><pubDate>Tue, 11 Nov 2025 20:27:38 +0000</pubDate></item><item><title>PBM Drug Pricing Distortion Report</title><link>https://www.46brooklyn.com/research/welcome-to-private-label-park-nuf485-8h5kw-wk8y2</link><description>&lt;doc fingerprint="b824819d3da25dab"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;This drug pricing distortion was supposed to go extinct. It’s never been more alive.&lt;/head&gt;
    &lt;p&gt;This summer, we launched a new research work that examined the costs yielded by a reborn and evolved species of the prescription drug supply chain: drug companies who are also part of larger vertically integrated pharmacy benefit manager (PBM) corporations. In essence, PBMs are intended to be a structural friction against the desired prices charged by drug manufacturers, so when the companies that are supposed to be the price-fighters become the price-setters, traditionally-assumed incentives become blurred at best and spun into reverse at worst.&lt;/p&gt;
    &lt;p&gt;More recently, fellow drug channel archaeologist Adam Fein recently also wondered what PBM-owned drug companies list price behavior might tell us about the incentives within the U.S. drug pricing system. A newly published paper from researchers Michael Carrier and Rachel Sachs further explores the competitive concerns and seeming misalignment of PBM incentives.&lt;/p&gt;
    &lt;p&gt;So while our study from this summer dug into the publicly-available data that shows the output of the costs reported in the Medicaid, Medicare, and Commercial markets for medicines produced by one specific PBM-affiliated drug company, Quallent Pharmaceuticals, it is worth noting that the reported end costs of these medicines don’t tell the entirety of the story – specifically, the beginning.&lt;/p&gt;
    &lt;p&gt;As the PBM trade group, the Pharmaceutical Care Management Association, often reminds us, drug pricing realities begin with the setting of the list price of medicines by drug companies. In fact, in the ongoing food fight in DC and the states over who is to blame for high drug prices, PCMA has drawn such a line in the sand, they have even launched their own web portal – lowerthelistprice.com – in an effort to demonstrate the PBM industry’s desire for drug companies to lower the sticker prices of their medicines. As they put it, “the price is the problem when it comes to Americans facing difficulty affording their prescription drugs.”&lt;/p&gt;
    &lt;p&gt;As long-time industry researchers, we understand that drug affordability is a complicated problem with many cooks in the kitchen that can impact the end cost of a medicine. And while the list price of a medicine is certainly a key ingredient, we know that often, the list price can be an illusion of sorts, due to its frequent disconnect from the true underlying cost of the drug or the cost experienced by the end payer.&lt;/p&gt;
    &lt;p&gt;Regardless, Fein’s query on PBM-drug company list prices and the recent public relations offensive of the PBM trade group professing their devotion to low list prices were enticing enough for a thorough examination of the list prices of medicines – both those that are set by PBM-affiliated drug companies and those set by standalone drug companies.&lt;/p&gt;
    &lt;p&gt;If large PBMs truly want lower list prices for prescription drugs, then when their parent corporations own a drug company that sells and sets the prices for their own labelled medicines, one would assume that with maximum control over the price, the PBMs would be able to fulfill their own wishes for the low prices they demand of the pharmaceutical industry – the ultimate opportunity to put their money where their mouths are and fulfill the old adage, “if you want something done right, you have to do it yourself.”&lt;/p&gt;
    &lt;p&gt;In today’s new report, we build on the learnings of the past and dig deep into the foundational DNA of a rare but re-emerging species of drug channel creatures known as “pseudo-pharmas” or “PBM private labelers” to assess what happens when the finger of blame for who creates drug pricing reality ends up getting pointed at someone at the same company town hall meetings.&lt;/p&gt;
    &lt;head rend="h2"&gt;Background: “You know, at times like this, one feels, well, perhaps extinct animals should be left extinct.”&lt;/head&gt;
    &lt;p&gt;Like field paleontologists brushing desert sand from bone shards, evaluating prescription drug prices can feel like a grueling excavation into ancient eras. This is because discussions on drug pricing generally occur in regards to reference drug prices; things like Wholesale Acquisition Cost (WAC) or National Average Drug Acquisition Cost (NADAC). WAC can inform us about changes brand manufacturers are making to drug list prices (i.e. the prices “set” by manufacturers that are usually referenced in news stories about drug manufacturer prices and whose changes are chronicled on our Brand Drug List Price Change Box Score), and NADAC can describe what pharmacies are paying to acquire drugs (pharmacies typically buy drugs from wholesalers and those reported prices are tracked via NADAC, which is chronicled on our NADAC Drug Pricing Dashboard).&lt;/p&gt;
    &lt;p&gt;While WAC and NADAC both have their limitations, they are critical tools – backed by public policy and methods that give legitimacy to their usefulness – to assess the environment with which prescription drug costs exist. Yet despite the reliance that manufacturers, wholesalers, pharmacies, media, researchers, and Medicaid programs have on WAC and NADAC, these benchmarks tend not to be the ones referenced in drug pricing contracts that ultimately govern how prescription claims are negotiated or how actual cost is experienced for patients and plan sponsors. More simply put, two of the best measures that exist to evaluate drug prices generally aren’t even used in the contracts that determine the actual prices paid for medicines.&lt;/p&gt;
    &lt;p&gt;So, if you want to move away from the headlines and research papers and get closer to the place where drug pricing reality in the market is forged, you have to dig into the contracts that negotiate that reality. And if you are fortunate enough to see those contracts, you will likely pick up on the need to excavate an economic fossil record of another drug pricing benchmark: Average Wholesale Price (AWP), which is essentially an even more inflated sticker price of sorts with no policy-backed, objective definition and no meaningful, consistent alignment to any other pricing benchmark in the system. In the pharmacy world, you may hear it referred to as “ain’t what’s paid” or “a wrong price.” We’ve often referred to it as “always what’s profitable.” Regardless of what you call it, when it comes to traditional PBM contracts, AWP is everywhere.&lt;/p&gt;
    &lt;p&gt;So despite the importance of all other pricing benchmarks (and the ones we typically rely upon at 46brooklyn), AWP arguably matters more than anything, because the vast majority of contracts that negotiate drug prices are tied to AWP-based payment terms. For example, those contracts that can be found in the public domain tie drug reimbursement to AWP-based discounts:&lt;/p&gt;
    &lt;p&gt;And now feels like a good point to let our readers know that we’ve updated 46brooklyn.com to include more resources that will be relevant for this report. Whereas before we had a glossary tab at the top of our web page where users could find definitions to key pharmacy terms or acronyms, we’ve revamped that tab to be a broader resources hub. The resource tab retains the 46brooklyn glossary of pharmacy terms but also includes a link to a bunch of publicly-available drug channel contracts that we have scoured from the internet. And if you go to the Resource Tab on the top banner and click on the Drug Channel Contract button and browse the various contracts we’ve found, you’ll notice a AWP-based payment term theme. (Note we also added to the Resource Tab a link to Public Drug Pricing Data Sources we use for our work as well; though that doesn’t really come up in this report we figured we would let you know that is there too)&lt;/p&gt;
    &lt;p&gt;Is it wrong to tie drug pricing to AWP? That is largely a matter of perspective. Each stratum of this long‑buried benchmark preserves a story about how list prices were set, who profited, and why those figures still shape reimbursement long after the marketplace itself has evolved. Yet, just as a Tyrannosaurus imprint is no guide to the metabolism of a sparrow, the AWP embedded in pharmacy contracts tells us little about the real costs of modern dispensing. Worse, its petrified numbers can still bite.&lt;/p&gt;
    &lt;p&gt;AWP is the oldest drug pricing benchmark. Created in response to a California Medicaid pharmacist’s desire to help pay for prescription drugs, the 1970 edition of Red Book was the first to publish AWP prices. According to the work of Ernst Berndt and Joseph Newhouse at the Harvard Kennedy School of Government, the 1970 edition of Red Book carried the following description of AWP:&lt;/p&gt;
    &lt;p&gt;Soon after the publication of AWP by Red Book, their competitor, the American Druggist Blue Book, also began publishing AWPs. Perhaps unsurprisingly, as more and more health plans began offering prescription drug insurance, AWP became the dominant benchmark in terms of directing how drug prices were paid from the 1970s onwards. It was the case that during the 1960s, it was uncommon to have prescription drug coverage (approximately 4% had it), but that it increased over time with nearly all employers eventually offering health coverage including prescription drug benefit coverage as part of their offerings.&lt;/p&gt;
    &lt;p&gt;At the same time, it was knowable quite early on that AWP wasn’t that reliable of a drug pricing benchmark (if the goal of the benchmark is to understand the acquisition cost of the drug being sold; which it may or may not be depending upon your perspective). As early as 1974, the U.S. Department of Health, Education and Welfare put into the federal notice a warning that “Red Book data, Blue Book data (i.e. AWP) and other standard prices…were frequently in excess of actual acquisition cost.” In 1984, the Health &amp;amp; Human Services Office of Inspector General identified that most pharmacies purchased drugs 15.9% below AWP, with some being as much as 42% lower than the drug’s AWP. As HHS OIG stated at the time, “‘Average Wholesale Prices’ (AWPs) reported in the Red and Blue Book compendia were, for the drugs studied, in excess of actual acquisition prices.” In 1992, HHS OIG went so far as to identify that, “[T]here is no single discount rate which can be applied to AWP to provide a reasonably consistent estimate of physician’s acquisition cost.” And yet, AWP-based pricing continued into the 1990s and early 2000s.&lt;/p&gt;
    &lt;p&gt;At the start of the twenty-first century, the drug compendia First Databank (who had acquired the Blue Book from the American Druggist) and the wholesaler McKesson were alleged to have engaged in a scheme to raise AWP prices (we say this because they eventually settled the resulting lawsuits and expressly denied liability of any kind). This alleged activity, once discovered, was a cataclysmic event at the time. One could say the strata of our drug pricing history bears its own dramatic “K‑Pg boundary” from the mid‑2000s, when plaintiffs unearthed evidence that publishers and manufacturers had quietly widened the markup from WAC to AWP (Can you tell we watched Jurassic Park more recently than our beloved Star Wars?). If you were someone who paid for prescription drugs in reference to AWP (which to be clear was effectively everyone), this AWP-markup increased the price of medicines and undermined credibility in the benchmark and system as a whole. As the Wall Street Journal put it at the time, “The term [average wholesale price] is a misnomer because it no longer represents a price paid to wholesalers and is not an average of anything.” (Note, we’re not sure it EVER actually did represent a price paid to a wholesaler, but we digress)&lt;/p&gt;
    &lt;p&gt;In 2009, a federal court oversaw the resulting settlements of this AWP-based lawsuit, which included FDB’s decision to roll back the spread for hundreds of brand drugs back to a uniform 1.20 × WAC and to retrofit thousands of other National Drug Codes (NDCs) to the same ratio. The ruling reminded the market that AWP was never an “average” at all — merely a multiplier — yet even this carefully chiseled correction did not erase the benchmark; it simply reset the fossil clock.&lt;/p&gt;
    &lt;p&gt;In 2009, Adam Fein published a blog post questioning what the world would look like after AWP went boom. At some point after these lawsuits, legislative investigations into AWP appear to have dried up – maybe under the belief that AWP did in fact go boom. And who can blame anyone for trusting that the coast was clear? At the time, it was believed AWP distortions would be a thing of the past, largely due to the immense size of the settlements and just the obviousness of AWP’s flaws. For example, in her March 2009 order and opinion in the seminal AWP case, Judge Patti Saris’ blunt assessment of AWP’s true essence would lead many to believe that the jig was up for good when she stated, “AWP has been exposed as a faux inflated price unrelated to actual drug prices. Reliance on AWP is a trap for unwary and unsophisticated TPP purchasers and results in consumers paying unwarranted co-payments. Not only do FDB and Medi-Span have the right to make these changes, but in my view, after eight years of this MDL, rolling back AWPs or phasing them out as a pricing benchmark is in the public interest and to the benefit of the class.”&lt;/p&gt;
    &lt;p&gt;As a brief aside, it may be helpful (or at least it comes up in conversations we have) to think of AWP in the same line as LIBOR (London Inter-Bank Offered Rate) in finance and real estate. LIBOR represented the average interest rate major banks claimed they would charge each other for short-term loans. As discussed with AWP above, this isn’t that different from AWP which represents this price “designed to show the average price retailers throughout the country are paying to the wholesaler for a particular item” - see Figure 2 above. If AWP is this reference price used for prescription drug reimbursement, particularly in the insurance space, and LIBOR served as this global benchmark of interest rates for which major banks borrow from one another, then the parallel is that both act(-ed) as anchor points for transactions even though neither necessarily reflect true market conditions (said differently, both are numbers that thousands of transactions depend(ed) on to determine what should be paid or charged). Of course, much like AWP was found to potentially be inflated, LIBOR was discovered to not always reflect actual transaction prices and rather was ‘estimates’ submitted by banks. Banks which stood to benefit from tweaking their submissions to LIBOR can speak to the potential value that can be gained from perverse incentives to influence or manipulate the underlying benchmark used in complicated financial transactions (be they real estate or prescription drug benefits). In other words, AWP in healthcare and LIBOR in finance show how convenient but opaque reference benchmarks can dominate markets, yet become vulnerable to distortion and exploitation.&lt;/p&gt;
    &lt;p&gt;But in a tale as old as time, the drug channel took its licks in the litigation fallout and eventually found ways to facilitate similar yields with slightly modified financial logistics. Today, we know that AWP didn’t actually go extinct, and that most contracts today continue to “control” drug prices with AWP-based terms.&lt;/p&gt;
    &lt;p&gt;Fast‑forward to the present day, where three private‑label drug manufacturer “species” have emerged from the PBM ecosystem: CVS Health’s Cordavis, Cigna/Evernorth’s Quallent, and UnitedHealth/Optum’s Nuvaila. As we highlighted in this summer’s report, these three companies represent what we know to be the strongest environment in existence to control both the front-end and tail-end of a pharmacy transaction, as the same parent company owns the manufacturer/labeler that sets the list price, the PBM who negotiates the end payment to the provider, and the pharmacy that pays to put the drug on its shelf and receives that end PBM payment.&lt;/p&gt;
    &lt;p&gt;So, if AWP is ultimately the primary pricing benchmark with which drug prices are negotiated by PBMs — and PBM-affiliated companies have the power to set the AWPs of their own drug products — we now have an unprecedented opportunity to evaluate the pricing of these products and to see what they can teach us about drug pricing in the modern era. Further, we can examine what the behaviors of companies who claim to be universally and solely working to lower drug prices may actually reveal about their underlying biology.&lt;/p&gt;
    &lt;head rend="h2"&gt;Quallent’s AWP fossil record: “That’s the great thing about bones. They never run away.”&lt;/head&gt;
    &lt;p&gt;As we mentioned in this summer’s report, Cigna/Evernorth/Express Scripts’ drug company Quallent Pharmaceuticals has the most expansive portfolio of drug offerings under its label of the PBM private labelers (at least at the time of writing this), and thus, it will be the primary focus again of today’s report as well. At today’s metaphorical drug pricing dig site, we gathered up the AWP, WAC, and NADAC price points for all Quallent products from Elsevier’s Gold Standard Drug Database at the end of June 2025. We used this product list to also find any equivalent products on the active ingredient, dosage form, and strength (i.e., because Quallent has their own version of abiraterone 250mg tablets, we gathered up all other abiraterone 250mg tablet products from other manufacturers as well [and we did this for all the rest of the products as well]). The following Figure 3 tries to outline the population of drug products we’re looking at.&lt;/p&gt;
    &lt;p&gt;What Figure 3 above is trying to tell you is that we have almost 6,000 NDCs we’re analyzing, across 124 unique active ingredient/dosage form/strength combinations, across 49 unique active ingredients, and three different dosage form types. Almost all the dosage forms we will focus on in this report are simple tablets or capsules (as the injectable biosimilars have been covered extensively by others elsewhere).&lt;/p&gt;
    &lt;p&gt;Now, we would love to be able to just tell you all the prices to the penny; however, we cannot do that. For whatever reasons, AWP and WAC are considered proprietary prices, and so we technically cannot tell you what exact price goes with what drug. And while that is ridiculous that even in a system where we know the list prices for medicines are inflated and essentially both real and fake, it is appropriate for our fossil record analogy. Just as we’ll likely never know what dinosaurs actually looked like (were they green and scaly or covered in vibrant feathers?), this report will have to tell you the outline of what we’re observing. However, we can describe the universe of prices we’re observing as a kind of impression of the hollowed out place where the metaphorical drug pricing bones fell in the soil — a fossil that we can hopefully fill in enough of the gaps with to get the shape of what the reference prices look like for these drugs.&lt;/p&gt;
    &lt;p&gt;We start by trying to put together an assessment of where the AWPs for a given product fall. To do this, as shown in the below gallery, we plot the characteristics, on an AWP-basis, for the 124 products (active ingredient, dosage form, strength) we can examine. For each active ingredient across all drug companies/labelers, we outline on a dosage form and strength basis the minimum observed AWP per unit (green dot), the median AWP (purple), the average AWP (grey), the Quallent AWP (blue), and the maximum AWP (red dot). You’ll notice that in order to maintain the proprietary nature of the specific prices, the axis of all graphs are hidden (some have unit prices that are pennies per units, whereas others have prices that are thousands of dollars). While everything is appropriately scaled so that you can get the fossilized impression and relative scale of what we’re seeing, we cannot actually tell you the AWP for any of the 49 active ingredients in our gallery below, but even so, we think you’ll walk away with some meaningful learnings from what we’ve put together:&lt;/p&gt;
    &lt;p&gt;If you scroll through the gallery, we believe you’ll walk away with the idea that Quallent is generally on the higher-end of AWP values for the products in question and rarely at the lower end. If we could show you the the exact prices, you’d see that Quallent is, on a product-level average, 33-times more expensive at the AWP basis than the cheapest AWP option for any of these therapies. At the same time, Quallent is, on a product-level average, 83.8% of the maximum AWP-based price. Further, there are no products where Quallent is the lowest AWP price point, but a handful where they are actually the most expensive.&lt;/p&gt;
    &lt;p&gt;In case it is helpful, the following table gives characteristics of Quallent prices as either the multiplier from the minimum price or what percentage of the maximum price Quallent’s price point is (it’s the best we think we can do without telling you the actual prices):&lt;/p&gt;
    &lt;p&gt;In an ecosystem where AWP is a predominant basis for determining the drug pricing experience of the end payer (see payer contracts we found), high AWP prices would appear to either confer high cost onto those end payers and/or provide higher revenues to dispensing pharmacies. This is because if you live in a world of AWP-based discounts determining your drug pricing reality, then an 85.5% discount off of a higher number will result in a bigger number than that same 85.5% discount off of a lower number.&lt;/p&gt;
    &lt;p&gt;In case the math isn’t clear, consider two identical products: one with an AWP of $100 and one with an AWP of $1,000. In a world where drug cost is determined as an AWP-based discount, say AWP minus 85.5% (the 2025 generic drug discount in the City of Mesa contract we led this report with), then the payer will spend more and/or the pharmacy will make more money dispensing the $1,000 AWP product relative to dispensing the lower price product. For you shoppers out there, imagine you are a department store like Macy’s or Boscov’s, and you have a customer with a 60% off coupon with two pairs of pants, where one pair has a sticker price of $50 and the other has a sticker price of $150. After the coupon discounts each respective purchase, the two pairs of pants yields the department store $20 and $60. And while you wouldn’t normally buy the $150 dollar pants, if you can get them for effectively $60 with you’re coupon, maybe you would. Ergo, more actual dollars can be harvested off items with higher sticker prices.&lt;/p&gt;
    &lt;p&gt;We make this observation as it would seem counter-intuitive to some when thinking about the structural incentives of the drug channel. By basing contracts in AWP-linked pricing, aren’t you incentivizing higher — not lower — prices? By basing pharmacy benefits contracts in AWP, it would appear interesting behavior for a PBM-owned drug company to set relatively high AWP for their drug products (given PBM claims that they’re the only one working to lower drug prices).&lt;/p&gt;
    &lt;p&gt;We guess from a certain point of view, by setting high average wholesale prices for the medicines they sell, it provides the opportunity to provide relatively larger discounts to AWP for those products, but is that actual lower prices or just the perception of lower prices?&lt;/p&gt;
    &lt;p&gt;Look, if you’re a reader of 46brooklyn, you know that we are not fans of fake, artificially inflated drug prices (of which AWP is the embodiment of that behavior), so we may already be prone to negativity when it comes to evaluations of AWP-based price points. We think the above analysis is a fair characterization of the observed behavior, which in simple terms is this: PBMs contract with plan sponsors in a way that offers aggregated discounts off of AWP. They also will contract with pharmacies and pay them as an aggregated discount off AWP. Through their parent company’s private labeling business, they can actually set the dollar value of whatever AWP is. So by setting a higher AWP, it devalues the nature of its contractual discounts in ways that can inflate costs to their plan sponsor clients and provide increased margins for the pharmacies dispensing medicines that reflect those high AWP values.&lt;/p&gt;
    &lt;p&gt;But this ain’t our first drug pricing excavation, and we are well prepared for the typical pushbacks, excuses, and proverbial shooting of the messenger. So let’s take this further (because this report isn’t long enough) and re-do the analysis above, but this time, instead of evaluating AWP-pricing behavior, which lacks a federal definition for what it is and what it’s supposed to represent, let’s turn instead to Wholesale Acquisition Cost (WAC). As a pricing benchmark, WAC has a federal definition (in short, WAC is the list price between manufacturer and wholesaler transaction), and so when we evaluate these prices, we can at least know that it represents something (even if the only thing it represents is an artificially inflated list price that does not reflect rebates and other related discounts, but at least WAC is something — not the quasi-nothing that is AWP). So let’s dig deeper into our drug reference pricing sandstone.&lt;/p&gt;
    &lt;head rend="h2"&gt;The next layer: Quallent’s WAC pricing fossil record&lt;/head&gt;
    &lt;p&gt;If we re-do the earlier AWP gallery with WAC, we get the following images of WAC pricing for Quallent products. Again, if you scroll through this below image gallery, we think you’ll walk away with some further learnings (even if we can’t talk about the actual prices):&lt;/p&gt;
    &lt;p&gt;We believe the impression you’ll get from the above gallery is the exact opposite behavior that we saw for AWP with Quallent products. On a WAC basis, Quallent products are on a product-level average just a 3.4 fold multiplier above the lowest WAC price and just 15% of the maximum WAC price for these products (as a reminder, these same characteristics were higher when we were assessing AWP-based behavior, 33-fold the AWP min and 83.8% the AWP-max price). We do not know what the dynamics actually look like when you effectively sell something to yourself, but we find it telling that the behavior of Quallent’s WAC — which is the list price for them to sell/buy to themself (at least that is what the federal definition of WAC tells us it represents) — is on the low end of observable WAC prices for a given product, but when Quallent sells to others, the list price (AWP is the basis for most payer contracts) is on the high end of observable prices. Again, we draw your attention to our resource tab - look at the Express Scripts contracts we’ve found - you’ll see they’re AWP-based contracts by and large (and we acknowledge, we don’t have every ESI contract but from what we’ve gathered we see a trend to AWP).&lt;/p&gt;
    &lt;p&gt;Again, to try to observe this behavior more directly — but not as directly as publishing the price (because we can’t publish WAC prices either) — when we look at the multiplier of the Quallent products’ WAC price relative to the lowest available WAC in the market, or the their products’ WAC price as a percentage of the maximum WAC prices available in the market, we see an entirely different animal than what we saw with the AWP-side of things.&lt;/p&gt;
    &lt;p&gt;As you can see in the above, while Quallent’s AWPs (earlier in this report) were consistently many double-digit multiples of the minimum AWP or 70%+ the maximum AWP, when it comes to WACs, their products are almost always smaller (both as a multiple to the minimum or a percentage of the maximum).&lt;/p&gt;
    &lt;p&gt;If you’re a fan of dinosaurs, trying to picture what these creatures actually look like is a big part of being a dinosaur fan (we asked our kids and they confirmed that this is part of the kid code or something). However, if the beaver tail is any indication, we would likely be surprised to discover what Dinos actually look like. All of this is to say, what are we to make of the widely different perspectives on where Quallent’s drug list prices fall based upon whether we contextualize their price in an AWP-based manner or in a WAC-based manner?&lt;/p&gt;
    &lt;p&gt;Or saying it differently, if you are going to try to construct an image of what a drug’s price truly is, but the basis of your construction of that price is AWP, you are doomed to get it wrong effectively 100% of the time. To demonstrate this, we decided to revisit our favorite dinosaur drug-equivalent — imatinib.&lt;/p&gt;
    &lt;head rend="h3"&gt;Our favorite dinosaur: Imatinib Mesylate&lt;/head&gt;
    &lt;p&gt;If you’ve not followed us from the beginning, you may not realize that the first thing 46brooklyn wrote back in 2018 was about imatinib mesylate (generic for Gleevec). Back in the day, Gleevec was a wonder drug, fundamentally changing the way we treat blood cancer. In the time before Gleevec, chronic myeloid leukemia (CML) was a blood cancer that had a very poor prognosis. After Gleevec came to market, the success rate of treating CML skyrocketed — literally saving lives. Gleevec was first approved by the FDA in 2001 with the first generic being approved in 2016. And while undoubtedly, the brand manufacturer of Gleevec, Novartis, made a bunch of really nice profits off of a medication to treat cancer (estimated $4 billion in annual revenue from the drug by 2010), the generic market was supposed to deflate the price after the brand had made all its money.&lt;/p&gt;
    &lt;p&gt;And so you can imagine our dismay when we originally strung together drug pricing and payment data back in 2018 and saw that generic Gleevec was often associated with large mark-ups on cost (continued big thanks to our local Columbus Dispatch for their foundational reporting that gave our early work life, Bob Herman for the original national shout-out, and the incomparable Robert Langreth for the issue validation). Our observations were such that this treatment for cancer was likely thousands of dollars more expensive per prescription than it would otherwise have been if pricing based on the actual acquisition cost of the drug had prevailed (much like Cost Plus Drugs or NADAC-based models that some PBMs and many Medicaid programs are doing today). At the time, we didn’t understand AWP as well as we do today (give us a break, we were still in drug pricing padawan training).&lt;/p&gt;
    &lt;p&gt;Back then, we were naive enough to think that if knowledge of this pricing markup was published and understood by the broader payer market and/or regulators, it would go away. Of course, that is generally not what has happened, despite the multitude of public awareness opportunities borne out of our own work, cost-plus pharmacies, the Federal Trade Commission (FTC), Congress, and media, we still often see cancer patients and/or their plan sponsors paying high prices for an imatinib product that has become incredibly cheap for pharmacies to acquire. We’re talking about people paying thousands of dollars for something that is commonly being sold for less than $50 outside our typical convoluted drug distribution channel. So we revisit imatinib today, because it likely can help us understand the importance of AWP &amp;amp; WAC pricing in a manner that our aggregate view may not.&lt;/p&gt;
    &lt;p&gt;If we look at the imatinib example from our last report, we observed that in 2023, the highest priced per-unit imatinib product was Quallent’s:&lt;/p&gt;
    &lt;p&gt;And if we look at our imatinib AWP chart from the above gallery in this report, we see that Quallent is almost, but not quite, the most expensive imatinib product in the market on an AWP-basis for 400 mg strength (and maybe for the 100 mg):&lt;/p&gt;
    &lt;p&gt;Of course, if we look at Quallent’s WAC price, we can see that it is on the low end of the market.&lt;/p&gt;
    &lt;p&gt;Knowing these data points, and recognizing the importance of AWP in terms of determining the amounts charged to plan sponsors, may help explain why Medicare’s observations were what they were in this summer’s report on Quallent product costs across payer markets. If a PBM’s payer contract for imatinib is determining drug prices in reference to an AWP-based discount, it stands to reason that if Quallent’s imatinib is one of the highest AWPs, its yielded negotiated price will be at the top; especially if the only realistic way to get a Quallent imatinib product is from the PBM-owned pharmacy (i.e., less other variables, like individual maximum allowable cost (MAC) rates or unique discounts are likely less relative to other labelers).&lt;/p&gt;
    &lt;p&gt;We can see some confirmation for this if we look at Archis Pharma’s AWP (listed as “Archi” from CMS’ Part D Dashboard). Archis Pharma has the lowest AWP unit price for imatinib products across the 20 labelers in our database and across the dozen labelers used in Medicare. So by the inverse logic of that for Quallent, it stands to reason that Archis would be associated with the lowest Medicare price as a function of its substantially lower AWP. Again, if both products are subject to an AWP-based discount to determine pricing, then it stands to reason that the lower the AWP price, the lower the end cost to the end payer.&lt;/p&gt;
    &lt;p&gt;A couple additional observations before moving on:&lt;/p&gt;
    &lt;p&gt;In a traditional market, if a product class of interchangeable products with immaterial and indistinguishable differences has multiple suppliers already, one would reasonably assume that if another supplier entered the market, their pricing would have to be within the range of existing suppliers, or more likely, even cheaper. After all, how else could a new supplier attract purchasers if they entered the market with essentially the same product but at anything but a more competitive price? This would be especially true given PBMs’ stated affinity for low prices.&lt;/p&gt;
    &lt;p&gt;Well, Quallent’s pricing decisions appear to defy those reasonable expectations. Quallent’s imatinib came to market in 2023 amid a marketplace that already had 10 versions already being supplied by other drug labelers (based upon date of each labeler’s first WAC price in Elsevier GSDD). Quallent’s imatinib launch even came after Archis, who had entered the market in 2022. Despite this competitive environment, Quallent actually came to market with a higher AWP — one that was around 17% more than the average of all available manufacturers of imatinib at the time and 7,000% more than Archis (based upon the 400 mg strength).&lt;/p&gt;
    &lt;p&gt;Additionally, we note that although Archis did have more utilization in Medicare in 2023 (see dosage units in Figure 4), it wasn’t the most utilized product despite having the lowest price. Furthermore, when we consider what total gross Medicare spending looks like, Archis products grossed about $1.2 million in Medicare, whereas Quallent products grossed $800K, but Quallent units were more than 70-fold more expensive on an AWP-basis than Archis — essentially meaning Quallent and their distributing pharmacies (i.e. pharmacies owned by Quallent’s parent company) made a lot of money for relatively little volume of work (i.e., dispensed prescriptions) when compared to peer interchangeable products.&lt;/p&gt;
    &lt;p&gt;However, these observations of the differences between Quallent and Archis may miss the broader point. What if we told you there was a similarity between Archis’ and Quallent’s imatinib pricing that isn’t immediately clear from what we’ve told you thus far? It turns out that Archis’ and Quallent’s imatinib have the same, low WAC price. The divergence is in their AWP price, and that difference gets at the heart of perhaps what Adam Fein’s query at the start of this report is getting to.&lt;/p&gt;
    &lt;head rend="h3"&gt;AWP-to-WAC ratios: “Hold on to your butts.”&lt;/head&gt;
    &lt;p&gt;If you go back to the background section of this report, or if you lived in the drug pricing space during the time of the old AWP lawsuits, you may think that there is one universal truth to the relationship between AWP and WAC. You may think that following the First Databank rollback and the lawsuits more generally that all drugs have a AWP-to-WAC relationship whereby the AWP is 120% (1.2x) the WAC. However, that would be a mistake, as that AWP-to-WAC relationship generally only applies to brand-name products (not generics). And if you don’t realize that AWP-to-WAC is not a fixed ratio for most drugs (generics make up nearly 90% of all drug transactions), then the manner in which prescription benefits are secured in a contract can result in outcomes you may not expect.&lt;/p&gt;
    &lt;p&gt;To start, we should recognize that drug pricing historically functions much like any other transaction in the broader consumer market whereby sellers of products are looking to buy low and sell high as a means to maximize profit. Even in cost-plus pharmacy reimbursement models, the goal is for the pharmacy to sell at above the cost it took to acquire drug inventory and process and dispense the prescription (i.e., pharmacies want — and need — their selling price to be higher than their purchase price). However, AWP can juice the system in ways that people may not appreciate if they’re not familiar with its true nature and the incentives it can create for prescription transaction stakeholders. Consider the relationships between AWP-to-WAC for Quallent’s products, arranged in Figure 7 below from high-to-low.&lt;/p&gt;
    &lt;p&gt;While there are two products at the 1.2 AWP-to-WAC ratio within Quallent’s portfolio (those on the far left of Figure 7), the rest of Quallent’s products are many times greater than the 1.2 AWP-to-WAC ratio. The average ratio, on a product basis, is approximately a 38-fold ratio (meaning the AWP price is 38-times higher than the underlying WAC price). At the extreme, four products have an AWP-to-WAC ratio above 100x, with the highest being 200-fold.&lt;/p&gt;
    &lt;p&gt;As a result, the ability of a plan sponsor’s AWP-based contract to price medicines ‘reasonably’ largely depends upon what versions of those medicines are being sold through their pharmacy benefits program. Consider again the example City of Mesa contract we referenced at the beginning of this report (embedded again here).&lt;/p&gt;
    &lt;p&gt;Focusing on the brand payment, and knowing what we know about AWP-to-WAC relationships, this contract states that the City of Mesa’s resulting negotiated drug pricing experience from their PBM should be in excess of what most consider to be the manufacturer’s list price, which is WAC (yes, both AWP and WAC can be considered list prices when using shorthand, but in general, when lawmakers, reporters, researchers, etc. refer to a list price, it is almost always is WAC).&lt;/p&gt;
    &lt;p&gt;Consider, for example, a brand-name drug with a $100 per unit WAC price. At a $100 WAC unit price, we can reasonably assume the AWP of this product is $120 per unit (given the 1.2 multiplier established from the AWP lawsuits). Well, if the payer’s contract with the PBM states that the PBM’s brand effective rate guarantee is going to be AWP-10.75%, then for this hypothetical product, we can anticipate spending $107.10 per unit on this product (or approximately 7% more than the manufacturer’s WAC list price). Was it the manufacturer’s list price that was responsible for spending above list price or was it the PBM contractual mechanics or both? We’ll let you ponder that one. Note we’ve previously observed price points above WAC for brand prices in Medicare and this observation may help explain some of those observations.&lt;/p&gt;
    &lt;p&gt;And if you follow all that, what does our AWP-to-WAC relationship potentially tell us about generic drug pricing? Again, if there is a generic product with a $10 WAC unit price, and we assume (probably mistakenly) that the same 20% mark-up applies to its AWP, then we would anticipate that an AWP-based generic discount of 80% would mean a plan would pay just $2.40 per unit on this product (or 76% below the WAC price). However, what if the AWP-to-WAC ratio is 38-to-1 (the average AWP for Quallent’s products in this study) or approximately 200-to-1 (the top end of Quallent’s AWP-to-WAC ratios in their product portfolio). At these more significantly disconnected ratios, a plan sponsor’s AWP-based discount of 80% now means they’re paying $76 per unit or $400 per unit (38 vs. 200 multiplier respectively). Under such context, the importance of the multipliers comes more into focus. The multipliers mean getting a good perception on drug pricing can be difficult, as the actual multipliers are higher than we might expect (would you expect a 200-fold difference?), and even the most attractive-looking discounts may be insufficient to make up for this gap.&lt;/p&gt;
    &lt;p&gt;To put more plainly, and using hypothetical numbers to demonstrate, if you have a generic drug that has a $100 WAC and a $2,000 AWP being managed under a contract that is producing an AWP-80% discount on generic drugs, on paper, the PBM would have saved $1,600 off the AWP — a massive savings considering the sticker price. But from an alternative perspective, one would expect pharmacies to be acquiring this drug below WAC, so less than $100. Assuming pass-through payments, meaning no spreads or hidden PBM markups between the price the pharmacy is paid vs what they charge the plan sponsor, that would mean the pharmacy would be paid $400 for a drug that that actually cost less than $100. Considering that the average pharmacy needs just around $10 in margin per claim to break even on their overhead, a $300+ markup isn’t a deal; it’s arguably a rip-off. So if you’re sitting there wondering why a PBM’s affiliated drug labeler isn’t setting AWPs at or below the lowest levels in the market, and if you’re wondering why the PBM’s affiliated pharmacies are stocking a product with such a high AWP, and if you’re wondering why the PBM isn’t refusing to cover a product with such a high AWP or at least finding a different cost-based way of achieving a negotiated price, perhaps an answer is that if all these entities are under common ownership, vertical integration isn’t being used for efficiency but instead for profit maximization. Gasp,right?&lt;/p&gt;
    &lt;p&gt;Look, we already mentioned it in our last report, but it bears repeating here. We should not lose sight of the fact that at the same time Quallent is setting these prices for products like imatinib, other groups like Mark Cuban Cost Plus Drug Company had the ability to give you an entire year’s worth of the medication for less than the price of one Quallent-sourced prescription (at least with imatinib as our judge). While it is interesting to contemplate the true intent that drive these kinds of financial outcomes — although perhaps the FTC dropped a good hint — rather we think it is far more worthwhile to focus the failure of our system to incentivize the things we want it to actually do.&lt;/p&gt;
    &lt;p&gt;For the removal of doubt, we do not think Quallent is alone in following these trends. In fact, we think there are others in the drug supply chain who may have made the “evolutionary leap” with these kind of drug reference pricing games before PBM private label products. We’ll try to review them quickly in our already lengthy analysis today.&lt;/p&gt;
    &lt;head rend="h3"&gt;Evolutionary ancestors: Wholesaler-owned private labels&lt;/head&gt;
    &lt;p&gt;Before vertical PBM companies launched private label products, other companies had already etched their place in the annals of history. As a matter of fact, we have federal definitions of private label distribution and distributor because of the actions of companies before PBM-private label products existed. We also have a technical distinction between private label activities from those of repacking activities related to prescription drugs (again, also because of the actions of companies that pre-date PBM private label products). In other words, PBMs aren’t the first companies to play in the private-label gene pool, so let’s look at what other members of the supply chain who have private label products might teach us.&lt;/p&gt;
    &lt;p&gt;The three largest prescription drug wholesalers in the United States are McKesson, Cencora, and Cardinal Health. At least two of the three have their own private label catalogs and private label catalogs that pre-date PBM these activities. For the unfamiliar, the following links each wholesaler with their known private label catalog (maybe Cardinal Health has one and we’re just unaware):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;McKesson has NorthStar and Aisling&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Cencora has Blue Point&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;What this means is that we can compare the pricing dynamics of these wholesaler private label products, which came before PBM private labels (and therefore may have influenced PBMs’ own strategies with their affiliated private label offerings) and make comparisons between their pricing behavior and Quallent. For the sake of this analysis, we’re not going to recreate all the AWP and WAC charts for each wholesaler private labels on all the Quallent-equivalent products. Rather, we’re just going to add their AWP-to-WAC ratios to the earlier Quallent bar chart (Figure 8). Again, we’re taking this short cut because the AWP-to-WAC ratio is perhaps the most interesting as it speaks to the disconnect between the price that might be anticipated to be incurred to put the product on the shelf (WAC) and the price that might be anticipated to be reimbursed when selling the product off that same shelf (i.e., AWP). When we do so, you can see that while Quallent does seem to be at the high-end of these ratios; only not the case in about a third of all the comparisons that can be made in the overlapping product portfolios of Quallent, McKesson and/or Cencora.&lt;/p&gt;
    &lt;p&gt;What does this ultimately tell us? Well, to start, probably that we need to spend some more time exploring some of these wholesaler behaviors, but that is probably a story for another day. As it relates to what we’re discussing today, on its surface it demonstrates the PBMs are not the only ones that understand the importance of AWP price points. As PBMs adapt their business model to the ever-changing healthcare landscape, it stands to reason they’d look to see what has worked for others. McKesson, for example, is company #9 and Cencora is #10 on Fortune’s 500 list, which we suspect may mean PBMs monitor their behavior as part of their everyday business (and potentially model or adapt some of their programs and initiatives based upon what they’ve encountered working with and alongside these wholesalers). Even if PBM’s don’t, Figure 8 demonstrates that PBMs are not the only ones who this report could have been about; except we’re not aware of the wholesalers telling us they’re the only one working to lower prices (which I guess, in fairness, Quallent’s WACs are low so kudos there)&lt;/p&gt;
    &lt;head rend="h3"&gt;AWP vs. NADAC pricing trends: “But you didn’t ask for reality. You asked for more teeth.”&lt;/head&gt;
    &lt;p&gt;Before we wrap things up, we feel remiss to not at least bring some NADAC learnings into this discussion. By definition, NADAC is a pricing benchmark outside of manufacturer list price control. A manufacturer sets their sticker prices, but invoice price observations between pharmacy purchases from wholesalers ultimately determine the NADAC value. And if a wholesaler is doing something potentially weird, or out-of-the-norm things with their pricing, then NADAC will essentially smooth that out, as it serves as a kind of MAC list for generic prices (an average of the submitted pharmacy acquisition costs, meaning some highs and lows will occur therein). Objectively, if you look at the 124 products that make up this analysis (the Quallent catalog), on an AWP-basis, the underlying NDCs have seen effectively no price movement over time. If we take each product and look at the original published AWP price point to its current one (at least within what Elsevier offers), what we see is that AWPs don’t really ever change, and if they do change, they tend to increase more than they decrease.&lt;/p&gt;
    &lt;head rend="h2"&gt;AWP Price Change Behavior, Quallent &amp;amp; Equivalent Products&lt;/head&gt;
    &lt;p&gt;Alternatively, if we look at the NADAC price points for these same products, comparing the first NADAC price to the current NADAC price, what we see is the exact opposite. Almost every price has decreased over time.&lt;/p&gt;
    &lt;head rend="h2"&gt;NADAC Price Change Behavior, Quallent &amp;amp; Equivalent Products&lt;/head&gt;
    &lt;p&gt;And although these products are launching and coming to market at different times, the average NADAC change experience from their respective starting points (which we recognize is a bunch of different starting points) to current is a decline in NADAC price of 58% (all end prices were pulled at start of July 2025). To be clear, the average time from a product’s first NADAC price to its current NADAC price across these products is 3,878 days (or around 10 years). So these products have, in general, been around for a long time and have seen a great deal of variability in their NADAC prices over that time. Now if you don’t like pricing volatility (i.e. a responsive marketplace) and if you don’t like drug prices going down, we can see how AWP would be an appealing benchmark, but we’re not sure that is what anyone actually says they want drug prices to do.&lt;/p&gt;
    &lt;p&gt;So to put a bow on this NADAC vs AWP analysis, the main takeaways are the following: The sticker prices for these medicines in general never change, and if they do, they increase more often than they decrease over time. Alternatively, the price of the medication as bought over time seems to be getting cheaper (if you know about NADAC pricing anyway).&lt;/p&gt;
    &lt;head rend="h3"&gt;It’s a dino-eat-dino world out there. Be at the table; not on the menu.&lt;/head&gt;
    &lt;p&gt;Perhaps more interesting, and more relevant to Adam Fein’s query that started us down this pathway, is why we are observing the pricing behavior in these data. No one forces a drug manufacturer to set an AWP price point. It is actually optional. In fact, many brands that come to market anymore do not directly establish an AWP. Rather, editorial policies of the prominent drug pricing compendia (Medi-Span, First Databank, etc.) will set an AWP for products that do not publish them (at the 1.2-multiple of the WAC; see example policy here).&lt;/p&gt;
    &lt;p&gt;Mark Cuban ran into this when they initially launched their albendazole product. Despite setting the lowest WAC price available at the time, we understand that they did not get very many typical drug channel customers for their albendazole product. We now can understand that this likely occurred because if you’re a pharmacy locked into an AWP-based payment contract (which, to be clear, isn’t always the way pharmacy is paid), then you cannot afford to buy a product that is going to be reimbursed to you at AWP-80%+ if the gap between your purchase price and the sale price isn’t sufficiently high enough to cover this gap. Again, we note that the incentives of the current ecosystem seem to favor high prices, not low (which may explain why Cuban often has to work outside prevailing health insurance company models to drive savings to the end payer).&lt;/p&gt;
    &lt;p&gt;Ultimately, what we find most interesting about this analysis is trying to understand what drove Quallent to its AWP-price point decisions. We observe that Quallent doesn’t appear to have a standard approach to the setting and maintenance of their drug prices. We say this because there is no standard ratio between their products’ AWPs and their underlying WACs (see Figure 7 earlier). If they had simply not set an AWP price point, the editorial policies of the system would have set one fixed ratio for them, for their products. But that is not what we observe. In fact, we don’t observe any obvious one policy to their pricing (as ratios go from the 1.2 up to over 200x; again Figure 7). This would seem to speak to some strategy here, but not one that seems obviously aligned with “low prices" (at least outside of WAC, which isn’t the price point that generally matters to the end payer). We also see at least some real-world evidence for what the reality may have looked like if they had adopted low-AWP prices (at least with imatinib as our judge and their AWP-to-WAC ratio vs. that of one of their competitors). As a manufacturer, they have control over both price points, and yet they are set in perhaps unexpected ways (if the goal is to work towards lowering drug prices, right?). How many PBM customers have contracts that pay on an AWP basis vs. how many have contracts that pay on WAC basis? If we look at financial statements of these companies, they tell us they understand the importance of drug reference prices. While this is not financial advice, we encourage you to look at the 10-K statements of the PBM parent companies. You’ll find guidance to their investors or potential investors that looks something like what the Cigna Group’s says in Figure 9:&lt;/p&gt;
    &lt;p&gt;And if you go back in time, you’ll see that as recently as 2019, the guidance was more explicit. The 10-K back then for the same group (depending upon your Ship of Theseus feelings on sameness of group post acquisitions) mentions AWP pricing by name, essentially disclosing that if AWP goes away, it could have a material adverse effect on their business and results of their operations (Figure 10):&lt;/p&gt;
    &lt;p&gt;Look, it is admirable that companies like Express Scripts, CVS, and Optum say they are working towards cost-plus models. CVS specifically has highlighted the pitfalls of the market basket pricing that has been borne out of AWP-based models. However, those legacy PBM efforts seem to be worth a little more scrutiny as the PBMs also become drug companies. After all, it was the PBMs that told us that manufacturers need to lower their list prices. And yet, now that PBMs and their parent companies are donning the robes of the drug manufacturer, the Quallent generic data points and the broader PBM private label biosimilar offerings do not appear to be following their own advice (or at least not fully given AWP and WAC disconnects). In a world where PBMs traditionally anchor pricing guarantees for plan sponsors to AWP, and the AWPs set by the PBM’s affiliated drug labeler are not just routinely above the norm, but in fact are way above the norm, how are we honestly supposed to assess the integrity of their claims to desire low prices when they are so obviously setting high ones?&lt;/p&gt;
    &lt;p&gt;And building on this theme, if PBMs do in fact forgo their legacy AWP pricing models in favor of cost-plus pricing, if they get to subjectively define what cost is, then are we really making any real progress here? If PBMs are using AWP-based models as a profit-generation tool instead of a fiduciary-like endeavor to maximize savings for payers, then why should we expect anything materially different from the same companies’ cost-plus models moving forward? Just because you set a low ingredient cost doesn’t mean that the cost of the dispensing fee, shipping fee, administrative fee or others fees that may drive a cost-plus pricing future are fair and reasonable. As publicly-traded companies with fiduciary obligations to shareholders and histories of making old money with new logistics in the face of public scrutiny, why shouldn’t we be looking at these latest contortions as the just another long journey back to the starting line?&lt;/p&gt;
    &lt;p&gt;Which, in turn means, we think we ultimately need to recognize that these harsh realities may be more of a function of the drug pricing environment we find ourselves in: an environment that does not set financial rewards for making us healthier, but rather sets financial rewards for those who are best able to control the information about what a drug’s price is. This begs the question, is the environment actually hostile to lower drug prices such that adaptive responses that seek to lower them (be they Mark Cuban, Civica, or others) doomed to evolutionary failure?&lt;/p&gt;
    &lt;p&gt;Our collective actions in what we decide we want drug pricing to do and accomplish will ultimately be the judge of that.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45892469</guid><pubDate>Tue, 11 Nov 2025 20:34:51 +0000</pubDate></item><item><title>Meticulous (YC S21) is hiring to redefine software dev</title><link>https://jobs.ashbyhq.com/meticulous/3197ae3d-bb26-4750-9ed7-b830f640515e</link><description>&lt;doc fingerprint="e10fcdab2cdf53e4"&gt;
  &lt;main&gt;
    &lt;p&gt;You need to enable JavaScript to run this app.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45892773</guid><pubDate>Tue, 11 Nov 2025 21:00:35 +0000</pubDate></item><item><title>CACM Practice section welcomes submissions</title><link>https://dl.acm.org/doi/10.1145/3771297</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45892809</guid><pubDate>Tue, 11 Nov 2025 21:03:19 +0000</pubDate></item><item><title>X5.1 solar flare, G4 geomagnetic storm watch</title><link>https://www.spaceweatherlive.com/en/news/view/593/20251111-x5-1-solar-flare-g4-geomagnetic-storm-watch.html</link><description>&lt;doc fingerprint="77527d2b7f75e40e"&gt;
  &lt;main&gt;
    &lt;p&gt;Tuesday, 11 November 2025 19:07 UTC&lt;/p&gt;
    &lt;p&gt;Here she blows! Sunspot region 4274 produced its strongest solar flare thus far since it appeared on the east limb and the sixth strongest solar flare of the current solar cycle. An impressive long duration and highly eruptive X5.1 (R3-strong) solar flare peaked this morning at 10:04 UTC.&lt;/p&gt;
    &lt;p&gt;It became quickly clear that the eruption would be followed by an impressive coronal mass ejection (CME). The resulting coronal wave following the solar explosion as well as the coronal dimming observed as the CME was propelled into space were of a spectacular magnitude as can be seen in the animation below provided by halocme.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Another eruption from AR12474, associated with an X5.1 flare. It has become a full halo CME. I am truly impressed by how fast and global this coronal wave is. The CME will arrive on November 13, but because of earlier CMEs it will be challenging to isolate the ICME from this. pic.twitter.com/H6eNjzQUGz&lt;/p&gt;— Halo CME (@halocme) November 11, 2025&lt;/quote&gt;
    &lt;p&gt;Taking a look at coronagraph imagery provided by GOES-19 CCOR-1 we see the gorgeous fast halo coronal mass ejection as it propagates away from the Sun. It doesn't take a rocket scientist to come to the conclusion that this plasma cloud of course has an earth-directed component and it is pretty clear that this will be a strong impact when it arrives at our planet. This rightfully so prompted the NOAA SWPC to issue a G4 or greater geomagnetic storm watch for tomorrow as the cloud could impact our planet as early as 16 UTC on 12 November. Not only is the CME fast but it will also travel trough an area with high ambient solar wind speed and low density thanks to two other CMEs released earlier by this region. More about that below.&lt;/p&gt;
    &lt;p&gt;If the solar wind and interplanetary magnetic field values at Earth are favorable this could result in a geomagnetic storm which is strong enough for aurora to become visible from locations as far south as northern France, Germany, Ukraine, Switzerland and Austria. In the US it could become visible as far south as Nevada and Arkansas. No guarantees of course, this is space weather we are talking about but be sure to download the SpaceWeatherLive app to your mobile device, turn on the alerts and keep an eye on the solar wind data from ACE and DSCOVR!&lt;/p&gt;
    &lt;p&gt;We also want to remind you that we still have two coronal mass ejections on their way to Earth. These are not as impressive as this X5.1 CME but these two plasma clouds will likely arrive within the next 6 to 18 hours. This is a tricky one as they could arrive as one impact or two impacts close intill each other. More information in yesterday's news.&lt;/p&gt;
    &lt;p&gt;Thank you for reading this article! Did you have any trouble with the technical terms used in this article? Our help section is the place to be where you can find in-depth articles, a FAQ and a list with common abbreviations. Still puzzled? Just post on our forum where we will help you the best we can!&lt;/p&gt;
    &lt;p&gt;A lot of people come to SpaceWeatherLive to follow the Solar activity or if there is a chance to see the aurora, but with more traffic comes higher costs to keep the servers online. If you like SpaceWeatherLive and want to support the project you can choose a subscription for an ad-free site or consider a donation. With your help we can keep SpaceWeatherLive online!&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Last X-flare&lt;/cell&gt;
        &lt;cell&gt;2025/11/11&lt;/cell&gt;
        &lt;cell&gt;X5.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Last M-flare&lt;/cell&gt;
        &lt;cell&gt;2025/11/11&lt;/cell&gt;
        &lt;cell&gt;M1.4&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Last geomagnetic storm&lt;/cell&gt;
        &lt;cell&gt;2025/11/08&lt;/cell&gt;
        &lt;cell&gt;Kp6+ (G2)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Spotless days&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Last spotless day&lt;/cell&gt;
        &lt;cell&gt;2022/06/08&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Monthly mean Sunspot Number&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;October 2025&lt;/cell&gt;
        &lt;cell&gt;114.6 -15.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;November 2025&lt;/cell&gt;
        &lt;cell&gt;91.9 -22.7&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Last 30 days&lt;/cell&gt;
        &lt;cell&gt;97.1 -31.7&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45893004</guid><pubDate>Tue, 11 Nov 2025 21:18:26 +0000</pubDate></item><item><title>I didn't reverse-engineer the protocol for my blood pressure monitor in 24 hours</title><link>https://james.belchamber.com/articles/blood-pressure-monitor-reverse-engineering/</link><description>&lt;doc fingerprint="b6398d534b2d77a1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;I didn't reverse-engineer the protocol for my blood pressure monitor in 24 hours&lt;/head&gt;
    &lt;p&gt;Yesterday after receiving my yearly flu vaccine at the pharmacy I was offered a blood pressure test, which reported a reading that made the young pharmacist who had just given me my vaccine a bit worried.&lt;/p&gt;
    &lt;p&gt;Off the back of this she offered me a 24 hour study, and then strapped a cuff to my arm plumbed into a little device which I had to wear in a little caddy - the cuff would inflate every 30 minutes during the day and every 60 minutes during the night, and then tomorrow I would bring it back for analysis.&lt;/p&gt;
    &lt;p&gt;"Can I read the measurements?" I asked, as it was being strapped to me.&lt;/p&gt;
    &lt;p&gt;"Oh, no, that will just stress you out. We turn that off". Fair enough.&lt;/p&gt;
    &lt;p&gt;Thing is, this device had a little micro-USB port on the side.&lt;/p&gt;
    &lt;head rend="h1"&gt;Doing things the proper way&lt;/head&gt;
    &lt;p&gt;I had started researching the device - a Microlife WatchBP O3 - before I got out of the chemist, and once I'd got back to the office I downloaded the software that's freely available to interact with it, setting up a Bottles instance to run the software since I don't (knowingly) have a Windows machine within 100 metres of me.&lt;/p&gt;
    &lt;p&gt;Unfortunately it didn't seem to be able to access the device, and I had no clue why. In Linux it was just presenting as a standard &lt;code&gt;hidraw&lt;/code&gt; device:&lt;/p&gt;
    &lt;code&gt;[33301.736724] hid-generic 0003:04D9:B554.001E: hiddev96,hidraw1: USB HID v1.11 Device [USB HID UART Bridge] on usb-0000:c5:00.0-1/input0
&lt;/code&gt;
    &lt;p&gt;Fine, I'll install windows.&lt;/p&gt;
    &lt;p&gt;After dodging around Microsoft's idea of UX, and then forwarding the USB device to the VM (I used Gnome Boxes for this, works nicely), I finally got to see WatchBP Analyzer with the data downloaded from the device.&lt;/p&gt;
    &lt;p&gt;But I don't want to open a Virtual Machine running Windows to see this data, and anyway - I'm pretty sure that reverse-engineering this will be good for my blood pressure.&lt;/p&gt;
    &lt;head rend="h1"&gt;Sniffing the traffic&lt;/head&gt;
    &lt;p&gt;Since I'm running this in a Virtual Machine I can just rely on Wireshark in Linux to get the traffic between the host and the device. &lt;code&gt;usbmon&lt;/code&gt; is already installed and we know that the device is on Bus 3, so we can select usbmon3 on startup and start capturing.&lt;/p&gt;
    &lt;p&gt;I'm very much out of my depth at this point but, being one of those who could land a plane in an emergency (why would you talk yourself out of it?!) I decided to crack on regardless. I know that the interesting stuff is sent after I press "Download", and I know that something in there is gonna say "my blood pressure is 137/113" - so let's look for that. Just convert to show bytes as decimal and..&lt;/p&gt;
    &lt;p&gt;..that looks like a blood pressure! Let's copy that out as hex:&lt;/p&gt;
    &lt;code&gt;05 0a 89 71 43 9b
&lt;/code&gt;
    &lt;p&gt;I'm not sure if this is "valid" HID Data (Wireshark seems convinced that only the first byte is the Vendor Data, with the rest being padding) but it seems like the data is being sent in 32-byte "chunks", of which the first byte tells you the number of significant (SIG) following bits in the chunk (I deleted the rest - all zeroes - for clarity). The third byte is my Systolic blood Pressure (SYS), the fourth is my Diastolic blood pressure (DIA), and the fifth is my heart rate (HR) - no clue what the second or last byte is, but let's find all other bytes with my blood pressure in them (in decimal this time, because I can't read hex without help):&lt;/p&gt;
    &lt;code&gt;SIG ??? SYS DIA  HR ??? ??? ???
  5  10 137 113  67 155
  5   0 132  86  68 155
  6   0 126  84  82 155  83
  6  10 128  80  61 155  83
  7   0 148  93  65 155  83  64
  7   0 121  92  74 155  83  94
  7   0 123  83  65 155  83  95
  7   0 123  79  78 155  83 129
&lt;/code&gt;
    &lt;p&gt;Hmm. So we're still looking for the Oscillometric signal peak pressure (OPP)as well as some timestamps (we can calculate Mean arterial pressure - MAP - as &lt;code&gt;(2*DIA+SYS)/3&lt;/code&gt;, according to the manual, and Pulse Pressure (PP) is just &lt;code&gt;SYS-DIA&lt;/code&gt;). We can see the OPP in the packets that come after each of those above, but they don't seem to consistently come in on the same line:&lt;/p&gt;
    &lt;code&gt; 10  82  195   80 *121    0    0    0    0    0    0
 10  82  223   80  *95    0    0    0    0    0    0
  9   1   80  *90    0    0    0    0    0    0
  9  35   80  *86    0    0    0    0    0    0
  8  80 *103    0    0    0    0    0    0
  8  80 *106    0    0    0    0    0    0
  8  80  *90    0    0    0    0    0    0
 10  80  *88    0    0    0    0    0    0   29  251
&lt;/code&gt;
    &lt;p&gt;Oh. Maybe if I stick them together?&lt;/p&gt;
    &lt;code&gt;??? SYS DIA  HR ??? ??? ??? ??? OPP ??? ??? ??? ??? ??? ??? ??? ???
 10 137 113  67 155  82 195  80 121   0   0   0   0   0   0
  0 132  86  68 155  82 223  80  95   0   0   0   0   0   0
  0 126  84  82 155  83   1  80  90   0   0   0   0   0   0
 10 128  80  61 155  83  35  80  86   0   0   0   0   0   0
  0 148  93  65 155  83  64  80 103   0   0   0   0   0   0
  0 121  92  74 155  83  94  80 106   0   0   0   0   0   0
  0 123  83  65 155  83  95  80  90   0   0   0   0   0   0
  0 123  79  78 155  83 129  80  88   0   0   0   0   0   0  29 251
&lt;/code&gt;
    &lt;p&gt;Right, timestamps. I first guessed that the four populated contiguous bytes between &lt;code&gt;HR&lt;/code&gt; and &lt;code&gt;OPP&lt;/code&gt; are a 32-bit unix timestamp, but that would make the first one &lt;code&gt;9B52C350&lt;/code&gt;; either &lt;code&gt;Jul 29 2052&lt;/code&gt; or &lt;code&gt;Dec 08 2012&lt;/code&gt; depending on which endianness the protocol is into. The 8 readings we have here are all from &lt;code&gt;November 10th&lt;/code&gt;, at &lt;code&gt;11:03&lt;/code&gt;, &lt;code&gt;11:31&lt;/code&gt;, &lt;code&gt;12:01&lt;/code&gt;, &lt;code&gt;12:35&lt;/code&gt;, &lt;code&gt;13:00&lt;/code&gt;, &lt;code&gt;13:30&lt;/code&gt;, &lt;code&gt;13:31&lt;/code&gt; and &lt;code&gt;14:01&lt;/code&gt;, which isn't.. isn't that.&lt;/p&gt;
    &lt;p&gt;But note that the number in the 6th column flips from &lt;code&gt;82&lt;/code&gt; to &lt;code&gt;83&lt;/code&gt; when we switch from AM to PM - that's something, and when it does the 7th column resets. And hey - &lt;code&gt;1&lt;/code&gt;, &lt;code&gt;35&lt;/code&gt;, &lt;code&gt;64&lt;/code&gt;, &lt;code&gt;94&lt;/code&gt;, &lt;code&gt;95&lt;/code&gt;.. that seems dangerously close to &lt;code&gt;12:01&lt;/code&gt;, &lt;code&gt;12:35&lt;/code&gt;, &lt;code&gt;13:00&lt;/code&gt;, &lt;code&gt;13:30&lt;/code&gt; and &lt;code&gt;13:31&lt;/code&gt; if you were just to count the minutes. What's going on?&lt;/p&gt;
    &lt;head rend="h1"&gt;Deadlines and dead ends&lt;/head&gt;
    &lt;p&gt;I tried feeding a lot of this into various Als (Kagi gives you access to a few with a nice interface) and I found that they mostly were stupid in ways that made me think. A few times I thought they had "cracked the case" but actually they just made me waste time. But they did remind me e.g. of endianness, so I did get a bit out of them.&lt;/p&gt;
    &lt;p&gt;I also spent quite a bit of time trying to write some Python that emulated the initial handshake and download button of the interface so that it could push out the data as a stream instead of me having to wrestle it out of Wireshark - again, Al had a habit of giving me incorrect code (although it did turn me on to pyhidapi).&lt;/p&gt;
    &lt;p&gt;But ultimately I had a deadline, and I had to return the device even though I wanted to spend more time with it. Possibly for the best - while it did give me some reverse engineering practice (which it turns out I really enjoy), I should do some work instead of procrastinating.&lt;/p&gt;
    &lt;p&gt;My final lesson was a new word - Normotension, normal blood pressure - and a new phrase - White Coat Hypertension, the phenomena of high blood pressure in a clinical setting. Turns out that when you check someone's blood pressure after giving them an injection, it's higher than normal.&lt;/p&gt;
    &lt;p&gt;I don't think I'd recommend getting your blood pressure tested after your next flu jab. But then, I'm not a doctor.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45893095</guid><pubDate>Tue, 11 Nov 2025 21:25:19 +0000</pubDate></item></channel></rss>