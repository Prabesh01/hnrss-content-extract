<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Sun, 21 Sep 2025 16:10:26 +0000</lastBuildDate><item><title>Bluefin LTS Is Released</title><link>https://docs.projectbluefin.io/blog/bluefin-lts-ga/</link><description>&lt;doc fingerprint="bd404c229122b527"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Bluefin LTS: Reinventing the Steel&lt;/head&gt;
    &lt;p&gt;Achillobator giganticus&lt;/p&gt;
    &lt;p&gt;After nine months of development Bluefin LTS and Bluefin GDX are now Generally Available(GA). The reign of Achillobator has begun. Find the download links on the website, or snag them below.&lt;/p&gt;
    &lt;p&gt;Bluefin LTS is a workstation designed for people who prefer Long Term Support but desire a modern desktop. This species of raptor is for users who prefer a slower release cadence, about a three-to-five year lifespan on a single release. Like other Bluefins it features first-class support for Flathub via Bazaar, Homebrew, ZFS, and all the other goodies.&lt;/p&gt;
    &lt;p&gt;Bluefin LTS is composed of:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Mostly the same packages of Bluefin and Bluefin GTS, but built with CentOS Stream 10 and EPEL for extra packages. &lt;list rend="ul"&gt;&lt;item&gt;The same features since they share the same source RPMs, just built on CentOS&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;A backported GNOME 48 desktop&lt;/item&gt;
      &lt;item&gt;ARM (aarch64) based images&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Bluefin LTS also offers a hardware enablement branch (&lt;code&gt;bluefin:lts-hwe&lt;/code&gt;) with:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Updated Linux kernel, currently 6.15.9.&lt;/item&gt;
      &lt;item&gt;Toggle between branches with &lt;code&gt;ujust rebase-helper&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Dedicated HWE ISOs for newer kit like the Framework 12 and Framework Desktop.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Rationale&lt;/head&gt;
    &lt;p&gt;Bluefin LTS ships with Linux 6.12.0, which is the kernel for the lifetime of release. An optional &lt;code&gt;hwe&lt;/code&gt; branch with new kernels is available, offering the same modern kernel you'll find in Bluefin and Bluefin GTS. Both vanilla and HWE ISOs are available, and you can always choose to switch back and forth after installation.&lt;/p&gt;
    &lt;p&gt;I have been dogfooding Bluefin LTS for most of this year on my Framework 13 and my Framework Desktop. This is the most "work focused" image and is suitable for "set it and forget it" style desktops. We are proud of this one!&lt;/p&gt;
    &lt;p&gt;Here's how I pitched the idea earlier this year:&lt;/p&gt;
    &lt;head rend="h2"&gt;A modern GNOME Desktop&lt;/head&gt;
    &lt;p&gt;Bluefin LTS provides a backported GNOME desktop so that you are not left behind. This is an important thing for us. James has been diligenlty working on GNOME backports with the upstream CentOS community, and we feel bringing modern GNOME desktops to an LTS makes sense. I may be old but I'm not dead!&lt;/p&gt;
    &lt;p&gt;A very special thanks to Jordan Petridis from GNOME for technical advice and review.&lt;/p&gt;
    &lt;head rend="h2"&gt;New Installation Experience&lt;/head&gt;
    &lt;p&gt;Installation is via a live session with the new Anaconda webui. This installer is miles better than the ones we used to ship, thanks to the Anaconda team.&lt;/p&gt;
    &lt;p&gt;Secure boot and all those goodies are available:&lt;/p&gt;
    &lt;head rend="h2"&gt;Update Cadence&lt;/head&gt;
    &lt;p&gt;Updates will come as often as we need them for now and will settle into weekly releases on Tuesdays. Follow updates on changelogs.projectbluefin.io.&lt;/p&gt;
    &lt;head rend="h2"&gt;Errata&lt;/head&gt;
    &lt;p&gt;There are a few lingering issues that will take more time:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;hwe doesn't have secure boot - we hope to revisit later this fall&lt;/item&gt;
      &lt;item&gt;Bazaar curated config is missing - the curated section isn't available yet&lt;/item&gt;
      &lt;item&gt;gnome-user-share is missing - we recommend using rclone mount instead&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;What is Bluefin GDX?&lt;/head&gt;
    &lt;p&gt;Bluefin GDX is designed to be an AI Workstation by providng Nvidia drivers and CUDA in one image. It combines Bluefin LTS with the Bluefin Developer Experience. There's no cool expansion of GDX: GPU Developer Experience I guess. Maybe someday we'll call it Bluefin CUDA Edition. (Jensen call me!)&lt;/p&gt;
    &lt;p&gt;The reason we brand it differently is that it is designed for AI and Machine Learning professionals. Instead of a multitude of Nvidia images like Bluefin we will concentrate on this one image to focus on one thing: this is our platform for open source AI. Improvements made in GDX will make it's way into Bluefin's developer mode. GDX gives us a place to rev fast with some new friends:&lt;/p&gt;
    &lt;p&gt;We are happy to announce that we've formed a community partnership with the Red Hat Enterprise Linux Command Line Assistant team. We are collaborating on upstream and open source AI and ML tools to provide system-wide inference and an enhanced experience for Bluefin LTS and GDX users.&lt;/p&gt;
    &lt;p&gt;This will be the lab that will keep Bluefin on the leading edge of open source AI. Here's Mo Duffy on Destination Linux to give you an idea of what we're thinking about.&lt;/p&gt;
    &lt;head rend="h2"&gt;Features&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Nvidia CUDA and VSCode integrations and full secureboot support out of the box. There are many parts of the CUDA ecosystem that need to be included, but the raw core is there and ready to be expanded upon.&lt;/item&gt;
      &lt;item&gt;Ramalama for local management and serving of AI models&lt;/item&gt;
      &lt;item&gt;uv for Python package management&lt;/item&gt;
      &lt;item&gt;... and more! Check the AI and Machine Learning section for more!&lt;/item&gt;
      &lt;item&gt;We are looking for AI/ML enthusiasts with strong opinions who want to be involved! Inquire within!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You can find Bluefin GDX on the conference circuit!&lt;/p&gt;
    &lt;head rend="h1"&gt;What this all means&lt;/head&gt;
    &lt;p&gt;Bluefin LTS will end up being way more sustainable than Bluefin and Bluefin GTS from a developer perspective. It's more of an initial setup and then we don't touch it as often. We have had periods in beta development where we didn't need to touch it for weeks. If you look changelogs.projectbluefin.io you'll soon notice the pattern, mostly minor version bumps. Nice.&lt;/p&gt;
    &lt;p&gt;It's also much more advantageous for us to derive off of a base image that ends up being a commercial product -- there is no doubt that CentOS and Red Hat have their weight behind these base images, whereas we are unable to get that level of commitment from Fedora. And as Steven Rosenberg pointed out, Fedora isn't really improving in this area, and with &lt;code&gt;bootc&lt;/code&gt;'s composefs work coming along nicely we now have multiple base images to choose from. It will be an interesting year!&lt;/p&gt;
    &lt;head rend="h2"&gt;What does this mean for Bluefin GTS?&lt;/head&gt;
    &lt;p&gt;As it turns out, Bluefin LTS HWE is in the exact same ecological niche as GTS. They will end up being competitors. There's no death knell or anything like that, once development moves on it doesn't cost us much to keep it running. And we do love our pets. Check out the awesome brand new image by Delphic Melody and ahmedadan:&lt;/p&gt;
    &lt;p&gt;As you can see, it's getting a bit crowded. We'll see how people react to LTS, and I expect we'd hide the GTS option from the website but continue to offer it.&lt;/p&gt;
    &lt;p&gt;With bootc we can deliver a desktop experience with the latest GNOME, and a new kernel -- but on a solid base with less regressions. The previous generation of thinking kept CentOS in a very locked set of use cases. The old boring ones. Now with bootc + containers + flathub + homebrew, we feel that this less churny base makes for a compelling desktop. We'll see how they compete!&lt;/p&gt;
    &lt;head rend="h2"&gt;Less bitey Bluefin and Bluefin GTS&lt;/head&gt;
    &lt;p&gt;Since Bluefin LTS is "bootc natural" and not a transplant, it comes with less compromises out of the box. Bluefin LTS doesn't support local layering at all and AppImages don't work either. (Told ya'll those things were not gonna make it lol.) Bluefin LTS also does not support older machines with v2 CPU instructions.&lt;/p&gt;
    &lt;p&gt;This also lets us be less strict in Bluefin. We've decided to leave local layering enabled by default in Bluefin and Bluefin GTS. There are users who use that ecosystem, so no worries there. Savages. The Fedora based images will continue to serve these use cases. James also has his own tunaOS, which offers a wide variety of Bluefin-derived variants, including an AlmaLinux based sister to Bluefin LTS. That covers just about everybody - The &lt;code&gt;bootc&lt;/code&gt; community around CentOS is quite diverse, and offers a variety of options.&lt;/p&gt;
    &lt;p&gt;The downloads page is looking pretty good these days but I am very interested to see what you decide since we do measure everything, so feel free to peruse that list. 😄&lt;/p&gt;
    &lt;head rend="h1"&gt;Merch&lt;/head&gt;
    &lt;p&gt;Now let's get on to the good stuff. store.projectbluefin.io will take you to the new Bluefin store, which has a ton of awesome items!&lt;/p&gt;
    &lt;p&gt;We celebrate this release with this T-shirt, the "Reign of Achillobator", signifying Bluefin LTS's role in this ecosystem as a top predator, along with some other goodies:&lt;/p&gt;
    &lt;p&gt;And of course we've got stuff for the kids, and some other weird things! Currently this store is US only for now.&lt;/p&gt;
    &lt;p&gt;Proceeds from the store items will go towards paying for more paleoartwork. I think this is a fair deal, Bluefin would have never gotten this far without the work of these fine artists. Having a way for the community to sponsor the artwork in return for the awesome comfort of an Achillobator giganticus hoodie? Peak Linux.&lt;/p&gt;
    &lt;head rend="h1"&gt;Special Thanks&lt;/head&gt;
    &lt;p&gt;Bluefin is brought to you by Tulip Blossom and James Reilly. The team grew this cycle with some fantastic new folks helping out to finish Bluefin LTS:&lt;/p&gt;
    &lt;p&gt;Yulian Kuncheff (Daegalus) hopped in to help with the GitHub actions and the &lt;code&gt;lts-hwe&lt;/code&gt; branch. Ahmed Adan and M. Gopal (Delphic Melody) round out the new team with fantastic work on the artwork, website, documentation, and testing. And don't forget to check out the new coloring book!&lt;/p&gt;
    &lt;p&gt;Special thanks to Carl George, Laura Santamaria, Shaun McCance, and the entire bootc team for their (continuing) support of this project! The game has started. The clue is: &lt;code&gt;Gardener&lt;/code&gt;&lt;/p&gt;
    &lt;head rend="h1"&gt;The Road Ahead&lt;/head&gt;
    &lt;p&gt;And lastly, there is some missing functionality compared to the Fedora build as there are some creature comforts that are missing. We call these &lt;code&gt;parity&lt;/code&gt; bugs, so if find them, file them. There are some things that won't be coming with; CentOS Stream's focus is on long term support, so we may choose to drop a feature if it's not straightforward to bring to Bluefin LTS.&lt;/p&gt;
    &lt;p&gt;Imagine choosing between LTS, GTS, and stable with just a slider on an update page in a control panel. They should feel and act the same as each other. I'm pretty much there with my personal machines, sometimes I have to check which machine is which because it doesn't really matter. I feel the pain on the infrastructure side instead. 😄&lt;/p&gt;
    &lt;head rend="h2"&gt;Shred to Achillobator&lt;/head&gt;
    &lt;p&gt;Two new soundtracks accompany this release, enjoy!&lt;/p&gt;
    &lt;head rend="h1"&gt;Downloads&lt;/head&gt;
    &lt;p&gt;Remember you cannot rebase between CentOS and Fedora Bluefins, ain't no one testing that. Beware. Also this is the new installer -- you'll love it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Download Bluefin LTS&lt;/head&gt;
    &lt;p&gt;The long term support experience.&lt;/p&gt;
    &lt;p&gt;📖 Read the documentation to learn about features and differences. Choose HWE for Framework Computers or if you just want newer kernels.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Version&lt;/cell&gt;
        &lt;cell role="head"&gt;GPU&lt;/cell&gt;
        &lt;cell role="head"&gt;Download&lt;/cell&gt;
        &lt;cell role="head"&gt;Checksum&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Bluefin LTS&lt;/cell&gt;
        &lt;cell&gt;AMD/Intel&lt;/cell&gt;
        &lt;cell&gt;📥 bluefin-lts-x86_64.iso&lt;/cell&gt;
        &lt;cell&gt;🔐 Verify&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Bluefin LTS&lt;/cell&gt;
        &lt;cell&gt;ARM (aarch64)&lt;/cell&gt;
        &lt;cell&gt;📥 bluefin-lts-aarch64.iso&lt;/cell&gt;
        &lt;cell&gt;🔐 Verify&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Bluefin LTS HWE&lt;/cell&gt;
        &lt;cell&gt;AMD/Intel&lt;/cell&gt;
        &lt;cell&gt;📥 bluefin-lts-hwe-x86_64.iso&lt;/cell&gt;
        &lt;cell&gt;🔐 Verify&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Bluefin LTS HWE&lt;/cell&gt;
        &lt;cell&gt;ARM (aarch64)&lt;/cell&gt;
        &lt;cell&gt;📥 bluefin-lts-hwe-aarch64.iso&lt;/cell&gt;
        &lt;cell&gt;🔐 Verify&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;Download Bluefin GDX&lt;/head&gt;
    &lt;p&gt;The AI workstation with Nvidia and CUDA.&lt;/p&gt;
    &lt;p&gt;📖 Read the documentation to learn about features and differences.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Version&lt;/cell&gt;
        &lt;cell role="head"&gt;GPU&lt;/cell&gt;
        &lt;cell role="head"&gt;Download&lt;/cell&gt;
        &lt;cell role="head"&gt;Checksum&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Bluefin GDX&lt;/cell&gt;
        &lt;cell&gt;AMD/Intel + Nvidia&lt;/cell&gt;
        &lt;cell&gt;📥 bluefin-gdx-x86_64.iso&lt;/cell&gt;
        &lt;cell&gt;🔐 Verify&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Bluefin GDX&lt;/cell&gt;
        &lt;cell&gt;ARM (aarch64) + Nvidia&lt;/cell&gt;
        &lt;cell&gt;📥 bluefin-gdx-lts-aarch64.iso&lt;/cell&gt;
        &lt;cell&gt;🔐 Verify&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45296550</guid><pubDate>Fri, 19 Sep 2025 00:13:22 +0000</pubDate></item><item><title>Writing a competitive BZip2 encoder in Ada from scratch in a few days – part 3</title><link>https://gautiersblog.blogspot.com/2025/09/writing-competitive-bzip2-encoder-in.html</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=45312202</guid><pubDate>Sat, 20 Sep 2025 10:55:53 +0000</pubDate></item><item><title>FLX1s phone is launched</title><link>https://furilabs.com/flx1s-is-launched/</link><description>&lt;doc fingerprint="dfe351495ea32377"&gt;
  &lt;main&gt;
    &lt;p&gt;It is with great excitement that we can now release the FLX1s. Pre-sales are open and the phone is in production which is due to complete end of October 2025. Following that we can start shipping. Existing orders will be opted into the FLX1s or refunded.&lt;lb/&gt;To all our amazing FLX1 owners and those waiting patiently for their order, you have been the most wonderful and supportive community that we could ever have imagined.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;Thank-you from the FuriLabs Team.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45312326</guid><pubDate>Sat, 20 Sep 2025 11:20:04 +0000</pubDate></item><item><title>Ultrasonic Chef's Knife</title><link>https://seattleultrasonics.com/</link><description>&lt;doc fingerprint="c4ae7ddc24d05992"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;head rend="h2"&gt;The World's First&lt;/head&gt;&lt;lb/&gt;Ultrasonic Chef's Knife&lt;lb/&gt;For Home Cooks&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;head rend="h2"&gt;Switch on the ultrasonics and feel the blade glide effortlessly through food. Clean cuts, minimal force, less sticking.&lt;/head&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Regular price $399.00 &lt;/p&gt;
    &lt;p&gt; Regular price &lt;del class="price-item price-item--regular" data-product-id="7320581177440" rend="overstrike"&gt; $399.00 &lt;/del&gt; Sale price $399.00 &lt;/p&gt;
    &lt;p&gt;Pre-Order now for estimated shipping by January, 2026 (Batch 1). Cancel anytime before your order ships. What is a Pre-Order?&lt;/p&gt;
    &lt;p&gt;Regular price $499.00 &lt;/p&gt;
    &lt;p&gt; Regular price &lt;del class="price-item price-item--regular" data-product-id="7497201942624" rend="overstrike"&gt; $548.00 &lt;/del&gt; Sale price $499.00 &lt;/p&gt;
    &lt;p&gt;Pre-Order now for estimated shipping by January 2026 (Batch 1). Cancel anytime before your order ships. What is a Pre-Order?&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45314592</guid><pubDate>Sat, 20 Sep 2025 16:12:56 +0000</pubDate></item><item><title>Designing NotebookLM</title><link>https://jasonspielman.com/notebooklm</link><description>&lt;doc fingerprint="bf50da1b9d312eaa"&gt;
  &lt;main&gt;
    &lt;p&gt;User Journey â¢ Annotated Overview&lt;/p&gt;
    &lt;p&gt;Home&lt;/p&gt;
    &lt;p&gt;Next&lt;/p&gt;
    &lt;p&gt;Jason Spielman&lt;/p&gt;
    &lt;p&gt;Design Lead&lt;/p&gt;
    &lt;p&gt;UX + Identity&lt;/p&gt;
    &lt;p&gt;2024&lt;/p&gt;
    &lt;p&gt;I led design for NotebookLM, shaping the productâs core user experience, brand identity, and visual system from experiment to launch.&lt;/p&gt;
    &lt;p&gt;Â&lt;/p&gt;
    &lt;p&gt;This remains one of my proudest projects, and Iâm incredibly grateful for the opportunity to design something entirely new from the ground up. It was a chance to explore fresh paradigms, invent new patterns, and bring a product to life that hadnât existed before. None of it wouldâve been possible without the tight-knit, cross-functional team I was fortunate to collaborate with.&lt;/p&gt;
    &lt;p&gt;Podcast â¢ Sequoia&lt;/p&gt;
    &lt;p&gt;Raiza and I discuss the journey building NotebookLM.&lt;/p&gt;
    &lt;p&gt;NotebookLM â¢ Winner!&lt;/p&gt;
    &lt;p&gt;Recognized as one of TIMEâs Best Inventions of 2024.&lt;/p&gt;
    &lt;p&gt;|&lt;/p&gt;
    &lt;p&gt;Architecture&lt;/p&gt;
    &lt;p&gt;|&lt;/p&gt;
    &lt;p&gt;3 Panel&lt;/p&gt;
    &lt;p&gt;|&lt;/p&gt;
    &lt;p&gt;Visual Assets&lt;/p&gt;
    &lt;p&gt;|&lt;/p&gt;
    &lt;p&gt;Architecture&lt;/p&gt;
    &lt;p&gt;UI Evolution&lt;/p&gt;
    &lt;p&gt;Early Prototype&lt;/p&gt;
    &lt;p&gt;This is what the UI looked like when I first joined the early project.&lt;/p&gt;
    &lt;p&gt;Notes driven UI&lt;/p&gt;
    &lt;p&gt;Exploratory chat UI introduced as an overlay on the note canvas.&lt;/p&gt;
    &lt;p&gt;3-Panel Structure&lt;/p&gt;
    &lt;p&gt;Synthesizes learnings into a scalable and adaptive layout.&lt;/p&gt;
    &lt;p&gt;One of the core problems we set out to solve with NotebookLM was âtab overwhelmâ the scattered, fractured experience of jumping between tools while trying to synthesize ideas. We wanted to create a space where every part of the creation journey could happen in one place.&lt;/p&gt;
    &lt;p&gt;â Inputs&lt;/p&gt;
    &lt;p&gt;Outputs â&lt;/p&gt;
    &lt;p&gt;Chat&lt;/p&gt;
    &lt;p&gt;â¢ Citations&lt;/p&gt;
    &lt;p&gt;Creation&lt;/p&gt;
    &lt;p&gt;â¢ Multiple entry points&lt;/p&gt;
    &lt;p&gt;This visual shows how the core building blocks came together.&lt;/p&gt;
    &lt;p&gt;The structure you see now may seem obvious but it took After what felt like 1000 iterations to get there. Trying to put these blocks together in a way that allowed for a clear mental model and digstible UI. These early sketches are from a plane. I ran out of paper and ultimately found the final solution when drawing on a napkin.&lt;/p&gt;
    &lt;p&gt;The mental model of NotebookLM was built around the creation journey: starting with inputs, moving through conversation, and ending with outputs. Users bring in their sources (documents, notes, references), then interact with them through chat by asking questions, clarifying, and synthesizing before transforming those insights into structured outputs like notes, study guides, and Audio Overviews.&lt;/p&gt;
    &lt;p&gt;Â&lt;/p&gt;
    &lt;p&gt;By grounding the design in this linear but flexible flow (Inputs â Chat â Outputs) we gave users a clear sense of place within the product while keeping the complexity of new AI interactions digestible and intuitive.&lt;/p&gt;
    &lt;p&gt;Itâs rare to find a product that brings reading, writing, and creation together in a truly integrated way largely because juggling all three can be overwhelming. But with AI reducing friction, the opportunity emerged to design a space where every part of the creative process could coexist.&lt;/p&gt;
    &lt;p&gt;Â&lt;/p&gt;
    &lt;p&gt;To make that possible, I designed a responsive panel system that adapts to the userâs needs, scaling fluidly while preserving quick access to key elements like sources and notes, even at the smallest sizes.&lt;/p&gt;
    &lt;p&gt;The default layout, offering a balanced view of sources, chat, and notes.&lt;/p&gt;
    &lt;p&gt;Optimized for referencing sources and generating responses with citations.&lt;/p&gt;
    &lt;p&gt;A popular request, designed for users focused on drafting and iteration.&lt;/p&gt;
    &lt;p&gt;Ideal for composing while keeping source material in view.&lt;/p&gt;
    &lt;p&gt;Â&lt;/p&gt;
    &lt;p&gt;To optimize spatial utility, I created a set of responsive panels that scale based on user needs, retaining essential icons for sources and notes even at minimal widths.&lt;/p&gt;
    &lt;p&gt;Â&lt;/p&gt;
    &lt;p&gt;Scalability was a core principle. While the content within these panels can shift and evolve, the underlying system is built to support growth, accommodating new tools, modes, and workflows without breaking the structure.&lt;/p&gt;
    &lt;p&gt;Source Panel&lt;/p&gt;
    &lt;p&gt;Studio Panel&lt;/p&gt;
    &lt;p&gt;See how the team has continued to scale this system with the newest launch of flashcards, quizzes, professional reports.&lt;/p&gt;
    &lt;p&gt;Chat Panel&lt;/p&gt;
    &lt;p&gt;Hereâs what that looks like:&lt;/p&gt;
    &lt;p&gt;User Journey â¢ Annotated Overview&lt;/p&gt;
    &lt;p&gt;Read the full story&lt;/p&gt;
    &lt;p&gt;Defining the brand identity was a fast-paced effort, made possible by close collaboration with Google Labs and the central brand team. Shoutout Feel Hwang, Nick Mcginnis, Jennifer Leartanasan and team.&lt;/p&gt;
    &lt;p&gt;Jason Spielman&lt;/p&gt;
    &lt;p&gt;Linkedin â&lt;/p&gt;
    &lt;p&gt;Jason Spielman&lt;/p&gt;
    &lt;p&gt;Linkedin â&lt;/p&gt;
    &lt;p&gt;Home&lt;/p&gt;
    &lt;p&gt;Next&lt;/p&gt;
    &lt;p&gt;Jason Spielman&lt;/p&gt;
    &lt;p&gt;Design Lead&lt;/p&gt;
    &lt;p&gt;UX + Identity&lt;/p&gt;
    &lt;p&gt;2024&lt;/p&gt;
    &lt;p&gt;I led design for NotebookLM, shaping the productâs core user experience, brand identity, and visual system from experiment to launch.&lt;/p&gt;
    &lt;p&gt;Â&lt;/p&gt;
    &lt;p&gt;This remains one of my proudest projects, and Iâm incredibly grateful for the opportunity to design something entirely new from the ground up. It was a chance to explore fresh paradigms, invent new patterns, and bring a product to life that hadnât existed before. None of it wouldâve been possible without the tight-knit, cross-functional team I was fortunate to collaborate with.&lt;/p&gt;
    &lt;p&gt;|&lt;/p&gt;
    &lt;p&gt;Architecture&lt;/p&gt;
    &lt;p&gt;|&lt;/p&gt;
    &lt;p&gt;3 Panel&lt;/p&gt;
    &lt;p&gt;|&lt;/p&gt;
    &lt;p&gt;Brand Identity&lt;/p&gt;
    &lt;p&gt;|&lt;/p&gt;
    &lt;p&gt;Visual Assets&lt;/p&gt;
    &lt;p&gt;|&lt;/p&gt;
    &lt;p&gt;Takeaways&lt;/p&gt;
    &lt;p&gt;Architecture&lt;/p&gt;
    &lt;p&gt;UI Evolution&lt;/p&gt;
    &lt;p&gt;Early Prototype&lt;/p&gt;
    &lt;p&gt;This is what the UI looked like when I first joined the early project.&lt;/p&gt;
    &lt;p&gt;Notes driven UI&lt;/p&gt;
    &lt;p&gt;Exploratory chat UI introduced as an overlay on the note canvas.&lt;/p&gt;
    &lt;p&gt;3-Panel Structure&lt;/p&gt;
    &lt;p&gt;Synthesizes learnings into a scalable and adaptive layout.&lt;/p&gt;
    &lt;p&gt;One of the core problems we set out to solve with NotebookLM was âtab overwhelmâ the scattered, fractured experience of jumping between tools while trying to synthesize ideas. We wanted to create a space where every part of the creation journey could happen in one place.&lt;/p&gt;
    &lt;p&gt;â Inputs&lt;/p&gt;
    &lt;p&gt;Outputs â&lt;/p&gt;
    &lt;p&gt;Chat&lt;/p&gt;
    &lt;p&gt;â¢ Citations&lt;/p&gt;
    &lt;p&gt;Creation&lt;/p&gt;
    &lt;p&gt;â¢ Multiple entry points&lt;/p&gt;
    &lt;p&gt;This visual shows how the core building blocks came together.&lt;/p&gt;
    &lt;p&gt;The structure you see now may seem obvious but it took After what felt like 1000 iterations to get there. Trying to put these blocks together in a way that allowed for a clear mental model and digstible UI. These early sketches are from a plane. I ran out of paper and ultimately found the final solution when drawing on a napkin.&lt;/p&gt;
    &lt;p&gt;The mental model of NotebookLM was built around the creation journey: starting with inputs, moving through conversation, and ending with outputs. Users bring in their sources (documents, notes, references), then interact with them through chat by asking questions, clarifying, and synthesizing before transforming those insights into structured outputs like notes, study guides, and Audio Overviews.&lt;/p&gt;
    &lt;p&gt;Â&lt;/p&gt;
    &lt;p&gt;By grounding the design in this linear but flexible flow (Inputs â Chat â Outputs) we gave users a clear sense of place within the product while keeping the complexity of new AI interactions digestible and intuitive.&lt;/p&gt;
    &lt;p&gt;Itâs rare to find a product that brings reading, writing, and creation together in a truly integrated way largely because juggling all three can be overwhelming. But with AI reducing friction, the opportunity emerged to design a space where every part of the creative process could coexist.&lt;/p&gt;
    &lt;p&gt;Â&lt;/p&gt;
    &lt;p&gt;To make that possible, I designed a responsive panel system that adapts to the userâs needs, scaling fluidly while preserving quick access to key elements like sources and notes, even at the smallest sizes.&lt;/p&gt;
    &lt;p&gt;Standard&lt;/p&gt;
    &lt;p&gt;The default layout, offering a balanced view of sources, chat, and notes.&lt;/p&gt;
    &lt;p&gt;Reading + Chat&lt;/p&gt;
    &lt;p&gt;Optimized for referencing sources and generating responses with citations.&lt;/p&gt;
    &lt;p&gt;To optimize spatial utility, I created a set of responsive panels that scale based on user needs, retaining essential icons for sources and notes even at minimal widths.&lt;/p&gt;
    &lt;p&gt;Â&lt;/p&gt;
    &lt;p&gt;Scalability was a core principle. While the content within these panels can shift and evolve, the underlying system is built to support growth, accommodating new tools, modes, and workflows without breaking the structure.&lt;/p&gt;
    &lt;p&gt;Source Panel&lt;/p&gt;
    &lt;p&gt;Studio Panel&lt;/p&gt;
    &lt;p&gt;See how the team has continued to scale this system with the newest launch of flashcards, quizzes, professional reports.&lt;/p&gt;
    &lt;p&gt;Chat Panel&lt;/p&gt;
    &lt;p&gt;Hereâs what that looks like:&lt;/p&gt;
    &lt;p&gt;User Journey â¢ Annotated Overview&lt;/p&gt;
    &lt;p&gt;Read the full story&lt;/p&gt;
    &lt;p&gt;Defining the brand identity was a fast-paced effort, made possible by close collaboration with Google Labs and the central brand team. Shoutout Feel Hwang, Nick Mcginnis, Jennifer Leartanasan and team.&lt;/p&gt;
    &lt;p&gt;Jason Spielman&lt;/p&gt;
    &lt;p&gt;Linkedin â&lt;/p&gt;
    &lt;p&gt;Wanda Wingleton&lt;/p&gt;
    &lt;p&gt;Linkedin â&lt;/p&gt;
    &lt;p&gt;Home&lt;/p&gt;
    &lt;p&gt;Next&lt;/p&gt;
    &lt;p&gt;Jason Spielman&lt;/p&gt;
    &lt;p&gt;Design Lead&lt;/p&gt;
    &lt;p&gt;UX + Identity&lt;/p&gt;
    &lt;p&gt;2024&lt;/p&gt;
    &lt;p&gt;I led design for NotebookLM, shaping the productâs core user experience, brand identity, and visual system from experiment to launch.&lt;/p&gt;
    &lt;p&gt;Â&lt;/p&gt;
    &lt;p&gt;This remains one of my proudest projects, and Iâm incredibly grateful for the opportunity to design something entirely new from the ground up. It was a chance to explore fresh paradigms, invent new patterns, and bring a product to life that hadnât existed before. None of it wouldâve been possible without the tight-knit, cross-functional team I was fortunate to collaborate with.&lt;/p&gt;
    &lt;p&gt;Podcast â¢ Seqouia Training Data&lt;/p&gt;
    &lt;p&gt;Raiza and I discuss the journey building NotebookLM.&lt;/p&gt;
    &lt;p&gt;NotebookLM â¢ Winner!&lt;/p&gt;
    &lt;p&gt;Recognized as one of TIMEâs Best Inventions of 2024.&lt;/p&gt;
    &lt;p&gt;|&lt;/p&gt;
    &lt;p&gt;Architecture&lt;/p&gt;
    &lt;p&gt;|&lt;/p&gt;
    &lt;p&gt;3 Panel&lt;/p&gt;
    &lt;p&gt;|&lt;/p&gt;
    &lt;p&gt;Brand Identity&lt;/p&gt;
    &lt;p&gt;|&lt;/p&gt;
    &lt;p&gt;Visual Assets&lt;/p&gt;
    &lt;p&gt;|&lt;/p&gt;
    &lt;p&gt;Takeaways&lt;/p&gt;
    &lt;p&gt;Architecture&lt;/p&gt;
    &lt;p&gt;Design Evolution&lt;/p&gt;
    &lt;p&gt;Early Prototype&lt;/p&gt;
    &lt;p&gt;This is what the UI looked like when I first joined the early project.&lt;/p&gt;
    &lt;p&gt;Notes driven UI&lt;/p&gt;
    &lt;p&gt;Exploratory chat UI introduced as an overlay on the note canvas.&lt;/p&gt;
    &lt;p&gt;3-Panel Structure&lt;/p&gt;
    &lt;p&gt;Synthesizes learnings into a scalable and adaptive layout.&lt;/p&gt;
    &lt;p&gt;One of the core problems we set out to solve with NotebookLM was âtab overwhelmâ the scattered, fractured experience of jumping between tools while trying to synthesize ideas. We wanted to create a space where every part of the creation journey could happen in one place.&lt;/p&gt;
    &lt;p&gt;â Inputs&lt;/p&gt;
    &lt;p&gt;Outputs â&lt;/p&gt;
    &lt;p&gt;Chat&lt;/p&gt;
    &lt;p&gt;â¢ Citations&lt;/p&gt;
    &lt;p&gt;Creation&lt;/p&gt;
    &lt;p&gt;â¢ Multiple entry points&lt;/p&gt;
    &lt;p&gt;This visual shows how the core building blocks came together.&lt;/p&gt;
    &lt;p&gt;The structure you see now may look obvious but it took what felt like a thousand iterations to get there. I was trying to arrange these blocks in a way that supported a clear mental model and a UI that felt intuitive and digestible.&lt;/p&gt;
    &lt;p&gt;Â&lt;/p&gt;
    &lt;p&gt;These sketches were done on a plane. I ran out of paper and ended up sketching the final solution across a few napkins.&lt;/p&gt;
    &lt;p&gt;The mental model of NotebookLM was built around the creation journey: starting with inputs, moving through conversation, and ending with outputs. Users bring in their sources (documents, notes, references), then interact with them through chat by asking questions, clarifying, and synthesizing before transforming those insights into structured outputs like notes, study guides, and Audio Overviews.&lt;/p&gt;
    &lt;p&gt;Â&lt;/p&gt;
    &lt;p&gt;By grounding the design in this linear but flexible flow (Inputs â Chat â Outputs) we gave users a clear sense of place within the product while keeping the complexity of new AI interactions digestible and intuitive.&lt;/p&gt;
    &lt;p&gt;Itâs rare to find a product that brings reading, writing, and creation together in a truly integrated way largely because juggling all three can be overwhelming. But with AI reducing friction, the opportunity emerged to design a space where every part of the creative process could coexist.&lt;/p&gt;
    &lt;p&gt;Â&lt;/p&gt;
    &lt;p&gt;To make that possible, I designed a responsive panel system that adapts to the userâs needs, scaling fluidly while preserving quick access to key elements like sources and notes, even at the smallest sizes.&lt;/p&gt;
    &lt;p&gt;Standard&lt;/p&gt;
    &lt;p&gt;The default layout, offering a balanced view of sources, chat, and notes.&lt;/p&gt;
    &lt;p&gt;Reading + Chat&lt;/p&gt;
    &lt;p&gt;Optimized for referencing sources and generating responses with citations.&lt;/p&gt;
    &lt;p&gt;Chat + Writing&lt;/p&gt;
    &lt;p&gt;A popular request, designed for users focused on drafting and iteration.&lt;/p&gt;
    &lt;p&gt;Reading + Writing&lt;/p&gt;
    &lt;p&gt;Ideal for composing while keeping source material in view.&lt;/p&gt;
    &lt;p&gt;Â&lt;/p&gt;
    &lt;p&gt;To optimize spatial utility, I created a set of responsive panels that scale based on user needs, retaining essential icons for sources and notes even at minimal widths.&lt;/p&gt;
    &lt;p&gt;Â&lt;/p&gt;
    &lt;p&gt;Scalability was a core principle. While the content within these panels can shift and evolve, the underlying system is built to support growth, accommodating new tools, modes, and workflows without breaking the structure.&lt;/p&gt;
    &lt;p&gt;Source Panel&lt;/p&gt;
    &lt;p&gt;Studio Panel&lt;/p&gt;
    &lt;p&gt;See how the team has continued to scale this system with the newest launch of flashcards, quizzes, professional reports.&lt;/p&gt;
    &lt;p&gt;Chat Panel&lt;/p&gt;
    &lt;p&gt;Hereâs what that looks like:&lt;/p&gt;
    &lt;p&gt;Read the full story&lt;/p&gt;
    &lt;p&gt;Defining the brand identity was a fast-paced effort, made possible by close collaboration with Google Labs and the central brand team. Shoutout Feel Hwang, Nick Mcginnis, Jennifer Leartanasan and team.&lt;/p&gt;
    &lt;p&gt;Jason Spielman&lt;/p&gt;
    &lt;p&gt;Linkedin â&lt;/p&gt;
    &lt;p&gt;Jason Spielman&lt;/p&gt;
    &lt;p&gt;Linkedin â&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45315312</guid><pubDate>Sat, 20 Sep 2025 17:25:58 +0000</pubDate></item><item><title>$2 WeAct Display FS adds a 0.96-inch USB information display to your computer</title><link>https://www.cnx-software.com/2025/09/18/2-weact-display-fs-adds-a-0-96-inch-usb-information-display-to-your-computer/</link><description>&lt;doc fingerprint="aa305d1c39dee0f3"&gt;
  &lt;main&gt;
    &lt;p&gt;WeAct Display FS is an inexpensive 0.96-inch USB display dongle designed to add an information display or a tiny secondary display to your computer or SBC.&lt;/p&gt;
    &lt;p&gt;We’ve seen this type of information display with products such as the Turing Smart Screen, a larger 3.5-inch color display, or small OLEDs integrated into cases such as the Pironman 5 Max to disable text. The WeAct Display FS V1 may be tiny, but it’s also a full-color 160×80 resolution display that can be customized with software provided by WeAct.&lt;/p&gt;
    &lt;p&gt;WeAct Display FS V1 specifications:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Display – 0.96-inch RGB565 display with 160×80 resolution&lt;/item&gt;
      &lt;item&gt;Host interface – “Reversible” USB 2.0 Type-A Full Speed (FS) port showing as a CDC device&lt;/item&gt;
      &lt;item&gt;Dimensions – 43 x 14.5 mm&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Since you wouldn’t want to get a display only for it to face the wrong direction, for instance, the desk or the wall, the company made the USB-A port reversible, and the user only needs to install one of the two provided pads on the unused side of the port to avoid short circuits.&lt;/p&gt;
    &lt;p&gt;WeAct provides two programs for it. The first one is the WeAct Studio System Monitor based on a fork of Matthieu Houdebine’s Turing Smart Screen Python project. This allows users to create UIs/themes with text, images, weather, and other features… WeAct says the little device only works on Windows, but the open-source project is supposed to also work on macOS, Linux (including Raspberry Pi OS), and essentially any operating system with support for Python 3.9+.&lt;/p&gt;
    &lt;p&gt;The second program is called WeAct Studio Screen Projection, and as I understand it, it emulates an actual display, so you could move any window/program to the USB display. I’m just not sure how a desktop OS like Windows will handle a tiny 160×80 “monitor”… I suppose it could be used to play a full-screen YouTube video or display photos for whatever reason. That one only works on Windows, and there’s no source code.&lt;/p&gt;
    &lt;p&gt;You’ll find the WeAct Display FS V1 (0.96-inch) on AliExpress for about $2 plus shipping, but while looking for information, I also noticed a 3.5-inch variant with 480×320 resolution for about $11.&lt;/p&gt;
    &lt;p&gt;Jean-Luc started CNX Software in 2010 as a part-time endeavor, before quitting his job as a software engineering manager, and starting to write daily news, and reviews full time later in 2011.&lt;/p&gt;
    &lt;p&gt;Support CNX Software! Donate via cryptocurrencies, become a Patron on Patreon, or purchase goods on Amazon or Aliexpress. We also use affiliate links in articles to earn commissions if you make a purchase after clicking on those links.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45317527</guid><pubDate>Sat, 20 Sep 2025 21:04:47 +0000</pubDate></item><item><title>In defence of swap: common misconceptions (2018)</title><link>https://chrisdown.name/2018/01/02/in-defence-of-swap.html</link><description>&lt;doc fingerprint="be79d81b38a2af88"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;In defence of swap: common misconceptions&lt;/head&gt;
    &lt;p&gt;This post is also available in Japanese, Chinese, and Russian.&lt;/p&gt;
    &lt;p&gt;tl;dr:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Having swap is a reasonably important part of a well functioning system. Without it, sane memory management becomes harder to achieve.&lt;/item&gt;
      &lt;item&gt;Swap is not generally about getting emergency memory, it's about making memory reclamation egalitarian and efficient. In fact, using it as "emergency memory" is generally actively harmful.&lt;/item&gt;
      &lt;item&gt;Disabling swap does not prevent disk I/O from becoming a problem under memory contention. Instead, it simply shifts the disk I/O thrashing from anonymous pages to file pages. Not only may this be less efficient, as we have a smaller pool of pages to select from for reclaim, but it may also contribute to getting into this high contention state in the first place.&lt;/item&gt;
      &lt;item&gt;The swapper on kernels before 4.0 has a lot of pitfalls, and has contributed to a lot of people's negative perceptions of swap due to its overeagerness to swap out pages. On kernels &amp;gt;4.0, the situation is significantly better.&lt;/item&gt;
      &lt;item&gt;On SSDs, swapping out anonymous pages and reclaiming file pages are essentially equivalent in terms of performance and latency. On older spinning disks, swap reads are slower due to random reads, so a lower &lt;code&gt;vm.swappiness&lt;/code&gt;setting makes sense there (read on for more about&lt;code&gt;vm.swappiness&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;Disabling swap doesn't prevent pathological behaviour at near-OOM, although it's true that having swap may prolong it. Whether the global OOM killer is invoked with or without swap, or was invoked sooner or later, the result is the same: you are left with a system in an unpredictable state. Having no swap doesn't avoid this.&lt;/item&gt;
      &lt;item&gt;You can achieve better swap behaviour under memory pressure and prevent thrashing by utilising &lt;code&gt;memory.low&lt;/code&gt;and friends in cgroup v2.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As part of my work improving kernel memory management and cgroup v2, I've been talking to a lot of engineers about attitudes towards memory, especially around application behaviour under pressure and operating system heuristics used under the hood for memory management.&lt;/p&gt;
    &lt;p&gt;A repeated topic in these discussions has been swap. Swap is a hotly contested and poorly understood topic, even by those who have been working with Linux for many years. Many see it as useless or actively harmful: a relic of a time where memory was scarce, and disks were a necessary evil to provide much-needed space for paging. This is a statement that I still see being batted around with relative frequency in recent years, and I've had many discussions with colleagues, friends, and industry peers to help them understand why swap is still a useful concept on modern computers with significantly more physical memory available than in the past.&lt;/p&gt;
    &lt;p&gt;There's also a lot of misunderstanding about the purpose of swap – many people just see it as a kind of "slow extra memory" for use in emergencies, but don't understand how it can contribute during normal load to the healthy operation of an operating system as a whole.&lt;/p&gt;
    &lt;p&gt;Many of us have heard most of the usual tropes about memory: "Linux uses too much memory", "swap should be double your physical memory size", and the like. While these are either trivial to dispel, or discussion around them has become more nuanced in recent years, the myth of "useless" swap is much more grounded in heuristics and arcana rather than something that can be explained by simple analogy, and requires somewhat more understanding of memory management to reason about.&lt;/p&gt;
    &lt;p&gt;This post is mostly aimed at those who administer Linux systems and are interested in hearing the counterpoints to running with undersized/no swap or running with &lt;code&gt;vm.swappiness&lt;/code&gt; set to 0.&lt;/p&gt;
    &lt;head rend="h2"&gt;Background&lt;/head&gt;
    &lt;p&gt;It's hard to talk about why having swap and swapping out pages are good things in normal operation without a shared understanding of some of the basic underlying mechanisms at play in Linux memory management, so let's make sure we're on the same page.&lt;/p&gt;
    &lt;head rend="h3"&gt;Types of memory&lt;/head&gt;
    &lt;p&gt;There are many different types of memory in Linux, and each type has its own properties. Understanding the nuances of these is key to understanding why swap is important.&lt;/p&gt;
    &lt;p&gt;For example, there are pages ("blocks" of memory, typically 4k) responsible for holding the code for each process being run on your computer. There are also pages responsible for caching data and metadata related to files accessed by those programs in order to speed up future access. These are part of the page cache, and I will refer to them as file memory.&lt;/p&gt;
    &lt;p&gt;There are also pages which are responsible for the memory allocations made inside that code, for example, when new memory that has been allocated with &lt;code&gt;malloc&lt;/code&gt; is written to, or when using &lt;code&gt;mmap&lt;/code&gt;'s &lt;code&gt;MAP_ANONYMOUS&lt;/code&gt; flag. These are "anonymous" pages – so called because they are not backed by anything – and I will refer to them as anon memory.&lt;/p&gt;
    &lt;p&gt;There are other types of memory too – shared memory, slab memory, kernel stack memory, buffers, and the like – but anonymous memory and file memory are the most well known and easy to understand ones, so I will use these in my examples, although they apply equally to these types too.&lt;/p&gt;
    &lt;head rend="h3"&gt;Reclaimable/unreclaimable memory&lt;/head&gt;
    &lt;p&gt;One of the most fundamental questions when thinking about a particular type of memory is whether it is able to be reclaimed or not. "Reclaim" here means that the system can, without losing data, purge pages of that type from physical memory.&lt;/p&gt;
    &lt;p&gt;For some page types, this is typically fairly trivial. For example, in the case of clean (unmodified) page cache memory, we're simply caching something that we have on disk for performance, so we can drop the page without having to do any special operations.&lt;/p&gt;
    &lt;p&gt;For some page types, this is possible, but not trivial. For example, in the case of dirty (modified) page cache memory, we can't just drop the page, because the disk doesn't have our modifications yet. As such we either need to deny reclamation or first get our changes back to disk before we can drop this memory.&lt;/p&gt;
    &lt;p&gt;For some page types, this is not possible. For example, in the case of the anonymous pages mentioned previously, they only exist in memory and in no other backing store, so they have to be kept there.&lt;/p&gt;
    &lt;head rend="h2"&gt;On the nature of swap&lt;/head&gt;
    &lt;p&gt;If you look for descriptions of the purpose of swap on Linux, you'll inevitably find many people talking about it as if it is merely an extension of the physical RAM for use in emergencies. For example, here is a random post I got as one of the top results from typing "what is swap" in Google:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Swap is essentially emergency memory; a space set aside for times when your system temporarily needs more physical memory than you have available in RAM. It's considered "bad" in the sense that it's slow and inefficient, and if your system constantly needs to use swap then it obviously doesn't have enough memory. […] If you have enough RAM to handle all of your needs, and don't expect to ever max it out, then you should be perfectly safe running without a swap space.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;To be clear, I don't blame the poster of this comment at all for the content of their post – this is accepted as "common knowledge" by a lot of Linux sysadmins and is probably one of the most likely things that you will hear from one if you ask them to talk about swap. It is unfortunately also, however, a misunderstanding of the purpose and use of swap, especially on modern systems.&lt;/p&gt;
    &lt;p&gt;Above, I talked about reclamation for anonymous pages being "not possible", as anonymous pages by their nature have no backing store to fall back to when being purged from memory – as such, their reclamation would result in complete data loss for those pages. What if we could create such a store for these pages, though?&lt;/p&gt;
    &lt;p&gt;Well, this is precisely what swap is for. Swap is a storage area for these seemingly "unreclaimable" pages that allows us to page them out to a storage device on demand. This means that they can now be considered as equally eligible for reclaim as their more trivially reclaimable friends, like clean file pages, allowing more efficient use of available physical memory.&lt;/p&gt;
    &lt;p&gt;Swap is primarily a mechanism for equality of reclamation, not for emergency "extra memory". Swap is not what makes your application slow – entering overall memory contention is what makes your application slow.&lt;/p&gt;
    &lt;p&gt;So in what situations under this "equality of reclamation" scenario would we legitimately choose to reclaim anonymous pages? Here are, abstractly, some not uncommon scenarios:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;During initialisation, a long-running program may allocate and use many pages. These pages may also be used as part of shutdown/cleanup, but are not needed once the program is "started" (in an application-specific sense). This is fairly common for daemons which have significant dependencies to initialise.&lt;/item&gt;
      &lt;item&gt;During the program's normal operation, we may allocate memory which is only used rarely. It may make more sense for overall system performance to require a major fault to page these in from disk on demand, instead using the memory for something else that's more important.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Examining what happens with/without swap&lt;/head&gt;
    &lt;p&gt;Let's look at typical situations, and how they perform with and without swap present. I talk about metrics around "memory contention" in my talk on cgroup v2.&lt;/p&gt;
    &lt;head rend="h3"&gt;Under no/low memory contention&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;With swap: We can choose to swap out rarely-used anonymous memory that may only be used during a small part of the process lifecycle, allowing us to use this memory to improve cache hit rate, or do other optimisations.&lt;/item&gt;
      &lt;item&gt;Without swap: We cannot swap out rarely-used anonymous memory, as it's locked in memory. While this may not immediately present as a problem, on some workloads this may represent a non-trivial drop in performance due to stale, anonymous pages taking space away from more important use.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Under moderate/high memory contention&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;With swap: All memory types have an equal possibility of being reclaimed. This means we have a better chance to be able to reclaim pages successfully – that is, we can reclaim pages that are not quickly faulted back in again (thrashing).&lt;/item&gt;
      &lt;item&gt;Without swap: Anonymous pages are locked into memory as they have nowhere to go. The chance of successful long-term page reclamation is lower, as we have only some types of memory eligible to be reclaimed at all. The risk of page thrashing is higher. The casual reader might think that this would still be better as it might avoid having to do disk I/O, but this isn't true – we simply transfer the disk I/O of swapping to dropping hot page caches and dropping code segments we need soon.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Under temporary spikes in memory usage&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;With swap: We're more resilient to temporary spikes, but in cases of severe memory starvation, the period from memory thrashing beginning to the OOM killer may be prolonged. We have more visibility into the instigators of memory pressure and can act on them more reasonably, and can perform a controlled intervention.&lt;/item&gt;
      &lt;item&gt;Without swap: The OOM killer is triggered more quickly as anonymous pages are locked into memory and cannot be reclaimed. We're more likely to thrash on memory, but the time between thrashing and OOMing is reduced. Depending on your application, this may be better or worse. For example, a queue-based application may desire this quick transfer from thrashing to killing. That said, this is still too late to be really useful – the OOM killer is only invoked at moments of severe starvation, and relying on this method for such behaviour would be better replaced with more opportunistic killing of processes as memory contention is reached in the first place.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Okay, so I want system swap, but how can I tune it for individual applications?&lt;/head&gt;
    &lt;p&gt;You didn't think you'd get through this entire post without me plugging cgroup v2, did you? ;-)&lt;/p&gt;
    &lt;p&gt;Obviously, it's hard for a generic heuristic algorithm to be right all the time, so it's important for you to be able to give guidance to the kernel. Historically the only tuning you could do was at the system level, using &lt;code&gt;vm.swappiness&lt;/code&gt;. This has two problems: &lt;code&gt;vm.swappiness&lt;/code&gt; is incredibly hard to reason about because it only feeds in as a small part of a larger heuristic system, and it also is system-wide instead of being granular to a smaller set of processes.&lt;/p&gt;
    &lt;p&gt;You can also use &lt;code&gt;mlock&lt;/code&gt; to lock pages into memory, but this requires either modifying program code, fun with &lt;code&gt;LD_PRELOAD&lt;/code&gt;, or doing horrible things with a debugger at runtime. In VM-based languages this also doesn't work very well, since you generally have no control over allocation and end up having to &lt;code&gt;mlockall&lt;/code&gt;, which has no precision towards the pages you actually care about.&lt;/p&gt;
    &lt;p&gt;cgroup v2 has a tunable per-cgroup in the form of &lt;code&gt;memory.low&lt;/code&gt;, which allows us to tell the kernel to prefer other applications for reclaim below a certain threshold of memory used. This allows us to not prevent the kernel from swapping out parts of our application, but prefer to reclaim from other applications under memory contention. Under normal conditions, the kernel's swap logic is generally pretty good, and allowing it to swap out pages opportunistically generally increases system performance. Swap thrash under heavy memory contention is not ideal, but it's more a property of simply running out of memory entirely than a problem with the swapper. In these situations, you typically want to fail fast by self-killing non-critical processes when memory pressure starts to build up.&lt;/p&gt;
    &lt;p&gt;You can not simply rely on the OOM killer for this. The OOM killer is only invoked in situations of dire failure when we've already entered a state where the system is severely unhealthy and may well have been so for a while. You need to opportunistically handle the situation yourself before ever thinking about the OOM killer.&lt;/p&gt;
    &lt;p&gt;Determination of memory pressure is somewhat difficult using traditional Linux memory counters, though. We have some things which seem somewhat related, but are merely tangential – memory usage, page scans, etc – and from these metrics alone it's very hard to tell an efficient memory configuration from one that's trending towards memory contention. There is a group of us at Facebook, spearheaded by Johannes, working on developing new metrics that expose memory pressure more easily that should help with this in future. If you're interested in hearing more about this, I go into detail about one metric being considered in my talk on cgroup v2.&lt;/p&gt;
    &lt;head rend="h2"&gt;Tuning&lt;/head&gt;
    &lt;head rend="h3"&gt;How much swap do I need, then?&lt;/head&gt;
    &lt;p&gt;In general, the minimum amount of swap space required for optimal memory management depends on the number of anonymous pages pinned into memory that are rarely reaccessed by an application, and the value of reclaiming those anonymous pages. The latter is mostly a question of which pages are no longer purged to make way for these infrequently accessed anonymous pages.&lt;/p&gt;
    &lt;p&gt;If you have a bunch of disk space and a recent (4.0+) kernel, more swap is almost always better than less. In older kernels &lt;code&gt;kswapd&lt;/code&gt;, one of the kernel processes responsible for managing swap, was historically very overeager to swap out memory aggressively the more swap you had. In recent times, swapping behaviour when a large amount of swap space is available has been significantly improved. If you're running kernel 4.0+, having a larger swap on a modern kernel should not result in overzealous swapping. As such, if you have the space, having a swap size of a few GB keeps your options open on modern kernels.&lt;/p&gt;
    &lt;p&gt;If you're more constrained with disk space, then the answer really depends on the tradeoffs you have to make, and the nature of the environment. Ideally you should have enough swap to make your system operate optimally at normal and peak (memory) load. What I'd recommend is setting up a few testing systems with 2-3GB of swap or more, and monitoring what happens over the course of a week or so under varying (memory) load conditions. As long as you haven't encountered severe memory starvation during that week – in which case the test will not have been very useful – you will probably end up with some number of MB of swap occupied. As such, it's probably worth having at least that much swap available, in addition to a little buffer for changing workloads. &lt;code&gt;atop&lt;/code&gt; in logging mode can also show you which applications are having their pages swapped out in the &lt;code&gt;SWAPSZ&lt;/code&gt; column, so if you don't already use it on your servers to log historic server state you probably want to set it up on these test machines with logging mode as part of this experiment. This also tells you when your application started swapping out pages, which you can tie to log events or other key data.&lt;/p&gt;
    &lt;p&gt;Another thing worth considering is the nature of the swap medium. Swap reads tend to be highly random, since we can't reliably predict which pages will be refaulted and when. On an SSD this doesn't matter much, but on spinning disks, random I/O is extremely expensive since it requires physical movement to achieve. On the other hand, refaulting of file pages is likely less random, since files related to the operation of a single application at runtime tend to be less fragmented. This might mean that on a spinning disk you may want to bias more towards reclaiming file pages instead of swapping out anonymous pages, but again, you need to test and evaluate how this balances out for your workload.&lt;/p&gt;
    &lt;p&gt;For laptop/desktop users who want to hibernate to swap, this also needs to be taken into account – in this case your swap file should be at least your physical RAM size.&lt;/p&gt;
    &lt;head rend="h3"&gt;What should my swappiness setting be?&lt;/head&gt;
    &lt;p&gt;First, it's important to understand what &lt;code&gt;vm.swappiness&lt;/code&gt; does. &lt;code&gt;vm.swappiness&lt;/code&gt; is a sysctl that biases memory reclaim either towards reclamation of anonymous pages, or towards file pages. It does this using two different attributes: &lt;code&gt;file_prio&lt;/code&gt; (our willingness to reclaim file pages) and &lt;code&gt;anon_prio&lt;/code&gt; (our willingness to reclaim anonymous pages). &lt;code&gt;vm.swappiness&lt;/code&gt; plays into this, as it becomes the default value for &lt;code&gt;anon_prio&lt;/code&gt;, and it also is subtracted from the default value of 200 for &lt;code&gt;file_prio&lt;/code&gt;, which means for a value of &lt;code&gt;vm.swappiness = 50&lt;/code&gt;, the outcome is that &lt;code&gt;anon_prio&lt;/code&gt; is 50, and &lt;code&gt;file_prio&lt;/code&gt; is 150 (the exact numbers don't matter as much as their relative weight compared to the other).&lt;/p&gt;
    &lt;p&gt;This means that, in general, vm.swappiness is simply a ratio of how costly reclaiming and refaulting anonymous memory is compared to file memory for your hardware and workload. The lower the value, the more you tell the kernel that infrequently accessed anonymous pages are expensive to swap out and in on your hardware. The higher the value, the more you tell the kernel that the cost of swapping anonymous pages and file pages is similar on your hardware. The memory management subsystem will still try to mostly decide whether it swaps file or anonymous pages based on how hot the memory is, but swappiness tips the cost calculation either more towards swapping or more towards dropping filesystem caches when it could go either way. On SSDs these are basically as expensive as each other, so setting &lt;code&gt;vm.swappiness = 100&lt;/code&gt; (full equality) may work well. On spinning disks, swapping may be significantly more expensive since swapping in general requires random reads, so you may want to bias more towards a lower value.&lt;/p&gt;
    &lt;p&gt;The reality is that most people don't really have a feeling about which their hardware demands, so it's non-trivial to tune this value based on instinct alone – this is something that you need to test using different values. You can also spend time evaluating the memory composition of your system and core applications and their behaviour under mild memory reclamation.&lt;/p&gt;
    &lt;p&gt;When talking about &lt;code&gt;vm.swappiness&lt;/code&gt;, an extremely important change to consider from recent(ish) times is this change to vmscan by Satoru Moriya in 2012, which changes the way that &lt;code&gt;vm.swappiness = 0&lt;/code&gt; is handled quite significantly.&lt;/p&gt;
    &lt;p&gt;Essentially, the patch makes it so that we are extremely biased against scanning (and thus reclaiming) any anonymous pages at all with &lt;code&gt;vm.swappiness = 0&lt;/code&gt;, unless we are already encountering severe memory contention. As mentioned previously in this post, that's generally not what you want, since this prevents equality of reclamation prior to extreme memory pressure occurring, which may actually lead to this extreme memory pressure in the first place. &lt;code&gt;vm.swappiness = 1&lt;/code&gt; is the lowest you can go without invoking the special casing for anonymous page scanning implemented in that patch.&lt;/p&gt;
    &lt;p&gt;The kernel default here is &lt;code&gt;vm.swappiness = 60&lt;/code&gt;. This value is generally not too bad for most workloads, but it's hard to have a general default that suits all workloads. As such, a valuable extension to the tuning mentioned in the "how much swap do I need" section above would be to test these systems with differing values for vm.swappiness, and monitor your application and system metrics under heavy (memory) load. Some time in the near future, once we have a decent implementation of refault detection in the kernel, you'll also be able to determine this somewhat workload-agnostically by looking at cgroup v2's page refaulting metrics.&lt;/p&gt;
    &lt;head rend="h3"&gt;Update as of 2019-07: memory pressure metrics in kernel 4.20+&lt;/head&gt;
    &lt;p&gt;The refault metrics mentioned as in development earlier are now in the kernel from 4.20 onwards and can be enabled with &lt;code&gt;CONFIG_PSI=y&lt;/code&gt;. See my talk at SREcon at around the 25:05 mark:&lt;/p&gt;
    &lt;head rend="h2"&gt;In conclusion&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Swap is a useful tool to allow equality of reclamation of memory pages, but its purpose is frequently misunderstood, leading to its negative perception across the industry. If you use swap in the spirit intended, though – as a method of increasing equality of reclamation – you'll find that it's a useful tool instead of a hindrance.&lt;/item&gt;
      &lt;item&gt;Disabling swap does not prevent disk I/O from becoming a problem under memory contention, it simply shifts the disk I/O thrashing from anonymous pages to file pages. Not only may this be less efficient, as we have a smaller pool of pages to select from for reclaim, but it may also contribute to getting into this high contention state in the first place.&lt;/item&gt;
      &lt;item&gt;Swap can make a system slower to OOM kill, since it provides another, slower source of memory to thrash on in out of memory situations – the OOM killer is only used by the kernel as a last resort, after things have already become monumentally screwed. The solutions here depend on your system: &lt;list rend="ul"&gt;&lt;item&gt;You can opportunistically change the system workload depending on cgroup-local or global memory pressure. This prevents getting into these situations in the first place, but solid memory pressure metrics are lacking throughout the history of Unix. Hopefully this should be better soon with the addition of refault detection.&lt;/item&gt;&lt;item&gt;You can bias reclaiming (and thus swapping) away from certain processes per-cgroup using &lt;code&gt;memory.low&lt;/code&gt;, allowing you to protect critical daemons without disabling swap entirely.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Many thanks to Rahul, Tejun, and Johannes for their extensive suggestions and feedback on this post.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45318798</guid><pubDate>Sun, 21 Sep 2025 00:06:38 +0000</pubDate></item><item><title>AI was supposed to help juniors shine. why does it mostly make seniors stronger?</title><link>https://elma.dev/notes/ai-makes-seniors-stronger/</link><description>&lt;doc fingerprint="a6601955c2e51a7c"&gt;
  &lt;main&gt;
    &lt;p&gt;The question “Will coding be taken over entirely by AI?” has been asked to death already, and people keep trying to answer it. I’m not sure there’s anything truly new to say, but I want to share my own observations.&lt;/p&gt;
    &lt;p&gt;The early narrative was that companies would need fewer seniors, and juniors together with AI could produce quality code. At least that’s what I kept seeing. But now, partly because AI hasn’t quite lived up to the hype, it looks like what companies actually need is not junior + AI, but senior + AI.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why?&lt;/head&gt;
    &lt;p&gt;Let’s look at where AI is good and where it falls short in coding.&lt;/p&gt;
    &lt;p&gt;Where it helps:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Cranking out boilerplate and scaffolding&lt;/item&gt;
      &lt;item&gt;Automating repetitive routines&lt;/item&gt;
      &lt;item&gt;Trying out different implementations&lt;/item&gt;
      &lt;item&gt;Validating things quickly thanks to fast iteration&lt;/item&gt;
      &lt;item&gt;Shipping features fast, as long as you know what you want&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And who benefits most from that? Obviously seniors. In the hands of a junior, these things are harder to turn into real value. Still possible, but much tougher.&lt;/p&gt;
    &lt;p&gt;Where it backfires:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Code review: AI can’t really reason. Reviews can be useful, but once edge cases pop up (and they do a lot more in AI-generated code), you’re left needing a senior anyway.&lt;/item&gt;
      &lt;item&gt;Bad prompts: Who writes good prompts? The people who actually understand what they’re building. If someone lacks the knowledge, they might still get “okay-ish” results, but with no proper checks in place it just leads to bugs and headaches.&lt;/item&gt;
      &lt;item&gt;Architecture: Without solid architecture, software quickly loses value. Today AI can’t truly design good architecture; it feels like it might, but this kind of reasoning still requires humans. Projects that start with weak architecture end up drowning in technical debt.&lt;/item&gt;
      &lt;item&gt;Code quality: Choosing the right abstractions, applying design patterns properly, keeping things clean and context-appropriate. AI still struggles here.&lt;/item&gt;
      &lt;item&gt;Security: Think of it like a house without doors, or with broken locks. Security holes pop up more often with junior + AI combinations. Sure, security bugs exist everywhere, but at least with seniors you have some level of awareness and caution.&lt;/item&gt;
      &lt;item&gt;Wrong learning: If someone can’t really evaluate the code, they may not realize what’s wrong with what AI produces. Inside a company that can mean producing damage instead of value.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;There are more examples, but the main point is this: AI is not really a threat to senior developers yet. It may even be the opposite. And this is not about criticizing juniors. It is about not throwing them into risky situations with unrealistic expectations.&lt;/p&gt;
    &lt;p&gt;Where we should use AI:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Fast prototyping: Perfect for trying out an idea quickly.&lt;/item&gt;
      &lt;item&gt;Speeding up routines: The most important use. Automate the things you already know well and repeat often.&lt;/item&gt;
      &lt;item&gt;Multi-disciplinary work: Filling gaps in your knowledge, suggesting useful methods or libraries, helping connect the dots when multiple domains collide.&lt;/item&gt;
      &lt;item&gt;Function tests: Simple, repetitive, low-risk code you can easily double-check.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;From my perspective, that is the current state of things. We still have to read every line AI writes. It is far from perfect. No awareness. Reasoning is imitation. It is non-deterministic, which is why we rely on deterministic things like tests. But then, are you really going to trust the AI to write the tests that verify its own code?&lt;/p&gt;
    &lt;p&gt;It reminds me of something I tweeted: there was a prompt making AI say “I don’t know” when it didn’t know. My take was: “If such AI says ‘I don’t know,’ you can’t be sure it knows that either.”&lt;/p&gt;
    &lt;p&gt;Of course, the junior + AI pairing was tempting. It looked cheaper, and it fed the fear that “AI will take our jobs.” But when you compare software to other professions, the field still shows signs of immaturity. In construction, architects design. In software, even the architects are still laying bricks by writing code. Our roles are still not specialized or merit-driven enough, and cost-cutting dominates. That devalues the work and burns people out.&lt;/p&gt;
    &lt;p&gt;So instead of democratizing coding, AI right now has mostly concentrated power in the hands of experts. Expectations did not quite match reality. We will see what happens next. I am optimistic about AI’s future, but in the short run we should probably reset our expectations before they warp any further.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45319062</guid><pubDate>Sun, 21 Sep 2025 00:56:59 +0000</pubDate></item><item><title>The bloat of edge-case first libraries</title><link>https://43081j.com/2025/09/bloat-of-edge-case-libraries</link><description>&lt;doc fingerprint="6e55d2411b082c98"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The bloat of edge-case first libraries&lt;/head&gt;
    &lt;p&gt;This is just some of what I’ve been pondering recently - particularly in terms of how we ended up with such overly-granular dependency trees.&lt;/p&gt;
    &lt;p&gt;I think we’ve ended up with many libraries in our ecosystem which are edge-case-first, the opposite to what I’d expect. I’ll give a few examples and some thoughts around this, mostly in the hope we can start to trim some of it away.&lt;/p&gt;
    &lt;head rend="h1"&gt;The problem&lt;/head&gt;
    &lt;p&gt;I believe a lot of the questionably small libraries hiding in our deep dependency trees are a result of over-engineering for inputs and edge cases we’ve probably never seen.&lt;/p&gt;
    &lt;p&gt;For example, say we’re building a &lt;code&gt;clamp&lt;/code&gt; function:&lt;/p&gt;
    &lt;code&gt;export function clamp(value: number, min: number, max: number): number {
  return Math.min(Math.max(value, min), max);
}
&lt;/code&gt;
    &lt;p&gt;Pretty simple!&lt;/p&gt;
    &lt;p&gt;What if someone passes nonsensical ranges? Let’s handle that.&lt;/p&gt;
    &lt;code&gt;export function clamp(value: number, min: number, max: number): number {
  if (min &amp;gt; max) {
    throw new Error('min must be less than or equal to max');
  }
  return Math.min(Math.max(value, min), max);
}
&lt;/code&gt;
    &lt;p&gt;This is probably as far as I’d go. But let’s over-engineer - what if someone passes a number-like string?&lt;/p&gt;
    &lt;code&gt;export function clamp(value: number | string, min: number | string, max: number | string): number {
  if (typeof value === 'string' &amp;amp;&amp;amp; Number.isNaN(Number(value))) {
    throw new Error('value must be a number or a number-like string');
  }
  if (typeof min === 'string' &amp;amp;&amp;amp; Number.isNaN(Number(min))) {
    throw new Error('min must be a number or a number-like string');
  }
  if (typeof max === 'string' &amp;amp;&amp;amp; Number.isNaN(Number(max))) {
    throw new Error('max must be a number or a number-like string');
  }
  if (Number(min) &amp;gt; Number(max)) {
    throw new Error('min must be less than or equal to max');
  }
  return Math.min(Math.max(value, min), max);
}
&lt;/code&gt;
    &lt;p&gt;At this point, it seems clear to me we’ve just poorly designed our function. It solely exists to clamp numbers, so why would we accept strings?&lt;/p&gt;
    &lt;p&gt;But hey, let’s go further! What if other libraries also want to accept such loose inputs? Let’s extract this into a separate library:&lt;/p&gt;
    &lt;code&gt;import isNumber from 'is-number';

export function clamp(value: number | string, min: number | string, max: number | string): number {
  if (!isNumber(value)) {
    throw new Error('value must be a number or a number-like string');
  }
  if (!isNumber(min)) {
    throw new Error('min must be a number or a number-like string');
  }
  if (!isNumber(max)) {
    throw new Error('max must be a number or a number-like string');
  }
  if (Number(min) &amp;gt; Number(max)) {
    throw new Error('min must be less than or equal to max');
  }
  return Math.min(Math.max(value, min), max);
}
&lt;/code&gt;
    &lt;p&gt;Whoops! We’ve just created the infamous &lt;code&gt;is-number&lt;/code&gt; library!&lt;/p&gt;
    &lt;head rend="h1"&gt;How it should be&lt;/head&gt;
    &lt;p&gt;This, in my opinion, is poor technical design we’ve all ended up dealing with over the years. Carrying the baggage of these overly-granular libraries that exist to handle edge cases we’ve probably never encountered.&lt;/p&gt;
    &lt;p&gt;I think it should have been:&lt;/p&gt;
    &lt;code&gt;export function clamp(value: number, min: number, max: number): number {
  return Math.min(Math.max(value, min), max);
}
&lt;/code&gt;
    &lt;p&gt;Maybe with some &lt;code&gt;min &amp;lt;= max&lt;/code&gt; validation, but even that is debatable. At this point, you may as well inline the &lt;code&gt;Math.min(Math.max(...))&lt;/code&gt; expression instead of using a dependency.&lt;/p&gt;
    &lt;p&gt;We should be able to define our functions to accept the inputs they are designed for, and not try to handle every possible edge case.&lt;/p&gt;
    &lt;p&gt;There are two things at play here:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Data types&lt;/item&gt;
      &lt;item&gt;Values&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A well designed library would assume the right data types have been passed in, but may validate that the values make sense (e.g. &lt;code&gt;min&lt;/code&gt; is less than or equal to &lt;code&gt;max&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;These over-engineered libraries have decided to implement both at runtime - essentially run-time type checking and value validation. One could argue that this is just a result of building in the pre-TypeScript era, but that still doesn’t justify the overly specific value validation (e.g. the real &lt;code&gt;is-number&lt;/code&gt; also checks that it is finite).&lt;/p&gt;
    &lt;head rend="h1"&gt;What we shouldn’t do&lt;/head&gt;
    &lt;p&gt;We shouldn’t build edge-case-first libraries, i.e. those which solve for edge cases we have yet to encounter or are unlikely to ever encounter.&lt;/p&gt;
    &lt;head rend="h2"&gt;Example: &lt;code&gt;is-arrayish&lt;/code&gt; (76M downloads/week)&lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;is-arrayish&lt;/code&gt; library determines if a value is an &lt;code&gt;Array&lt;/code&gt; or behaves like one.&lt;/p&gt;
    &lt;p&gt;There will be some edge cases where this matters a lot, where we want to accept something we can index into but don’t care if it is a real &lt;code&gt;Array&lt;/code&gt; or not.&lt;/p&gt;
    &lt;p&gt;However, the common use case clearly will not be that and we could’ve just used &lt;code&gt;Array.isArray()&lt;/code&gt; all along.&lt;/p&gt;
    &lt;head rend="h2"&gt;Example: &lt;code&gt;is-number&lt;/code&gt; (90M downloads/week)&lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;is-number&lt;/code&gt; library determines if a value is a positive, finite number or number-like string (maybe we should name it &lt;code&gt;is-positive-finite-number&lt;/code&gt; to be more accurate).&lt;/p&gt;
    &lt;p&gt;Again, there will be edge cases where we want to deal with number-like strings or we want to validate that a number is within a range (e.g. finite).&lt;/p&gt;
    &lt;p&gt;The common use case will not be this. The common use case will be that we want to check &lt;code&gt;typeof n === 'number'&lt;/code&gt; and be done with it.&lt;/p&gt;
    &lt;p&gt;For those edge cases where we want to additionally validate what kind of number it is, we could use a library (but one which exists for the validation, not for the type check).&lt;/p&gt;
    &lt;head rend="h2"&gt;Example: &lt;code&gt;pascalcase&lt;/code&gt; (9.7M downloads/week)&lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;pascalcase&lt;/code&gt; library transforms text to PascalCase.&lt;/p&gt;
    &lt;p&gt;It has 1 dependency (&lt;code&gt;camelcase&lt;/code&gt;) and accepts a variety of input types:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;strings&lt;/item&gt;
      &lt;item&gt;null&lt;/item&gt;
      &lt;item&gt;undefined&lt;/item&gt;
      &lt;item&gt;arrays of strings&lt;/item&gt;
      &lt;item&gt;functions&lt;/item&gt;
      &lt;item&gt;arbitrary objects with &lt;code&gt;toString&lt;/code&gt;methods&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In reality, almost every user will be passing a &lt;code&gt;string&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;Example: &lt;code&gt;is-regexp&lt;/code&gt; (10M downloads/week)&lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;is-regexp&lt;/code&gt; library checks if a value is a &lt;code&gt;RegExp&lt;/code&gt; object, and supports cross-realm values.&lt;/p&gt;
    &lt;p&gt;In reality, almost every user will be passing a &lt;code&gt;RegExp&lt;/code&gt; object, and not one from another realm.&lt;/p&gt;
    &lt;p&gt;For context, cross-realm values can happen when you retrieve a value from an &lt;code&gt;iframe&lt;/code&gt; or VM for example:&lt;/p&gt;
    &lt;code&gt;const iframe = document.createElement('iframe');
iframe.contentWindow.RegExp === RegExp; // false

const iframeRegex = iframe.contentWindow.someRegexp;

iframeRegex instanceof RegExp; // false
isRegex(iframeRegex); // true
&lt;/code&gt;
    &lt;p&gt;This is indeed useful, and I do support this myself in chai (which I maintain). However, this is an edge case most libraries don’t need to care about.&lt;/p&gt;
    &lt;head rend="h1"&gt;What we should do&lt;/head&gt;
    &lt;p&gt;We should build libraries which solve the common use case and make assumptions about the input types they will be given.&lt;/p&gt;
    &lt;head rend="h2"&gt;Example: scule (1.8M downloads/week)&lt;/head&gt;
    &lt;p&gt;scule is a library for transforming casing of text (e.g. camel case, etc).&lt;/p&gt;
    &lt;p&gt;It only accepts inputs it is designed for (strings and arrays of strings) and has zero dependencies.&lt;/p&gt;
    &lt;p&gt;In most of the functions it exports, it assumes valid input data types.&lt;/p&gt;
    &lt;head rend="h2"&gt;Example: dlv (14.9M downloads/week)&lt;/head&gt;
    &lt;p&gt;dlv is a library for deep property access.&lt;/p&gt;
    &lt;p&gt;It only accepts strings and arrays of strings as the path to access, and assumes this (i.e. does no validation).&lt;/p&gt;
    &lt;head rend="h1"&gt;Validation is important&lt;/head&gt;
    &lt;p&gt;Validation is important, and I want to be clear that I’m not saying we should stop validating our data.&lt;/p&gt;
    &lt;p&gt;However, we should usually be validating the data in the project that owns it (e.g. at the app level), and not in every library that later consumes it as input.&lt;/p&gt;
    &lt;p&gt;Deep dependencies applying validation like this actually shift the burden from where it belongs (at data boundaries) to deep in the dependency tree.&lt;/p&gt;
    &lt;p&gt;Often at this point, it is invisible to the consumer of the library.&lt;/p&gt;
    &lt;p&gt;How many people are passing values into &lt;code&gt;is-number&lt;/code&gt; (via other libraries), not realising it will prevent them from using negative numbers and &lt;code&gt;Infinity&lt;/code&gt;?&lt;/p&gt;
    &lt;head rend="h1"&gt;A note on overly-granular libraries&lt;/head&gt;
    &lt;p&gt;This post isn’t about overly-granular libraries in general, but I’d like to briefly mention them for visibility.&lt;/p&gt;
    &lt;p&gt;An overly-granular library is one where someone took a useful library and split it up into an almost atomic-level of granularity.&lt;/p&gt;
    &lt;p&gt;Some examples:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;shebang-regex&lt;/code&gt;- 2LOC, does the same as&lt;code&gt;startsWith('#!')&lt;/code&gt;, 86M downloads/week&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;is-whitespace&lt;/code&gt;- 7LOC, checks if a string is only whitespace, 1M downloads/week&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;is-npm&lt;/code&gt;- 8LOC, checks&lt;code&gt;npm_config_user_agent&lt;/code&gt;or&lt;code&gt;npm_package_json&lt;/code&gt;are set, 7M downloads/week&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is a personal preference some maintainers clearly prefer. The thought seems to be that by having atomic libraries, you can easily build your next library mostly from the existing building blocks you have.&lt;/p&gt;
    &lt;p&gt;I don’t really agree with this and think downloading a package for &lt;code&gt;#!&lt;/code&gt; 86 million times a week is a bit much.&lt;/p&gt;
    &lt;head rend="h1"&gt;What can be done about this?&lt;/head&gt;
    &lt;p&gt;The e18e community is already tackling a lot of this by contributing performance improvements across the ecosystem, including removing and replacing dependencies with more modern, performant ones.&lt;/p&gt;
    &lt;p&gt;Through these efforts, there’s already a useful list of replacements and an ESLint plugin.&lt;/p&gt;
    &lt;head rend="h2"&gt;As a maintainer&lt;/head&gt;
    &lt;p&gt;If you’re maintaining a library, it would be worth reviewing your dependencies to see if:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Any are replaceable by native functionality these days (e.g. &lt;code&gt;Array.isArray&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Any are replaceable by smaller, less granular and/or more performant alternatives (e.g. &lt;code&gt;scule&lt;/code&gt;instead of&lt;code&gt;pascalcase&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Any are redundant if you make more assumptions about input types&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Tools like npmgraph can help you visualise your dependency tree to make this task easier.&lt;/p&gt;
    &lt;p&gt;Also, being stricter around input types will allow you to reduce a lot of code and dependencies.&lt;/p&gt;
    &lt;p&gt;If you can assume the data being passed in is the correct type, you can leave validation up to the consumer.&lt;/p&gt;
    &lt;head rend="h2"&gt;As a user&lt;/head&gt;
    &lt;p&gt;Keep a close eye on your dependencies (both deep and direct), and what alternatives are available to your direct dependencies.&lt;/p&gt;
    &lt;p&gt;Often, it is easy to stick with a dependency from long ago and forget to re-visit it one day in case there is a better way. Many of these packages are possible natively, or have more modern alternatives.&lt;/p&gt;
    &lt;p&gt;Useful tools:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;npmgraph for visualising your dependency tree&lt;/item&gt;
      &lt;item&gt;node-modules.dev for visualising your dependencies and lots of useful meta data&lt;/item&gt;
      &lt;item&gt;Dependabot for keeping your dependencies up to date&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;On the topic of data, it is also worth ensuring validation happens at data boundaries rather than being delegated to various dependencies. Try to validate the type and value up front, before passing into dependencies.&lt;/p&gt;
    &lt;head rend="h1"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Most of these libraries exist to handle edge cases that do certainly exist. However, we are all paying the cost of that rather than only those who need to support those edge cases.&lt;/p&gt;
    &lt;p&gt;This is the wrong way around. Libraries should implement the main use case, and alternatives (or plugins) can exist to provide the edge cases the minority needs.&lt;/p&gt;
    &lt;p&gt;We should all be more aware of what is in our dependency tree, and should push for more concise, lighter libraries.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45319399</guid><pubDate>Sun, 21 Sep 2025 02:09:43 +0000</pubDate></item><item><title>iFixit iPhone Air teardown</title><link>https://www.ifixit.com/News/113171/iphone-air-teardown</link><description>&lt;doc fingerprint="b569c1074bc717dd"&gt;
  &lt;main&gt;
    &lt;p&gt;To be honest, we were holding our breath for the iPhone Air. Thinner usually means flimsier, harder to fix, and more glued-down parts. But the iPhone Air proves otherwise. Apple has somehow built its thinnest iPhone ever without tanking repairability.&lt;/p&gt;
    &lt;p&gt;Just a few months ago, Samsung’s Galaxy S25 Edge pulled off a similar trick in an ultra-thin package. How’d they do it? And how’d Apple follow suit?&lt;/p&gt;
    &lt;p&gt;The secret: Thinner can actually be more repairable, with clever design.&lt;/p&gt;
    &lt;head rend="h2"&gt;Clever Use of Space&lt;/head&gt;
    &lt;p&gt;Apple made one huge design shift in the Air, which they teased in their keynote and we confirmed with our Lumafield Neptune CT scanner: The middle of this phone is basically just a battery with a frame around it. Apple popped the logic board up above the battery, a large part of how their design got thinner without compromising repair.&lt;/p&gt;
    &lt;p&gt;When we score repairability, 80% of our score is determined by the ease of replacing the parts that are most important and most likely to break. To figure this out, we build a model of the repair process. What’s the path you have to take to get to the battery, or to the screen? We call this the “disassembly tree.” The ideal (if unlikely) disassembly tree is flat. No parts in the way of other parts.&lt;/p&gt;
    &lt;p&gt;A thin device often means, advantageously, a flat disassembly tree. Stacked parts are thicker than parts side-by-side. Our friends over at Framework have been saying this for a long time: It’s totally possible to make a thin and light device that’s also built for repair. The Framework Laptop has done this from the start, with nearly all major components accessible when you remove the cover.&lt;/p&gt;
    &lt;p&gt;And that’s exactly what we’re seeing in the Air. The logic board shift freed up room for the battery and helped the phone stay thin without cramming parts on top of each other. It also conveniently puts less stress on the board if the phone flexes in your pocket. It’s a smart workaround for the “bendgate” problems that haunted earlier slim iPhone designs. Not that the Air’s really going to be bending much, as Zack’s test at Jerry Rig Everything suggests.&lt;/p&gt;
    &lt;p&gt;(By the way, did you see we’re teaming up with Zack to bring you a toolkit that’s made for on-the-go repairs and durability testing?)&lt;/p&gt;
    &lt;p&gt;The Air trims a few extras compared to the Plus line it succeeds, losing the lower speaker and a rear camera. Like the 16e, it’s got just a single rear camera.&lt;/p&gt;
    &lt;p&gt;Inside, though, it packs the upgraded C1X modem, a new N1 WiFi chip, and the A19 Pro system-on-chip, all tucked into the logic board sandwich. It’s a lean, efficient setup that makes the most of limited space. This reduced complexity also contributes to quicker disassembly—fewer features, fewer parts, and fewer points of failure.&lt;/p&gt;
    &lt;head rend="h2"&gt;Battery Life? Eh. Battery Swaps? No Big Deal&lt;/head&gt;
    &lt;p&gt;There’s been a lot of buzz about battery life on this phone. Apple said “all-day battery life,” and tech reviewers of the world, noting the lack of watt-hour specificity and immediate announcement of an add-on battery pack, said, “really now?” At 12.26 Wh, this battery is certainly smaller than recent iPhones (closest comparison being the 13 Pro’s 11.97 Wh), and that raises questions about longevity. More charging cycles usually means faster wear. Still, Apple’s efficiency tricks give it solid runtime, at least for now.&lt;/p&gt;
    &lt;p&gt;But no battery lasts forever, so how difficult will swaps be? We’re relieved to see that the Air has all the greatest hits of the last few iPhone battery designs.&lt;/p&gt;
    &lt;p&gt;The Air’s battery is easy to find and accessible through the back glass thanks to Apple’s dual entry design. Even better, it’s a metal-encased battery. This thin layer of armor makes it more bend resistant and safer to replace. Even better than that, it’s mounted with electrically debonding adhesive strips. Hook them up to a power source and the battery lifts right out, no dangerous prying required. We used our FixHub Portable Power Station for an easy 12 V, and each strip freed after about 70 seconds.&lt;/p&gt;
    &lt;p&gt;Even though it’s comparably a small battery, its heft accounts for 28% of the phone’s total weight, more than any other component.&lt;/p&gt;
    &lt;p&gt;And in a fun twist, we’ve confirmed that it’s the exact same cell found in Apple’s MagSafe battery pack. You can swap between them and the phone still boots up just fine. Like a rear-mounted spare tire on an SUV, an iPhone Air with a MagSafe battery pack is ready for an on-the-go swap, if you will. Granted it’ll take a bit more than a tire iron to make it happen.&lt;/p&gt;
    &lt;head rend="h2"&gt;Modular Port, but How About Parts to Back It Up?&lt;/head&gt;
    &lt;p&gt;How about other likely-to-fail parts? USB-C ports are among the most common failure points in modern phones. Ports tend to collect moisture, which can cause corrosion, and no one is immune to pocket lint. Not to mention the standard port problems caused by mechanical wear and tear.&lt;/p&gt;
    &lt;p&gt;Now, to be clear, if your phone stops charging consistently, you shouldn’t jump straight to replacing the port. Every time you stick a charge cable into the port, you’re jamming pocket lint against the back. Give your charge port a cleanout before you replace it.&lt;/p&gt;
    &lt;head rend="h3"&gt;How to Clean the Ports on your Electronic Device&lt;/head&gt;
    &lt;p&gt;Use this guide to clean the ports on your…&lt;/p&gt;
    &lt;p&gt;But when you do need to replace an Air charging port, you’ll be glad to know it’s decently modular, following the trend of the last few iPhone models. It’s a tedious process, with delicate flex cables, adhesive, and hard-to-reach screws, but it’s still feasible.&lt;/p&gt;
    &lt;p&gt;Interestingly, the modularity of the USB-C port doesn’t seem to be a serviceability choice. Apple won’t do USB-C repairs in-house and they don’t sell replacement ports for iPhones. Of course that won’t stop us from selling the parts as soon as we can get them—and regardless of intent, this modularity is nice to have.&lt;/p&gt;
    &lt;p&gt;Third-party parts manufacturers may take a bit to catch up, since this is a brand new architecture for the housing of the USB port. Apple reportedly used 3D printing to shrink the housing to fit the slim frame of the 6.5mm iPhone Air.&lt;/p&gt;
    &lt;p&gt;Apple says this process reduced material usage by 33% compared to conventional forging processes. Granted, the USB-C port is already tiny. But this isn’t the only place they’re using it: The Apple Watch Ultra 3 uses the same titanium-printing process in its case.&lt;/p&gt;
    &lt;p&gt;We took a close look at the titanium material in the USB-C port, with our Evident DSX2000 microscope.&lt;/p&gt;
    &lt;p&gt;What we saw was fascinating: these regular bubble-like structures.&lt;/p&gt;
    &lt;p&gt;We tapped in some friends in the additive manufacturing industry, who said it wasn’t quite like any metal 3D printing they’d seen before. Their best guess is that Apple’s using a binder or aerosol jet process in addition to some after-printing machining. This aligns with a binder jetting patent Apple inherited back in 2015 when they acquired Metaio. Whatever the exact process, the result is some truly impressive titanium manipulation.&lt;/p&gt;
    &lt;p&gt;(If you’re a metal 3D printing expert and want to give us your thoughts in the comments, we’d love to hear from you!)&lt;/p&gt;
    &lt;head rend="h2"&gt;How Strong Is Thin?&lt;/head&gt;
    &lt;p&gt;Titanium may have retired from the rest of the iPhone line (possibly for geopolitical more than technical reasons) but it’s back as the backbone of this slim smartphone. This tough metal is a good choice, but it’s only as strong as its weak points. Our empty-frame bend test snapped the Air at its plastic antenna passthroughs—a necessity if you want your phone to phone properly. CT scans make it clear: Apple reinforced the center section, but the top and bottom remain vulnerable.&lt;/p&gt;
    &lt;p&gt;Of course, the center is where the phone is most likely to bend, and so far testing hasn’t given any indication of undue flexibility. Will that design affect the durability of the phone? We doubt we’ll see instances of Airs snapping at the ends, but only time will tell.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Verdict: A 7 out of 10 Repairability Score&lt;/head&gt;
    &lt;p&gt;At 6.5 mm, the Air is a hair thinner than Samsung’s Galaxy S25 Edge, yet it manages to keep modular parts and early battery access. Apple’s dual entry design makes battery swaps simple and keeps the fancy OLED out of harm’s way. Electrically debonding adhesive makes battery replacements a lot more consistent than traditional or stretch-release adhesive, and most other major components are simple to access and remove. Apple also kept their best-in-class clipped- and screwed-in screen and back glass architecture, enabling quick reassembly without requiring special adhesive.&lt;/p&gt;
    &lt;p&gt;Combined with Apple’s continued commitment to day-one repair manuals, the iPhone Air earns a provisional 7 out of 10 repairability score. (We’re waiting on Apple to make good on their parts availability commitment as well as final results on our parts pairing tests. Their recent track record’s pretty good, though.)&lt;/p&gt;
    &lt;p&gt;Apple has proved that thin doesn’t have to mean unfixable. The iPhone Air is slimmer than any iPhone before it, but its layout and design tradeoffs make repairs more approachable, not less. It still has limits, but the design shows that good engineering can make even the slimmest devices last longer in the real world. Successful field test for your new foldable, Apple. We’re onto you!&lt;/p&gt;
    &lt;p&gt;More Apple 2025 lineup teardowns coming soon. Bonus round: Can TechWoven handle… hot sauce?&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45319690</guid><pubDate>Sun, 21 Sep 2025 03:09:47 +0000</pubDate></item><item><title>Spectral Labs releases SGS-1: the first generative model for structured CAD</title><link>https://www.spectrallabs.ai/research/SGS-1</link><description>&lt;doc fingerprint="a55b8b523846d2a2"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Introducing SGS-1&lt;/head&gt;
    &lt;p&gt;Spectral Labs releases SGS-1: the first generative model for structured CAD.&lt;/p&gt;
    &lt;p&gt;Today we are announcing SGS-1, a foundation model that can generate fully manufacturable and parametric 3D geometry. You can try a research preview of SGS-1 here.&lt;/p&gt;
    &lt;p&gt;Given an image or a 3D mesh, SGS-1 can generate CAD B-Rep parts in STEP format. Unlike all other existing generative models, SGS-1 outputs are accurate and can be edited easily in traditional CAD software.&lt;/p&gt;
    &lt;p&gt;Overview of SGS-1 - users can provide an image or “dumb” 3D file, and get back a parametric B-Rep file that can be easily edited to match specific dimensions&lt;/p&gt;
    &lt;p&gt;SGS-1 shows strong general results, producing much more complex and diverse CAD shapes than existing methods.&lt;/p&gt;
    &lt;p&gt;Illustrative results from SGS-1&lt;/p&gt;
    &lt;p&gt;SGS-1 can be used for real-world engineering tasks. In the below example, SGS-1 is used to design a bracket for a roller assembly from partial context and a text description (additional details below in Generating Parametric Geometry in Assembly Context section).&lt;/p&gt;
    &lt;head rend="h2"&gt;Results and comparing SGS-1 to prior models&lt;/head&gt;
    &lt;p&gt;We compare SGS-1 to SOTA multimodal reasoning LLMs and open-source image-to-CAD models: GPT-5 thinking, a large reasoning model by OpenAI that can produce CadQuery code to represent parametric geometry, and HoLa, a 205M parameter latent diffusion model with 181M parameter VAE that generate B-Rep geometry conditioned on a single input image. We develop a benchmark set of 75 images depicting medium to high complexity parametric geometry, sourced from CAD image renders of various styles, engineering sketches, and images generated by generative AI models. Model performance is evaluated by successful/failed creation of a single valid watertight solid that is an accurate representation of the input image using distance metrics (Success Ratio).&lt;/p&gt;
    &lt;p&gt;Quantitative evaluations&lt;/p&gt;
    &lt;p&gt;We run each model 10 times and show scores for all 10 runs, as well as for the best output of the 10. Although GPT-5 and HoLa BRep can attain non-zero performance on the easiest images, SGS-1 is the best performing model with at least one success for all but the most complex objects.&lt;/p&gt;
    &lt;p&gt;Outputs from the SOTA large reasoning model (GPT-5) demonstrate a clear lack of spatial understanding, producing outputs that are unusable or too simple to actually be useful. We use both SGS-1 and GPT-5 to generate the parametric geometry for the rail mount from the input image, in order to produce the desired target complete assembly.&lt;/p&gt;
    &lt;p&gt;SGS-1 accurately represents the geometry and can be plugged into an assembly context, while the output from the large reasoning model is missing core spatial features.&lt;/p&gt;
    &lt;head rend="h2"&gt;Generating Parametric Geometry in Assembly Context&lt;/head&gt;
    &lt;p&gt;With SGS-1, you can create new parametric geometry within your current assembly context. In this example, SGS-1 takes in a partial CAD assembly and a text description/image of a bracket, and produces a 3D design for a bracket that is feasible for the context.&lt;/p&gt;
    &lt;p&gt;First, render the partial assembly and come up with a text description of the parts you want to add. Next, run it through SGS-1, which will output a parametric B-Rep in the form of a downloadable STEP fileFinally, import the STEP file into your partial assembly and adjust dimensions until the part fits correctly into the assembly&lt;/p&gt;
    &lt;p&gt;SGS-1 is capable of generating diverse designs for tasks like this - several bracket designs created by SGS-1 are shown below:&lt;/p&gt;
    &lt;head rend="h2"&gt;Converting Sketches and Engineering Drawings to B-Rep&lt;/head&gt;
    &lt;p&gt;SGS-1 can be used to convert simple freehand sketches and engineering drawings into geometry that you can work in in your CAD editor. In this example, we run sketches and drawings through SGS-1 to create parametric geometry.&lt;/p&gt;
    &lt;p&gt;Use SGS-1 to transform sketches and drawings into 3D CAD files&lt;/p&gt;
    &lt;p&gt;This works well on simple hand sketches, enabling powerful design workflows.&lt;/p&gt;
    &lt;p&gt;This also works on structured engineering drawings.&lt;/p&gt;
    &lt;head rend="h2"&gt;Automating Reverse Engineering and STL to STEP File Conversion&lt;/head&gt;
    &lt;p&gt;SGS-1 can be used to convert scans and standalone STL or other mesh files to parametric STEP files without any human input, automating reverse engineering of many shapes.&lt;/p&gt;
    &lt;p&gt;Use SGS-1 to convert dumb 3D representations to parametric geometry&lt;/p&gt;
    &lt;head rend="h2"&gt;Limitations&lt;/head&gt;
    &lt;p&gt;SGS-1 is designed to generate parametric 3D geometry for engineering use cases, and struggles when tasked with generating creative assets and organic shapes with complex curvature. In addition, SGS-1 has a limited 3D resolution and struggles with generating very thin structures. Finally, SGS-1 cannot create full assemblies in one shot. We plan to address these limitations with our next model generation.&lt;/p&gt;
    &lt;head rend="h2"&gt;Next Steps&lt;/head&gt;
    &lt;p&gt;SGS-1 represents a significant step forward for foundation models that can generate 3D geometry for engineering tasks. We plan to continue pushing forward the frontier, by training models that can engineer physical systems of increasing complexity. The next generation of models will be natively multimodal, support larger and more complex spatial context, and will be capable of performing more advanced physical reasoning through longer range planning. As we continue to scale up these models, we are excited about scaling up reinforcement learning using physical simulation feedback, which will unlock new physical reasoning capabilities for our models.&lt;/p&gt;
    &lt;p&gt;If you are interested in deploying SGS-1 or collaborating on research, please contact us through this form.&lt;/p&gt;
    &lt;p&gt;We are also hiring! Our team is composed of top AI researchers and engineers with previous experience at institutions such as Autodesk Research, Samsung Research, CMU, and Meta. If you're interested in our work and mission, please get in touch.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45319876</guid><pubDate>Sun, 21 Sep 2025 03:46:07 +0000</pubDate></item><item><title>Vibe coding cleanup as a service</title><link>https://donado.co/en/articles/2025-09-16-vibe-coding-cleanup-as-a-service/</link><description>&lt;doc fingerprint="315083d599a714cd"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Vibe Coding Cleanup as a Service&lt;/head&gt;
    &lt;p&gt;A new service category is quietly emerging in tech: Vibe Coding cleanup. What started as LinkedIn jokes about “fixing AI messes” has become a real business opportunity. The harsh reality nobody wants to admit: most AI-generated code is production-unready, and companies are desperately hiring specialists to fix it before their technical debt spirals out of control.&lt;/p&gt;
    &lt;head rend="h2"&gt;The vibe coding explosion&lt;/head&gt;
    &lt;p&gt;When Andrej Karpathy coined “vibe coding” in early 2025, he perfectly captured how developers now work: chatting with AI to generate entire functions instead of writing them. The approach promises 10x productivity gains through natural language programming. GitHub reports that 92% of developers now use AI coding tools, with Copilot alone generating billions of lines of code monthly.&lt;/p&gt;
    &lt;p&gt;But there’s a problem nobody talks about at conferences. GitClear’s analysis of 150 million lines of code reveals AI assistance correlates with 41% more code churn - code that gets reverted or rewritten within two weeks. Stanford researchers found that developers using AI assistants produce significantly less secure code while believing it’s more secure. The tools amplify bad practices: no input validation, outdated dependencies, and architectural decisions that make senior engineers weep.&lt;/p&gt;
    &lt;head rend="h2"&gt;The cleanup economy is real&lt;/head&gt;
    &lt;p&gt;404 Media’s investigation reveals developers are building entire careers around fixing AI-generated code. Hamid Siddiqi manages 15-20 cleanup projects simultaneously, charging premium rates to untangle what he calls “AI spaghetti” - inconsistent interfaces, redundant functions, and business logic that makes no sense. Software consultancy Ulam Labs now advertises “Vibe Coding cleanup” as a core service.&lt;/p&gt;
    &lt;p&gt;The demand is so high that VibeCodeFixers.com launched as a dedicated marketplace. Within weeks, 300 specialists signed up and dozens of projects were matched. Founder Swatantra Sohni describes a typical client: “They burned through $5,000 in OpenAI credits, have a half-working prototype they’re emotionally attached to, and need it production-ready yesterday.” TechCrunch reports that 25% of Y Combinator’s current startup cohort has codebases that are 95% AI-generated, highlighting the massive scale of this trend across Silicon Valley.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why AI code fails at scale&lt;/head&gt;
    &lt;p&gt;The fundamental issue isn’t that AI writes bad code - it’s that it writes locally optimized code without understanding system context. Stack Overflow’s analysis shows AI excels at small, isolated tasks but fails at architectural decisions. Every prompt creates technical debt: inconsistent patterns, duplicated logic, and security holes that automated scanners miss.&lt;/p&gt;
    &lt;p&gt;Georgetown University research shows that at least 48% of AI-generated code contains security vulnerabilities. The tools leak secrets into code, suggest deprecated libraries, and create race conditions that only appear under load. Worse, developers often don’t understand the generated code well enough to spot these issues. Thoughtworks warns this creates “competency debt” - teams lose the ability to maintain their own systems as they become dependent on AI-generated code they don’t fully understand.&lt;/p&gt;
    &lt;head rend="h2"&gt;The market opportunity&lt;/head&gt;
    &lt;p&gt;The Vibe Coding cleanup market is growing rapidly, though exact numbers are hard to pin down. What we know: Gartner predicts 75% of enterprise software engineers will use AI code assistants by 2028. If even a fraction of those projects need cleanup - and current data suggests most will - we’re looking at a massive emerging market.&lt;/p&gt;
    &lt;p&gt;The economics are compelling. Startups save weeks getting to MVP with Vibe Coding, then spend comparable time and budget on cleanup. But that’s still faster than traditional development. The specialists who can efficiently refactor AI messes command $200-400/hour rates. Some are building productized services: fixed-price cleanup packages, AI code audits, and “vibe-to-production” pipelines.&lt;/p&gt;
    &lt;p&gt;Thoughtworks reports that refactoring activity has declined while code churn increases with AI assistance, with most AI-assisted projects requiring significant cleanup before production. Multiple consultancies are now hiring specifically for “AI code remediation” roles. The market is real, growing, and largely untapped.&lt;/p&gt;
    &lt;head rend="h2"&gt;What this means for engineering&lt;/head&gt;
    &lt;p&gt;We’re witnessing a fundamental shift in how software gets built. AI handles the initial implementation, humans handle architecture, testing, and cleanup. It’s not the future we expected, but it’s the one we’re getting.&lt;/p&gt;
    &lt;p&gt;Gergely Orosz argues AI tools are like “very eager junior developers” - they write code quickly but need constant supervision. The difference is that AI juniors never become seniors. They’ll always need cleanup specialists.&lt;/p&gt;
    &lt;p&gt;This creates interesting career paths. Junior developers who master Vibe Coding cleanup can command senior salaries within two years. Senior engineers who understand both AI capabilities and limitations become invaluable. Companies that build robust cleanup processes gain competitive advantage.&lt;/p&gt;
    &lt;head rend="h2"&gt;Our stance&lt;/head&gt;
    &lt;p&gt;At Donado Labs, we’ve cleaned up enough vibe-coded disasters to recognize the pattern. AI acceleration works, but only with professional cleanup built into the process. We use AI for prototyping and routine tasks, but architecture and critical logic remain human-written. Our “Vibe to Production” service takes AI prototypes and makes them enterprise-ready: proper testing, security hardening, and documentation that won’t make your successor cry.&lt;/p&gt;
    &lt;p&gt;The companies succeeding with AI coding aren’t the ones using it most - they’re the ones using it smartly. They prototype with AI, then invest in cleanup before technical debt compounds. They treat Vibe Coding like any other tool: powerful but dangerous without expertise.&lt;/p&gt;
    &lt;p&gt;Next time someone claims AI will replace programmers, ask them who’s going to clean up the code. That’s where the real opportunity lies.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45320431</guid><pubDate>Sun, 21 Sep 2025 06:01:49 +0000</pubDate></item><item><title>They Thought They Were Free (1955)</title><link>https://press.uchicago.edu/Misc/Chicago/511928.html</link><description>&lt;doc fingerprint="74a0991400f71e59"&gt;
  &lt;main&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;
          &lt;head&gt;An excerpt from&lt;/head&gt;
          &lt;head&gt;They Thought They Were Free&lt;/head&gt;
          &lt;head&gt;The Germans, 1933-45&lt;/head&gt;
          &lt;head&gt;Milton Mayer&lt;/head&gt;
          &lt;head&gt;But Then It Was Too Late&lt;/head&gt;
          &lt;p&gt;"What no one seemed to notice," said a colleague of mine, a philologist, "was the ever widening gap, after 1933, between the government and the people. Just think how very wide this gap was to begin with, here in Germany. And it became always wider. You know, it doesn’t make people close to their government to be told that this is a people’s government, a true democracy, or to be enrolled in civilian defense, or even to vote. All this has little, really nothing, to do with knowing one is governing.&lt;/p&gt;
          &lt;p&gt;"What happened here was the gradual habituation of the people, little by little, to being governed by surprise; to receiving decisions deliberated in secret; to believing that the situation was so complicated that the government had to act on information which the people could not understand, or so dangerous that, even if the people could not understand it, it could not be released because of national security. And their sense of identification with Hitler, their trust in him, made it easier to widen this gap and reassured those who would otherwise have worried about it.&lt;/p&gt;
          &lt;p&gt;"This separation of government from people, this widening of the gap, took place so gradually and so insensibly, each step disguised (perhaps not even intentionally) as a temporary emergency measure or associated with true patriotic allegiance or with real social purposes. And all the crises and reforms (real reforms, too) so occupied the people that they did not see the slow motion underneath, of the whole process of government growing remoter and remoter.&lt;/p&gt;
          &lt;p&gt;"You will understand me when I say that my Middle High German was my life. It was all I cared about. I was a scholar, a specialist. Then, suddenly, I was plunged into all the new activity, as the university was drawn into the new situation; meetings, conferences, interviews, ceremonies, and, above all, papers to be filled out, reports, bibliographies, lists, questionnaires. And on top of that were the demands in the community, the things in which one had to, was ‘expected to’ participate that had not been there or had not been important before. It was all rigmarole, of course, but it consumed all one’s energies, coming on top of the work one really wanted to do. You can see how easy it was, then, not to think about fundamental things. One had no time."&lt;/p&gt;
          &lt;p&gt;"Those," I said, "are the words of my friend the baker. ‘One had no time to think. There was so much going on.’"&lt;/p&gt;
          &lt;p&gt;"Your friend the baker was right," said my colleague. "The dictatorship, and the whole process of its coming into being, was above all diverting. It provided an excuse not to think for people who did not want to think anyway. I do not speak of your ‘little men,’ your baker and so on; I speak of my colleagues and myself, learned men, mind you. Most of us did not want to think about fundamental things and never had. There was no need to. Nazism gave us some dreadful, fundamental things to think about—we were decent people—and kept us so busy with continuous changes and ‘crises’ and so fascinated, yes, fascinated, by the machinations of the ‘national enemies,’ without and within, that we had no time to think about these dreadful things that were growing, little by little, all around us. Unconsciously, I suppose, we were grateful. Who wants to think?&lt;/p&gt;
          &lt;p&gt;"To live in this process is absolutely not to be able to notice it—please try to believe me—unless one has a much greater degree of political awareness, acuity, than most of us had ever had occasion to develop. Each step was so small, so inconsequential, so well explained or, on occasion, ‘regretted,’ that, unless one were detached from the whole process from the beginning, unless one understood what the whole thing was in principle, what all these ‘little measures’ that no ‘patriotic German’ could resent must some day lead to, one no more saw it developing from day to day than a farmer in his field sees the corn growing. One day it is over his head.&lt;/p&gt;
          &lt;p&gt;"How is this to be avoided, among ordinary men, even highly educated ordinary men? Frankly, I do not know. I do not see, even now. Many, many times since it all happened I have pondered that pair of great maxims, Principiis obsta and Finem respice—‘Resist the beginnings’ and ‘Consider the end.’ But one must foresee the end in order to resist, or even see, the beginnings. One must foresee the end clearly and certainly and how is this to be done, by ordinary men or even by extraordinary men? Things might have. And everyone counts on that might.&lt;/p&gt;
          &lt;p&gt;"Your ‘little men,’ your Nazi friends, were not against National Socialism in principle. Men like me, who were, are the greater offenders, not because we knew better (that would be too much to say) but because we sensed better. Pastor Niemöller spoke for the thousands and thousands of men like me when he spoke (too modestly of himself) and said that, when the Nazis attacked the Communists, he was a little uneasy, but, after all, he was not a Communist, and so he did nothing; and then they attacked the Socialists, and he was a little uneasier, but, still, he was not a Socialist, and he did nothing; and then the schools, the press, the Jews, and so on, and he was always uneasier, but still he did nothing. And then they attacked the Church, and he was a Churchman, and he did something—but then it was too late."&lt;/p&gt;
          &lt;p&gt;"Yes," I said.&lt;/p&gt;
          &lt;p&gt;"You see," my colleague went on, "one doesn’t see exactly where or how to move. Believe me, this is true. Each act, each occasion, is worse than the last, but only a little worse. You wait for the next and the next. You wait for one great shocking occasion, thinking that others, when such a shock comes, will join with you in resisting somehow. You don’t want to act, or even talk, alone; you don’t want to ‘go out of your way to make trouble.’ Why not?—Well, you are not in the habit of doing it. And it is not just fear, fear of standing alone, that restrains you; it is also genuine uncertainty.&lt;/p&gt;
          &lt;p&gt;"Uncertainty is a very important factor, and, instead of decreasing as time goes on, it grows. Outside, in the streets, in the general community, ‘everyone’ is happy. One hears no protest, and certainly sees none. You know, in France or Italy there would be slogans against the government painted on walls and fences; in Germany, outside the great cities, perhaps, there is not even this. In the university community, in your own community, you speak privately to your colleagues, some of whom certainly feel as you do; but what do they say? They say, ‘It’s not so bad’ or ‘You’re seeing things’ or ‘You’re an alarmist.’&lt;/p&gt;
          &lt;p&gt;"And you are an alarmist. You are saying that this must lead to this, and you can’t prove it. These are the beginnings, yes; but how do you know for sure when you don’t know the end, and how do you know, or even surmise, the end? On the one hand, your enemies, the law, the regime, the Party, intimidate you. On the other, your colleagues pooh-pooh you as pessimistic or even neurotic. You are left with your close friends, who are, naturally, people who have always thought as you have.&lt;/p&gt;
          &lt;p&gt;"But your friends are fewer now. Some have drifted off somewhere or submerged themselves in their work. You no longer see as many as you did at meetings or gatherings. Informal groups become smaller; attendance drops off in little organizations, and the organizations themselves wither. Now, in small gatherings of your oldest friends, you feel that you are talking to yourselves, that you are isolated from the reality of things. This weakens your confidence still further and serves as a further deterrent to—to what? It is clearer all the time that, if you are going to do anything, you must make an occasion to do it, and then you are obviously a troublemaker. So you wait, and you wait.&lt;/p&gt;
          &lt;p&gt;"But the one great shocking occasion, when tens or hundreds or thousands will join with you, never comes. That’s the difficulty. If the last and worst act of the whole regime had come immediately after the first and smallest, thousands, yes, millions would have been sufficiently shocked—if, let us say, the gassing of the Jews in ’43 had come immediately after the ‘German Firm’ stickers on the windows of non-Jewish shops in ’33. But of course this isn’t the way it happens. In between come all the hundreds of little steps, some of them imperceptible, each of them preparing you not to be shocked by the next. Step C is not so much worse than Step B, and, if you did not make a stand at Step B, why should you at Step C? And so on to Step D.&lt;/p&gt;
          &lt;p&gt;"And one day, too late, your principles, if you were ever sensible of them, all rush in upon you. The burden of self-deception has grown too heavy, and some minor incident, in my case my little boy, hardly more than a baby, saying ‘Jewish swine,’ collapses it all at once, and you see that everything, everything, has changed and changed completely under your nose. The world you live in—your nation, your people—is not the world you were born in at all. The forms are all there, all untouched, all reassuring, the houses, the shops, the jobs, the mealtimes, the visits, the concerts, the cinema, the holidays. But the spirit, which you never noticed because you made the lifelong mistake of identifying it with the forms, is changed. Now you live in a world of hate and fear, and the people who hate and fear do not even know it themselves; when everyone is transformed, no one is transformed. Now you live in a system which rules without responsibility even to God. The system itself could not have intended this in the beginning, but in order to sustain itself it was compelled to go all the way.&lt;/p&gt;
          &lt;p&gt;"You have gone almost all the way yourself. Life is a continuing process, a flow, not a succession of acts and events at all. It has flowed to a new level, carrying you with it, without any effort on your part. On this new level you live, you have been living more comfortably every day, with new morals, new principles. You have accepted things you would not have accepted five years ago, a year ago, things that your father, even in Germany, could not have imagined.&lt;/p&gt;
          &lt;p&gt;"Suddenly it all comes down, all at once. You see what you are, what you have done, or, more accurately, what you haven’t done (for that was all that was required of most of us: that we do nothing). You remember those early meetings of your department in the university when, if one had stood, others would have stood, perhaps, but no one stood. A small matter, a matter of hiring this man or that, and you hired this one rather than that. You remember everything now, and your heart breaks. Too late. You are compromised beyond repair.&lt;/p&gt;
          &lt;p&gt;"What then? You must then shoot yourself. A few did. Or ‘adjust’ your principles. Many tried, and some, I suppose, succeeded; not I, however. Or learn to live the rest of your life with your shame. This last is the nearest there is, under the circumstances, to heroism: shame. Many Germans became this poor kind of hero, many more, I think, than the world knows or cares to know."&lt;/p&gt;
          &lt;p&gt;I said nothing. I thought of nothing to say.&lt;/p&gt;
          &lt;p&gt;"I can tell you," my colleague went on, "of a man in Leipzig, a judge. He was not a Nazi, except nominally, but he certainly wasn’t an anti-Nazi. He was just—a judge. In ’42 or ’43, early ’43, I think it was, a Jew was tried before him in a case involving, but only incidentally, relations with an ‘Aryan’ woman. This was ‘race injury,’ something the Party was especially anxious to punish. In the case at bar, however, the judge had the power to convict the man of a ‘nonracial’ offense and send him to an ordinary prison for a very long term, thus saving him from Party ‘processing’ which would have meant concentration camp or, more probably, deportation and death. But the man was innocent of the ‘nonracial’ charge, in the judge’s opinion, and so, as an honorable judge, he acquitted him. Of course, the Party seized the Jew as soon as he left the courtroom."&lt;/p&gt;
          &lt;p&gt;"And the judge?"&lt;/p&gt;
          &lt;p&gt;"Yes, the judge. He could not get the case off his conscience—a case, mind you, in which he had acquitted an innocent man. He thought that he should have convicted him and saved him from the Party, but how could he have convicted an innocent man? The thing preyed on him more and more, and he had to talk about it, first to his family, then to his friends, and then to acquaintances. (That’s how I heard about it.) After the ’44 Putsch they arrested him. After that, I don’t know."&lt;/p&gt;
          &lt;p&gt;I said nothing.&lt;/p&gt;
          &lt;p&gt;"Once the war began," my colleague continued, "resistance, protest, criticism, complaint, all carried with them a multiplied likelihood of the greatest punishment. Mere lack of enthusiasm, or failure to show it in public, was ‘defeatism.’ You assumed that there were lists of those who would be ‘dealt with’ later, after the victory. Goebbels was very clever here, too. He continually promised a ‘victory orgy’ to ‘take care of’ those who thought that their ‘treasonable attitude’ had escaped notice. And he meant it; that was not just propaganda. And that was enough to put an end to all uncertainty.&lt;/p&gt;
          &lt;p&gt;"Once the war began, the government could do anything ‘necessary’ to win it; so it was with the ‘final solution of the Jewish problem,’ which the Nazis always talked about but never dared undertake, not even the Nazis, until war and its ‘necessities’ gave them the knowledge that they could get away with it. The people abroad who thought that war against Hitler would help the Jews were wrong. And the people in Germany who, once the war had begun, still thought of complaining, protesting, resisting, were betting on Germany’s losing the war. It was a long bet. Not many made it."&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row/&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45321663</guid><pubDate>Sun, 21 Sep 2025 10:56:42 +0000</pubDate></item><item><title>Meta exposé author faces bankruptcy after ban on criticising company</title><link>https://www.theguardian.com/technology/2025/sep/21/meta-expose-author-sarah-wynn-williams-faces-bankruptcy-after-ban-on-criticising-company</link><description>&lt;doc fingerprint="2ea8491b1aa230fe"&gt;
  &lt;main&gt;
    &lt;p&gt;A former Meta executive who wrote an explosive exposé making allegations about the social media company’s dealings with China and its treatment of teenagers is said to be “on the verge of bankruptcy” after publishing the book.&lt;/p&gt;
    &lt;p&gt;An MP has claimed in parliament that Mark Zuckerberg’s company was trying to “silence and punish” Sarah Wynn-Williams, the former director of global public policy at Meta’s precursor, Facebook, after her decision to speak out about her time at the company.&lt;/p&gt;
    &lt;p&gt;Louise Haigh, the former Labour transport secretary, said Wynn-Williams was facing a fine of $50,000 (£37,000) every time she breached an order secured by Meta preventing her from talking disparagingly about the company.&lt;/p&gt;
    &lt;p&gt;Wynn-Williams made a series of claims about the social media company’s behaviour and culture in her book Careless People, published this year. It also contained allegations of sexual harassment denied by the company. It states she was fired for “poor performance and toxic behaviour”.&lt;/p&gt;
    &lt;p&gt;However, the former diplomat was barred from publicising the memoir after Meta, which owns Facebook and Instagram, secured a ruling preventing her from doing so. She subsequently appeared before a US Senate judiciary subcommittee, in which she said Meta worked “hand in glove” with Beijing over censorship tools – something the company has denied.&lt;/p&gt;
    &lt;p&gt;Pan Macmillan, which published the memoir, said it had sold more than 150,000 copies across all formats. The book was also named in The Sunday Times‘ bestselling hardbacks of 2025 so far. The paperback edition is due to be published early next year.&lt;/p&gt;
    &lt;p&gt;New York magazine has previously reported that Wynn-Williams was paid an advance for the book of more than $500,000 (£370,000).&lt;/p&gt;
    &lt;p&gt;Haigh highlighted Wynn-Williams’s case in the House of Commons during a debate about employment rights on Monday. She said Wynn-Williams’s decision to speak out had plunged her into financial peril.&lt;/p&gt;
    &lt;p&gt;“Despite previous public statements that Meta no longer uses NDAs [non-disclosure agreements] in cases of sexual harassment – which Sarah has repeatedly alleged – she is being pushed to financial ruin through the arbitration system in the UK, as Meta seeks to silence and punish her for speaking out,” she said.&lt;/p&gt;
    &lt;p&gt;“Meta has served a gagging order on Sarah and is attempting to fine her $50,000 for every breach of that order. She is on the verge of bankruptcy. I am sure that the whole house and the government will stand with Sarah as we pass this legislation to ensure that whistleblowers and those with the moral courage to speak out are always protected.”&lt;/p&gt;
    &lt;p&gt;It is understood that the $50,000 figure represents the damages Wynn-Williams has to pay for material breaches of the separation agreement she signed when she left Meta in 2017. Meta has emphasised that Wynn-Williams entered into the non-disparagement agreement voluntarily as part of her departure.&lt;/p&gt;
    &lt;p&gt;Meta said that to date, Wynn-Williams had not been forced to make any payments under the agreement.&lt;/p&gt;
    &lt;p&gt;The company did not wish to comment on Haigh’s intervention. It has previously said that Wynn-Williams’s Senate testimony was “divorced from reality and riddled with false claims” about China and the company’s treatment of teenagers.&lt;/p&gt;
    &lt;p&gt;Meta has described the book as a “mix of out-of-date and previously reported claims about the company and false accusations about our executives”. It has said she was fired for “poor performance and toxic behaviour” and that an investigation concluded she made misleading and unfounded allegations of harassment.&lt;/p&gt;
    &lt;p&gt;It said the ruling preventing her from publicising the memoir confirmed the “false and defamatory book should never have been published”.&lt;/p&gt;
    &lt;p&gt;The ruling stated Wynn-Williams should stop promoting the book and, to the extent she could, stop further publication. It did not order any action by Pan Macmillan.&lt;/p&gt;
    &lt;p&gt;Wynn-Williams has not spoken in public since appearing at the Senate hearing in April. In a written statement this month, she said she was grateful that the US Senate was continuing to investigate Meta’s behaviour.&lt;/p&gt;
    &lt;p&gt;“I wish I could say more,” she said. “I urge other tech employees and those who are thinking of whistleblowing to share what they know before more children are harmed.”&lt;/p&gt;
    &lt;p&gt;Her lawyer confirmed Wynn-Williams “remains silenced about the very matters Congress is investigating, despite clear and unanimous voices from Congress calling on Meta to end their arbitration proceedings which threaten to bankrupt her”.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45322050</guid><pubDate>Sun, 21 Sep 2025 12:15:11 +0000</pubDate></item><item><title>That DEA agent's 'credit card' could be eavesdropping on you</title><link>https://www.independent.co.uk/news/world/americas/dea-surveillance-hidden-cameras-federal-law-enforcement-b2828606.html</link><description>&lt;doc fingerprint="2770825d78b172cb"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;That DEA agent’s ‘credit card’ could be secretly eavesdropping on you&lt;/head&gt;
    &lt;p&gt;Exclusive: The Drug Enforcement Administration’s latest piece of covert surveillance gadgetry joins previous hidden cameras and listening devices in everything from streetlights to traffic cones to vacuum cleaners&lt;/p&gt;
    &lt;p&gt;Over the past several years, federal law enforcement agencies have gotten increasingly creative in their surveillance techniques, hiding cameras in, among other things, streetlights, traffic cones, toolboxes and vacuum cleaners.&lt;/p&gt;
    &lt;p&gt;The Drug Enforcement Administration has been especially forward-thinking in its placement of high-tech but unseen monitoring devices. In 2018, the DEA quietly installed automatic license plate readers in an unknown number of the ubiquitous radar speed signs that show approaching drivers how fast they’re going. It has only grown in the years since.&lt;/p&gt;
    &lt;p&gt;Now, according to federal procurement data reviewed by The Independent, the DEA – which has recently diverted agents from their usual drug-fighting mandate to assist Immigration and Customs Enforcement in carrying out President Donald Trump’s mass deportation efforts – is outfitting agents, presumably undercover, with audio-video recorders camouflaged to look like everyday credit cards.&lt;/p&gt;
    &lt;p&gt;“Covert audio/video devices must match the form factor of a United States credit card and other common form factors of that size and be able to accept a printed overlay that is detailed enough to be able to pass a close visual inspection,” states a summary of the September 12 purchase order. “The card must be able to be disguised by printing of specific art-work directly on to the card. Physical Dimensions can be no more than 85 x 54 x 1.5 mm, 34 x 2 x 0.05 inches Weight &amp;lt; 5 g.”&lt;/p&gt;
    &lt;p&gt;A so-called J&amp;amp;A among the tranche of paperwork attached to the purchase, or, a “Justification &amp;amp; Approval” from higher-ups to issue a contract to a specific seller without putting it out for competitive bids, says the Bond-esque gadgets will be coming from a Swiss company called Nagra.&lt;/p&gt;
    &lt;p&gt;“Tested another vendor's product that was similar but when the design was printed on the card, it was not a clear print and the ink was able to be rubbed off,” the J&amp;amp;A explains.&lt;/p&gt;
    &lt;p&gt;The agency is buying 57 credit card recorders in all, the documentation shows: 30 with model codes indicating both audio and video capabilities, and 27 with model codes indicating they are audio-only. Both come with 16GB of memory, a built-in mono microphone and a lithium battery pack, according to the DEA solicitation.&lt;/p&gt;
    &lt;p&gt;Although the solicitation and J&amp;amp;A do not specify how the faux credit cards will be used, a high-profile arrest in 2022 by the DEA involved luring an alleged Yakuza leader to Manhattan, where he was handcuffed over a steak dinner at Morton’s.&lt;/p&gt;
    &lt;p&gt;In addition to listing requirements for USB cables and docking stations, the solicitation also lays out a need for “badge holder[s],” suggesting the possibility of some portion of the devices perhaps being disguised as ID cards.&lt;/p&gt;
    &lt;p&gt;It is unclear how much the DEA is spending on the devices, but the J&amp;amp;A was made public – a requirement for all purchases exceeding $25,000.&lt;/p&gt;
    &lt;p&gt;They will be shipped to the agency’s Office of Investigative Technology in Lorton, Virginia, according to the documentation.&lt;/p&gt;
    &lt;p&gt;Little more is known about the recorders; although Nagra manufactures high-end consumer grade electronics, it restricts details of its covert surveillance products to law enforcement agencies only. Another one of its products transmits live audio from a similar credit card-sized device.&lt;/p&gt;
    &lt;p&gt;Past DEA purchases provide a glimpse at some of the ways it uses covert technology. Several years ago, it paid a California company $42,595 for a “custom Shop Vac concealment with Canon M50B,” a “high-sensitivity…PTZ [Pan-Tilt-Zoom] network camera” that “captures video with remarkable color and clarity, even in very low-light environments.”&lt;/p&gt;
    &lt;p&gt;During the same period, the DEA paid $71,685 for a “Tool Box System Portable surveillance platform in tool box concealment black in color.” And it put up a little more than $20,000 to hide video cameras in streetlights, a job that went to a Houston, Texas company called Cowboy Streetlight Concealments, LLC.&lt;/p&gt;
    &lt;p&gt;In an interview at the time with Quartz, Christie Crawford, who owns Cowboy Streetlight Concealments with her police officer husband, said, “We do streetlight concealments and camera enclosures. Basically, there’s businesses out there that will build concealments for the government and that’s what we do. They specify what’s best for them, and we make it. And that’s about all I can probably say.”&lt;/p&gt;
    &lt;p&gt;For its part, the DEA – which in 2021 began requiring all agents to wear body cameras – nixed the program earlier this year after Donald Trump began his second term in office.&lt;/p&gt;
    &lt;p&gt;A DEA spokesperson did not respond on Wednesday to The Independent’s request for comment. A message sent to Nagra went unanswered.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45322101</guid><pubDate>Sun, 21 Sep 2025 12:24:31 +0000</pubDate></item><item><title>Sequoia: Rust OpenPGP Implementation</title><link>https://gitlab.com/sequoia-pgp/sequoia</link><description>&lt;doc fingerprint="59270d7e6d4d1842"&gt;
  &lt;main&gt;
    &lt;p&gt;Skip to content GitLab Menu Why GitLab Pricing Contact Sales Explore Why GitLab Pricing Contact Sales Explore Sign in Get free trial sequoia Loading&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45322328</guid><pubDate>Sun, 21 Sep 2025 13:00:40 +0000</pubDate></item><item><title>Disk Utility still can't check and repair APFS volumes and containers</title><link>https://eclecticlight.co/2021/11/19/disk-utility-still-cant-check-and-repair-apfs-volumes-and-containers/</link><description>&lt;doc fingerprint="a711dd1523b6157c"&gt;
  &lt;main&gt;
    &lt;p&gt;Checking and repairing disks is one of the more important tasks performed by Disk Utility, but ever since the introduction of APFS, it has been more fraught than it should have been. One of its most persistent and pervasive problems has been complete failure because Disk Utility has been unable to unmount volumes or containers. To my shock, in Monterey 12.0.1 this problem appears worse than ever, and I now have one disk which Disk Utility is completely unable to check or repair. This article suggests ways around this, while we wait for Apple to fix this bug.&lt;/p&gt;
    &lt;p&gt;For much of this period, the First Aid tool in Disk Utility has relied on the command tool &lt;code&gt;fsck_apfs&lt;/code&gt; to do the work, calling it using two options, &lt;code&gt;y&lt;/code&gt; and &lt;code&gt;x&lt;/code&gt;. The &lt;code&gt;y&lt;/code&gt; option simply agrees to make all the repairs suggested by the tool, but the &lt;code&gt;x&lt;/code&gt; option is private. I suspect that privacy isn’t sinister, merely allowing communication between the tool and app using XPC.&lt;/p&gt;
    &lt;p&gt;Best practice for performing disk checks and repairs like this isn’t with a live file system, and those options won’t work when the item being checked is still mounted. So to prepare for the call to &lt;code&gt;fsck_apfs&lt;/code&gt;, Disk Utility has to unmount the volume or container, and that’s the step which appears to go awry.&lt;/p&gt;
    &lt;p&gt;In Catalina and Big Sur, the error reported was confusing, and the recommendation to “back up the data on this volume” inappropriate. It would have been far better if Disk Utility had told us honestly that “this is a known bug, and some day we might get round to fixing it.”&lt;/p&gt;
    &lt;p&gt;That some day clearly hasn’t come in Monterey 12.0.1. When I was researching yesterday’s article about how to check Time Machine backup volumes, it hit me again and again, so I went back and had a closer look at what now goes wrong, and what to do about it.&lt;/p&gt;
    &lt;p&gt;This may happen persistently when you try to check and repair an APFS volume.&lt;/p&gt;
    &lt;p&gt;It can also happen every time with an APFS container.&lt;/p&gt;
    &lt;p&gt;Oddly, though, it doesn’t seem to affect HFS+ volumes.&lt;/p&gt;
    &lt;p&gt;One way around this is to cave in, start up in Recovery, and use Disk Utility there, where there’s no excuse for problems unmounting anything. If you’re intending to check and repair the boot volume group (System and/or Data) then this is the preferred way. macOS does now provide a good means of ‘freezing’ file system access if you do try that when running in normal user mode, but Recovery is always better.&lt;/p&gt;
    &lt;p&gt;The best news of all is that you can still use the command tool &lt;code&gt;fsck_apfs&lt;/code&gt; directly, and work around this bug in Disk Utility. The bizarre twist is that you can use Disk Utility’s Unmount tool to unmount volumes and containers which the app itself appears unable to unmount successfully. Here’s a summary of the process:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;In Disk Utility (or Terminal) obtain the device name of the APFS container or volume you want to check. In this case, I’ll use &lt;code&gt;disk7s2&lt;/code&gt;, which is the sort of volume name you’re looking for, or&lt;code&gt;disk7&lt;/code&gt;for a container.&lt;/item&gt;
      &lt;item&gt;In Disk Utility (or Terminal) unmount the container or volume, by selecting it and clicking on the Unmount tool. When you’re checking a container, it’s best to unmount each of its volumes before unmounting the container itself.&lt;/item&gt;
      &lt;item&gt;Open Terminal and type the chosen command with the correct device name. Then enter your admin user’s password at the prompt.&lt;/item&gt;
      &lt;item&gt;Watch as the volume or container is checked.&lt;/item&gt;
      &lt;item&gt;Once that has completed, consider whether the container or volume needs any repair using &lt;code&gt;fsck_apfs&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;In Disk Utility (or Terminal) mount the container or volume again, by selecting it and clicking on the Mount tool.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The command to use depends on whether you just want to check the item, or repair it as well. For the former, use the &lt;code&gt;-n&lt;/code&gt; option, and for the latter the &lt;code&gt;-y&lt;/code&gt; option will perform all repairs automatically. I advise you to include all snapshots, which are important, but if you want to exclude them all, use the &lt;code&gt;-S&lt;/code&gt; option.&lt;/p&gt;
    &lt;p&gt;If the volume is encrypted, you could keep it mounted and use the &lt;code&gt;-l&lt;/code&gt; option to check the live file system, or you can use a command like&lt;code&gt;diskutil apfs unlockVolume /dev/disk7s2 -nomount&lt;/code&gt;&lt;lb/&gt; to unlock the volume without mounting it (thanks to kapitainsky for suggesting that).&lt;/p&gt;
    &lt;p&gt;For example, choose between the commands:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;sudo fsck_apfs -n /dev/disk7s2&lt;/code&gt;just to check the volume disk7s2 but not repair it, and include snapshots.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;sudo fsck_apfs -y /dev/disk7s2&lt;/code&gt;to check and repair all errors automatically, and include snapshots.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;sudo fsck_apfs -n -S /dev/disk7s2&lt;/code&gt;to check but not repair, but excluding all snapshots.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;sudo fsck_apfs -n -S /dev/disk7&lt;/code&gt;to check but not repair the container disk7, excluding all snapshots.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You can find details of all available options in &lt;code&gt;man fsck_apfs&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Apple recommends that you first check and repair volumes within a container, then the container itself, and finally the disk (which you can do completely within Disk Utility). That is oddly the exact opposite order previously recommended by many, and duplicates checks on volumes which are normally repeated when you check their container.&lt;/p&gt;
    &lt;p&gt;The disk which I have such problems with is a little unusual in that it’s partitioned into two: a small HFS+ volume, and a much larger APFS container. The irony is that Disk Utility’s advice to back up the affect volume is being offered for my Time Machine backup, which is not only a backup volume itself, but can’t be backed up because its backup snapshots can’t be copied to another disk.&lt;/p&gt;
    &lt;p&gt;It will be so good when Apple finally sorts these problems we’ve suffered for the last four years.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45322623</guid><pubDate>Sun, 21 Sep 2025 13:37:13 +0000</pubDate></item><item><title>UUIDv7 in Postgres 18. With time extraction</title><link>https://www.thenile.dev/blog/uuidv7</link><description>&lt;doc fingerprint="3d1d03952c8f26ac"&gt;
  &lt;main&gt;
    &lt;p&gt;PostgreSQL 18 is on the horizon, with beta testing now underway. Among the many improvements in this release is support for UUIDv7. A timestamp-based UUID variant that plays nicely with btree indexes. In this post, we'll discuss UUIDs in general, why UUIDv7 is so useful and how you'll want to use it in Postgres.&lt;/p&gt;
    &lt;head rend="h2"&gt;PostgreSQL 18&lt;/head&gt;
    &lt;p&gt;PostgreSQL 18 beta 1 was released few days ago. The release is packed with new features, improvements and bug fixes. As usual, the community is encouraged to try it out and report issues, with the goal of shipping a high quality release in September.&lt;/p&gt;
    &lt;p&gt;The highlights of the release include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Async I/O (with io_uring) — 2-3x speedups on seq scans, vacuums&lt;/item&gt;
      &lt;item&gt;Skip scan on multi-column btree indexes + smarter OR/IN optimizations&lt;/item&gt;
      &lt;item&gt;Keep planner stats during major upgrades&lt;/item&gt;
      &lt;item&gt;UUIDv7 functions&lt;/item&gt;
      &lt;item&gt;Virtual generated columns&lt;/item&gt;
      &lt;item&gt;OAuth login + md5 deprecation warning&lt;/item&gt;
      &lt;item&gt;EXPLAIN ANALYZE now shows I/O, CPU, WAL&lt;/item&gt;
      &lt;item&gt;Temporal constraints, LIKE on nondeterministic collation, casefolding&lt;/item&gt;
      &lt;item&gt;New wire protocol version: 3.2 (first since 2003!)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;While &lt;code&gt;uuidv7()&lt;/code&gt; is not the most exciting feature (that would be async I/O), it's probably the most awaited one. It was close to being added in 17, and many users have been a bit disappointed that it didn't make the cut. I'm so excited about it, that I decided to take the beta for a spin and write a blog post about it.&lt;/p&gt;
    &lt;head rend="h2"&gt;What is a UUID and why are they useful?&lt;/head&gt;
    &lt;p&gt;UUIDs are 128-bit values used as identifiers for various items - anything from transactions to companies. They are designed to be unique across space and time and can be generated efficiently at high rates without depending on centralized services.&lt;/p&gt;
    &lt;p&gt;Traditionally, relational databases used auto-incrementing types (like &lt;code&gt;SERIAL&lt;/code&gt; or &lt;code&gt;identity&lt;/code&gt;) to generate unique identifiers. This can be done efficiently on a single machine (although there are drawbacks even in this case), but once you need to scale out, you need a way to generate identifiers that are unique across all nodes. Instagram team wrote a short blog about their migration to UUIDs as they sharded their Postgres database.&lt;/p&gt;
    &lt;p&gt;UUIDs are useful as primary keys in databases in several common scenarios:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Generating unique IDs in a distributed database:&lt;lb/&gt;While many distributed databases can support auto-increment (identity) columns, they have limitations and performance issues.&lt;/item&gt;
      &lt;item&gt;Unguessable public identifiers:&lt;lb/&gt;Properly generated, UUIDs can't be guessed, predicted or used to infer information about the system. If you use auto-increment as a customer identifier, for instance, attackers can scan all existing identifiers and attempt to use them, they can guess the next identifier and estimate how many customers you have.&lt;/item&gt;
      &lt;item&gt;Allowing clients to generate identifiers:&lt;lb/&gt;Using UUIDs allows clients to generate identifiers that they can use without coordinating with the server. This is useful in mobile apps and serverless environments where you want to minimize communication to the server.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As a result of these benefits, UUIDs are used as primary keys in many databases. However, there are also 3 concerns with the use of UUIDs in databases:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Sorting: UUIDs are not meaningfully sortable by value.&lt;/item&gt;
      &lt;item&gt;Index locality: New UUIDs are not close to each other in the index, this means that inserts will be performed at random locations. This can cause index bloat and other performance issues, as you can see in the charts in this blog post.&lt;/item&gt;
      &lt;item&gt;Size: UUIDs are 128-bit values. Most developers default to using &lt;code&gt;INT&lt;/code&gt;(32-bit) or&lt;code&gt;BIGINT&lt;/code&gt;(64-bit) for their primary keys. For tables with large number of very small records, this can be meaningful overhead.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As I'll explain in the next section, UUIDv7 addresses 2 out of these 3 concerns.&lt;/p&gt;
    &lt;p&gt;The size of the UUID may be a problem when disk space or network bandwidth is limited, but it is worth noting that modern CPUs can compare 128-bit values in a single instruction (&lt;code&gt;CMEQ&lt;/code&gt;, part of SIMD instructions), so database operations on UUIDs are highly optimized. The key here is to make sure you use binary representation of UUIDs (proper UUID type) in both the database and the application, and not the string representation.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why UUIDv7?&lt;/head&gt;
    &lt;p&gt;UUIDs were first standardized in RFC 4122 in 2005. This RFC defines 5 variants of UUIDs, of which variant 1 and 4 are the most common. The specification was later revised to add variants 6-8 in RFC 9562 which was published in May 2024 (although the first public working draft was published in 2020). Happy Birthday RFC 9562 and UUIDv7!&lt;/p&gt;
    &lt;p&gt;To motivate the specification update, RFC 9562 discusses the common use case of using UUIDs as primary keys in databases:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;One area in which UUIDs have gained popularity is database keys ... but UUID versions 1-5, which were originally defined by [RFC4122], lack certain other desirable characteristics, such as:&lt;/p&gt;
      &lt;p&gt;UUID versions that are not time ordered, such as UUIDv4 (described in Section 5.4), have poor database-index locality. This means that new values created in succession are not close to each other in the index; thus, they require inserts to be performed at random locations. The resulting negative performance effects on the common structures used for this (B-tree and its variants) can be dramatic.&lt;/p&gt;
      &lt;p&gt;many widely distributed database applications and large application vendors have sought to solve the problem of creating a better time-based, sortable unique identifier for use as a database key. This has led to numerous implementations over the past 10+ years solving the same problem in slightly different ways.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The RFC proceeds to specify 16 (!) different implementations of non-standard UUIDs, each with their own trade-offs. This includes the popular &lt;code&gt;ULID&lt;/code&gt;, Twitter's &lt;code&gt;Snowflake&lt;/code&gt;, Instagram's &lt;code&gt;ShardId&lt;/code&gt; and many more.
All of these implementations were evaluated when designing the new specification.&lt;/p&gt;
    &lt;p&gt;While the new RFC specifies 3 new variants of UUIDs, the only interesting one is UUIDv7. UUIDv6 is introduced for backwards compatibility only - the RFC says "Systems that do not involve legacy UUIDv1 SHOULD use UUIDv7 instead". UUIDv8 provides a format for experimental and vendor-specific extensions.&lt;/p&gt;
    &lt;p&gt;UUIDv7 solves both the sorting and index locality concerns. It uses Unix Epoch timestamp as the most significant 48 bits, keeping the other 74 bits for random values (additional bits are used for version and variant). This makes UUIDs sortable by time sequence and unique. The standard also provides the option to include millisecond timestamp in the UUID and/or carefully seeded counter, to support ordering within a single second (if needed). As a result, UUIDv7 is a great fit for use as a primary key in databases - it is guaranteed to be unique, sortable and have good index locality.&lt;/p&gt;
    &lt;head rend="h2"&gt;UUIDv7 in PostgreSQL 18&lt;/head&gt;
    &lt;p&gt;Until PostgreSQL 18, UUIDv7 was not natively supported. The built-in &lt;code&gt;gen_random_uuid()&lt;/code&gt; function generated UUIDv4, and while the popular &lt;code&gt;uuid-ossp&lt;/code&gt; extension added
support for additional UUID variants, it was limited to the variants specified in RFC 4122.&lt;/p&gt;
    &lt;p&gt;PostgreSQL 18 adds a new function: &lt;code&gt;uuidv7()&lt;/code&gt;, which generates UUIDv7 values. The Postgres implementation includes a 12-bit sub-millisecond timestamp fraction immediately after the timestamp (as allowed but not required by the standard). This guarantees monotonicity for all UUIDv7 values generated by the same Postgres session (same backend process).&lt;/p&gt;
    &lt;p&gt;For consistency, PostgreSQL 18 added &lt;code&gt;uuidv4()&lt;/code&gt; as an alias for &lt;code&gt;gen_random_uuid()&lt;/code&gt;, to match the naming.&lt;/p&gt;
    &lt;p&gt;Calling &lt;code&gt;uuidv7()&lt;/code&gt; will generate a new UUIDv7 value where the timestamp is the current time. If you need to generate a UUIDv7 value for a different time, you can pass an optional &lt;code&gt;interval&lt;/code&gt; to the function.&lt;/p&gt;
    &lt;p&gt;Postgres' existing functions for extracting timestamp and version from a UUID are also updated to support UUIDv7. Here is an example of how to use the new functions:&lt;/p&gt;
    &lt;code&gt;postgres=# select uuidv7();
                uuidv7
--------------------------------------
 0196ea4a-6f32-7fd0-a9d9-9c815a0750cd
(1 row)

postgres=# select uuidv7(INTERVAL '1 day');
                uuidv7
--------------------------------------
 0196ef74-8d09-77b0-a84b-5301262f05ad
(1 row)

postgres=# SELECT uuid_extract_version(uuidv4());
 uuid_extract_version
----------------------
                    4
(1 row)

postgres=# SELECT uuid_extract_version(uuidv7());
 uuid_extract_version
----------------------
                    7
(1 row)

postgres=# SELECT uuid_extract_timestamp(uuidv7());
   uuid_extract_timestamp
----------------------------
 2025-05-19 20:50:40.381+00
(1 row)

postgres=# SELECT uuid_extract_timestamp(uuidv7(INTERVAL '1 hour'));
   uuid_extract_timestamp
----------------------------
 2025-05-19 21:50:59.388+00
(1 row)

postgres=# SELECT uuid_extract_timestamp(uuidv7(INTERVAL '-1 day'));
   uuid_extract_timestamp
----------------------------
 2025-05-18 20:51:15.774+00
(1 row)
&lt;/code&gt;
    &lt;p&gt;Using &lt;code&gt;uuidv7()&lt;/code&gt; as the primary key in a table is straightforward, and together with the ability to extract the timestamp, it makes it easy to use the UUID as a sortable key and even inspect the creation time of the record:&lt;/p&gt;
    &lt;code&gt;CREATE TABLE test (
    id uuid DEFAULT uuidv7() PRIMARY KEY,
    name text
);

INSERT INTO test (name) VALUES ('foo');
INSERT INTO test (name) VALUES ('bar');
-- this will be sorted to the beginning of the list since we are making it 1h older than the other two
INSERT INTO test (id, name) VALUES (uuidv7(INTERVAL '-1 hour'), 'oldest');

SELECT uuid_extract_timestamp(id), name FROM test ORDER BY id;

   uuid_extract_timestamp   |  name
----------------------------+--------
 2025-05-19 19:55:43.87+00  | oldest
 2025-05-19 20:55:01.304+00 | foo
 2025-05-19 20:55:01.305+00 | bar
(3 rows)
&lt;/code&gt;
    &lt;p&gt;All these functions are documented in the PostgreSQL documentation and if you are interested in the implementation details, you can review the patch.&lt;/p&gt;
    &lt;head rend="h2"&gt;Try it out!&lt;/head&gt;
    &lt;p&gt;Once PostgreSQL 18 is released, you will be able to use &lt;code&gt;uuidv7()&lt;/code&gt; and all the other new functionality by installing it as you normally do.
While the official release is planned for September, &lt;code&gt;Beta 1&lt;/code&gt; version is already available and the community encourages users to try it out and report issues.&lt;/p&gt;
    &lt;p&gt;The installations instructions for the beta versions and nightly snapshots are available here.&lt;/p&gt;
    &lt;head rend="h2"&gt;Final Thoughts&lt;/head&gt;
    &lt;p&gt;PostgreSQL 18 delivers practical improvements that experienced developers will really appreciate. Native support for UUIDv7 is a quiet but impactful addition that addresses long-standing pain points in database design.&lt;/p&gt;
    &lt;p&gt;UUIDs have always been a tradeoff: secure, guaranteed to be unique, efficient to generate in distributed systems. but with performance drawbacks for use with B-tree indexes. UUIDv7 brings the best of both worlds — globally unique, yet ordered in a way that plays nicely with B-tree indexes and write-heavy workloads. Postgres 18 makes them that much more convenient to use.&lt;/p&gt;
    &lt;p&gt;If you've ever hesitated to use UUIDs for primary keys, this is your chance to revisit that decision. Try the beta, test it in your schema, and see how it behaves. Whether you're building multi-tenant apps or just want more stable ID generation, UUIDv7 is worth a look.&lt;/p&gt;
    &lt;p&gt;The best way to shape the future of Postgres is to get involved early — so go ahead, spin up a test instance and let the community know what you find.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45323008</guid><pubDate>Sun, 21 Sep 2025 14:24:09 +0000</pubDate></item><item><title>New thermoelectric cooling breakthrough nearly doubles efficiency</title><link>https://www.sciencedaily.com/releases/2025/09/250919085242.htm</link><description>&lt;doc fingerprint="31bed7467a83e1fc"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;New cooling breakthrough nearly doubles efficiency&lt;/head&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Date:&lt;/item&gt;
      &lt;item rend="dd-1"&gt;September 20, 2025&lt;/item&gt;
      &lt;item rend="dt-2"&gt;Source:&lt;/item&gt;
      &lt;item rend="dd-2"&gt;Johns Hopkins University Applied Physics Laboratory&lt;/item&gt;
      &lt;item rend="dt-3"&gt;Summary:&lt;/item&gt;
      &lt;item rend="dd-3"&gt;CHESS thin-film materials nearly double refrigeration efficiency compared to traditional methods. Scalable and versatile, they promise applications from household cooling to space exploration.&lt;/item&gt;
      &lt;item rend="dt-4"&gt;Share:&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Researchers at the Johns Hopkins Applied Physics Laboratory (APL) in Laurel, Maryland, have developed a new, easily manufacturable solid-state thermoelectric refrigeration technology with nano-engineered materials that is twice as efficient as devices made with commercially available bulk thermoelectric materials. As global demand grows for more energy-efficient, reliable and compact cooling solutions, this advancement offers a scalable alternative to traditional compressor-based refrigeration.&lt;/p&gt;
    &lt;p&gt;In a paper published in Nature Communications, a team of researchers from APL and refrigeration engineers from Samsung Research demonstrated improved heat-pumping efficiency and capacity in refrigeration systems attributable to high-performance nano-engineered thermoelectric materials invented at APL known as controlled hierarchically engineered superlattice structures (CHESS).&lt;/p&gt;
    &lt;p&gt;The CHESS technology is the result of 10 years of APL research in advanced nano-engineered thermoelectric materials and applications development. Initially developed for national security applications, the material has also been used for noninvasive cooling therapies for prosthetics and won an R&amp;amp;D 100 award in 2023.&lt;/p&gt;
    &lt;p&gt;"This real-world demonstration of refrigeration using new thermoelectric materials showcases the capabilities of nano-engineered CHESS thin films," said Rama Venkatasubramanian, principal investigator of the joint project and chief technologist for thermoelectrics at APL. "It marks a significant leap in cooling technology and sets the stage for translating advances in thermoelectric materials into practical, large-scale, energy-efficient refrigeration applications."&lt;/p&gt;
    &lt;p&gt;A New Benchmark for Solid-State Cooling&lt;/p&gt;
    &lt;p&gt;The push for more efficient and compact cooling technologies is fueled by a variety of factors, including population growth, urbanization and an increasing reliance on advanced electronics and data infrastructure. Conventional cooling systems, while effective, are often bulky, energy intensive and reliant on chemical refrigerants that can be harmful to the environment.&lt;/p&gt;
    &lt;p&gt;Thermoelectric refrigeration is widely regarded as a potential solution. This method cools by using electrons to move heat through specialized semiconductor materials, eliminating the need for moving parts or harmful chemicals, making these next-generation refrigerators quiet, compact, reliable and sustainable. Bulk thermoelectric materials are used in small devices like mini-fridges, but their limited efficiency, low heat-pumping capacity and incompatibility with scalable semiconductor chip fabrication have historically prevented their wider use in high-performance systems.&lt;/p&gt;
    &lt;p&gt;In the study, researchers compared refrigeration modules using traditional bulk thermoelectric materials with those using CHESS thin-film materials in standardized refrigeration tests, measuring and comparing the electrical power needed to achieve various cooling levels in the same commercial refrigerator test systems. Samsung Research's Life Solution Team, led by executive vice president Joonhyun Lee, collaborated with APL to validate the results through detailed thermal modeling, quantifying heat loads and thermal resistance parameters to ensure accurate performance evaluation under real-world conditions.&lt;/p&gt;
    &lt;p&gt;The results were striking: Using CHESS materials, the APL team achieved nearly 100% improvement in efficiency over traditional thermoelectric materials at room temperature (around 80 degrees Fahrenheit, or 25 C). They then translated these material-level gains into a near 75% improvement in efficiency at the device level in thermoelectric modules built with CHESS materials and a 70% improvement in efficiency in a fully integrated refrigeration system, each representing a significant improvement over state-of-the-art bulk thermoelectric devices. These tests were completed under conditions that involved significant amounts of heat pumping to replicate practical operation.&lt;/p&gt;
    &lt;p&gt;Built to Scale&lt;/p&gt;
    &lt;p&gt;Beyond improving efficiency, the CHESS thin-film technology uses remarkably less material -- just 0.003 cubic centimeters, or about the size of a grain of sand, per refrigeration unit. This reduction in material means APL's thermoelectric materials could be mass-produced using semiconductor chip production tools, driving cost efficiency and enabling widespread market adoption.&lt;/p&gt;
    &lt;p&gt;"This thin-film technology has the potential to grow from powering small-scale refrigeration systems to supporting large building HVAC applications, similar to the way that lithium-ion batteries have been scaled to power devices as small as mobile phones and as large as electric vehicles," Venkatasubramanian said.&lt;/p&gt;
    &lt;p&gt;Additionally, the CHESS materials were created using a well-established process commonly used to manufacture high-efficiency solar cells that power satellites and commercial LED lights.&lt;/p&gt;
    &lt;p&gt;"We used metal-organic chemical vapor deposition (MOCVD) to produce the CHESS materials, a method well known for its scalability, cost-effectiveness and ability to support large-volume manufacturing," said Jon Pierce, a senior research engineer who leads the MOCVD growth capability at APL. "MOCVD is already widely used commercially, making it ideal for scaling up CHESS thin-film thermoelectric materials production."&lt;/p&gt;
    &lt;p&gt;These materials and devices continue to show promise for a broad range of energy harvesting and electronics applications in addition to the recent advances in refrigeration. APL plans to continue to partner with organizations to refine the CHESS thermoelectric materials with a focus on boosting efficiency to approach that of conventional mechanical systems. Future efforts include demonstrating larger-scale refrigeration systems, including freezers, and integrating artificial intelligence-driven methods to optimize energy efficiency in compartmentalized or distributed cooling in refrigeration and HVAC equipment.&lt;/p&gt;
    &lt;p&gt;"Beyond refrigeration, CHESS materials are also able to convert temperature differences, like body heat, into usable power," said Jeff Maranchi, Exploration Program Area manager in APL's Research and Exploratory Development Mission Area. "In addition to advancing next-generation tactile systems, prosthetics and human-machine interfaces, this opens the door to scalable energy-harvesting technologies for applications ranging from computers to spacecraft -- capabilities that weren't feasible with older bulkier thermoelectric devices."&lt;/p&gt;
    &lt;p&gt;"The success of this collaborative effort demonstrates that high-efficiency solid-state refrigeration is not only scientifically viable but manufacturable at scale," said Susan Ehrlich, an APL technology commercialization manager. "We're looking forward to continued research and technology transfer opportunities with companies as we work toward translating these innovations into practical, real-world applications."&lt;/p&gt;
    &lt;p&gt;Story Source:&lt;/p&gt;
    &lt;p&gt;Materials provided by Johns Hopkins University Applied Physics Laboratory. Note: Content may be edited for style and length.&lt;/p&gt;
    &lt;p&gt;Journal Reference:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Jake Ballard, Matthew Hubbard, Sung-Jin Jung, Vanessa Rojas, Richard Ung, Junwoo Suh, MinSoo Kim, Joonhyun Lee, Jonathan M. Pierce, Rama Venkatasubramanian. Nano-engineered thin-film thermoelectric materials enable practical solid-state refrigeration. Nature Communications, 2025; 16 (1) DOI: 10.1038/s41467-025-59698-y&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Cite This Page:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45323187</guid><pubDate>Sun, 21 Sep 2025 14:43:21 +0000</pubDate></item><item><title>How to Stop Functional Programming</title><link>https://brianmckenna.org/blog/howtostopfp</link><description>&lt;doc fingerprint="7ad3b18718b551b1"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;How to stop functional programming&lt;/head&gt;
    &lt;p&gt;The following has never happened to me but I often hear stories.&lt;/p&gt;
    &lt;p&gt;You go into work and discover that a coworker isn't happy with some code you wrote because they don't understand it. They go to your manager and tell them that you're being a problem by writing code they don't understand. Your manager, being very skilled in conflict resolution, makes a technical decision to avoid whatever tool you used which caused the problem. In your case it was functional programming.&lt;/p&gt;
    &lt;p&gt;That's it. You've been told. No more functional programming.&lt;/p&gt;
    &lt;p&gt;The manager has figured out what's good for the business and you figure that listening is what's good for your job.&lt;/p&gt;
    &lt;p&gt;You get back to your desk and take a ticket from JIRA. You've got to add a page listing a person's coworkers to your internal employee directory. First you write a function you need.&lt;/p&gt;
    &lt;code&gt;def userCoworkers(u: User): List[Employee] =
  u.departments.flatMap(_.employees)&lt;/code&gt;
    &lt;p&gt;But it's pure. You're doing functional programming! Stop&lt;/p&gt;
    &lt;code&gt;def userCoworkers(u: User): List[Employee] = {
  val coworkers = ListBuffer[Employee]()
  for { d &amp;lt;- departments }
    coworkers ++ d.employees
  coworkers.toList
}&lt;/code&gt;
    &lt;p&gt;Well, there's a side-effect involved, but the whole method is pure. You're still doing functional programming!&lt;/p&gt;
    &lt;code&gt;def userCoworkers(u: User): List[Employee] = {
  logger.info("Collecting coworkers")
  val coworkers = ListBuffer[Employee]()
  for { d &amp;lt;- departments }
    coworkers ++ d.employees
  coworkers.toList
}&lt;/code&gt;
    &lt;p&gt;Now the method has 1 external side-effect. Is that enough? With "no functional programming" you've been given a lower-bound of 1 side-effect per method but we don't really know what the ideal number is. Hopefully you can slip it through code review.&lt;/p&gt;
    &lt;p&gt;After this exercise you've learned how easy it is to not do functional programming.&lt;/p&gt;
    &lt;p&gt;You show it to your product manager. They didn't realise how many coworkers the average person had. The page is huge. They ask you to just change it to a number.&lt;/p&gt;
    &lt;p&gt;You're going to have to add numbers together. Without being pure. You're going to have to think about this one.&lt;/p&gt;
    &lt;p&gt;Maybe you should ask your manager how to do it. Good luck.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45323297</guid><pubDate>Sun, 21 Sep 2025 14:55:20 +0000</pubDate></item></channel></rss>