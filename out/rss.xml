<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Mon, 08 Dec 2025 19:09:04 +0000</lastBuildDate><item><title>Twelve Days of Shell</title><link>https://12days.cmdchallenge.com</link><description>&lt;doc fingerprint="ceb061ce57b8258"&gt;
  &lt;main&gt;
    &lt;p&gt;It looks like you don't have javascript enabled which is required for cmdchallenge. Create a reaction survey in your browser! View Solutions Learn&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46190577</guid><pubDate>Mon, 08 Dec 2025 10:13:07 +0000</pubDate></item><item><title>Flow: Actor-based language for C++, used by FoundationDB</title><link>https://github.com/apple/foundationdb/tree/main/flow</link><description>&lt;doc fingerprint="3b74d0de090e4684"&gt;
  &lt;main&gt;
    &lt;p&gt;We read every piece of feedback, and take your input very seriously.&lt;/p&gt;
    &lt;p&gt;To see all available qualifiers, see our documentation.&lt;/p&gt;
    &lt;p&gt;There was an error while loading. Please reload this page.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46191763</guid><pubDate>Mon, 08 Dec 2025 13:08:38 +0000</pubDate></item><item><title>Colors of Growth</title><link>https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5804462</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46191814</guid><pubDate>Mon, 08 Dec 2025 13:13:12 +0000</pubDate></item><item><title>IBM to Acquire Confluent</title><link>https://www.confluent.io/blog/ibm-to-acquire-confluent/</link><description>&lt;doc fingerprint="f1828a5ffb71a56d"&gt;
  &lt;main&gt;
    &lt;p&gt;Hands-on Workshop: Implementing Stream Processing with Apache Flink® | Register Now&lt;/p&gt;
    &lt;p&gt;We are excited to announce that Confluent has entered into a definitive agreement to be acquired by IBM. After the transaction is closed (subject to customary closing conditions and regulatory approvals), together, IBM and Confluent will aim to provide a platform that unifies the world’s largest enterprises, unlocking data for cloud/microservices, accelerating time-to-value, and building the real-time data foundation required to scale AI across every organization.&lt;/p&gt;
    &lt;p&gt;The below email was shared earlier today from Jay Kreps, CEO and Co-Founder of Confluent to our Confluent team.&lt;/p&gt;
    &lt;p&gt;We want to thank our team members for their continued hard work and dedication that defined a new category of data streaming and paved the way for this next chapter. We look forward to their ongoing contributions as a part of IBM after the transaction closes. For more information, please see the announcement press release (click to view).&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Confluent Team,&lt;/p&gt;
      &lt;p&gt;I’m excited to share that a few moments ago, we announced that Confluent has signed an agreement to be acquired by IBM in an all cash deal for $31.00 per share. Confluent will continue to operate as a distinct brand and business within IBM post-close.&lt;/p&gt;
      &lt;p&gt;Later this morning, December 8, at 9 a.m. PT and again in the evening at 7 p.m. PT, we will have a Company All Hands meeting, where I will share details about this announcement and answer your questions, alongside the rest of the executive team. In the meantime, we have a FAQ in the wiki with some more details. I know this may be surprising so I wanted to take the time to walk you through why this is the best path for Confluent.&lt;/p&gt;
      &lt;p&gt;In the letter I wrote at our IPO in 2021, I said that “There is a saying that a fox knows many things, but a hedgehog knows one big thing--Confluent is a company that knows a very big thing” and that rings true for me today.&lt;/p&gt;
      &lt;p&gt;Data is at the heart of what companies need to do to harness AI, modernize their operations, and build the next generation of applications; and Confluent is at the heart of what companies need to harness their data. This has been our goal in the team we’ve built, the products we’ve shipped, and the customer relationships we’ve cultivated. That conviction has only grown stronger.&lt;/p&gt;
      &lt;p&gt;IBM sees the same future we do: one in which enterprises run on continuous, event-driven intelligence, with data moving freely and reliably across every part of the business. They see that this connective layer will define how companies operate for decades to come, they understand open source and its power, and they work with some of the largest hybrid enterprises in the world. By joining forces, we can bring this architecture to far more organizations, accelerating the shift toward real-time and AI-powered operations globally.&lt;/p&gt;
      &lt;p&gt;IBM also has a long history of supporting open source and has demonstrated real leadership in this area with their prior acquisitions of Red Hat and HashiCorp. Our shared values of technical leadership, customer trust, and the belief that data is foundational to the next generation of AI is a big part of why I'm excited.&lt;/p&gt;
      &lt;p&gt;Becoming part of IBM won’t change Confluent’s mission; it will amplify it. The idea that sparked Kafka, grew into Confluent, and shaped an entire new category of data infrastructure now enters a phase where it can scale even more broadly and meaningfully.&lt;/p&gt;
      &lt;p&gt;Serving as CEO and leading this team over the past eleven years has been and continues to be the great privilege of my career. I’m profoundly proud of what we’ve built—the products, the technology, and equally important, the culture that has defined Confluent from the very beginning. Your passion, your talent, and your unwavering commitment have been a constant source of energy and inspiration. There isn’t a group I’d be happier to be in the trenches with, and I could not be more excited about this next chapter.&lt;/p&gt;
      &lt;p&gt;We’re still pretty early in this process, so there are many details that still need to be figured out. Until the deal officially closes (subject to customary closing conditions and regulatory approvals, which we expect by the middle of 2026), Confluent will continue to operate as a separate, independent company, and our priorities remain the same. I’m committed to being as transparent as possible throughout the coming months to keep you informed and up-to-date on timelines and integration plans. For now, your role, manager, pay, benefits, and our policies stay the same, and we still need to deliver on our Q4 and future commitments to our customers, partners, and team.&lt;/p&gt;
      &lt;p&gt;Now, more than ever, we’re here to set data in motion.&lt;/p&gt;
      &lt;p&gt;Thank you,&lt;/p&gt;
      &lt;p&gt;Jay&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Additional Information and Where to Find It&lt;/p&gt;
    &lt;p&gt;This communication may be deemed to be solicitation material in respect of the proposed acquisition of Confluent, Inc. (the “Company”) by International Business Machines Corporation (“Parent”) pursuant to the Agreement and Plan of Merger, dated as of December 7, 2025, by and among the Company, Parent and Corvo Merger Sub, Inc. The Company intends to file a preliminary and definitive proxy statement with the U.S. Securities and Exchange Commission (the “SEC”) with respect to a special meeting of stockholders to be held in connection with the proposed acquisition. After filing the definitive proxy statement (the “Proxy Statement”) with the SEC, the Company will mail the Proxy Statement and a proxy card to each stockholder of the Company entitled to vote at the special meeting. The Proxy Statement will contain important information about the proposed transaction and related matters. BEFORE MAKING ANY VOTING OR INVESTMENT DECISION, THE COMPANY’S STOCKHOLDERS AND INVESTORS ARE URGED TO READ THE PROXY STATEMENT (INCLUDING ANY AMENDMENTS OR SUPPLEMENTS THERETO) IN ITS ENTIRETY WHEN IT BECOMES AVAILABLE AND ANY OTHER DOCUMENTS FILED BY THE COMPANY WITH THE SEC RELATING TO THE PROPOSED ACQUISITION OR INCORPORATED BY REFERENCE THEREIN BECAUSE THEY WILL CONTAIN IMPORTANT INFORMATION ABOUT THE PROPOSED ACQUISITION. Investors and stockholders of the Company may obtain a free copy of the preliminary and definitive versions of the proxy statement once filed, as well as other relevant filings containing information about the Company and the proposed acquisition, including materials that are incorporated by reference into the Proxy Statement, without charge, at the SEC’s website (https://www.sec.gov) or from the Company by going to the Company’s Investor Relations Page on its website ().&lt;/p&gt;
    &lt;p&gt;Participants in the Solicitation&lt;/p&gt;
    &lt;p&gt;The Company and its directors, and certain of its executive officers, consisting of Lara Caimi, Jonathan Chadwick, Alyssa Henry, Matthew Miller, Neha Narkhede, Greg Schott, Eric Vishria, Michelangelo Volpi, who are the non‑employee members of the Board of Directors of the Company (the “Board”), and Jay Kreps, Chief Executive Officer and Chairman of the Board, Rohan Sivaram, Chief Financial Officer, and Ryan Mac Ban, Chief Revenue Officer, may be deemed to be participants in the solicitation of proxies from the Company’s stockholders in connection with the proposed acquisition. Information regarding the Company’s directors and certain of its executive officers, including a description of their direct or indirect interests, by security holdings or otherwise, can be found under the captions “Security Ownership of Certain Beneficial Owners and Management,” “Executive Compensation,” and “Director Compensation” contained in the Company’s definitive proxy statement on Schedule 14A for the Company’s 2025 annual meeting of stockholders, which was filed with the SEC on April 23, 2025. To the extent holdings of the Company’s securities by its directors or executive officers have changed since the applicable “as of” date described in its 2025 proxy statement, such changes have been or will be reflected on Initial Statements of Beneficial Ownership on Form 3 or Statements of Beneficial Ownership on Form 4 filed with the SEC, including (i) the Form 4s filed by Ms. Narkhede on May 6, 2025, June 4, 2025, June 12, 2025, September 11, 2025, October 31, 2025, November 5, 2025 and December 3, 2025; (ii) the Form 4s filed by Mr. Sivaram on May 22, 2025, June 4, 2025, June 9, 2025, August 22, 2025, September 10, 2025, October 31, 2025, November 24, 2025 and December 3, 2025; (iii) the Form 4s filed by Mr. Kreps on May 19, 2025, May 22, 2025, June 9, 2025, August 18, 2025, August 22, 2025, September 8, 2025, November 17, 2025 and November 24, 2025; (iv) the Form 4 filed by Mr. Chadwick on April 4, 2025 and June 12, 2025; (v) the Form 3 filed by Mr. Ban on May 16, 2025 and the Form 4s filed by Mr. Ban on May 22, 2025, June 24, 2025, August 22, 2025, September 24, 2025 and November 24, 2025; (vi) the Form 4s filed by Mr. Vishria on May 21, 2025, June 9, 2025, June 12, 2025, September 2, 2025 and October 31, 2025; (vii) the Form 4 filed by Mr. Volpi on June 9, 2025; (viii) the Form 4 filed by Ms. Caimi on June 12, 2025; (ix) the Form 4 filed by Mr. Schott on June 12, 2025; and (x) the Form 4 filed by Ms. Henry on June 12, 2025. Additional information regarding the identity of potential participants, and their direct or indirect interests, by security holdings or otherwise, will be included in the definitive proxy statement relating to the proposed acquisition when it is filed with the SEC. These documents (when available) may be obtained free of charge from the SEC’s website at www.sec.gov and the Company’s website at .&lt;/p&gt;
    &lt;p&gt;Forward Looking Statements&lt;/p&gt;
    &lt;p&gt;This communication contains “forward-looking statements” within the meaning of the “safe harbor” provisions of the United States Private Securities Litigation Reform Act of 1995. All statements other than statements of historical fact are statements that could be deemed “forward-looking statements”, including all statements regarding the intent, belief or current expectation of the companies and members of their senior management teams. Words such as “may,” “will,” “could,” “would,” “should,” “expect,” “plan,” “anticipate,” “intend,” “believe,” “estimate,” “predict,” “project,” “potential,” “continue,” “target,” variations of such words, and similar expressions are intended to identify such forward-looking statements, although not all forward-looking statements contain these identifying words.&lt;/p&gt;
    &lt;p&gt;These forward-looking statements include, but are not limited to, statements regarding the benefits of and timeline for closing the Company’s proposed transaction with Parent. These statements are based on various assumptions, whether or not identified in this communication, and on the current expectations of the Company’s management and are not predictions of actual performance. These forward-looking statements are provided for illustrative purposes only and are not intended to serve as, and must not be relied on by any investor as, a guarantee, an assurance, a prediction or a definitive statement of fact or probability. Actual events and circumstances are difficult or impossible to predict and may differ from assumptions. Many actual events and circumstances are beyond the control of the Company. These forward-looking statements are subject to a number of risks and uncertainties, including the timing, receipt and terms and conditions of any required governmental and regulatory approvals of the proposed transaction that could delay the consummation of the proposed transaction or cause the parties to abandon the proposed transaction; the occurrence of any event, change or other circumstances that could give rise to the termination of the merger agreement entered into in connection with the proposed transaction; the possibility that the Company’s stockholders may not approve the proposed transaction; the risk that the parties to the merger agreement may not be able to satisfy the conditions to the proposed transaction in a timely manner or at all; risks related to disruption of management time from ongoing business operations due to the proposed transaction; the risk that any announcements relating to the proposed transaction could have adverse effects on the market price of the common stock of the Company; the risk of any unexpected costs or expenses resulting from the proposed transaction; the risk of any litigation relating to the proposed transaction; and the risk that the proposed transaction and its announcement could have an adverse effect on the ability of the Company to retain and hire key personnel and to maintain relationships with customers, vendors, partners, employees, stockholders and other business relationships and on its operating results and business generally.&lt;/p&gt;
    &lt;p&gt;Further information on factors that could cause actual results to differ materially from the results anticipated by the forward-looking statements is included in the Company’s Annual Report on Form 10‑K for the fiscal year ended December 31, 2024, Quarterly Reports on Form 10‑Q, Current Reports on Form 8‑K, the Proxy Statement and other filings made by the Company from time to time with the Securities and Exchange Commission. These filings, when available, are available on the investor relations section of the Company’s website () or on the SEC’s website (https://www.sec.gov). If any of these risks materialize or any of these assumptions prove incorrect, actual results could differ materially from the results implied by these forward-looking statements. There may be additional risks that the Company presently does not know of or that the Company currently believes are immaterial that could also cause actual results to differ from those contained in the forward-looking statements. The forward-looking statements included in this communication are made only as of the date hereof. The Company assumes no obligation and does not intend to update these forward-looking statements, except as required by law.&lt;/p&gt;
    &lt;p&gt;This blog post highlights how Forrester Research has named Confluent a leader in The Forrester Wave™: Streaming Data Platforms, Q4 2025, and explains the value and benefits of the Confluent Data Streaming Platform.&lt;/p&gt;
    &lt;p&gt;Confluent achieves FedRAMP Ready status for its Confluent Cloud for Government offering, marking an essential milestone in providing secure data streaming services to government agencies, and showing a commitment to rigorous security standards. This certification marks a key step towards full...&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46192130</guid><pubDate>Mon, 08 Dec 2025 13:43:59 +0000</pubDate></item><item><title>Strong earthquake hits northern Japan, tsunami warning issued</title><link>https://www3.nhk.or.jp/nhkworld/en/news/20251209_02/</link><description>&lt;doc fingerprint="63b8315eee71b4fd"&gt;
  &lt;main&gt;
    &lt;p&gt;A strong earthquake has struck northern Japan. The quake struck off the eastern coast of Aomori Prefecture at 11:15 p.m. on Monday. Its focus was 54 kilometers deep, and the magnitude is estimated at 7.5.&lt;/p&gt;
    &lt;head rend="h3"&gt;Strong tremors felt across the region&lt;/head&gt;
    &lt;p&gt;The Japan Meteorological Agency has downgraded the magnitude of the quake centered off the Pacific coast in Aomori Prefecture to 7.5 from 7.6.&lt;/p&gt;
    &lt;p&gt;The tremor struck at 11:15 p.m. on Monday. The depth has also been adjusted to 54 kilometers, from an initial estimate of 50 kilometers.&lt;/p&gt;
    &lt;p&gt;Tremors with an intensity of upper 6 on the Japanese intensity scale of 0 to 7 were observed in the city of Hachinohe in Aomori Prefecture.&lt;/p&gt;
    &lt;p&gt;A hotel employee in Hachinohe City said: It seems there are multiple injured people. Everyone appears to be conscious.&lt;/p&gt;
    &lt;p&gt;If you are in these areas, try to remain in a safe place and protect yourself. Use extreme caution if you must move. Damage around you could be heavy. Stay alert. More tremors are possible&lt;/p&gt;
    &lt;head rend="h3"&gt;Tsunami warnings downgraded to advisories&lt;/head&gt;
    &lt;p&gt;The Meteorological Agency issued a tsunami warning for the Pacific coastline in northern Japan, including Iwate Prefecture, and parts of Hokkaido and Aomori prefectures late Monday night. But the agency has downgraded tsunami warnings to advisories for coastal areas in the Hokkaido and Tohoku regions. Officials say people should still stay away from bodies of water.&lt;/p&gt;
    &lt;p&gt;The agency says: it is the first time the agency has issued a tsunami warning since July, when a powerful quake off Kamchatka, Russia, prompted it to issue one for Japan's Pacific coastal areas.&lt;/p&gt;
    &lt;p&gt;Tsunami advisories are in place for parts of Hokkaido and Aomori Prefecture, Iwate, Miyagi and Fukushima prefectures.&lt;/p&gt;
    &lt;p&gt;The agency is calling on people to stay away from the coastline, as well as the mouths of rivers.&lt;/p&gt;
    &lt;p&gt;According to authorities, long-period ground motions were recorded during the Monday earthquake.&lt;/p&gt;
    &lt;p&gt;Long-period ground motions are slow, large-amplitude seismic waves with frequencies of 2 seconds or longer that occur during a large earthquake. Such motions are known to have a significant impact on high-rise buildings.&lt;/p&gt;
    &lt;p&gt;Strong long-period motions, classified class-3, the second highest in the 4-level scale were observed in the village of Rokkasho in Aomori Prefecture. Such class-3 waves are strong enough to make it difficult for people in a high-rise building to stand up.&lt;/p&gt;
    &lt;head rend="h3"&gt;Residents ordered to evacuate&lt;/head&gt;
    &lt;p&gt;After tsunami warnings were issued, some municipalities in Hokkaido, and the Tohoku region issued evacuation orders to residents.&lt;/p&gt;
    &lt;head rend="h3"&gt;Traffic disrupted&lt;/head&gt;
    &lt;p&gt;East Japan Railway Company says that as of Tuesday, outbound trains on the Tohoku Shinkansen have been suspended between Fukushima and Shin-Aomori stations due to the earthquake. The company says three trains stopped in this section.&lt;/p&gt;
    &lt;p&gt;The company says that it is checking for any damage to railway tracks and that it remains unclear when services will resume.&lt;/p&gt;
    &lt;p&gt;The Morioka branch of East Japan Railway says that as of midnight on Tuesday, services on the Tohoku Main Line were suspended in Iwate Prefecture.&lt;/p&gt;
    &lt;p&gt;It says two trains made emergency stops. It remains unclear when services will resume. There are no reports of injuries.&lt;/p&gt;
    &lt;p&gt;As for Hokkaido, the operator of its busiest airport, New Chitose Airport near Sapporo, says that as of 11:40 p.m. on Monday, it was checking whether there are any abnormalities on two runways.&lt;/p&gt;
    &lt;p&gt;Highways have been affected. East Nippon Expressway Company says that as of 11:45 p.m. on Monday, traffic was completely stopped between the Shiraoi and Shinchitose Airport Interchanges and between the Tomakomai Higashi and Numanohata Nishi Interchanges.&lt;/p&gt;
    &lt;head rend="h3"&gt;Power Companies: No abnormalities at nuclear plants&lt;/head&gt;
    &lt;p&gt;Tokyo Electric Power Company says it has confirmed that there are no abnormalities at the Fukushima Daiichi and Daini nuclear plants.&lt;/p&gt;
    &lt;p&gt;The company says it halted the release of treated and diluted water from the Fukushima Daiichi nuclear power plant at 11:42 pm on Monday, as per predetermined procedures.&lt;/p&gt;
    &lt;p&gt;The facility suffered a triple meltdown during the March 2011 earthquake and tsunami. The water used to cool molten fuel has been mixing with rain and groundwater.&lt;/p&gt;
    &lt;p&gt;That has been treated to remove most radioactive substances, except tritium. It's then diluted, reducing levels of tritium to well below the World Health Organization's guidance for drinking water, before it is released into the ocean.&lt;/p&gt;
    &lt;p&gt;TEPCO also ordered some employees at the facility to evacuate. There have been no reports so far of injuries at the nuclear power plant.&lt;/p&gt;
    &lt;p&gt;Tohoku Electric Power Company says no abnormalities have been detected at the Higashidori nuclear power plant in Aomori Prefecture and the Onagawa plant in Miyagi Prefecture.&lt;/p&gt;
    &lt;p&gt;Hokkaido Electric Power Company says no problems have been found at the Tomari nuclear power plant in the prefecture.&lt;/p&gt;
    &lt;head rend="h3"&gt;Government bracing for damages&lt;/head&gt;
    &lt;p&gt;The Japanese government set up a task force at the crisis management center in the prime minister's office at 11:16 p.m. on Monday in response to the earthquake.&lt;/p&gt;
    &lt;p&gt;Prime Minister Takaichi Sanae entered the prime minister's office shortly after 11:50 p.m.&lt;/p&gt;
    &lt;p&gt;She instructed the government to immediately provide information on any tsunami and evacuation orders to the people in an appropriate manner, take thorough measures to prevent harm, such as evacuating residents, and get a grasp of the extent of damage as soon as possible.&lt;/p&gt;
    &lt;p&gt;Takaichi: The central government will work closely with local governments and make the utmost effort to carry out measures, such as emergency response, including rescue for the affected people.&lt;/p&gt;
    &lt;p&gt;Chief Cabinet Secretary Kihara Minoru held a news conference on Tuesday. Kihara said the government continues to assess the extent of the damage.&lt;/p&gt;
    &lt;p&gt;He added that the government is devoting all its efforts to disaster prevention measures, with rescue and relief efforts as its top priority, led by the police, fire departments, Self-Defense Forces, and Japan Coast Guard.&lt;/p&gt;
    &lt;head rend="h3"&gt;Expert view on the quake&lt;/head&gt;
    &lt;p&gt;Sakai Shinichi, professor at the Earthquake Research Institute of the University of Tokyo, says: If this was a shallow earthquake centered in the sea, there is a high possibility that a tsunami has already occurred. People should stay away from the coast. It is important to evacuate and to take measures to stay warm.&lt;/p&gt;
    &lt;p&gt;Sakai says: The epicenter may be north of the epicenter area of the 2011 Great East Japan Earthquake. This time, the earthquake is believed to have occurred at the plate boundary, so I think it was a slightly larger earthquake. The magnitude could be revised in the future.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46192846</guid><pubDate>Mon, 08 Dec 2025 14:50:48 +0000</pubDate></item><item><title>Nova Programming Language</title><link>https://nova-lang.net</link><description>&lt;doc fingerprint="a3f8f96f303ef394"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Welcome!&lt;/head&gt;
    &lt;code&gt;|- Welcome to Nova! -|
    ~ Nova is a lightweight language for... ~
        . sketching out ideas,
        . documents, notes and personal tools,
        . casual modeling and thinking,
        . computing without computers
&lt;/code&gt;
    &lt;p&gt;If you've ever wanted to make a computer come to life through programming, you probably know how complicated it can be. Intricate incantations, confusing instructions, and large, complicated tools can make approaching programming incredibly difficult.&lt;/p&gt;
    &lt;p&gt;To address this, we've built something we call Nova. It is a programming language, a note-taking system, a way of sketching, and a way of conversing with programmers and machines!&lt;/p&gt;
    &lt;p&gt;We invite you to investigate what we've discovered and try it for yourself!&lt;/p&gt;
    &lt;head rend="h1"&gt;I want to...&lt;/head&gt;
    &lt;head rend="h2"&gt;Learn To Write Nova&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;"Nova, mechanically." and other articles by yumaikas.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Find A Nova For Me&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Online Nova IDE&lt;/item&gt;
      &lt;item&gt;Nova implementations (for if you want to connect Nova to existing code)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Find Community&lt;/head&gt;
    &lt;p&gt;Join us on...&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46192997</guid><pubDate>Mon, 08 Dec 2025 15:03:09 +0000</pubDate></item><item><title>Berkshire Hathaway Announces Leadership Appointments [pdf]</title><link>https://berkshirehathaway.com/news/dec0825.pdf</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46193116</guid><pubDate>Mon, 08 Dec 2025 15:12:28 +0000</pubDate></item><item><title>AMD GPU Debugger</title><link>https://thegeeko.me/blog/amd-gpu-debugging/</link><description>&lt;doc fingerprint="ae1ad8512d473b89"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;AMD GPU Debugger&lt;/head&gt;
    &lt;p&gt;I’ve always wondered why we don’t have a GPU debugger similar to the one used for CPUs. A tool that allows pausing execution and examining the current state. This capability feels essential, especially since the GPU’s concurrent execution model is much harder to reason about. After searching for solutions, I came across rocgdb, a debugger for AMD’s ROCm environment. Unfortunately, its scope is limited to that environment. Still, this shows it’s technically possible. I then found a helpful series of blog posts by Marcell Kiss, detailing how he achieved this, which inspired me to try to recreate the process myself.&lt;/p&gt;
    &lt;head rend="h1"&gt;Let’s Try To Talk To The GPU Directly&lt;/head&gt;
    &lt;p&gt;The best place to start learning about this is RADV. By tracing what it does, we can find how to do it. Our goal here is to run the most basic shader &lt;code&gt;nop 0&lt;/code&gt; without using Vulkan, aka RADV in our case.&lt;/p&gt;
    &lt;p&gt;First of all, we need to open the DRM file to establish a connection with the KMD, using a simple open(“/dev/dri/cardX”), then we find that it’s calling &lt;code&gt;amdgpu_device_initialize&lt;/code&gt;, which is a function defined in &lt;code&gt;libdrm&lt;/code&gt;, which is a library that acts as middleware between user mode drivers(UMD) like &lt;code&gt;RADV&lt;/code&gt; and and kernel mode drivers(KMD) like amdgpu driver, and then when we try to do some actual work we have to create a context which can be achieved by calling &lt;code&gt;amdgpu_cs_ctx_create&lt;/code&gt; from &lt;code&gt;libdrm&lt;/code&gt; again, next up we need to allocate 2 buffers one of them for our code and the other for writing our commands into, we do this by calling a couple of functions, here’s how I do it:&lt;/p&gt;
    &lt;code&gt;void bo_alloc(amdgpu_t* dev, size_t size, u32 domain, bool uncached, amdgpubo_t* bo) {
 s32    ret         = -1;
 u32    alignment   = 0;
 u32    flags       = 0;
 size_t actual_size = 0;

 amdgpu_bo_handle bo_handle = NULL;
 amdgpu_va_handle va_handle = NULL;
 u64              va_addr   = 0;
 void*            host_addr = NULL;&lt;/code&gt;
    &lt;p&gt;Here we’re choosing the domain and assigning flags based on the params, some buffers we will need uncached, as we will see:&lt;/p&gt;
    &lt;code&gt; if (
   domain != AMDGPU_GEM_DOMAIN_GWS &amp;amp;&amp;amp; domain != AMDGPU_GEM_DOMAIN_GDS &amp;amp;&amp;amp;
   domain != AMDGPU_GEM_DOMAIN_OA) {
  actual_size = (size + 4096 - 1) &amp;amp; 0xFFFFFFFFFFFFF000ULL;
  alignment   = 4096;
  flags       = AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED | AMDGPU_GEM_CREATE_VRAM_CLEARED |
          AMDGPU_GEM_CREATE_VM_ALWAYS_VALID;
  flags |=
    uncached ? (domain == AMDGPU_GEM_DOMAIN_GTT) * AMDGPU_GEM_CREATE_CPU_GTT_USWC : 0;
 } else {
  actual_size = size;
  alignment   = 1;
  flags       = AMDGPU_GEM_CREATE_NO_CPU_ACCESS;
 }

 struct amdgpu_bo_alloc_request req = {
  .alloc_size     = actual_size,
  .phys_alignment = alignment,
  .preferred_heap = domain,
  .flags          = flags,
 };

 // memory aquired!!
 ret = amdgpu_bo_alloc(dev-&amp;gt;dev_handle, &amp;amp;req, &amp;amp;bo_handle);
 HDB_ASSERT(!ret, "can't allocate bo");&lt;/code&gt;
    &lt;p&gt;Now we have the memory, we need to map it. I opt to map anything that can be CPU-mapped for ease of use. We have to map the memory to both the GPU and the CPU virtual space. The KMD creates the page table when we open the DRM file, as shown here.&lt;/p&gt;
    &lt;p&gt;So map it to the GPU VM and, if possible, to the CPU VM as well. Here, at this point, there’s a libdrm function that does all of this setup for us and maps the memory, but I found that even when specifying &lt;code&gt;AMDGPU_VM_MTYPE_UC&lt;/code&gt;, it doesn’t always tag the page as uncached, not quite sure if it’s a
bug in my code or something in &lt;code&gt;libdrm&lt;/code&gt; anyways, the function is &lt;code&gt;amdgpu_bo_va_op&lt;/code&gt;, I opted to do it manually here and issue the IOCTL call myself:&lt;/p&gt;
    &lt;code&gt; u32 kms_handle = 0;
 amdgpu_bo_export(bo_handle, amdgpu_bo_handle_type_kms, &amp;amp;kms_handle);

 ret = amdgpu_va_range_alloc(
   dev-&amp;gt;dev_handle,
   amdgpu_gpu_va_range_general,
   actual_size,
   4096,
   0,
   &amp;amp;va_addr,
   &amp;amp;va_handle,
   0);
 HDB_ASSERT(!ret, "can't allocate VA");

 u64 map_flags =
   AMDGPU_VM_PAGE_EXECUTABLE | AMDGPU_VM_PAGE_READABLE | AMDGPU_VM_PAGE_WRITEABLE;
 map_flags |= uncached ? AMDGPU_VM_MTYPE_UC | AMDGPU_VM_PAGE_NOALLOC : 0;

 struct drm_amdgpu_gem_va va = {
  .handle       = kms_handle,
  .operation    = AMDGPU_VA_OP_MAP,
  .flags        = map_flags,
  .va_address   = va_addr,
  .offset_in_bo = 0,
  .map_size     = actual_size,

 };

 ret = drm_ioctl_write_read(dev-&amp;gt;drm_fd, DRM_AMDGPU_GEM_VA, &amp;amp;va, sizeof(va));
 HDB_ASSERT(!ret, "can't map bo in GPU space");
 // ret = amdgpu_bo_va_op(bo_handle, 0, actual_size, va_addr, map_flags,
 // AMDGPU_VA_OP_MAP);

 if (flags &amp;amp; AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED) {
  ret = amdgpu_bo_cpu_map(bo_handle, &amp;amp;host_addr);
  HDB_ASSERT(!ret, "can't map bo in CPU space");

  // AMDGPU_GEM_CREATE_VRAM_CLEARED doesn't really memset the memory to 0 anyways for
  // debug I'll just do it manually for now
  memset(host_addr, 0x0, actual_size);
 }

 *bo = (amdgpubo_t){
  .bo_handle = bo_handle,
  .va_handle = va_handle,
  .va_addr   = va_addr,
  .size      = actual_size,
  .host_addr = host_addr,
 };
}&lt;/code&gt;
    &lt;p&gt;Now we have the context and 2 buffers. Next, fill those buffers and send our commands to the KMD, which will then forward them to the Command Processor (CP) in the GPU for processing.&lt;/p&gt;
    &lt;p&gt;Let’s compile our code. We can use clang assembler for that, like this:&lt;/p&gt;
    &lt;code&gt;# https://gitlab.freedesktop.org/martty/radbg-poc/-/blob/master/ll-as.sh
clang -c -x assembler -target amdgcn-amd-amdhsa -mcpu=gfx1100 -o asm.o "$1"
objdump -h asm.o | grep .text | awk '{print "dd if='asm.o' of='asmc.bin' bs=1 count=$[0x" $3 "] skip=$[0x" $6 "] status=none"}' | bash
#rm asm.o&lt;/code&gt;
    &lt;p&gt;The bash script compiles the code, and then we’re only interested in the actual machine code, so we use objdump to figure out the offset and the size of the section and copy it to a new file called asmc.bin, then we can just load the file and write its bytes to the CPU-mapped address of the code buffer.&lt;/p&gt;
    &lt;p&gt;Next up, filling in the commands. This was extremely confusing for me because it’s not well documented. It was mostly learning how &lt;code&gt;RADV&lt;/code&gt; does things and trying to do similar things. Also, shout-out to the folks on the Graphics Programming Discord server for helping me, especially Picoduck. The commands are encoded in a special format called &lt;code&gt;PM4 Packets&lt;/code&gt;, which has multiple types. We only care about &lt;code&gt;Type 3&lt;/code&gt;: each packet has an opcode and the number of bytes it contains.&lt;/p&gt;
    &lt;p&gt;The first thing we need to do is program the GPU registers, then dispatch the shader. Some of those registers are &lt;code&gt;rsrc[1-3]&lt;/code&gt;; those registers are responsible for a number of configurations, pgm_[lo/hi], which hold the pointer to the code buffer and &lt;code&gt;num_thread_[x/y/z]&lt;/code&gt;; those are responsible for the number of threads inside a work group. All of those are set using the &lt;code&gt;set shader register&lt;/code&gt; packets, and here is how to encode them:&lt;/p&gt;
    &lt;p&gt;It’s worth mentioning that we can set multiple registers in 1 packet if they’re consecutive.&lt;/p&gt;
    &lt;code&gt;void pkt3_set_sh_reg(pkt3_packets_t* packets, u32 reg, u32 value) {
 HDB_ASSERT(
   reg &amp;gt;= SI_SH_REG_OFFSET &amp;amp;&amp;amp; reg &amp;lt; SI_SH_REG_END,
   "can't set register outside sh registers span");

 // packet header
 da_append(packets, PKT3(PKT3_SET_SH_REG, 1, 0));
 // offset of the register
 da_append(packets, (reg - SI_SH_REG_OFFSET) / 4);
 da_append(packets, value);
}&lt;/code&gt;
    &lt;p&gt;Then we append the dispatch command:&lt;/p&gt;
    &lt;code&gt;// we're going for 1 thread since we want the simplest case here.

da_append(&amp;amp;pkt3_packets, PKT3(PKT3_DISPATCH_DIRECT, 3, 0) | PKT3_SHADER_TYPE_S(1));
da_append(&amp;amp;pkt3_packets, 1u);
da_append(&amp;amp;pkt3_packets, 1u);
da_append(&amp;amp;pkt3_packets, 1u);
da_append(&amp;amp;pkt3_packets, dispatch_initiator);&lt;/code&gt;
    &lt;p&gt;Now we want to write those commands into our buffer and send them to the KMD:&lt;/p&gt;
    &lt;code&gt;void dev_submit(
  amdgpu_t*         dev,
  pkt3_packets_t*   packets,
  amdgpu_bo_handle* buffers,
  u32               buffers_count,
  amdgpu_submit_t*  submit
) {
 s32        ret = -1;
 amdgpubo_t ib  = { 0 };

 bo_alloc(dev, pkt3_size(packets), AMDGPU_GEM_DOMAIN_GTT, false, &amp;amp;ib);
 bo_upload(&amp;amp;ib, packets-&amp;gt;data, pkt3_size(packets));

 amdgpu_bo_handle* bo_handles = // +1 for the indirect buffer
   (amdgpu_bo_handle*)malloc(sizeof(amdgpu_bo_handle) * (buffers_count + 1));

 bo_handles[0] = ib.bo_handle;
 for_range(i, 0, buffers_count) {
  bo_handles[i + 1] = buffers[i];
 }

 amdgpu_bo_list_handle bo_list = NULL;
 ret =
   amdgpu_bo_list_create(dev-&amp;gt;dev_handle, buffers_count + 1, bo_handles, NULL, &amp;amp;bo_list);
 HDB_ASSERT(!ret, "can't create a bo list");
 free(bo_handles);

 struct amdgpu_cs_ib_info ib_info = {
  .flags         = 0,
  .ib_mc_address = ib.va_addr,
  .size          = packets-&amp;gt;count,
 };

 struct amdgpu_cs_request req = {
  .flags                  = 0,
  .ip_type                = AMDGPU_HW_IP_COMPUTE,
  .ip_instance            = 0,
  .ring                   = 0,
  .resources              = bo_list,
  .number_of_dependencies = 0,
  .dependencies           = NULL,
  .number_of_ibs          = 1,
  .ibs                    = &amp;amp;ib_info,
  .seq_no                 = 0,
  .fence_info             = { 0 },
 };

 ret = amdgpu_cs_submit(dev-&amp;gt;ctx_handle, 0, &amp;amp;req, 1);
 HDB_ASSERT(!ret, "can't submit indirect buffer request");

 *submit = (amdgpu_submit_t){
    .ib = ib,
    .bo_list = bo_list,
    .fence = {
      .context = dev-&amp;gt;ctx_handle,
      .ip_type = AMDGPU_HW_IP_COMPUTE,
      .ip_instance = 0,
      .ring = 0,
      .fence = req.seq_no,
    },
  };
}&lt;/code&gt;
    &lt;p&gt;Here is a good point to make a more complex shader that outputs something. For example, writing 1 to a buffer.&lt;/p&gt;
    &lt;p&gt;No GPU hangs ?! nothing happened ?! cool, cool, now we have a shader that runs on the GPU, what’s next? Let’s try to hang the GPU by pausing the execution, aka make the GPU trap.&lt;/p&gt;
    &lt;head rend="h1"&gt;TBA/TMA&lt;/head&gt;
    &lt;p&gt;The RDNA3’s ISA manual does mention 2 registers, &lt;code&gt;TBA, TMA&lt;/code&gt;; here’s how they describe them respectively:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Holds the pointer to the current trap handler program address. Per-VMID register. Bit [63] indicates if the trap handler is present (1) or not (0) and is not considered part of the address (bit[62] is replicated into address bit[63]). Accessed via S_SENDMSG_RTN.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;Temporary register for shader operations. For example, it can hold a pointer to memory used by the trap handler.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;You can configure the GPU to enter the trap handler when encountering certain exceptions listed in the RDNA3 ISA manual.&lt;/p&gt;
    &lt;p&gt;We know from Marcell Kiss’s blog posts that we need to compile a trap handler, which is a normal shader the GPU switches to when encountering a &lt;code&gt;s_trap&lt;/code&gt;. The TBA register has a special bit that indicates whether the trap handler is enabled.&lt;/p&gt;
    &lt;p&gt;Since these are privileged registers, we cannot write to them from user space. To bridge this gap for debugging, we can utilize the debugfs interface. Luckily, we have UMR, which uses that debugfs interface, and it’s open source; we copy AMD’s homework here which is great.&lt;/p&gt;
    &lt;head rend="h1"&gt;AMDGPU Debugfs&lt;/head&gt;
    &lt;p&gt;The amdgpu KMD has a couple of files in debugfs under &lt;code&gt;/sys/kernel/debug/dri/{PCI address}&lt;/code&gt;; one of them is &lt;code&gt;regs2&lt;/code&gt;, which is an interface to a &lt;code&gt;amdgpu_debugfs_regs2_write&lt;/code&gt; in the kernel that writes to the registers. It works by simply opening the file, seeking the register’s offset, and then writing; it also performs some synchronisation and writes the value correctly. We need to provide more parameters about the register before writing to the file, tho and do that by using an ioctl call. Here are the ioctl arguments:&lt;/p&gt;
    &lt;code&gt;typedef struct amdgpu_debugfs_regs2_iocdata_v2 {
 __u32 use_srbm, use_grbm, pg_lock;
 struct {
  __u32 se, sh, instance;
 } grbm;
 struct {
  __u32 me, pipe, queue, vmid;
 } srbm;
 __u32 xcc_id;
} regs2_ioc_data_t;&lt;/code&gt;
    &lt;p&gt;The 2 structs are because there are 2 types of registers, GRBM and SRBM, each of which is banked by different constructs; you can learn more about some of them here in the Linux kernel documentation.&lt;/p&gt;
    &lt;p&gt;Turns out our registers here are SBRM registers and banked by VMIDs, meaning each VMID has its own TBA and TMA registers. Cool, now we need to figure out the VMID of our process. As far as I understand, VMIDs are a way for the GPU to identify a specific process context, including the page table base address, so the address translation unit can translate a virtual memory address. The context is created when we open the DRM file. They get assigned dynamically at dispatch time, which is a problem for us; we want to write to those registers before dispatch.&lt;/p&gt;
    &lt;p&gt;We can obtain the VMID of the dispatched process by querying the &lt;code&gt;HW_ID2&lt;/code&gt; register with s_getreg_b32. I do a hack here, by enabling the trap handler in every VMID, and there are 16 of them, the first being special, and used by the KMD and the last 8 allocated to the amdkfd driver. We loop over the remaining VMIDs and write to those registers. This can cause issues to other processes using other VMIDs by enabling trap handlers in them and writing the virtual address of our trap handler, which is only valid within our virtual memory address space. It’s relatively safe tho since most other processes won’t cause a trap1.&lt;/p&gt;
    &lt;p&gt;Now we can write to TMA and TBA, here’s the code:&lt;/p&gt;
    &lt;code&gt;void dev_op_reg32(
  amdgpu_t* dev, gc_11_reg_t reg, regs2_ioc_data_t ioc_data, reg_32_op_t op, u32* value) {
 s32 ret = 0;

 reg_info_t reg_info     = gc_11_regs_infos[reg];
 uint64_t   reg_offset   = gc_11_regs_offsets[reg];
 uint64_t   base_offset  = dev-&amp;gt;gc_regs_base_addr[reg_info.soc_index];
 uint64_t   total_offset = (reg_offset + base_offset);

 // seems like we're multiplying by 4 here because the registers database in UMRs
 // source has them in indexes rather than bytes.
 total_offset *= (reg_info.type == REG_MMIO) ? 4 : 1;

 ret = hdb_ioctl(dev-&amp;gt;regs2_fd, AMDGPU_DEBUGFS_REGS2_IOC_SET_STATE_V2, &amp;amp;ioc_data);
 HDB_ASSERT(!ret, "Failed to set registers state");

 size_t size = lseek(dev-&amp;gt;regs2_fd, total_offset, SEEK_SET);
 HDB_ASSERT(size == total_offset, "Failed to seek register address");

 switch (op) {
 case REG_OP_READ : size = read(dev-&amp;gt;regs2_fd, value, 4); break;
 case REG_OP_WRITE: size = write(dev-&amp;gt;regs2_fd, value, 4); break;
 default          : HDB_ASSERT(false, "unsupported op");
 }

 HDB_ASSERT(size == 4, "Failed to write/read the values to/from the register");
}&lt;/code&gt;
    &lt;p&gt;And here’s how we write to &lt;code&gt;TMA&lt;/code&gt; and &lt;code&gt;TBA&lt;/code&gt;:
If you noticed, I’m using bitfields. I use them because working with them is much easier than macros, and while the byte order is not guaranteed by the C spec, it’s guaranteed by System V ABI, which Linux adheres to.&lt;/p&gt;
    &lt;code&gt;void dev_setup_trap_handler(amdgpu_t* dev, u64 tba, u64 tma) {
 reg_sq_shader_tma_lo_t tma_lo = { .raw = (u32)(tma) };
 reg_sq_shader_tma_hi_t tma_hi = { .raw = (u32)(tma &amp;gt;&amp;gt; 32) };

 reg_sq_shader_tba_lo_t tba_lo = { .raw = (u32)(tba &amp;gt;&amp;gt; 8) };
 reg_sq_shader_tba_hi_t tba_hi = { .raw = (u32)(tba &amp;gt;&amp;gt; 40) };

 tba_hi.trap_en = 1;

 regs2_ioc_data_t ioc_data = {
  .use_srbm = 1,
  .xcc_id   = -1,
 };

 // NOTE(hadi):
 // vmid's get assigned when code starts executing before hand we don't know which vmid
 // will get assigned to our process so we just set all of them
 for_range(i, 1, 9) {
  ioc_data.srbm.vmid = i;
  dev_op_reg32(dev, REG_SQ_SHADER_TBA_LO, ioc_data, REG_OP_WRITE, &amp;amp;tba_lo.raw);
  dev_op_reg32(dev, REG_SQ_SHADER_TBA_HI, ioc_data, REG_OP_WRITE, &amp;amp;tba_hi.raw);

  dev_op_reg32(dev, REG_SQ_SHADER_TMA_LO, ioc_data, REG_OP_WRITE, &amp;amp;tma_lo.raw);
  dev_op_reg32(dev, REG_SQ_SHADER_TMA_HI, ioc_data, REG_OP_WRITE, &amp;amp;tma_hi.raw);
 }
}&lt;/code&gt;
    &lt;p&gt;Anyway, now that we can write to those registers, if we enable the trap handler correctly, the GPU should hang when we launch our shader if we added &lt;code&gt;s_trap&lt;/code&gt; instruction to it, or we enabled the &lt;code&gt;TRAP_ON_START&lt;/code&gt; bit in rsrc32 register.&lt;/p&gt;
    &lt;p&gt;Now, let’s try to write a trap handler.&lt;/p&gt;
    &lt;head rend="h1"&gt;The Trap Handler&lt;/head&gt;
    &lt;p&gt;If you wrote a different shader that outputs to a buffer, u can try writing to that shader from the trap handler, which is nice to make sure it’s actually being run.&lt;/p&gt;
    &lt;p&gt;We need 2 things: our trap handler and some scratch memory to use when needed, which we will store the address of in the TMA register.&lt;/p&gt;
    &lt;p&gt;The trap handler is just a normal program running in privileged state, meaning we have access to special registers like TTMP[0-15]. When we enter a trap handler, we need to first ensure that the state of the GPU registers is saved, just as the kernel does for CPU processes when context-switching, by saving a copy of the stable registers and the program counter, etc. The problem, tho, is that we don’t have a stable ABI for GPUs, or at least not one I’m aware of, and compilers use all the registers they can, so we need to save everything.&lt;/p&gt;
    &lt;p&gt;AMD GPUs’ Command Processors (CPs) have context-switching functionality, and the amdkfd driver does implement some context-switching shaders. The problem is they’re not documented, and we have to figure them out from the amdkfd driver source and from other parts of the driver stack that interact with it, which is a pain in the ass. I kinda did a workaround here since I didn’t find luck understanding how it works, and some other reasons I’ll discuss later in the post.&lt;/p&gt;
    &lt;p&gt;The workaround here is to use only TTMP registers and a combination of specific instructions to copy the values of some registers, allowing us to use more instructions to copy the remaining registers. The main idea is to make use of the &lt;code&gt;global_store_addtid_b32&lt;/code&gt; instruction, which adds the index of the current thread within the wave to the writing address, aka&lt;/p&gt;
    &lt;p&gt;This allows us to write a unique value per thread using only TTMP registers, which are unique per wave, not per thread3, so we can save the context of a single wave.&lt;/p&gt;
    &lt;p&gt;The problem is that if we have more than 1 wave, they will overlap, and we will have a race condition.&lt;/p&gt;
    &lt;p&gt;Here is the code:&lt;/p&gt;
    &lt;code&gt;start:
 ;; save the STATUS word into ttmp8
 s_getreg_b32 ttmp8, hwreg(HW_REG_STATUS)

 ;; save exec into ttmp[2:3]
 s_mov_b64 ttmp[2:3], exec

 ;; getting the address of our tma buffer
 s_sendmsg_rtn_b64 ttmp[4:5], sendmsg(MSG_RTN_GET_TMA)
 s_waitcnt lgkmcnt(0)

 ;; save vcc
 s_mov_b64 ttmp[6:7], vcc

 ;; enable all threads so they can write their vgpr registers
 s_mov_b64 exec, -1

 ;; FIXME(hadi): this assumes only 1 wave is running
 global_store_addtid_b32 v0, ttmp[4:5], offset:TMA_VREG_OFFSET        glc slc dlc
 global_store_addtid_b32 v1, ttmp[4:5], offset:TMA_VREG_OFFSET + 256  glc slc dlc
 global_store_addtid_b32 v2, ttmp[4:5], offset:TMA_VREG_OFFSET + 512  glc slc dlc
 global_store_addtid_b32 v3, ttmp[4:5], offset:TMA_VREG_OFFSET + 768  glc slc dlc
 global_store_addtid_b32 v4, ttmp[4:5], offset:TMA_VREG_OFFSET + 1024 glc slc dlc
 global_store_addtid_b32 v5, ttmp[4:5], offset:TMA_VREG_OFFSET + 1280 glc slc dlc
 global_store_addtid_b32 v6, ttmp[4:5], offset:TMA_VREG_OFFSET + 1536 glc slc dlc
 s_waitcnt vmcnt(0)

 ;; only first thread is supposed to write sgprs of the wave
 s_mov_b64 exec, 1
 v_mov_b32 v1, s0
 v_mov_b32 v2, s1
 v_mov_b32 v3, s2
 v_mov_b32 v4, s3
 v_mov_b32 v5, s4
 v_mov_b32 v0, 0
 global_store_b32 v0, v1, ttmp[4:5], offset:TMA_SREG_OFFSET glc slc dlc
 global_store_b32 v0, v2, ttmp[4:5], offset:TMA_SREG_OFFSET + 4 glc slc dlc
 global_store_b32 v0, v3, ttmp[4:5], offset:TMA_SREG_OFFSET + 8 glc slc dlc
 global_store_b32 v0, v4, ttmp[4:5], offset:TMA_SREG_OFFSET + 12 glc slc dlc
 global_store_b32 v0, v5, ttmp[4:5], offset:TMA_SREG_OFFSET + 16 glc slc dlc
 s_waitcnt vmcnt(0)

 ;; enable all threads
 s_mov_b64 exec, -1&lt;/code&gt;
    &lt;p&gt;Now that we have those values in memory, we need to tell the CPU: Hey, we got the data, and pause the GPU’s execution until the CPU issues a command. Also, notice we can just modify those from the CPU.&lt;/p&gt;
    &lt;p&gt;Before we tell the CPU, we need to write some values that might help the CPU. Here are they:&lt;/p&gt;
    &lt;code&gt; ;; IDs to identify which parts of the hardware we are running on exactly
 s_getreg_b32 ttmp10, hwreg(HW_REG_HW_ID1)
 s_getreg_b32 ttmp11, hwreg(HW_REG_HW_ID2)
 v_mov_b32 v3, ttmp10
 v_mov_b32 v4, ttmp11
 global_store_dwordx2 v1, v[3:4], ttmp[4:5], offset:TMA_DATA_OFFSET glc slc dlc

 ;; the original vcc mask
 v_mov_b32 v3, ttmp6
 v_mov_b32 v4, ttmp7
 global_store_dwordx2 v1, v[3:4], ttmp[4:5], offset:2048 glc slc dlc
 s_waitcnt vmcnt(0)

 ;; the original exec mask
 v_mov_b32 v3, ttmp2
 v_mov_b32 v4, ttmp3
 global_store_dwordx2 v1, v[3:4], ttmp[4:5], offset:2056 glc slc dlc
 s_waitcnt vmcnt(0)

 ;; the program counter
 v_mov_b32 v3, ttmp0
 v_mov_b32 v4, ttmp1
 v_and_b32 v4, v4, 0xffff
 global_store_dwordx2 v1, v[3:4], ttmp[4:5], offset:16 glc slc dlc

 s_waitcnt vmcnt(0)&lt;/code&gt;
    &lt;p&gt;Now the GPU should just wait for the CPU, and here’s the spin code it’s implemented as described by Marcell Kiss here:&lt;/p&gt;
    &lt;code&gt;SPIN:
 global_load_dword v1, v2, ttmp[4:5] glc slc dlc

SPIN1:
 // I found the bit range of 10 to 15 using trial and error in the
 // isa manual specifies that it's a 6-bit number but the offset 10
 // is just trial and error
  s_getreg_b32 ttmp13, hwreg(HW_REG_IB_STS, 10, 15)
 s_and_b32 ttmp13, ttmp13, ttmp13
 s_cbranch_scc1 SPIN1

 v_readfirstlane_b32 ttmp13, v1
 s_and_b32 ttmp13, ttmp13, ttmp13
 s_cbranch_scc0 SPIN

CLEAR:
 v_mov_b32 v2, 0
 v_mov_b32 v1, 0
 global_store_dword v1, v2, ttmp[4:5] glc slc dlc
 s_waitcnt vmcnt(0)&lt;/code&gt;
    &lt;p&gt;The main loop in the CPU is like enable trap handler, then dispatch shader, then wait for the GPU to write some specific value in a specific address to signal all data is there, then examine and display, and tell the GPU all clear, go ahead.&lt;/p&gt;
    &lt;p&gt;Now that our uncached buffers are in play, we just keep looping and checking whether the GPU has written the register values. When it does, the first thing we do is halt the wave by writing into the &lt;code&gt;SQ_CMD&lt;/code&gt; register to allow us to do whatever with the wave without causing any issues, tho if we halt for too long, the GPU CP will reset the command queue and kill the process, but we can change that behaviour by adjusting lockup_timeout parameter of the amdgpu kernel module:&lt;/p&gt;
    &lt;code&gt;reg_sq_wave_hw_id1_t hw1 = { .raw = tma[2] };
reg_sq_wave_hw_id2_t hw2 = { .raw = tma[3] };

reg_sq_cmd_t halt_cmd = {
 .cmd  = 1,
 .mode = 1,
 .data = 1,
};

regs2_ioc_data_t ioc_data = {
 .use_srbm = false,
 .use_grbm = true,
};

dev_op_reg32(&amp;amp;amdgpu, REG_SQ_CMD, ioc_data, REG_OP_WRITE, &amp;amp;halt_cmd.raw);
gpu_is_halted = true;&lt;/code&gt;
    &lt;p&gt;From here on, we can do whatever with the data we have. All the data we need to build a proper debugger. We will come back to what to do with the data in a bit; let’s assume we did what was needed for now.&lt;/p&gt;
    &lt;p&gt;Now that we’re done with the CPU, we need to write to the first byte in our TMA buffer, since the trap handler checks for that, then resume the wave, and the trap handler should pick it up. We can resume by writing to the &lt;code&gt;SQ_CMD&lt;/code&gt; register again:&lt;/p&gt;
    &lt;code&gt;halt_cmd.mode = 0;
dev_op_reg32(&amp;amp;amdgpu, REG_SQ_CMD, ioc_data, REG_OP_WRITE, &amp;amp;halt_cmd.raw);
gpu_is_halted = false;&lt;/code&gt;
    &lt;p&gt;Then the GPU should continue. We need to restore everything and return the program counter to the original address. Based on whether it’s a hardware trap or not, the program counter may point to the instruction before or the instruction itself. The ISA manual and Marcell Kiss’s posts explain that well, so refer to them.&lt;/p&gt;
    &lt;code&gt;RETURN:
 ;; extract the trap ID from ttmp1
 s_and_b32 ttmp9, ttmp1, PC_HI_TRAP_ID_MASK
 s_lshr_b32 ttmp9, ttmp9, PC_HI_TRAP_ID_SHIFT

 ;; if the trapID == 0, then this is a hardware trap,
 ;; we don't need to fix up the return address
 s_cmpk_eq_u32 ttmp9, 0
 s_cbranch_scc1 RETURN_FROM_NON_S_TRAP

 ;; restore PC
 ;; add 4 to the faulting address, with carry
 s_add_u32 ttmp0, ttmp0, 4
 s_addc_u32 ttmp1, ttmp1, 0

RETURN_FROM_NON_S_TRAP:
 s_load_dwordx4 s[0:3], ttmp[4:5], TMA_SREG_OFFSET glc dlc
 s_load_dword s4, ttmp[4:5], TMA_SREG_OFFSET + 16 glc dlc
 s_waitcnt lgkmcnt(0)

 s_mov_b64 exec, -1
 global_load_addtid_b32 v0, ttmp[4:5], offset:TMA_VREG_OFFSET        glc slc dlc
 global_load_addtid_b32 v1, ttmp[4:5], offset:TMA_VREG_OFFSET + 256  glc slc dlc
 global_load_addtid_b32 v2, ttmp[4:5], offset:TMA_VREG_OFFSET + 512  glc slc dlc
 global_load_addtid_b32 v3, ttmp[4:5], offset:TMA_VREG_OFFSET + 768  glc slc dlc
 global_load_addtid_b32 v4, ttmp[4:5], offset:TMA_VREG_OFFSET + 1024 glc slc dlc
 global_load_addtid_b32 v5, ttmp[4:5], offset:TMA_VREG_OFFSET + 1280 glc slc dlc
 global_load_addtid_b32 v6, ttmp[4:5], offset:TMA_VREG_OFFSET + 1536 glc slc dlc
 s_waitcnt vmcnt(0)

 ;; mask off non-address high bits from ttmp1
 s_and_b32 ttmp1, ttmp1, 0xffff

 ;; restore exec
 s_load_b64 vcc, ttmp[4:5], 2048 glc dlc
 s_load_b64 ttmp[2:3], ttmp[4:5], 2056 glc dlc
 s_waitcnt lgkmcnt(0)
 s_mov_b64 exec, ttmp[2:3]

 ;; restore STATUS.EXECZ, not writable by s_setreg_b32
 s_and_b64 exec, exec, exec

 ;; restore STATUS.VCCZ, not writable by s_setreg_b32
 s_and_b64 vcc, vcc, vcc

 ;; restore STATUS.SCC
 s_setreg_b32 hwreg(HW_REG_STATUS, 0, 1), ttmp8

 s_waitcnt vmcnt(0) lgkmcnt(0) expcnt(0)  ; Full pipeline flush
 ;; return from trap handler and restore STATUS.PRIV
 s_rfe_b64 [ttmp0, ttmp1]&lt;/code&gt;
    &lt;head rend="h1"&gt;SPIR-V&lt;/head&gt;
    &lt;p&gt;Now we can run compiled code directly, but we don’t want people to compile their code manually, then extract the text section, and give it to us. The plan is to take SPIR-V code, compile it correctly, then run it, or, even better, integrate with RADV and let RADV give us more information to work with.&lt;/p&gt;
    &lt;p&gt;My main plan was making like fork RADV and then add then make report for us the vulkan calls and then we can have a better view on the GPU work know the buffers/textures it’s using etc, This seems like a lot more work tho so I’ll keep it in mind but not doing that for now unless someone is willing to pay me for that ;).&lt;/p&gt;
    &lt;p&gt;For now, let’s just use RADV’s compiler &lt;code&gt;ACO&lt;/code&gt;. Luckily, RADV has a &lt;code&gt;null_winsys&lt;/code&gt; mode, aka it will not do actual work or open DRM files, just a fake Vulkan device, which is perfect for our case here, since we care about nothing other than just compiling code. We can enable it by setting the env var &lt;code&gt;RADV_FORCE_FAMILY&lt;/code&gt;, then we just call what we need like this:&lt;/p&gt;
    &lt;code&gt;int32_t hdb_compile_spirv_to_bin(
  const void* spirv_binary,
  size_t size,
  hdb_shader_stage_t stage,
  hdb_shader_t* shader
) {
 setenv("RADV_FORCE_FAMILY", "navi31", 1);
 //  setenv("RADV_DEBUG", "nocache,noopt", 1);
 setenv("ACO_DEBUG", "nocache,noopt", 1);

 VkInstanceCreateInfo i_cinfo = {
  .sType = VK_STRUCTURE_TYPE_INSTANCE_CREATE_INFO,
  .pApplicationInfo =
    &amp;amp;(VkApplicationInfo){
      .sType              = VK_STRUCTURE_TYPE_APPLICATION_INFO,
      .pApplicationName   = "HDB Shader Compiler",
      .applicationVersion = 1,
      .pEngineName        = "HDB",
      .engineVersion      = 1,
      .apiVersion         = VK_API_VERSION_1_4,
    },
 };

 VkInstance vk_instance = {};
 radv_CreateInstance(&amp;amp;i_cinfo, NULL, &amp;amp;vk_instance);

 struct radv_instance* instance = radv_instance_from_handle(vk_instance);
 instance-&amp;gt;debug_flags |=
   RADV_DEBUG_NIR_DEBUG_INFO | RADV_DEBUG_NO_CACHE | RADV_DEBUG_INFO;

 uint32_t         n       = 1;
 VkPhysicalDevice vk_pdev = {};
 instance-&amp;gt;vk.dispatch_table.EnumeratePhysicalDevices(vk_instance, &amp;amp;n, &amp;amp;vk_pdev);

 struct radv_physical_device* pdev = radv_physical_device_from_handle(vk_pdev);
 pdev-&amp;gt;use_llvm                    = false;

 VkDeviceCreateInfo d_cinfo = { VK_STRUCTURE_TYPE_DEVICE_CREATE_INFO };
 VkDevice vk_dev = {};
 pdev-&amp;gt;vk.dispatch_table.CreateDevice(vk_pdev, &amp;amp;d_cinfo, NULL, &amp;amp;vk_dev);

 struct radv_device* dev = radv_device_from_handle(vk_dev);

 struct radv_shader_stage radv_stage = {
  .spirv.data = spirv_binary,
  .spirv.size = size,
  .entrypoint = "main",
  .stage      = MESA_SHADER_COMPUTE,
  .layout = {
   .push_constant_size = 16,
  },
  .key = {
   .optimisations_disabled = true,
  },
 };

 struct radv_shader_binary* cs_bin = NULL;
 struct radv_shader*        cs_shader =
   radv_compile_cs(dev, NULL, &amp;amp;radv_stage, true, true, false, true, &amp;amp;cs_bin);

 *shader = (hdb_shader_t){
  .bin              = cs_shader-&amp;gt;code,
  .bin_size         = cs_shader-&amp;gt;code_size,
  .rsrc1            = cs_shader-&amp;gt;config.rsrc1,
  .rsrc2            = cs_shader-&amp;gt;config.rsrc2,
  .rsrc3            = cs_shader-&amp;gt;config.rsrc3,
  .debug_info       = cs_shader-&amp;gt;debug_info,
  .debug_info_count = cs_shader-&amp;gt;debug_info_count,
 };

 return 0;
}&lt;/code&gt;
    &lt;p&gt;Now that we have a well-structured loop and communication between the GPU and the CPU, we can run SPIR-V binaries to some extent. Let’s see how we can make it an actual debugger.&lt;/p&gt;
    &lt;head rend="h1"&gt;An Actual Debugger&lt;/head&gt;
    &lt;p&gt;We talked earlier about CPs natively supporting context-switching, this appears to be compute spcific feature, which prevents from implementing it for other types of shaders, tho, it appears that mesh shaders and raytracing shaders are just compute shaders under the hood, which will allow us to use that functionality. For now debugging one wave feels enough, also we can moify the wave parameters to debug some specific indices.&lt;/p&gt;
    &lt;p&gt;Here’s some of the features&lt;/p&gt;
    &lt;head rend="h2"&gt;Breakpoints and Stepping&lt;/head&gt;
    &lt;p&gt;For stepping, we can use 2 bits: one in &lt;code&gt;RSRC1&lt;/code&gt; and the other in &lt;code&gt;RSRC3&lt;/code&gt;. They’re &lt;code&gt;DEBUG_MODE&lt;/code&gt; and &lt;code&gt;TRAP_ON_START&lt;/code&gt;, respectively. The former enters the trap handler after each instruction, and the latter enters before the first instruction. This means we can automatically enable instruction-level stepping.&lt;/p&gt;
    &lt;p&gt;Regarding breakpoints, I haven’t implemented them, but they’re rather simple to implement here by us having the base address of the code buffer and knowing the size of each instruction; we can calculate the program counter location ahead and have a list of them available to the GPU, and we can do a binary search on the trap handler.&lt;/p&gt;
    &lt;head rend="h2"&gt;Source Code Line Mapping&lt;/head&gt;
    &lt;p&gt;The ACO shader compiler does generate instruction-level source code mapping, which is good enough for our purposes here. By taking the offset4 of the current program counter and indexing into the code buffer, we can retrieve the current instruction and disassemble it, as well as find the source code mapping from the debug info.&lt;/p&gt;
    &lt;head rend="h2"&gt;Address Watching aka Watchpoints&lt;/head&gt;
    &lt;p&gt;We can implement this by marking the GPU page as protected. On a GPU fault, we enter the trap handler, check whether it’s within the range of our buffers and textures, and then act accordingly. Also, looking at the registers, we can find these:&lt;/p&gt;
    &lt;code&gt;typedef union {
 struct {
  uint32_t addr: 16;
 };
 uint32_t raw;
} reg_sq_watch0_addr_h_t;

typedef union {
 struct {
  uint32_t __reserved_0 : 6;
  uint32_t addr: 26;
 };
 uint32_t raw;
} reg_sq_watch0_addr_l_t;&lt;/code&gt;
    &lt;p&gt;which suggests that the hardware already supports this natively, so we don’t even need to do that dance. It needs more investigation on my part, tho, since I didn’t implement this.&lt;/p&gt;
    &lt;head rend="h2"&gt;Variables Types and Names&lt;/head&gt;
    &lt;p&gt;This needs some serious plumbing, since we need to make NIR(Mesa’s intermediate representation) optimisation passes propagate debug info correctly. I already started on this here. Then we need to make ACO track variables and store the information.&lt;/p&gt;
    &lt;head rend="h2"&gt;Vulkan Integration&lt;/head&gt;
    &lt;p&gt;This requires ditching our simple UMD we made earlier and using RADV, which is what should happen eventually, then we have our custom driver maybe pause on before a specific frame, or get triggered by a key, and then ask before each dispatch if to attach to it or not, or something similar, since we have a full proper Vulkan implementation we already have all the information we would need like buffers, textures, push constants, types, variable names, .. etc, that would be a much better and more pleasant debugger to use.&lt;/p&gt;
    &lt;p&gt;Finally, here’s some live footage:&lt;/p&gt;
    &lt;head rend="h1"&gt;Bonus Round&lt;/head&gt;
    &lt;p&gt;Here is an incomplete user-mode page walking code for gfx11, aka rx7900xtx&lt;/p&gt;
    &lt;code&gt;typedef struct {
 u64 valid         : 1;  // 0
 u64 system        : 1;  // 1
 u64 coherent      : 1;  // 2
 u64 __reserved_0  : 3;  // 5
 u64 pte_base_addr : 42; // 47
 u64 pa_rsvd       : 4;  // 51
 u64 __reserved_1  : 2;  // 53
 u64 mall_reuse    : 2;  // 55
 u64 tfs_addr      : 1;  // 56
 u64 __reserved_2  : 1;  // 57
 u64 frag_size     : 5;  // 62
 u64 pte           : 1;  // 63
} pde_t;

typedef struct {
 u64 valid          : 1; // = pte_entry &amp;amp; 1;
 u64 system         : 1; // = (pte_entry &amp;gt;&amp;gt; 1) &amp;amp; 1;
 u64 coherent       : 1; // = (pte_entry &amp;gt;&amp;gt; 2) &amp;amp; 1;
 u64 tmz            : 1; // = (pte_entry &amp;gt;&amp;gt; 3) &amp;amp; 1;
 u64 execute        : 1; // = (pte_entry &amp;gt;&amp;gt; 4) &amp;amp; 1;
 u64 read           : 1; // = (pte_entry &amp;gt;&amp;gt; 5) &amp;amp; 1;
 u64 write          : 1; // = (pte_entry &amp;gt;&amp;gt; 6) &amp;amp; 1;
 u64 fragment       : 5; // = (pte_entry &amp;gt;&amp;gt; 7) &amp;amp; 0x1F;
 u64 page_base_addr : 36;
 u64 mtype          : 2; // = (pte_entry &amp;gt;&amp;gt; 48) &amp;amp; 3;
 u64 prt            : 1; // = (pte_entry &amp;gt;&amp;gt; 51) &amp;amp; 1;
 u64 software       : 2; // = (pte_entry &amp;gt;&amp;gt; 52) &amp;amp; 3;
 u64 pde            : 1; // = (pte_entry &amp;gt;&amp;gt; 54) &amp;amp; 1;
 u64 __reserved_0   : 1;
 u64 further        : 1; // = (pte_entry &amp;gt;&amp;gt; 56) &amp;amp; 1;
 u64 gcr            : 1; // = (pte_entry &amp;gt;&amp;gt; 57) &amp;amp; 1;
 u64 llc_noalloc    : 1; // = (pte_entry &amp;gt;&amp;gt; 58) &amp;amp; 1;
} pte_t;

static inline pde_t decode_pde(u64 pde_raw) {
 pde_t pde         = *((pde_t*)(&amp;amp;pde_raw));
 pde.pte_base_addr = (u64)pde.pte_base_addr &amp;lt;&amp;lt; 6;
 return pde;
}

static inline pte_t decode_pte(u64 pde_raw) {
 pte_t pte          = *((pte_t*)(&amp;amp;pde_raw));
 pte.page_base_addr = (u64)pte.page_base_addr &amp;lt;&amp;lt; 12;
 return pte;
}

static inline u64 log2_range_round_up(u64 s, u64 e) {
 u64 x = e - s - 1;
 return (x == 0 || x == 1) ? 1 : 64 - __builtin_clzll(x);
}

void dev_linear_vram(amdgpu_t* dev, u64 phy_addr, size_t size, void* buf) {
 HDB_ASSERT(!((phy_addr &amp;amp; 3) || (size &amp;amp; 3)), "Must be page aligned address and size");

 size_t offset = lseek(dev-&amp;gt;vram_fd, phy_addr, SEEK_SET);
 HDB_ASSERT(offset == phy_addr, "Couldn't seek to the requested addr");

 offset = read(dev-&amp;gt;vram_fd, buf, size);
 HDB_ASSERT(offset == size, "Couldn't read the full requested size");
}

void dev_decode(amdgpu_t* dev, u32 vmid, u64 va_addr) {
 reg_gcmc_vm_fb_location_base_t fb_base_reg   = { 0 };
 reg_gcmc_vm_fb_location_top_t  fb_top_reg    = { 0 };
 reg_gcmc_vm_fb_offset_t        fb_offset_reg = { 0 };

 regs2_ioc_data_t ioc_data = { 0 };
 dev_op_reg32(
   dev, REG_GCMC_VM_FB_LOCATION_BASE, ioc_data, REG_OP_READ, &amp;amp;fb_base_reg.raw);
 dev_op_reg32(dev, REG_GCMC_VM_FB_LOCATION_TOP, ioc_data, REG_OP_READ, &amp;amp;fb_top_reg.raw);
 dev_op_reg32(dev, REG_GCMC_VM_FB_OFFSET, ioc_data, REG_OP_READ, &amp;amp;fb_offset_reg.raw);

 u64 fb_offset = (u64)fb_offset_reg.fb_offset;

 // TODO(hadi): add zfb mode support
 bool zfb = fb_top_reg.fb_top + 1 &amp;lt; fb_base_reg.fb_base;
 HDB_ASSERT(!zfb, "ZFB mode is not implemented yet!");

 // printf(
 //   "fb base: 0x%x\nfb_top: 0x%x\nfb_offset: 0x%x\n",
 //   fb_base_reg.raw,
 //   fb_top_reg.raw,
 //   fb_offset_reg.raw);

 gc_11_reg_t pt_start_lo_id = { 0 };
 gc_11_reg_t pt_start_hi_id = { 0 };
 gc_11_reg_t pt_end_lo_id   = { 0 };
 gc_11_reg_t pt_end_hi_id   = { 0 };
 gc_11_reg_t pt_base_hi_id  = { 0 };
 gc_11_reg_t pt_base_lo_id  = { 0 };
 gc_11_reg_t ctx_cntl_id    = { 0 };

 switch (vmid) {
 case 0:
  pt_start_lo_id = REG_GCVM_CONTEXT0_PAGE_TABLE_START_ADDR_LO32;
  pt_start_hi_id = REG_GCVM_CONTEXT0_PAGE_TABLE_START_ADDR_HI32;
  pt_end_lo_id   = REG_GCVM_CONTEXT0_PAGE_TABLE_END_ADDR_LO32;
  pt_end_hi_id   = REG_GCVM_CONTEXT0_PAGE_TABLE_END_ADDR_HI32;
  pt_base_lo_id  = REG_GCVM_CONTEXT0_PAGE_TABLE_BASE_ADDR_LO32;
  pt_base_hi_id  = REG_GCVM_CONTEXT0_PAGE_TABLE_BASE_ADDR_HI32;
  ctx_cntl_id    = REG_GCVM_CONTEXT0_CNTL;
  break;
 case 1:
  pt_start_lo_id = REG_GCVM_CONTEXT1_PAGE_TABLE_START_ADDR_LO32;
  pt_start_hi_id = REG_GCVM_CONTEXT1_PAGE_TABLE_START_ADDR_HI32;
  pt_end_lo_id   = REG_GCVM_CONTEXT1_PAGE_TABLE_END_ADDR_LO32;
  pt_end_hi_id   = REG_GCVM_CONTEXT1_PAGE_TABLE_END_ADDR_HI32;
  pt_base_lo_id  = REG_GCVM_CONTEXT1_PAGE_TABLE_BASE_ADDR_LO32;
  pt_base_hi_id  = REG_GCVM_CONTEXT1_PAGE_TABLE_BASE_ADDR_HI32;
  ctx_cntl_id    = REG_GCVM_CONTEXT1_CNTL;
  break;
 case 2:
  pt_start_lo_id = REG_GCVM_CONTEXT2_PAGE_TABLE_START_ADDR_LO32;
  pt_start_hi_id = REG_GCVM_CONTEXT2_PAGE_TABLE_START_ADDR_HI32;
  pt_end_lo_id   = REG_GCVM_CONTEXT2_PAGE_TABLE_END_ADDR_LO32;
  pt_end_hi_id   = REG_GCVM_CONTEXT2_PAGE_TABLE_END_ADDR_HI32;
  pt_base_lo_id  = REG_GCVM_CONTEXT2_PAGE_TABLE_BASE_ADDR_LO32;
  pt_base_hi_id  = REG_GCVM_CONTEXT2_PAGE_TABLE_BASE_ADDR_HI32;
  ctx_cntl_id    = REG_GCVM_CONTEXT2_CNTL;
  break;
 case 3:
  pt_start_lo_id = REG_GCVM_CONTEXT3_PAGE_TABLE_START_ADDR_LO32;
  pt_start_hi_id = REG_GCVM_CONTEXT3_PAGE_TABLE_START_ADDR_HI32;
  pt_end_lo_id   = REG_GCVM_CONTEXT3_PAGE_TABLE_END_ADDR_LO32;
  pt_end_hi_id   = REG_GCVM_CONTEXT3_PAGE_TABLE_END_ADDR_HI32;
  pt_base_lo_id  = REG_GCVM_CONTEXT3_PAGE_TABLE_BASE_ADDR_LO32;
  pt_base_hi_id  = REG_GCVM_CONTEXT3_PAGE_TABLE_BASE_ADDR_HI32;
  ctx_cntl_id    = REG_GCVM_CONTEXT3_CNTL;
  break;
 case 4:
  pt_start_lo_id = REG_GCVM_CONTEXT4_PAGE_TABLE_START_ADDR_LO32;
  pt_start_hi_id = REG_GCVM_CONTEXT4_PAGE_TABLE_START_ADDR_HI32;
  pt_end_lo_id   = REG_GCVM_CONTEXT4_PAGE_TABLE_END_ADDR_LO32;
  pt_end_hi_id   = REG_GCVM_CONTEXT4_PAGE_TABLE_END_ADDR_HI32;
  pt_base_lo_id  = REG_GCVM_CONTEXT4_PAGE_TABLE_BASE_ADDR_LO32;
  pt_base_hi_id  = REG_GCVM_CONTEXT4_PAGE_TABLE_BASE_ADDR_HI32;
  ctx_cntl_id    = REG_GCVM_CONTEXT4_CNTL;
  break;
 case 5:
  pt_start_lo_id = REG_GCVM_CONTEXT5_PAGE_TABLE_START_ADDR_LO32;
  pt_start_hi_id = REG_GCVM_CONTEXT5_PAGE_TABLE_START_ADDR_HI32;
  pt_end_lo_id   = REG_GCVM_CONTEXT5_PAGE_TABLE_END_ADDR_LO32;
  pt_end_hi_id   = REG_GCVM_CONTEXT5_PAGE_TABLE_END_ADDR_HI32;
  pt_base_lo_id  = REG_GCVM_CONTEXT5_PAGE_TABLE_BASE_ADDR_LO32;
  pt_base_hi_id  = REG_GCVM_CONTEXT5_PAGE_TABLE_BASE_ADDR_HI32;
  ctx_cntl_id    = REG_GCVM_CONTEXT5_CNTL;
  break;
 case 6:
  pt_start_lo_id = REG_GCVM_CONTEXT6_PAGE_TABLE_START_ADDR_LO32;
  pt_start_hi_id = REG_GCVM_CONTEXT6_PAGE_TABLE_START_ADDR_HI32;
  pt_end_lo_id   = REG_GCVM_CONTEXT6_PAGE_TABLE_END_ADDR_LO32;
  pt_end_hi_id   = REG_GCVM_CONTEXT6_PAGE_TABLE_END_ADDR_HI32;
  pt_base_lo_id  = REG_GCVM_CONTEXT6_PAGE_TABLE_BASE_ADDR_LO32;
  pt_base_hi_id  = REG_GCVM_CONTEXT6_PAGE_TABLE_BASE_ADDR_HI32;
  ctx_cntl_id    = REG_GCVM_CONTEXT6_CNTL;
  break;
 case 7:
  pt_start_lo_id = REG_GCVM_CONTEXT7_PAGE_TABLE_START_ADDR_LO32;
  pt_start_hi_id = REG_GCVM_CONTEXT7_PAGE_TABLE_START_ADDR_HI32;
  pt_end_lo_id   = REG_GCVM_CONTEXT7_PAGE_TABLE_END_ADDR_LO32;
  pt_end_hi_id   = REG_GCVM_CONTEXT7_PAGE_TABLE_END_ADDR_HI32;
  pt_base_lo_id  = REG_GCVM_CONTEXT7_PAGE_TABLE_BASE_ADDR_LO32;
  pt_base_hi_id  = REG_GCVM_CONTEXT7_PAGE_TABLE_BASE_ADDR_HI32;
  ctx_cntl_id    = REG_GCVM_CONTEXT7_CNTL;
  break;
 case 8:
  pt_start_lo_id = REG_GCVM_CONTEXT8_PAGE_TABLE_START_ADDR_LO32;
  pt_start_hi_id = REG_GCVM_CONTEXT8_PAGE_TABLE_START_ADDR_HI32;
  pt_end_lo_id   = REG_GCVM_CONTEXT8_PAGE_TABLE_END_ADDR_LO32;
  pt_end_hi_id   = REG_GCVM_CONTEXT8_PAGE_TABLE_END_ADDR_HI32;
  pt_base_lo_id  = REG_GCVM_CONTEXT7_PAGE_TABLE_BASE_ADDR_LO32;
  pt_base_hi_id  = REG_GCVM_CONTEXT7_PAGE_TABLE_BASE_ADDR_HI32;
  ctx_cntl_id    = REG_GCVM_CONTEXT7_CNTL;
  break;
 case 9:
  pt_start_lo_id = REG_GCVM_CONTEXT9_PAGE_TABLE_START_ADDR_LO32;
  pt_start_hi_id = REG_GCVM_CONTEXT9_PAGE_TABLE_START_ADDR_HI32;
  pt_end_lo_id   = REG_GCVM_CONTEXT9_PAGE_TABLE_END_ADDR_LO32;
  pt_end_hi_id   = REG_GCVM_CONTEXT9_PAGE_TABLE_END_ADDR_HI32;
  pt_base_lo_id  = REG_GCVM_CONTEXT7_PAGE_TABLE_BASE_ADDR_LO32;
  pt_base_hi_id  = REG_GCVM_CONTEXT7_PAGE_TABLE_BASE_ADDR_HI32;
  ctx_cntl_id    = REG_GCVM_CONTEXT7_CNTL;
  break;
 case 10:
  pt_start_lo_id = REG_GCVM_CONTEXT10_PAGE_TABLE_START_ADDR_LO32;
  pt_start_hi_id = REG_GCVM_CONTEXT10_PAGE_TABLE_START_ADDR_HI32;
  pt_end_lo_id   = REG_GCVM_CONTEXT10_PAGE_TABLE_END_ADDR_LO32;
  pt_end_hi_id   = REG_GCVM_CONTEXT10_PAGE_TABLE_END_ADDR_HI32;
  pt_base_lo_id  = REG_GCVM_CONTEXT10_PAGE_TABLE_BASE_ADDR_LO32;
  pt_base_hi_id  = REG_GCVM_CONTEXT10_PAGE_TABLE_BASE_ADDR_HI32;
  ctx_cntl_id    = REG_GCVM_CONTEXT10_CNTL;
  break;
 case 11:
  pt_start_lo_id = REG_GCVM_CONTEXT11_PAGE_TABLE_START_ADDR_LO32;
  pt_start_hi_id = REG_GCVM_CONTEXT11_PAGE_TABLE_START_ADDR_HI32;
  pt_end_lo_id   = REG_GCVM_CONTEXT11_PAGE_TABLE_END_ADDR_LO32;
  pt_end_hi_id   = REG_GCVM_CONTEXT11_PAGE_TABLE_END_ADDR_HI32;
  pt_base_lo_id  = REG_GCVM_CONTEXT11_PAGE_TABLE_BASE_ADDR_LO32;
  pt_base_hi_id  = REG_GCVM_CONTEXT11_PAGE_TABLE_BASE_ADDR_HI32;
  ctx_cntl_id    = REG_GCVM_CONTEXT11_CNTL;
  break;
 case 12:
  pt_start_lo_id = REG_GCVM_CONTEXT12_PAGE_TABLE_START_ADDR_LO32;
  pt_start_hi_id = REG_GCVM_CONTEXT12_PAGE_TABLE_START_ADDR_HI32;
  pt_end_lo_id   = REG_GCVM_CONTEXT12_PAGE_TABLE_END_ADDR_LO32;
  pt_end_hi_id   = REG_GCVM_CONTEXT12_PAGE_TABLE_END_ADDR_HI32;
  pt_base_lo_id  = REG_GCVM_CONTEXT12_PAGE_TABLE_BASE_ADDR_LO32;
  pt_base_hi_id  = REG_GCVM_CONTEXT12_PAGE_TABLE_BASE_ADDR_HI32;
  ctx_cntl_id    = REG_GCVM_CONTEXT12_CNTL;
  break;
 case 13:
  pt_start_lo_id = REG_GCVM_CONTEXT13_PAGE_TABLE_START_ADDR_LO32;
  pt_start_hi_id = REG_GCVM_CONTEXT13_PAGE_TABLE_START_ADDR_HI32;
  pt_end_lo_id   = REG_GCVM_CONTEXT13_PAGE_TABLE_END_ADDR_LO32;
  pt_end_hi_id   = REG_GCVM_CONTEXT13_PAGE_TABLE_END_ADDR_HI32;
  pt_base_lo_id  = REG_GCVM_CONTEXT13_PAGE_TABLE_BASE_ADDR_LO32;
  pt_base_hi_id  = REG_GCVM_CONTEXT13_PAGE_TABLE_BASE_ADDR_HI32;
  ctx_cntl_id    = REG_GCVM_CONTEXT13_CNTL;
  break;
 case 14:
  pt_start_lo_id = REG_GCVM_CONTEXT14_PAGE_TABLE_START_ADDR_LO32;
  pt_start_hi_id = REG_GCVM_CONTEXT14_PAGE_TABLE_START_ADDR_HI32;
  pt_end_lo_id   = REG_GCVM_CONTEXT14_PAGE_TABLE_END_ADDR_LO32;
  pt_end_hi_id   = REG_GCVM_CONTEXT14_PAGE_TABLE_END_ADDR_HI32;
  pt_base_lo_id  = REG_GCVM_CONTEXT14_PAGE_TABLE_BASE_ADDR_LO32;
  pt_base_hi_id  = REG_GCVM_CONTEXT14_PAGE_TABLE_BASE_ADDR_HI32;
  ctx_cntl_id    = REG_GCVM_CONTEXT14_CNTL;
  break;
 case 15:
  pt_start_lo_id = REG_GCVM_CONTEXT15_PAGE_TABLE_START_ADDR_LO32;
  pt_start_hi_id = REG_GCVM_CONTEXT15_PAGE_TABLE_START_ADDR_HI32;
  pt_end_lo_id   = REG_GCVM_CONTEXT15_PAGE_TABLE_END_ADDR_LO32;
  pt_end_hi_id   = REG_GCVM_CONTEXT15_PAGE_TABLE_END_ADDR_HI32;
  pt_base_lo_id  = REG_GCVM_CONTEXT15_PAGE_TABLE_BASE_ADDR_LO32;
  pt_base_hi_id  = REG_GCVM_CONTEXT15_PAGE_TABLE_BASE_ADDR_HI32;
  ctx_cntl_id    = REG_GCVM_CONTEXT15_CNTL;
  break;
 default: HDB_ASSERT(false, "Out of range VMID 0-15 trying to access %u", vmid);
 }

 // all the types of the contexts are the same so will just use 0 but pass the correct
 // register enum to the read function
 reg_gcvm_context0_page_table_start_addr_lo32_t pt_start_lo = { 0 };
 reg_gcvm_context0_page_table_start_addr_hi32_t pt_start_hi = { 0 };
 reg_gcvm_context0_page_table_end_addr_lo32_t   pt_end_lo   = { 0 };
 reg_gcvm_context0_page_table_end_addr_hi32_t   pt_end_hi   = { 0 };
 reg_gcvm_context0_page_table_base_addr_lo32_t  pt_base_lo  = { 0 };
 reg_gcvm_context0_page_table_base_addr_hi32_t  pt_base_hi  = { 0 };
 reg_gcvm_context0_cntl_t                       ctx_cntl    = { 0 };

 dev_op_reg32(dev, pt_start_lo_id, ioc_data, REG_OP_READ, &amp;amp;pt_start_lo.raw);
 dev_op_reg32(dev, pt_start_hi_id, ioc_data, REG_OP_READ, &amp;amp;pt_start_hi.raw);
 dev_op_reg32(dev, pt_end_lo_id, ioc_data, REG_OP_READ, &amp;amp;pt_end_lo.raw);
 dev_op_reg32(dev, pt_end_hi_id, ioc_data, REG_OP_READ, &amp;amp;pt_end_hi.raw);
 dev_op_reg32(dev, pt_base_lo_id, ioc_data, REG_OP_READ, &amp;amp;pt_base_lo.raw);
 dev_op_reg32(dev, pt_base_hi_id, ioc_data, REG_OP_READ, &amp;amp;pt_base_hi.raw);
 dev_op_reg32(dev, ctx_cntl_id, ioc_data, REG_OP_READ, &amp;amp;ctx_cntl.raw);

 u64 pt_start_addr = ((u64)pt_start_lo.raw &amp;lt;&amp;lt; 12) | ((u64)pt_start_hi.raw &amp;lt;&amp;lt; 44);
 u64 pt_end_addr   = ((u64)pt_end_lo.raw &amp;lt;&amp;lt; 12) | ((u64)pt_end_hi.raw &amp;lt;&amp;lt; 44);
 u64 pt_base_addr  = ((u64)pt_base_lo.raw &amp;lt;&amp;lt; 0) | ((u64)pt_base_hi.raw &amp;lt;&amp;lt; 32);
 u32 pt_depth      = ctx_cntl.page_table_depth;
 u32 ptb_size      = ctx_cntl.page_table_block_size;

 HDB_ASSERT(pt_base_addr != 0xffffffffffffffffull, "Invalid page table base addr");

 printf(
   "\tPage Table Start: 0x%lx\n\tPage Table End: 0x%lx\n\tPage Table Base: "
   "0x%lx\n\tPage Table Depth: %u\n\tBlock Size: %u\n",
   pt_start_addr,
   pt_end_addr,
   pt_base_addr,
   pt_depth,
   ptb_size);

 // decode base PDB
 pde_t pde = decode_pde(pt_base_addr);
 pt_base_addr -= fb_offset * !pde.system; // substract only on vram

 u64 pt_last_byte_addr = pt_end_addr + 0xfff; // 0xfff is 1 page
 HDB_ASSERT(
   pt_start_addr &amp;lt;= va_addr || va_addr &amp;lt; pt_last_byte_addr,
   "Invalid virtual address outside the range of the root page table of this vm");

 va_addr -= pt_start_addr;
 //
 // Size of the first PDB depends on the total coverage of the
 // page table and the PAGE_TABLE_BLOCK_SIZE.
 // Entire table takes ceil(log2(total_vm_size)) bits
 // All PDBs except the first one take 9 bits each
 // The PTB covers at least 2 MiB (21 bits)
 // And PAGE_TABLE_BLOCK_SIZE is log2(num 2MiB ranges PTB covers)
 // As such, the formula for the size of the first PDB is:
 //                       PDB1, PDB0, etc.      PTB covers at least 2 MiB
 //                                        Block size can make it cover more
 //   total_vm_bits - (9 * num_middle_pdbs) - (page_table_block_size + 21)
 //
 // we need the total range range here not the last byte addr like above
 u32 total_vaddr_bits = log2_range_round_up(pt_start_addr, pt_end_addr + 0x1000);

 u32 total_pdb_bits = total_vaddr_bits;
 // substract everything from the va_addr to leave just the pdb bits
 total_pdb_bits -= 9 * (pt_depth - 1); // middle PDBs each is 9 bits
 total_pdb_bits -= (ptb_size + 21);    // at least 2mb(21) bits + ptb_size

 // u64 va_mask = (1ull &amp;lt;&amp;lt; total_pdb_bits) - 1;
 // va_mask &amp;lt;&amp;lt;= (total_vaddr_bits - total_pdb_bits);

 // pde_t pdes[8]  = { 0 };
 // u32   curr_pde = 0;
 // u64   pde_addr = 0;
 // u64  loop_pde = pt_base_addr;

 if (pt_depth == 0) { HDB_ASSERT(false, "DEPTH = 0 is not implemented yet"); }

 pde_t curr_pde    = pde;
 u64   entry_bits  = 0;
 s32   curr_depth  = pt_depth;
 bool  pde0_is_pte = false;
 // walk all middle PDEs
 while (curr_depth &amp;gt; 0) {
  // printf("pde(%u):0x%lx \n", curr_depth, curr_pde.pte_base_addr);
  u64 next_entry_addr = 0;

  u32 shift_amount = total_vaddr_bits;
  shift_amount -= total_pdb_bits;
  // for each pdb shift 9 more
  shift_amount -= ((pt_depth - curr_depth) * 9);

  // shift address and mask out unused bits
  u64 next_pde_idx = va_addr &amp;gt;&amp;gt; shift_amount;
  next_pde_idx &amp;amp;= 0x1ff;

  // if on vram we need to apply this offset
  if (!curr_pde.system) curr_pde.pte_base_addr -= fb_offset;

  next_entry_addr = curr_pde.pte_base_addr + next_pde_idx * 8;
  curr_depth--;

  if (!curr_pde.system) {
   dev_linear_vram(dev, next_entry_addr, 8, &amp;amp;entry_bits);
   curr_pde = decode_pde(entry_bits);
   printf(
     "\tPage Dir Entry(%u):\n\t  Addr:0x%lx\n\t  Base: 0x%lx\n\n\t        ↓\n\n",
     curr_depth,
     next_entry_addr,
     curr_pde.pte_base_addr);
  } else {
   HDB_ASSERT(false, "GTT physical memory access is not implemented yet");
  }

  if (!curr_pde.valid) { break; }

  if (curr_pde.pte) {
   // PDB0 can act as a pte
   // also I'm making an assumption here that UMRs code doesn't make
   // that the the PDB0 as PTE path can't have the further bit set
   pde0_is_pte = true;
   break;
  }
 }

 if (pde0_is_pte) { HDB_ASSERT(false, "PDE0 as PTE is not implemented yet"); }

 // page_table_block_size is the number of 2MiB regions covered by a PTB
 // If we set it to 0, then PTB cover 2 MiB
 // If it's 9 PTB cover 1024 MiB
 // pde0_block_fragment_size tells us how many 4 KiB regions each PTE covers
 // If it's 0 PTEs cover 4 KiB
 // If it's 9 PTEs cover 2 MiB
 // So the number of PTEs in a PTB is 2^(9+ptbs-pbfs)
 //
 // size here is actually the log_2 of the size
 u32 pte_page_size  = curr_pde.frag_size;
 u32 ptes_per_ptb   = 9 + ptb_size - pte_page_size;
 u64 pte_index_mask = (1ul &amp;lt;&amp;lt; ptes_per_ptb) - 1;

 u32 pte_bits_count   = pte_page_size + 12;
 u64 page_offset_mask = (1ul &amp;lt;&amp;lt; pte_bits_count) - 1; // minimum of 12

 u64 pte_index = (va_addr &amp;gt;&amp;gt; pte_bits_count) &amp;amp; pte_index_mask;
 u64 pte_addr  = curr_pde.pte_base_addr + pte_index * 8;

 pte_t pte = { 0 };
 if (!curr_pde.system) {
  dev_linear_vram(dev, pte_addr, 8, &amp;amp;entry_bits);
  pte = decode_pte(entry_bits);

  printf("\tPage Table Entry: 0x%lx\n", pte.page_base_addr);
 } else {
  HDB_ASSERT(false, "GTT physical memory access is not implemented yet");
 }

 if (pte.further) { HDB_ASSERT(false, "PTE as PDE walking is not implemented yet"); }
 if (!pte.system) pte.page_base_addr -= fb_offset;

 u64 offset_in_page = va_addr &amp;amp; page_offset_mask;
 u64 physical_addr  = pte.page_base_addr + offset_in_page;
 printf("\tFinal Physical Address: 0x%lx\n", physical_addr);
}&lt;/code&gt;
    &lt;head rend="h2"&gt;Footnotes&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Other processes need to have a s_trap instruction or have trap on exception flags set, which is not true for most normal GPU processes. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Available since RDNA3, if I’m not mistaken. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;VGPRs are unique per thread, and SGPRs are unique per wave ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;We can get that by subtracting the current program counter from the address of the code buffer. ↩&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46193931</guid><pubDate>Mon, 08 Dec 2025 16:06:14 +0000</pubDate></item><item><title>How the Creator Economy Destroyed the Internet</title><link>https://www.theverge.com/cs/features/810002/influencers-creator-economy-special-series</link><description>&lt;doc fingerprint="39d49219bc35d714"&gt;
  &lt;main&gt;
    &lt;head rend="h4"&gt;A special series from The Verge&lt;/head&gt;
    &lt;p&gt;Itâs fashionable to talk about the creator economy like itâs a new thing, but the harsh reality is that the creator economy is the media and internet economy now. Platforms like TikTok, Instagram, and YouTube generate billions of dollars in revenue in ways that have created some of the richest companies in the history of the world, and they so dominate culture that the future of politics often looks more like creator drama than actual policymaking.&lt;/p&gt;
    &lt;p&gt;But all of that money and influence is based on a deeply unsustainable foundation: These platforms all put algorithms between creators and their audiences while paying them essentially nothing, creating an endless race to go viral in service of sponsorships and brand deals. There are not many industries where itâs more valuable to stop selling bits and start selling atoms, but every major creator eventually pivots to selling products because itâs more lucrative than chasing views and brand deals. The Jake and Logan Paul brothers sell literal bottled water now, and earlier this year financial documents revealed that the YouTube arm of the MrBeast empire has spent three straight years in the red, including a whopping negative $110 million in 2024. All of those viral videos are just a marketing front for the real MrBeast business: a line of chocolate bars, available at your local Walmart.&lt;/p&gt;
    &lt;p&gt;This is the media ecosystem we live in now â a supercharged shopping system that thrives on outrage, dominates the culture, and resists any real scrutiny because no oneâs really in charge, and another generation of creators is always there to exploit. Itâs not just coming. Itâs already here.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Editors: Kevin Nguyen &amp;amp; Sarah Jeong&lt;/item&gt;
      &lt;item&gt;Engagement: Esther Cohen &amp;amp; Tristan Cooper&lt;/item&gt;
      &lt;item&gt;Creative Director: Kristen Radtke&lt;/item&gt;
      &lt;item&gt;Managing Editor: Kara Verlaney&lt;/item&gt;
      &lt;item&gt;Engineer: Graham MacAree&lt;/item&gt;
      &lt;item&gt;Copy Editor: Kallie Plagge&lt;/item&gt;
      &lt;item&gt;Fact Checker: Becca Laurie&lt;/item&gt;
      &lt;item&gt;Editor-in-Chief: Nilay Patel&lt;/item&gt;
      &lt;item&gt;Publisher: Helen Havlak&lt;/item&gt;
      &lt;item&gt;Hub Concept: Victoria Barrios&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46194302</guid><pubDate>Mon, 08 Dec 2025 16:32:12 +0000</pubDate></item><item><title>Let's put Tailscale on a jailbroken Kindle</title><link>https://tailscale.com/blog/tailscale-jailbroken-kindle</link><description>&lt;doc fingerprint="77000a1e01fd3fdc"&gt;
  &lt;main&gt;
    &lt;p&gt;“It’s a rite of passage to run Tailscale on weird devices.”&lt;/p&gt;
    &lt;p&gt;So writes Mitanshu Sukhwani on his blog, detailing the steps for getting Tailscale onto a jailbroken Kindle. Getting there, and seeing a kindle entry with a satisfying green dot in your Tailscale admin console, takes some doing. But take the trip, and you’ll end up with an e-reader that can run some neat unofficial apps, and is more open to third-party and DRM-free ebooks. And with a Tailscale connection, it’s easier to connect to files and a command line on your underpowered little Linux slab.&lt;/p&gt;
    &lt;p&gt;“For me, it's the freedom of being able to do anything with the device I own,” Sukhwani writes by email. “What I can do with the freedom is a different story.”&lt;/p&gt;
    &lt;head rend="h2"&gt;What is a jailbroken Kindle, exactly?&lt;/head&gt;
    &lt;p&gt;Jailbreaking refers to removing the software restrictions on a device put there by its maker. Getting around these restrictions, typically by gaining “root” or administrative access, allows for accessing operating system internals, running unapproved software, and generally doing more things than a manufacturer intended. With the Kindle, you still get the standard Kindle reading experience, including Amazon's store and the ability to send the Kindle books from apps like Libby. You just add many more options, too.&lt;/p&gt;
    &lt;p&gt;The term gained purchase after the first iPhone’s debut in mid-2007; since then, nearly every device with a restricted environment has gained its own jailbreaking scene, including Kindles (debuting five months after the iPhone).&lt;/p&gt;
    &lt;p&gt;Kindle jailbreaks come along every so often. Right now, an unlocking scheme based on Amazon’s own lockscreen ads, “AdBreak,” is available for all but the most up-to-date Kindles (earlier than firmware version 5.18.5.0.2). I know this because I wrote this paragraph and the next on my 11th-generation Kindle, using the open-source Textadept editor, a Bluetooth keyboard, and Tailscale to move this draft file around.&lt;/p&gt;
    &lt;p&gt;One paragraph doesn’t seem that impressive until you consider that on a standard Kindle, you cannot do any of that. Transferring files by SSH, or Taildrop, is certainly not allowed. And that’s in addition to other upgrades you can get by jailbreaking a Kindle, including the feature-rich, customizable e-reader KOReader, and lots of little apps available in repositories like KindleForge.&lt;/p&gt;
    &lt;p&gt;If your Kindle has been connected to Wi-Fi all this time (as of early December 2025), it may have automatically updated itself and no longer be ready for jailbreaking. If you think it still has a chance, immediately put it into airplane mode and follow along.&lt;/p&gt;
    &lt;p&gt;Obligatory notice here: You’re running a risk of bricking your device (having it become unresponsive and unrecoverable) and voiding your warranty when you do this. That having been noted, let's dig further.&lt;/p&gt;
    &lt;head rend="h2"&gt;What Tailscale adds to a jailbroken Kindle&lt;/head&gt;
    &lt;p&gt;Tailscale isn’t necessary on a jailbroken Kindle, but it really helps. Here are some of the ways Tailscale makes messing about with an opened-up Kindle more fun:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A persistent IP address (100.xx.yyy.zzz), just like any other Tailscale device, instead of having to remember yet another 192.168.random.number&lt;/item&gt;
      &lt;item&gt;Easier SSH access with magicDNS: ssh root@kindle and you’re in&lt;/item&gt;
      &lt;item&gt;Taildrop for sending files to whatever Kindle directory you want&lt;/item&gt;
      &lt;item&gt;Setting up a self-hosted Calibre Web library with Tailscale, then securely grabbing books from it anywhere with KOReader.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Key to the Kindle-plus-Tailscale experience is an easier way (SSH and Taildrop) to get epub, mobi, and other e-book and document formats into the /documents folder, ready for your KOReader sessions. Tailscale also helps with setting up some of the key jailbreak apps, saving you from plugging and unplugging the Kindle into a computer via USB cord (and then finding a second USB cord, because the first one never works, for some reason).&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting your Kindle ready&lt;/head&gt;
    &lt;p&gt;What follows is by no means a comprehensive guide to jailbreaking and accessing your Kindle. You will want to read the documentation for each tool and app closely. Pay particular attention to which Kindle you have, which version number of the Kindle firmware it’s running, and how much space you have left on that device.&lt;/p&gt;
    &lt;p&gt;The first step is to check your Kindle’s version number (Settings &amp;gt; Device info) and see if there is a jailbreak method available for it. The Kindle Modding Wiki is the jailbreaking community’s go-to resource. As of this writing, there is a “WinterBreak” process available for Kindles running firmware below 15.18.1, and AdBreak is available for firmwares from 15.18.1 through 5.18.5.0.1.&lt;/p&gt;
    &lt;p&gt;If your Kindle’s version number fits one of those ranges, put it in Airplane mode and move on. If not, you’re going to have to wait until the next jailbreak method comes along.&lt;/p&gt;
    &lt;head rend="h2"&gt;The actual jailbreaking part&lt;/head&gt;
    &lt;p&gt;Before you dive in, have a computer (PC, Mac, or Linux) and USB cable that works with your Kindle handy. Have your Kindle on reliable Wi-Fi, like your home network—but don’t take your Kindle off airplane mode if you’ve been keeping it that way.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Follow these steps to jailbreak your Kindle. The techniques are different, but you may need to do some other tasks, like enable advertisements, or fill your Kindle with junk files to prevent automatic updates midway through the process.&lt;/item&gt;
      &lt;item&gt;Install a hotfix and disable over-the-air updates so that you can keep your Kindle on Wi-Fi and not have its jailbreak undone&lt;/item&gt;
      &lt;item&gt;Install the Kindle Unified Application Launcher (KUAL) and MRPI (MobileRead Package Installer). KUAL is vital to installing most jailbroken apps, including Tailscale.&lt;/item&gt;
      &lt;item&gt;You will almost certainly want to install KOReader, too.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Those bits above are standard jailbreaking procedures. If you want Tailscale on your Kindle, you’ll go a bit further.&lt;/p&gt;
    &lt;head rend="h2"&gt;Adding Tailscale to a jailbroken Kindle&lt;/head&gt;
    &lt;p&gt;Make sure you have KUAL and MRPI installed and working. Next up: install this “simple” version of USBNetworking for Kindle.&lt;/p&gt;
    &lt;p&gt;Before you go further, you’ll want to choose between Mitanshu’s “standard” Tailscale repository, or the fork of it that enables Taildrop. I recommend the Taildrop-enabled fork; if it goes wrong, or stops being updated, it’s fairly easy (relative to doing this kind of project) to wipe it and go back to Mitanshu’s “vanilla” version.Either way, you’ll want to get USB access to your Kindle for this next part. If you toggled on USBNetworking to try it out, toggle it off; you can’t get USB access while it’s running, as its name somewhat implies.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Download the Tailscale/KUAL repository of your choice using git clone or download a ZIP from the Code button on GitHub&lt;/item&gt;
      &lt;item&gt;Head to Tailscale’s page of static Linux binaries and grab the latest arm (not arm64) release&lt;/item&gt;
      &lt;item&gt;Copy the tailscale and tailscaled binaries from the Tailscale download and place them into the /extensions/tailscale/bin directory of the KUAL/Kindle repository you’ll be copying over&lt;/item&gt;
      &lt;item&gt;Head to your Tailscale admin console and generate an authentication key. Name it something like kindle; you’ll want to enable the “Reusable” and “Pre-approved” options. Copy the key that is generated.&lt;/item&gt;
      &lt;item&gt;Open the file extensions/tailscale/config/auth_key.txt for editing while it is on your (non-Kindle) computer. Paste in the key text you generated.&lt;/item&gt;
      &lt;item&gt;If you’re using the variant with Taildrop, you can set a custom directory in which to deliver Taildrop files by editing extensions/tailscale/config/taildrop_dir.txt; setting /mnt/us/documents makes sense if you’re mostly sending yourself things to read in KOReader.&lt;/item&gt;
      &lt;item&gt;Head into the extensions folder on your computer and copy the tailscale folder you’ve set up into the extensions folder on your Kindle.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With all that done, open up KUAL on your Kindle. Go into USBNetLite and click USBNetwork Status to ensure it is enabled (tap the Toggle button if not). Go back (with the “/” button at the bottom), tap Tailscale, and first tap Start Tailscaled (note the “d” at the end). Wait about 10 seconds to give the Tailscaled daemon time to start, then tap Start Tailscale.&lt;/p&gt;
    &lt;p&gt;If everything is settled, you should be able to see your Kindle as connected on your Tailscale admin console. Once you’ve finished smiling to yourself, click the three dots on the right-hand side of the Kindle row and select “Disable key expiry.” In most situations, you’re better off not having to patch a new key value into a Kindle text file every few months.&lt;/p&gt;
    &lt;head rend="h2"&gt;Enjoy your (slightly) less wonky Kindle&lt;/head&gt;
    &lt;p&gt;With Tailscale installed, it’s easier to get into your Kindle via SSH for file management and installing and configuring other apps. Getting a Bluetooth keyboard to work via the Kindle’s quirky command-line Bluetooth interface would not have been fun using a touchscreen keyboard.&lt;/p&gt;
    &lt;p&gt;Because the Kindle is on your tailnet, it can access anything else you have hosted there. Kindles set up this way can use tools like the Shortcut Browser to become dashboards for Home Assistant, or access a self-hosted Calibre-Web e-book server (with some tweaking).&lt;/p&gt;
    &lt;p&gt;Having Taildrop handy, and having it drop files directly into the documents folder, is probably my favorite upgrade. I was on my phone, at a train station, when I came across Annalee Newitz’s Automatic Noodle at Bookshop.org. I bought it on my phone and downloaded the DRM-free epub file. When I got home, I opened and unlocked my Kindle, sent the epub to the Kindle via Taildrop, then tapped Receive Taildrop Files in the Tailscale app inside KUAL. Epubs, PDFs, comic book archives, DjVu files—they’re all ready to be dropped in.&lt;/p&gt;
    &lt;p&gt;If you’ve gotten Tailscale to run on weird (or just uncommon) devices, we’d more than love to hear about it. Let us know on Reddit, Discord, Bluesky, Mastodon, or LinkedIn.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46194337</guid><pubDate>Mon, 08 Dec 2025 16:34:08 +0000</pubDate></item><item><title>Hunting for North Korean Fiber Optic Cables</title><link>https://nkinternet.com/2025/12/08/hunting-for-north-korean-fiber-optic-cables/</link><description>&lt;doc fingerprint="2fecda19652117ea"&gt;
  &lt;main&gt;
    &lt;p&gt;Before we go any further, one thing that I want to make clear is that the word assume is going to be doing some heavy lifting throughout this post. This was a rabbit hole that I recently went down and I probably have more questions than answers, but I still wanted to document what I had found so far. If you have additional information or findings you want to share, as always feel free to reach out: contact@nkinternet.com.&lt;/p&gt;
    &lt;p&gt;It all started with a PowerPoint that I came across a few weeks ago. It was presented by the DPRK to the ICAO on the state of their aviation industry and their ADS-B deployment inside North Korea. However, one slide in particular caught my eye because it showed a fiber optic cable running across the country&lt;/p&gt;
    &lt;p&gt;This got me wondering more about the physical layout of the network inside North Korea. From the map we know that there’s a connection between Pyongyang and Odaejin, although given the mountains in the middle of the country it probably isn’t a direct link. There isn’t a lot of information on fiber in North Korea, but there are a few outside sources that help provide clues about how things might be laid out.&lt;/p&gt;
    &lt;p&gt;Historic Fiber Information&lt;/p&gt;
    &lt;p&gt;38North first reported the connection from Russia’s TTK to the DPRK over the Korea–Russia Friendship Bridge back in 2017. Additionally, a picture found on Flickr looking toward Tumangang after the bridge doesn’t show any utility poles and instead seems to display some kind of infrastructure in the grass to the side of the tracks. Assuming this interpretation is correct, the fiber is likely buried underground as it enters the country and passes through the vicinity of Tumangang Station.&lt;/p&gt;
    &lt;p&gt;According to a report from The Nautilus Institute we can gather a few additional details about the internet inside North Korea&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;One of the first lines was installed in September 1995 between Pyongyang and Hamhung&lt;/item&gt;
      &lt;item&gt;In February 1998 a link between Pyongyang and Sinuiju was completed&lt;/item&gt;
      &lt;item&gt;As of 2000, DPRK’s operational optical fiber telecom lines included: Pyongyang – Hamhung; Pyongyang – Sinuiju including all cities and counties in North Pyongan Province; Hamhung Rajin-Sonbong; Rajin-Songbong – Hunchun (China), Pyongyang – Nampo.&lt;/item&gt;
      &lt;item&gt;In 2003 the original domestic cell phone network was built for North Korean citizens in Pyongyang, Namp’o, reportedly in all provincial capitals, on the Pyongyang-Myohyangsan tourist highway, and the Pyongyang-Kaesong and Wonsan-Hamhung highways&lt;/item&gt;
      &lt;item&gt;The Kwangmyong network’s data is transmitted via fiber optic cable with a backbone capacity of 2.5 GB per second between all the provinces.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Based on these notes, it starts to paint a picture that the fiber link coming from Russia likely travels down the east coast of the DPRK before connecting to Pyongyang. Several city pairs—Pyongyang–Hamhung and Rajin–Sonbong—line up with earlier deployments of east-coast fiber infrastructure.&lt;/p&gt;
    &lt;p&gt;Kwangmyong Internal Topology&lt;/p&gt;
    &lt;p&gt;The report also notes that all of the provinces in North Korea were connected to the Kwangmyong via fiber. The Kwangmyong for those not familiar is the intranet that most citizens in the DPRK can access as they do not have access to the outside internet. While not much information is available about the Kwangmyong, these notes from Choi Sung, Professor of Computer Science at Namseoul University provides some additional details on how the network is laid how, as well as information on the regional networks that are connected. A map provided in his notes shows some of the main points of the Kwangmyong with three of them located along the northeast of North Korea.&lt;/p&gt;
    &lt;p&gt;Railways, Roads, and Practical Fiber Routing&lt;/p&gt;
    &lt;p&gt;This starts to paint a rough picture of how the network is physically deployed in North Korea but we can also look to some outside sources to get some confirmation. 38North once again provides some great detail on cell phone towers in North Korea. The interesting thing being an apparent line down the east coast which follows major roads and highways but would also in theory have easier access to the fiber back haul to support the cell network.&lt;/p&gt;
    &lt;p&gt;All of this seems to suggest that the fiber lines were run along major roads and railways up the east coast. A map from Beyond Parallel shows the major rail lines, which has the Pyongra line up the east coast.&lt;/p&gt;
    &lt;p&gt;Looking For Clues Along the Railway&lt;/p&gt;
    &lt;p&gt;Some additional digging for pictures from along the line suggest that there is infrastructure deployed along the tracks, although it’s difficult to confirm from pictures exactly what is buried. The following shows what appears to be a junction box at the base of a pole along the line.&lt;/p&gt;
    &lt;p&gt;The line does have a path along it as well with mile markers. While it is used by bikes and pedestrians, it provides a nice path for supporting fiber and other communications runs along the tracks.&lt;/p&gt;
    &lt;p&gt;The Pyongra line also crosses through the mountains at points but it is assumed at certain junctions the fiber was laid along the AH 6/National Highway 7 up the coast as there are parts of the line discovered that do not have a path along the tracks. In these places it is assumed they follow the road, although finding pictures of the highway to further examine is challenging.&lt;/p&gt;
    &lt;p&gt;Lastly at certain stations we can see utility boxes along the side of the track suggesting buried conduits/cables are laid along the tracks.&lt;/p&gt;
    &lt;p&gt;From a video taken in 2012 there does appear to be some signs of objects along the tracks, although difficult to confirm due to the video quality. The screenshot below is the clearest I could find of a rectangular box buried in a clearing along the line.&lt;/p&gt;
    &lt;p&gt;Based on this information of what is confirmed and looking at major cities, it appears there is a route that follows Pyongyang → Wonsan → Hamhung → Chongjin → Rajin → Tumangang which follows the Pyongra line as well as the AH 6/National Highway 7 up the coast. The following map highlights a rough path.&lt;/p&gt;
    &lt;p&gt;Interestingly by mapping out the possible fiber locations we can start to draw conclusions based on other sources. According to a video by Cappy’s Army he proposes that when the US Navy Seals landed in NOrth Korea in 2019 the most likely place this would have occurred is Sinpo. As the goal was to depoy a covert listening device this could also line up with supporting the idea that a fiber backbone runs down the east coast of North Korea as Sinpo would be relatively close.&lt;/p&gt;
    &lt;p&gt;What Does This Mean For the Network?&lt;/p&gt;
    &lt;p&gt;In addition to the fiber link via Russia, the other fiber optic cable into North Korea comes in via China by way of Sinuiju and Dandong. Although we don’t know for sure where servers are deployed inside North Korea, based on the map of Kwangmyong the first assumption is that things are mainly centralized in Pyongyang.&lt;/p&gt;
    &lt;p&gt;Out of the 1,024 IPs assigned to North Korea we observe the following behavior based on the CIDR block:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;175.45.176.0/24 is exclusively routed via China Unicom&lt;/item&gt;
      &lt;item&gt;175.45.177.0/24 is exclusively routed via Russia TransTelekom&lt;/item&gt;
      &lt;item&gt;175.45.178.0/24 is dual-homed and can take either path before crossing into North Korea&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With this information in mind, running a traceroute with the TCP flag set gives us a slightly better look at how traffic behaves once it reaches the country. For the following tests we’re going to assume there is a fiber path on the west coming in from China toward Pyongyang, as well as a path on the east side coming from Russia.&lt;/p&gt;
    &lt;p&gt;From the US east coast to 175.45.176.71, the final hop in China before entering North Korea shows roughly 50 ms of additional latency before reaching the DPRK host. This suggests there may be extra devices, distance, or internal routing inside the country before the packet reaches its final destination.&lt;/p&gt;
    &lt;quote&gt;10 103.35.255.254 (103.35.255.254) 234.306 ms 234.082 ms 234.329 ms&lt;lb/&gt;11 * * *&lt;lb/&gt;12 * * *&lt;lb/&gt;13 * * *&lt;lb/&gt;14 175.45.176.71 (175.45.176.71) 296.081 ms 294.795 ms 294.605 ms&lt;lb/&gt;15 175.45.176.71 (175.45.176.71) 282.938 ms 284.446 ms 282.227 ms&lt;/quote&gt;
    &lt;p&gt;Interestingly, running a traceroute to 175.45.177.10 shows a similar pattern in terms of missing hops, but with much lower internal latency. In fact, the ~4 ms difference between the last Russian router and the DPRK host suggests the handoff between Russia and North Korea happens very close—network-wise—to where this device is located. This contrasts with the China path, which appears to take a longer or more complex route before reaching its final destination.&lt;/p&gt;
    &lt;quote&gt;10 188.43.225.153 185.192 ms 183.649 ms 189.089 ms&lt;lb/&gt;11 * *&lt;lb/&gt;12 * *&lt;lb/&gt;13 * *&lt;lb/&gt;14 175.45.177.10 195.996 ms 186.801 ms 186.353 ms&lt;lb/&gt;15 175.45.177.10 188.886 ms 201.103 ms 193.334&lt;/quote&gt;
    &lt;p&gt;If everything is centralized in Pyongyang this would mean the handoff from Russia is completed in Pyongyang as well. However, it could also indicate that 175.45.177.0/24 is not hosted in Pyongyang at all and is instead located closer to the Russia–North Korea border. More testing is definitely required however before any conclusions can be drawn about where these devices physically reside.&lt;/p&gt;
    &lt;p&gt;What can we learn from all of this?&lt;/p&gt;
    &lt;p&gt;Making some assumptions we can get a better idea of how the internet works and is laid out inside North Korea. While not much is officially confirmed using some other sources we can get a possible idea of how things work. As mentioned at the start, the word assume does a lot of heavy lifting. However if you do have other information or ideas feel free to reach out at contact@nkinternet.com&lt;/p&gt;
    &lt;head rend="h3"&gt;Discover more from North Korean Internet&lt;/head&gt;
    &lt;p&gt;Subscribe to get the latest posts sent to your email.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46194384</guid><pubDate>Mon, 08 Dec 2025 16:38:08 +0000</pubDate></item><item><title>A series of tricks and techniques I learned doing tiny GLSL demos</title><link>https://blog.pkh.me/p/48-a-series-of-tricks-and-techniques-i-learned-doing-tiny-glsl-demos.html</link><description>&lt;doc fingerprint="b2a292151eec032f"&gt;
  &lt;main&gt;
    &lt;p&gt;In the past two months or so, I spent some time making tiny GLSL demos. I wrote an article about the first one, Red Alp. There, I went into details about the whole process, so I recommend to check it out first if you're not familiar with the field.&lt;/p&gt;
    &lt;p&gt;We will look at 4 demos: Moonlight, Entrance 3, Archipelago, and Cutie. But this time, for each demo, we're going to cover one or two things I learned from it. It won't be a deep dive into every aspect because it would be extremely redundant. Instead, I'll take you along a journey of learning experiences.&lt;/p&gt;
    &lt;head rend="h2"&gt;Moonlight&lt;/head&gt;
    &lt;code&gt;// Moonlight [460] by bµg
// License: CC BY-NC-SA 4.0
void main(){vec3 o,p,u=vec3((P+P-R)/R.y,1),Q;Q++;for(float d,a,m,i,t;i++&amp;lt;1e2;p=t&amp;lt;7.2?Q:vec3(2,1,0),d=abs(d)*.15+.1,o+=p/m+(t&amp;gt;9.?d=9.,Q:p/d),t+=min(m,d))for(p=normalize(u)*t,p.z-=5e1,m=max(length(p)-1e1,.01),p.z+=T,d=5.-length(p.xy*=mat2(cos(t*.2+vec4(0,33,11,0)))),a=.01;a&amp;lt;1.;a+=a)p.xz*=mat2(8,6,-6,8)*.1,d-=abs(dot(sin(p/a*.6-T*.3),p-p+a)),m+=abs(dot(sin(p/a/5.),p-p+a/5.));o/=4e2;O=vec4(tanh(mix(vec3(-35,-15,8),vec3(118,95,60),o-o*length(u.xy*.5))*.01),1);}
&lt;/code&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;See it on its official page, or play with the code on its Shadertoy portage.&lt;/p&gt;
    &lt;p&gt;In Red Alp, I used volumetric raymarching to go through the clouds and fog, and it took quite a significant part of the code to make the absorption and emission convincing. But there is an alternative technique that is surprisingly simpler.&lt;/p&gt;
    &lt;p&gt;In the raymarching loop, the color contribution at each iteration becomes 1/d or c/d where d is the density of the material at the current ray position, and c an optional color tint if you don't want to work in grayscale level. Some variants exist, for example 1/d^2, but we'll focus on 1/d.&lt;/p&gt;
    &lt;head rend="h3"&gt;1/d explanation&lt;/head&gt;
    &lt;p&gt;Let's see how it looks in practice with a simple cube raymarch where we use this peculiar contribution:&lt;/p&gt;
    &lt;code&gt;void main() {
    float d, t;
    vec3 o, p,
         u = normalize(vec3(P+P-R,R.y)); // screen to world coordinate

    for (int i = 0; i &amp;lt; 30; i++) {
        p = u * t; // ray position

        p.z -= 3.; // take a step back

        // Rodriguez rotation with an arbitrary angle of π/2
        // and unaligned axis
        vec3 a = normalize(cos(T+vec3(0,2,4)));
        p = a*dot(a,p)-cross(a,p);

        // Signed distance function of a cube of size 1
        p = abs(p)-1.;
        d = length(max(p,0.)) + min(max(p.x,max(p.y,p.z)),0.);

        // Maxed out to not enter the solid
        d = max(d,.001);

        t += d; // stepping forward by that distance

        // Our mysterious contribution to the output
        o += 1./d;
    }

    // Arbitrary scale within visible range
    O = vec4(o/200., 1);
}
&lt;/code&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;The signed function of the cube is from the classic Inigo Quilez page. For the rotation you can refer to Xor or Blackle article. For the general understanding of the code, see my previous article on Red Alp.&lt;/p&gt;
    &lt;p&gt;The first time I saw it, I wondered whether it was a creative take, or if it was backed by physical properties.&lt;/p&gt;
    &lt;p&gt;Let's simplify the problem with the following figure:&lt;/p&gt;
    &lt;p&gt;The glowing object sends photons that spread all around it. The further we go from the object, the more spread these photons are, basically following the inverse square law 1/r^2, which gives the photons density, where r is the distance to the target object.&lt;/p&gt;
    &lt;p&gt;Let's say we send a ray and want to know how many photons are present along the whole path. We have to "sum", or rather integrate, all these photons density measures along the ray. Since we are doing a discrete sampling (the dots on the figure), we need to interpolate the photons density between each sampling point as well.&lt;/p&gt;
    &lt;p&gt;Given two arbitrary sampling points and their corresponding distance d_n and d_{n+1}, any intermediate distance can be linearly interpolated with r=\mathrm{mix}(d_n,d_{n+1},t) where t is within [0,1]. Applying the inverse square law from before (1/r^2), the integrated photons density between these 2 points can be expressed with this formula (in t range):&lt;/p&gt;
    &lt;p&gt;t being normalized, the \Delta t is here to covers the actual segment distance. With the help of Sympy we can do the integration:&lt;/p&gt;
    &lt;code&gt;&amp;gt;&amp;gt;&amp;gt; a, b, D, t = symbols('a b D t', real=True)
&amp;gt;&amp;gt;&amp;gt; mix = a*(1-t) + b*t
&amp;gt;&amp;gt;&amp;gt; D * integrate(1/mix**2, (t,0,1)).simplify()
 D
───
a⋅b
&lt;/code&gt;
    &lt;p&gt;So the result of this integration is:&lt;/p&gt;
    &lt;p&gt;Now the key is that in the loop, \Delta t stepping is actually d_{n+1}, so we end up with:&lt;/p&gt;
    &lt;p&gt;And we find back our mysterious 1/d. It's "physically correct", assuming vacuum space. Of course, reality is more complex, and we don't even need to stick to that formula, but it was nice figuring out that this simple fraction is a fairly good model of reality.&lt;/p&gt;
    &lt;head rend="h3"&gt;Going through the object&lt;/head&gt;
    &lt;p&gt;In the cube example we didn't go through the object, using &lt;code&gt;max(d, .001)&lt;/code&gt;. But
if we were to add some transparency, we could have used &lt;code&gt;d = A*abs(d)+B&lt;/code&gt;
instead, where &lt;code&gt;A&lt;/code&gt; could be interpreted as absorption and &lt;code&gt;B&lt;/code&gt; the pass-through,
or transparency.&lt;/p&gt;
    &lt;p&gt;I first saw this formula mentioned in Xor article on volumetric. To understand it a bit better, here is my intuitive take: the &lt;code&gt;+B&lt;/code&gt; causes a
potential penetration into the solid at the next iteration, which wouldn't
happen otherwise (or only very marginally). When inside the solid, the &lt;code&gt;abs(d)&lt;/code&gt;
causes the ray to continue further (by the amount of the distance to the closest
edge). Then the multiplication by &lt;code&gt;A&lt;/code&gt; makes sure we don't penetrate too fast
into it; it's the absorption, or "damping".&lt;/p&gt;
    &lt;p&gt;This is basically the technique I used in Moonlight to avoid the complex absorption/emission code.&lt;/p&gt;
    &lt;head rend="h2"&gt;Entrance 3&lt;/head&gt;
    &lt;code&gt;// Entrance 3 [465] by bµg
// License: CC BY-NC-SA 4.0
#define V for(s++;d&amp;lt;l&amp;amp;&amp;amp;s&amp;gt;.001;q=abs(p+=v*s)-45.,b=abs(p+vec3(mod(T*5.,80.)-7.,45.+sin(T*10.)*.2,12))-vec3(1,7,1),d+=s=min(max(p.y,-min(max(abs(p.y+28.)-17.,abs(p.z+12.)-4.),max(q.x,max(q.y,q.z)))),max(b.x,max(b.y,b.z))))
void main(){float d,s,r=1.7,l=2e2;vec3 b,v=b-.58,q,p=mat3(r,0,-r,-1,2,-1,b+1.4)*vec3((P+P-R)/R.y*20.4,30);V;r=exp(-d*d/1e4)*.2;l=length(v=-vec3(90,30,10)-p);v/=l;d=1.;V;r+=50.*d/l/l;O=vec4(pow(mix(vec3(0,4,9),vec3(80,7,2),r*r)*.01,p-p+.45),1);}
&lt;/code&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;See it on its official page, or play with the code on its Shadertoy portage.&lt;/p&gt;
    &lt;p&gt;This demo was probably one of the most challenging, but I'm pretty happy with its atmospheric vibe. It's kind of different than the usual demos for this size.&lt;/p&gt;
    &lt;p&gt;I initially tried with some voxels, but I couldn't make it work with the light under 512 characters (the initialization code was too large, not the branchless DDA stepping). It also had annoying limitations (typically the animation was unit bound), so I fell back to a classic raymarching.&lt;/p&gt;
    &lt;p&gt;The first thing I did differently was to use an L-∞ norm instead of an euclidean norm for the distance function: every solid is a cube so it's appropriate to use simpler formulas.&lt;/p&gt;
    &lt;p&gt;For the light, it's not an illusion, it's an actual light: after the first raymarch to a solid, the ray direction is reoriented toward the light and the march runs again (it's the &lt;code&gt;V&lt;/code&gt; macro). Hitting a solid or not defines if the
fragment should be lighten up or not.&lt;/p&gt;
    &lt;head rend="h3"&gt;Mobile bugs&lt;/head&gt;
    &lt;p&gt;A bad surprise of this demo was uncovering two driver bugs on mobile:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;One with tricky for-loop compounds on Snapdragon/Adreno because I was trying hard to avoid the macros and functions.&lt;/item&gt;
      &lt;item&gt;One with chained assignments on Imagination/PowerVR (typically affect Google Pixel Pro 10).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The first was worked around with the &lt;code&gt;V&lt;/code&gt; macro (actually saved 3 characters in
the process), but the 2nd one had to be unpacked and made me lose 2 characters.&lt;/p&gt;
    &lt;head rend="h3"&gt;Isometry&lt;/head&gt;
    &lt;p&gt;Another thing I studied was how to set up the camera in a non-perspective isometric or dimetric view. I couldn't make sense of the maths from the Wikipedia page (it just didn't work), but Sympy rescued me again:&lt;/p&gt;
    &lt;code&gt;# Counter-clockwise rotation
a, ax0, ax1, ax2 = symbols('a ax0:3')
c, s = cos(a), sin(a)
k = 1-c
rot = Matrix(3,3, [
    # col 1            col 2              # col 3
    ax0*ax0*k + c,     ax0*ax1*k + ax2*s, ax0*ax2*k - ax1*s, # row 1
    ax1*ax0*k - ax2*s, ax1*ax1*k + c,     ax1*ax2*k + ax0*s, # row 2
    ax2*ax0*k + ax1*s, ax2*ax1*k - ax0*s, ax2*ax2*k + c      # row 3
])

# Rotation by 45° on the y-axis
m45 = rot.subs({a:rad(-45), ax0:0, ax1:1, ax2:0})

# Apply the 2nd rotation on the x-axis to get the transform matrices for two
# classic projections
# Note: asin(tan(rad(30))) is the same as atan(sin(rad(45)))
isometric = m45 * rot.subs({a:asin(tan(rad(30))), ax0:1, ax1:0, ax2:0})
dimetric  = m45 * rot.subs({a:         rad(30),   ax0:1, ax1:0, ax2:0})
&lt;/code&gt;
    &lt;p&gt;Inspecting the matrices and factoring out the common terms, we obtain the following transform matrices:&lt;/p&gt;
    &lt;p&gt;The ray direction is common to all fragments, so we use the central UV coordinate (0,0) as reference point. We push it forward for convenience: (0,0,1), and transform it with our matrix. This gives the central screen coordinate in world space. Since the obtained point coordinate is relative to the world origin, to go from that point to the origin, we just have to flip its sign. The ray direction formula is then:&lt;/p&gt;
    &lt;p&gt;To get the ray origin of every other pixel, the remaining question is: what is the smallest distance we need to step back the screen coordinates such that, when applying the transformation, the view wouldn't clip into the ground at y=0.&lt;/p&gt;
    &lt;p&gt;This requirement can be modeled with the following expression:&lt;/p&gt;
    &lt;p&gt;The -1 being the lowest y-screen coordinate (which we don't want into the ground). The lazy bum in me just asks Sympy to solve it for me:&lt;/p&gt;
    &lt;code&gt;x, z = symbols("x z", real=True)
u = m * Matrix([x, -1, z])
uz = solve(u[1] &amp;gt; 0, z)
&lt;/code&gt;
    &lt;p&gt;We get z&amp;gt;\sqrt{2} for isometric, and z&amp;gt;\sqrt{3} for dimetric.&lt;/p&gt;
    &lt;p&gt;With an arbitrary scale &lt;code&gt;S&lt;/code&gt; of the coordinate we end up with the following:&lt;/p&gt;
    &lt;code&gt;const float S = 50.;
vec2 u = (P+P-R)/R.y * S; // scaled screen coordinates

float A=sqrt(2.), B=sqrt(3.);

// Isometric
vec3 rd = -vec3(1)/B,
     ro = mat3(B,0,-B,-1,2,-1,A,A,A)/A/B * vec3(u, A*S + eps);

// Dimetric
vec3 rd = -vec3(B,A,B)/A/2.,
     ro = mat3(2,0,-2,-1,A*B,-1,B,A,B)/A/2. * vec3(u, B*S + eps);
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;eps&lt;/code&gt; is an arbitrary small value to make sure the y-coordinate ends up
above 0.&lt;/p&gt;
    &lt;p&gt;In Entrance 3, I used a rough approximation of the isometric setup.&lt;/p&gt;
    &lt;head rend="h2"&gt;Archipelago&lt;/head&gt;
    &lt;code&gt;// Archipelago [472] by bµg
// License: CC BY-NC-SA 4.0
#define r(a)*=mat2(cos(a+vec4(0,11,33,0))),
void main(){vec3 p,q,k;for(float w,x,a,b,i,t,h,e=.1,d=e,z=.001;i++&amp;lt;50.&amp;amp;&amp;amp;d&amp;gt;z;h+=k.y,w=h-d,t+=d=min(d,h)*.8,O=vec4((w&amp;gt;z?k.zxx*e:k.zyz/20.)+i/1e2+max(1.-abs(w/e),z),1))for(p=normalize(vec3(P+P-R,R.y))*t,p.zy r(1.)p.z+=T+T,p.x+=sin(w=T*.4)*2.,p.xy r(cos(w)*e)d=p.y+=4.,h=d-2.3+abs(p.x*.2),q=p,k-=k,a=e,b=.8;a&amp;gt;z;a*=.8,b*=.5)q.xz r(.6)p.xz r(.6)k.y+=abs(dot(sin(q.xz*.4/b),R-R+b)),k.x+=w=a*exp(sin(x=p.x/a*e+T+T)),p.x-=w*cos(x),d-=w;}
&lt;/code&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;See it on its official page, or play with the code on its Shadertoy portage.&lt;/p&gt;
    &lt;p&gt;For this infinite procedurally generated Japan, I wanted to mark a rupture with my red/orange obsession. Technically speaking, it's actually fairly basic if you're familiar with Red Alp. I used the same noise for the mountains/islands, but the water uses a different noise.&lt;/p&gt;
    &lt;p&gt;The per octave noise curve is &lt;code&gt;w=exp(sin(x))&lt;/code&gt;, with the particularity of
shifting the &lt;code&gt;x&lt;/code&gt; coordinate with its derivative: &lt;code&gt;x-=w*cos(x)&lt;/code&gt;. This is some
form of domain warping that gives the nice effect here. When I say &lt;code&gt;x&lt;/code&gt;, I'm
really referring to the x-axis position. It is not needed to work with the
z-component (xz forms the flat plane) because each octave of the fbm has a
rotation that "mixes" both axis, so &lt;code&gt;z&lt;/code&gt; is actually backed in &lt;code&gt;x&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;I didn't come up with the formula, but found it first one this video by Acerola. I don't know if he's the original author, but I've seen the formula being replicated in various places.&lt;/p&gt;
    &lt;head rend="h2"&gt;Cutie&lt;/head&gt;
    &lt;code&gt;// Cutie [602] by bµg
// License: CC BY-NC-SA 4.0
#define V vec3
#define L length(p
#define C(A,B,X,Y)d=min(d,-.2*log2(exp2(X-L-A)/.2)+exp2(Y-L-B)/.2)))
#define H(Z)S,k=fract(T*1.5+s),a=V(1.3,.2,Z),b=V(1,.3*max(1.-abs(3.*k-1.),z),Z*.75+3.*max(-k*S,k-1.)),q=b*S,q+=a+sqrt(1.-dot(q,q))*normalize(V(-b.y,b.x,0)),C(a,q,3.5,2.5),C(q,a-b,2.5,2.)
void main(){float i,t,k,z,s,S=.5,d=S;for(V p,q,a,b;i++&amp;lt;5e1&amp;amp;&amp;amp;d&amp;gt;.001;t+=d=min(d,s=L+V(S-2.*p.x,-1,S))-S))p=normalize(V(P+P-R,R.y))*t,p.z-=5.,p.zy*=mat2(cos(vec4(1,12,34,1))),p.xz*=mat2(cos(sin(T)+vec4(0,11,33,0))),d=1.+p.y,C(z,V(z,z,1.2),7.5,6.),s=p.x&amp;lt;z?p.x=-p.x,z:H(z),s+=H(1.);O=vec4(V(exp(-i/(s&amp;gt;d?1e2:9.))),1);}
&lt;/code&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;See it on its official page, or play with the code on its Shadertoy portage.&lt;/p&gt;
    &lt;p&gt;Here I got cocky and thought I could manage to fit it in 512 chars. I failed, by 90 characters. I did use the smoothmin operator for the first time: every limb of the body of Cutie is composed of two spheres creating a rounded cone (two sphere of different size smoothly merged like metaballs).&lt;/p&gt;
    &lt;p&gt;Then I used simple IK kinetics for the animation. Using leg parts with a size of 1 helped simplifying the formula and make it shorter.&lt;/p&gt;
    &lt;p&gt;You may be wondering about the smooth visuals itself: I didn't use the depth map but simply the number of iterations. Due to the nature of the raymarching algorithm, when a ray passes close to a shape, it slows down significantly, increasing the number of iterations. This is super useful because it exaggerate the contour of the shapes naturally. It's wrapped into an exponential, but &lt;code&gt;i&lt;/code&gt;
defines the output color directly.&lt;/p&gt;
    &lt;head rend="h2"&gt;What's next&lt;/head&gt;
    &lt;p&gt;I will continue making more of those, keeping my artistic ambition low because of the 512 characters constraint I'm imposing on myself.&lt;/p&gt;
    &lt;p&gt;You may be wondering why I keep this obsession about 512 characters, and many people called me out on this one. There are actually many arguments:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A tiny demo has to focus on one or two very scoped aspects of computer graphics, which makes it perfect as a learning support.&lt;/item&gt;
      &lt;item&gt;It's part of the artistic performance: it's not just techniques and visuals, the wizardry of the code is part of why it's so impressive. We're in an era of visuals, people have been fed with the craziest VFX ever. But have they seen them with a few hundreds bytes of code?&lt;/item&gt;
      &lt;item&gt;The constraint helps me finish the work: when making art, there is always this question of when to stop. Here there is an intractable point where I just cannot do more and I have to move on.&lt;/item&gt;
      &lt;item&gt;Similarly, it prevents my ambition from tricking me into some colossal project I will never finish or even start. That format has a ton of limitations, and that's its strength.&lt;/item&gt;
      &lt;item&gt;Working on such a tiny piece of code for days/weeks just brings me joy. I do feel like a craftsperson, spending an unreasonable amount of time perfecting it, for the beauty of it.&lt;/item&gt;
      &lt;item&gt;I'm trying to build a portfolio, and it's important for me to keep it consistent. If the size limit was different, I would have done things differently, so I can't change it now. If I had hundreds more characters, Red Alp might have had birds, the sky opening to lit a beam of light on the mountains, etc.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Why 512 in particular? It happens to be the size of a toot on my Mastodon instance so I can fit the code there, and I found it to be a good balance.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46194477</guid><pubDate>Mon, 08 Dec 2025 16:44:42 +0000</pubDate></item><item><title>Microsoft has a problem: nobody wants to buy or use its shoddy AI products</title><link>https://www.windowscentral.com/artificial-intelligence/microsoft-has-a-problem-nobody-wants-to-buy-or-use-its-shoddy-ai</link><description>&lt;doc fingerprint="2f162c409bb6c8d7"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Microsoft has a problem: nobody wants to buy or use its shoddy AI products — as Google's AI growth begins to outpace Copilot products&lt;/head&gt;
    &lt;p&gt;A new report details how Microsoft has cut some internal goals for its AI sales people, why? Nobody wants to use its weak products.&lt;/p&gt;
    &lt;p&gt;Enjoy our content? Make sure to set Windows Central as a preferred source in Google Search, and find out why you should so that you can stay up-to-date on the latest news, reviews, features, and more.&lt;/p&gt;
    &lt;p&gt;If there's one thing that typifies Microsoft under CEO Satya Nadella's tenure: it's a general inability to connect with customers.&lt;/p&gt;
    &lt;p&gt;Microsoft shut down its retail arm quietly over the past few years, closed up shop on mountains of consumer products, while drifting haphazardly from tech fad to tech fad. From blockchain to "metaverse" and now to artificial intelligence — Microsoft CEO Satya Nadella can't seem to prioritize effectively, and the cracks are starting to shine through.&lt;/p&gt;
    &lt;p&gt;A recent report from The Information detailed how Microsoft's internal AI efforts are going awry, with cut forecasts and sales goals for its Azure AI products across the board. The Information said that Microsoft's sales people are "struggling" to meet goals, owing to a complete lack of demand. Microsoft denied the reports, but it can't deny market share growth trends — all of which point to Google Gemini surging ahead.&lt;/p&gt;
    &lt;p&gt;Last week we wrote about how Microsoft Copilot's backend partner OpenAI issued a "code red" situation. ChatGPT has fallen behind Google Gemini in problem solving, and Nano Banana image generation has outpaced OpenAI's own DALLE by leaps and bounds.&lt;/p&gt;
    &lt;p&gt;With OpenAI's business model under constant scrutiny and racking up genuinely dangerous levels of debt, it's become a cascading problem for Microsoft to have tied up layer upon layer of its business in what might end up being something of a lame duck.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;
          &lt;p&gt;#&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell role="head"&gt;
          &lt;p&gt;Generative AI Chatbot&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell role="head"&gt;
          &lt;p&gt;AI Search Market Share&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell role="head"&gt;
          &lt;p&gt;Estimated Quarterly User Growth&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;1&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;ChatGPT (excluding Copilot)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;61.30%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;7% ▲&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;2&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Microsoft Copilot&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;14.10%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;2% ▲&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;3&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Google Gemini&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;13.40%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;12% ▲&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;4&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Perplexity&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;6.40%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;4% ▲&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;5&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Claude AI&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;3.80%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;14% ▲&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;6&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Grok&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;0.60%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;6% ▲&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;7&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Deepseek&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;0.20%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;10% ▲&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;There are reams of research that suggest agentic AI tools require human intervention at a frequency ratio that makes them cost ineffective, but Microsoft seems unbothered that its tools are poorly conceived.&lt;/p&gt;
    &lt;p&gt;In any case, OpenAI is supposedly going to launch future models of ChatGPT early in attempts to combat the rise of Google Gemini. I suspect the issues are deeper for Microsoft, who have worked tirelessly under Satya Nadella to create doubt around its products.&lt;/p&gt;
    &lt;p&gt;All the latest news, reviews, and guides for Windows and Xbox diehards.&lt;/p&gt;
    &lt;p&gt;SEO and analytics firm FirstPageSage has released its AI market share report for the start of December, and it shows Google Gemini actively poised to supplant Microsoft Copilot. Based on reports that Google Gemini is now actively beating ChatGPT's best models, FirstPageSage has Google Gemini sprinting past Microsoft Copilot quarter over quarter, although ChatGPT itself will remain the front runner.&lt;/p&gt;
    &lt;head rend="h2"&gt;Google's AI advantages are accumulating, as Microsoft's disadvantages snowball&lt;/head&gt;
    &lt;p&gt;Whether it's Google's Tensor server tech or dominating position with Google Play-bound Android, Microsoft's lack of forethought and attention paid to their actual customers is starting to catch up with the firm. Nadella has sought to blame the company's unwieldy size for the lack of innovation, but it reads like an excuse to me. It's all about priorities — and Nadella has chased shareholder sentiment over delivering for its customers or employees, and that short-termism is going to put Microsoft on the backfoot if AI actually does deliver another computing paradigm shift.&lt;/p&gt;
    &lt;p&gt;Microsoft depends almost entirely on pricy NVIDIA technology for its data centers, whereas Google is actively investing to own the entire stack. Microsoft has also worked incredibly hard to cram half-baked AI features into its products, whereas Google has arguably been a lot more thoughtful in its approach. Microsoft sprinted out of the gate like a bull in a China shop, and investors rewarded them for it — but fast forward to 2025, and Google's AI products simply work better, and are more in-tune with how people might actually use them.&lt;/p&gt;
    &lt;p&gt;I am someone who is actively using the AI features across Google Android and Microsoft Windows on a day to day basis, and the delta between the two companies is growing ever wider. Basic stuff like the photo editing features on Google Pixel phones are lightyears beyond the abysmal tools found in the Microsoft Photos app on Windows. Google Gemini in Google Apps is also far smarter and far more intuitive than Copilot on Microsoft 365, as someone actively using both across the two businesses I work in.&lt;/p&gt;
    &lt;p&gt;Dare I say it, Gemini is actually helpful, and can usually execute tasks you might actually need in a day to day job. "Find me a meeting slot on this date to accommodate these timezones" — Gemini will actually do it. Copilot 365 doesn't even have the capability to schedule a calendar event with natural language in the Outlook mobile app, or even provide something as basic as clickable links in some cases. At least Xbox's Gaming Copilot has a beta tag to explain why it fails half of the time. It's truly absurd how half-baked a lot of these features are, and it's odd that Microsoft sought to ship them in this state. And Microsoft wants to make Windows 12 AI first? Please.&lt;/p&gt;
    &lt;p&gt;Microsoft's "ship it now fix it later" attitude risks giving its AI products an Internet Explorer-like reputation for poor quality, sacrificing the future to more patient, thoughtful companies who spend a little more time polishing first. Microsoft's strategy for AI seems to revolve around offering cheaper, lower quality products at lower costs (Microsoft Teams, hi), over more expensive higher-quality options its competitors are offering. Whether or not that strategy will work for artificial intelligence, which is exorbitantly expensive to run, remains to be seen.&lt;/p&gt;
    &lt;p&gt;Microsoft's savvy early investment in OpenAI gave it an incredibly strong position early on, but as we get deeper into the cycle, some cracks are starting to show. Many of Microsoft's AI products to date simply scream of a total lack of direction and utter chaos, but it's not all hopeless. Some of Microsoft's enterprise solutions for AI are seeing strong growth. Github Copilot has been something of a success story for Redmond, and Microsoft is exploring its own Maia and Cobalt chips and even language models, in attempts to decouple itself from NVIDIA and OpenAI respectively. But Satya Nadella's Microsoft has an uncanny knack for failing to deliver on promising initiatives like those.&lt;/p&gt;
    &lt;p&gt;Without a stronger emphasis on quality, Microsoft's future in AI could simply end up revolving around re-selling NVIDIA server tech and jacking up local electricity prices, rather than providing any real home-grown innovation in the space. Shareholders will be more than happy for Microsoft to simply be a server reseller, but it would be a ignoble legacy for what was previously one of tech's most innovative companies.&lt;/p&gt;
    &lt;p&gt;Follow Windows Central on Google News to keep our latest news, insights, and features at the top of your feeds!&lt;/p&gt;
    &lt;p&gt;Jez Corden is the Executive Editor at Windows Central, focusing primarily on all things Xbox and gaming. Jez is known for breaking exclusive news and analysis as relates to the Microsoft ecosystem while being powered by tea. Follow on Twitter (X) and tune in to the XB2 Podcast, all about, you guessed it, Xbox!&lt;/p&gt;
    &lt;p&gt;You must confirm your public display name before commenting&lt;/p&gt;
    &lt;p&gt;Please logout and then login again, you will then be prompted to enter your display name.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46194615</guid><pubDate>Mon, 08 Dec 2025 16:54:31 +0000</pubDate></item><item><title>Legion Health (YC S21) is hiring a founding engineer (SF, in-person)</title><link>https://news.ycombinator.com/item?id=46194720</link><description>&lt;doc fingerprint="3541ec5e48d7b5a8"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Legion Health (YC S21) operates a psychiatric practice and is building the AI-native operations layer for mental health care. We focus on the operational backend: scheduling, intake, documentation, billing, and care coordination. These workflows—not diagnostics—are the main bottlenecks in mental health delivery.&lt;/p&gt;
      &lt;p&gt;We run our own clinic, so the systems you build ship directly into real patient care. Our agent infrastructure currently supports more than 2,000 patients with one human support lead.&lt;/p&gt;
      &lt;p&gt;We’re hiring a Founding Engineer (in-person, San Francisco). You’d work directly with the founders on:&lt;/p&gt;
      &lt;p&gt;event-driven backend systems (Node.js, TypeScript, Postgres/Supabase, AWS)&lt;/p&gt;
      &lt;p&gt;LLM agent tooling (tool use, retries, memory, context management)&lt;/p&gt;
      &lt;p&gt;internal operations tools for both humans and agents&lt;/p&gt;
      &lt;p&gt;state/coordination logic that represents a patient’s journey&lt;/p&gt;
      &lt;p&gt;HIPAA-compliant data and audit pipelines&lt;/p&gt;
      &lt;p&gt;We’re open to backend or full-stack/product engineers who think in systems and have owned real workflows end-to-end. Prior experience with LLMs is optional; interest is required.&lt;/p&gt;
      &lt;p&gt;Details: full-time, in-person SF, salary $130k–$180k, equity 0.1–0.6%.&lt;/p&gt;
      &lt;p&gt;Apply here: https://www.ycombinator.com/companies/legion-health/jobs/oc6...&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46194720</guid><pubDate>Mon, 08 Dec 2025 17:01:11 +0000</pubDate></item><item><title>RIP Tetsu Yamauchi (Former Free and Faces Bassist)</title><link>https://www.loudersound.com/bands-artists/former-free-and-faces-bassist-tetsu-yamauchi-dead-at-79</link><description>&lt;doc fingerprint="27b5478ae8b6c6d0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Former Free and Faces bassist Tetsu Yamauchi dead at 79&lt;/head&gt;
    &lt;p&gt;Tetsu Yamauchi replaced Andy Fraser in Free and Ronnie Lane in the Faces&lt;/p&gt;
    &lt;p&gt;Tetsu Yamauchi, former bassist with Free and the Faces, has died at the age of 79. The news was confirmed in a statement released on social media by his family.&lt;/p&gt;
    &lt;p&gt;"To all of you who have always supported us," read the statement. "On December 4, Reiwa 7 [The year 2025 in the Japanese calendar], Tetsu Yamauchi passed away peacefully, surrounded by family.&lt;/p&gt;
    &lt;p&gt;"We sincerely thank everyone who enjoyed Tetsu's music and offered kind words until now. Those were fun times. It's a long time, but a short time."&lt;/p&gt;
    &lt;p&gt;Tetsu Yamauchi was born in Fukuoka, Japan, in October 1946 and joined Japanese progressive rockers Micky Curtis &amp;amp; The Samurais in the late 1960s, with whom he recorded two albums, Kappa and Samurai, both released in 1971.&lt;/p&gt;
    &lt;p&gt;Later that year, he hooked up with Free guitarist Paul Kossoff and drummer Simon Kirke, plus keyboardist John ‘Rabbit’ Bundrick, to record a one-off album after Free had temporarily splintered amid disagreements between frontman Paul Rodgers and bassist Andy Fraser.&lt;/p&gt;
    &lt;p&gt;The Kossoff, Kirke, Tetsu &amp;amp; Rabbit album was a collection of rootsy blues and funk rock that lacked Free’s bite and Paul Rodgers’s voice, but it got the increasingly troubled Kossoff working again, and Free reunited in early 1972.&lt;/p&gt;
    &lt;p&gt;Within months, Fraser left the band, and Yamauchi was drafted in to replace him. He subsequently appeared on the Free's final album, Heartbreaker, and co-wrote the classic Wishing Well.&lt;/p&gt;
    &lt;p&gt;Sign up below to get the latest from Classic Rock, plus exclusive special offers, direct to your inbox!&lt;/p&gt;
    &lt;p&gt;Free broke up for the final time after a US tour in March 1973, and Yamauchi replaced Ronnie Lane in the Faces, where he remained for two years. He played on the 1974 live album Coast to Coast: Overture and Beginners, and fully embraced the rock'n'roll lifestyle at a time when his bandmates were attempting to moderate their own behaviour.&lt;/p&gt;
    &lt;p&gt;"Tetsu was a real wild card after Ronnie Lane left the band," Ronnie Wood told Classic Rock. "Too crazy."&lt;/p&gt;
    &lt;p&gt;Yamauchi's only studio contribution to the Faces came with the single You Can Make Me Dance, Sing or Anything (Even Take The Dog For A Walk, Mend A Fuse, Fold Away The Ironing Board, Or Any Other Domestic Shortcomings), which was released in late 1972 and still holds the record for the longest-titled song ever to chart in the UK.&lt;/p&gt;
    &lt;p&gt;After The Faces broke up, Yamauchi recorded his second solo album, Kikyou (his first, Tetsu, came out in 1972), and worked as a session musician before returning to Japan, where he formed Tetsu Yamauchi &amp;amp; the Good Times Roll Band, who released a live album in 1977.&lt;/p&gt;
    &lt;p&gt;In 1985, he formed the Ope Band with free jazz drummer Shoji Hano, a relationship that also produced Dare Devil, a 1992 live album recorded with renowned free jazz saxophonist and clarinettist Peter Brötzmann and guitarist Haruhiko Gotsu.&lt;/p&gt;
    &lt;p&gt;For the last 15 years of his life Yamauchi lived quietly, refusing requests for interviews, although he returned to the stage in 2023 and 2024 as Meets Duo alongside drummer Yoshitaka Shimada, one of the original members of his Good Times Roll Band.&lt;/p&gt;
    &lt;p&gt;“Just heard that Tetsu passed away," Simon Kirke wrote on social media. "He was a good friend and a great bass player. My condolences to his family and close friends. May he rest in peace."&lt;/p&gt;
    &lt;p&gt;Online Editor at Louder/Classic Rock magazine since 2014. 39 years in music industry, online for 26. Also bylines for: Metal Hammer, Prog Magazine, The Word Magazine, The Guardian, The New Statesman, Saga, Music365. Former Head of Music at Xfm Radio, A&amp;amp;R at Fiction Records, early blogger, ex-roadie, published author. Once appeared in a Cure video dressed as a cowboy, and thinks any situation can be improved by the introduction of cats. Favourite Serbian trumpeter: Dejan Petrović.&lt;/p&gt;
    &lt;p&gt;You must confirm your public display name before commenting&lt;/p&gt;
    &lt;p&gt;Please logout and then login again, you will then be prompted to enter your display name.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46194805</guid><pubDate>Mon, 08 Dec 2025 17:08:36 +0000</pubDate></item><item><title>Launch HN: Nia (YC S25) – Give better context to coding agents</title><link>https://www.trynia.ai/</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46194828</guid><pubDate>Mon, 08 Dec 2025 17:10:14 +0000</pubDate></item><item><title>Show HN: DuckDB for Kafka Stream Processing</title><link>https://sql-flow.com/docs/tutorials/intro/</link><description>&lt;doc fingerprint="e508bd350561bb82"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Quickstart&lt;/head&gt;
    &lt;p&gt;Create a stream processor that reads data from Kafka in less than 5 minutes.&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting Started&lt;/head&gt;
    &lt;p&gt;Get started by running a stream processor that executes SQL against a kafka stream and writes the output to the console.&lt;/p&gt;
    &lt;head rend="h3"&gt;What you'll need&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Docker&lt;/item&gt;
      &lt;item&gt;A copy of turbolytics/sql-flow cloned on your local machine (https://github.com/turbolytics/sql-flow)&lt;/item&gt;
      &lt;item&gt;turbolytics/sql-flow Python dependencies installed&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;cd path/to/turbolytics/sql-flow/github/repo &amp;amp;&amp;amp; pip install -r requirements.txt&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The turbolytics/sql-flow docker image&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;docker pull turbolytics/sql-flow:latest&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Kafka running on your local machine&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;cd path/to/turbolytics/sql-flow/github/repo &amp;amp;&amp;amp; docker-compose -f dev/kafka-single.yml up -d&lt;/code&gt;
    &lt;head rend="h2"&gt;Test the SQLFlow configuration file&lt;/head&gt;
    &lt;p&gt;SQLFlow ships with cli support to test a stream configuration against any fixture file of test data. The goal is to support testing and linting of a configuration file before executing in a stream environment.&lt;/p&gt;
    &lt;p&gt;Run the invoke command to test the configuration file against a set of test data:&lt;/p&gt;
    &lt;code&gt;docker run -v $(pwd)/dev:/tmp/conf -v /tmp/sqlflow:/tmp/sqlflow turbolytics/sql-flow:latest dev invoke /tmp/conf/config/examples/basic.agg.mem.yml /tmp/conf/fixtures/simple.json&lt;/code&gt;
    &lt;p&gt;The following output should show:&lt;/p&gt;
    &lt;code&gt;[{'city': 'New York', 'city_count': 28672}, {'city': 'Baltimore', 'city_count': 28672}]&lt;/code&gt;
    &lt;head rend="h2"&gt;Run SQLFlow against a Kafka stream&lt;/head&gt;
    &lt;p&gt;This section runs SQLFlow as a stream processor that reads data from a Kafka topic and writes the output to the console. SQLFow runs as a daemon and will continuously read data from kafka, execute the SQL and write the output to the console.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Publish test messages to the Kafka topic&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;python3 cmd/publish-test-data.py --num-messages=10000 --topic="input-simple-agg-mem"&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Start the Kafka Console Consumer, to view the SQLFlow output&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;docker exec -it kafka1 kafka-console-consumer --bootstrap-server=kafka1:9092 --topic=output-simple-agg-mem&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Start SQLFlow&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;docker run -v $(pwd)/dev:/tmp/conf -v /tmp/sqlflow:/tmp/sqlflow -e SQLFLOW_KAFKA_BROKERS=host.docker.internal:29092 turbolytics/sql-flow:latest run /tmp/conf/config/examples/basic.agg.mem.yml --max-msgs-to-process=10000&lt;/code&gt;
    &lt;p&gt;The following output should begin to show in the kafka console consumer:&lt;/p&gt;
    &lt;code&gt;...&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46195007</guid><pubDate>Mon, 08 Dec 2025 17:25:54 +0000</pubDate></item><item><title>Quanta to Publish Popular Math and Physics Titles by Terence Tao and David Tong</title><link>https://www.simonsfoundation.org/2025/12/08/quanta-books-to-publish-popular-math-and-physics-titles-by-terence-tao-and-david-tong/</link><description>&lt;doc fingerprint="2d041011891b9ef8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Quanta Books to Publish Popular Math and Physics Titles by Terence Tao and David Tong&lt;/head&gt;
    &lt;p&gt;Quanta Books is delighted to announce two new upcoming books by mathematician Terence Tao and theoretical physicist David Tong.&lt;/p&gt;
    &lt;p&gt;Six Math Essentials will be Tao’s first math book written for a popular audience. In the book, Tao — a recipient of the Fields Medal and one of the world’s top mathematicians — will explore six ideas that have guided mathematicians throughout history. This short and friendly volume is for all readers, Tao says, because he believes that “mathematics has become unnecessarily intimidating and abstruse to the general public while being more essential than ever in the modern world.” Six Math Essentials will be available internationally, with translated editions in Chinese, French, Greek, Italian, Polish and other languages. It will arrive in U.S. bookstores in November 2026.&lt;/p&gt;
    &lt;p&gt;Tong’s book, Everything Is Fields, will illuminate quantum field theory — the physics that explains the fundamental makeup of the universe — drawing from Tong’s distinguished track record as a quantum field theorist and public communicator. “This book reveals the hidden unity that ties together particles and forces,” says Tong. “Everything — matter, light, even you — are just waves on a restless sea known as a quantum field.”&lt;/p&gt;
    &lt;p&gt;“Terry Tao and David Tong are intellectual powerhouses and seasoned communicators,” says Thomas Lin, publisher of Quanta Books and founding editor of the Pulitzer Prize–winning Quanta Magazine. “Their books embody the curiosity and ambition that animate our imprint, and I can’t wait to share them with readers everywhere.”&lt;/p&gt;
    &lt;p&gt;Quanta Books is an editorially independent subsidiary of the Simons Foundation and a partner imprint of Farrar, Straus and Giroux. The imprint publishes books that illuminate and elucidate the central questions and fundamental ideas of modern science for readers, inviting a deeper understanding of the universe through artful storytelling. Quanta Books’ first title, The Proof in the Code by math journalist Kevin Hartnett, will be published in June 2026 and is available for preorder now.&lt;/p&gt;
    &lt;p&gt;For more information, visit QuantaBooks.org.&lt;/p&gt;
    &lt;head rend="h2"&gt;Six Math Essentials&lt;/head&gt;
    &lt;p&gt;In Six Math Essentials, Tao, the world’s most renowned mathematician, introduces readers to six core ideas that have guided mathematicians from antiquity to the frontiers of what we know today. This elegant volume explores: numbers as the gateway to quantitative thinking, algebra as the gateway to abstraction, geometry as a way to go beyond what we can see, probability as a tool to navigate uncertainty with rigorous thinking, analysis as a means to tame the very large or very small, and dynamics as the mathematics of change. Six Math Essentials — Tao’s first popular math book — offers a glimpse into the workings of an incomparable mind and how he thinks about the creativity, beauty, and interconnectedness of the mathematical enterprise. Math, Tao insists, isn’t magic — it’s a powerful way of thinking that anyone can learn.&lt;/p&gt;
    &lt;head rend="h2"&gt;Everything Is Fields&lt;/head&gt;
    &lt;p&gt;In Everything Is Fields, Tong leads readers on a lively tour through quantum field theory. Tong, a leading theoretical physicist and University of Cambridge professor, explores Quantum field theory, or QFT. The theory forms the underlying mathematical framework of the Standard Model, the deepest description we have of the fundamental laws of physics. And, as Tong shows, it reveals a startling truth: that, at our most basic level, we are made not of particles or forces, but fields, fluid-like substances stretched throughout the entire universe. With his infectious sense of wonder and characteristic wit, Tong buoys our journey through the most difficult topic in theoretical physics. He revels in all that we’ve learned about our world and illuminates the questions we’re still trying to answer about the stuff that makes up you, me, and everything else.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Proof in the Code&lt;/head&gt;
    &lt;p&gt;The Proof in the Code is the definitive account of the birth and rise of Lean, a proof assistant developed at Microsoft that is transforming the enterprise of mathematics and ushering in a new era of human-computer collaboration. Although Lean was originally conceived of as a code-checking program, a small group of mathematicians recognized its potential to become something far more powerful: the “truth oracle” that thinkers have sought for centuries, a tool to definitively verify or refute any mathematical or logical assertion, no matter how complex. This is the story of the grassroots effort to make that dream a reality. Filled with insights about the future of math, computers, and AI, The Proof in the Code is a brilliant work of journalism by Hartnett, a leading math writer whose research and reporting offer a profound answer to a longstanding mystery: Can computers reveal universal truths?&lt;/p&gt;
    &lt;head rend="h3"&gt;Information for Press&lt;/head&gt;
    &lt;p&gt;For more information, please contact [email protected].&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46195225</guid><pubDate>Mon, 08 Dec 2025 17:39:55 +0000</pubDate></item><item><title>Jepsen: NATS 2.12.1</title><link>https://jepsen.io/analyses/nats-2.12.1</link><description>&lt;doc fingerprint="2790905544351bb0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;1 Background&lt;/head&gt;
    &lt;p&gt;NATS is a popular streaming system. Producers publish messages to streams, and consumers subscribe to those streams, fetching messages from them. Regular NATS streams are allowed to drop messages. However, NATS has a subsystem called JetStream, which uses the Raft consensus algorithm to replicate data among nodes. JetStream promises âat least onceâ delivery: messages may be duplicated, but acknowledged messages1 should not be lost.2 Moreover, JetStream streams are totally ordered logs.&lt;/p&gt;
    &lt;p&gt;JetStream is intended to âself-heal and always be availableâ. The documentation also states that âthe formal consistency model of NATS JetStream is Linearizableâ. At most one of these claims can be true: the CAP theorem tells us that Linearizable systems can not be totally available.3 In practice, they tend to be available so long as a majority of nodes are non-faulty and communicating. If, say, a single node loses network connectivity, operations must fail on that node. If three out of five nodes crash, all operations must fail.&lt;/p&gt;
    &lt;p&gt;Indeed, a later section of the JetStream docs acknowledges this fact, saying that streams with three replicas can tolerate the loss of one server, and those with five can tolerate the simultaneous loss of two.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Replicas=5 - Can tolerate simultaneous loss of two servers servicing the stream. Mitigates risk at the expense of performance.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;When does NATS guarantee a message will be durable? The JetStream developer docs say that once a JetStream clientâs &lt;code&gt;publish&lt;/code&gt; request is acknowledged by the server, that message has âbeen successfully persistedâ. The clustering configuration documentation says:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;In order to ensure data consistency across complete restarts, a quorum of servers is required. A quorum is Â½ cluster size + 1. This is the minimum number of nodes to ensure at least one node has the most recent data and state after a catastrophic failure. So for a cluster size of 3, youâll need at least two JetStream enabled NATS servers available to store new messages. For a cluster size of 5, youâll need at least 3 NATS servers, and so forth.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;With these guarantees in mind, we set out to test NATS JetStream behavior under a variety of simulated faults.&lt;/p&gt;
    &lt;head rend="h1"&gt;2 Test Design&lt;/head&gt;
    &lt;p&gt;We designed a test suite for NATS JetStream using the Jepsen testing library, using JNATS (the official Java client) at version 2.24.0. Most of our tests ran in Debian 12 containers under LXC; some tests ran in Antithesis, using the official NATS Docker images. In all our tests we created a single JetStream stream with a target replication factor of five. Per NATSâ recommendations, our clusters generally contained three or five nodes. We tested a variety of versions, but the bulk of this work focused on NATS 2.12.1.&lt;/p&gt;
    &lt;p&gt;The test harness injected a variety of faults, including process pauses, crashes, network partitions, and packet loss, as well as single-bit errors and truncation of data files. We limited file corruption to a minority of nodes. We also simulated power failureâa crash with partial amnesiaâusing the LazyFS filesystem. LazyFS allows Jepsen to drop any writes which have not yet been flushed using a call to (e.g.) &lt;code&gt;fsync&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Our tests did not measure Linearizability or Serializability. Instead we ran several producer processes, each bound to a single NATS client, which published globally unique values to a single JetStream stream. Each message included the process number and a sequence number within that process, so message &lt;code&gt;4-0&lt;/code&gt; denoted the first &lt;code&gt;publish&lt;/code&gt; attempted by process &lt;code&gt;4&lt;/code&gt;, message &lt;code&gt;4-1&lt;/code&gt; denoted the second, and so on. At the end of the test we ensured all nodes were running, resolved any network partitions or other faults, subscribed to the stream, and attempted to read all acknowledged messages from the the stream. Each reader called &lt;code&gt;fetch&lt;/code&gt; until it had observed (at least) the last acknowledged message published by each process, or timed out.&lt;/p&gt;
    &lt;p&gt;We measured JetStreamâs at-least-once semantics based on the union of all published and read messages. We considered a message OK if it was attempted and read. Messages were lost if they were acknowledged as published, but never read by any process. We divided lost messages into three epochs, based on the first and last OK messages written by the same process.4 We called those lost before the first OK message the lost-prefix, those lost after all the last OK message the lost-postfix, and all others the lost-middle. This helped to distinguish between lagging readers and true data loss.&lt;/p&gt;
    &lt;p&gt;In addition to verifying each acknowledged message was delivered to at least one consumer across all nodes, we also checked the set of messages read by all consumers connected to a specific node. We called it divergence, or split-brain, when an acknowledged message was missing from some nodes but not others.&lt;/p&gt;
    &lt;head rend="h1"&gt;3 Results&lt;/head&gt;
    &lt;p&gt;We begin with a belated note on total data loss in version 2.10.22, then continue with four findings related to data loss and replica divergence in version 2.12.1: two with file corruption, and two with power failures.&lt;/p&gt;
    &lt;head rend="h2"&gt;3.1 Total Data Loss on Crash in 2.10.22 (#6888)&lt;/head&gt;
    &lt;p&gt;Before discussing version 2.12.1, we present a long-overdue finding from earlier work. In versions 2.10.20 through 2.10.22 (released 2024-10-17), we found that process crashes alone could cause the total loss of a JetStream stream and all its associated data. Subscription requests would return &lt;code&gt;"No matching streams for subject"&lt;/code&gt;, and &lt;code&gt;getStreamNames()&lt;/code&gt; would return an empty list. These conditions would persist for hours: in this test run, we waited 10,000 seconds for the cluster to recover, but the stream never returned.&lt;/p&gt;
    &lt;p&gt;Jepsen reported this issue to NATS as #6888, but it appears that NATS had already identified several potential causes for this problem and resolved them. In #5946, a cluster-wide crash occurring shortly after a stream was created could cause the loss of the stream. A new leader would be elected with a snapshot which preceded the creation of the stream, and replicate that empty snapshot to followers, causing everyone to delete their copy of the stream. In #5700, tests running in Antithesis found that out-of-order delivery of snapshot messages could cause streams to be deleted and re-created as well. In #6061, process crashes could cause nodes to delete their local Raft state. All of these fixes were released as a part of 2.10.23, and we no longer observed the problem in that version.&lt;/p&gt;
    &lt;head rend="h2"&gt;3.2 Lost Writes With &lt;code&gt;.blk&lt;/code&gt; File Corruption (#7549)&lt;/head&gt;
    &lt;p&gt;NATS has several checksum mechanisms meant to detect data corruption in on-disk files. However, we found that single-bit errors or truncation of JetStreamâs &lt;code&gt;.blk&lt;/code&gt; files could cause the cluster to lose large windows of writes. This occurred even when file corruption was limited to just one or two nodes out of five. For instance, file corruption in this test run caused NATS to lose 679,153 acknowledged writes out of 1,367,069 total, including 201,286 which were missing even though later values written by the same process were later read.&lt;/p&gt;
    &lt;p/&gt;
    &lt;p&gt;In some cases, file corruption caused the quiet loss of just a single message. In others, writes vanished in large blocks. Even worse, bitflips could cause split-brain, where different nodes returned different sets of messages. In this test, NATS acknowledged a total of 1,479,661 messages. However, single-bit errors in &lt;code&gt;.blk&lt;/code&gt; files on nodes &lt;code&gt;n1&lt;/code&gt; and &lt;code&gt;n3&lt;/code&gt; caused nodes &lt;code&gt;n1&lt;/code&gt;, &lt;code&gt;n3&lt;/code&gt;, and &lt;code&gt;n5&lt;/code&gt; to lose up to 78% of those acknowledged messages. Node &lt;code&gt;n1&lt;/code&gt; lost 852,413 messages, and nodes &lt;code&gt;n3&lt;/code&gt; and &lt;code&gt;n5&lt;/code&gt; lost 1,167,167 messages, despite &lt;code&gt;n5&lt;/code&gt;âs data files remaining intact. Messages were lost in prefix, middle, and postfix: the stream, at least on those three nodes, resembled Swiss cheese.&lt;/p&gt;
    &lt;p&gt;NATS is investigating this issue (#7549).&lt;/p&gt;
    &lt;head rend="h2"&gt;3.3 Total Data Loss With Snapshot File Corruption (#7556)&lt;/head&gt;
    &lt;p&gt;When we truncated or introduced single-bit errors into JetStreamâs snapshot files in &lt;code&gt;data/jetstream/$SYS/_js_/&lt;/code&gt;, we found that nodes would sometimes decide that a stream had been orphaned, and delete all its data files. This happened even when only a minority of nodes in the cluster experienced file corruption. The cluster would never recover quorum, and the stream remained unavailable for the remainder of the test.&lt;/p&gt;
    &lt;p&gt;In this test run, we introduced single-bit errors into snapshots on nodes &lt;code&gt;n3&lt;/code&gt; and &lt;code&gt;n5&lt;/code&gt;. During the final recovery period, node &lt;code&gt;n3&lt;/code&gt; became the metadata leader for the cluster and decided to clean up &lt;code&gt;jepsen-stream&lt;/code&gt;, which stored all the testâs messages.&lt;/p&gt;
    &lt;code&gt;[1010859] 2025/11/15 20:27:02.947432 [INF]
Self is new JetStream cluster metadata leader
[1010859] 2025/11/15 20:27:14.996174 [WRN]
Detected orphaned stream 'jepsen &amp;gt;
jepsen-stream', will cleanup&lt;/code&gt;
    &lt;p&gt;Nodes &lt;code&gt;n3&lt;/code&gt; and &lt;code&gt;n5&lt;/code&gt; then deleted all files in the stream directory. This might seem defensibleâafter all, some of &lt;code&gt;n3&lt;/code&gt;âs data files were corrupted. However, &lt;code&gt;n3&lt;/code&gt; managed to become the leader of the cluster despite its corrupt state! In general, leader-based consensus systems must be careful to ensure that any node which becomes a leader is aware of majority committed state. Becoming a leader, then opting to delete a stream full of committed data, is particularly troubling.&lt;/p&gt;
    &lt;p&gt;Although nodes &lt;code&gt;n1&lt;/code&gt;, &lt;code&gt;n2&lt;/code&gt;, and &lt;code&gt;n4&lt;/code&gt; retained their data files, &lt;code&gt;n1&lt;/code&gt; struggled to apply snapshots; &lt;code&gt;n4&lt;/code&gt; declared that &lt;code&gt;jepsen-stream&lt;/code&gt; had no quorum and stalled. Every attempt to subscribe to the stream threw &lt;code&gt;[SUB-90007] No matching streams for subject&lt;/code&gt;. Jepsen filed issue #7556 for this, and the NATS team is looking into it.&lt;/p&gt;
    &lt;head rend="h2"&gt;3.4 Lazy &lt;code&gt;fsync&lt;/code&gt; by Default (#7564)&lt;/head&gt;
    &lt;p&gt;NATS JetStream promises that once a &lt;code&gt;publish&lt;/code&gt; call has been acknowledged, it is âsuccessfully persistedâ. This is not exactly true. By default, NATS calls &lt;code&gt;fsync&lt;/code&gt; to flush data to disk only once every two minutes, but acknowledges messages immediately. Consequently, recently acknowledged writes are generally not persisted, and could be lost to coordinated power failure, kernel crashes, etc. For instance, simulated power failures in this test run caused NATS to lose roughly thirty seconds of writes: 131,418 out of 930,005 messages.&lt;/p&gt;
    &lt;p/&gt;
    &lt;p&gt;Because the default flush interval is quite large, even killing a single node at a time is sufficient to cause data loss, so long as nodes fail within a few seconds of each other. In this run, a series of single-node failures in the first two minutes of the test caused NATS to delete the entire stream, along with all of its messages.&lt;/p&gt;
    &lt;p&gt;There are only two mentions of this behavior in the NATS documentation. The first is in the 2.10 release notes. The second, buried in the configuration docs, describes the &lt;code&gt;sync_interval&lt;/code&gt; option:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Change the default fsync/sync interval for page cache in the filestore. By default JetStream relies on stream replication in the cluster to guarantee data is available after an OS crash. If you run JetStream without replication or with a replication of just 2 you may want to shorten the fsync/sync interval. You can force an fsync after each messsage [sic] with&lt;/p&gt;&lt;code&gt;always&lt;/code&gt;, this will slow down the throughput to a few hundred msg/s.&lt;/quote&gt;
    &lt;p&gt;Consensus protocols often require that nodes sync to disk before acknowledging an operation. For example, the famous 2007 paper Paxos Made Live remarks:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Note that all writes have to be flushed to disk immediately before the system can proceed any further.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The Raft thesis on which NATS is based is clear that nodes must âflush [new log entries] to their disksâ before acknowledging. Section 11.7.3 discusses the possibility of instead writing data to disk asynchronously, and concludes:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The trade-off is that data loss is possible in catastrophic events. For example, if a majority of the cluster were to restart simultaneously, the cluster would have potentially lost entries and would not be able to form a new view. Raft could be extended in similar ways to support disk-less operation, but we think the risk of availability or data loss usually outweighs the benefits.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;For similar reasons, replicated systems like MongoDB, etcd, TigerBeetle, Zookeeper, Redpanda, and TiDB sync data to disk before acknowledging an operation as committed.&lt;/p&gt;
    &lt;p&gt;However, some systems do choose to &lt;code&gt;fsync&lt;/code&gt; asynchronously. YugabyteDBâs default is to acknowledge un-fsynced writes. Liskov and Cowlingâs Viewstamped Replication Revisited assumes replicas are âhighly unlikely to fail at the same timeââbut acknowledges that if they were to fail simultaneously, state would be lost. Apache Kafka makes a similar choice, but claims that it is not vulnerable to coordinated failure because Kafka âdoesnât store unflushed data in its own memory, but in the page cacheâ. This offers resilience to the Kafka process itself crashing, but not power failure.5 Jepsen remains skeptical of this approach: as Alagappan et al. argue, extensive literature on correlated failures suggests we should continue to take this risk seriously. Heat waves, grid instability, fires, lightning, tornadoes, and floods are not necessarily constrained to a single availability zone.&lt;/p&gt;
    &lt;p&gt;Jepsen suggests that NATS change the default value for &lt;code&gt;fsync&lt;/code&gt; to &lt;code&gt;always&lt;/code&gt;, rather than every two minutes. Alternatively, NATS documentation should prominently disclose that JetStream may lose data when nodes experience correlated power failure, or fail in rapid succession (#7564).&lt;/p&gt;
    &lt;head rend="h2"&gt;3.5 A Single OS Crash Can Cause Split-Brain (#7567)&lt;/head&gt;
    &lt;p&gt;In response to #7564, NATS engineers noted that most production deployments run with each node in a separate availability zone, which reduces the probability of correlated failure. This raises the question: how many power failures (or hardware faults, kernel crashes, etc.) are required to cause data loss? Perhaps surprisingly, in an asynchronous network the answer is âjust oneâ.&lt;/p&gt;
    &lt;p&gt;To understand why, consider that a system which remains partly available when a minority of nodes are unavailable must allow states in which a committed operation is presentâsolely in memoryâon a bare majority of nodes. For example, in a leader-follower protocol the leader of a three-node cluster may consider a write committed as soon as a single follower has responded: it has two acknowledgements, counting itself. Under normal operation there will usually be some window of committed operations in this state.6.&lt;/p&gt;
    &lt;p&gt;Now imagine that one of those two nodes loses power and restarts. Because the write was stored only in memory, rather than on disk, the acknowledged write is no longer present on that node. There now exist two out of three nodes which do not have the write. Since the system is fault-tolerant, these two nodes must be able to form a quorum and continue processing requestsâcreating new states of the system in which the acknowledged write never happened.&lt;/p&gt;
    &lt;p&gt;Strictly speaking, this fault requires nothing more than a single power failure (or HW fault, kernel crash, etc.) and an asynchronous networkâone which is allowed to deliver messages arbitrarily late. Whether it occurs in practice depends on the specific messages exchanged by the replication system, which node fails, how long it remains offline, the order of message delivery, and so on. However, one can reliably induce data loss by killing, pausing, or partitioning away a minority of nodes before and after a simulated OS crash.&lt;/p&gt;
    &lt;p&gt;For example, process pauses and a single simulated power failure in this test run caused JetStream to lose acknowledged writes for windows roughly on par with &lt;code&gt;sync_interval&lt;/code&gt;. Stranger still, the cluster entered a persistent split-brain which continued after all nodes were restarted and the network healed. Consider these two plots of lost writes, based on final reads performed against nodes &lt;code&gt;n1&lt;/code&gt; and &lt;code&gt;n5&lt;/code&gt; respectively:&lt;/p&gt;
    &lt;p/&gt;
    &lt;p/&gt;
    &lt;p&gt;Consumers talking to &lt;code&gt;n1&lt;/code&gt; failed to observe a short window of acknowledged messages written around 42 seconds into the test. Meanwhile, consumers talking to &lt;code&gt;n5&lt;/code&gt; would miss acknowledged messages written around 58 seconds. Both windows of write loss were on the order of our choice of &lt;code&gt;sync_interval = 10s&lt;/code&gt; for this run. In repeated testing, we found that any node in the cluster could lose committed writes, including the node which failed, those which received writes before the failure, and those which received writes afterwards.&lt;/p&gt;
    &lt;p&gt;The fact that a single power failure can cause data loss is not new. In 2023, RedPanda wrote a detailed blog post showing that Kafkaâs default lazy &lt;code&gt;fsync&lt;/code&gt; could lead to data loss under exactly this scenario. However, it is especially concerning that this scenario led to persistent replica divergence, not just data loss! We filed #7567 for this issue, and the NATS team is investigating.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;â&lt;/cell&gt;
        &lt;cell role="head"&gt;Summary&lt;/cell&gt;
        &lt;cell role="head"&gt;Event Required&lt;/cell&gt;
        &lt;cell role="head"&gt;Fixed in&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;#6888&lt;/cell&gt;
        &lt;cell&gt;Stream deleted on crash in 2.10.22&lt;/cell&gt;
        &lt;cell&gt;Crashes&lt;/cell&gt;
        &lt;cell&gt;2.10.23&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;#7549&lt;/cell&gt;
        &lt;cell&gt;Lost writes due to &lt;code&gt;.blk&lt;/code&gt; file corruption&lt;/cell&gt;
        &lt;cell&gt;Minority truncation or bitflip&lt;/cell&gt;
        &lt;cell&gt;Unresolved&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;#7556&lt;/cell&gt;
        &lt;cell&gt;Stream deleted due to snapshot file corruption&lt;/cell&gt;
        &lt;cell&gt;Minority truncation or bitflip&lt;/cell&gt;
        &lt;cell&gt;Unresolved&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;#7564&lt;/cell&gt;
        &lt;cell&gt;Write loss due to lazy &lt;code&gt;fsync&lt;/code&gt; policy&lt;/cell&gt;
        &lt;cell&gt;Coordinated OS crash&lt;/cell&gt;
        &lt;cell&gt;Documented&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;#7567&lt;/cell&gt;
        &lt;cell&gt;Write loss and split-brain&lt;/cell&gt;
        &lt;cell&gt;Single OS crash and pause&lt;/cell&gt;
        &lt;cell&gt;Unresolved&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h1"&gt;4 Discussion&lt;/head&gt;
    &lt;p&gt;In NATS 2.10.22, process crashes could cause JetStream to forget a stream ever existed (#6888). This issue was identified independently by NATS and resolved in version 2.10.23, released on 2024-12-10. We did not observe data loss with simple network partitions, process pauses, or crashes in version 2.12.1.&lt;/p&gt;
    &lt;p&gt;However, we found that in NATS 2.12.1, file corruption and simulated OS crashes could both lead to data loss and persistent split-brain. Bitflips or truncation of either &lt;code&gt;.blk&lt;/code&gt; (#7549) or snapshot (#7556) files, even on a minority of nodes, could cause the loss of single messages, large windows of messages, or even cause some nodes to delete their stream data altogether. Messages could be missing on some nodes and present on others. NATS has multiple checksum mechanisms designed to limit the impact of file corruption; more thorough testing of these mechanisms seems warranted.&lt;/p&gt;
    &lt;p&gt;By default, NATS only flushes data to disk every two minutes, but acknowledges operations immediately. This approach can lead to the loss of committed writes when several nodes experience a power failure, kernel crash, or hardware fault concurrentlyâor in rapid succession (#7564). In addition, a single OS crash combined with process crashes, pauses, or network partitions can cause the loss of acknowledged messages and persistent split-brain (#7567). We recommended NATS change the default value of &lt;code&gt;fsync&lt;/code&gt; to &lt;code&gt;always&lt;/code&gt;, or clearly document these hazards. NATS has added new documentation to the JetStream Concepts page.&lt;/p&gt;
    &lt;p&gt;This documentation also describes several goals for JetStream, including that â[t]he system must self-heal and always be available.â This is impossible: the CAP theorem states that Linearizable systems cannot be totally available in an asynchronous network. In our three and five-node clusters JetStream generally behaved like a typical Raft implementation. Operations proceeded on a majority of connected nodes but isolated nodes were unavailable, and if a majority failed, the system as a whole became unavailable. Jepsen suggests clarifying this part of the documentation.&lt;/p&gt;
    &lt;p&gt;As always, Jepsen takes an experimental approach to safety verification: we can prove the presence of bugs, but not their absence. While we make extensive efforts to find problems, we cannot prove correctness.&lt;/p&gt;
    &lt;head rend="h2"&gt;4.1 LazyFS&lt;/head&gt;
    &lt;p&gt;This work demonstrates that systems which do not exhibit data loss under normal process crashes (e.g.Â &lt;code&gt;kill -9 &amp;lt;PID&amp;gt;&lt;/code&gt;) may lose data or enter split-brain under simulated OS-level crashes. Our tests relied heavily on LazyFS, a project of INESC TEC at the University of Porto.7 After killing a process, we used LazyFS to simulate the effects of a power failure by dropping writes to the filesystem which had not yet been &lt;code&gt;fsync&lt;/code&gt;ed to disk.&lt;/p&gt;
    &lt;p&gt;While this work focused purely on the loss of unflushed writes, LazyFS can also simulate linear and non-linear torn writes: an anomaly where a storage device persists part, but not all, of written data thanks to (e.g.) IO cache reordering. Our 2024 paper When Amnesia Strikes discusses these faults in more detail, highlighting bugs in PostgreSQL, Redis, ZooKeeper, etcd, LevelDB, PebblesDB, and the Lightning Network.&lt;/p&gt;
    &lt;head rend="h2"&gt;4.2 Future Work&lt;/head&gt;
    &lt;p&gt;We designed only a simple workload for NATS which checked for lost records either across all consumers, or across all consumers bound to a single node. We did not check whether single consumers could miss messages, or the order in which they were delivered. We did not check NATSâ claims of Linearizable writes or Serializable operations in general. We also did not evaluate JetStreamâs âexactly-once semanticsâ. All of these could prove fruitful avenues for further tests.&lt;/p&gt;
    &lt;p&gt;In some tests, we added and removed nodes from the cluster. This work generated some preliminary results. However, the NATS documentation for membership changes was incorrect and incomplete: it gave the wrong command for removing peers, and there appears to be an undocumented but mandatory health check step for newly-added nodes. As of this writing, Jepsen is unsure how to safely add or remove nodes to a NATS cluster. Consequently, we leave membership changes for future research.&lt;/p&gt;
    &lt;p&gt;Our thanks to INESC TEC and everyone on the LazyFS team, including Maria Ramos, JoÃ£o Azevedo, JosÃ© Pereira, TÃ¢nia Esteves, Ricardo Macedo, and JoÃ£o Paulo. Jepsen is also grateful to Silvia Botros, Kellan Elliott-McCrea, Carla Geisser, Coda Hale, and Marc Hedlund for their expertise regarding datacenter power failures, correlated kernel panics, disk faults, and other causes of OS-level crashes. Finally, our thanks to Irene Kannyo for her editorial support. This research was performed independently by Jepsen, without compensation, and conducted in accordance with the Jepsen ethics policy.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Throughout this report we use âacknowledged messageâ to describe a message whose&lt;/p&gt;&lt;code&gt;publish&lt;/code&gt;request was acknowledged successfully by some server. NATS also offers a separate notion of acknowledgement, which indicates when a message has been processed and need not be delivered again.â©ï¸&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;JetStream also promises âexactly once semanticsâ in some scenarios. We leave this for later research.â©ï¸&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The CAP theoremâs definition of âavailabilityâ requires that all operations on non-faulty nodes must succeed.â©ï¸&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;This is overly conservative: in a system with Linearizable writes, we should never observe a lost message which was acknowledged prior to the invocation of the&lt;/p&gt;&lt;code&gt;publish&lt;/code&gt;call for an OK message, regardless of process. However, early testing with NATS suggested that it might be better to test a weaker property, and come to stronger conclusions about data loss.â©ï¸&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Redpanda argues that the situation is actually worse: a single power failure, combined with network partitions or process pauses, can cause Kafka to lose committed data.â©ï¸&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Some protocols, like Raft, consider an operation committed as soon as it is acknowledged by a majority of nodes. These systems offer lower latencies, but at any given time there are likely a few committed operations which are missing from a minority of nodes due to normal network latency. Other systems, like Kafka, require acknowledgement from all âonlineâ nodes before considering an operation committed. These systems offer worse latency in healthy clusters (since they must wait for the slowest node) but in exchange, committed operations can only be missing from some node when the fault detector decides that node is no longer online (e.g.Â due to elevated latency).â©ï¸&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Jepsen contributed some funds, testing, and integration assistance to LazyFS, but most credit belongs to the LazyFS team.â©ï¸&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46196105</guid><pubDate>Mon, 08 Dec 2025 18:51:03 +0000</pubDate></item><item><title>GitHub Notifications triggered by spam accounts are now correctly hidden</title><link>https://github.blog/changelog/2025-12-04-notifications-triggered-by-spam-accounts-are-now-correctly-hidden/</link><description>&lt;doc fingerprint="f306438def20e7e8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Notifications triggered by spam accounts are now correctly hidden&lt;/head&gt;
    &lt;p&gt;We’ve improved the way notifications from spammy repositories and users are handled, resulting in clearer, more accurate notification counts.&lt;/p&gt;
    &lt;head rend="h3"&gt;Notifications triggered by spam accounts are now correctly hidden&lt;/head&gt;
    &lt;p&gt;We’ve updated notification handling so that when a user or repository is flagged as spam, all notifications originating from their activity are reliably hidden, including past mentions from suspicious accounts. Previously, notifications triggered by spammy activity could remain visible and contribute to your unread repository counters, even after hiding or flagging them. This led to confusion and unnecessary clutter in the notification experience.&lt;/p&gt;
    &lt;p&gt;Now, once an account or repository is marked as spam, notifications they triggered will no longer show up in sidebar counters or counts, even if you were mentioned by them before the spam was detected. As part of this change, close to 6 million spam-related notifications were cleaned up across GitHub to help keep your notifications clear and actionable.&lt;/p&gt;
    &lt;head rend="h3"&gt;How to give feedback&lt;/head&gt;
    &lt;p&gt;If you have thoughts or feedback on this release, leave a comment on our Community discussion post.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46196260</guid><pubDate>Mon, 08 Dec 2025 19:03:21 +0000</pubDate></item></channel></rss>