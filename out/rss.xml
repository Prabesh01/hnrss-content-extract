<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Fri, 03 Oct 2025 21:32:02 +0000</lastBuildDate><item><title>Niri – A scrollable-tiling Wayland compositor</title><link>https://github.com/YaLTeR/niri</link><description>&lt;doc fingerprint="f76fe4761cb74b90"&gt;
  &lt;main&gt;
    &lt;p&gt;A scrollable-tiling Wayland compositor.&lt;/p&gt;
    &lt;p&gt;Getting Started | Configuration | Setup Showcase&lt;/p&gt;
    &lt;p&gt;Windows are arranged in columns on an infinite strip going to the right. Opening a new window never causes existing windows to resize.&lt;/p&gt;
    &lt;p&gt;Every monitor has its own separate window strip. Windows can never "overflow" onto an adjacent monitor.&lt;/p&gt;
    &lt;p&gt;Workspaces are dynamic and arranged vertically. Every monitor has an independent set of workspaces, and there's always one empty workspace present all the way down.&lt;/p&gt;
    &lt;p&gt;The workspace arrangement is preserved across disconnecting and connecting monitors where it makes sense. When a monitor disconnects, its workspaces will move to another monitor, but upon reconnection they will move back to the original monitor.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Built from the ground up for scrollable tiling&lt;/item&gt;
      &lt;item&gt;Dynamic workspaces like in GNOME&lt;/item&gt;
      &lt;item&gt;An Overview that zooms out workspaces and windows&lt;/item&gt;
      &lt;item&gt;Built-in screenshot UI&lt;/item&gt;
      &lt;item&gt;Monitor and window screencasting through xdg-desktop-portal-gnome &lt;list rend="ul"&gt;&lt;item&gt;You can block out sensitive windows from screencasts&lt;/item&gt;&lt;item&gt;Dynamic cast target that can change what it shows on the go&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Touchpad and mouse gestures&lt;/item&gt;
      &lt;item&gt;Group windows into tabs&lt;/item&gt;
      &lt;item&gt;Configurable layout: gaps, borders, struts, window sizes&lt;/item&gt;
      &lt;item&gt;Gradient borders with Oklab and Oklch support&lt;/item&gt;
      &lt;item&gt;Animations with support for custom shaders&lt;/item&gt;
      &lt;item&gt;Live-reloading config&lt;/item&gt;
      &lt;item&gt;Works with screen readers&lt;/item&gt;
    &lt;/list&gt;
    &lt;head class="px-3 py-2"&gt;demo.mp4&lt;/head&gt;
    &lt;p&gt;Also check out this video from Brodie Robertson that showcases a lot of the niri functionality: Niri Is My New Favorite Wayland Compositor&lt;/p&gt;
    &lt;p&gt;Niri is stable for day-to-day use and does most things expected of a Wayland compositor. Many people are daily-driving niri, and are happy to help in our Matrix channel.&lt;/p&gt;
    &lt;p&gt;Give it a try! Follow the instructions on the Getting Started page. Have your waybars and fuzzels ready: niri is not a complete desktop environment. Also check out awesome-niri, a list of niri-related links and projects.&lt;/p&gt;
    &lt;p&gt;Here are some points you may have questions about:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Multi-monitor: yes, a core part of the design from the very start. Mixed DPI works.&lt;/item&gt;
      &lt;item&gt;Fractional scaling: yes, plus all niri UI stays pixel-perfect.&lt;/item&gt;
      &lt;item&gt;NVIDIA: seems to work fine.&lt;/item&gt;
      &lt;item&gt;Floating windows: yes, starting from niri 25.01.&lt;/item&gt;
      &lt;item&gt;Input devices: niri supports tablets, touchpads, and touchscreens. You can map the tablet to a specific monitor, or use OpenTabletDriver. We have touchpad gestures, but no touchscreen gestures yet.&lt;/item&gt;
      &lt;item&gt;Wlr protocols: yes, we have most of the important ones like layer-shell, gamma-control, screencopy. You can check on wayland.app at the bottom of each protocol's page.&lt;/item&gt;
      &lt;item&gt;Performance: while I run niri on beefy machines, I try to stay conscious of performance. I've seen someone use it fine on an Eee PC 900 from 2008, of all things.&lt;/item&gt;
      &lt;item&gt;Xwayland: integrated via xwayland-satellite starting from niri 25.08.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;niri: Making a Wayland compositor in Rust · December 2024&lt;/p&gt;
    &lt;p&gt;My talk from the 2024 Moscow RustCon about niri, and how I do randomized property testing and profiling, and measure input latency. The talk is in Russian, but I prepared full English subtitles that you can find in YouTube's subtitle language selector.&lt;/p&gt;
    &lt;p&gt;An interview with Ivan, the developer behind Niri · June 2025&lt;/p&gt;
    &lt;p&gt;An interview by a German tech podcast Das Triumvirat (in English). We talk about niri development and history, and my experience building and maintaining niri.&lt;/p&gt;
    &lt;p&gt;A tour of the niri scrolling-tiling Wayland compositor · July 2025&lt;/p&gt;
    &lt;p&gt;An LWN article with a nice overview and introduction to niri.&lt;/p&gt;
    &lt;p&gt;If you'd like to help with niri, there are plenty of both coding- and non-coding-related ways to do so. See CONTRIBUTING.md for an overview.&lt;/p&gt;
    &lt;p&gt;Niri is heavily inspired by PaperWM which implements scrollable tiling on top of GNOME Shell.&lt;/p&gt;
    &lt;p&gt;One of the reasons that prompted me to try writing my own compositor is being able to properly separate the monitors. Being a GNOME Shell extension, PaperWM has to work against Shell's global window coordinate space to prevent windows from overflowing.&lt;/p&gt;
    &lt;p&gt;Here are some other projects which implement a similar workflow:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;PaperWM: scrollable tiling on top of GNOME Shell.&lt;/item&gt;
      &lt;item&gt;karousel: scrollable tiling on top of KDE.&lt;/item&gt;
      &lt;item&gt;scroll and papersway: scrollable tiling on top of sway/i3.&lt;/item&gt;
      &lt;item&gt;hyprscrolling and hyprslidr: scrollable tiling on top of Hyprland.&lt;/item&gt;
      &lt;item&gt;PaperWM.spoon: scrollable tiling on top of macOS.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Our main communication channel is a Matrix chat, feel free to join and ask a question: https://matrix.to/#/#niri:matrix.org&lt;/p&gt;
    &lt;p&gt;We also have a community Discord server: https://discord.gg/vT8Sfjy7sx&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45461500</guid><pubDate>Fri, 03 Oct 2025 11:08:42 +0000</pubDate></item><item><title>The Faroes</title><link>https://photoblog.nk412.com/Faroe2025/Faroes/n-cPCNFr</link><description>&lt;doc fingerprint="b6601c6e8caa7e9e"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The Faroes (2025)&lt;/head&gt;
    &lt;p&gt;The Faroe Islands are like the child that Denmark and Iceland had, but forgot to tell the world about. This group of eighteen small islands receives the least amount of sunshine in the world per year. Constant rain and heavy winds have always battered these lands.&lt;lb/&gt;Politically part of Denmark (for now) but fiercely independent in spirit, the Faroes exist in their own bubble of Nordic culture. Here, sheep outnumber humans two to one, villages of colorful houses cling to clifftops like they're holding on for dear life, and the weather can shift from apocalyptic storms to sunny calm in the space of an hour.&lt;/p&gt;
    &lt;p&gt;Situated between Iceland, Norway and Scotland, the Faroes face the brunt of the North Atlantic weather system. Constant storms and crashing waves have sculpted the volcanic rock over millions of years into some of the most jaw-dropping (and vertigo-inducing) coastlines on Earth. These towering basalt cliffs can reach heights of over 400 meters, dropping straight into churning seas below.&lt;lb/&gt;What's most striking is how abruptly the land stops. There are no sandy beaches or gentle slopes here—the islands simply plunge headfirst into the Atlantic. One step you're on grass-covered clifftops, the next you're staring down hundreds of meters of sheer volcanic rock to where waves explode against the base far below.&lt;/p&gt;
    &lt;p&gt;The weather here is unpredictable, and changes faster than you can put your raincoat on—one minute you're in thick fog, the next you're hit with winds and piercing rain that'll knock you sideways, then suddenly the clouds part to reveal views that'll make your camera work overtime.&lt;/p&gt;
    &lt;p&gt;Meet the true locals of the Faroes. These wooly sheep have been roaming the islands for over a thousand years, and they outnumber people on the islands. They couldn't care less about your hiking plans and will casually block paths or graze on the edge of 200-meter cliffs like it's the most natural thing in the world.&lt;lb/&gt;Faroe's name comes from a combination of fær (sheep) and eyjar (islands). &lt;/p&gt;
    &lt;p&gt;Unlike their farm-bound cousins elsewhere, Faroese sheep roam completely free across the islands, somehow always managing to find the most photogenic spots for an impromptu rest. This fellow right here is the only one that gave me any sort of attention. Otherwise, they are all busy grazing on all the grass they could ever ask for.&lt;/p&gt;
    &lt;p&gt;Why fight the landscape? For over a millennium, islanders have been topping their huts with birch bark and soil and let the grass grow wild. They act as insulation, and the thick roots are an excellent waterproof seal against the weather.&lt;lb/&gt;The grass grows quickly and does need tending every once in a while. In typical Faroese fashion, the solution is simple: put a sheep on top for an afternoon.&lt;/p&gt;
    &lt;p&gt;On the northern tip of Kalsoy lies the Kallur lighthouse. Like most regions on the islands, the land is privately owned. Hiking usually incurs a modest fee paid at the trailhead to the land owners, and the rest is up to you. Trails are just sheep paths, worn smooth by countless hooves over years rather than any official trail maintenance.&lt;/p&gt;
    &lt;p&gt;There are no guardrails, no warning signs, and definitely no liability waivers - just you, the weather, and whatever route the sheep decided made sense. The approach to Kallur is particularly gnarly, following a knife-edge ridge with steep drops on both sides before reaching the lighthouse perched dramatically on sea cliffs.&lt;/p&gt;
    &lt;p&gt;In No Time To Die (2021), Daniel Craig's James Bond meets his end at the villain's lair, which happened to be here on Kalsoy. The Faroese then followed through with the obvious next step.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45462297</guid><pubDate>Fri, 03 Oct 2025 12:41:03 +0000</pubDate></item><item><title>Arbitrary code execution in Unity Runtime</title><link>https://flatt.tech/research/posts/arbitrary-code-execution-in-unity-runtime/</link><description>&lt;doc fingerprint="b56811138876f9e9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;CVE-2025-59489: Arbitrary Code Execution in Unity Runtime&lt;/head&gt;
    &lt;head rend="h5"&gt;Posted on October 3, 2025 • 6 minutes • 1067 words&lt;/head&gt;
    &lt;head class="flex items-center font-bold py-2 px-4 cursor-pointer justify-between select-none text-black dark:text-white"&gt;Table of contents&lt;/head&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;Hello, I’m RyotaK (@ryotkak ), a security engineer at GMO Flatt Security Inc.&lt;/p&gt;
    &lt;p&gt;In May 2025, I participated in the Meta Bug Bounty Researcher Conference 2025. During this event, I discovered a vulnerability (CVE-2025-59489) in the Unity Runtime that affects games and applications built on Unity 2017.1 and later.&lt;/p&gt;
    &lt;p&gt;In this article, I will explain the technical aspects of this vulnerability and its impact.&lt;/p&gt;
    &lt;p&gt;This vulnerability was disclosed to Unity following responsible disclosure practices.&lt;lb/&gt; Unity has since released patches for Unity 2019.1 and later, as well as a Unity Binary Patch tool to address the issue, and I strongly encourage developers to download the updated versions of Unity, recompile affected games or applications, and republish as soon as possible.&lt;/p&gt;
    &lt;p&gt;For the official security advisory, please refer to Unity’s advisory here: https://unity.com/security/sept-2025-01&lt;/p&gt;
    &lt;p&gt;We appreciate Unity’s commitment to addressing this issue promptly and their ongoing efforts to enhance the security of their platform.&lt;lb/&gt; Security vulnerabilities are an inherent challenge in software development, and by working together as a community, we can continue to make software systems safer for everyone.&lt;/p&gt;
    &lt;head rend="h2"&gt;TL;DR&lt;/head&gt;
    &lt;p&gt;A vulnerability was identified in the Unity Runtime’s intent handling process for Unity games and applications.&lt;lb/&gt; This vulnerability allows malicious intents to control command line arguments passed to Unity applications, enabling attackers to load arbitrary shared libraries (&lt;code&gt;.so&lt;/code&gt; files) and execute malicious code, depending on the platform.&lt;/p&gt;
    &lt;p&gt;In its default configuration, this vulnerability allowed malicious applications installed on the same device to hijack permissions granted to Unity applications.&lt;lb/&gt; In specific cases, the vulnerability could be exploited remotely to execute arbitrary code, although I didn’t investigate third-party Unity applications to find an app with the functionality required to enable this exploit.&lt;/p&gt;
    &lt;p&gt;Unity has addressed this issue and has updated all affected Unity versions starting with 2019.1. Developers are strongly encouraged to download them, recompile their games and applications, and republish to ensure their projects remain secure.&lt;/p&gt;
    &lt;head rend="h2"&gt;About Unity&lt;/head&gt;
    &lt;p&gt;Unity is a popular game engine used to develop games and applications for various platforms, including Android.&lt;/p&gt;
    &lt;p&gt;According to Unity’s website, 70% of top mobile games are built with Unity. This includes popular games like Among Us and Pokémon GO, along with many other applications that use Unity for development.&lt;/p&gt;
    &lt;head rend="h2"&gt;Technical Details&lt;/head&gt;
    &lt;p&gt;Note: During the analysis, I used Android 16.0 on the Android Emulator of Android Studio. The behavior and impact of this vulnerability may differ on older Android versions.&lt;/p&gt;
    &lt;head rend="h3"&gt;Unity’s Intent Handler&lt;/head&gt;
    &lt;p&gt;To support debugging Unity applications on Android devices, Unity automatically adds a handler for the intent containing the &lt;code&gt;unity&lt;/code&gt; extra to the UnityPlayerActivity. This activity serves as the default entry point for applications and is exported to other applications.&lt;/p&gt;
    &lt;p&gt;https://docs.unity3d.com/6000.0/Documentation/Manual/android-custom-activity-command-line.html&lt;/p&gt;
    &lt;code&gt;adb shell am start -n "com.Company.MyGame/com.unity3d.player.UnityPlayerActivity" -e unity "-systemallocator"
&lt;/code&gt;
    &lt;p&gt;As documented above, the &lt;code&gt;unity&lt;/code&gt; extra is parsed as command line arguments for Unity.&lt;/p&gt;
    &lt;p&gt;While Android’s permission model manages feature access by granting permissions to applications, it does not restrict which intents can be sent to an application.&lt;lb/&gt; This means any application can send the &lt;code&gt;unity&lt;/code&gt; extra to a Unity application, allowing attackers to control the command line arguments passed to that application.&lt;/p&gt;
    &lt;head rend="h3"&gt;xrsdk-pre-init-library Command Line Argument&lt;/head&gt;
    &lt;p&gt;After loading the Unity Runtime binary into Ghidra, I discovered the following command line argument:&lt;/p&gt;
    &lt;code&gt;initLibPath = FUN_00272540(uVar5, "xrsdk-pre-init-library");
&lt;/code&gt;
    &lt;p&gt;The value of this command line argument is later passed to &lt;code&gt;dlopen&lt;/code&gt;, causing the path specified in &lt;code&gt;xrsdk-pre-init-library&lt;/code&gt; to be loaded as a native library.&lt;/p&gt;
    &lt;code&gt;lVar2 = dlopen(initLibPath, 2);  
&lt;/code&gt;
    &lt;p&gt;This behavior allows attackers to execute arbitrary code within the context of the Unity application, leveraging its permissions by launching them with the -xrsdk-pre-init-library argument.&lt;/p&gt;
    &lt;head rend="h2"&gt;Attack Scenarios&lt;/head&gt;
    &lt;head rend="h3"&gt;Local Attack&lt;/head&gt;
    &lt;p&gt;Any malicious application installed on the same device can exploit this vulnerability by:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Extracting the native library with the &lt;code&gt;android:extractNativeLibs&lt;/code&gt;attribute set to&lt;code&gt;true&lt;/code&gt;in the AndroidManifest.xml&lt;/item&gt;
      &lt;item&gt;Launching the Unity application with the &lt;code&gt;-xrsdk-pre-init-library&lt;/code&gt;argument pointing to the malicious library&lt;/item&gt;
      &lt;item&gt;The Unity application would then load and execute the malicious code with its own permissions&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Remote Exploitation via Browser&lt;/head&gt;
    &lt;p&gt;In specific cases, this vulnerability could potentially be exploited remotely although the condition .&lt;lb/&gt; For example, if an application exports &lt;code&gt;UnityPlayerActivity&lt;/code&gt; or &lt;code&gt;UnityPlayerGameActivity&lt;/code&gt; with the &lt;code&gt;android.intent.category.BROWSABLE&lt;/code&gt; category (allowing browser launches), websites can specify extras passed to the activity using intent URLs:&lt;/p&gt;
    &lt;code&gt;intent:#Intent;package=com.example.unitygame;scheme=custom-scheme;S.unity=-xrsdk-pre-init-library%20/data/local/tmp/malicious.so;end;
&lt;/code&gt;
    &lt;p&gt;At first glance, it might appear that malicious websites could exploit this vulnerability by forcing browsers to download &lt;code&gt;.so&lt;/code&gt; files and load them via the &lt;code&gt;xrsdk-pre-init-library&lt;/code&gt; argument.&lt;/p&gt;
    &lt;head rend="h3"&gt;SELinux Restrictions&lt;/head&gt;
    &lt;p&gt;However, Android’s strict SELinux policy prevents &lt;code&gt;dlopen&lt;/code&gt; from opening files in the downloads directory, which mitigates almost all remote exploitation scenarios.&lt;/p&gt;
    &lt;code&gt;library "/sdcard/Download/libtest.so" ("/storage/emulated/0/Download/libtest.so") needed 
or dlopened by "/data/app/~~24UwD8jnw7asNjRwx1MOBg==/com.DefaultCompany.com.unity.template. 
mobile2D-E043IptGJDwcTqq56BocIA==/lib/arm64/libunity.so" is not accessible for the 
namespace: [name="clns-9", ld_library_paths="",default_library_paths="/data/app/~~24UwD8jnw7asNjRwx1MOBg==/com.DefaultCompany.com.unity.template. 
mobile2D-E043IptGJDwcTqq56BocIA==/lib/arm64:/data/app/~~24UwD8jnw7asNjRwx1MOBg==/com.DefaultCompany.com.unity.template.mobile2D-E043IptGJDwcTqq56BocIA==/base.apk!/lib/arm64-v8a", permitted_paths="/data:/mnt/expand:/data/data/com.DefaultCompany.com.unity.template.mobile2D"]
&lt;/code&gt;
    &lt;p&gt;That being said, since the &lt;code&gt;/data/&lt;/code&gt; directory is included in &lt;code&gt;permitted_paths&lt;/code&gt;, if the target application writes files to its private storage, it can be used to bypass this restriction.&lt;/p&gt;
    &lt;p&gt;Furthermore, &lt;code&gt;dlopen&lt;/code&gt; doesn’t require the &lt;code&gt;.so&lt;/code&gt; file extension. If attackers can control the content of a file in an application’s private storage, they can exploit this vulnerability by creating a file containing malicious native library binary. This is actually a common pattern when applications cache data.&lt;/p&gt;
    &lt;p&gt;For example, another vulnerability in Messenger was exploited using the application’s cache: https://www.hexacon.fr/slides/Calvanno-Defense_through_Offense_Building_a_1-click_Exploit_Targeting_Messenger_for_Android.pdf&lt;/p&gt;
    &lt;head rend="h3"&gt;Requirements for Remote Exploitation&lt;/head&gt;
    &lt;p&gt;To exploit this vulnerability remotely, the following conditions must be met:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The application exports &lt;code&gt;UnityPlayerActivity&lt;/code&gt;or&lt;code&gt;UnityPlayerGameActivity&lt;/code&gt;with the&lt;code&gt;android.intent.category.BROWSABLE&lt;/code&gt;category&lt;/item&gt;
      &lt;item&gt;The application writes files with attacker-controlled content to its private storage (e.g., through caching)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Even without these conditions, local exploitation remains possible for any Unity application.&lt;/p&gt;
    &lt;head rend="h2"&gt;Demonstration&lt;/head&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;In this article, I explained a vulnerability in Unity Runtime that allows arbitrary code execution in almost all Unity applications on Android.&lt;/p&gt;
    &lt;p&gt;I hope this article helps you understand that vulnerabilities can exist in the frameworks and libraries you depend on, and you should always be mindful of the security implications of the features you use.&lt;/p&gt;
    &lt;head rend="h2"&gt;Shameless plug&lt;/head&gt;
    &lt;p&gt;At GMO Flatt Security, we provide top-notch penetration testing for a wide range of targets, from Web apps to IoT devices.&lt;/p&gt;
    &lt;p&gt;https://flatt.tech/en/professional/penetration_test&lt;/p&gt;
    &lt;p&gt;We also developed Takumi, our AI security engineer. It’s an autonomous agent that finds vulnerabilities in source code and has already discovered CVEs in major libraries like Vim and Next.js. https://flatt.tech/en/takumi&lt;/p&gt;
    &lt;p&gt;Recently, we’ve expanded Takumi’s capabilities. It’s no longer just a SAST (white-box testing) tool; we’ve added DAST (black-box testing) to enable high-fidelity gray-box scanning for more accurate results.&lt;/p&gt;
    &lt;p&gt;Based in Japan, we work with clients globally, including industry leaders like Canonical Ltd.&lt;/p&gt;
    &lt;p&gt;If you’d like to learn more, please contact us at https://flatt.tech/en&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45462713</guid><pubDate>Fri, 03 Oct 2025 13:21:44 +0000</pubDate></item><item><title>Webbol: A minimal static web server written in COBOL</title><link>https://github.com/jmsdnns/webbol</link><description>&lt;doc fingerprint="11bcbc59fd061f87"&gt;
  &lt;main&gt;
    &lt;p&gt;A minimal static web server written in COBOL using GnuCOBOL.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Serves static files from the current directory&lt;/item&gt;
      &lt;item&gt;Automatic MIME type detection for common file types&lt;/item&gt;
      &lt;item&gt;HTTP status codes: 200 (OK), 403 (Forbidden), 404 (Not Found), 413 (Payload Too Large)&lt;/item&gt;
      &lt;item&gt;Path traversal attack prevention&lt;/item&gt;
      &lt;item&gt;Clean request logging with full HTTP headers&lt;/item&gt;
      &lt;item&gt;Defaults to &lt;code&gt;index.html&lt;/code&gt;for root path requests&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;GnuCOBOL (cobc) compiler&lt;/item&gt;
      &lt;item&gt;POSIX-compatible operating system (Linux, macOS, BSD)&lt;/item&gt;
      &lt;item&gt;make&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;macOS:&lt;/p&gt;
    &lt;code&gt;brew install gnucobol&lt;/code&gt;
    &lt;p&gt;Ubuntu/Debian:&lt;/p&gt;
    &lt;code&gt;sudo apt-get install gnucobol&lt;/code&gt;
    &lt;p&gt;Fedora/RHEL:&lt;/p&gt;
    &lt;code&gt;sudo dnf install gnucobol&lt;/code&gt;
    &lt;p&gt;Clone or download the repository, then compile:&lt;/p&gt;
    &lt;code&gt;make&lt;/code&gt;
    &lt;p&gt;This will compile all modules and create the &lt;code&gt;webserver&lt;/code&gt; executable.&lt;/p&gt;
    &lt;p&gt;To clean build artifacts:&lt;/p&gt;
    &lt;code&gt;make clean&lt;/code&gt;
    &lt;p&gt;Start the server from the directory you want to serve:&lt;/p&gt;
    &lt;code&gt;./webserver&lt;/code&gt;
    &lt;p&gt;The server will start on port 8080 and serve files from the current directory.&lt;/p&gt;
    &lt;code&gt;# Create a test HTML file
echo "&amp;lt;html&amp;gt;&amp;lt;body&amp;gt;&amp;lt;h1&amp;gt;Hello from COBOL!&amp;lt;/h1&amp;gt;&amp;lt;/body&amp;gt;&amp;lt;/html&amp;gt;" &amp;gt; index.html

# Start the server
./webserver

# In another terminal, test it
curl http://localhost:8080/&lt;/code&gt;
    &lt;p&gt;Once running, you can access files via:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;http://localhost:8080/&lt;/code&gt;- serves&lt;code&gt;index.html&lt;/code&gt;from the current directory&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;http://localhost:8080/filename.html&lt;/code&gt;- serves the specified file&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;http://localhost:8080/path/to/file.txt&lt;/code&gt;- serves files from subdirectories&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Press &lt;code&gt;Ctrl+C&lt;/code&gt; to stop the server.&lt;/p&gt;
    &lt;p&gt;To change the server port, edit &lt;code&gt;config.cpy&lt;/code&gt; and modify the &lt;code&gt;SERVER-PORT&lt;/code&gt; value:&lt;/p&gt;
    &lt;code&gt;01 SERVER-PORT          PIC 9(5) VALUE 8080.&lt;/code&gt;
    &lt;p&gt;Then recompile with &lt;code&gt;make&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;webbol/
├── Makefile              # Build configuration
├── README.md            # This file
├── config.cpy           # Server configuration
├── socket-defs.cpy      # Socket structure definitions
├── http-structs.cpy     # HTTP data structures
├── file-structs.cpy     # File handling structures
├── path-utils.cbl       # Path validation and sanitization
├── mime-types.cbl       # MIME type detection
├── file-ops.cbl         # File reading operations
├── http-handler.cbl     # HTTP request/response handling
└── webserver.cbl        # Main server program
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;HTML: &lt;code&gt;text/html&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;CSS: &lt;code&gt;text/css&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;JavaScript: &lt;code&gt;application/javascript&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;JSON: &lt;code&gt;application/json&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;XML: &lt;code&gt;application/xml&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Plain text: &lt;code&gt;text/plain&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;PNG: &lt;code&gt;image/png&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;JPEG: &lt;code&gt;image/jpeg&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;GIF: &lt;code&gt;image/gif&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;SVG: &lt;code&gt;image/svg+xml&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;ICO: &lt;code&gt;image/x-icon&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;PDF: &lt;code&gt;application/pdf&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Additional MIME types can be added by editing &lt;code&gt;mime-types.cbl&lt;/code&gt;.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Path traversal prevention: Blocks requests containing &lt;code&gt;..&lt;/code&gt;sequences&lt;/item&gt;
      &lt;item&gt;Directory access restriction: Only serves files from the current directory and subdirectories&lt;/item&gt;
      &lt;item&gt;Safe file handling: Validates all paths before file system access&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Single-threaded: Handles one request at a time&lt;/item&gt;
      &lt;item&gt;No SSL/TLS support&lt;/item&gt;
      &lt;item&gt;Maximum file size: 64KB&lt;/item&gt;
      &lt;item&gt;Line sequential file organization only (text files)&lt;/item&gt;
      &lt;item&gt;No caching or compression&lt;/item&gt;
      &lt;item&gt;No range requests or partial content support&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Port already in use:&lt;/p&gt;
    &lt;code&gt;Bind failed - check if port is in use
&lt;/code&gt;
    &lt;p&gt;Another process is using port 8080. Either stop that process or change the port in &lt;code&gt;config.cpy&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Permission denied: Ensure the files you're trying to serve have read permissions and the current user can access them.&lt;/p&gt;
    &lt;p&gt;File not found (404): Verify the file exists in the current directory where the server is running. File paths are case-sensitive.&lt;/p&gt;
    &lt;p&gt;This project is released into the public domain. Use it however you'd like.&lt;/p&gt;
    &lt;p&gt;Built with GnuCOBOL, demonstrating that COBOL can still be used for modern systems programming tasks.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45463251</guid><pubDate>Fri, 03 Oct 2025 14:13:03 +0000</pubDate></item><item><title>I turned the Lego Game Boy into a working Game Boy</title><link>https://blog.nataliethenerd.com/i-turned-the-lego-game-boy-into-a-working-game-boy-part-1/</link><description>&lt;doc fingerprint="15cf6c54ebc4b5f9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;I turned the Lego Game Boy into a working Game Boy part. 1&lt;/head&gt;
    &lt;p&gt;Through my documentation of Game Boy boards, I have drawn up schematics of each device. I know them pretty well. Check out my board scan wiki https://wiki.nataliethenerd.com/&lt;/p&gt;
    &lt;p&gt;I jokingly made this tweet when the kit was announced, but decided to actually do it.&lt;/p&gt;
    &lt;p&gt;I know from experience of routing Game Boy CPU PCBs that there isn't much to it. There's the RAM, CPU, some decoupling capacitors and power regulation. &lt;lb/&gt;Note: I went with the MGB (Pocket) CPU rather than DMG for a couple of reasons.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;They are pretty much the same&lt;/item&gt;
      &lt;item&gt;I have more of them&lt;/item&gt;
      &lt;item&gt;They are cheaper and easier to get. This opens up the project to more people&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The DMG CPU has external VRAM, the MGB CPU has internal VRAM and in a very space conscious build that was the biggest factor.&lt;/p&gt;
    &lt;head rend="h2"&gt;Pre Planning&lt;/head&gt;
    &lt;p&gt;I only had the press pictures to work off. I used the dimensions to scale the image on my PC and from that I got measurements for the screen inserts; since that's where I plan to put the Game Boy.&lt;/p&gt;
    &lt;p&gt;I incorporated the power circuit I use for my Safer Charger boards, changed the power switch to a soft latching power button, added pin outs for the button matrix and audio.&lt;/p&gt;
    &lt;p&gt;I didn't really know what the buttons on the Lego would be like, but the fact that they could be pressed was enough for me to know I could implement them. At the moment I have them wired up to custom 3D printed *toy brick* parts. Same with the USB C&lt;/p&gt;
    &lt;p&gt;I am currently working on refining the board now I have the Lego build in my hands. This project will be released in full once I am finished with it - so stay tuned!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45463319</guid><pubDate>Fri, 03 Oct 2025 14:18:56 +0000</pubDate></item><item><title>LinkedIn sues software company allegedly scraping data from profiles</title><link>https://therecord.media/linkedin-sues-data-scraping-company</link><description>&lt;doc fingerprint="a8a449dc503497ed"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;LinkedIn sues software company allegedly scraping data from millions of profiles&lt;/head&gt;
    &lt;p&gt;Social media giant LinkedIn on Thursday filed a lawsuit against a company which it says operates a network of millions of fake accounts used to scrape data from LinkedIn members before selling the information to third parties without permission.&lt;/p&gt;
    &lt;p&gt;ProAPIs, a software company, and its CEO Rahmat Alam allegedly run an operation which LinkedIn says charges customers up to $15,000 per month for scraped user data taken from the social media platform.&lt;/p&gt;
    &lt;p&gt;With the rise of artificial intelligence, companies which scrape user data at scale are proliferating and are increasingly undermining consumer privacy.&lt;/p&gt;
    &lt;p&gt;Data scraped by ProAPIs allegedly includes LinkedIn member information as well as their posts, reactions and comments, according to the lawsuit, which was filed in a Northern California federal court.&lt;/p&gt;
    &lt;p&gt;“Defendants’ industrial-scale fake account mill scrapes member information that real people have posted on LinkedIn, including data that is only available behind LinkedIn’s password wall and that Defendants’ customers may not otherwise be allowed to access, and certainly are not allowed to copy and keep in perpetuity,” the lawsuit says.&lt;/p&gt;
    &lt;p&gt;ProAPIs did not respond to a request for comment. Creating fake accounts is against LinkedIn’s terms of service.&lt;/p&gt;
    &lt;p&gt;The lawsuit says that Microsoft-owned LinkedIn routinely detects ProAPIs’ scraping within hours of it beginning, but because the software firm creates “hundreds if not thousands” of fake accounts daily it is impossible to stop all of the activity.&lt;/p&gt;
    &lt;p&gt;ProAPIs uses LinkedIn’s trademark to promote its product, according to the lawsuit, which says the copyright abuse falsely suggests that the company is endorsed by the social network.&lt;/p&gt;
    &lt;p&gt;Suzanne Smalley&lt;/p&gt;
    &lt;p&gt;is a reporter covering privacy, disinformation and cybersecurity policy for The Record. She was previously a cybersecurity reporter at CyberScoop and Reuters. Earlier in her career Suzanne covered the Boston Police Department for the Boston Globe and two presidential campaign cycles for Newsweek. She lives in Washington with her husband and three children.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45464111</guid><pubDate>Fri, 03 Oct 2025 15:30:32 +0000</pubDate></item><item><title>Anduril and Palantir battlefield communication system has flaws, Army memo says</title><link>https://www.cnbc.com/2025/10/03/anduril-palantir-ngc2-deep-flaws-army.html</link><description>&lt;doc fingerprint="a573ccbb7c3e5585"&gt;
  &lt;main&gt;
    &lt;p&gt;The much-needed modernization of the U.S. Army's battlefield communications network being undertaken by Anduril, Palantir and others is rife with "fundamental security" problems and vulnerabilities, and should be treated as a "very high risk," according to a recent internal Army memo.&lt;/p&gt;
    &lt;p&gt;The two Silicon Valley companies, led by allies of U.S. President Donald Trump, have gained access to the Pentagon's lucrative flow of contracts on the promise of quickly providing less expensive and more sophisticated weapons than the Pentagon's longstanding arms providers.&lt;/p&gt;
    &lt;p&gt;But the September memo from the Army's chief technology officer about the NGC2 platform that connects soldiers, sensors, vehicles and commanders with real-time data paints a bleak picture of the initial product.&lt;/p&gt;
    &lt;p&gt;"We cannot control who sees what, we cannot see what users are doing, and we cannot verify that the software itself is secure," the memo says.&lt;/p&gt;
    &lt;p&gt;Palantir and Anduril did not comment for this story.&lt;/p&gt;
    &lt;p&gt;The assessment, seen by Reuters and first reported by Breaking Defense, comes just months after defense drone and software maker Anduril was awarded a $100 million to create a prototype of NGC2 with partners including Palantir, Microsoft and several smaller contractors.&lt;/p&gt;
    &lt;p&gt;The Army should treat the NGC2 prototype version as “very high risk” because of the “likelihood of an adversary gaining persistent undetectable access," wrote Gabrielle Chiulli, the Army chief technology officer authorizing official.&lt;/p&gt;
    &lt;p&gt;Despite the early September memo's scathing critique, Leonel Garciga, Army chief information officer and Chiulli's supervisor, said in a statement to Reuters that the report was part of a process that helped in "triaging cybersecurity vulnerabilities" and mitigating them.&lt;/p&gt;
    &lt;p&gt;In March, the 4th Infantry Division used the system in live-fire artillery training at Fort Carson, Colorado, in an exercise Anduril described as demonstrating faster and more reliable performance than legacy systems.&lt;/p&gt;
    &lt;p&gt;The Army memo identifies some major security gaps.&lt;/p&gt;
    &lt;p&gt;The report says the system allows any authorized user to access all applications and data regardless of their clearance level or operational need. As a result, "Any user can potentially access and misuse sensitive" classified information, the memo states, with no logging to track their actions.&lt;/p&gt;
    &lt;p&gt;Other deficiencies highlighted in the memo include the hosting of third-party applications that have not undergone Army security assessments. One application revealed 25 high-severity code vulnerabilities. Three additional applications under review each contain over 200 vulnerabilities requiring assessment, according to the document.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45464269</guid><pubDate>Fri, 03 Oct 2025 15:46:11 +0000</pubDate></item><item><title>Cancelling async Rust</title><link>https://sunshowers.io/posts/cancelling-async-rust/</link><description>&lt;doc fingerprint="310499976ca2b6bc"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Cancelling async Rust&lt;/head&gt;
    &lt;p&gt;This is an edited, written version of my RustConf 2025 talk about cancellations in async Rust. Like the written version of my RustConf 2023 talk, I’ve tried to retain the feel of a talk while making it readable as a standalone blog entry. Some links:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Video of the talk on YouTube.&lt;/item&gt;
      &lt;item&gt;Slides on Google Slides.&lt;/item&gt;
      &lt;item&gt;Repository with links and notes on GitHub.&lt;/item&gt;
      &lt;item&gt;Coverage on Linux Weekly News.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Introduction#&lt;/head&gt;
    &lt;p&gt;Let’s start with a simple example – you decide to read from a channel in a loop and gather a bunch of messages:&lt;/p&gt;
    &lt;code&gt;loop {
    match rx.recv().await {
        Ok(msg) =&amp;gt; process(msg),
        Err(_) =&amp;gt; return,
    }
}
&lt;/code&gt;
    &lt;p&gt;All good, nothing wrong with this, but you realize sometimes the channel is empty for long periods of time, so you add a timeout and print a message:&lt;/p&gt;
    &lt;code&gt;loop {
    match timeout(Duration::from_secs(5), rx.recv()).await {
        Ok(Ok(msg)) =&amp;gt; process(msg),
        Ok(Err(_)) =&amp;gt; return,
        Err(_) =&amp;gt; println!("no messages for 5 seconds"),
    }
}
&lt;/code&gt;
    &lt;p&gt;There’s nothing wrong with this code—it behaves as expected.&lt;/p&gt;
    &lt;p&gt;Now you realize you need to write a bunch of messages out to a channel in a loop:&lt;/p&gt;
    &lt;code&gt;loop {
    let msg = next_message();
    match tx.send(msg).await {
        Ok(_) =&amp;gt; println!("sent successfully"),
        Err(_) =&amp;gt; return,
    }
}
&lt;/code&gt;
    &lt;p&gt;But sometimes the channel gets too full and blocks, so you add a timeout and print a message:&lt;/p&gt;
    &lt;code&gt;loop {
    let msg = next_message();
    match timeout(Duration::from_secs(5), tx.send(msg)).await {
        Ok(Ok(_)) =&amp;gt; println!("sent successfully"),
        Ok(Err(_)) =&amp;gt; return,
        Err(_) =&amp;gt; println!("no space for 5 seconds"),
    }
}
&lt;/code&gt;
    &lt;p&gt;It turns out that this code is often incorrect, because not all messages make their way to the channel.&lt;/p&gt;
    &lt;p&gt;Hi, I’m Rain, and this post is about cancelling async Rust. This post is split into three parts:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;What is cancellation? It’s an extremely powerful part of async Rust but also one that is very hard to reason thoroughly about.&lt;/item&gt;
      &lt;item&gt;Analyzing cancellations: Going deep into their mechanics and providing some helpful ways to think about them.&lt;/item&gt;
      &lt;item&gt;What can be done? Solutions, including practical guidance, and real bugs we’ve found and fixed in production codebases.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Before we begin, I want to lay my cards on the table – I really love async Rust!&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;I gave a talk at RustConf a couple years ago talking about how async Rust is a great fit for signal handling in complex applications.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I’m also the author of cargo-nextest, a next-generation test runner for Rust, where async Rust is the best way I know of to express some really complex algorithms that I wouldn’t know how to express otherwise. I wrote a blog post about this a few years ago.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Now, I work at Oxide Computer Company, where we make cloud-in-a-box computers. We make vertically integrated systems where you provide power and networking on one end, and the software you want to run on the other end, and we take care of everything in between.&lt;/p&gt;
    &lt;p&gt;Of course, we use Rust everywhere, and in particular we use async Rust extensively for our higher-level software, such as storage, networking and the customer-facing management API. But along the way we’ve encountered a number of issues around async cancellation, and a lot of this post is about what we learned along the way.&lt;/p&gt;
    &lt;head rend="h2"&gt;1. What is cancellation?#&lt;/head&gt;
    &lt;p&gt;What does cancellation mean? Logically, a cancellation is exactly what it sounds like: you start some work, and then change your mind and decide to stop doing that work.&lt;/p&gt;
    &lt;p&gt;As you might imagine this is a useful thing to do:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;You may have started a large download or a long network request&lt;/item&gt;
      &lt;item&gt;Maybe you’ve started reading a file, similar to the &lt;code&gt;head&lt;/code&gt;command.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But then you change your mind: you want to cancel it rather than continue it to completion.&lt;/p&gt;
    &lt;head rend="h3"&gt;Cancellations in synchronous Rust#&lt;/head&gt;
    &lt;p&gt;Before we talk about async Rust, it’s worth thinking about how you’d do cancellations in synchronous Rust.&lt;/p&gt;
    &lt;p&gt;One option is to have some kind of flag you periodically check, maybe stored in an atomic:&lt;/p&gt;
    &lt;code&gt;while !should_cancel.load(Ordering::Relaxed) {
    expensive_operation();
}
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The code that wishes to perform the cancellation can set that flag.&lt;/item&gt;
      &lt;item&gt;Then, the code which checks that flag can exit early.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This approach is fine for smaller bits of code but doesn’t really scale well to large chunks of code since you’d have to sprinkle these checks everywhere.&lt;/p&gt;
    &lt;p&gt;A related option, if you’re working with a framework as part of your work, is to panic with a special payload of some kind.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If that feels strange to you, you’re not alone! But the Salsa framework for incremental computation, used by—among other things—rust-analyzer, uses this approach.&lt;/item&gt;
      &lt;item&gt;Something I learned recently was that this only works on build targets which have a notion of panic unwinding, or being able to bubble up the panic. Not all platforms support this, and in particular, Wasm doesn’t. This means that Salsa cancellations don’t work if you build rust-analyzer for Wasm.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A third option is to kill the whole process. This is a very heavyweight approach, but an effective one in case you spawn processes to do your work.&lt;/p&gt;
    &lt;p&gt;Rather than kill the whole process, can you kill a single thread?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;While some OSes have APIs to perform this action, they tend to warn very strongly against it. That’s because in general, most code is just not ready for a thread disappearing from underneath.&lt;/item&gt;
      &lt;item&gt;In particular, thread killing is not permitted by safe Rust, since it can cause serious corruption. For example, Rust mutexes would likely stay locked forever.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;All of these options are suboptimal or of limited use in some way. In general, the way I think about it is that there isn’t a universal protocol for cancellation in synchronous Rust.&lt;/p&gt;
    &lt;p&gt;In contrast, there is such a protocol in async Rust, and in fact cancellations are extraordinarily easy to perform in async Rust.&lt;/p&gt;
    &lt;p&gt;Why is that so? To understand that, let’s look at what a future is.&lt;/p&gt;
    &lt;head rend="h3"&gt;What is a future?#&lt;/head&gt;
    &lt;p&gt;Here’s a simple example of a future:&lt;/p&gt;
    &lt;code&gt;// This creates a state machine.
let future = async {
    let data = request().await;
    process(data).await
};

// Nothing executes yet. `future` is just a struct in memory.
&lt;/code&gt;
    &lt;p&gt;In this future, you first perform a network request which returns some data, and then you process it.&lt;/p&gt;
    &lt;p&gt;The Rust compiler looks at this future and generates a state machine, which is just a struct or enum in memory:&lt;/p&gt;
    &lt;code&gt;// The compiler generates something like:
enum MyFuture {
    Start,
    WaitingForNetwork(NetworkFuture),
    WaitingForProcess(ProcessFuture, Data),
    Done(Result),
}

// It's just data, no running code!
&lt;/code&gt;
    &lt;p&gt;If you’ve written async Rust before the &lt;code&gt;async&lt;/code&gt; and &lt;code&gt;await&lt;/code&gt; keywords, you’ve probably written code like it by hand. It’s basically just an enum describing all the possible states the future can be in.&lt;/p&gt;
    &lt;p&gt;The compiler also generates an implementation of the &lt;code&gt;Future&lt;/code&gt; trait for this future:&lt;/p&gt;
    &lt;code&gt;impl Future for MyFuture {
    fn poll(/* ... */) -&amp;gt; Poll&amp;lt;Self::Output&amp;gt; {
        match self {
            Start =&amp;gt; { /* ... */ }
            WaitingForNetwork(fut) =&amp;gt; { /* ... */ }
            // etc
        }
    }
}
&lt;/code&gt;
    &lt;p&gt;and when you call &lt;code&gt;.await&lt;/code&gt; on the future, it gets translated down to this underlying &lt;code&gt;poll&lt;/code&gt; function. It is only when &lt;code&gt;await&lt;/code&gt; or this &lt;code&gt;poll&lt;/code&gt; function is called that something actually happens.&lt;/p&gt;
    &lt;p&gt;Note that this is diametrically opposed to how async works in other languages like Go, JavaScript, or C#. In those languages, when you create a future to await on, it starts doing its thing, immediately, in the background:&lt;/p&gt;
    &lt;code&gt;// JavaScript: starts running immediately
const promise = fetch('/api/data');
&lt;/code&gt;
    &lt;p&gt;That’s regardless of whether you await it or not.&lt;/p&gt;
    &lt;p&gt;In Rust, this &lt;code&gt;get&lt;/code&gt; call does nothing until you actually call &lt;code&gt;.await&lt;/code&gt; on it:&lt;/p&gt;
    &lt;code&gt;// Rust: just data, does nothing!
let future = reqwest::get("/api/data");
&lt;/code&gt;
    &lt;p&gt;I know I sound a bit like a broken record here, but if you can take away one thing from this post, it would be that futures are passive, and completely inert until awaited or polled.&lt;/p&gt;
    &lt;head rend="h3"&gt;The universal protocol#&lt;/head&gt;
    &lt;p&gt;So what does the universal protocol to cancel futures look like? It is simply to drop the future, or to not await it, or poll it any more. Since a future is just a state machine, you can throw it away at any time the poll function isn’t actively being called.&lt;/p&gt;
    &lt;code&gt;let future = some_async_work();
drop(future); // cancelled
&lt;/code&gt;
    &lt;p&gt;The upshot of all this is that any Rust future can be cancelled at any await point.&lt;/p&gt;
    &lt;p&gt;Given how hard cancellation tends to be in synchronous environments, the ability to easily cancel futures in async Rust is extraordinarily powerful—in many ways its greatest strength!&lt;/p&gt;
    &lt;p&gt;But there is a flip side, which is that cancelling futures is far, far too easy. This is for two reasons.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;First, it’s just way too easy to quietly drop a future. As we’re going to see, there are all kinds of code patterns that lead to silently dropping futures.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Now this wouldn’t be so bad, if not for the second reason: that cancellation of parent futures propagates down to child futures.&lt;/p&gt;
        &lt;p&gt;Because of Rust’s single ownership model, child futures are owned by parent ones. If a parent future is dropped or cancelled, the same happens to the child.&lt;/p&gt;
        &lt;p&gt;To figure out whether a child future’s cancellation can cause issues, you have to look at its parent, and grandparent, and so on. Reasoning about cancellation becomes a very complicated non-local operation.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;2. Analyzing cancellations#&lt;/head&gt;
    &lt;p&gt;I’m going to cover some examples in a bit, but before we do that I want to talk about a couple terms, some of which you might have seen references to already.&lt;/p&gt;
    &lt;head rend="h3"&gt;Cancel safety and cancel correctness#&lt;/head&gt;
    &lt;p&gt;The first term is cancel safety. You might have seen mentions of this in the Tokio documentation. Cancel safety, as generally defined, means the property of a future that can be cancelled (i.e. dropped) without any side effects.&lt;/p&gt;
    &lt;p&gt;For example, a Tokio sleep future is cancel safe: you can just stop waiting on the sleep and it’s completely fine.&lt;/p&gt;
    &lt;code&gt;let future = tokio::time::sleep();
drop(future); // this has no side effects
&lt;/code&gt;
    &lt;p&gt;An example of a future that is not cancel safe is Tokio’s MPSC send, which sends a message over a channel:&lt;/p&gt;
    &lt;code&gt;let message = /* ... */;
let future = sender.send(message);
drop(future); // message is lost!
&lt;/code&gt;
    &lt;p&gt;If this future is dropped, the message is lost forever.&lt;/p&gt;
    &lt;p&gt;The important thing is that cancel safety is a local property of an individual future.&lt;/p&gt;
    &lt;p&gt;But cancel safety is not all that one needs to care about. What actually matters is the context the cancellation happens in, or in other words whether the cancellation actually causes some kind of larger property in the system to be violated.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;For example, if you drop a future which sends a message, but for whatever reason you don’t care about the message any more, it’s not really a bug!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To capture this I tend to use a different term called cancel correctness, which I define as a global property of system correctness in the face of cancellations. (This isn’t a standard term, but it’s a framing I’ve found really helpful in understanding cancellations.)&lt;/p&gt;
    &lt;p&gt;When is cancel correctness violated? It requires three things:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;The system has a cancel-unsafe future somewhere within it. As we’ll see, many APIs that are cancel-unsafe can be reworked to be cancel-safe. If there aren’t any cancel-unsafe futures in the system, then the system is cancel correct.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;A cancel-unsafe future is actually cancelled. This may sound a bit trivial, but if cancel-unsafe futures are always run to completion, then the system can’t have cancel correctness bugs.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Cancelling the future violates some property of a system. This could be data loss as with&lt;/p&gt;&lt;code&gt;Sender::send&lt;/code&gt;, some kind of invariant violation, or some kind of cleanup that must be performed but isn’t.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So a lot of making Rust async robust is about trying to tackle one of these three things.&lt;/p&gt;
    &lt;p&gt;I want to zoom in for a second on invariant violations and talk about an example of a Tokio API that is very prone to cancel correctness issues: Tokio mutexes.&lt;/p&gt;
    &lt;head rend="h3"&gt;The pain of Tokio mutexes#&lt;/head&gt;
    &lt;p&gt;The way Tokio mutexes work is: you create a mutex, you lock it which gives you mutable access to the data underneath, and then you unlock it by releasing the mutex.&lt;/p&gt;
    &lt;code&gt;let guard = mutex.lock().await;
// Access guard.data, protected by the mutex...
drop(guard);
&lt;/code&gt;
    &lt;p&gt;If you look at the &lt;code&gt;lock&lt;/code&gt; function’s documentation, in the “cancel safety” section it says:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;This method uses a queue to fairly distribute locks in the order they were requested. Cancelling a call to lock makes you lose your place in the queue.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Okay, so not totally cancel safe, but the only kind of unsafety is fairness, which doesn’t sound too bad.&lt;/p&gt;
    &lt;p&gt;But the problems lie in what you actually do with the mutex. In practice, most uses of mutexes are in order to temporarily violate invariants that are otherwise upheld when a lock isn’t held.&lt;/p&gt;
    &lt;p&gt;I’ll use a real world example of a cancel correctness bug that we found at my job at Oxide: we had code to manage a bunch of data sent over by our computers, which we call sleds. The shared state was guarded by a mutex, and a typical operation was:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Obtain a lock on the mutex.&lt;/item&gt;
      &lt;item&gt;Obtain the sled-specific data by value, moving it to an invalid &lt;code&gt;None&lt;/code&gt;state.&lt;/item&gt;
      &lt;item&gt;Perform an action.&lt;/item&gt;
      &lt;item&gt;Set the sled-specific data back to the next valid state.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here’s a rough sketch of what that looks like:&lt;/p&gt;
    &lt;code&gt;let guard = mutex.lock().await;
// guard.data is Option&amp;lt;T&amp;gt;: Some to begin with
let data = guard.data.take(); // guard.data is now None

let new_data = process_data(data);
guard.data = Some(new_data); // guard.data is Some again
&lt;/code&gt;
    &lt;p&gt;This is all well and good, but the problem is that the action being performed actually had an await point within it:&lt;/p&gt;
    &lt;code&gt;let guard = mutex.lock().await;
// guard.data is Option&amp;lt;T&amp;gt;: Some to begin with
let data = guard.data.take(); // guard.data is now None

// DANGER: cancellation here leaves data in None state!
let new_data = process_data(data).await;
guard.data = Some(new_data); // guard.data is Some again
&lt;/code&gt;
    &lt;p&gt;If the code that operated on the mutex got cancelled at that await point, then the data would be stuck in the invalid &lt;code&gt;None&lt;/code&gt; state. Not great!&lt;/p&gt;
    &lt;p&gt;And keep in mind the non-local reasoning aspect: when doing this analysis, you need to look at the whole chain of callers.&lt;/p&gt;
    &lt;head rend="h3"&gt;Cancellation patterns#&lt;/head&gt;
    &lt;p&gt;Now that we’ve talked about some of the bad things that can happen during cancellations, it’s worth asking what kinds of code patterns lead to futures being cancelled.&lt;/p&gt;
    &lt;p&gt;The most straightforward example, and maybe a bit of a silly one, is that you create a future but simply forget to call &lt;code&gt;.await&lt;/code&gt; on it.&lt;/p&gt;
    &lt;code&gt;some_async_work(); // missing .await
&lt;/code&gt;
    &lt;p&gt;Now Rust actually warns you if you don’t call &lt;code&gt;.await&lt;/code&gt; on the future:&lt;/p&gt;
    &lt;code&gt;warning: unused implementer of `Future` that must be used
   |
11 |     some_async_work();
   |     ^^^^^^^^^^^^^^^^^
   |
   = note: futures do nothing unless you `.await` or poll them
&lt;/code&gt;
    &lt;p&gt;But a code pattern I’ve sometimes made mistakes with is that the future returns a &lt;code&gt;Result&lt;/code&gt;, and you want to ignore the result so you assign it to an underscore like so:&lt;/p&gt;
    &lt;code&gt;let _ = some_async_work(); // future returns Result
&lt;/code&gt;
    &lt;p&gt;If I forget to call &lt;code&gt;.await&lt;/code&gt; on the future, Rust doesn’t warn me about it at all, and then I’m left scratching my head about why this code didn’t run. I know this sounds really silly and basic, but I’ve made this mistake a bunch of times.&lt;/p&gt;
    &lt;p&gt;(After my talk, it was pointed out to me that Clippy 1.67 and above have a &lt;code&gt;let_underscore_future&lt;/code&gt; warn-by-default lint for this. Hooray!)&lt;/p&gt;
    &lt;p&gt;Another example of futures being cancelled is &lt;code&gt;try&lt;/code&gt; operations, such as Tokio’s &lt;code&gt;try_join&lt;/code&gt; macro. For example:&lt;/p&gt;
    &lt;code&gt;async fn do_stuff_async() -&amp;gt; Result&amp;lt;(), &amp;amp;'static str&amp;gt; {
    // async work
}

async fn more_async_work() -&amp;gt; Result&amp;lt;(), &amp;amp;'static str&amp;gt; {
    // more here
}

let res = tokio::try_join!(
    do_stuff_async(),
    more_async_work(),
);

// ...
&lt;/code&gt;
    &lt;p&gt;If you call &lt;code&gt;try_join&lt;/code&gt; with a bunch of futures, and all of them succeed, it’s all good. But if one of them fails, the rest simply get cancelled.&lt;/p&gt;
    &lt;p&gt;In fact, at Oxide we had a pretty bad bug around this: we had code to stop a bunch of services, all expressed as futures. We used &lt;code&gt;try_join&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;try_join!(
    stop_service_a(),
    stop_service_b(),
    stop_service_c(),
)?;
&lt;/code&gt;
    &lt;p&gt;If one of these operations failed for whatever reason, we would stop running the code to wait for the other services to exit. Oops!&lt;/p&gt;
    &lt;p&gt;But perhaps the most well-known source of cancellations is Tokio’s &lt;code&gt;select&lt;/code&gt; macro. Select is this incredibly beautiful operation. It is called with a set of futures, and it drives all of them forward concurrently:&lt;/p&gt;
    &lt;code&gt;tokio::select! {
    result1 = future1 =&amp;gt; handle_result1(result1),
    result2 = future2 =&amp;gt; handle_result2(result2),
}
&lt;/code&gt;
    &lt;p&gt;Each future has a code block associated with it (above, &lt;code&gt;handle_result1&lt;/code&gt; and &lt;code&gt;handle_result2&lt;/code&gt;). If one of the futures completes, the corresponding code block is called. But also, all of the other futures are always cancelled!&lt;/p&gt;
    &lt;p&gt;For a variety of reasons, select statements in general, and select loops in particular, are particularly prone to cancel correctness issues. So a lot of the documentation about cancel safety talks about select loops. But I want to emphasize here that select is not the only source of cancellations, just a particularly notable one.&lt;/p&gt;
    &lt;head rend="h2"&gt;3. What can be done?#&lt;/head&gt;
    &lt;p&gt;So, now that we’ve looked at all of these issues with cancellations, what can be done about it?&lt;/p&gt;
    &lt;p&gt;First, I want to break the bad news to you – there is no general, fully reliable solution for this in Rust today. But in our experience there are a few patterns that have been successful at reducing the likelihood of cancellation bugs.&lt;/p&gt;
    &lt;p&gt;Going back to our definition of cancel correctness, there are three prongs all of which come together to produce a bug:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A cancel-unsafe future exists&lt;/item&gt;
      &lt;item&gt;This cancel-unsafe future is cancelled&lt;/item&gt;
      &lt;item&gt;The cancellation violates a system property&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Most solutions we’ve come up with try and tackle one of these prongs.&lt;/p&gt;
    &lt;head rend="h3"&gt;Making futures cancel-safe#&lt;/head&gt;
    &lt;p&gt;Let’s look at the first prong: the system has a cancel-unsafe future somewhere in it. Can we use code patterns to make futures be cancel-safe? It turns out we can! I’ll give you two examples here.&lt;/p&gt;
    &lt;p&gt;The first is MPSC sends. Let’s come back to the example from earlier where we would lose messages entirely:&lt;/p&gt;
    &lt;code&gt;loop {
    let msg = next_message();
    match timeout(Duration::from_secs(5), tx.send(msg)).await {
        Ok(Ok(_)) =&amp;gt; println!("sent successfully"),
        Ok(Err(_)) =&amp;gt; return,
        Err(_) =&amp;gt; println!("no space for 5 seconds"),
    }
}
&lt;/code&gt;
    &lt;p&gt;Can we find a way to make this cancel safe?&lt;/p&gt;
    &lt;p&gt;In this case, yes, and we do so by breaking up the operation into two parts:&lt;/p&gt;
    &lt;code&gt;loop {
    let msg = next_message();
    loop {
        match timeout(Duration::from_secs(5), tx.reserve()).await {
            Ok(Ok(permit)) =&amp;gt; { permit.send(msg); break; }
            Ok(Err(_)) =&amp;gt; return,
            Err(_) =&amp;gt; println!("no space for 5 seconds"),
        }
    }
}
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The first component is the operation to reserve a permit or slot in the channel. This is an initial async operation that’s cancel-safe.&lt;/item&gt;
      &lt;item&gt;The second is to actually send the message, which is an operation that becomes infallible.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;(I want to put an asterisk here that reserve is not entirely cancel-safe, since Tokio’s MPSC follows a first-in-first-out pattern and dropping the future means losing your place in line. Keep this in mind for now.)&lt;/p&gt;
    &lt;p&gt;The second is with Tokio’s &lt;code&gt;AsyncWrite&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;If you’ve written synchronous Rust you’re probably familiar with the &lt;code&gt;write_all&lt;/code&gt; method, which writes an entire buffer out:&lt;/p&gt;
    &lt;code&gt;use std::io::Write;

let buffer: &amp;amp;[u8] = /* ... */;
writer.write_all(buffer)?;
&lt;/code&gt;
    &lt;p&gt;In synchronous Rust, this is a great API. But within async Rust, the &lt;code&gt;write_all&lt;/code&gt; pattern is absolutely not cancel safe! If the future is dropped before completion, you have no idea how much of this buffer was written out.&lt;/p&gt;
    &lt;code&gt;use tokio::io::AsyncWriteExt;

let buffer: &amp;amp;[u8] = /* ... */;
writer.write_all(buffer).await?; // Not cancel-safe!
&lt;/code&gt;
    &lt;p&gt;But there’s an alternative API that is cancel-safe, called &lt;code&gt;write_all_buf&lt;/code&gt;. This API is carefully designed to enable the reporting of partial progress, and it doesn’t just accept a buffer, but rather something that looks like a cursor on top of it:&lt;/p&gt;
    &lt;code&gt;use tokio::io::AsyncWriteExt;

let mut buffer: io::Cursor&amp;lt;&amp;amp;[u8]&amp;gt; = /* ... */;
writer.write_all_buf(&amp;amp;mut buffer).await?;
&lt;/code&gt;
    &lt;p&gt;When part of the buffer is written out, the cursor is advanced by that number of bytes. So if you call &lt;code&gt;write_all_buf&lt;/code&gt; in a loop, you’ll be resuming from this partial progress, which works great.&lt;/p&gt;
    &lt;head rend="h3"&gt;Not cancelling futures#&lt;/head&gt;
    &lt;p&gt;Going back to the three prongs: the second prong is about actually cancelling futures. What code patterns can be used to not cancel futures? Here are a couple of examples.&lt;/p&gt;
    &lt;p&gt;The first one is, in a place like a select loop, resume futures rather than cancelling them each time. You’d typically achieve this by pinning a future, and then polling a mutable reference to that future. For example:&lt;/p&gt;
    &lt;code&gt;let mut future = Box::pin(channel.reserve());
loop {
    tokio::select! {
        result = &amp;amp;mut future =&amp;gt; break result,
        _ = other_condition =&amp;gt; continue,
    }
}
&lt;/code&gt;
    &lt;p&gt;Coming back to our example of MPSC sends, the one asterisk with &lt;code&gt;reserve&lt;/code&gt; is that cancelling it makes you lose your place in line. Instead, if you pin the &lt;code&gt;reserve&lt;/code&gt; future and poll a mutable reference to it, you don’t lose your place in line.&lt;/p&gt;
    &lt;p&gt;(Does the difference here matter? It depends, but you can now have this strategy available to you.)&lt;/p&gt;
    &lt;p&gt;The second example is to use tasks. I mentioned earlier that futures are Rust are diametrically opposed to similar notions in languages like JavaScript. Well, there’s an alternative in async Rust that’s much closer to the JavaScript idea, and that’s tasks.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Unlike futures which are driven by the caller, tasks are driven by the runtime (such as Tokio).&lt;/item&gt;
      &lt;item&gt;With Tokio, dropping a handle to a task does not cause it to be cancelled, which means they’re a good place to run cancel-unsafe code.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A fun example is that at Oxide, we have an HTTP server called Dropshot. Previously, whenever an HTTP request came in, we’d use a future for it, and drop the future if the TCP connection was closed.&lt;/p&gt;
    &lt;code&gt;// Before: Future cancelled on TCP close
handle_request(req).await;
&lt;/code&gt;
    &lt;p&gt;This was really bad because future cancellations could happen due to the behavior of not just the parent future, but of a process that was running across a network! This is a rather extreme form of non-local reasoning.&lt;/p&gt;
    &lt;p&gt;We addressed this by spinning up a task for each HTTP request, and by running the code to completion even if the connection is closed:&lt;/p&gt;
    &lt;code&gt;// After: Task runs to completion
tokio::spawn(handle_request(req));
&lt;/code&gt;
    &lt;head rend="h3"&gt;Systematic solutions?#&lt;/head&gt;
    &lt;p&gt;The last thing I want to say is that this sucks!&lt;/p&gt;
    &lt;p&gt;The promise of Rust is that you don’t need to do this kind of non-local reasoning—that you can analyze small bits of code for local correctness, and scale that up to global correctness. Almost everything in Rust, from &lt;code&gt;&amp;amp;&lt;/code&gt; and &lt;code&gt;&amp;amp;mut&lt;/code&gt; to &lt;code&gt;unsafe&lt;/code&gt;, is geared towards making that possible. Future cancellations fly directly in the face of that, and I think they’re probably the least Rusty part of Rust. This is all really unfortunate.&lt;/p&gt;
    &lt;p&gt;Can we come up with something more systematic than this kind of ad-hoc reasoning?&lt;/p&gt;
    &lt;p&gt;There doesn’t exist anything in safe Rust today, but there are a few different ideas people have come up with. I wanted to give a nod to those ideas:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Async drop would let you run async code when a future is cancelled. This would handle some, though not all, of the cases we discussed today.&lt;/item&gt;
      &lt;item&gt;There’s also a couple different proposals for what are called linear types, where you could force some code to be run on drop, or mark a particular future as non-cancellable (once it’s been created it must be driven to completion).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;All of these options have really significant implementation challenges, though. This blog post from boats covers some of these solutions, and the implementation challenges with them.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion#&lt;/head&gt;
    &lt;p&gt;In this post, we:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Saw that futures are passive&lt;/item&gt;
      &lt;item&gt;Introduced cancel safety and cancel correctness as concepts&lt;/item&gt;
      &lt;item&gt;Examined some bugs that can occur with cancellation&lt;/item&gt;
      &lt;item&gt;Looked at some recommendations you can use to mitigate the downsides of cancellation&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Some of the recommendations are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Avoid Tokio mutexes&lt;/item&gt;
      &lt;item&gt;Rewrite APIs to make futures cancel-safe&lt;/item&gt;
      &lt;item&gt;Find ways to ensure that cancel-unsafe futures are driven to completion&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;There’s a very deep well of complexity here, a lot more than I can cover in one blog post:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Why are futures passive, anyway?&lt;/item&gt;
      &lt;item&gt;Cooperative cancellation: cancellation tokens&lt;/item&gt;
      &lt;item&gt;Actor model as an alternative to Tokio mutexes&lt;/item&gt;
      &lt;item&gt;Task aborts&lt;/item&gt;
      &lt;item&gt;Structured concurrency&lt;/item&gt;
      &lt;item&gt;Relationship to panic safety and mutex poisoning&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you’re curious about any of these, check out this link where I’ve put together a collection of documents and blog posts about these concepts. In particular, I’d recommend reading these two Oxide RFDs:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;RFD 397 Challenges with async/await in the control plane by David Pacheco&lt;/item&gt;
      &lt;item&gt;RFD 400 Dealing with cancel safety in async Rust by myself&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Thank you for reading this post to the end! And thanks to many of my coworkers at Oxide for reviewing the talk and the RFDs linked above, and for suggestions and constructive feedback.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45464632</guid><pubDate>Fri, 03 Oct 2025 16:18:29 +0000</pubDate></item><item><title>Germany must stand firmly against client-side scanning in Chat Control [pdf]</title><link>https://signal.org/blog/pdfs/germany-chat-control.pdf</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45464921</guid><pubDate>Fri, 03 Oct 2025 16:44:02 +0000</pubDate></item><item><title>Simple Hotkey Daemon for macOS, Ported to Zig</title><link>https://github.com/jackielii/skhd.zig</link><description>&lt;doc fingerprint="b607defb1717a1dc"&gt;
  &lt;main&gt;
    &lt;p&gt;Simple Hotkey Daemon for macOS, ported from skhd to Zig.&lt;/p&gt;
    &lt;p&gt;This implementation is fully compatible with the original skhd configuration format - your existing &lt;code&gt;.skhdrc&lt;/code&gt; files will work without modification. Additionally, it includes new features like process groups and command definitions (&lt;code&gt;.define&lt;/code&gt;) for cleaner configs, key forwarding/remapping, and improved error reporting.&lt;/p&gt;
    &lt;p&gt;The easiest way to install skhd.zig:&lt;/p&gt;
    &lt;code&gt;brew install jackielii/tap/skhd-zig&lt;/code&gt;
    &lt;p&gt;Download the latest release for your architecture:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;skhd-arm64-macos.tar.gz&lt;/code&gt;- For Apple Silicon Macs&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;skhd-x86_64-macos.tar.gz&lt;/code&gt;- For Intel Macs&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Extract and install:&lt;/p&gt;
    &lt;code&gt;tar -xzf skhd-*.tar.gz
sudo cp skhd /usr/local/bin/&lt;/code&gt;
    &lt;p&gt;If you need builds with different optimization levels (Debug, ReleaseSafe, ReleaseFast, ReleaseSmall), you can download them directly from GitHub Actions:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Go to the CI workflow in Actions tab. Filter by branch &lt;code&gt;main&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Click on the latest successful run&lt;/item&gt;
      &lt;item&gt;Scroll down to the "Artifacts" section&lt;/item&gt;
      &lt;item&gt;Download the build artifact for your desired optimization level: &lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;skhd-Debug&lt;/code&gt;- Debug build with full debugging symbols&lt;/item&gt;&lt;item&gt;&lt;code&gt;skhd-ReleaseSafe&lt;/code&gt;- Release build with safety checks and runtime safety&lt;/item&gt;&lt;item&gt;&lt;code&gt;skhd-ReleaseFast&lt;/code&gt;- Optimized for performance (recommended for daily use)&lt;/item&gt;&lt;item&gt;&lt;code&gt;skhd-ReleaseSmall&lt;/code&gt;- Optimized for binary size&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Clone the repository
git clone https://github.com/jackielii/skhd.zig
cd skhd.zig

# Build in release mode
zig build -Doptimize=ReleaseFast

# Install (copy to /usr/local/bin)
sudo cp zig-out/bin/skhd /usr/local/bin/&lt;/code&gt;
    &lt;p&gt;After installation, run skhd as a service for automatic startup:&lt;/p&gt;
    &lt;code&gt;# Install and start the service
skhd --install-service
skhd --start-service

# Check if skhd is running properly
skhd --status

# Restart service (useful for restarting after giving accessibility permissions)
skhd --restart-service

# Stop service
skhd --stop-service

# Uninstall service
skhd --uninstall-service&lt;/code&gt;
    &lt;p&gt;The service will:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Start automatically on login&lt;/item&gt;
      &lt;item&gt;Create logs at &lt;code&gt;/tmp/skhd_$USER.log&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Use your config from &lt;code&gt;~/.config/skhd/skhdrc&lt;/code&gt;or&lt;code&gt;~/.skhdrc&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Automatically reload on config changes&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Event capturing: Uses macOS Core Graphics Event Tap for system-wide keyboard event interception&lt;/item&gt;
      &lt;item&gt;Hotkey mapping: Maps key combinations to shell commands with full modifier support&lt;/item&gt;
      &lt;item&gt;Process-specific bindings: Different commands for different applications&lt;/item&gt;
      &lt;item&gt;Key forwarding/remapping: Remap keys to other key combinations&lt;/item&gt;
      &lt;item&gt;Modal system: Multi-level modal hotkey system with capture modes&lt;/item&gt;
      &lt;item&gt;Configuration file: Compatible with original skhd configuration format&lt;/item&gt;
      &lt;item&gt;Hot reloading: Automatic config reload on file changes&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Process groups: Define named groups of applications for cleaner configs&lt;/item&gt;
      &lt;item&gt;Command definitions: Define reusable commands with placeholders to reduce repetition&lt;/item&gt;
      &lt;item&gt;Key Forwarding: Forward / remap key binding to another key binding&lt;/item&gt;
      &lt;item&gt;Mode activation with command: Execute a command when switching modes (e.g., &lt;code&gt;cmd - w ; window : echo "Window mode"&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;--version&lt;/code&gt;/&lt;code&gt;-v&lt;/code&gt;- Display version information&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--help&lt;/code&gt;- Show usage information&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-c&lt;/code&gt;/&lt;code&gt;--config&lt;/code&gt;- Specify config file location&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-o&lt;/code&gt;/&lt;code&gt;--observe&lt;/code&gt;- Observe mode (echo keycodes and modifiers)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-V&lt;/code&gt;/&lt;code&gt;--verbose&lt;/code&gt;- Debug output with detailed logging&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-k&lt;/code&gt;/&lt;code&gt;--key&lt;/code&gt;- Synthesize keypress for testing&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-t&lt;/code&gt;/&lt;code&gt;--text&lt;/code&gt;- Synthesize text input&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-r&lt;/code&gt;/&lt;code&gt;--reload&lt;/code&gt;- Signal reload to running instance&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-h&lt;/code&gt;/&lt;code&gt;--no-hotload&lt;/code&gt;- Disable hotloading&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-P&lt;/code&gt;/&lt;code&gt;--profile&lt;/code&gt;- Profile event handling (Debug and ReleaseSafe builds only)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;--install-service&lt;/code&gt;- Install launchd service&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--uninstall-service&lt;/code&gt;- Remove launchd service&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--start-service&lt;/code&gt;- Start as service&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--restart-service&lt;/code&gt;- Restart service&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--stop-service&lt;/code&gt;- Stop service&lt;/item&gt;
      &lt;item&gt;PID file management (&lt;code&gt;/tmp/skhd_$USER.pid&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Service logging (&lt;code&gt;/tmp/skhd_$USER.log&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Blacklisting: Exclude applications from hotkey processing&lt;/item&gt;
      &lt;item&gt;Shell customization: Use custom shell for command execution&lt;/item&gt;
      &lt;item&gt;Left/right modifier distinction: Support for lcmd, rcmd, lalt, ralt, etc.&lt;/item&gt;
      &lt;item&gt;Special key support: Function keys, media keys, arrow keys&lt;/item&gt;
      &lt;item&gt;Passthrough mode: Execute command but still send keypress to application&lt;/item&gt;
      &lt;item&gt;Config includes: Load additional config files with &lt;code&gt;.load&lt;/code&gt;directive&lt;/item&gt;
      &lt;item&gt;Comprehensive error reporting: Detailed error messages with line numbers&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Build the project (creates executable in zig-out/bin/)
zig build

# Build in release mode with optimizations
zig build -Doptimize=ReleaseFast

# Run the application
zig build run

# Run with arguments
zig build run -- -V -c ~/.config/skhd/skhdrc

# Run tests
zig build test&lt;/code&gt;
    &lt;p&gt;skhd.zig looks for configuration files in the following order:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Path specified with &lt;code&gt;-c&lt;/code&gt;flag&lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;~/.config/skhd/skhdrc&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;~/.skhdrc&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The configuration syntax is fully compatible with the original skhd. See SYNTAX.md for the complete syntax reference and grammar.&lt;/p&gt;
    &lt;code&gt;# Use custom shell (skips interactive shell overhead)
.shell "/bin/dash"

# Blacklist applications (skip hotkey processing)
.blacklist [
    "dota2"
    "Microsoft Remote Desktop"
    "VMware Fusion"
]

# Load additional config files
.load "~/.config/skhd/extra.skhdrc"

# Define process groups for reuse (New in skhd.zig!)
.define terminal_apps ["kitty", "wezterm", "terminal"]
.define native_apps ["kitty", "wezterm", "chrome", "whatsapp"]
.define browser_apps ["chrome", "safari", "firefox", "edge"]

# Define reusable commands with placeholders (New in skhd.zig!)
.define yabai_focus : yabai -m window --focus {{1}} || yabai -m display --focus {{1}}
.define yabai_swap : yabai -m window --swap {{1}} || (yabai -m window --display {{1}} &amp;amp;&amp;amp; yabai -m display --focus {{1}})
.define toggle_app : open -a "{{1}}" || osascript -e 'tell app "{{1}}" to quit'
.define resize_window : yabai -m window --resize {{1}}:{{2}}:{{3}}
.define toggle_scratchpad : yabai -m window --toggle {{1}} || open -a "{{2}}"&lt;/code&gt;
    &lt;code&gt;# Basic format: modifier - key : command
cmd - a : echo "Command+A pressed"

# Multiple modifiers
cmd + shift - t : open -a Terminal

# Different modifier combinations
ctrl - h : echo "Control+H"
alt - space : echo "Alt+Space"
shift - f1 : echo "Shift+F1"&lt;/code&gt;
    &lt;code&gt;# Basic modifiers
cmd     # Command key
ctrl    # Control key
alt     # Alt/Option key
shift   # Shift key
fn      # Function key

# Left/right specific modifiers
lcmd, rcmd    # Left/right Command
lctrl, rctrl  # Left/right Control
lalt, ralt    # Left/right Alt
lshift, rshift # Left/right Shift

# Special modifier combinations
hyper   # cmd + shift + alt + ctrl
meh     # shift + alt + ctrl&lt;/code&gt;
    &lt;code&gt;# Navigation keys
cmd - left : echo "Left arrow"
cmd - right : echo "Right arrow"
cmd - up : echo "Up arrow"
cmd - down : echo "Down arrow"

# Special keys
cmd - space : echo "Space"
cmd - return : echo "Return/Enter"
cmd - tab : echo "Tab"
cmd - escape : echo "Escape"
cmd - delete : echo "Delete/Backspace"
cmd - home : echo "Home"
cmd - end : echo "End"
cmd - pageup : echo "Page Up"
cmd - pagedown : echo "Page Down"

# Function keys
cmd - f1 : echo "F1"
cmd - f12 : echo "F12"

# Media keys
sound_up : echo "Volume Up"
sound_down : echo "Volume Down"
mute : echo "Mute"
brightness_up : echo "Brightness Up"
brightness_down : echo "Brightness Down"&lt;/code&gt;
    &lt;code&gt;# Different commands for different applications
cmd - n [
    "terminal" : echo "New terminal window"
    "safari"   : echo "New safari window"
    "finder"   : echo "New finder window"
    *          : echo "New window in other apps"
]&lt;/code&gt;
    &lt;code&gt;# Keyboard layout fixes
0xa | 0x32             # UK keyboard § to `
shift - 0xa | shift - 0x32  # shift - § to ~

# Function key navigation (for laptop keyboards)
fn - j | down
fn - k | up
fn - h | left
fn - l | right

# When you have cmd - number for yabai spaces,
# and you still want the cmd - number to work in applications
ctrl - 1 | cmd - 1
ctrl - 2 | cmd - 2
ctrl - 3 | cmd - 3&lt;/code&gt;
    &lt;code&gt;# Execute command but still send keypress to application
cmd - p -&amp;gt; : echo "This runs but Cmd+P still goes to app"&lt;/code&gt;
    &lt;code&gt;# Window management mode with anybar visual indicator
# Install anybar: brew install --cask anybar

# Define window management mode for warp/stack operations
# Use anybar to indicate the mode: https://github.com/tonsky/AnyBar
:: winmode @ : echo -n "red" | nc -4u -w0 localhost 1738
:: default : echo -n "hollow" | nc -4u -w0 localhost 1738

# Enter window mode with meh + m (shift + alt + ctrl + m)
meh - w ; winmode
winmode &amp;lt; escape ; default
winmode &amp;lt; meh - w ; default

# Alternative: Enter window mode AND show notification (New in skhd.zig!)
# This executes the command when switching to the mode
# It allows for different commands to execute and switch to another mode
meh - w ; winmode : osascript -e 'display notification "Window mode active" with title "skhd"'
winmode &amp;lt; escape ; default : osascript -e 'display notification "Normal mode" with title "skhd"'

# Focus operations - basic hjkl for focus
winmode &amp;lt; h : yabai -m window --focus west || yabai -m display --focus west
winmode &amp;lt; j : yabai -m window --focus south || yabai -m display --focus south
winmode &amp;lt; k : yabai -m window --focus north || yabai -m display --focus north
winmode &amp;lt; l : yabai -m window --focus east || yabai -m display --focus east

# Move operations - shift + hjkl for moving
winmode &amp;lt; shift - h : yabai -m window --move rel:-80:0
winmode &amp;lt; shift - j : yabai -m window --move rel:0:80
winmode &amp;lt; shift - k : yabai -m window --move rel:0:-80
winmode &amp;lt; shift - l : yabai -m window --move rel:80:0

# Warp operations - alt + shift + hjkl for warping
winmode &amp;lt; alt + shift - h : yabai -m window --warp west
winmode &amp;lt; alt + shift - j : yabai -m window --warp south
winmode &amp;lt; alt + shift - k : yabai -m window --warp north
winmode &amp;lt; alt + shift - l : yabai -m window --warp east

# Stack operations - ctrl + shift + hjkl for stacking
winmode &amp;lt; ctrl + shift - h : yabai -m window --stack west
winmode &amp;lt; ctrl + shift - j : yabai -m window --stack south
winmode &amp;lt; ctrl + shift - k : yabai -m window --stack north
winmode &amp;lt; ctrl + shift - l : yabai -m window --stack east

# Stack management shortcuts
winmode &amp;lt; s : yabai -m window --insert stack  # Toggle stack mode
winmode &amp;lt; u : yabai -m window --toggle float; yabai -m window --toggle float  # Unstack window
winmode &amp;lt; n : yabai -m window --focus stack.next  # Navigate stack next
winmode &amp;lt; p : yabai -m window --focus stack.prev  # Navigate stack prev

# Resize submode
winmode &amp;lt; r ; resize
:: resize @ : echo -n "orange" | nc -4u -w0 localhost 1738
resize &amp;lt; h : yabai -m window --resize left:-20:0
resize &amp;lt; j : yabai -m window --resize bottom:0:20
resize &amp;lt; k : yabai -m window --resize top:0:-20
resize &amp;lt; l : yabai -m window --resize right:20:0
resize &amp;lt; escape ; winmode&lt;/code&gt;
    &lt;code&gt;# Focus windows using command definitions (New in skhd.zig!)
cmd - h : @yabai_focus("west")
cmd - j : @yabai_focus("south")
cmd - k : @yabai_focus("north")
cmd - l : @yabai_focus("east")

# Move/swap windows using command definitions
cmd + shift - h : @yabai_swap("west")
cmd + shift - j : @yabai_swap("south")
cmd + shift - k : @yabai_swap("north")
cmd + shift - l : @yabai_swap("east")

# Resize windows using command definitions
cmd + ctrl - h : @resize_window("left", "-20", "0")
cmd + ctrl - l : @resize_window("right", "20", "0")

# Switch spaces
cmd - 1 : yabai -m space --focus 1
cmd - 2 : yabai -m space --focus 2&lt;/code&gt;
    &lt;code&gt;# Quick app launching (traditional way)
alt - return : open -a Terminal
alt - b : open -a Safari

# Toggle apps using command definitions (New in skhd.zig!)
alt - f : @toggle_app("Finder")
alt - c : @toggle_app("Visual Studio Code")

# Scratchpad apps with yabai (New in skhd.zig!)
# In yabairc: yabai -m rule --add app="^YouTube Music$" scratchpad=music grid=11:11:1:1:9:9
alt - m : @toggle_scratchpad("music", "YouTube Music")
alt - n : @toggle_scratchpad("notes", "Notes")&lt;/code&gt;
    &lt;code&gt;# Linux-style word navigation and deletion
ctrl - backspace [
    @native_apps ~         # Terminal apps handle natively
    *            | alt - backspace  # Other apps: delete word
]

ctrl - left [
    @native_apps ~         # Terminal apps handle natively
    *            | alt - left       # Other apps: move word left
]

ctrl - right [
    @native_apps ~         # Terminal apps handle natively
    *            | alt - right      # Other apps: move word right
]

# Home/End key behavior (with shift for selection)
home [
    @native_apps ~         # Terminal apps handle natively
    *            | cmd - left       # Other apps: line start
]

shift - home [
    @native_apps ~         # Terminal apps handle natively
    *            | cmd + shift - left  # Other apps: select to line start
]

# Ctrl+Home/End for document navigation
ctrl - home [
    @native_apps ~         # Terminal apps handle natively
    *            | cmd - up         # Other apps: document start
]

ctrl - end [
    @native_apps ~         # Terminal apps handle natively
    *            | cmd - down       # Other apps: document end
]&lt;/code&gt;
    &lt;p&gt;Important: The logging and profiling behavior differs between build modes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;ReleaseFast builds (installed via Homebrew or built with &lt;code&gt;-Doptimize=ReleaseFast&lt;/code&gt;):&lt;list rend="ul"&gt;&lt;item&gt;Only show errors and warnings, even with &lt;code&gt;-V&lt;/code&gt;/&lt;code&gt;--verbose&lt;/code&gt;flag&lt;/item&gt;&lt;item&gt;Profiling (&lt;code&gt;-P&lt;/code&gt;/&lt;code&gt;--profile&lt;/code&gt;) is disabled - all tracing code is compiled out for maximum performance&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Only show errors and warnings, even with &lt;/item&gt;
      &lt;item&gt;ReleaseSafe builds (built with &lt;code&gt;-Doptimize=ReleaseSafe&lt;/code&gt;):&lt;list rend="ul"&gt;&lt;item&gt;Show errors, warnings, and info messages with &lt;code&gt;-V&lt;/code&gt;/&lt;code&gt;--verbose&lt;/code&gt;flag&lt;/item&gt;&lt;item&gt;Profiling (&lt;code&gt;-P&lt;/code&gt;/&lt;code&gt;--profile&lt;/code&gt;) is available for production debugging&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Show errors, warnings, and info messages with &lt;/item&gt;
      &lt;item&gt;Debug builds (default &lt;code&gt;zig build&lt;/code&gt;):&lt;list rend="ul"&gt;&lt;item&gt;Show all log levels including debug messages with &lt;code&gt;-V&lt;/code&gt;/&lt;code&gt;--verbose&lt;/code&gt;flag&lt;/item&gt;&lt;item&gt;Profiling (&lt;code&gt;-P&lt;/code&gt;/&lt;code&gt;--profile&lt;/code&gt;) is available with full trace details&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Show all log levels including debug messages with &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;However, command output will be shown if verbose flag is specified in release builds.&lt;/p&gt;
    &lt;p&gt;This is a trade-off between convenience and performance:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Performance mode (default): Command output is discarded for faster execution&lt;/item&gt;
      &lt;item&gt;Verbose mode (&lt;code&gt;-V&lt;/code&gt;): Command output is preserved, which may add slight overhead but helps with trouble shooting&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To debug hotkey events and see detailed logging:&lt;/p&gt;
    &lt;code&gt;# Verbose logging for troubleshooting config issues
# Note: In release builds, verbose mode only shows errors and warnings.
# To see debug/info logs, use a debug build:
zig build run -- -V&lt;/code&gt;
    &lt;p&gt;Performance: The event loop is allocation-free in release builds, ensuring consistent low-latency hotkey processing.&lt;/p&gt;
    &lt;code&gt;# Test key combinations and hex code (observe mode)
skhd -o

# Profile event handling (show after CTRL+C)
# Note: Profiling works in Debug and ReleaseSafe builds only
zig build &amp;amp;&amp;amp; ./zig-out/bin/skhd -P
# or for production debugging:
zig build -Doptimize=ReleaseSafe &amp;amp;&amp;amp; ./zig-out/bin/skhd -P

# Test specific keypress
skhd -k "cmd + shift - t"

# Test text synthesis
skhd -t "hello world"

# Reload config of running instance
skhd -r

# Debug memory allocations with real-time tracking
zig build alloc -- -V&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Fork the repository&lt;/item&gt;
      &lt;item&gt;Create a feature branch&lt;/item&gt;
      &lt;item&gt;Make your changes&lt;/item&gt;
      &lt;item&gt;Run tests: &lt;code&gt;zig build test&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Submit a pull request&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This project maintains compatibility with the original skhd license.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45465276</guid><pubDate>Fri, 03 Oct 2025 17:18:21 +0000</pubDate></item><item><title>Email was the user interface for the first AI recommendation engines</title><link>https://buttondown.com/blog/ringo-email-as-an-ai-interface</link><description>&lt;doc fingerprint="a69329413b20cfa9"&gt;
  &lt;main&gt;
    &lt;p&gt;Spinning the radio dials like mini roulette wheels was, in 1993, the best way to discover new music. Static, a snatch of a familiar song, a news report, then ah wait that sounds interesting. You’d walk into a record store with that song stuck in your head, and with any luck would walk out after a conversation with the clerk a few tapes richer, a few dozen dollars poorer.&lt;/p&gt;
    &lt;p&gt;One year later, everything had changed. For in 1994, the best way to discover new music was to email an AI.&lt;/p&gt;
    &lt;p&gt;“This sounds ****in’ moronic,” said one Dave Dell in response to the idea, convinced his love of Lustmord meant the AI couldn’t possibly match his tastes. Even he acquiesced: “I'll try it anyways.”&lt;/p&gt;
    &lt;p&gt;By the time The Cranberries released their hit single “Zombie” that September, over two thousand Daves had tried their luck, emailing Ringo with their favorite artists. To their surprise, they’d get a reply from what appeared to be early artificial intelligence, filled with recommendations of new music they’d love. Enough that, seven years later, science fiction writer Cory Doctorow would reminisce that “half the music in my collection came out of Ringo,” that nascent music AI.&lt;/p&gt;
    &lt;p&gt;Yet incredibly, Ringo was little more than those couple thousand users’ recommendations, averaged and redistributed via email. An email that users would quickly come to think of as a human, a friend, one in a quick succession of email-powered crowdsourced recommendation AIs.&lt;/p&gt;
    &lt;p&gt;It all started with what MIT assistant professor Paul Resnick called “A deceptively simple idea,” in 1994.&lt;/p&gt;
    &lt;p&gt;“People who agreed in the past are likely to agree again,” he postulated, an idea that’d been christened Social Filtering by MIT Thomas Malone seven years earlier. If you and another person both like the same song, or book, or author, there’s a pretty good chance that if one of you likes a new artist, the other will like it as well. The more overlapping agreements you have, the better one’s tastes should be predictive of another’s.&lt;/p&gt;
    &lt;p&gt;And, maybe, social filtering could be an organizing principle of the internet.&lt;/p&gt;
    &lt;p&gt;For as the nascent world wide web grew exponentially from a single website in 1991 and ten in 1992 to 623 sites in 1993 and over 10,000 by the end of 1994, the ancient prophecy that “knowledge shall increase” suddenly seemed more an omen of content overload than a portent of good things. Cataloguing and categorization could only go so far. It would be easy enough to find another Cranberries album once you knew you liked them. Finding the next new band that you’d love required something beyond lists. What good was infinite knowledge and limitless content without a way to discover it?&lt;/p&gt;
    &lt;p&gt;“The exploding volume of digital information makes it difficult for the user, equipped with only search capability, to keep up with the fast pace of information generation,” wrote Stanford’s Tak W. Yan and Hector Garcia-Molina, in their stab at solving the same problem. “There is a need for technology to help us wade through all the information to find the items we really want and need, and to rid us of the things we do not want to be bothered with,” as MIT Media Lab’s Upendra Shardanand summarized the issue.&lt;/p&gt;
    &lt;p&gt;Maybe the best option would be to ask someone else. We like getting recommendations from others, after all. “Choice under uncertainty is an opportunity to benefit from other more knowledgeable people,” wrote a Bellcore research team of their stab at the same problem in 1993.&lt;/p&gt;
    &lt;p&gt;Social filtering, teams from Xerox and Bell, Stanford and MIT alike agreed, seemed the perfect discovery mechanism of the future. That is, if they could gather everyone’s preferences and turn them into predictions accurately.&lt;/p&gt;
    &lt;p&gt;The idea behind Tapestry’s social filtering&lt;/p&gt;
    &lt;p&gt;Decades before the explosion of email newsletters, newsgroups were filling up early inboxes at a time when hard drives cost as much as $4,000 per gigabyte. Storage wasn’t the only scarce resource; no one had time to read every rant and reply in their inboxes.&lt;/p&gt;
    &lt;p&gt;Automatic filters were too restrictive—and they weren’t intelligent. Intelligence was when a friend read a message that hit the spot, and forwarded it on to you. It was when a colleague deleted a message, a silent vote that others might also deem that message irrelevant, or when they saved or replied to was a message in a vote towards its relevance.&lt;/p&gt;
    &lt;p&gt;Therein lay an idea: “More effective filtering can be done by involving humans in the filtering process,” postulated David Goldberg, David Nichols, Brian Oki, and Douglas Terry of the Xerox PARC team. That, in 1992, was the insight behind Tapestry, a short-lived collaborative email app inside Xerox PARC.&lt;/p&gt;
    &lt;p&gt;Tapestry sorted newsgroup emails with ratings. When you read an email or other document that you liked, you’d add an endorsement that Tapestry would store in a database alongside others’ endorsements. The next time someone searched for a message or document, they’d first see the messages that had the most endorsements, or could filter by specific users’ endorsements to follow the likes of a particular tastemaker.&lt;/p&gt;
    &lt;p&gt;“Eager readers will read all the documents ... in order to get immediate access,” the team surmised, while “more casual readers will wait for the eager readers to annotate, and read documents based on their reviews.”&lt;/p&gt;
    &lt;p&gt;SIFT’s social filtering model&lt;/p&gt;
    &lt;p&gt;Tapestry, it seems, never left Xerox’ bounds. But two years later, in February 1994, a Stanford team took up the mantle with SIFT, or Stanford Information Filtering Tool. It, too, sorted through messages based on crowdsourced wisdom. But it didn’t require a new app. SIFT, instead, was built around email.&lt;/p&gt;
    &lt;p&gt;“Email communications is the lowest common denominator of network connectivity,” wrote the team. “By having an email interface, a SIFT server is accessible from users with less powerful machines, with limited network capability, or behind Internet-access firewalls,” features that, to this day, make email one of the most universally accessible bits of the internet, even behind corporate firewalls and government censorships.&lt;/p&gt;
    &lt;p&gt;A SIFT email preview in an early browser&lt;/p&gt;
    &lt;p&gt;SIFT put email front and center. You’d sign up with an early web form and choose a topic of interest, like “underwater archeology.” SIFT would then regularly email you a list of articles and their first few lines to see what piqued your interest—an early curated email newsletter of top headlines. You’d then reply again with the articles you wanted to read, and SIFT would both email you the full messages, and store your choices as votes to help it refine what it’d recommend next time.&lt;/p&gt;
    &lt;p&gt;SIFT had some hits and some misses, and users were surprisingly accepting when things went wrong. “Well, nothing is perfect,” surmised Jiří Peterka in an early review. But SIFT clearly hit a nerve. “Within ten days of the announcement, we received well over a thousand profiles,” reported the team. By November, ten months after launch, SIFT was matching 45,000 articles each week to over 13,000 subscribers’ profiles.&lt;/p&gt;
    &lt;p&gt;Meanwhile, on opposite coasts, a Bell team was pondering decision paralysis. “Future users of the national information infrastructure will be overwhelmed with choices,” wrote Bellcore researchers Will Hill, Larry Stead, Mark Rosenstein and George Furnas in their 1995 writeup of the project. The best way out was to ask an expert, they decided. “When making a choice in the absence of decisive first-hand knowledge, choosing as other like-minded, similarly-situated people have successfully chosen in the past is a good strategy.”&lt;/p&gt;
    &lt;p&gt;So a team from the same Bell roots as UNIX and C++ and the transistor itself decided to harness social filtering to help you figure out which movie to rent from Blockbuster.&lt;/p&gt;
    &lt;p&gt;Movies lent themselves well to the model. There were a limited number of movies to sort and recommend, and existing expert ratings from the likes of Roger Ebert to pre-seed the database. If people would share their favorite movies, the system could match them with others who liked the same movies, and recommend other movies those people liked.&lt;/p&gt;
    &lt;p&gt;And it all ran over email.&lt;/p&gt;
    &lt;p&gt;An example videos@bellcore.com email&lt;/p&gt;
    &lt;p&gt;“The Internet email interface is currently a subject-line command interface,” the team wrote. For a few short months, from October 1993 to May 1994, you could email the subject “ratings” to videos@bellcore.com, and receive a reply with an overwhelming 500 movies.&lt;/p&gt;
    &lt;p&gt;Reply with your reviews of the movies you’d watched on a 1 to 10 scale, and Bellcore’s server would parse your reply and add your ratings to a database. Then, it’d look for “correlations between the new user's ratings and ratings from a random subsample of known users,” then “evaluate every unseen movie, sort them by highest prediction and skim off the top to recommend.”&lt;/p&gt;
    &lt;p&gt;Minutes later, you’d get back a reply, recommending you watch Alien and Blade Runner, say, along with a list of people who shared your tastes. It was a recommendation engine and nascent social network in one, where you just might find your next favorite movie and make a friend.&lt;/p&gt;
    &lt;p&gt;“Virtual communities may also sprout up around other domains such as music, books and catalog products,” predicted the team.&lt;/p&gt;
    &lt;p&gt;It didn’t take long. Two months after videos@bellcore.com shut down, a new MIT project launched: Ringo. “Our system, in our opinion, tackled the much more difficult problem domain of music,” wrote co-founder Upendra Shardanand in his master’s thesis.&lt;/p&gt;
    &lt;p&gt;The ingredients were in place. Social filtering had been proven out by Tapestry and SIFT. “The user interface for videos@bellcore.com system was used as a reference when designing the e-mail interface for Ringo,” said Shardanand. Along with that, Ringo added email-based accounts to learn from your preferences over time, and a constrained Pearson Correlation algorithm to rate “commonality between two users when computing the weights ... proportional to the number of artists both users have rated in common.”&lt;/p&gt;
    &lt;p&gt;That, and a personality. Ringo’s original emails were written as if they came from a person, such as “I recommend that you check out these artists...” It got toned down over time, partly to ensure people sent clear, precise instructions that Ringo could understand instead of fluent, natural language—but it quickly became clear that people thought of Ringo as a friend.&lt;/p&gt;
    &lt;p&gt;You’d email Ringo to sign up, and it’d reply with a more manageable list of 125 artists that you’d rate from 1 (“Pass the earplugs,” in Ringo’s description of its lowest rating) to 7 (“BOOM! One of my FAVORITE few!”). Ringo would then match your favorites to others with similar tastes, and reply with the eight artists it thought you’d like most.&lt;/p&gt;
    &lt;p&gt;The first email promoting Ringo, on a USENET group&lt;/p&gt;
    &lt;p&gt;On July 1, 1994—four years and 11 months before Napster’s launch—Ringo opened its inbox to the world. Shardanand marketed it on USENET groups. “The more users that use Ringo, the better Ringo's predictions,” he wrote, “so tell a friend.” Soon enough, Ringo was spreading by word-of-mouth in early email newsletters and other Usenet groups.&lt;/p&gt;
    &lt;p&gt;It worked—but it wasn’t always right. “In the first couple weeks of Ringo’s life, Ringo was relatively incompetent,” Shardanand recalled, and yet that, somehow, didn’t dampen people’s enthusiasm for the email bot they quickly came to love.&lt;/p&gt;
    &lt;p&gt;Example Ringo artist suggestions&lt;/p&gt;
    &lt;p&gt;“People would see the suggestions, and say things like ‘That one’s about right, that’s right, well that one I’d probably rate lower, but wow, it’s working,’” he recalled. “I watched them score artists, then I saw the poor suggestions, and was thinking, ‘You’ve got to be kidding.’ It’s as if they expected it [to] work, and therefore it did.”&lt;/p&gt;
    &lt;p&gt;Even when it came back with no recommendations, people were impressed. “It said there weren't enough participants out there like me,” Pat Anders recalled months after Ringo’s launch. “I was flattered.”&lt;/p&gt;
    &lt;p&gt;Others, though, found magic in the recommendations. “RINGO always came up with a great selection,” said Joe Morris. “I got turned on to a bunch of stuff I would of never found otherwise.” As did Cory Doctorow, with a music library still years later influenced by Ringo’s recommendations.&lt;/p&gt;
    &lt;p&gt;For it wasn’t simple recommendations over email. It was trust in a system that seemed greater than the sum of its parts. “A fellow Media Labber commented that possibly it is because people see Ringo’s suggestions as a ‘reflection of themselves,’” said Shardanand. “Once you have decided that there must be a logical connection between what you have told the system about your tastes and the system’s recommendations, then you are much more inclined to believe the predictions to be right.”&lt;/p&gt;
    &lt;p&gt;“I am largely of the opinion that great AI consists of aggregated human decisions, not machine generated decisions,” said Cory Doctorow, reminiscing about Ringo’s music recommendations.&lt;/p&gt;
    &lt;p&gt;For in every take on email-powered social filtering, the secret ingredient wasn’t the interface, nor was it the algorithm. It was in the crowdsourced data, and the system’s ability to match you with others of similar taste.&lt;/p&gt;
    &lt;p&gt;Some folks defied classification, felt that AI recommendations would devalue their unique tastes. Others embraced the crowdsourced models, found comfort in peeking in an AI-filtered mirror.&lt;/p&gt;
    &lt;p&gt;An early web form to sign up for Ringo&lt;/p&gt;
    &lt;p&gt;The services themselves faded with time. SIFT grew into a commercial project that sold ads for early newsletters, while the Stanford Library Project itself later provided the primary funding for Larry Page and Sergey Brin as they developed the ideas behind PageRank that turned into Google search (itself another way to harness the wisdom of the crowds).&lt;/p&gt;
    &lt;p&gt;Ringo later launched on the web first as HOMR (Helpful Online Music Service) then as Firefly (a community website for collaborative filtering that, fun fact, pitched RSS-competitor ICE to Microsoft), and won second place in MIT’s 1995 student business competition. The magic faded as it left email behind, though, and when Microsoft acquired it for $40 million in 1998, the only thing the software giant kept was their email-based login that, over time, morphed into Microsoft Passport.&lt;/p&gt;
    &lt;p&gt;31 years after those early crowdsourced email tools provided people’s first interactions with AI, today’s GPTs are eerily reminiscent of both those early app’s crowdsourced wisdom, and of our credulity to seemingly intelligent agents that mirror our preferences. The social filtering lives on, in ever less personal ways, in Google’s PageRank, Facebook’s feed algorithm, Netflix’s suggestions, and Spotify’s Daily Mix playlists.&lt;/p&gt;
    &lt;p&gt;And, in a roundabout way, email newsletters’ staying power may be due to the same selection bias and sorting powers that inspired social filtering-powered software. If you read something you like, there’s a fair chance you’ll continue to like the other things that person writes. Newsletters are the easiest way to select into that author’s filter bubble and find new things you like from their recommendations.&lt;/p&gt;
    &lt;p&gt;“As the information barrage continues to accelerate, agents will be as indispensable as E-mail,” predicted Ringo’s Shardanand in a foreshadowing of today’s AI and MCP servers and agents. People, it turned out though, were what was indispensable to email and recommendations you’d love.&lt;/p&gt;
    &lt;p&gt;Image Credits: Header image by Samuel Regan-Asante via Unsplash.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45465392</guid><pubDate>Fri, 03 Oct 2025 17:27:02 +0000</pubDate></item><item><title>ICE Wants to Build Out a 24/7 Social Media Surveillance Team</title><link>https://www.wired.com/story/ice-social-media-surveillance-24-7-contract/</link><description>&lt;doc fingerprint="f5bc5ade2ba8abd9"&gt;
  &lt;main&gt;
    &lt;p&gt;United States immigration authorities are moving to dramatically expand their social media surveillance, with plans to hire nearly 30 contractors to sift through posts, photos, and messages—raw material to be transformed into intelligence for deportation raids and arrests.&lt;/p&gt;
    &lt;p&gt;Federal contracting records reviewed by WIRED show that the agency is seeking private vendors to run a multiyear surveillance program out of two of its little-known targeting centers. The program envisions stationing nearly 30 private analysts at Immigration and Customs Enforcement facilities in Vermont and Southern California. Their job: Scour Facebook, TikTok, Instagram, YouTube, and other platforms, converting posts and profiles into fresh leads for enforcement raids.&lt;/p&gt;
    &lt;p&gt;The initiative is still at the request-for-information stage, a step agencies use to gauge interest from contractors before an official bidding process. But draft planning documents show the scheme is ambitious: ICE wants a contractor capable of staffing the centers around the clock, constantly processing cases on tight deadlines, and supplying the agency with the latest and greatest subscription-based surveillance software.&lt;/p&gt;
    &lt;p&gt;The facilities at the heart of this plan are two of ICE’s three targeting centers, responsible for producing leads that feed directly into the agency’s enforcement operations. The National Criminal Analysis and Targeting Center sits in Williston, Vermont. It handles cases across much of the eastern US. The Pacific Enforcement Response Center, based in Santa Ana, California, oversees the western region and is designed to run 24 hours a day, seven days a week.&lt;/p&gt;
    &lt;p&gt;Internal planning documents show that each site would be staffed with a mix of senior analysts, shift leads, and rank-and-file researchers. Vermont would see a team of a dozen contractors, including a program manager and 10 analysts. California would host a larger, nonstop watch floor with 16 staff. At all times, at least one senior analyst and three researchers would be on duty at the Santa Ana site.&lt;/p&gt;
    &lt;p&gt;Together, these teams would operate as intelligence arms of ICE’s Enforcement and Removal Operations division. They will receive tips and incoming cases, research individuals online, and package the results into dossiers that could be used by field offices to plan arrests.&lt;/p&gt;
    &lt;p&gt;The scope of information contractors are expected to collect is broad. Draft instructions specify open-source intelligence: public posts, photos, and messages on platforms from Facebook to Reddit to TikTok. Analysts may also be tasked with checking more obscure or foreign-based sites, such as Russia’s VKontakte.&lt;/p&gt;
    &lt;p&gt;They would also be armed with powerful commercial databases such as LexisNexis Accurint and Thomson Reuters CLEAR, which knit together property records, phone bills, utilities, vehicle registrations, and other personal details into searchable files.&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell role="head"&gt;Got a Tip?&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Are you a current or former government employee or contractor who wants to talk about immigration enforcement? We'd like to hear from you. Using a nonwork phone or computer, contact the reporter securely on Signal at dell.3030.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The plan calls for strict turnaround times. Urgent cases, such as suspected national security threats or people on ICE’s Top Ten Most Wanted list, must be researched within 30 minutes. High-priority cases get one hour; lower-priority leads must be completed within the workday. ICE expects at least three-quarters of all cases to meet those deadlines, with top contractors hitting closer to 95 percent.&lt;/p&gt;
    &lt;p&gt;The plan goes beyond staffing. ICE also wants algorithms, asking contractors to spell out how they might weave artificial intelligence into the hunt—a solicitation that mirrors other recent proposals. The agency has also set aside more than a million dollars a year to arm analysts with the latest surveillance tools.&lt;/p&gt;
    &lt;p&gt;ICE did not immediately respond to a request for comment.&lt;/p&gt;
    &lt;p&gt;Earlier this year, The Intercept revealed that ICE had floated plans for a system that could automatically scan social media for “negative sentiment” toward the agency and flag users thought to show a “proclivity for violence.” Procurement records previously reviewed by 404 Media identified software used by the agency to build dossiers on flagged individuals, compiling personal details, family links, and even using facial recognition to connect images across the web. Observers warned it was unclear how such technology could distinguish genuine threats from political speech.&lt;/p&gt;
    &lt;p&gt;ICE’s main investigative database, built by Palantir Technologies, already uses algorithmic analysis to filter huge populations and generate leads. The new contract would funnel fresh social media and open-source inputs directly into that system, further automating the process.&lt;/p&gt;
    &lt;p&gt;Planning documents say some restrictions are necessary to head off abuse. Contractors are barred from creating fake profiles, interacting with people online, or storing personal data on their own networks. All analysis must remain on ICE servers. Past experience, however, shows such guardrails can be flimsy, honored more in paperwork than in practice. Other documents obtained by 404 Media this summer revealed that police in Medford, Oregon, performed license plate reader searches for ICE’s Homeland Security Investigations division, while HSI agents later ran searches in federal databases at the request of local police—an informal back-and=forth that effectively gave ICE access to tools it wasn’t authorized to use.&lt;/p&gt;
    &lt;p&gt;Other surveillance contracts have raised similar alarms. In September 2024, ICE signed a $2 million contract with Paragon, an Israeli spyware company whose flagship product, Graphite, can allegedly remotely hack messaging apps like WhatsApp and Signal. The Biden White House quickly froze the deal under an executive order restricting spyware use, but ICE reactivated it in August 2025 under the Trump administration. Last month, 404 Media filed a freedom of information lawsuit demanding ICE release the contract and related records, citing widespread concern that the tool could be used to target immigrants, journalists, and activists.&lt;/p&gt;
    &lt;p&gt;The Electronic Privacy Information Center has similarly sued ICE, calling its reliance on data brokers a “significant threat to privacy and liberty.” The American Civil Liberties Union has argued that buying bulk datasets—such as smartphone location trails gathered from ordinary apps—helps ICE sidestep warrant requirements and helps it pull in vast amounts of data with no clear link to its enforcement mandate.&lt;/p&gt;
    &lt;p&gt;The newly proposed social media program is only the latest in a string of surveillance contracts ICE has pursued over the past few years.&lt;/p&gt;
    &lt;p&gt;In 2020 and 2021, ICE bought access to ShadowDragon’s SocialNet, a tool that aggregates data from more than 200 social networks and services into searchable maps of a person’s connections. Around the same time, the agency contracted with Babel Street for Locate X, which supplies location histories from ordinary smartphone apps, letting investigators reconstruct people’s movements without a warrant. ICE also adopted LexisNexis Accurint, used by agents to look up addresses, vehicles, and associates, though the scale of spending on that service is unclear. In September, ICE signed a multimillion-dollar contract with Clearview AI, a facial recognition company that built its database by scraping billions of images from social media and the public web.&lt;/p&gt;
    &lt;p&gt;Throughout, ICE has leaned on Palantir’s Investigative Case Management system to combine disparate streams of data into a single investigative platform. Recent contract updates show the system lets agents search people using hundreds of categories, from immigration status and country of origin to scars, tattoos, and license-plate reader data. Each surveillance contract ICE signs adds another layer—location trails, social networks, financial records, biometric identifiers—feeding into Palantir’s hub. ICE’s new initiative is about scaling up the human side of the equation, stationing analysts around the clock to convert the firehose of data into raid-ready leads.&lt;/p&gt;
    &lt;p&gt;ICE argues it needs these tools to modernize enforcement. Its planning documents note that “previous approaches … which have not incorporated open web sources and social media information, have had limited success.” The agency suggests that tapping social media and open web data helps identify aliases, track movements, and detect patterns that traditional methods often miss.&lt;/p&gt;
    &lt;p&gt;With plenty of historical analogs to choose from, privacy advocates warn that any surveillance that starts as a method of capturing immigrants could soon be deployed for ulterior purposes. ICE’s proposal to track “negative sentiment” is a clear example of how the agency’s threat monitoring bleeds into the policing of dissent. By drawing in the online activity of not only its targets but also friends, family, and community members, ICE is certain to collect far more information outside its mandate than it is likely to publicly concede.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45465964</guid><pubDate>Fri, 03 Oct 2025 18:13:16 +0000</pubDate></item><item><title>PEP 810 – Explicit lazy imports</title><link>https://pep-previews--4622.org.readthedocs.build/pep-0810/</link><description>&lt;doc fingerprint="d0324610caa1e4dc"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;PEP 810 – Explicit lazy imports&lt;/head&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Author:&lt;/item&gt;
      &lt;item rend="dd-1"&gt;Pablo Galindo &amp;lt;pablogsal at python.org&amp;gt;, Germán Méndez Bravo &amp;lt;german.mb at gmail.com&amp;gt;, Thomas Wouters &amp;lt;thomas at python.org&amp;gt;, Dino Viehland &amp;lt;dinoviehland at gmail.com&amp;gt;, Brittany Reynoso &amp;lt;brittanyrey at gmail.com&amp;gt;, Noah Kim &amp;lt;noahbkim at gmail.com&amp;gt;, Tim Stumbaugh &amp;lt;me at tjstum.com&amp;gt;&lt;/item&gt;
      &lt;item rend="dt-2"&gt;Discussions-To:&lt;/item&gt;
      &lt;item rend="dd-2"&gt;Discourse thread&lt;/item&gt;
      &lt;item rend="dt-3"&gt;Status:&lt;/item&gt;
      &lt;item rend="dd-3"&gt;Draft&lt;/item&gt;
      &lt;item rend="dt-4"&gt;Type:&lt;/item&gt;
      &lt;item rend="dd-4"&gt;Standards Track&lt;/item&gt;
      &lt;item rend="dt-5"&gt;Created:&lt;/item&gt;
      &lt;item rend="dd-5"&gt;02-Oct-2025&lt;/item&gt;
      &lt;item rend="dt-6"&gt;Python-Version:&lt;/item&gt;
      &lt;item rend="dd-6"&gt;3.15&lt;/item&gt;
      &lt;item rend="dt-7"&gt;Post-History:&lt;/item&gt;
      &lt;item rend="dd-7"&gt;03-Oct-2025&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Abstract&lt;/head&gt;
    &lt;p&gt;This PEP introduces syntax for lazy imports as an explicit language feature:&lt;/p&gt;
    &lt;code&gt;lazy import json
lazy from json import dumps
&lt;/code&gt;
    &lt;p&gt;Lazy imports defer the loading and execution of a module until the first time the imported name is used, in contrast to ‘normal’ imports, which eagerly load and execute a module at the point of the import statement.&lt;/p&gt;
    &lt;p&gt;By allowing developers to mark individual imports as lazy with explicit syntax, Python programs can reduce startup time, memory usage, and unnecessary work. This is particularly beneficial for command-line tools, test suites, and applications with large dependency graphs.&lt;/p&gt;
    &lt;p&gt;This proposal preserves full backwards compatibility: normal import statements remain unchanged, and lazy imports are enabled only where explicitly requested.&lt;/p&gt;
    &lt;head rend="h2"&gt;Motivation&lt;/head&gt;
    &lt;p&gt;The dominant convention in Python code is to place all imports at the module level, typically at the beginning of the file. This avoids repetition, makes import dependencies clear and minimizes runtime overhead by only evaluating an import statement once per module.&lt;/p&gt;
    &lt;p&gt;A major drawback with this approach is that importing the first module for an execution of Python (the “main” module) often triggers an immediate cascade of imports, and optimistically loads many dependencies that may never be used. The effect is especially costly for command-line tools with multiple subcommands, where even running the command with &lt;code&gt;--help&lt;/code&gt; can load dozens of
unnecessary modules and take several seconds. This basic example demonstrates
what must be loaded just to get helpful feedback to the user on how to run the
program at all. Inefficiently, the user incurs this overhead again when they
figure out the command they want and invoke the program “for real.”&lt;/p&gt;
    &lt;p&gt;A somewhat common way to delay imports is to move the imports into functions (inline imports), but this practice requires more work to implement and maintain, and can be subverted by a single inadvertent top-level import. Additionally, it obfuscates the full set of dependencies for a module. Analysis of the Python standard library shows that approximately 17% of all imports outside tests (nearly 3500 total imports across 730 files) are already placed inside functions or methods specifically to defer their execution. This demonstrates that developers are already manually implementing lazy imports in performance-sensitive code, but doing so requires scattering imports throughout the codebase and makes the full dependency graph harder to understand at a glance.&lt;/p&gt;
    &lt;p&gt;The standard library provides the &lt;code&gt;LazyLoader&lt;/code&gt; class to
solve some of these inefficiency problems. It permits imports at the module
level to work mostly like inline imports do. Many scientific Python
libraries have adopted a similar pattern, formalized in
SPEC 1.
There’s also the third-party lazy_loader package, yet another
implementation of lazy imports. Imports used solely for static type checking
are another source of potentially unneeded imports, and there are similarly
disparate approaches to minimizing the overhead. The various approaches used
here to defer or remove eager imports do not cover all potential use-cases for
a general lazy import mechanism. There is no clear standard, and there are
several drawbacks including runtime overhead in unexpected places, or worse
runtime introspection.&lt;/p&gt;
    &lt;p&gt;This proposal introduces syntax for lazy imports with a design that is local, explicit, controlled, and granular. Each of these qualities is essential to making the feature predictable and safe to use in practice.&lt;/p&gt;
    &lt;p&gt;The behavior is local: laziness applies only to the specific import marked with the &lt;code&gt;lazy&lt;/code&gt; keyword, and it does not cascade recursively into other
imports. This ensures that developers can reason about the effect of laziness
by looking only at the line of code in front of them, without worrying about
whether imported modules will themselves behave differently. A &lt;code&gt;lazy import&lt;/code&gt;
is an isolated decision each time it is used, not a global shift in semantics.&lt;/p&gt;
    &lt;p&gt;The semantics are explicit. When a name is imported lazily, the binding is created in the importing module immediately, but the target module is not loaded until the first time the name is accessed. After this point, the binding is indistinguishable from one created by a normal import. This clarity reduces surprises and makes the feature accessible to developers who may not be deeply familiar with Python’s import machinery.&lt;/p&gt;
    &lt;p&gt;Lazy imports are controlled, in the sense that deferred loading is only triggered by the importing code itself. In the general case, a library will only experience lazy imports if its own authors choose to mark them as such. This avoids shifting responsibility onto downstream users and prevents accidental surprises in library behavior. Since library authors typically manage their own import subgraphs, they retain predictable control over when and how laziness is applied.&lt;/p&gt;
    &lt;p&gt;The mechanism is also granular. It is introduced through explicit syntax on individual imports, rather than a global flag or implicit setting. This allows developers to adopt it incrementally, starting with the most performance-sensitive areas of a codebase. As this feature is introduced to the community, we want to make the experience of onboarding optional, progressive, and adaptable to the needs of each project.&lt;/p&gt;
    &lt;p&gt;Lazy imports provide several concrete advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Command-line tools are often invoked directly by a user, so latency – in particular startup latency – is quite noticeable. These programs are also typically short-lived processes (contrasted with, e.g., a web server). With lazy imports, only the code paths actually reached will import a module. This can reduce startup time by 50-70% in practice, providing a significant improvement to a common user experience and improving Python’s competitiveness in domains where fast startup matters most.&lt;/item&gt;
      &lt;item&gt;Type annotations frequently require imports that are never used at runtime. The common workaround is to wrap them in &lt;code&gt;if TYPE_CHECKING:&lt;/code&gt;blocks [1]. With lazy imports, annotation-only imports impose no runtime penalty, eliminating the need for such guards and making annotated codebases cleaner.&lt;/item&gt;
      &lt;item&gt;Large applications often import thousands of modules, and each module creates function and type objects, incurring memory costs. In long-lived processes, this noticeably raises baseline memory usage. Lazy imports defer these costs until a module is needed, keeping unused subsystems unloaded. Memory savings of 30-40% have been observed in real workloads.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Rationale&lt;/head&gt;
    &lt;p&gt;The design of this proposal is centered on clarity, predictability, and ease of adoption. Each decision was made to ensure that lazy imports provide tangible benefits without introducing unnecessary complexity into the language or its runtime.&lt;/p&gt;
    &lt;p&gt;It is also worth noting that while this PEP outlines one specific approach, we list alternate implementation strategies for some of the core aspects and semantics of the proposal. If the community expresses a strong preference for a different technical path that still preserves the same core semantics or there is fundamental disagreement over the specific option, we have included the brainstorming we have already completed in preparation for this proposal as reference.&lt;/p&gt;
    &lt;p&gt;The choice to introduce a new &lt;code&gt;lazy&lt;/code&gt; keyword reflects the need for explicit
syntax. Import behavior is too fundamental to be left implicit or hidden
behind global flags or environment variables. By marking laziness directly at
the import site, the intent is immediately visible to both readers and tools.
This avoids surprises, reduces the cognitive burden of reasoning about
imports, and keeps lazy import semantics in line with Python’s tradition of
explicitness.&lt;/p&gt;
    &lt;p&gt;Another important decision is to represent lazy imports with proxy objects in the module’s namespace, rather than by modifying dictionary lookup. Earlier approaches experimented with embedding laziness into dictionaries, but this blurred abstractions and risked affecting unrelated parts of the runtime. The dictionary is a fundamental data structure in Python – literally every object is built on top of dicts – and adding hooks to dictionaries would prevent critical optimizations and complicate the entire runtime. The proxy approach is simpler: it behaves like a placeholder until first use, at which point it resolves the import and rebinds the name. From then on, the binding is indistinguishable from a normal import. This makes the mechanism easy to explain and keeps the rest of the interpreter unchanged.&lt;/p&gt;
    &lt;p&gt;Compatibility for library authors was also a key concern. Many maintainers need a migration path that allows them to support both new and old versions of Python at once. For this reason, the proposal includes the &lt;code&gt;__lazy_modules__&lt;/code&gt; global as a transitional mechanism. A module can
declare which imports should be treated as lazy (by listing the module names
as strings), and on Python 3.15 or later those imports will become lazy
automatically, as if they were imported with the &lt;code&gt;lazy&lt;/code&gt; keyword. On earlier
versions the declaration is ignored, leaving imports eager. This gives authors
a practical bridge until they can rely on the keyword as the canonical syntax.&lt;/p&gt;
    &lt;p&gt;Finally, the feature is designed to be adopted incrementally. Nothing changes unless a developer explicitly opts in, and adoption can begin with just a few imports in performance-sensitive areas. This mirrors the experience of gradual typing in Python: a mechanism that can be introduced progressively, without forcing projects to commit globally from day one. Notably, the adoption can also be done from the “outside in”, permitting CLI authors to introduce lazy imports and speed up user-facing tools, without requiring changes to every library the tool might use.&lt;/p&gt;
    &lt;head rend="h3"&gt;Other design decisions&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The scope of laziness is deliberately local and non-recursive. A lazy import only affects the specific statement where it appears; it does not cascade into other modules or submodules. This choice is crucial for predictability. When developers read code, they can reason about import behavior line by line, without worrying about hidden laziness deeper in the dependency graph. The result is a feature that is powerful but still easy to understand in context.&lt;/item&gt;
      &lt;item&gt;In addition, it is useful to provide a mechanism to activate or deactivate lazy imports at a global level. While the primary design centers on explicit syntax, there are scenarios – such as large applications, testing environments, or frameworks – where enabling laziness consistently across many modules provides the most benefit. A global switch makes it easy to experiment with or enforce consistent behavior, while still working in combination with the filtering API to respect exclusions or tool-specific configuration. This ensures that global adoption can be practical without reducing flexibility or control.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Specification&lt;/head&gt;
    &lt;head rend="h3"&gt;Grammar&lt;/head&gt;
    &lt;p&gt;A new soft keyword &lt;code&gt;lazy&lt;/code&gt; is added. A soft keyword is a context-sensitive
keyword that only has special meaning in specific grammatical contexts;
elsewhere it can be used as a regular identifier (e.g., as a variable name).
The &lt;code&gt;lazy&lt;/code&gt; keyword only has special meaning when it appears before import
statements:&lt;/p&gt;
    &lt;code&gt;import_name:
    | 'lazy'? 'import' dotted_as_names

import_from:
    | 'lazy'? 'from' ('.' | '...')* dotted_name 'import' import_from_targets
    | 'lazy'? 'from' ('.' | '...')+ 'import' import_from_targets
&lt;/code&gt;
    &lt;head rend="h4"&gt;Syntax restrictions&lt;/head&gt;
    &lt;p&gt;The soft keyword is only allowed at the global (module) level, not inside functions, class bodies, with &lt;code&gt;try&lt;/code&gt;/&lt;code&gt;with&lt;/code&gt; blocks, or &lt;code&gt;import *&lt;/code&gt;. Import
statements that use the soft keyword are potentially lazy. Imports that
can’t be lazy are unaffected by the global lazy imports flag, and instead are
always eager.&lt;/p&gt;
    &lt;p&gt;Examples of syntax errors:&lt;/p&gt;
    &lt;code&gt;# SyntaxError: lazy import not allowed inside functions
def foo():
    lazy import json

# SyntaxError: lazy import not allowed inside classes
class Bar:
    lazy import json

# SyntaxError: lazy import not allowed inside try/except blocks
try:
    lazy import json
except ImportError:
    pass

# SyntaxError: lazy import not allowed inside with blocks
with suppress(ImportError):
    lazy import json

# SyntaxError: lazy from ... import * is not allowed
lazy from json import *
&lt;/code&gt;
    &lt;head rend="h3"&gt;Semantics&lt;/head&gt;
    &lt;p&gt;When the &lt;code&gt;lazy&lt;/code&gt; keyword is used, the import becomes potentially lazy.
Unless lazy imports are disabled or suppressed (see below), the module is not
loaded immediately at the import statement; instead, a lazy proxy object is
created and bound to the name. The actual module is loaded on first use of
that name.&lt;/p&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;import sys

lazy import json

print('json' in sys.modules)  # False - module not loaded yet

# First use triggers loading
result = json.dumps({"hello": "world"})

print('json' in sys.modules)  # True - now loaded
&lt;/code&gt;
    &lt;p&gt;A module may contain a &lt;code&gt;__lazy_modules__&lt;/code&gt; attribute, which is a
sequence of fully qualified module names (strings) to make potentially lazy
(as if the &lt;code&gt;lazy&lt;/code&gt; keyword was used). This attribute is checked on each
&lt;code&gt;import&lt;/code&gt; statement to determine whether the import should be made
potentially lazy. When a module is made lazy this way, from-imports using
that module are also lazy, but not necessarily imports of sub-modules.&lt;/p&gt;
    &lt;p&gt;The normal (non-lazy) import statement will check the global lazy imports flag. If it is “enabled”, all imports are potentially lazy (except for imports that can’t be lazy, as mentioned above.)&lt;/p&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;__lazy_modules__ = ["json"]
import json
print('json' in sys.modules)  # False
result = json.dumps({"hello": "world"})
print('json' in sys.modules)  # True
&lt;/code&gt;
    &lt;p&gt;If the global lazy imports flag is set to “disabled”, no potentially lazy import is ever imported lazily, and the behavior is equivalent to a regular import statement: the import is eager (as if the lazy keyword was not used).&lt;/p&gt;
    &lt;p&gt;For a potentially lazy import, the lazy imports filter (if set) is called with the name of the module doing the import, the name of the module being imported, and (if applicable) the fromlist. If the lazy import filter returns &lt;code&gt;True&lt;/code&gt;, the potentially lazy import becomes a lazy import. Otherwise, the
import is not lazy, and the normal (eager) import continues.&lt;/p&gt;
    &lt;head rend="h3"&gt;Lazy import mechanism&lt;/head&gt;
    &lt;p&gt;When an import is lazy, &lt;code&gt;__lazy_import__&lt;/code&gt; is called instead of
&lt;code&gt;__import__&lt;/code&gt;. &lt;code&gt;__lazy_import__&lt;/code&gt; has the same function signature as
&lt;code&gt;__import__&lt;/code&gt;. It adds the module name to &lt;code&gt;sys.lazy_modules&lt;/code&gt;, a set of
fully-qualified module names which have been lazily imported at some point
(primarily for diagnostics and introspection), and returns a “lazy module
object.”&lt;/p&gt;
    &lt;p&gt;The implementation of &lt;code&gt;from ... import&lt;/code&gt; (the &lt;code&gt;IMPORT_FROM&lt;/code&gt; bytecode
implementation) checks if the module it’s fetching from is a lazy module
object, and if so, returns a lazy object for each name instead.&lt;/p&gt;
    &lt;p&gt;The end result of this process is that lazy imports (regardless of how they are enabled) result in lazy objects being assigned to global variables.&lt;/p&gt;
    &lt;p&gt;Lazy module objects do not appear in &lt;code&gt;sys.modules&lt;/code&gt;, they’re just listed in
the &lt;code&gt;sys.lazy_modules&lt;/code&gt; set. Under normal operation lazy objects should only
end up stored in global variables, and the common ways to access those
variables (regular variable access, module attributes) will resolve lazy
imports (“reify”) and replace them when they’re accessed.&lt;/p&gt;
    &lt;p&gt;It is still possible to expose lazy objects through other means, like debuggers. This is not considered a problem.&lt;/p&gt;
    &lt;head rend="h3"&gt;Reification&lt;/head&gt;
    &lt;p&gt;When a lazy object is first used, it needs to be reified. This means resolving the import at that point in the program and replacing the lazy object with the concrete one. Reification imports the module in the same way as it would have been if it had been imported eagerly, barring intervening changes to the import system (e.g. to &lt;code&gt;sys.path&lt;/code&gt;, &lt;code&gt;sys.meta_path&lt;/code&gt;, &lt;code&gt;sys.path_hooks&lt;/code&gt; or
&lt;code&gt;__import__&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;Reification still calls &lt;code&gt;__import__&lt;/code&gt; to resolve the import. When the module
is first reified, it’s removed from &lt;code&gt;sys.lazy_modules&lt;/code&gt; (even if there are
still other unreified lazy references to it). When a package is reified and
submodules in the package were also previously lazily imported, those
submodules are not automatically reified but they are added to the reified
package’s globals (unless the package already assigned something else to the
name of the submodule).&lt;/p&gt;
    &lt;p&gt;If reification fails (e.g., due to an &lt;code&gt;ImportError&lt;/code&gt;), the exception is
enhanced with chaining to show both where the lazy import was defined and
where it was first accessed (even though it propagates from the code that
triggered reification). This provides clear debugging information:&lt;/p&gt;
    &lt;code&gt;# app.py - has a typo in the import
lazy from json import dumsp  # Typo: should be 'dumps'

print("App started successfully")
print("Processing data...")

# Error occurs here on first use
result = dumsp({"key": "value"})
&lt;/code&gt;
    &lt;p&gt;The traceback shows both locations:&lt;/p&gt;
    &lt;code&gt;App started successfully
Processing data...
Traceback (most recent call last):
  File "app.py", line 2, in &amp;lt;module&amp;gt;
    lazy from json import dumsp
ImportError: deferred import of 'json.dumsp' raised an exception during resolution

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "app.py", line 8, in &amp;lt;module&amp;gt;
    result = dumsp({"key": "value"})
             ^^^^^
ImportError: cannot import name 'dumsp' from 'json'. Did you mean: 'dump'?
&lt;/code&gt;
    &lt;p&gt;This exception chaining clearly shows: (1) where the lazy import was defined, (2) that it was deferred, and (3) where the actual access happened that triggered the error.&lt;/p&gt;
    &lt;p&gt;Reification does not automatically occur when a module that was previously lazily imported is subsequently eagerly imported. Reification does not immediately resolve all lazy objects (e.g. &lt;code&gt;lazy from&lt;/code&gt; statements) that
referenced the module. It only resolves the lazy object being accessed.&lt;/p&gt;
    &lt;p&gt;Accessing a lazy object (from a global variable or a module attribute) reifies the object. Accessing a module’s &lt;code&gt;__dict__&lt;/code&gt; reifies all lazy objects in
that module. Operations that indirectly access &lt;code&gt;__dict__&lt;/code&gt; (such as
&lt;code&gt;dir()&lt;/code&gt;) also trigger this behavior.&lt;/p&gt;
    &lt;p&gt;Example using &lt;code&gt;__dict__&lt;/code&gt; from external code:&lt;/p&gt;
    &lt;code&gt;# my_module.py
import sys
lazy import json

print('json' in sys.modules)  # False - still lazy

# main.py
import sys
import my_module

# Accessing __dict__ from external code DOES reify all lazy imports
d = my_module.__dict__

print('json' in sys.modules)  # True - reified by __dict__ access
print(type(d['json']))  # &amp;lt;class 'module'&amp;gt;
&lt;/code&gt;
    &lt;p&gt;However, calling &lt;code&gt;globals()&lt;/code&gt; does not trigger reification – it returns
the module’s dictionary, and accessing lazy objects through that dictionary
still returns lazy proxy objects that need to be manually reified upon use. A
lazy object can be resolved explicitly by calling the &lt;code&gt;get&lt;/code&gt; method. Other,
more indirect ways of accessing arbitrary globals (e.g. inspecting
&lt;code&gt;frame.f_globals&lt;/code&gt;) also do not reify all the objects.&lt;/p&gt;
    &lt;p&gt;Example using &lt;code&gt;globals()&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;import sys
lazy import json

# Calling globals() does NOT trigger reification
g = globals()

print('json' in sys.modules)  # False - still lazy
print(type(g['json']))  # &amp;lt;class 'lazy_import'&amp;gt;

# Explicitly reify using the get() method
resolved = g['json'].get()

print(type(resolved))  # &amp;lt;class 'module'&amp;gt;
print('json' in sys.modules)  # True - now loaded
&lt;/code&gt;
    &lt;head rend="h2"&gt;Reference Implementation&lt;/head&gt;
    &lt;p&gt;A reference implementation is available at: https://github.com/LazyImportsCabal/cpython/tree/lazy&lt;/p&gt;
    &lt;head rend="h3"&gt;Bytecode and adaptive specialization&lt;/head&gt;
    &lt;p&gt;Lazy imports are implemented through modifications to four bytecode instructions: &lt;code&gt;IMPORT_NAME&lt;/code&gt;, &lt;code&gt;IMPORT_FROM&lt;/code&gt;, &lt;code&gt;LOAD_GLOBAL&lt;/code&gt;, and
&lt;code&gt;LOAD_NAME&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;lazy&lt;/code&gt; syntax sets a flag in the &lt;code&gt;IMPORT_NAME&lt;/code&gt; instruction’s oparg
(&lt;code&gt;oparg &amp;amp; 0x01&lt;/code&gt;). The interpreter checks this flag and calls
&lt;code&gt;_PyEval_LazyImportName()&lt;/code&gt; instead of &lt;code&gt;_PyEval_ImportName()&lt;/code&gt;, creating a
lazy import object rather than executing the import immediately. The
&lt;code&gt;IMPORT_FROM&lt;/code&gt; instruction checks whether its source is a lazy import
(&lt;code&gt;PyLazyImport_CheckExact()&lt;/code&gt;) and creates a lazy object for the attribute
rather than accessing it immediately.&lt;/p&gt;
    &lt;p&gt;When a lazy object is accessed, it must be reified. The &lt;code&gt;LOAD_GLOBAL&lt;/code&gt;
instruction (used in function scopes) and &lt;code&gt;LOAD_NAME&lt;/code&gt; instruction (used at
module and class level) both check whether the object being loaded is a lazy
import. If so, they call &lt;code&gt;_PyImport_LoadLazyImportTstate()&lt;/code&gt; to perform the
actual import and store the module in &lt;code&gt;sys.modules&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;This check incurs a very small cost on each access. However, Python’s adaptive interpreter can specialize &lt;code&gt;LOAD_GLOBAL&lt;/code&gt; after observing that a lazy import
has been reified. After several executions, &lt;code&gt;LOAD_GLOBAL&lt;/code&gt; becomes
&lt;code&gt;LOAD_GLOBAL_MODULE&lt;/code&gt;, which accesses the module dictionary directly without
checking for lazy imports.&lt;/p&gt;
    &lt;p&gt;Examples of the bytecode generated:&lt;/p&gt;
    &lt;code&gt;lazy import json  # IMPORT_NAME with flag set
&lt;/code&gt;
    &lt;p&gt;Generates:&lt;/p&gt;
    &lt;code&gt;IMPORT_NAME              1 (json + lazy)
&lt;/code&gt;
    &lt;code&gt;lazy from json import dumps  # IMPORT_NAME + IMPORT_FROM
&lt;/code&gt;
    &lt;p&gt;Generates:&lt;/p&gt;
    &lt;code&gt;IMPORT_NAME              1 (json + lazy)
IMPORT_FROM              1 (dumps)
&lt;/code&gt;
    &lt;code&gt;lazy import json
x = json  # Module-level access
&lt;/code&gt;
    &lt;p&gt;Generates:&lt;/p&gt;
    &lt;code&gt;LOAD_NAME                0 (json)
&lt;/code&gt;
    &lt;code&gt;lazy import json

def use_json():
    return json.dumps({})  # Function scope
&lt;/code&gt;
    &lt;p&gt;Before any calls:&lt;/p&gt;
    &lt;code&gt;LOAD_GLOBAL              0 (json)
LOAD_ATTR                2 (dumps)
&lt;/code&gt;
    &lt;p&gt;After several calls, &lt;code&gt;LOAD_GLOBAL&lt;/code&gt; specializes to &lt;code&gt;LOAD_GLOBAL_MODULE&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;LOAD_GLOBAL_MODULE       0 (json)
LOAD_ATTR_MODULE         2 (dumps)
&lt;/code&gt;
    &lt;head rend="h3"&gt;Lazy imports filter&lt;/head&gt;
    &lt;p&gt;This PEP adds two new functions to the &lt;code&gt;sys&lt;/code&gt; module to manage the lazy
imports filter:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;sys.set_lazy_imports_filter(func)&lt;/code&gt;- Sets the filter function. The&lt;code&gt;func&lt;/code&gt;parameter must have the signature:&lt;code&gt;func(importer: str, name: str, fromlist: tuple[str, ...] | None) -&amp;gt; bool&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;sys.get_lazy_imports_filter()&lt;/code&gt;- Returns the currently installed filter function, or&lt;code&gt;None&lt;/code&gt;if no filter is set.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The filter function is called for every potentially lazy import, and must return &lt;code&gt;True&lt;/code&gt; if the import should be lazy. This allows for fine-grained
control over which imports should be lazy, useful for excluding modules with
known side-effect dependencies or registration patterns.&lt;/p&gt;
    &lt;p&gt;The filter mechanism serves as a foundation that tools, debuggers, linters, and other ecosystem utilities can leverage to provide better lazy import experiences. For example, static analysis tools could detect modules with side effects and automatically configure appropriate filters. In the future (out of scope for this PEP), this foundation may enable better ways to declaratively specify which modules are safe for lazy importing, such as package metadata, type stubs with lazy-safety annotations, or configuration files. The current filter API is designed to be flexible enough to accommodate such future enhancements without requiring changes to the core language specification.&lt;/p&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;import sys

def exclude_side_effect_modules(importer, name, fromlist):
    """
    Filter function to exclude modules with import-time side effects.

    Args:
        importer: Name of the module doing the import
        name: Name of the module being imported
        fromlist: Tuple of names being imported (for 'from' imports), or None

    Returns:
        True to allow lazy import, False to force eager import
    """
    # Modules known to have important import-time side effects
    side_effect_modules = {'legacy_plugin_system', 'metrics_collector'}

    if name in side_effect_modules:
        return False  # Force eager import

    return True  # Allow lazy import

# Install the filter
sys.set_lazy_imports_filter(exclude_side_effect_modules)

# These imports are checked by the filter
lazy import data_processor        # Filter returns True -&amp;gt; stays lazy
lazy import legacy_plugin_system  # Filter returns False -&amp;gt; imported eagerly

print('data_processor' in sys.modules)       # False - still lazy
print('legacy_plugin_system' in sys.modules) # True - loaded eagerly

# First use of data_processor triggers loading
result = data_processor.transform(data)
print('data_processor' in sys.modules)       # True - now loaded
&lt;/code&gt;
    &lt;head rend="h3"&gt;Global lazy imports control&lt;/head&gt;
    &lt;p&gt;The global lazy imports flag can be controlled through:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The &lt;code&gt;-X lazy_imports=&amp;lt;mode&amp;gt;&lt;/code&gt;command-line option&lt;/item&gt;
      &lt;item&gt;The &lt;code&gt;PYTHON_LAZY_IMPORTS=&amp;lt;mode&amp;gt;&lt;/code&gt;environment variable&lt;/item&gt;
      &lt;item&gt;The &lt;code&gt;sys.set_lazy_imports(mode)&lt;/code&gt;function (primarily for testing)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Where &lt;code&gt;&amp;lt;mode&amp;gt;&lt;/code&gt; can be:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;"default"&lt;/code&gt;(or unset): Only explicitly marked lazy imports are lazy&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;"enabled"&lt;/code&gt;: All module-level imports (except in&lt;code&gt;try&lt;/code&gt;or&lt;code&gt;with&lt;/code&gt;blocks and&lt;code&gt;import *&lt;/code&gt;) become potentially lazy&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;"disabled"&lt;/code&gt;: No imports are lazy, even those explicitly marked with&lt;code&gt;lazy&lt;/code&gt;keyword&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When the global flag is set to &lt;code&gt;"enabled"&lt;/code&gt;, all imports at the global level
of all modules are potentially lazy except for those inside a &lt;code&gt;try&lt;/code&gt; or
&lt;code&gt;with&lt;/code&gt; block or any wild card (&lt;code&gt;from ... import *&lt;/code&gt;) import.&lt;/p&gt;
    &lt;p&gt;If the global lazy imports flag is set to &lt;code&gt;"disabled"&lt;/code&gt;, no potentially
lazy import is ever imported lazily, the import filter is never called, and
the behavior is equivalent to a regular &lt;code&gt;import&lt;/code&gt; statement: the import is
eager (as if the lazy keyword was not used).&lt;/p&gt;
    &lt;head rend="h2"&gt;Backwards Compatibility&lt;/head&gt;
    &lt;p&gt;Lazy imports are opt-in. Existing programs continue to run unchanged unless a project explicitly enables laziness (via &lt;code&gt;lazy&lt;/code&gt; syntax,
&lt;code&gt;__lazy_modules__&lt;/code&gt;, or an interpreter-wide switch).&lt;/p&gt;
    &lt;head rend="h3"&gt;Unchanged semantics&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Regular &lt;code&gt;import&lt;/code&gt;and&lt;code&gt;from ... import ...&lt;/code&gt;statements remain eager unless explicitly made potentially lazy by the local or global mechanisms provided.&lt;/item&gt;
      &lt;item&gt;Dynamic import APIs remain eager and unchanged: &lt;code&gt;__import__()&lt;/code&gt;and&lt;code&gt;importlib.import_module()&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Import hooks and loaders continue to run under the standard import protocol when a lazy object is reified.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Observable behavioral shifts (opt-in only)&lt;/head&gt;
    &lt;p&gt;These changes are limited to bindings explicitly made lazy:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Error timing. Exceptions that would have occurred during an eager import (for example &lt;code&gt;ImportError&lt;/code&gt;or&lt;code&gt;AttributeError&lt;/code&gt;for a missing member) now occur at the first use of the lazy name.&lt;quote&gt;# With eager import - error at import statement import broken_module # ImportError raised here # With lazy import - error deferred lazy import broken_module print("Import succeeded") broken_module.foo() # ImportError raised here on first use&lt;/quote&gt;&lt;/item&gt;
      &lt;item&gt;Side-effect timing. Import-time side effects in lazily imported modules occur at first use of the binding, not at module import time.&lt;/item&gt;
      &lt;item&gt;Import order. Because modules are imported on first use, the order in which modules are imported may differ from how they appear in code.&lt;/item&gt;
      &lt;item&gt;Presence in ``sys.modules``. A lazily imported module does not appear in &lt;code&gt;sys.modules&lt;/code&gt;until first use. After reification, it must appear in&lt;code&gt;sys.modules&lt;/code&gt;. If some other code eagerly imports the same module before first use, the lazy binding resolves to that existing (lazy) module object when it is first used.&lt;/item&gt;
      &lt;item&gt;Proxy visibility. Before first use, the bound name refers to a lazy proxy. Indirect introspection that touches the value may observe a proxy lazy object representation. After first use, the name is rebound to the real object and becomes indistinguishable from an eager import.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Thread-safety and reification&lt;/head&gt;
    &lt;p&gt;First use of a lazy binding follows the existing import-lock discipline. Exactly one thread performs the import and atomically rebinds the importing module’s global to the resolved object. Concurrent readers thereafter observe the real object.&lt;/p&gt;
    &lt;p&gt;Lazy imports are thread-safe and have no special considerations for free-threading. A module that would normally be imported in the main thread may be imported in a different thread if that thread triggers the first access to the lazy import. This is not a problem: the import lock ensures thread safety regardless of which thread performs the import.&lt;/p&gt;
    &lt;p&gt;Subinterpreters are supported. Each subinterpreter maintains its own &lt;code&gt;sys.lazy_modules&lt;/code&gt; and import state, so lazy imports in one subinterpreter
do not affect others.&lt;/p&gt;
    &lt;head rend="h3"&gt;Typing and tools&lt;/head&gt;
    &lt;p&gt;Type checkers and static analyzers may treat &lt;code&gt;lazy&lt;/code&gt; imports as ordinary
imports for name resolution. At runtime, annotation-only imports can be marked
&lt;code&gt;lazy&lt;/code&gt; to avoid startup overhead. IDEs and debuggers should be prepared to
display lazy proxies before first use and the real objects thereafter.&lt;/p&gt;
    &lt;head rend="h2"&gt;Security Implications&lt;/head&gt;
    &lt;p&gt;There are no known security vulnerabilities introduced by lazy imports.&lt;/p&gt;
    &lt;head rend="h2"&gt;How to Teach This&lt;/head&gt;
    &lt;p&gt;The new &lt;code&gt;lazy&lt;/code&gt; keyword will be documented as part of the language standard.&lt;/p&gt;
    &lt;p&gt;As this feature is opt-in, new Python users should be able to continue using the language as they are used to. For experienced developers, we expect them to leverage lazy imports for the variety of benefits listed above (decreased latency, decreased memory usage, etc) on a case-by-case basis. Developers interested in the performance of their Python binary will likely leverage profiling to understand the import time overhead in their codebase and mark the necessary imports as &lt;code&gt;lazy&lt;/code&gt;. In addition, developers can mark imports
that will only be used for type annotations as &lt;code&gt;lazy&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Below is guidance on how to best take advantage of lazy imports and how to avoid incompatibilities:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;When adopting lazy imports, users should be aware that eliding an import until it is used will result in side effects not being executed. In turn, users should be wary of modules that rely on import time side effects. Perhaps the most common reliance on import side effects is the registry pattern, where population of some external registry happens implicitly during the importing of modules, often via decorators but sometimes implemented via metaclasses or &lt;code&gt;__init_subclass__&lt;/code&gt;. Instead, registries of objects should be constructed via explicit discovery processes (e.g. a well-known function to call).&lt;quote&gt;# Problematic: Plugin registers itself on import # my_plugin.py from plugin_registry import register_plugin @register_plugin("MyPlugin") class MyPlugin: pass # In main code: lazy import my_plugin # Plugin NOT registered yet - module not loaded! # Better: Explicit discovery # plugin_registry.py def discover_plugins(): from my_plugin import MyPlugin register_plugin(MyPlugin) # In main code: plugin_registry.discover_plugins() # Explicit loading&lt;/quote&gt;&lt;/item&gt;
      &lt;item&gt;Always import needed submodules explicitly. It is not enough to rely on a different import to ensure a module has its submodules as attributes. Plainly, unless there is an explicit &lt;code&gt;from . import bar&lt;/code&gt;in&lt;code&gt;foo/__init__.py&lt;/code&gt;, always use&lt;code&gt;import foo.bar; foo.bar.Baz&lt;/code&gt;, not&lt;code&gt;import foo; foo.bar.Baz&lt;/code&gt;. The latter only works (unreliably) because the attribute&lt;code&gt;foo.bar&lt;/code&gt;is added as a side effect of&lt;code&gt;foo.bar&lt;/code&gt;being imported somewhere else.&lt;/item&gt;
      &lt;item&gt;Users who are moving imports into functions to improve startup time, should instead consider keeping them where they are but adding the &lt;code&gt;lazy&lt;/code&gt;keyword. This allows them to keep dependencies clear and avoid the overhead of repeatedly re-resolving the import but will still speed up the program.&lt;quote&gt;# Before: Inline import (repeated overhead) def process_data(data): import json # Re-resolved on every call return json.dumps(data) # After: Lazy import at module level lazy import json def process_data(data): return json.dumps(data) # Loaded once on first call&lt;/quote&gt;&lt;/item&gt;
      &lt;item&gt;Avoid using wild card (star) imports, as those are always eager.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;FAQ&lt;/head&gt;
    &lt;p&gt;Q: How does this differ from the rejected PEP 690?&lt;/p&gt;
    &lt;p&gt;A: PEP 810 takes an explicit, opt-in approach instead of PEP 690’s implicit global approach. The key differences are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Explicit syntax: &lt;code&gt;lazy import foo&lt;/code&gt;clearly marks which imports are lazy.&lt;/item&gt;
      &lt;item&gt;Local scope: Laziness only affects the specific import statement, not cascading to dependencies.&lt;/item&gt;
      &lt;item&gt;Simpler implementation: Uses proxy objects instead of modifying core dictionary behavior.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Q: What happens when lazy imports encounter errors?&lt;/p&gt;
    &lt;p&gt;A: Import errors (&lt;code&gt;ImportError&lt;/code&gt;, &lt;code&gt;ModuleNotFoundError&lt;/code&gt;, syntax errors) are
deferred until first use of the lazy name. This is similar to moving an import
into a function. The error will occur with a clear traceback pointing to the
first access of the lazy object.&lt;/p&gt;
    &lt;p&gt;The implementation provides enhanced error reporting through exception chaining. When a lazy import fails during reification, the original exception is preserved and chained, showing both where the import was defined and where it was first used:&lt;/p&gt;
    &lt;code&gt;Traceback (most recent call last):
  File "test.py", line 1, in &amp;lt;module&amp;gt;
    lazy import broken_module
ImportError: deferred import of 'broken_module' raised an exception during resolution

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "test.py", line 3, in &amp;lt;module&amp;gt;
    broken_module.foo()
    ^^^^^^^^^^^^^
  File "broken_module.py", line 2, in &amp;lt;module&amp;gt;
    1/0
ZeroDivisionError: division by zero
&lt;/code&gt;
    &lt;p&gt;Q: How do lazy imports affect modules with import-time side effects?&lt;/p&gt;
    &lt;p&gt;A: Side effects are deferred until first use. This is generally desirable for performance, but may require code changes for modules that rely on import-time registration patterns. We recommend:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Use explicit initialization functions instead of import-time side effects&lt;/item&gt;
      &lt;item&gt;Call initialization functions explicitly when needed&lt;/item&gt;
      &lt;item&gt;Avoid relying on import order for side effects&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Q: Can I use lazy imports with &lt;code&gt;from ... import ...&lt;/code&gt; statements?&lt;/p&gt;
    &lt;p&gt;A: Yes, as long as you don’t use &lt;code&gt;from ... import *&lt;/code&gt;. Both &lt;code&gt;lazy import
foo&lt;/code&gt; and &lt;code&gt;lazy from foo import bar&lt;/code&gt; are supported. The &lt;code&gt;bar&lt;/code&gt; name will be
bound to a lazy object that resolves to &lt;code&gt;foo.bar&lt;/code&gt; on first use.&lt;/p&gt;
    &lt;p&gt;Q: Does &lt;code&gt;lazy from module import Class&lt;/code&gt; load the entire module or just
the class?&lt;/p&gt;
    &lt;p&gt;A: It loads the entire module, not just the class. This is because Python’s import system always executes the complete module file – there’s no mechanism to execute only part of a &lt;code&gt;.py&lt;/code&gt; file. When you first access
&lt;code&gt;Class&lt;/code&gt;, Python:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Loads and executes the entire &lt;code&gt;module.py&lt;/code&gt;file&lt;/item&gt;
      &lt;item&gt;Extracts the &lt;code&gt;Class&lt;/code&gt;attribute from the resulting module object&lt;/item&gt;
      &lt;item&gt;Binds &lt;code&gt;Class&lt;/code&gt;to the name in your namespace&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is identical to eager &lt;code&gt;from module import Class&lt;/code&gt; behavior. The only
difference with lazy imports is that steps 1-3 happen on first use instead of
at the import statement.&lt;/p&gt;
    &lt;code&gt;# heavy_module.py
print("Loading heavy_module")  # This ALWAYS runs when module loads

class MyClass:
    pass

class UnusedClass:
    pass  # Also gets defined, even though we don't import it

# app.py
lazy from heavy_module import MyClass

print("Import statement done")  # heavy_module not loaded yet
obj = MyClass()                  # NOW "Loading heavy_module" prints
                                 # (and UnusedClass gets defined too)
&lt;/code&gt;
    &lt;p&gt;Key point: Lazy imports defer when a module loads, not what gets loaded. You cannot selectively load only parts of a module – Python’s import system doesn’t support partial module execution.&lt;/p&gt;
    &lt;p&gt;Q: What about type annotations and &lt;code&gt;TYPE_CHECKING&lt;/code&gt; imports?&lt;/p&gt;
    &lt;p&gt;A: Lazy imports eliminate the common need for &lt;code&gt;TYPE_CHECKING&lt;/code&gt; guards. You
can write:&lt;/p&gt;
    &lt;code&gt;lazy from collections.abc import Sequence, Mapping  # No runtime cost

def process(items: Sequence[str]) -&amp;gt; Mapping[str, int]:
    ...
&lt;/code&gt;
    &lt;p&gt;Instead of:&lt;/p&gt;
    &lt;code&gt;from typing import TYPE_CHECKING
if TYPE_CHECKING:
    from collections.abc import Sequence, Mapping

def process(items: Sequence[str]) -&amp;gt; Mapping[str, int]:
    ...
&lt;/code&gt;
    &lt;p&gt;Q: What’s the performance overhead of lazy imports?&lt;/p&gt;
    &lt;p&gt;A: The overhead is minimal:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Zero overhead after first use thanks to the adaptive interpreter optimizing the slow path away.&lt;/item&gt;
      &lt;item&gt;Small one-time cost to create the proxy object.&lt;/item&gt;
      &lt;item&gt;Reification (first use) has the same cost as a regular import.&lt;/item&gt;
      &lt;item&gt;No ongoing performance penalty unlike &lt;code&gt;importlib.util.LazyLoader&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Benchmarking with the pyperformance suite shows the implementation is performance neutral when lazy imports are not used.&lt;/p&gt;
    &lt;p&gt;Q: Can I mix lazy and eager imports of the same module?&lt;/p&gt;
    &lt;p&gt;A: Yes. If module &lt;code&gt;foo&lt;/code&gt; is imported both lazily and eagerly in the same
program, the eager import takes precedence and both bindings resolve to the
same module object.&lt;/p&gt;
    &lt;p&gt;Q: How do I migrate existing code to use lazy imports?&lt;/p&gt;
    &lt;p&gt;A: Migration is incremental:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Identify slow-loading modules using profiling tools.&lt;/item&gt;
      &lt;item&gt;Add &lt;code&gt;lazy&lt;/code&gt;keyword to imports that aren’t needed immediately.&lt;/item&gt;
      &lt;item&gt;Test that side-effect timing changes don’t break functionality.&lt;/item&gt;
      &lt;item&gt;Use &lt;code&gt;__lazy_modules__&lt;/code&gt;for compatibility with older Python versions.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Q: What about star imports (&lt;code&gt;from module import *&lt;/code&gt;)?&lt;/p&gt;
    &lt;p&gt;A: Wild card (star) imports cannot be lazy - they remain eager. This is because the set of names being imported cannot be determined without loading the module. Using the &lt;code&gt;lazy&lt;/code&gt; keyword with star imports will be a syntax
error. If lazy imports are globally enabled, star imports will still be eager.&lt;/p&gt;
    &lt;p&gt;Q: How do lazy imports interact with import hooks and custom loaders?&lt;/p&gt;
    &lt;p&gt;A: Import hooks and loaders work normally. When a lazy object is first used, the standard import protocol runs, including any custom hooks or loaders that were in place at reification time.&lt;/p&gt;
    &lt;p&gt;Q: What happens in multi-threaded environments?&lt;/p&gt;
    &lt;p&gt;A: Lazy import reification is thread-safe. Only one thread will perform the actual import, and the binding is atomically updated. Other threads will see either the lazy proxy or the final resolved object.&lt;/p&gt;
    &lt;p&gt;Q: Can I force reification of a lazy import without using it?&lt;/p&gt;
    &lt;p&gt;A: Yes, accessing a module’s &lt;code&gt;__dict__&lt;/code&gt; will reify all lazy objects in that
module. Individual lazy objects can be resolved by calling their &lt;code&gt;get()&lt;/code&gt;
method.&lt;/p&gt;
    &lt;p&gt;Q: What’s the difference between &lt;code&gt;globals()&lt;/code&gt; and &lt;code&gt;mod.__dict__&lt;/code&gt; for lazy imports?&lt;/p&gt;
    &lt;p&gt;A: Calling &lt;code&gt;globals()&lt;/code&gt; returns the module’s dictionary without reifying lazy
imports – you’ll see lazy proxy objects when accessing them through the
returned dictionary. However, accessing &lt;code&gt;mod.__dict__&lt;/code&gt; from external code
reifies all lazy imports in that module first. This design ensures:&lt;/p&gt;
    &lt;code&gt;# In your module:
lazy import json

g = globals()
print(type(g['json']))  # &amp;lt;class 'lazy_import'&amp;gt; - your problem

# From external code:
import sys
mod = sys.modules['your_module']
d = mod.__dict__
print(type(d['json']))  # &amp;lt;class 'module'&amp;gt; - reified for external access
&lt;/code&gt;
    &lt;p&gt;This distinction means adding lazy imports and calling &lt;code&gt;globals()&lt;/code&gt; is your
responsibility to manage, while external code accessing &lt;code&gt;mod.__dict__&lt;/code&gt;
always sees fully loaded modules.&lt;/p&gt;
    &lt;p&gt;Q: Why not use &lt;code&gt;importlib.util.LazyLoader&lt;/code&gt; instead?&lt;/p&gt;
    &lt;p&gt;A: &lt;code&gt;LazyLoader&lt;/code&gt; has significant limitations:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Requires verbose setup code for each lazy import.&lt;/item&gt;
      &lt;item&gt;Has ongoing performance overhead on every attribute access.&lt;/item&gt;
      &lt;item&gt;Doesn’t work well with &lt;code&gt;from ... import&lt;/code&gt;statements.&lt;/item&gt;
      &lt;item&gt;Less clear and standard than dedicated syntax.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Q: Will this break tools like &lt;code&gt;isort&lt;/code&gt; or &lt;code&gt;black&lt;/code&gt;?&lt;/p&gt;
    &lt;p&gt;A: Tools will need updates to recognize the &lt;code&gt;lazy&lt;/code&gt; keyword, but the changes
should be minimal since the import structure remains the same. The keyword
appears at the beginning, making it easy to parse.&lt;/p&gt;
    &lt;p&gt;Q: How do I know if a library is compatible with lazy imports?&lt;/p&gt;
    &lt;p&gt;A: Most libraries should work fine with lazy imports. Libraries that might have issues:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Those with essential import-time side effects (registration, monkey-patching).&lt;/item&gt;
      &lt;item&gt;Those that expect specific import ordering.&lt;/item&gt;
      &lt;item&gt;Those that modify global state during import.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When in doubt, test lazy imports with your specific use cases.&lt;/p&gt;
    &lt;p&gt;Q: What happens if I globally enable lazy imports mode and a library doesn’t work correctly?&lt;/p&gt;
    &lt;p&gt;A: Note: This is an advanced feature. You can use the lazy imports filter to exclude specific modules that are known to have problematic side effects:&lt;/p&gt;
    &lt;code&gt;import sys

def my_filter(importer, name, fromlist):
    # Don't lazily import modules known to have side effects
    if name in {'problematic_module', 'another_module'}:
        return False  # Import eagerly
    return True  # Allow lazy import

sys.set_lazy_imports_filter(my_filter)
&lt;/code&gt;
    &lt;p&gt;The filter function receives the importer module name, the module being imported, and the fromlist (if using &lt;code&gt;from ... import&lt;/code&gt;). Returning &lt;code&gt;False&lt;/code&gt;
forces an eager import.&lt;/p&gt;
    &lt;p&gt;Alternatively, set the global mode to &lt;code&gt;"disabled"&lt;/code&gt; via &lt;code&gt;-X
lazy_imports=disabled&lt;/code&gt; to turn off all lazy imports for debugging.&lt;/p&gt;
    &lt;p&gt;Q: Can I use lazy imports inside functions?&lt;/p&gt;
    &lt;p&gt;A: No, the &lt;code&gt;lazy&lt;/code&gt; keyword is only allowed at module level. For
function-level lazy loading, use traditional inline imports or move the import
to module level with &lt;code&gt;lazy&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Q: What about forwards compatibility with older Python versions?&lt;/p&gt;
    &lt;p&gt;A: Use the &lt;code&gt;__lazy_modules__&lt;/code&gt; global for compatibility:&lt;/p&gt;
    &lt;code&gt;# Works on Python 3.15+ as lazy, eager on older versions
__lazy_modules__ = ['expensive_module', 'expensive_module_2']
import expensive_module
from expensive_module_2 import MyClass
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;__lazy_modules__&lt;/code&gt; attribute is a list of module name strings. When
an import statement is executed, Python checks if the module name being
imported appears in &lt;code&gt;__lazy_modules__&lt;/code&gt;. If it does, the import is
treated as if it had the &lt;code&gt;lazy&lt;/code&gt; keyword (becoming potentially lazy). On
Python versions before 3.15 that don’t support lazy imports, the
&lt;code&gt;__lazy_modules__&lt;/code&gt; attribute is simply ignored and imports proceed
eagerly as normal.&lt;/p&gt;
    &lt;p&gt;This provides a migration path until you can rely on the &lt;code&gt;lazy&lt;/code&gt; keyword. For
maximum predictability, it’s recommended to define &lt;code&gt;__lazy_modules__&lt;/code&gt;
once, before any imports. But as it is checked on each import, it can be
modified between &lt;code&gt;import&lt;/code&gt; statements.&lt;/p&gt;
    &lt;p&gt;Q: How do explicit lazy imports interact with PEP-649/PEP-749&lt;/p&gt;
    &lt;p&gt;A: If an annotation is not stringified, it is an expression that is evaluated at a later time. It will only be resolved if the annotation is accessed. In the example below, the &lt;code&gt;fake_typing&lt;/code&gt; module is only loaded when the user
inspects the &lt;code&gt;__annotations__&lt;/code&gt; dictionary. The &lt;code&gt;fake_typing&lt;/code&gt; module would
also be loaded if the user uses &lt;code&gt;annotationlib.get_annotations()&lt;/code&gt; or
&lt;code&gt;getattr&lt;/code&gt; to access the annotations.&lt;/p&gt;
    &lt;code&gt;lazy from fake_typing import MyFakeType
def foo(x: MyFakeType):
  pass
print(foo.__annotations__)  # Triggers loading the fake_typing module
&lt;/code&gt;
    &lt;p&gt;Q: How do lazy imports interact with &lt;code&gt;dir()&lt;/code&gt;, &lt;code&gt;getattr()&lt;/code&gt;, and
module introspection?&lt;/p&gt;
    &lt;p&gt;A: Accessing lazy imports through normal attribute access or &lt;code&gt;getattr()&lt;/code&gt;
will trigger reification. Calling &lt;code&gt;dir()&lt;/code&gt; on a module will reify all lazy
imports in that module to ensure the directory listing is complete. This is
similar to accessing &lt;code&gt;mod.__dict__&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;lazy import json

# Before any access
# json not in sys.modules

# Any of these trigger reification:
dumps_func = json.dumps
dumps_func = getattr(json, 'dumps')
dir(json)
# Now json is in sys.modules
&lt;/code&gt;
    &lt;p&gt;Q: Do lazy imports work with circular imports?&lt;/p&gt;
    &lt;p&gt;A: Lazy imports don’t automatically solve circular import problems. If two modules have a circular dependency, making the imports lazy might help only if the circular reference isn’t accessed during module initialization. However, if either module accesses the other during import time, you’ll still get an error.&lt;/p&gt;
    &lt;p&gt;Example that works (deferred access in functions):&lt;/p&gt;
    &lt;code&gt;# user_model.py
lazy import post_model

class User:
    def get_posts(self):
        # OK - post_model accessed inside function, not during import
        return post_model.Post.get_by_user(self.name)

# post_model.py
lazy import user_model

class Post:
    @staticmethod
    def get_by_user(username):
        return f"Posts by {username}"
&lt;/code&gt;
    &lt;p&gt;This works because neither module accesses the other at module level – the access happens later when &lt;code&gt;get_posts()&lt;/code&gt; is called.&lt;/p&gt;
    &lt;p&gt;Example that fails (access during import):&lt;/p&gt;
    &lt;code&gt;# module_a.py
lazy import module_b

result = module_b.get_value()  # Error! Accessing during import

def func():
    return "A"

# module_b.py
lazy import module_a

result = module_a.func()  # Circular dependency error here

def get_value():
    return "B"
&lt;/code&gt;
    &lt;p&gt;This fails because &lt;code&gt;module_a&lt;/code&gt; tries to access &lt;code&gt;module_b&lt;/code&gt; at import time,
which then tries to access &lt;code&gt;module_a&lt;/code&gt; before it’s fully initialized.&lt;/p&gt;
    &lt;p&gt;The best practice is still to avoid circular imports in your code design.&lt;/p&gt;
    &lt;p&gt;Q: Will lazy imports affect the performance of my hot paths?&lt;/p&gt;
    &lt;p&gt;A: After first use, lazy imports have zero overhead thanks to the adaptive interpreter. The interpreter specializes the bytecode (e.g., &lt;code&gt;LOAD_GLOBAL&lt;/code&gt;
becomes &lt;code&gt;LOAD_GLOBAL_MODULE&lt;/code&gt;) which eliminates the lazy check on subsequent
accesses. This means once a lazy import is reified, accessing it is just as
fast as a normal import.&lt;/p&gt;
    &lt;code&gt;lazy import json

def use_json():
    return json.dumps({"test": 1})

# First call triggers reification
use_json()

# After 2-3 calls, bytecode is specialized
use_json()
use_json()
&lt;/code&gt;
    &lt;p&gt;You can observe the specialization using &lt;code&gt;dis.dis(use_json, adaptive=True)&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;=== Before specialization ===
LOAD_GLOBAL              0 (json)
LOAD_ATTR                2 (dumps)

=== After 3 calls (specialized) ===
LOAD_GLOBAL_MODULE       0 (json)
LOAD_ATTR_MODULE         2 (dumps)
&lt;/code&gt;
    &lt;p&gt;The specialized &lt;code&gt;LOAD_GLOBAL_MODULE&lt;/code&gt; and &lt;code&gt;LOAD_ATTR_MODULE&lt;/code&gt; instructions
are optimized fast paths with no overhead for checking lazy imports.&lt;/p&gt;
    &lt;p&gt;Q: What about &lt;code&gt;sys.modules&lt;/code&gt;? When does a lazy import appear there?&lt;/p&gt;
    &lt;p&gt;A: A lazily imported module does not appear in &lt;code&gt;sys.modules&lt;/code&gt; until it’s
reified (first used). Once reified, it appears in &lt;code&gt;sys.modules&lt;/code&gt; just like
any eager import.&lt;/p&gt;
    &lt;code&gt;import sys
lazy import json

print('json' in sys.modules)  # False

result = json.dumps({"key": "value"})  # First use

print('json' in sys.modules)  # True
&lt;/code&gt;
    &lt;p&gt;Q: Why you chose ``lazy`` as the keyword name?&lt;/p&gt;
    &lt;p&gt;A: Not “why”… memorize! :)&lt;/p&gt;
    &lt;head rend="h2"&gt;Alternate Implementation Ideas&lt;/head&gt;
    &lt;p&gt;Here are some alternative design decisions that were considered during the development of this PEP. While the current proposal represents what we believe to be the best balance of simplicity, performance, and maintainability, these alternatives offer different trade-offs that may be valuable for implementers to consider or for future refinements.&lt;/p&gt;
    &lt;head rend="h3"&gt;Leveraging a subclass of dict&lt;/head&gt;
    &lt;p&gt;Instead of updating the internal dict object to directly add the fields needed to support lazy imports, we could create a subclass of the dict object to be used specifically for Lazy Import enablement. This would still be a leaky abstraction though - methods can be called directly such as &lt;code&gt;dict.__getitem__&lt;/code&gt; and it would impact the performance of globals lookup in
the interpreter.&lt;/p&gt;
    &lt;head rend="h3"&gt;Alternate keyword names&lt;/head&gt;
    &lt;p&gt;For this PEP, we decided to propose &lt;code&gt;lazy&lt;/code&gt; for the explicit keyword as it
felt the most familar to those already focused on optimizing import overhead.
We also considered a variety of other options to support explicit lazy
imports. The most compelling alternates were &lt;code&gt;defer&lt;/code&gt; and &lt;code&gt;delay&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;Rejected Ideas&lt;/head&gt;
    &lt;head rend="h3"&gt;Modification of the dict object&lt;/head&gt;
    &lt;p&gt;The initial PEP for lazy imports (PEP 690) relied heavily on the modification of the internal dict object to support lazy imports. We recognize that this data structure is highly tuned, heavily used across the codebase, and very performance sensitive. Because of the importance of this data structure and the desire to keep the implementation of lazy imports encapsulated from users who may have no interest in the feature, we’ve decided to invest in an alternate approach.&lt;/p&gt;
    &lt;p&gt;The dictionary is the foundational data structure in Python. Every object’s attributes are stored in a dict, and dicts are used throughout the runtime for namespaces, keyword arguments, and more. Adding any kind of hook or special behavior to dicts to support lazy imports would:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Prevent critical interpreter optimizations including future JIT compilation.&lt;/item&gt;
      &lt;item&gt;Add complexity to a data structure that must remain simple and fast.&lt;/item&gt;
      &lt;item&gt;Affect every part of Python, not just import behavior.&lt;/item&gt;
      &lt;item&gt;Violate separation of concerns – the hash table shouldn’t know about the import system.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Past decisions that violated this principle of keeping core abstractions clean have caused significant pain in the CPython ecosystem, making optimization difficult and introducing subtle bugs.&lt;/p&gt;
    &lt;head rend="h3"&gt;Placing the &lt;code&gt;lazy&lt;/code&gt; keyword in the middle of from imports&lt;/head&gt;
    &lt;p&gt;While we found &lt;code&gt;from foo lazy import bar&lt;/code&gt; to be a really intuitive placement
for the new explicit syntax, we quickly learned that placing the &lt;code&gt;lazy&lt;/code&gt;
keyword here is already syntactically allowed in Python. This is because
&lt;code&gt;from . lazy import bar&lt;/code&gt; is legal syntax (because whitespace does not
matter.)&lt;/p&gt;
    &lt;head rend="h3"&gt;Placing the &lt;code&gt;lazy&lt;/code&gt; keyword at the end of import statements&lt;/head&gt;
    &lt;p&gt;We discussed appending lazy to the end of import statements like such &lt;code&gt;import
foo lazy&lt;/code&gt; or &lt;code&gt;from foo import bar, baz lazy&lt;/code&gt; but ultimately decided that
this approach provided less clarity. For example, if multiple modules are
imported in a single statement, it is unclear if the lazy binding applies to
all of the imported objects or just a subset of the items.&lt;/p&gt;
    &lt;head rend="h3"&gt;Returning a proxy dict from &lt;code&gt;globals()&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;An alternative to reifying on &lt;code&gt;globals()&lt;/code&gt; or exposing lazy objects would be
to return a proxy dictionary that automatically reifies lazy objects when
they’re accessed through the proxy. This would seemingly give the best of both
worlds: &lt;code&gt;globals()&lt;/code&gt; returns immediately without reification cost, but
accessing items through the result would automatically resolve lazy imports.&lt;/p&gt;
    &lt;p&gt;However, this approach is fundamentally incompatible with how &lt;code&gt;globals()&lt;/code&gt; is
used in practice. Many standard library functions and built-ins expect
&lt;code&gt;globals()&lt;/code&gt; to return a real &lt;code&gt;dict&lt;/code&gt; object, not a proxy:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;exec(code, globals())&lt;/code&gt;requires a real dict.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;eval(expr, globals())&lt;/code&gt;requires a real dict.&lt;/item&gt;
      &lt;item&gt;Functions that check &lt;code&gt;type(globals()) is dict&lt;/code&gt;would break.&lt;/item&gt;
      &lt;item&gt;Dictionary methods like &lt;code&gt;.update()&lt;/code&gt;would need special handling.&lt;/item&gt;
      &lt;item&gt;Performance would suffer from the indirection on every access.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The proxy would need to be so transparent that it would be indistinguishable from a real dict in almost all cases, which is extremely difficult to achieve correctly. Any deviation from true dict behavior would be a source of subtle bugs.&lt;/p&gt;
    &lt;head rend="h3"&gt;Reifying lazy imports when &lt;code&gt;globals()&lt;/code&gt; is called&lt;/head&gt;
    &lt;p&gt;Calling &lt;code&gt;globals()&lt;/code&gt; returns the module’s namespace dictionary without
triggering reification of lazy imports. Accessing lazy objects through the
returned dictionary yields the lazy proxy objects themselves. This is an
intentional design decision for several reasons:&lt;/p&gt;
    &lt;p&gt;The key distinction: Adding a lazy import and calling &lt;code&gt;globals()&lt;/code&gt; is the
module author’s concern and under their control. However, accessing
&lt;code&gt;mod.__dict__&lt;/code&gt; from external code is a different scenario – it crosses
module boundaries and affects someone else’s code. Therefore, &lt;code&gt;mod.__dict__&lt;/code&gt;
access reifies all lazy imports to ensure external code sees fully realized
modules, while &lt;code&gt;globals()&lt;/code&gt; preserves lazy objects for the module’s own
introspection needs.&lt;/p&gt;
    &lt;p&gt;Technical challenges: It is impossible to safely reify on-demand when &lt;code&gt;globals()&lt;/code&gt; is called because we cannot return a proxy dictionary – this
would break common usages like passing the result to &lt;code&gt;exec()&lt;/code&gt; or other
built-ins that expect a real dictionary. The only alternative would be to
eagerly reify all lazy imports whenever &lt;code&gt;globals()&lt;/code&gt; is called, but this
behavior would be surprising and potentially expensive.&lt;/p&gt;
    &lt;p&gt;Performance concerns: It is impractical to cache whether a reification scan has been performed with just the globals dictionary reference, whereas module attribute access (the primary use case) can efficiently cache reification state in the module object itself.&lt;/p&gt;
    &lt;p&gt;Use case rationale: The chosen design makes sense precisely because of this distinction: adding a lazy import and calling &lt;code&gt;globals()&lt;/code&gt; is your
problem to manage, while having lazy imports visible in &lt;code&gt;mod.__dict__&lt;/code&gt;
becomes someone else’s problem. By reifying on &lt;code&gt;__dict__&lt;/code&gt; access but not on
&lt;code&gt;globals()&lt;/code&gt;, we ensure external code always sees fully loaded modules while
giving module authors control over their own introspection.&lt;/p&gt;
    &lt;p&gt;Note that three options were considered:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Calling &lt;code&gt;globals()&lt;/code&gt;or&lt;code&gt;mod.__dict__&lt;/code&gt;traverses and resolves all lazy objects before returning.&lt;/item&gt;
      &lt;item&gt;Calling &lt;code&gt;globals()&lt;/code&gt;or&lt;code&gt;mod.__dict__&lt;/code&gt;returns the dictionary with lazy objects present.&lt;/item&gt;
      &lt;item&gt;Calling &lt;code&gt;globals()&lt;/code&gt;returns the dictionary with lazy objects, but&lt;code&gt;mod.__dict__&lt;/code&gt;reifies everything.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We chose the third option because it properly delineates responsibility: if you add lazy imports to your module and call &lt;code&gt;globals()&lt;/code&gt;, you’re responsible
for handling the lazy objects. But external code accessing your module’s
&lt;code&gt;__dict__&lt;/code&gt; shouldn’t need to know about your lazy imports – it gets fully
resolved modules.&lt;/p&gt;
    &lt;head rend="h2"&gt;Acknowledgements&lt;/head&gt;
    &lt;p&gt;We would like to thank Paul Ganssle, Yury Selivanov, Łukasz Langa, Lysandros Nikolaou, Pradyun Gedam, Mark Shannon, Hana Joo and the Python Google team, the Python team(s) @ Meta, the Python @ HRT team, the Bloomberg Python team, the Scientific Python community, everyone who participated in the initial discussion of PEP 690, and many others who provided valuable feedback and insights that helped shape this PEP.&lt;/p&gt;
    &lt;head rend="h2"&gt;Footnotes&lt;/head&gt;
    &lt;head rend="h2"&gt;Copyright&lt;/head&gt;
    &lt;p&gt;This document is placed in the public domain or under the CC0-1.0-Universal license, whichever is more permissive.&lt;/p&gt;
    &lt;p&gt;Source: https://github.com/python/peps/blob/main/peps/pep-0810.rst&lt;/p&gt;
    &lt;p&gt;Last modified: 2025-10-03 20:29:13 GMT&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45466086</guid><pubDate>Fri, 03 Oct 2025 18:24:58 +0000</pubDate></item><item><title>Jules, remote coding agent from Google Labs, announces API</title><link>https://jules.google/docs/changelog/</link><description>&lt;doc fingerprint="2f246eb1ac650a9d"&gt;
  &lt;main&gt;
    &lt;p&gt;You can now programmatically access Julesâs capabilities to automate your work and build powerful integrations. The Jules API is designed to help you seamlessly integrate Jules into your existing development workflows, unlocking new ways to automate and enhance the entire software development lifecycle.&lt;/p&gt;
    &lt;p&gt;With the API, you can:&lt;/p&gt;
    &lt;p&gt;Create custom integrations with tools like Slack for âChatOpsâ workflows, allowing you to assign tasks directly from your chat client.&lt;/p&gt;
    &lt;p&gt;Automate bug fixing and feature implementation by connecting Jules to your project management tools like Linear or Jira.&lt;/p&gt;
    &lt;p&gt;Integrate Jules directly into your CI/CD pipelines in services like GitHub Actions.&lt;/p&gt;
    &lt;p&gt;Hereâs a quick example of how to create a new task (a âSessionâ) using a cURL command:&lt;/p&gt;
    &lt;p&gt;Weâre launching Jules Tools, a new command-line interface designed to give you direct control over your AI coding agent, making it scriptable, customizable, and easy to integrate into your existing workflows.&lt;/p&gt;
    &lt;p&gt;Key Features:&lt;/p&gt;
    &lt;p&gt;Direct Control: Create tasks (jules remote new), list active sessions (jules remote list), and monitor Jules without leaving your command line.&lt;/p&gt;
    &lt;p&gt;Apply Patches Locally: Instantly pull work-in-progress code from an active Jules session and apply it to your local machine. This lets you test changes immediately, without waiting for a commit to GitHub.&lt;/p&gt;
    &lt;p&gt;Scriptable &amp;amp; Composable: Integrate Jules into your automations by piping in output from other tools like gh, jq, or cat.&lt;/p&gt;
    &lt;p&gt;Interactive Dashboard: For a more guided experience, launch the built-in terminal user interface (TUI) to create and manage tasks step-by-step.&lt;/p&gt;
    &lt;p&gt;How to Install:&lt;/p&gt;
    &lt;p&gt;Install globally via npm: npm install -g @google/jules&lt;/p&gt;
    &lt;p&gt;Or run directly without a permanent installation: npx @google/jules&lt;/p&gt;
    &lt;p&gt;Starter Commands to Try:&lt;/p&gt;
    &lt;p&gt;See all available commands: jules help&lt;/p&gt;
    &lt;p&gt;List all repos connected to Jules: jules remote list --repo&lt;/p&gt;
    &lt;p&gt;Create a new task in a specific repo: jules remote new --repo torvalds/linux --session "write unit tests"&lt;/p&gt;
    &lt;p&gt;A Note for Google Workspace Users&lt;/p&gt;
    &lt;p&gt;Support for workspace users is coming later in October!&lt;/p&gt;
    &lt;p&gt;If you run into any issues, please share your experience with us via in-app feedback or on our Discord channel.&lt;/p&gt;
    &lt;p&gt;Jules gains memory!&lt;/p&gt;
    &lt;p&gt;September 30, 2025&lt;/p&gt;
    &lt;p&gt;Jules Memory for Repositories: Weâre excited to introduce a new Memory feature! Jules now has the ability to learn from your interactions.&lt;/p&gt;
    &lt;p&gt;How it works: During a task, Jules will save your preferences, nudges, and corrections.&lt;/p&gt;
    &lt;p&gt;The benefit: The next time you run the same or a similar task in that specific repository, Jules will reference its memory to better anticipate your needs and follow your established patterns, leading to more accurate results with less guidance.&lt;/p&gt;
    &lt;p&gt;Settings: You can toggle memory on or off for the repo in the repo settings page under âKnowledgeâ&lt;/p&gt;
    &lt;p&gt;Tell Jules exactly what file to work on using file selector&lt;/p&gt;
    &lt;p&gt;September 29, 2025&lt;/p&gt;
    &lt;p&gt;You can now tell Jules exactly which files to work with for any given task. Use the new file selector to easily and precisely reference specific files.&lt;/p&gt;
    &lt;p&gt;This removes ambiguity and gives you more granular control over Julesâs actions, helping to tighten the context for your task.&lt;/p&gt;
    &lt;p&gt;Jules Acts on PR Feedback&lt;/p&gt;
    &lt;p&gt;September 23, 2025&lt;/p&gt;
    &lt;p&gt;Jules is now able to read and respond to your comments on pull requests!&lt;/p&gt;
    &lt;p&gt;When you start a review, Jules will add a ð emoji to each comment to let you know itâs been read. Based on your feedback, Jules will then push a commit with the requested changes.&lt;/p&gt;
    &lt;p&gt;For more control, you can switch to Reactive Mode in your global Jules UI settings. In this mode, Jules will only act on comments where you specifically mention @Jules.&lt;/p&gt;
    &lt;p&gt;All Hands on Deck!&lt;/p&gt;
    &lt;p&gt;September 19, 2025&lt;/p&gt;
    &lt;p&gt;Ahoy, mateys! To celebrate International Talk Like a Pirate Day, weâve given Jules a temporary map to the treasure.&lt;/p&gt;
    &lt;p&gt;Jules Speaks Pirate: Youâll find your AI agentâs responses are a bit moreâ¦ swashbucklingâ¦ for today only.&lt;/p&gt;
    &lt;p&gt;Same Great Logic: Fear not! Beneath the eyepatch and Jolly Roger, itâs the same powerful coding engine ready to help you plunder that backlog and send bugs to Davy Jonesâ locker.&lt;/p&gt;
    &lt;p&gt;Image upload&lt;/p&gt;
    &lt;p&gt;September 9, 2025&lt;/p&gt;
    &lt;p&gt;You can now upload images when creating a task in Jules. Use this to show frontend bugs, design inspiration, UI mocks, or any visual context you want Jules to consider while generating code.&lt;/p&gt;
    &lt;p&gt;For now:&lt;/p&gt;
    &lt;p&gt;Only JPEG and PNG formats are supported.&lt;/p&gt;
    &lt;p&gt;You can uplaod as many images as you want, as long as the total size is under 5MB.&lt;/p&gt;
    &lt;p&gt;Image upload is only supported at task creation (weâre working on enabling it for follow-up prompts soon).&lt;/p&gt;
    &lt;p&gt;Note: If your task involves using assets (e.g. logos) directly in code, those must still be committed to your GitHub repo.&lt;/p&gt;
    &lt;p&gt;To improve the code review experience, weâve introduced a new stacked layout for the diff viewer. This change displays diffs for multiple files vertically on a single screen. The stacked view makes it easier to see related changes across your codebase at a glance, providing better context and speeding up your review process.&lt;/p&gt;
    &lt;p&gt;Changes:&lt;/p&gt;
    &lt;p&gt;The diff viewer now stacks file changes vertically by default&lt;/p&gt;
    &lt;p&gt;You can also toggle back to the previous tabbed diff viewer&lt;/p&gt;
    &lt;p&gt;Improved Jules Critic&lt;/p&gt;
    &lt;p&gt;September 3, 2025&lt;/p&gt;
    &lt;p&gt;Weâve shipped significant improvements to the Jules critic agent, making its feedback more insightful and reliable. To increase transparency and give you more insight into its evaluation process, you can now see the criticâs real-time analysis as it works.&lt;/p&gt;
    &lt;p&gt;Changes:&lt;/p&gt;
    &lt;p&gt;The criticâs thought process is now visible in the UI, showing its step-by-step evaluation of the code in real-time.&lt;/p&gt;
    &lt;p&gt;The criticâs now incorporates more contextual information when making decisions, leading to more accurate and relevant feedback on potential bugs and logic flaws.&lt;/p&gt;
    &lt;p&gt;Jules Sample Prompts&lt;/p&gt;
    &lt;p&gt;September 2, 2025&lt;/p&gt;
    &lt;p&gt;To help new users get started with Jules, weâve added sample prompts to the home page. These static prompts provide examples of how to use Jules and can be added to the text box with a single click.&lt;/p&gt;
    &lt;p&gt;Changes:&lt;/p&gt;
    &lt;p&gt;Sample prompts are now displayed on the home page for all users.&lt;/p&gt;
    &lt;p&gt;Clicking on a sample prompt will add the text of the prompt to the input box.&lt;/p&gt;
    &lt;p&gt;Render images in the diff viewer&lt;/p&gt;
    &lt;p&gt;August 22, 2025&lt;/p&gt;
    &lt;p&gt;Jules now intelligently renders images within the diff viewer, providing an immediate visual context for your modifications.&lt;/p&gt;
    &lt;p&gt;This means:&lt;/p&gt;
    &lt;p&gt;Instant Visual Feedback: When Jules generates images (like charts, diagrams, or web UI screenshots), youâll see the actual image in the diff, not just its code representation.&lt;/p&gt;
    &lt;p&gt;Streamlined Workflow: No need to switch between tools or download files to see the results. Jules keeps everything in one place.&lt;/p&gt;
    &lt;p&gt;Try it out! Ask Jules to render an output, like a graph based on data, and commit it to your repository. Youâll be able to see the generated image seamlessly within your diff viewer.&lt;/p&gt;
    &lt;p&gt;Export at any time&lt;/p&gt;
    &lt;p&gt;August 15, 2025&lt;/p&gt;
    &lt;p&gt;Youâre now in full control of when your code gets to GitHub. No need to wait for a task to finish or ask Jules to do it for you. At any point during a task, just click the GitHub icon in the top right to publish the current work-in-progress as a new branch or open a pull request. This gives you more flexibility and control to review, test, or take over whenever youâre ready.&lt;/p&gt;
    &lt;p&gt;Increasing the VM Size to 20GB&lt;/p&gt;
    &lt;p&gt;August 15, 2025&lt;/p&gt;
    &lt;p&gt;We heard your feedback about running into disk space limits on larger projects. To address this, weâve significantly increased the available disk space in the Jules VM to 20GB. This provides more room for large dependencies, build artifacts, and complex repositories, reducing disk-related failures so Jules can tackle bigger tasks. Happy Julesing!&lt;/p&gt;
    &lt;p&gt;Critic Agent&lt;/p&gt;
    &lt;p&gt;August 8, 2025&lt;/p&gt;
    &lt;p&gt;Great developers donât just write code, they question it. And now, so does Jules. Weâve built the Jules critic agent to ensure that every line of code isnât just functional, but robust, secure, and efficient. It acts as an internal peer reviewer, challenging every proposed change to elevate the quality of the final output.&lt;/p&gt;
    &lt;p&gt;Some high level notes:&lt;/p&gt;
    &lt;p&gt;Critic-augmented generation: The Jules critic is integrated directly into the generation process. Every proposed change undergoes adversarial review before completion.&lt;/p&gt;
    &lt;p&gt;Improved code quality: The critic flags subtle bugs, missed edge cases, and inefficient code. Jules then uses this feedback to improve the patch in real-time.&lt;/p&gt;
    &lt;p&gt;A new kind of review: The critic is not just another linter or test. It understands the intent and context behind code, similar to a human peer reviewer.&lt;/p&gt;
    &lt;p&gt;Built on research: This feature draws on research into multi-step, tool interactive critiquing and actor-critic reinforcement learning, where an âactorâ generates and a âcriticâ evaluates.&lt;/p&gt;
    &lt;p&gt;Interactive Plan&lt;/p&gt;
    &lt;p&gt;August 8, 2025&lt;/p&gt;
    &lt;p&gt;Meet Interactive Plan. Instead of jumping straight to the solution, Jules will now read your codebase, ask clarifying questions, and work with you to refine the plan. This collaborative approach gives you more control and ensures youâre on the same page, leading to higher-quality code and a more reliable solution.&lt;/p&gt;
    &lt;p&gt;In summary:&lt;/p&gt;
    &lt;p&gt;Trigger the interactive plan from the dropdown when you start a task&lt;/p&gt;
    &lt;p&gt;Jules will start a brainstorm with you and ask clarifying questions&lt;/p&gt;
    &lt;p&gt;Jules can surf the web&lt;/p&gt;
    &lt;p&gt;August 8, 2025&lt;/p&gt;
    &lt;p&gt;Jules can now proactively search the web for relevant content, documentation, or code snippets to help complete your tasks. This means Jules can get the information it needs, resulting in more accurate and successful task completion.&lt;/p&gt;
    &lt;p&gt;In Summary:&lt;/p&gt;
    &lt;p&gt;Jules can find the latest documentation for dependencies/libraries youâre using&lt;/p&gt;
    &lt;p&gt;Jules can proactively find examples or code snippets that can help inform its implementation&lt;/p&gt;
    &lt;p&gt;Note: web search works best when working on technical documentation. Queries like: âWhat is the latest news today?â are not supported.&lt;/p&gt;
    &lt;p&gt;Jules can test web-apps and show you the results&lt;/p&gt;
    &lt;p&gt;August 7, 2025&lt;/p&gt;
    &lt;p&gt;Next time you are working on a front end project with Jules, ask it to verify its work and itâll render the website and send you back a screenshot!&lt;/p&gt;
    &lt;p&gt;Ask Jules to complete a web development task and to verify the front end&lt;/p&gt;
    &lt;p&gt;Jules will send you a screenshot of the front end along with any code changes&lt;/p&gt;
    &lt;p&gt;The default Jules base image now includes Playwright for front end testing&lt;/p&gt;
    &lt;p&gt;Users can also add images in the form of public URLs for Jules to use as input&lt;/p&gt;
    &lt;p&gt;Jules is out of beta!&lt;/p&gt;
    &lt;p&gt;August 6, 2025&lt;/p&gt;
    &lt;p&gt;Today we are thrilled to announce that Jules is no longer in beta! Since launch just two months ago, Jules has passed over 140k public commits. Thank you to our amazing beta users for all your support and feedback.&lt;/p&gt;
    &lt;p&gt;In addition, weâre launching our pricing plans to unlock higher task limits, along with a bunch of quality improvements in the Jules app and agent. Here are the details:&lt;/p&gt;
    &lt;p&gt;Get higher task limits through the Google AI Pro and Ultra plans. More details at Limits and Plans.&lt;/p&gt;
    &lt;p&gt;Jules now uses the power of Gemini 2.5 thinking when creating its plan, resulting in higher quality plans and more complete tasks&lt;/p&gt;
    &lt;p&gt;Numerous bug fixes so Jules gets stuck less, and is better at following your instructions in agents.md&lt;/p&gt;
    &lt;p&gt;Environment snapshots for faster tasks&lt;/p&gt;
    &lt;p&gt;August 5, 2025&lt;/p&gt;
    &lt;p&gt;Jules now creates a snapshot of your environment when you add environment setup scripts. For complicated environment, users should see faster and more consistent task execution.&lt;/p&gt;
    &lt;p&gt;In summary:&lt;/p&gt;
    &lt;p&gt;Jules will now snapshot your environment when you provide an environment setup script&lt;/p&gt;
    &lt;p&gt;Snapshots are loaded automatically next time you run a task&lt;/p&gt;
    &lt;p&gt;This provides for faster task startups, especially for complex environments&lt;/p&gt;
    &lt;p&gt;You can find environment configuration by clicking the âcodebaseâ in the left hand panel, or by clicking the âconfigure environmentâ button in the task pane.&lt;/p&gt;
    &lt;p&gt;Open A PR directly from Jules&lt;/p&gt;
    &lt;p&gt;August 4, 2025&lt;/p&gt;
    &lt;p&gt;Closing the loop from task to merge ð¤&lt;/p&gt;
    &lt;p&gt;Jules can now open a pull request directly from the UI. After a task completes, just use the new dropdown next to the âPublish Branchâ button to open a PR. Jules will request to merge the newly published branch into main, streamlining your entire workflow. Less context switching, faster merging.&lt;/p&gt;
    &lt;p&gt;Added Bun runtime support&lt;/p&gt;
    &lt;p&gt;July 18, 2025&lt;/p&gt;
    &lt;p&gt;Jules now supports Bun. You can run tasks using Bun out of the box, no extra setup required. This expands compatibility for projects that use Bun instead of Node.&lt;/p&gt;
    &lt;p&gt;Read more about the jules base image and what tooling works with Jules.&lt;/p&gt;
    &lt;p&gt;Improved task controls and other ð UI delight&lt;/p&gt;
    &lt;p&gt;July 3, 2025&lt;/p&gt;
    &lt;p&gt;Pause, resume, and delete tasksâwithout losing your sense of place. Available from sidebar and repo view. You can even quickly copy task urls!&lt;/p&gt;
    &lt;p&gt;Non-urgent task icons are now more recessive&lt;/p&gt;
    &lt;p&gt;Certain hover statesâwhich did not look goodâhave been toned back.&lt;/p&gt;
    &lt;p&gt;System messages have more consistent padding and borders&lt;/p&gt;
    &lt;p&gt;Performance upgrades: Enjoy a smoother, faster Jules experience with recent under-the-hood improvements.&lt;/p&gt;
    &lt;p&gt;Quickly copy and download code: New copy and download buttons are now available in the code view pane, making it easier to grab your code directly from Jules.&lt;/p&gt;
    &lt;p&gt;Stay focused with task modals: Initiate multiple tasks seamlessly through a new modal option, allowing you to keep your context and workflow intact. Learn more about kicking off tasks.&lt;/p&gt;
    &lt;p&gt;Adjustable code panel: Customize your workspace by adjusting the width of the code panel to your preferred viewing experience.&lt;/p&gt;
    &lt;p&gt;Check out the docs to learn more about how to download code that Jules writes.&lt;/p&gt;
    &lt;p&gt;A faster, smoother and more reliable Jules&lt;/p&gt;
    &lt;p&gt;May 30, 2025&lt;/p&gt;
    &lt;p&gt;This week, our focus has been on improving reliability, fixing our GitHub integration, and scaling capacity.&lt;/p&gt;
    &lt;p&gt;Hereâs whatâs we shipped:&lt;/p&gt;
    &lt;p&gt;Updated our limits to 60 tasks per day, 5 concurrent.&lt;/p&gt;
    &lt;p&gt;We substantially improved the reliability of the GitHub sync. Export to GitHub should also be fixed on previously created tasks.&lt;/p&gt;
    &lt;p&gt;Weâve decreased the number of failure cases by 2/3&lt;/p&gt;
    &lt;p&gt;Weâve been heads down improving stability and fixing bugsâbig and smallâto make Jules faster, smoother, and more reliable for you.&lt;/p&gt;
    &lt;p&gt;Hereâs whatâs fixed:&lt;/p&gt;
    &lt;p&gt;Upgraded our queuing system and added more compute to reduce wait times during peak usage&lt;/p&gt;
    &lt;p&gt;Publish Branch button is now part of the summary UI in the activity feed so itâs easier to find&lt;/p&gt;
    &lt;p&gt;Bug vixes for task status and mobile&lt;/p&gt;
    &lt;p&gt;Learn more about how to publish a branch on GitHub.&lt;/p&gt;
    &lt;p&gt;Jules is here&lt;/p&gt;
    &lt;p&gt;May 19, 2025&lt;/p&gt;
    &lt;p&gt;Today, weâre launching Jules, a new AI coding agent.&lt;/p&gt;
    &lt;p&gt;Jules helps you move faster by working asynchronously on tasks in your GitHub repo. It can fix bugs, update dependencies, migrate code, and add new features.&lt;/p&gt;
    &lt;p&gt;Once you give Jules a task, it spins up a fresh dev environment in a VM, installs dependencies, writes tests, makes the changes, runs the tests, and opens a pull request. Jules shows its work as it makes progress, so you never have to guess what code itâs writing, or what itâs thinking.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45466588</guid><pubDate>Fri, 03 Oct 2025 19:08:11 +0000</pubDate></item><item><title>Procedural Generation with Wave Function Collapse</title><link>https://www.gridbugs.org/wave-function-collapse/</link><description>&lt;doc fingerprint="209cca5d0f320be1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Procedural Generation with Wave Function Collapse&lt;/head&gt;
    &lt;p&gt;(Edit 2022-05-03: I found out that the Wave Function Collapse algorithm was heavily inspired by an existing algorithm called “Model Synthesis”. I’ve added a section to further reading with links to the author’s website for more information.)&lt;/p&gt;
    &lt;p&gt;Wave Function Collapse is a procedural generation algorithm which produces images by arranging a collection of tiles according to rules about which tiles may be adjacent to each other tile, and relatively how frequently each tile should appear. The algorithm maintains, for each pixel of the output image, a probability distribution of the tiles which may be placed there. It repeatedly chooses a pixel to “collapse” - choosing a tile to use for that pixel based on its distribution. WFC gets its name from quantum physics.&lt;/p&gt;
    &lt;p&gt;The goal of this post is to build an intuition for how and why the WFC algorithm works.&lt;/p&gt;
    &lt;p&gt;I will break WFC into two separate algorithms and explain them separately. Each is interesting in its own right, and the interface between them is simple.&lt;/p&gt;
    &lt;head rend="h2"&gt;Core Interface&lt;/head&gt;
    &lt;code&gt;fn wfc_core(
    adjacency_rules: AdjacencyRules,
    frequency_rules: FrequencyHints,
    output_size: (u32, u32),
) -&amp;gt; Grid2D&amp;lt;TileIndex&amp;gt; { ... }
&lt;/code&gt;
    &lt;p&gt;This is the low-level part of the algorithm which solves the problem of arranging tiles into a grid according to some specified rules. I’ll give a “black box” description of the core here, and explain how it works internally below.&lt;/p&gt;
    &lt;head rend="h3"&gt;Adjacency Rules&lt;/head&gt;
    &lt;p&gt;The “core” receives a set of adjacency rules describing which tiles may appear next to other tiles in each cardinal direction. Some example rules are “Tile 6 may appear in the cell ABOVE a cell containing tile 4”, and “Tile 7 map appear in the cell to the LEFT of a cell containing tile 3.&lt;/p&gt;
    &lt;head rend="h3"&gt;Frequency Hints&lt;/head&gt;
    &lt;p&gt;It also receives a set of frequency hints, which is a mapping from each tile to a number indicating how frequently the tile should appear in the output, relative to other tiles. If tile 4 maps to 6, and tile 5 maps to 2, then tile 4 should appear 3 times as frequently than tile 5.&lt;/p&gt;
    &lt;head rend="h3"&gt;Tile Index&lt;/head&gt;
    &lt;p&gt;The core doesn’t actually get to see the tiles themselves. Rather, tiles are referred to by integers ranging from 0 to the number of tiles minus 1, which I’ll refer to as tile indices. The adjacency rules and frequency hints are all specified in terms of tile index.&lt;/p&gt;
    &lt;head rend="h3"&gt;Output&lt;/head&gt;
    &lt;p&gt;The algorithm populates a grid with tile indices in a way which completely respects adjacency rules, and probabilistically respects frequency hints. Every pair of adjacent tiles will be explicitly allowed by the adjacency rules, and the relative frequencies of tiles in the output will usually be about the same as in the frequency hints.&lt;/p&gt;
    &lt;head rend="h2"&gt;Image Processor&lt;/head&gt;
    &lt;p&gt;This is the “glue” between the core algorithm, and an input and output image. Typically WFC is used to generate output images which are “similar” to input images. There’s no requirement that the output image be the same dimensions as the input image. Specifically, similar means that for some tile size:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;every tile-sized square of pixels in the output image appears somewhere in the input image&lt;/item&gt;
      &lt;item&gt;the relative frequencies of tile-sized squares of pixels in the output image is roughly the same as in the input image&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In other words, the output image will have the same local features as the input image, but the global structure may be different.&lt;/p&gt;
    &lt;p&gt;Note that there are some alternative applications of WFC besides generating similar images, such as arranging hand-crafted tiles with user-specified adjacency rules and frequency hints. These applications would still use the same core algorithm, but the image processor would be different.&lt;/p&gt;
    &lt;head rend="h3"&gt;Pre Processing&lt;/head&gt;
    &lt;code&gt;fn wfc_pre_process_image(
    input_image: Image,
    tile_size: u32,     /* often 2 or 3 */
) -&amp;gt; (AdjacencyRules, FrequencyHints, HashMap&amp;lt;TileIndex, Colour&amp;gt;) { ... }
&lt;/code&gt;
    &lt;p&gt;The goal of this step is to generate adjacency rules and frequency hints which will be passed as input to the core algorithm.&lt;/p&gt;
    &lt;p&gt;First, we need to know what the different tiles are. Given an input image and tile size, enumerate all the tile-sized squares of pixels from the input image, including those squares whose top-left pixel occurs within tile size of the right and bottom edges, wrapping around to the other side of the input image in such cases.&lt;/p&gt;
    &lt;p&gt;Consider this 4x4 pixel input image:&lt;/p&gt;
    &lt;p&gt;With a tile size of 3, the first 3 tiles created by looking at squares of pixels with their top-left corners along the top row of pixels:&lt;/p&gt;
    &lt;p&gt;In the 3rd tile, we sampled off the edge of the image. In such cases, wrap around to the other side of the image. Effectively pretend that the image repeats forever in all directions.&lt;/p&gt;
    &lt;p&gt;Continuing in this fashion, enumerate all the tiles. In this example there are 16, and all are unique.&lt;/p&gt;
    &lt;p&gt;Assign each tile a tile index. In the example, we would use numbers from 0 to 15 as indices. Frequency hints and adjacency rules will be given in terms of tile indices, rather than tiles themselves.&lt;/p&gt;
    &lt;p&gt;The next few sections will explain how to construct frequency hints and adjacency rules so the core algorithm generates images which are similar to the input.&lt;/p&gt;
    &lt;p&gt;Here’s an image which is similar to the example image, generated using WFC:&lt;/p&gt;
    &lt;head rend="h4"&gt;Reflection and Rotation&lt;/head&gt;
    &lt;p&gt;When generating tiles from an input image, you may want to include tiles which aren’t necessarily present in the input image, but which are the rotation or reflection of tiles from the input image.&lt;/p&gt;
    &lt;p&gt;We need a new example image to demonstrate this, as each rotation and reflection of each tile is also in the tile set. Let’s use the following input image:&lt;/p&gt;
    &lt;p&gt;With a tile size of 3, the top-left tile we extract will be:&lt;/p&gt;
    &lt;p&gt;All rotations and reflections of this tile:&lt;/p&gt;
    &lt;p&gt;Repeat this for all the tiles extracted from the image.&lt;/p&gt;
    &lt;p&gt;To include these tiles in the output, proceed with the rest of the algorithm as normal, with these added to the tile set as fully qualified tiles, with their own unique tile indices.&lt;/p&gt;
    &lt;p&gt;A similar image to the input without rotations or reflections included:&lt;/p&gt;
    &lt;p&gt;Here’s an output with all rotations and reflections included:&lt;/p&gt;
    &lt;p&gt;The banner at the top of this page was generated from the following image including all rotations and reflections:&lt;/p&gt;
    &lt;p&gt;Here’s a miniature version:&lt;/p&gt;
    &lt;p&gt;Notice that the ground is missing? Since the input image is wrapped, there are no tiles in which the ground ends or changes direction. This means that if the ground is present in the output, it must form a solid line from one side of the output to the other. This is a fairly restrictive constraint, so in most output, there is no ground at all. It’s possible for the output to deviate from the frequency hints if the structure of the image means there is no way to place a tile without violating the adjacency rules.&lt;/p&gt;
    &lt;p&gt;There is a small chance of the output containing ground:&lt;/p&gt;
    &lt;p&gt;It’s not on the bottom of the screen, because the input image is wrapped.&lt;/p&gt;
    &lt;p&gt;Since this was generated with rotations and reflections included, there’s nothing to stop the ground from being vertical.&lt;/p&gt;
    &lt;head rend="h4"&gt;Frequency Hints&lt;/head&gt;
    &lt;p&gt;The number of occurrences of each tile in the input image is counted, and mappings from a tile’s index to its count make up the frequency hints.&lt;/p&gt;
    &lt;p&gt;Let’s modify the first example image:&lt;/p&gt;
    &lt;p&gt;The set of unique 3x3 tiles in this input image will be the same as the first example, however where in the first example each tile appeared exactly once, here some patterns appear several times.&lt;/p&gt;
    &lt;p&gt;The following tiles appear 5 times in this image:&lt;/p&gt;
    &lt;p&gt;The remaining tiles still just appear once.&lt;/p&gt;
    &lt;p&gt;The relative frequency the above 4 tiles will be 5, and the hint for the other tiles will be 1. This means that the above 4 tiles are 5 times as likely to appear in a given position as the other tiles.&lt;/p&gt;
    &lt;p&gt;How do you think this will change the output?&lt;/p&gt;
    &lt;p&gt;Increasing the odds of vertical lines appearing means there will likely be more vertical lines. This manifests as the grid cells in the image generally being taller than they are wide.&lt;/p&gt;
    &lt;head rend="h4"&gt;Adjacency Rules&lt;/head&gt;
    &lt;p&gt;The core will produce a grid of tile indices where each index corresponds to a single pixel in the output image. For a given tile index in a cell of this grid, the output pixel corresponding to the cell, will be given the colour of the tile corresponding to the tile index. Only the colour of the top-left pixel of the tile will be used. Keep that in mind for this section: for every tile placed, only a single pixel of the tile (its top-left pixel) is actually added to the output image. As the core assigns pixel indices to grid cells, we can say that the core assigns the top-left corners of tiles to output image pixels.&lt;/p&gt;
    &lt;p&gt;Remember that the goal of the image processor is to produce an output image where every tile-sized square of pixels occurs in the input image. In order to meet this goal, we must ensure that whenever the core assigns the top-left pixel of a tile to a pixel of the output image, the rest of the pixels of the tile end up in the right places as well. This is best explained with an example. Here’s a zoomed-in section of the banner:&lt;/p&gt;
    &lt;p&gt;Consider the 3x3 pixel square with a red border. It occurs in the input image (rotated anticlockwise 90 degrees, below the bottom-right flower). Let’s assume it gets tile index 7. The core algorithm decided that the grid cell corresponding to the top-left pixel in the red square, should contain tile index 7. Even though tile index 7 refers to the whole 3x3 tile (but only the image processor knows this), choosing tile index 7 for this cell resulted in only the top-left pixel of the output image being populated.&lt;/p&gt;
    &lt;p&gt;But somehow the rest of the red square ended up looking like the tile with tile index 7 as well. But why? Every time a tile index is assigned to a cell, we want a way to make sure that the entire tile’s pixels - not just its top-left pixel - end up in the right output pixels, relative to the tile’s placement. Since each tile placement just contributes the tile’s top-left pixel, we need to make sure that whenever a tile is placed within tile size pixels of an already-placed tile’s top-left pixel, that the newly placed tile’s pixels don’t conflict with the pixels of the already-placed tile.&lt;/p&gt;
    &lt;p&gt;A convenient fiction to help picture this, is to imagine that whenever the core places a tile in a cell, each pixel in the tile-sized square of pixels whose top-left corner is that cell, is coloured to match the corresponding pixel of the tile, but only the top-left cell is marked as “populated”. Unpopulated (but possibly coloured) cells can have tiles placed in them, as long as all cells in the square filled by the new tile are either not yet coloured, or contain the same colour as the corresponding pixel of the new tile.&lt;/p&gt;
    &lt;p&gt;Let’s generate adjacency rules to force the core to never place two tiles in positions where their overlapping pixels conflict. Recall that an adjacency rule is of the form: “Tile index A may appear in the cell 1 space in DIRECTION from a cell containing tile index B”. The rules should only permit A to be placed adjacent to B in DIRECTION if doing so would not cause a conflict. All non-conflicting adjacencies should be allowed.&lt;/p&gt;
    &lt;code&gt;let mut adjacency_rules = AdjacencyRules::new();
for a in all_tiles {
    for b in all_tiles {
        for direction in [LEFT, RIGHT, UP, DOWN] {
            if compatible(a, b, direction) {
                adjacency_rules.allow(a, b, direction);
            }
        }
    }
}
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;compatible(a: Tile, b: Tile, direction: Direction) -&amp;gt; bool&lt;/code&gt; function returns true if and only if overlapping
&lt;code&gt;a&lt;/code&gt; with &lt;code&gt;b&lt;/code&gt;, if &lt;code&gt;b&lt;/code&gt; is offset by 1 pixel in &lt;code&gt;direction&lt;/code&gt;, the overlapping parts
of the two tiles are identical.&lt;/p&gt;
    &lt;p&gt;In the example below, &lt;code&gt;compatible(A, B, RIGHT) == true&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;In this example tiles are 3x3, but these adjacency rules only ensure that adjacent tiles are compatible. It’s possible for a pair of tiles which are 2 pixels apart to overlap. What prevents them from conflicting?&lt;/p&gt;
    &lt;p&gt;Consider this example:&lt;/p&gt;
    &lt;p&gt;The red and blue squares surround tile placements which are 2 pixels apart. They aren’t adjacent, so the adjacency rules don’t explicitly forbid the red tile’s pixels and blue tile’s pixels from being different in the intersecting area.&lt;/p&gt;
    &lt;p&gt;However, the existence of the black square, which is adjacent to both the red and blue squares, means that the red and blue tile placements won’t conflict.&lt;/p&gt;
    &lt;p&gt;Because they are adjacent, the red/black intersection and the blue/black intersection are conflict-free. That is, the colours of pixels in the intersecting parts of the tiles are the same in both tiles. Red/blue is contained in both red/black and blue/black. Since the pixel colours in the red/blue region of red are the same as the pixel colours in the corresponding region of black, and the pixel colours in the red/blue region of black are the same as the pixel colours in the corresponding region of blue, the pixel colours in the red/blue region of red are the same is the pixel colours of the corresponding region of blue. That is, red/blue is conflict free.&lt;/p&gt;
    &lt;head rend="h4"&gt;Tile/Colour Mappings&lt;/head&gt;
    &lt;p&gt;After preprocessing, the frequency hints and adjacency rules will be passed to the core algorithm, and it will return a grid of tile indices. To produce the output image, we will need to know which tile index refers to which colour. To help with this, the preprocessor outputs a map from tile index to the colour of the top-left pixel of the corresponding tile.&lt;/p&gt;
    &lt;head rend="h3"&gt;Post Processing&lt;/head&gt;
    &lt;code&gt;fn wfc_post_process_image(
    tile_index_grid: Grid2D&amp;lt;TileIndex&amp;gt;,
    top_left_pixel_of_each_tile: HashMap&amp;lt;TileIndex, Colour&amp;gt;,
) -&amp;gt; Image { ... }
&lt;/code&gt;
    &lt;p&gt;The final step is trivial. Take the grid of tile indices produced by running the core algorithm on the adjacency rules and frequency hints from the preprocessor, and the top-left pixel colour map, also returned by the preprocessor, and create an output image of the same dimensions as the grid. For each tile index in the grid, set the corresponding pixel colour in the output image to be the colour associated with that tile index in the top-left pixel colour map.&lt;/p&gt;
    &lt;head rend="h3"&gt;Putting it all together&lt;/head&gt;
    &lt;p&gt;Composing these pieces gives the full WFC algorithm.&lt;/p&gt;
    &lt;code&gt;fn wfc_image(image: Image, tile_size: u32, output_size: (u32, u32)) -&amp;gt; Image {
    let (adjacency_rules, frequency_hints, top_left_pixel_of_each_tile) =
        wfc_pre_process_image(image, tile_size);
    let tile_index_grid = wfc_core(adjacency_rules, frequency_hints, output_size);
    return wfc_post_process_image(tile_index_grid, top_left_pixel_of_each_tile);
}
&lt;/code&gt;
    &lt;head rend="h2"&gt;Core Internals&lt;/head&gt;
    &lt;p&gt;My goal for this section is to impart a deep understanding of how and why the core algorithm works. This is the guide I wish I had during my implementation of WFC. Maybe it can help you through yours!&lt;/p&gt;
    &lt;head rend="h3"&gt;Sudoku&lt;/head&gt;
    &lt;p&gt;Imagine you’re solving a sudoku.&lt;/p&gt;
    &lt;p&gt;Your goal is to place a number from 1-9 in each empty cell, such that each row, column, and 3x3 square, contains each number from 1-9 exactly once.&lt;/p&gt;
    &lt;p&gt;Suppose you’re not super-confident in your sudoku-solving abilities, and your pencil comes equipped with an eraser. You could cheat, by writing in each cell (in tiny writing) all the possible values for that cell. Once all the cells are enumerated in this fashion, hopefully there will be some cells with a single possible value. For each of these cells, you erase the tiny pencil number, and rewrite the number in permanent marker, “locking in” your solution for that cell.&lt;/p&gt;
    &lt;p&gt;After locking in a cell, you want to update your enumerated possibilities for other cells in the same row, column, and 3x3 square, erasing all instances of the number you just locked in. You hope that by doing this, you’ll eliminate enough possibilities that the next choice becomes obvious, and so on until you’ve solved the entire sudoku in the least fun way possible! (This algorithm doesn’t actually work in all cases, so you may occasionally have to employ some actual thought!)&lt;/p&gt;
    &lt;p&gt;Now, imagine you wanted to solve an empty sudoku in the same way as just described:&lt;/p&gt;
    &lt;p&gt;Your goal is still to end up with the numbers from 1-9 in every row, column, and 3x3, but this time you’re searching for one of a large number of possible solutions. You start by writing in tiny pencil digits, the numbers from 1-9 in each cell. Now you have a conundrum! Which cell do you lock in first? What number to you write first?&lt;/p&gt;
    &lt;p&gt;Say you make your first choice arbitrarily, and write a big 3 in pen in the top-left corner. Now you can propagate the effect of this choice by scanning along the top-most row, left-most column, and top-left 3x3 square, and erasing all the 3s.&lt;/p&gt;
    &lt;p&gt;What next?&lt;/p&gt;
    &lt;p&gt;If you keep choosing arbitrarily, it’s very likely that at some point, you’ll lock in a number, propagate the effect, and end up erasing the last penciled-in number in a cell. That would be bad, as then there would be no way to complete the soduku!&lt;/p&gt;
    &lt;p&gt;Maybe you’re ok with screwing up occasionally, as long as there’s a reasonable chance that you’ll end up with a valid solution each time. Then if you get into a bad state (ie. a cell with no possibilities), you can just give up and start over.&lt;/p&gt;
    &lt;p&gt;Intuitively, when choosing which cell to lock in next, you might want to always choose the cell with the fewest remaining possibilities. If there are several cells tied for the fewest remaining possibilities, choose arbitrarily between them.&lt;/p&gt;
    &lt;p&gt;Concretely, your strategy is to repeat the following until all cells are locked in:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Choose a cell at random, considering only the cells with the fewest possible values.&lt;/item&gt;
      &lt;item&gt;Choose a random value to lock in for that cell, considering only the possible values for that cell.&lt;/item&gt;
      &lt;item&gt;Propagate the effects of locking in the cell, removing the locked-in value from the possibilities of cells sharing the cell’s row, column, and 3x3 square.&lt;/item&gt;
      &lt;item&gt;If during propagation, you removed the final possibility of a cell, give up and start again.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The core WFC algorithm looks very similar to this. In place of the rules of sudoku, there are the adjacency rules. The frequency hints mean that when choosing a value to lock in, you no longer make a uniformly random choice, but instead choose from a probability distribution of the possible tile indices, weighted by frequency hints. The fact that some tiles are more likely to appear than others means the choice of which cell to lock in next is a little more complicated than just choosing from the cells with the fewest possibilities. Finally, we’re going to be more clever about propagation than in the sudoku example; you don’t have to wait until a cell is locked in to eliminate possibilities from surrounding cells.&lt;/p&gt;
    &lt;head rend="h3"&gt;Rough Sketch&lt;/head&gt;
    &lt;p&gt;Here’s a rough outline of the types and methods that the core will use. I’ll fill in details as they become relevant. In the spirit of Wave Function Collapse, “collapsing” a cell will refer to locking-in the choice of which tile index will go in the cell.&lt;/p&gt;
    &lt;code&gt;// Tile indices are just integers
type TileIndex = usize;

// corresponds to a cell in the output grid
struct CoreCell {
    // possible[tile_index] == true means tile_index may be chosen for this cell.
    // Initially, every element of this vector is `true`.
    // There is one element for each unique tile index.
    possible: Vec&amp;lt;bool&amp;gt;,
    ...
}

struct CoreState {
    // corresponds to the output grid
    grid: Grid2D&amp;lt;CoreCell&amp;gt;,
    // initialised to the number of cells in `grid`
    remaining_uncollapsed_cells: usize,
    // arguments passed from image processor
    adjacency_rules: AdjacencyRules,
    frequency_hints: FrequencyHints,
    ...
}

impl CoreState {

    // return the coordinate of the next cell to collapse
    fn choose_next_cell(&amp;amp;self) -&amp;gt; Coord2D { ... }

    // collapse the cell at a given coordinate
    fn collapse_cell_at(&amp;amp;mut self, coord: Coord2D) { ... }

    // remove possibilities based on collapsed cell
    fn propagate(&amp;amp;mut self) { ... }

    // roughly the same as the empty sudoku solver
    fn run(&amp;amp;mut self) {
        while self.remaining_uncollapsed_cells &amp;gt; 0 {
            let next_coord = self.choose_next_cell();
            self.collapse_cell_at(next_coord);
            self.propagate();
            self.remaining_uncollapsed_cells -= 1;
        }
    }
}
&lt;/code&gt;
    &lt;p&gt;The remainder of this chapter will flesh out the details of &lt;code&gt;choose_next_cell&lt;/code&gt;,
&lt;code&gt;collapse_cell_at&lt;/code&gt;, and &lt;code&gt;propagate&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h3"&gt;Choosing the Next Cell to Collapse&lt;/head&gt;
    &lt;p&gt;In the sudoku example, we just chose randomly between the cells with the fewest valid choices, to reduce the odds of removing all possibilities from a cell. The intuition behind this is to lock in one of the cells with the least uncertainty. In information theory, this uncertainty is known as entropy. The goal of this step is to choose randomly between the cells whose entropy is lowest. The &lt;code&gt;possible&lt;/code&gt; field of &lt;code&gt;CoreCell&lt;/code&gt; can tell us which tiles are allowed in a
given cell, and the &lt;code&gt;FrequencyHints&lt;/code&gt; can tell us how likely a given tile is to
appear in any cell.&lt;/p&gt;
    &lt;head rend="h4"&gt;Entropy Primer&lt;/head&gt;
    &lt;p&gt;If you have an unknown value with &lt;code&gt;n&lt;/code&gt; possibilities: &lt;code&gt;x1&lt;/code&gt;, &lt;code&gt;x2&lt;/code&gt;, …, &lt;code&gt;xn&lt;/code&gt;,
and the probability of a given value x, represented as a number between 0 and 1, is expressed as &lt;code&gt;P(x)&lt;/code&gt;, then the
entropy of your unknown value is:&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;- P(x1)*log(P(x1))  -  P(x2)*log(P(x2))  -  ...  -  P(xn)*log(P(xn))&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;…where the base of the logarithm is arbitrary. Let’s use 2 as our base.&lt;/p&gt;
    &lt;p&gt;To help build an intuition for this, first note that &lt;code&gt;P(x)&lt;/code&gt; for all
possibilities will be between 0 and 1, and the sum:&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;P(x1) + P(x2) + ... + P(xn)&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;…is equal to 1. This is because &lt;code&gt;x1&lt;/code&gt;…&lt;code&gt;xn&lt;/code&gt; covers all possible outcomes, and one of the
outcomes will end up happening.&lt;/p&gt;
    &lt;p&gt;Each term in the entropy equation is negated. This is because &lt;code&gt;P(x)&lt;/code&gt; is between
0 and 1, and regardless of the base, the logarithm of values between (exclusive)
0 and 1 is negative. Here’s a graph of log base 2:&lt;/p&gt;
    &lt;p&gt;Since each term contains &lt;code&gt;log(P(x))&lt;/code&gt;, which is always negative, we negate each
term to make the resulting entropy positive, as otherwise it would always be
negative.&lt;/p&gt;
    &lt;head rend="h4"&gt;A special case&lt;/head&gt;
    &lt;p&gt;We can simplify the entropy equation for the purposes of this algorithm.&lt;/p&gt;
    &lt;p&gt;If you have an unknown value with &lt;code&gt;n&lt;/code&gt; possibilities: &lt;code&gt;x1&lt;/code&gt;, &lt;code&gt;x2&lt;/code&gt;, …, &lt;code&gt;xn&lt;/code&gt;,
and each probability &lt;code&gt;P(x)&lt;/code&gt; can be represented by &lt;code&gt;w1 / (w1 + w2 + ... + wn)&lt;/code&gt;
(&lt;code&gt;w&lt;/code&gt; stands for “weight”), then the above entropy equation can be simplified.
In practical terms, this is the case when you have a discrete probability
distribution - that is, you associate a weight &lt;code&gt;wk&lt;/code&gt; with each possibility &lt;code&gt;xk&lt;/code&gt;.
The probability of an outcome is its weight divided by the sum of all weights.&lt;/p&gt;
    &lt;p&gt;Let &lt;code&gt;W = w1 + w2 + ... + wn&lt;/code&gt; be the sum of all weights.&lt;/p&gt;
    &lt;p&gt;Then the entropy of the unknown value is:&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;log(W)  -  (w1*log(w1)  +  w2*log(w2)  +  ...  +  wn*log(wn))  /  W&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;You can derive this equation from the original entropy equation and log identities. It’s very satisfying. Try it!&lt;/p&gt;
    &lt;head rend="h4"&gt;Relative Tile Frequencies&lt;/head&gt;
    &lt;p&gt;This simplified entropy definition is relevant to choosing the next cell, as the frequency hint is effectively a discrete probability distribution of possible choices of tile.&lt;/p&gt;
    &lt;p&gt;Let’s declare some methods of &lt;code&gt;FrequencyHints&lt;/code&gt; and &lt;code&gt;CoreCell&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;impl FrequencyHints {
    // Returns the number of times the corresponding tile appears in the input.
    // This corresponds to the weight of a possibility in the simplified entropy
    // equation.
    fn relative_frequency(&amp;amp;self, tile_index: TileIndex) -&amp;gt; usize { ... }
}

impl CoreCell {
    // Add up the relative frequencies of all possible tiles.
    // This corresponds to the total weight (W) in the simplified entropy
    // equation.
    fn total_possible_tile_frequency(&amp;amp;self, freq_hint: &amp;amp;FrequencyHints) -&amp;gt; usize {
        let mut total = 0;
        for (tile_index, &amp;amp;is_possible) in self.possible.iter().enumerate() {
            if is_possible {
                total += freq_hint.relative_frequency(tile_index);
            }
        }
        return total;
    }
}
&lt;/code&gt;
    &lt;p&gt;Armed with these definitions, we can compute the entropy of a cell!&lt;/p&gt;
    &lt;code&gt;impl CoreCell {
    fn entropy(&amp;amp;self, freq_hint: &amp;amp;FrequencyHints) -&amp;gt; f32 {

        let total_weight = self.total_possible_tile_frequency(freq_hint) as f32;

        let sum_of_weight_log_weight =
            self.possible.iter().enumerate().map(|(tile_index, &amp;amp;is_possible)| {
                if is_possible {
                    let rf = freq_hint.relative_frequency(tile_index) as f32;
                    return rf * rf.log2();
                } else {
                    return 0 as f32;
                }
            })
            .sum();

        return total_weight.log2() - (sum_of_weight_log_weight / total_weight);
    }
}
&lt;/code&gt;
    &lt;head rend="h4"&gt;Caching&lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;CoreCell::entropy&lt;/code&gt; method currently iterates over all the tiles. It can be
made constant time with caching. Throughout the course of this algorithm,
possible tiles will be removed from cells. The only time a cell’s entropy
changes is when a possible tile is removed. The caching strategy will be to keep
a running total of:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;W = w1 + w2 + ... + wn&lt;/code&gt;of the possible tiles&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;w1*log(w1) + w2*log(w2) + ... + wn*log(wn)&lt;/code&gt;of the possible tiles&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Adding to the definition of &lt;code&gt;CoreCell&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;struct CoreCell {
    possible: Vec&amp;lt;bool&amp;gt;,

    // new fields:

    sum_of_possible_tile_weights: usize,

    sum_of_possible_tile_weight_log_weights: f32,
    ...
}
&lt;/code&gt;
    &lt;p&gt;Keep these fields up to date:&lt;/p&gt;
    &lt;code&gt;impl CoreCell {
    fn remove_tile(&amp;amp;mut self, tile_index: TileIndex, freq_hint: &amp;amp;FrequencyHints) {
        self.possible[tile_index] = false;

        let freq = freq_hint.relative_frequency(tile_index);

        self.sum_of_possible_tile_weights -= freq;

        self.sum_of_possible_tile_weight_log_weights -=
            (freq as f32) * (freq as f32).log2();
    }
}
&lt;/code&gt;
    &lt;p&gt;And now our entropy calculation becomes much simpler:&lt;/p&gt;
    &lt;code&gt;impl CoreCell {
    fn entropy(&amp;amp;self) -&amp;gt; f32 {
        return (self.sum_of_possible_tile_weights as f32).log2()
            - (self.sum_of_possible_weight_log_weights /
                self.sum_of_possible_tile_weights as f32)
    }
}
&lt;/code&gt;
    &lt;p&gt;It may also be worth it to cache &lt;code&gt;(freq as f32) * (freq as f32).log2()&lt;/code&gt; for
each relative frequency, inside the &lt;code&gt;FrequencyHints&lt;/code&gt; type, as it would then only
need to be computed once for each tile rather than each time a tile is removed
from a cell.&lt;/p&gt;
    &lt;head rend="h4"&gt;Choosing Randomly&lt;/head&gt;
    &lt;p&gt;The goal of this step is to choose randomly between the minimum entropy cells. So far we can compute the entropy of a cell, but if there’s a tie, how do we randomly break it. We could maintain a list of all the minimum entropy cells and then choose randomly from it, but this sounds like a lot of work. Instead, let’s just added a small amount of noise to each entropy calculation! We can save needing to invoke a random number generator for each entropy calculation by pre-computing a noise value for each cell.&lt;/p&gt;
    &lt;code&gt;struct CoreCell {
    possible: Vec&amp;lt;bool&amp;gt;,
    sum_of_possible_tile_weights: usize,
    sum_of_possible_tile_weight_log_weights: f32,

    // new fields:

    // initialise to a tiny random value:
    entropy_noise: f32,
    ...
}
&lt;/code&gt;
    &lt;p&gt;In the entropy calculation, just add &lt;code&gt;entropy_noise&lt;/code&gt; to the previously
calculated value to get a noisy value. If all the entropy calculations are
noisy, there won’t be any ties to break!&lt;/p&gt;
    &lt;head rend="h4"&gt;Choosing the Minimum Entropy Cell&lt;/head&gt;
    &lt;p&gt;We can now compute a noisy entropy for each cell, so now we just need to iterate over all the cells and keep track of the cell with the lowest entropy, right?&lt;/p&gt;
    &lt;p&gt;Well we could, but remember there’s one cell per output image pixel, and 200x200 pixel output images are not unheard of. Do we really want to iterate over 40,000 cells at every step of the algorithm? The definition of &lt;code&gt;CoreState::run&lt;/code&gt; above invokes &lt;code&gt;choose_next_cell&lt;/code&gt; once for each collapsed cell,
which is effectively once per cell again. 40,000x40,000 is not a nice number!&lt;/p&gt;
    &lt;p&gt;Rather than iterating over all the cells each time we need to choose the minimum entropy cell, maintain a heap of cells, keyed by their entropy. Whenever the entropy of a cell changes, push it to the heap. To find the minimum entropy cell, pop from the heap until you get a cell which hasn’t been collapsed yet. If a cell’s entropy changes multiple times, you’ll end up inserting it into the heap multiple times too. When popping from the heap, you need a way of knowing whether each cell that you pop has been collapsed yet so you can skip it.&lt;/p&gt;
    &lt;code&gt;struct CoreCell {
    possible: Vec&amp;lt;bool&amp;gt;,
    sum_of_possible_tile_weights: usize,
    sum_of_possible_tile_weight_log_weights: f32,
    entropy_noise: f32,

    // new fields:

    // initialise to false, set to true after collapsing
    is_collapsed: bool,
    ...
}

// We will populate the heap with `EntropyCoord`s rather than CoreCell
// references to keep the borrow checker happy!
struct EntropyCoord {
    entropy: f32,
    coord: Coord2D,
}

impl Ord for EntropyCoord {
    fn cmp(&amp;amp;self, other: &amp;amp;Self) -&amp;gt; Ordering {
        // just compare the entropies
    }
}

struct CoreState {
    grid: Grid2D&amp;lt;CoreCell&amp;gt;,
    remaining_uncollapsed_cells: usize,
    adjacency_rules: AdjacencyRules,
    frequency_hints: FrequencyHints,

    // new fields:

    entropy_heap: BinaryHeap&amp;lt;EntropyCoord&amp;gt;,
    ...
}

impl CoreState {
    fn choose_next_cell(&amp;amp;mut self) -&amp;gt; Coord2D {
        while let Some(entropy_coord) = self.entropy_heap.pop() {
            let cell = self.grid.get(entropy_coord.coord);
            if !cell.is_collapsed {
                return entropy_coord.coord;
            }
        }

        // should not end up here
        unreachable!("entropy_heap is empty, but there are still \
            uncollapsed cells");
    }
}
&lt;/code&gt;
    &lt;head rend="h4"&gt;Contradictions&lt;/head&gt;
    &lt;p&gt;It’s possible to get into a state where a cell has no possibilities remaining. I’ll call such a state a “contradiction”. The point of choosing the minimum entropy cell to collapse is to try to minimise this risk of contradiction, but sometimes it happens anyway. Certain sets of adjacency rules (ie. certain input images) increase the risk of contradictions.&lt;/p&gt;
    &lt;p&gt;In practice, when a contradiction is reached, most implementations of WFC (including mine) just give up and start again, maintaining a counter of times this has happened, and stopping when it gets too high. Alternatives to this might be:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;saving checkpoints of the core state at various points throughout generation, and rolling back to a previous checkpoint upon contradiction&lt;/item&gt;
      &lt;item&gt;making it possible to reverse the collapsing of a cell, making removed possibilities possible again, so the cell with no choices of tile has some choices again&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is an interesting topic, but it’s out of scope for this post.&lt;/p&gt;
    &lt;head rend="h3"&gt;Collapse Cell&lt;/head&gt;
    &lt;code&gt;impl CoreState {
    // collapse the cell at a given coordinate
    fn collapse_cell_at(&amp;amp;mut self, coord: Coord2D) { ... }
}
&lt;/code&gt;
    &lt;p&gt;The previous section explained how to choose which cell to collapse next. Now we need a way of choosing which tile to lock in. This method will select randomly between all possible tiles for the chosen cell, assigning probabilities based on &lt;code&gt;FrequencyHints&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;We’ll now choose from a probability distribution, where possible values are the tile indices yielded by this iterator, and weights come from &lt;code&gt;FrequencyHints::relative_frequency&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Say for a given cell, the remaining possible tile indices are 2, 4, 7, and 8, and their relative frequencies are indicated by the width of their section of the strip below.&lt;/p&gt;
    &lt;p&gt;We want to choose a random position within this strip, and see which section we ended up in. Naturally, we’re more likely to end up in one of the wider sections.&lt;/p&gt;
    &lt;p&gt;Here we landed on 7, so we lock in 7 for this cell.&lt;/p&gt;
    &lt;p&gt;Translating this diagram into code, we’ll choose a random number between &lt;code&gt;0&lt;/code&gt; and
&lt;code&gt;cell.sum_of_possible_tile_weights&lt;/code&gt; (introduced in the
Caching section). This is analogous to choosing a random position within the
strip. To determine the tile index, we’ll decrease
the chosen number by each weight (the width of strips) until doing so would make
it negative.&lt;/p&gt;
    &lt;code&gt;impl CoreCell {

    // it will be convenient to be able to iterate over all possible tile indices
    fn possible_tile_iter(&amp;amp;self) -&amp;gt; impl Iterator&amp;lt;Item=TileIndex&amp;gt; { ... }

    fn choose_tile_index(&amp;amp;self, frequency_hints: &amp;amp;FrequencyHints) -&amp;gt; TileIndex {
        // the random position in the strip
        let mut remaining =
            random_int_between(0, self.sum_of_possible_tile_weights);

        for possible_tile_index in self.possible_tile_iter() {

            // the width of the section of strip
            let weight =
                frequency_hints.relative_frequency(possible_tile_index);

            if remaining &amp;gt;= weight {
                remaining -= weight;
            } else {
                return possible_tile_index;
            }
        }

        // should not end up here
        unreachable!("sum_of_possible_weights was inconsistent with \
            possible_tile_iter and FrequencyHints::relative_frequency");
    }
}
&lt;/code&gt;
    &lt;p&gt;It’s now fairly straightforward to implement &lt;code&gt;collapse_cell_at&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;impl CoreState {
    // collapse the cell at a given coordinate
    fn collapse_cell_at(&amp;amp;mut self, coord: Coord2D) {
        let mut cell = self.grid.get(coord);
        let tile_index_to_lock_in = cell.choose_tile_index(&amp;amp;self.frequency_hints);

        cell.is_collapsed = true;

        // remove all other possibilities
        for (tile_index, possible) in cell.possible.iter_mut().enumerate() {
            if tile_index != tile_index_to_lock_in {
                *possible = false;
                // We _could_ call
                // `cell.remove_tile(tile_index, &amp;amp;self.frequency_hints)` here
                // instead of explicitly setting `possible` to false, however
                // there's no need to update the cached sums of weights for this
                // cell. It's collapsed now, so we no longer care about its
                // entropy.
            }
        }
    }
}
&lt;/code&gt;
    &lt;head rend="h3"&gt;Propagate&lt;/head&gt;
    &lt;p&gt;Propagation enforces the adjacency rules by eliminating choices of tiles from cells. If we propagate after every cell is collapsed, no matter which cell, or possible tiles for the cell, are chosen, the resulting output of the core algorithm will respect the adjacency rules.&lt;/p&gt;
    &lt;p&gt;Each time a tile is chosen for a cell, it’s likely that there will be fewer choices of tile available to the surrounding cells. This is because of the output must satisfy the adjacency rules. When a choice of tile is locked-in for a cell, the adjacency rules tell us which tiles may be chosen for the cells surrounding the locked-in cell.&lt;/p&gt;
    &lt;head rend="h4"&gt;Looking beyond immediate neighbours&lt;/head&gt;
    &lt;p&gt;In addition to updating the possibilities of cells adjacent to the collapsed cell, we can often remove some tile choices from cells further away. The key idea which this will rely on is:&lt;/p&gt;
    &lt;p&gt;If it’s possible to place a tile in a cell, then in each of that cell’s four immediate neighbours, it must be possible to place a compatible tile, according to the adjacency rules.&lt;/p&gt;
    &lt;p&gt;This is more applicable to propagation if we consider the contrapositive of this statement:&lt;/p&gt;
    &lt;p&gt;For a given cell and tile, if in any of that cell’s immediate neighbours, it’s not possible to place a compatible tile, then the original tile may not be placed in that cell.&lt;/p&gt;
    &lt;p&gt;As an example, let’s say the adjacency rules are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2 may appear to the RIGHT of 1&lt;/item&gt;
      &lt;item&gt;2 may appear BELOW 1&lt;/item&gt;
      &lt;item&gt;3 may appear to the RIGHT of 1&lt;/item&gt;
      &lt;item&gt;3 may appear to the RIGHT of 2&lt;/item&gt;
      &lt;item&gt;3 may appear to the RIGHT of 3&lt;/item&gt;
      &lt;item&gt;3 may appear BELOW 1&lt;/item&gt;
      &lt;item&gt;3 may appear BELOW 2&lt;/item&gt;
      &lt;item&gt;4 may appear to the RIGHT of 2&lt;/item&gt;
      &lt;item&gt;4 may appear BELOW 1&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For completeness, assume that each rule above has a corresponding rule in the opposite direction (e.g. the first would be “1 may appear to the LEFT of 2”).&lt;/p&gt;
    &lt;p&gt;Suppose the possibilities of cells are represented by the numbers in corresponding cells of this table, and we just locked in 1 in the top-left corner.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;1, 2, 3, 4&lt;/cell&gt;
        &lt;cell&gt;1, 2, 3, 4&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;1, 2, 3, 4&lt;/cell&gt;
        &lt;cell&gt;1, 2, 3, 4&lt;/cell&gt;
        &lt;cell&gt;1, 2, 3, 4&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;1, 2, 3, 4&lt;/cell&gt;
        &lt;cell&gt;1, 2, 3, 4&lt;/cell&gt;
        &lt;cell&gt;1, 2, 3, 4&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Propagating the effects to the immediate neighbours:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;2, 3&lt;/cell&gt;
        &lt;cell&gt;1, 2, 3, 4&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;2, 3, 4&lt;/cell&gt;
        &lt;cell&gt;1, 2, 3, 4&lt;/cell&gt;
        &lt;cell&gt;1, 2, 3, 4&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;1, 2, 3, 4&lt;/cell&gt;
        &lt;cell&gt;1, 2, 3, 4&lt;/cell&gt;
        &lt;cell&gt;1, 2, 3, 4&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;But we’re not done yet. For example, what can we say about the middle cell?&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;2, 3&lt;/cell&gt;
        &lt;cell&gt;1, 2, 3, 4&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;2, 3, 4&lt;/cell&gt;
        &lt;cell&gt;1, 2, 3, 4&lt;/cell&gt;
        &lt;cell&gt;1, 2, 3, 4&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;1, 2, 3, 4&lt;/cell&gt;
        &lt;cell&gt;1, 2, 3, 4&lt;/cell&gt;
        &lt;cell&gt;1, 2, 3, 4&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Can the middle cell contain a 4? It’s possible for a 2 to be in the cell to its left, but it’s not possible for a 1 to be in the cell above it, so we can remove the 4 from the middle cell.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;2, 3&lt;/cell&gt;
        &lt;cell&gt;1, 2, 3, 4&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;2, 3, 4&lt;/cell&gt;
        &lt;cell&gt;1, 2, 3&lt;/cell&gt;
        &lt;cell&gt;1, 2, 3, 4&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;1, 2, 3, 4&lt;/cell&gt;
        &lt;cell&gt;1, 2, 3, 4&lt;/cell&gt;
        &lt;cell&gt;1, 2, 3, 4&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Propagation continues in this fashion until no further possibilities can be removed.&lt;/p&gt;
    &lt;head rend="h4"&gt;Tile Enablers&lt;/head&gt;
    &lt;p&gt;For a given tile/cell combination, we’ll say that the possibility of another tile in an adjacent cell enables the first tile tile to appear in the first cell, if the adjacency of these 2 tiles would be allowed by the adjacency rules. We’ll say that a potential tile in a cell is enabled in a direction, if the immediate neighbour cell in that direction permits at least one tile which enables the first tile. Note that a tile/cell may have multiple enablers in a given direction. In the example above, the potential 3 in the middle cell is enabled in the LEFT direction by the potential 2 and the potential 3 in the cell to its left.&lt;/p&gt;
    &lt;p&gt;A tile may not be placed in a cell if it has 0 enablers in any direction. That is, it needs at least 1 enabler in every direction to be a candidate tile for the cell. The potential 4 in the middle cell was not enabled by any potential tiles in the cell above it, so the potential 4 was removed.&lt;/p&gt;
    &lt;head rend="h4"&gt;Cascading Removal&lt;/head&gt;
    &lt;p&gt;If removing the possibility of a tile from a cell caused a potential tile in a neighbouring cell to lose its last enabler, you must also remove the possibility of that second tile from the neighbouring cell. This can cause a cascade in which many potential tiles are removed from many cells in a single fell swoop.&lt;/p&gt;
    &lt;p&gt;To keep track of which tiles must be removed from which cells, whenever a potential tile is removed from a cell, we’ll update a stack of removal updates.&lt;/p&gt;
    &lt;code&gt;// indicates the potential for tile_index appearing in the cell at coord
// has been removed
struct RemovalUpdate {
    tile_index: TileIndex,
    coord: Coord2D,
}

struct CoreState {
    grid: Grid2D&amp;lt;CoreCell&amp;gt;,
    remaining_uncollapsed_cells: usize,
    adjacency_rules: AdjacencyRules,
    frequency_hints: FrequencyHints,
    entropy_heap: BinaryHeap&amp;lt;EntropyCoord&amp;gt;,

    // new fields:

    tile_removals: Vec&amp;lt;RemovalUpdate&amp;gt;,
    ...
}
&lt;/code&gt;
    &lt;p&gt;The general strategy for propagation will be popping removal commands from the stack, and checking if the potential tile was removed was the final enabler for a potential tile in a neighbouring cell. If it was, then remove that potential tile, and add a &lt;code&gt;RemovalUpdate&lt;/code&gt; about the
newly-removed tile to the stack.
When the stack is empty, propagation is complete, and its
time to collapse the next cell.&lt;/p&gt;
    &lt;p&gt;After collapsing a cell, the stack of &lt;code&gt;RemovalUpdate&lt;/code&gt;s will be populated with
updates about the removal of all but the locked-in tile. We can update the
&lt;code&gt;collapse_cell_at&lt;/code&gt; function with this in mind (see below the &lt;code&gt;// NEW CODE&lt;/code&gt;
comment).&lt;/p&gt;
    &lt;code&gt;impl CoreState {
    // collapse the cell at a given coordinate
    fn collapse_cell_at(&amp;amp;mut self, coord: Coord2D) {
        let mut cell = self.grid.get(coord);
        let tile_index_to_lock_in = cell.choose_tile_index(&amp;amp;self.frequency_hints);

        cell.is_collapsed = true;

        // remove all other possibilities
        for (tile_index, possible) in cell.possible.iter_mut().enumerate() {
            if tile_index != tile_index_to_lock_in {
                *possible = false;
                // We _could_ call
                // `cell.remove_tile(tile_index, &amp;amp;self.frequency_hints)` here
                // instead of explicitly setting `possible` to false, however
                // there's no need to update the cached sums of weights for this
                // cell. It's collapsed now, so we no longer care about its
                // entropy.

                // NEW CODE
                self.tile_removals.push(RemovalUpdate {
                    tile_index,
                    coord,
                });
            }
        }
    }
}
&lt;/code&gt;
    &lt;head rend="h4"&gt;Counting Enablers&lt;/head&gt;
    &lt;p&gt;Now we need a way of telling whether a removed potential tile was the final enabler for any potential tiles in neighbouring cells. For this, purpose we’ll maintain a count of enablers for each cell, for each potential tile, for each direction.&lt;/p&gt;
    &lt;code&gt;// define directions as integers
type Direction = usize;

const UP:    Direction = 0;
const DOWN:  Direction = 1;
const LEFT:  Direction = 2;
const RIGHT: Direction = 3;

const NUM_DIRECTIONS: usize = 4;
const ALL_DIRECTIONS: [Direction; NUM_DIRECTIONS] = [UP, DOWN, LEFT, RIGHT];

struct TileEnablerCount {
    // `by_direction[d]` will be the count of enablers in direction `d`
    by_direction: [usize; 4],
}

struct CoreCell {
    possible: Vec&amp;lt;bool&amp;gt;,
    sum_of_possible_tile_weights: usize,
    sum_of_possible_tile_weight_log_weights: f32,
    entropy_noise: f32,
    is_collapsed: bool,

    // new fields:

    // `tile_enabler_counts[tile_index]` will be the counts for the
    // corresponding tile
    tile_enabler_counts: Vec&amp;lt;TileEnablerCount&amp;gt;,
    ...
}
&lt;/code&gt;
    &lt;p&gt;For a given &lt;code&gt;cell&lt;/code&gt;, for a tile with index &lt;code&gt;A&lt;/code&gt;, in direction &lt;code&gt;D&lt;/code&gt;,
&lt;code&gt;cell.tile_enabler_counts[A].by_direction[D]&lt;/code&gt; is the number of different
tile indices permitted in the immediate neighbour of &lt;code&gt;cell&lt;/code&gt; in direction &lt;code&gt;D&lt;/code&gt;, which
according to the adjacency rules, are permitted to appear adjacent to tile
&lt;code&gt;A&lt;/code&gt; in direction &lt;code&gt;D&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;How should the counts be initialised?&lt;/p&gt;
    &lt;p&gt;First, observe each cell will start with an identical vector of &lt;code&gt;TileEnablerCount&lt;/code&gt;. As potential tiles are removed, the counts will change, but
they all start out the same.&lt;/p&gt;
    &lt;p&gt;As for the counts for each tile/direction combination, it should come as no surprise that we compute them from the adjacency rules:&lt;/p&gt;
    &lt;code&gt;fn initial_tile_enabler_counts(
    num_tiles: usize,
    adjacency_rules: &amp;amp;AdjacencyRules,
) -&amp;gt; Vec&amp;lt;TileEnablerCount&amp;gt;
{
    let mut ret = Vec::new():

    for tile_a in 0..num_tiles {

        let mut counts = TileEnablerCount {
            by_direction: [0, 0, 0, 0],
        };

        for &amp;amp;direction in ALL_DIRECTIONS.iter() {

            // iterate over all the tiles which may appear in the cell one space
            // in `direction` from a cell containing `tile_a`
            for tile_b in adjacency_rules.compatible_tiles(tile_a, direction) {
                counts.by_direction[direction] += 1;
            }
            ret.push(counts);
        }
        return ret;
    }
}

&lt;/code&gt;
    &lt;p&gt;Use this function to initialise the &lt;code&gt;tile_enabler_counts&lt;/code&gt; field of each
&lt;code&gt;CoreCell&lt;/code&gt; in the &lt;code&gt;grid&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h4"&gt;Propagation Algorithm&lt;/head&gt;
    &lt;p&gt;For each potential tile that was removed from a cell, propagation will visit each neighbour of that cell, and update their enabler counts. This relies on the fact that for a given tile permitted in a cell, all compatible potential tiles in a neighbouring cell will be contributing to the first tile’s enabler count in the appropriate direction. Removing potential compatible tiles therefore reduces the enabler count.&lt;/p&gt;
    &lt;code&gt;impl CoreState {
    // remove possibilities based on collapsed cell
    fn propagate(&amp;amp;mut self) {
        while let Some(removal_update) = self.tile_removals.pop() {
            // at some point in the recent past, removal_update.tile_index was
            // removed as a candidate for the tile in the cell at
            // removal_update.coord

            for &amp;amp;direction in ALL_DIRECTIONS.iter() {
                // propagate the effect to the neighbour in each direction
                let neighbour_coord = removal_update.coord.neighbour(direction);
                let neighbour_cell = self.grid.get_mut(neighbour_coord);

                // iterate over all the tiles which may appear in the cell one
                // space in `direction` from a cell containing
                // `removal_update.tile_index`
                for compatible_tile in self.adjacency_rules.compatible_tiles(
                    removal_update.tile_index,
                    direction,
                ) {

                    // relative to `neighbour_cell`, the cell at
                    // `removal_update.coord` is in the opposite direction to
                    // `direction`
                    let opposite_direction = opposite(direction);

                    // look up the count of enablers for this tile
                    let enabler_counts = &amp;amp;mut neighbour_cell
                        .tile_enabler_counts[compatible_tile];

                    // check if we're about to decrement this to 0
                    if enabler_counts.by_direction[direction] == 1 {

                        // if there is a zero count in another direction,
                        // the potential tile has already been removed,
                        // and we want to avoid removing it again
                        if !enabler_counts.contains_any_zero_count() {
                            // remove the possibility
                            neighbour_cell.remove_tile(
                                compatible_tile,
                                &amp;amp;self.frequency_hints,
                            );
                            // check for contradiction
                            if neighbour_cell.has_no_possible_tiles() {
                                // CONTRADICTION!!!
                            }
                            // this probably changed the cell's entropy
                            self.entropy_heap.push(EntropyCoord {
                                entropy: neighbour_cell.entropy(),
                                coord: neighbour_coord,
                            });
                            // add the update to the stack
                            self.tile_removals.push(RemovalUpdate {
                                tile_index: compatible_tile,
                                coord: neoighbour_coord,
                            });
                        }
                    }

                    enabler_counts.by_direction[direction] -= 1;
                }
            }
        }
    }
}
&lt;/code&gt;
    &lt;head rend="h3"&gt;Putting it all together&lt;/head&gt;
    &lt;p&gt;In the image processor section, the core exposed this interface:&lt;/p&gt;
    &lt;code&gt;fn wfc_core(
    adjacency_rules: AdjacencyRules,
    frequency_rules: FrequencyHints,
    output_size: (u32, u32),
) -&amp;gt; Grid2D&amp;lt;TileIndex&amp;gt; { ... }
&lt;/code&gt;
    &lt;p&gt;To satisfy this interface, we’ll need to construct a &lt;code&gt;CoreState&lt;/code&gt;, and invoke the
&lt;code&gt;run&lt;/code&gt; method. It then needs to extract a &lt;code&gt;Grid2D&amp;lt;TileIndex&amp;gt;&lt;/code&gt; from the &lt;code&gt;grid&lt;/code&gt;
field of &lt;code&gt;CoreState&lt;/code&gt;.
For simplicity, let’s assume that contradictions won’t happen.&lt;/p&gt;
    &lt;p&gt;The complete &lt;code&gt;CoreState&lt;/code&gt; and &lt;code&gt;CoreCell&lt;/code&gt; types:&lt;/p&gt;
    &lt;code&gt;struct CoreCell {
    possible: Vec&amp;lt;bool&amp;gt;,
    sum_of_possible_tile_weights: usize,
    sum_of_possible_tile_weight_log_weights: f32,
    entropy_noise: f32,
    is_collapsed: bool,
    tile_enabler_counts: Vec&amp;lt;TileEnablerCount&amp;gt;,
}

struct CoreState {
    grid: Grid2D&amp;lt;CoreCell&amp;gt;,
    remaining_uncollapsed_cells: usize,
    adjacency_rules: AdjacencyRules,
    frequency_hints: FrequencyHints,
    entropy_heap: BinaryHeap&amp;lt;EntropyCoord&amp;gt;,
    tile_removals: Vec&amp;lt;RemovalUpdate&amp;gt;,
}
&lt;/code&gt;
    &lt;p&gt;Recall that &lt;code&gt;run&lt;/code&gt; was implemented as:&lt;/p&gt;
    &lt;code&gt;impl CoreState {
    fn run(&amp;amp;mut self) {
        while self.remaining_uncollapsed_cells &amp;gt; 0 {
            let next_coord = self.choose_next_cell();
            self.collapse_cell_at(next_coord);
            self.propagate();
            self.remaining_uncollapsed_cells -= 1;
        }
    }
}
&lt;/code&gt;
    &lt;p&gt;Now let’s implement &lt;code&gt;wfc_core&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;fn wfc_core(
    adjacency_rules: AdjacencyRules,
    frequency_hints: FrequencyHints,
    output_size: (u32, u32),
) -&amp;gt; Grid2D&amp;lt;TileIndex&amp;gt;
{
    // the adjacency rules should know how many tiles there are
    let num_tiles = adjacency_rules.num_tiles();

    // every cell in the grid will be initialised to this
    let cell_template = CoreCell {
        // a vector of num_tiles bools where all are true
        possible: (0..num_tiles).map(|_| true).collect(),
        // add up all the relative frequencies
        sum_of_possible_tile_weights:
            (0..num_tiles)
                .map(|index| frequency_hints.relative_frequency(index))
                .sum(),
        // add up all the relative frequencies multiplied by their log2
        sum_of_possible_tile_weight_log_weights:
            (0..num_tiles)
                .map(|index| {
                    let w = frequency_hints.relative_frequency(index) as f32;
                    return w * w.log2();
                })
                .sum(),
        // small random number to add to entropy to break ties
        entropy_noise: random_float_between(0, 0.0000001),
        // initially every cell is uncollapsed
        is_collapsed: false,
        tile_enabler_counts:
            initial_tile_enabler_counts(num_tiles, &amp;amp;adjacency_rules),
    };

    // clone cell_template for each cell of the grid
    let grid = Grid2D::new_repeating(output_size.0, output_size.1, cell_template);

    let mut core_state = CoreState {
        grid,
        remaining_uncollapsed_cells: output_size.0 * output_size.1,
        adjacency_rules,
        frequency_hints,
        entropy_heap: BinaryHeap::new(), // starts empty
        tile_removals: Vec::new(), // starts empty
    };

    // run the core algorithm
    core_state.run();

    // copy the result into the output grid
    let output_grid = Grid2d::new_repeating(output_size.0, output_size.1, 0);
    for (coord, cell) in core_state.grid.enumerate_cells() {
        // all cells are collapsed, so this method will return the chosen
        // tile index for a cell
        let tile_index = cell.get_only_possible_tile_index();
        output_grid.set(coord, tile_index);
    }

    return output_grid;
}
&lt;/code&gt;
    &lt;head rend="h2"&gt;Further Reading&lt;/head&gt;
    &lt;head rend="h3"&gt;My Rust Libraries&lt;/head&gt;
    &lt;p&gt;Shameless plug! My rust libraries which implement this algorithm are here: github.com/gridbugs/wfc&lt;/p&gt;
    &lt;p&gt;There are 2 crates:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;wfc is the image processor and core, which works with any grid of comparable values&lt;/item&gt;
      &lt;item&gt;wfc_image is a wrapper of wfc specifically for working with image files&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It also contains some example applications and interesting input images.&lt;/p&gt;
    &lt;p&gt;Of note is the &lt;code&gt;animate&lt;/code&gt; example, which shows the process of generating the
image, representing uncollapsed pixels with the frequency-hint-weighted average
of colours of possible pixels. Visualising the possibilities for each cell, and
the order in which cells are collapsed can help get a better understanding of
WFC.&lt;/p&gt;
    &lt;code&gt;$ cargo run --manifest-path wfc-image/Cargo.toml --example=animate -- \
    --input wfc-image/examples/cat.png
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;simple&lt;/code&gt; example is also quite useful. It just generates images files based
on a specified image file. I used it for all the examples in this post.&lt;/p&gt;
    &lt;head rend="h3"&gt;Model Synthesis&lt;/head&gt;
    &lt;p&gt;In 2007, Paul Merrell published an algorithm called “Model Synthesis” which uses a constraint solver to generate textures from examples. The Wave Function Collapse algorithm is heavily based on this work.&lt;/p&gt;
    &lt;p&gt;For more info, see the description on Paul Merrell’s website, the source code for their implementation of Model Synthesis, and a thread on twitter comparing Model Synthesis to WFC.&lt;/p&gt;
    &lt;head rend="h3"&gt;WaveFunctionCollaspe Repo&lt;/head&gt;
    &lt;p&gt;My inspiration to make this library, came from this repo: github.com/mxgmn/WaveFunctionCollapse, There are many great WFC resources listed in the Notable ports, forks and spinoffs section.&lt;/p&gt;
    &lt;head rend="h3"&gt;fast-wfc&lt;/head&gt;
    &lt;p&gt;One such port is fast-wfc, which I found to be particularly helpful as a reference for understanding how the algorithm works. Most of my knowledge of WFC came from reverse engineering this project.&lt;/p&gt;
    &lt;head rend="h2"&gt;Outtakes&lt;/head&gt;
    &lt;head rend="h3"&gt;Accidental Procgen&lt;/head&gt;
    &lt;p&gt;While generating images for this post I accidentally ran WFC on this:&lt;/p&gt;
    &lt;p&gt;The output motivated me to add this outtakes section:&lt;/p&gt;
    &lt;p&gt;The gaps between the tiles in the input were transparent, and in the output they are black, which alerted me to the fact that the wfc_image crate currently throws away transparency.&lt;/p&gt;
    &lt;head rend="h3"&gt;Broken Probability Distribution&lt;/head&gt;
    &lt;p&gt;I was originally planning to use this image as an example:&lt;/p&gt;
    &lt;p&gt;I expected to see a roughly equal number of upwards sloping tiles and downwards sloping tiles (as the input image is wrapped when sampling tiles):&lt;/p&gt;
    &lt;p&gt;But instead, the output was almost entirely made up of upwards sloping tiles:&lt;/p&gt;
    &lt;p&gt;After much debugging, I traced the problem to a bug in my implementation of randomly choosing from a probability distribution:&lt;/p&gt;
    &lt;code&gt;commit ede0ea4ed4560bdcf85b4dda989937484bfec21e
Author: Stephen Sherratt &amp;lt;sfsherratt@gmail.com&amp;gt;
Date:   Sun Feb 10 21:37:59 2019 +0000

    Fix bug in probability distribution

diff --git a/wfc/src/wfc.rs b/wfc/src/wfc.rs
index 21ac889..03e6cc7 100644
--- a/wfc/src/wfc.rs
+++ b/wfc/src/wfc.rs
@@ -475,7 +475,7 @@ impl WaveCell {
         for (pattern_id, pattern_stats) in
             self.weighted_compatible_stats_enumerate(global_stats)
         {
-            if remaining &amp;gt; pattern_stats.weight() {
+            if remaining &amp;gt;= pattern_stats.weight() {
                 remaining -= pattern_stats.weight();
             } else {
                 assert!(global_stats.pattern_stats(pattern_id).is_some());
&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45466655</guid><pubDate>Fri, 03 Oct 2025 19:13:34 +0000</pubDate></item><item><title>Arenas in Rust</title><link>https://russellw.github.io/arenas</link><description>&lt;doc fingerprint="ae6a0c98694138e9"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Arenas in Rust&lt;/head&gt; August 5, 2025 A fast way to start an argument in a room full of programmers: “How do you implement a doubly linked list in Rust?” &lt;p&gt; At one level, this question is not very fair, because an answer to it as stated would be that one simply does not use doubly linked lists. They have been popular in introductory computer science lectures because they are a neat way to explain pointers in a data structure that's easy to draw on a whiteboard, but they are not a good match to modern hardware. The last time I used one was in the nineties. I know the Linux kernel uses them, but that design was also laid down in the nineties; if you were designing a kernel from scratch today, you would probably not do it that way. &lt;/p&gt;&lt;p&gt; At another level, it is fair because it is a simple, familiar proxy for all data structures with any kind of circular references. Consider a compiler holding a set of modules that may refer to each other. Or a game where objects may refer to their container. Or a graphic user interface where widgets may refer to a parent window. It might be reasonable to say some particular such structure is not the best solution in some particular case, but it is not reasonable to say that about all such structures in all cases. &lt;/p&gt;&lt;p&gt; And they are tricky in Rust because the language is founded on the idea that memory management should in general, be done by ownership. Essentially, Rust is an answer to the question, what happens if you start with C++ and encode the common ownership/RAII design patterns into the type system so fallible human brains don't need to enforce them. (And while we're at it, drop some of the legacy C baggage. And sprinkle in some features from ML family languages. And... okay, it's never really just one thing. But memory management is the focus here.) Circular references don't necessarily break ownership, but they do break the ability of the type system to keep track of it. &lt;/p&gt;&lt;p&gt; There are several ways to solve this problem. One way is to avoid using direct references to the particular class of objects at all. Instead, allocate a big array of objects, and refer to them with integer indexes into that array. There are several names that have been used for such arrays and indexes; let's call them arenas and handles. &lt;/p&gt;&lt;p&gt; At this point, smart programmers will spot what's going on. Essentially you are bypassing the notion of pointers provided directly by the hardware, only to reimplement your own address space, and your own notion of pointers within it. The next step of course is to write your own equivalents of &lt;code&gt;malloc&lt;/code&gt; and &lt;code&gt;free&lt;/code&gt; to allocate and deallocate objects within the arena.

&lt;/p&gt;&lt;p&gt; Doesn't that mean you are throwing out all the memory safety properties you were hoping to achieve by using Rust in the first place? Wouldn't you be as well off to just go back to C and at least be honest about the fact that you have reverted to purely manual memory management? &lt;/p&gt;&lt;p&gt; That argument sounds logical, but it's not actually correct. &lt;/p&gt;&lt;p&gt; Consider: why were we so scared of memory unsafety in the first place? What's so bad about memory safety bugs that it was considered worth inventing a whole new language to fix them? (Yes, as I mentioned earlier, Rust has some other nice properties, but those would not have sufficed to drive a movement to replace C++ with a new language. They are bonuses. Memory safety was the driving force behind Rust.) &lt;/p&gt;&lt;p&gt; Memory safety bugs have two properties that make them scarier than most other kinds. &lt;/p&gt;&lt;head rend="h2"&gt;Nondeterminism&lt;/head&gt; An array overflow, or use after free, is likely to manifest as an intermittent crash with no clear connection to the cause. Try to reproduce the problem in a debug build, maybe it goes away. Recompile with a couple of extra logging statements, maybe the crash goes away. Rerun the same binary with the same inputs, and thanks to ASLR, maybe the crash goes away. Maybe one day it shows a minute after the triggering cause, maybe another day it shows an hour after, or not at all. &lt;p&gt; Handles are deterministic. If a bug made your program crash on the last run, it will crash the same way on this run. &lt;/p&gt;&lt;head rend="h2"&gt;Security&lt;/head&gt; This one is even bigger. &lt;p&gt; If there exists an input that will cause a given C program to crash with a segmentation fault, what's the probability there exists another input that will allow remote code execution? In practice, the answer tends to be high, more than even expert intuition started off expecting. &lt;/p&gt;&lt;p&gt; And if the program is, say, a web browser, that can be bad. (It's not a coincidence Rust was invented by a browser company.) &lt;/p&gt;&lt;p&gt; There was a time you could say, okay but that only applies to the special category of security-critical programs. But these days (setting aside little scripts for personal use, assuming we are talking about published software), it's programs that don't have to cope with adversarial input, that are the special and shrinking category. This is true even of embedded systems; it's rare nowadays to find a smart device that doesn't expect to be connected to the Internet. &lt;/p&gt;&lt;p&gt; Given that the arena will still be subject to array bounds checking, handle bugs won't allow an attacker to overwrite arbitrary memory the way pointer bugs do. So using handles for your memory management, preserves the property, that bugs in your Rust code may lead to denial of service, but they are much less likely to lead to remote code execution. &lt;/p&gt;&lt;p&gt; And that is why, though arena memory management is isomorphic to old-fashioned manual memory management, it does not keep the particularly bad failure modes that motivated the move to Rust in the first place. &lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45467032</guid><pubDate>Fri, 03 Oct 2025 19:47:53 +0000</pubDate></item><item><title>AMD's EPYC 9355P: Inside a 32 Core Zen 5 Server Chip</title><link>https://chipsandcheese.com/p/amds-epyc-9355p-inside-a-32-core</link><description>&lt;doc fingerprint="6ef3d4bf70380ac1"&gt;
  &lt;main&gt;
    &lt;p&gt;High core count chips are headline grabbers. But maxing out the metaphorical core count slider isn’t the only way to go. Server players like Intel, AMD, and Arm aim for scalable designs that cover a range of core counts. Not all applications can take advantage of the highest core count models in their lineups, and per-core performance still matters.&lt;/p&gt;
    &lt;p&gt;AMD’s EPYC 9355P is a 32 core part. But rather than just being a lower core count part, the 9355P pulls levers to let each core count for more. First, it clocks up to 4.4 GHz. AMD has faster clocking chips in its server lineup, but 4.4 GHz is still a good bit higher than the 3.7 or 4.1 GHz that 128 or 192 core Zen 5 SKUs reach. Then, AMD uses eight CPU dies (CCDs) to house those 32 cores. Each CCD only has four cores enabled out of the eight physically present, but still has its full 32 MB of L3 cache usable. That provides a high cache capacity to core count ratio. Finally, each CCD connects to the IO die using a “GMI-Wide” setup, giving each CCD 64B/cycle of bandwidth to the rest of the system in both the read and write directions. GMI here stands for Global Memory Interconnect. Zen 5’s server IO die has 16 GMI links to support up to 16 CCDs for high core count parts, plus some xGMI (external) links to allow a dual socket setup. GMI-Wide uses two links per CCD, fully utilizing the IO die’s GMI links even though the EPYC 9355P only has eight CCDs.&lt;/p&gt;
    &lt;head rend="h1"&gt;Acknowledgments&lt;/head&gt;
    &lt;p&gt;Dell has kindly provided a PowerEdge R6715 for testing, and it came equipped with the aforementioned AMD EPYC 9355P along with 768 GB of DDR5-5200. The 12 memory controllers on the IO die provide a 768-bit memory bus, so the setup provides just under 500 GB/s of theoretical bandwidth. Besides providing a look into how a lower core count SKU behaves, we have BMC access which provides an opportunity to investigate different NUMA setups.&lt;/p&gt;
    &lt;p&gt;We’d also like to thank Zack and the rest of the fine folks at ZeroOne Technology for hosting the Dell PowerEdge R6715 at no cost to us.&lt;/p&gt;
    &lt;head rend="h1"&gt;Memory Subsystem and NUMA Characteristics&lt;/head&gt;
    &lt;p&gt;NPS1 mode stripes memory accesses across all 12 of the chip’s memory controllers, presenting software with a monolithic view of memory at the cost of latency. DRAM latency in that mode is slightly better than what Intel’s Xeon 6 achieves in SNC3 mode. SNC3 on Intel divides the chip into three NUMA nodes that correspond to its compute dies. The EPYC 9355P has good memory latency in that respect, but it falls behind compared to the Ryzen 9 9900X with DDR5-5600. Interconnects that tie more agents together tend to have higher latency. On AMD’s server platform, the Infinity Fabric network within the IO die has to connect up to 16 CCDs with 12 memory controllers and other IO, so the higher latency isn’t surprising.&lt;/p&gt;
    &lt;p&gt;Cache performance is similar across AMD’s desktop and server Zen 5 implementations, with the server variant only losing because of lower clock speeds. That’s not a surprise because AMD reuses the same CCDs on desktop and server products. But it does create a contrast to Intel’s approach, where client and server memory subsystems differ starting at L3. Intel trades L3 latency for capacity and the ability to a logical L3 instance across more cores.&lt;/p&gt;
    &lt;p&gt;Different NUMA configurations can subdivide EPYC 9355P, associating cores with the closest memory controllers to improve latency. NPS2 divides the chip into two hemispheres, and has 16 cores form a NUMA node with the six memory controllers on one half of the die. NPS4 divides the chip into quadrants, each with two CCDs and three memory controllers. Finally, the chip can present each CCD as a NUMA node. Doing so makes it easier to pin threads to cores that share a L3 cache, but doesn’t affect how memory is interleaved across channels. Memory addresses are still assigned to memory controllers according to the selected NPS1/2/4 scheme, which is a separate setting.&lt;/p&gt;
    &lt;p&gt;NPS2 and NPS4 only provide marginal latency improvements, and latency remains much higher than in a desktop platform. At the same time, crossing NUMA boundaries comes with little penalty. Apparently requests can traverse the huge IO die quite quickly, adding 20-30 ns at worst. I’m not sure what the underlying Infinity Fabric topology looks like, but the worst case unloaded cross-node latencies were under 140 ns. On Xeon 6, latency starts higher and can climb over 180 ns when cores on one compute die access memory controllers on the compute die at the other end of the chip.&lt;/p&gt;
    &lt;p&gt;EPYC 9355P can get close to theoretical memory bandwidth in any of the three NUMA nodes, as long as code keeps accesses within each node. NPS2 and NPS4 offer slightly better bandwidth, at the cost of requiring code to be NUMA aware. I tried to cause congestion on Infinity Fabric by having cores on each NUMA node access memory on another. That does lower achieved bandwidth, but not by a huge amount.&lt;/p&gt;
    &lt;p&gt;An individual NPS4 node achieves 117.33 GB/s to its local memory pool, and just over 107 GB/s to the memory on the other three nodes. The bandwidth penalty is minor, but a bigger potential pitfall is lower bandwidth to each NUMA node’s memory pool. Two CCDs can draw more bandwidth than the three memory controllers they’re associated with. Manually distributing memory accesses across NUMA nodes can improve bandwidth for a workload contained within one NUMA node’s cores. But doing so in practice may be an intricate exercise.&lt;/p&gt;
    &lt;p&gt;In general, EPYC 9355P has very mild NUMA characteristics and little penalty associated with running the chip in NPS1 or NPS2 mode. I imagine just using NPS1 mode would work well enough in the vast majority of cases, with little performance to be gained from carrying out NUMA optimizations.&lt;/p&gt;
    &lt;head rend="h1"&gt;Looking into GMI-Wide&lt;/head&gt;
    &lt;p&gt;GMI-Wide is AMD’s attempt to address bandwidth pinch points between CCDs and the rest of the system. With GMI-Wide, a single CCD can achieve 99.8 GB/s of read bandwidth, significantly more than the 62.5 GB/s from a Ryzen 9 9900X CCD with GMI-Narrow. GMI-Wide also allows better latency control under high bandwidth load. The Ryzen 9 9900X suffers from a corner case where a single core pulling maximum bandwidth can saturate the GMI-Narrow link and starve out another latency sensitive thread. That sends latency to nearly 500 ns, as observed by a latency test thread sharing a CCD with a thread linearly traversing an array. Having more threads generate bandwidth load seems to make QoS mechanisms kick in, which slightly reduces bandwidth throughput but brings latency back under control.&lt;/p&gt;
    &lt;p&gt;I previously wrote about loaded memory latency on the Ryzen 9 9950X when testing the system remotely, and thought it controlled latency well under high bandwidth load. But back then, George (Cheese) set that system up with very fast DDR5-8000 along with a higher 2.2 GHz FCLK. A single core was likely unable to monopolize off-CCD bandwidth in that setup, avoiding the corner case seen on my system. GMI-Wide increases off-CCD bandwidth by a much larger extent and has a similar effect. Under increasing bandwidth load, GMI-WIde can both achieve more total bandwidth and control latency better than its desktop single-link counterpart.&lt;/p&gt;
    &lt;p&gt;A read-modify-write pattern gets maximum bandwidth from GMI-Wide by exercising both the read and write paths. It doesn’t scale perfectly, but it’s a substantial improvement over using only reads or writes. A Ryzen 9 9900X CCD can theoretically get 48B/cycle to the IO die with a 2:1 read-to-write ratio. I tried modifying every other cacheline to achieve this ratio, but didn’t get better bandwidth probably because the memory controller is limited by a 32B/cycle link to Infinity Fabric. However, mixing in writes does get rid of the single bandwidth thread corner case, possibly because a single thread doesn’t saturate the 32B/cycle read link when mixing reads and writes.&lt;/p&gt;
    &lt;p&gt;On the desktop platform, latency under high load gets worse possibly because writes contend with reads at the DRAM controller. The DDR bus is unidirectional, and must waste cycles on “turnarounds” to switch between read and write mode. Bandwidth isn’t affected, probably because the Infinity Fabric bottleneck leaves spare cycles at the memory controller, which can absorb those turnarounds. However, reads from the latency test thread may be delayed while the memory controller drains writes before switching the bus back to read mode.&lt;/p&gt;
    &lt;p&gt;On the EPYC 9355P in NPS1 mode, bandwidth demands from a single GMI-Wide CCD leave plenty of spare cycles across the 12 memory controllers, so there’s little latency or bandwidth penalty when mixing reads and writes. The same isn’t true in NPS4 mode, where a GMI-Wide link can outmatch a NPS4 node’s three memory controllers. Everything’s fine with just reads, which actually benefit possibly because of lower latency and not having to traverse as much of the IO die. But with a read-modify-write pattern, bandwidth drops from 134 GB/s in NPS1 mode to 96.6 GB/s with NPS4. Latency gets worse too, rising to 248 ns. Again, NPS4 is something to be careful with, particularly if applications might require high bandwidth from a small subset of cores.&lt;/p&gt;
    &lt;head rend="h1"&gt;SPEC CPU2017&lt;/head&gt;
    &lt;p&gt;From a single thread perspective, the EPYC 9355P falls some distance behind the Ryzen 9 9900X. Desktop CPUs are designed around single threaded performance, so that’s to be expected. But with boost turned off on the desktop CPU to match clock speeds, performance is surprisingly close. Higher memory latency still hurts the EPYC 9355P, but it’s within striking distance.&lt;/p&gt;
    &lt;p&gt;NUMA modes barely make any difference. NPS4 technically wins, but by an insignificant margin. The latency advantage was barely measurable anyway. Compared to the more density optimized Graviton 4 and Xeon 6 6975P-C, the EPYC 9355P delivers noticeably better single threaded performance.&lt;/p&gt;
    &lt;p&gt;CCD-level bandwidth pinch points are worth a look too, since that’s traditionally been a distinguishing factor between AMD’s EPYC and more logically monolithic designs. Here, I’m filling a quad core CCD by running SPEC CPU2017’s rate tests with eight copies. I did the same on the Ryzen 9 9900X, pinning the eight copies to four cores and leaving the CCD’s other two cores unused. I bound the test to a single NUMA node on all tested setups.&lt;/p&gt;
    &lt;p&gt;SPEC’s floating point suite starts to tell a different story now. Several tests within the floating point suite are bandwidth hungry even from a single core. 549.fotonik3d for example pulled 28.23 GB/s from Meteor Lake’s memory controller when I first went through SPEC CPU2017’s tests. Running eight copies in parallel would multiply memory bandwidth demands, and that’s where server memory subsystems shine.&lt;/p&gt;
    &lt;p&gt;In 549.fotonik3d, high bandwidth demands make the Ryzen 9 9900X’s unloaded latency advantage irrelevant. The 9900X even loses to Redwood Cove cores on Xeon 6. The EPYC 9355P does very well in this test against both the 9900X and Xeon 6. Intel’s interconnect strategy tries to keep the chip logically monolithic and doesn’t have pinch points at cluster boundaries. But each core on Xeon 6 can only get to ~33 GB/s of DRAM bandwidth at best, using an even mix of reads and writes. AMD’s GMI-Wide can more than match that, and Intel’s advantage doesn’t show through in this scenario. However, Intel does have a potential advantage against more density focused AMD SKUs where eight cores sit in front of a narrower link.&lt;/p&gt;
    &lt;p&gt;NPS4 is also detrimental to the EPYC 9355P’s performance in this test. It only provides a minimal latency benefit at the cost of lower per-node bandwidth. The bandwidth part seems to hurt here, and taking the extra latency of striping accesses across 6 or 12 memory controllers gives a notable performance improvement.&lt;/p&gt;
    &lt;head rend="h1"&gt;Final Words&lt;/head&gt;
    &lt;p&gt;Core count isn’t the last word in server design. A lot of scenarios are better served by lower core count parts. Applications might not scale to fill a high core count chip. Bandwidth bound workloads might not benefit from adding cores. Traditionally lower core count server chips just traded core counts for higher clock speeds. Today, chips like the EPYC 9355P do a bit more, using both wider CCD-to-IOD links and more cache to maximize per-core performance.&lt;/p&gt;
    &lt;p&gt;Looking at EPYC 9355P’s NUMA characteristics reveals very consistent memory performance across NUMA modes. Intel’s Xeon 6 may be more monolithic from a caching point of view, but AMD’s DRAM access performance feels more monolithic than Intel’s. AMD made a tradeoff back in the Zen 2 days where they took lower local memory latency in exchange for more even memory performance across the socket. Measured latencies on EPYC 9355P are a bit higher than figures on the Zen 2 slide above. DDR5 is higher latency, and the Infinity Fabric topology is probably more complex these days to handle more CCDs and memory channels.&lt;/p&gt;
    &lt;p&gt;But the big picture remains. AMD’s Turin platform handles well in NPS1 mode, and cross-node penalties are low in NPS2/NPS4 modes. Those characteristics likely carry over across the Zen 5 EPYC SKU stack. It’s quite different from Intel’s Xeon 6 platform, which places memory controllers on compute dies like Zen 1 did. For now, AMD’s approach seems to be better at the DRAM level. Intel’s theoretical latency advantage in SNC3 mode doesn’t show through, and AMD gets to reap the benefits of a hub-and-spoke model while not getting hit where it should have downsides.&lt;/p&gt;
    &lt;p&gt;AMD seems to have found a good formula back in the Zen 2 days, and they’re content with reinforcing success. Intel is furiously iterating to find a setup that preserves a single level, logically monolithic interconnect while scaling well across a range of core counts. And of course, there’s Arm chips, which generally lean towards a single level monolithic interconnect too. It’ll be interesting to watch what all of these players do going forward as they continue to iterate and refine their designs.&lt;/p&gt;
    &lt;p&gt;And again, we’d like to thank both Dell and ZeroOne for, respectively, providing and hosting this PowerEdge R6715 without both of whom this article wouldn’t have been possible.&lt;/p&gt;
    &lt;p&gt;If you like the content then consider heading over to the Patreon or PayPal if you want to toss a few bucks to Chips and Cheese. Also consider joining the Discord.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45467166</guid><pubDate>Fri, 03 Oct 2025 20:01:36 +0000</pubDate></item><item><title>Sweden's National Bank Introduces Mandate for Offline Card Payments</title><link>https://www.riksbank.se/en-gb/press-and-published/notices-and-press-releases/press-releases/2025/offline-card-payments-should-be-possible-no-later-than-1-july-2026/</link><description>&lt;doc fingerprint="cb8cfa25092dfffd"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Offline card payments should be possible no later than 1 July 2026&lt;/head&gt;
    &lt;p&gt;Press release The Riksbank and representatives from the payment market have today reached an agreement to increase the possibility to make offline card payments for essential goods. The agreement is an important step in the work to strengthen Sweden's payment preparedness and increase resilience to disruptions in the digital payments system. The goal is for the measures to be in place no later than 1 July 2026.&lt;/p&gt;
    &lt;p&gt;“In Sweden, we pay digitally to a large degree and the use of cash is low. The general public being able to pay by card for example for food and medicines even in the event of a serious breakdown in data communication, that is offline, is a milestone in our intensified efforts to strengthen emergency preparedness”, says Governor Erik Thedéen.&lt;/p&gt;
    &lt;p&gt;The agreement describes the measures that participants in Swedish card payments – card issuers, card networks, card acquirers, the retail sector and the Riksbank – will implement to increase the possibility of offline payments by card. For instance, financial agents will adapt their regulatory frameworks, and the retail trade will introduce technological solutions. The Riksbank is leading this work and is responsible for monitoring its implementation.&lt;/p&gt;
    &lt;p&gt;“We are very pleased that all participants involved are taking responsibility for strengthening Sweden's payment readiness. Some are covered by the Riksbank's regulations, but far from all. We regard the fact that so many are nevertheless choosing to contribute as very positive for Sweden's overall civil preparedness”, concludes Erik Thedéen.&lt;/p&gt;
    &lt;p&gt;The online function shall apply to physical payment cards and accompanying PIN code when purchasing essential goods such as food, medicine and fuel. The Riksbank will continue its work on enabling offline payments for other payment methods after 1 July 2026.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45467500</guid><pubDate>Fri, 03 Oct 2025 20:36:03 +0000</pubDate></item><item><title>Interstellar Object 3I/Atlas Passed Mars Last Night</title><link>https://earthsky.org/space/new-interstellar-object-candidate-heading-toward-the-sun-a11pl3z/</link><description>&lt;doc fingerprint="fc5cf18231e00f9b"&gt;
  &lt;main&gt;
    &lt;p&gt;The world’s 3rd known interstellar object – 3I/ATLAS – has made its closest approach to Mars. The approach took place at 4 UTC on October 3, 2025 (11 p.m. CDT on October 2). At that time, the comet was approximately 18 million miles (29 million kilometers) from Mars. It was the object’s closest approach to any planet during its one-time journey through our solar system.&lt;/p&gt;
    &lt;p&gt;As of this writing (10 UTC on October 3), we have not seen any new images from the pass. But multiple space agencies, including NASA and the European Space Agency (ESA), are coordinating observations using various spacecraft and orbiters around Mars. Instruments on ESA’s Mars Express and ExoMars Trace Gas Orbiter, as well as NASA’s Mars Reconnaissance Orbiter, are focusing on capturing detailed data from this interstellar visitor. In an October 2 story from AP, Marcia Dunn reported:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Both of the European Space Agency’s satellites around Mars are already aiming their cameras at the comet, which is only the 3rd interstellar object known to have passed our way. NASA’s satellite and rovers at the red planet are also available to assist in the observations.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Previously, Marshall Eubanks of Space Initiatives had said:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;During the Mars close approach, the Mars Reconnaissance Orbiter will observe 3I with HiRISE, observing between 1 – 4 a.m. on October 2, and the CaSSIS camera on ESA’s Trace Gas Orbiter and the Mars Express’ High Resolution Stereo Camera will be observing on October 3.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;3I/ATLAS will reach perihelion, its closest point to the sun, on October 29, 2025. Its perihelion distance will be roughly 1.36 astronomical units (AU) from the sun – just inside the orbit of Mars.&lt;/p&gt;
    &lt;p&gt;If you’re interested in tracking the object, NASA’s Eyes on the Solar System tool offers interactive simulations of its path. Also, NASA just launched a new page devoted to 3I/ATLAS. And the latest updates on ESA observations are here&lt;/p&gt;
    &lt;p&gt;Please note that 3I/ATLAS will not be visible to the unaided eye from Earth at this Mars approach, or at any time. It will be possible to view the object with 8-inch (20 cm) or larger telescopes … but the best time for that won’t come until November. If you spot it then, you’ll be in good company. Between November 2 and 25, ESA’s Jupiter Icy Moons Explorer (Juice) will be observing the comet with various instruments. As Juice looks towards 3I/ATLAS so soon after its closest approach to the sun, it is likely to have the best view of the comet in a very active state, with a bright halo around its nucleus and a long tail stretching out behind it.&lt;/p&gt;
    &lt;head rend="h3"&gt;Interstellar object 3I/ATLAS: A look backward&lt;/head&gt;
    &lt;p&gt;Where did 3I/ATLAS come from? We know it came from the Sagittarius direction in our sky; that is, it came from the direction of the center of our Milky Way galaxy. But there are billions of stars in that direction. Which one is the home system of this object?&lt;/p&gt;
    &lt;p&gt;There have been many studies and ideas. One team of scientists, led by Xabier Pérez-Couto of the University of A Coruña in Spain, traced the path of interstellar object 3I/ATLAS back 10 million years. The astronomers were seeking its origin star, or any stars that might have perturbed its path as it traveled from its point of origin to our solar system.&lt;/p&gt;
    &lt;p&gt;The researchers examined 3I/ATLAS’s trajectory with the help of the Gaia space observatory’s data on stars. For 12 years, Gaia collected data on billions of stars in our Milky Way galaxy, precisely noting their positions again and again and thereby determining their motions. These astronomers’ calculations took them more than 100 million astronomical units (AU, or Earth-sun units) from our solar system. With these data in hand, researchers said they identified 93 nominal “encounters” for 3I/ATLAS, 62 of which were “significant.” Yet, they found that none of those encounters produced any meaningful perturbation of ATLAS’s orbit.&lt;/p&gt;
    &lt;p&gt;So, in other words, all of those 93 (or 62) encounters happened too fast, with the stars too far from 3I/ATLAS to meaningfully impact its trajectory. In the end, they didn’t find a star along 3I/ATLAS’s path that might have been responsible for bringing this 3rd-known interstellar object to us.&lt;/p&gt;
    &lt;head rend="h3"&gt;Tracing 3I/ATLAS’ path, a daunting task&lt;/head&gt;
    &lt;p&gt;And, as you might imagine, tracing 3I/ATLAS’ path backward through the galaxy is a daunting task. That’s in part because small uncertainties in orbits and stellar motions grow rapidly over time. But based on the researchers’ analyses of the interstellar object’s vertical motion in the galaxy (its path is known to weave up and down in the galactic disk), they concluded that it likely originated from the Milky Way’s thin disk, not its thick disk as was mentioned some months ago. The thin disk contains somewhat younger objects than the thick disk. But the researchers’ paper said:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;[3I/ATLAS] may nonetheless be an old object, consistent with ejection from a long-lived primordial planetesimal disk in an early-formed system.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The scientists published their not-yet peer-reviewed paper on arXiv on September 10, 2025.&lt;/p&gt;
    &lt;head rend="h3"&gt;Unlocking galactic mysteries with 3I/ATLAS&lt;/head&gt;
    &lt;p&gt;3I/ATLAS is thought to have been drifting through interstellar space for many billions of years before encountering our solar system. Pérez-Couto and team said that the interstellar comet is a:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;… key probe of the galactic population of icy planetesimals.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;In other words, the formation of solar systems is a messy process. In a solar system’s earliest days, rocks and pockets of gas and dust bang into each other and get swept up into clumps, which eventually get big enough to begin gathering yet more rocks, gas and dust to themselves via the force of gravity. Thus, planets come to be, astronomers think. According to theories of planet formation, clearing processes are also common, and those sometimes involve material – often the outer, icy regions of debris – getting ejected from a system altogether. As the paper said:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;… interstellar space should be filled with planetesimals.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;Other possibilities&lt;/head&gt;
    &lt;p&gt;Plus, there are other ways these interstellar interlopers might have achieved their lonely paths through our Milky Way galaxy. The possibilities range from close passages of other stars to tidal fragmentation of comets. So, as the paper said:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Identifying the origin of interstellar objects is key to understanding planet formation efficiency, the distribution of volatiles and organics in the galaxy, and the dynamical pathways by which planetary systems evolve.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;All that from a small chunk of icy stuff (we know it’s icy in part because 3I/ATLAS has formed a tail, as icy comets do)!&lt;/p&gt;
    &lt;head rend="h3"&gt;EarthSky interview with Colin Orion Chandler&lt;/head&gt;
    &lt;p&gt;On August 7, 2025, NASA shared an updated estimate of the size of the object’s nucleus, or core. Shortly after the object was first identified on July 1, 2025, 3I/ATLAS was estimated to have a diameter of about 12 miles (20 km). Then in late July – using data from the new Vera C. Rubin Observatory in Chile – the size estimate dropped to 6 miles (10 km). The latest analysis uses data from the NASA/ESA Hubble Space Telescope. It reduces the estimated diameter of 3I/ATLAS’s nucleus still further, to 3.5 miles (5.6 km).&lt;/p&gt;
    &lt;p&gt;And, the astronomers using Hubble data said, the object could be even smaller, as small as 1,050 feet (320 meters) across!&lt;/p&gt;
    &lt;p&gt;EarthSky’s Deborah Byrd interviewed Colin Orion Chandler of the DiRAC Institute of the University of Washington about size estimates for 3I/ATLAS. Watch in the player below, or on YouTube.&lt;/p&gt;
    &lt;p&gt;By the way, the two previously known interstellar objects are 1I/ ‘Oumuamua and 2I/Borisov. ‘Oumuamua’s size is thought to be about 656 feet (200 meters) across at its widest (you’ll recall it has an elongated shape). And Borisov is thought to be less than 3,280 feet (1 km) across.&lt;/p&gt;
    &lt;head rend="h3"&gt;An early EarthSky interview with Matthew Hopkins&lt;/head&gt;
    &lt;p&gt;Shortly after the discovery of 3I/ATLAS – on July 1, 2025 – astronomers were saying it was likely the oldest comet we’ve ever seen. That claim came from University of Oxford astronomer Matthew Hopkins, whose analysis suggested 3I/ATLAS might be more than 7 billion years old, predating our solar system by more than 3 billion years! Hear him explain in the player below, or on YouTube.&lt;/p&gt;
    &lt;head rend="h3"&gt;EarthSky interview with Colin Snodgrass&lt;/head&gt;
    &lt;p&gt;Scientists first spotted 3I/ATLAS in early July 2025. And since then, one question has been asked countless times: will we send out a spacecraft to take a closer look? EarthSky’s Will Triggs spoke to University of Edinburgh astronomer Colin Snodgrass on August 21, 2025, to find out the answer. Colin essentially said, no, we don’t have time to organize a space mission specifically for 3I/ATLAS. But he talked about a future mission, the European Space Agency’s Comet Interceptor. This upcoming spacecraft will be primed to intercept future interstellar objects. Watch Will’s interview with Colin in the player below, or on YouTube.&lt;/p&gt;
    &lt;p&gt;It’s worth noting that the behavior of 3I/ATLAS is much like the signature of previously seen sun-bound comets originating within our solar system. But 3I/ATLAS is moving fast. In fact, it’s traveling through our solar system at roughly 130,000 miles per hour (210,000 kph). That’s the highest velocity ever recorded for a solar system visitor.&lt;/p&gt;
    &lt;head rend="h3"&gt;How they spotted interstellar object 3I/ ATLAS&lt;/head&gt;
    &lt;p&gt;The Asteroid Terrestrial-impact Last Alert System (ATLAS) – a system of survey telescopes – detected our new interstellar visitor on July 1, 2025. And the Minor Planet Center confirmed its interstellar nature the following day (July 2, 2025), naming it 3I/ATLAS (or C/2025 N1). The “3I” means it’s the 3rd interstellar visitor that we’ve found. Its trajectory and speed revealed it as an object not from our solar system, but from another star system.&lt;/p&gt;
    &lt;p&gt;The Hubble Space Telescope imaged the object on July 21, 2025. See the post from Bluesky below.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Hubble Space Telescope images of interstellar comet 3I/ATLAS are out! These were taken 5 hours ago. Plenty of cosmic rays peppering the images, but the comet's coma looks very nice and puffy. Best of luck to the researchers trying to write up papers for this… archive.stsci.edu/proposal_sea… ?&lt;/p&gt;
      &lt;p&gt;— astrafoxen (@astrafoxen.bsky.social) July 21, 2025 at 4:28 PM&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;It’s still heading sunward&lt;/head&gt;
    &lt;p&gt;Our new visitor will get its closest to the sun – at about 2 astronomical units (AU), or twice as far as Earth is from the sun – in October. As it reaches perihelion – its closest point to the sun – it will be traveling at almost 15,500 miles per hour (25,000 kph).&lt;/p&gt;
    &lt;p&gt;The speedy nature of Comet 3I/ATLAS is another indication of its interstellar nature. It has to be moving at a blistering pace in order to escape the sun’s gravitational pull.&lt;/p&gt;
    &lt;p&gt;Marshall Eubanks, a physicist and Very-long-baseline interferometry radio astronomer and co-founder of Space Initiatives, said the comet will come within about 0.4 AU of Mars in October. That would make it just barely observable by the Mars Reconnaissance Orbiter.&lt;/p&gt;
    &lt;head rend="h3"&gt;Morning star charts here&lt;/head&gt;
    &lt;p&gt;After Comet 3I/ATLAS makes its close approach to the sun, you can find it in the morning sky.&lt;/p&gt;
    &lt;p&gt;Bottom line: Interstellar object 3I/ATLAS swept closest to Mars at 11 p.m. CDT on October 2 (4 UTC on October 3). Read about plans to observe it with spacecraft.&lt;lb/&gt; Via:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45467543</guid><pubDate>Fri, 03 Oct 2025 20:40:52 +0000</pubDate></item><item><title>TrueVault (YC W14) Is Hiring a BDR (Ex-ECommerce Manager)</title><link>https://www.ycombinator.com/companies/truevault/jobs/FaC8Apo-ecommerce-manager-bdr</link><description>&lt;doc fingerprint="2d0ed018ae96a6ad"&gt;
  &lt;main&gt;
    &lt;p&gt;We make Privacy Software for SMBs.&lt;/p&gt;
    &lt;p&gt;We’re looking for a Business Development Rep with real DTC eCommerce experience to help brands simplify privacy compliance. If you’ve been an eCommerce Manager before and want to pivot into SaaS sales, this is your chance to turn that experience into a revenue-driving role at a Y Combinator–backed startup.&lt;/p&gt;
    &lt;p&gt;Privacy is one of the most fundamental rights we have.&lt;/p&gt;
    &lt;p&gt;At TrueVault, we believe that when businesses have access to tools that make compliance simple, respecting consumer privacy becomes the obvious choice — and everyone wins. That’s why we build software that helps brands comply with complex privacy laws without drowning in legal costs or red tape.&lt;/p&gt;
    &lt;p&gt;We’ve cracked the code on making compliance something companies can handle themselves — no armies of lawyers required.&lt;/p&gt;
    &lt;p&gt;We’re a Y Combinator–backed startup based in San Francisco, obsessed with building products that solve hard problems and relentless about making our customers successful.&lt;/p&gt;
    &lt;p&gt;Joining TrueVault as a Business Development Representative (BDR) offers a unique opportunity to combine your first-hand DTC eCommerce experience with building relationships and driving revenue for a fast-growing SaaS company.&lt;/p&gt;
    &lt;p&gt;We aren’t looking for a traditional BDR. Instead, we want someone who’s been in the shoes of our customers — specifically, someone who has worked as an eCommerce Manager at a DTC brand. You understand the realities of running an online store, the pressure of growth targets, and the complexity of compliance — because you’ve lived it. Now, you’ll get to use that knowledge to connect with peers, build trust, and introduce them to how TrueVault can make their lives easier.&lt;/p&gt;
    &lt;p&gt;This role will give you exposure to a high-performing sales team, hands-on coaching, and a front-row seat to scaling a Y Combinator-backed startup.&lt;/p&gt;
    &lt;p&gt;We are seeking a Business Development Representative (BDR) with direct experience as an eCommerce Manager at a DTC brand. This is a sales development role — you’ll be on the front lines reaching out to prospective customers via emails, calls and social media, engaging them in conversations about their compliance challenges, and booking discovery meetings for our Account Executives.&lt;/p&gt;
    &lt;p&gt;Your unique background as an eCommerce Manager will allow you to connect authentically with prospects, understand their day-to-day pain points, and speak credibly about the value TrueVault brings.&lt;/p&gt;
    &lt;p&gt;This is a fully remote role, but you must be based in the United States.&lt;/p&gt;
    &lt;p&gt;This role is designed to be a launchpad into SaaS sales. High performers will have the opportunity to advance into Account Executive roles within 12–18 months, with higher OTE potential and greater ownership of deals. You’ll gain direct exposure to closing strategies, complex sales cycles, and leadership visibility that accelerates your career trajectory.&lt;/p&gt;
    &lt;p&gt;You’ll work directly with our CEO, who heads up our GTM team, as well as collaborating closely with Account Executives and Partnerships. This means hands-on coaching, direct mentorship, and visibility into how strategic deals are structured and closed.&lt;/p&gt;
    &lt;p&gt;We believe in setting clear, achievable expectations. Success in this role will be measured by:&lt;/p&gt;
    &lt;p&gt;This role offers $100,000 OTE, comprised of:&lt;/p&gt;
    &lt;p&gt;Please submit your application through WaaS.&lt;/p&gt;
    &lt;p&gt;Attach your resume in PDF format (no other formats will be accepted).&lt;/p&gt;
    &lt;p&gt;In the “What interests you about TrueVault” text box, include the following:&lt;/p&gt;
    &lt;p&gt;Why we ask for this: Writing persuasive outreach emails is a core part of the BDR role. This exercise isn’t about creating a “perfect” email — it’s about showing us your approach, how you think about personalization, and how you’d engage a real prospect. Keep it short, natural, and in your own voice.&lt;/p&gt;
    &lt;p&gt;We take applicant privacy seriously. Please review our Job Applicant Privacy Policy to understand how we collect and process your personal information in accordance with applicable laws, including the CCPA.&lt;/p&gt;
    &lt;p&gt;TrueVault builds software tools that help businesses comply with consumer data privacy laws. We believe if businesses have access to products that make getting and staying compliant simple, straightforward, and fully automated, respecting consumers' data privacy becomes the sensible default. And we all benefit from that.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45467717</guid><pubDate>Fri, 03 Oct 2025 21:00:32 +0000</pubDate></item></channel></rss>