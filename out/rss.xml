<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Wed, 01 Oct 2025 21:32:46 +0000</lastBuildDate><item><title>Show HN: ChartDB Agent – Cursor for DB schema design</title><link>https://app.chartdb.io/ai</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=45437594</guid><pubDate>Wed, 01 Oct 2025 13:38:36 +0000</pubDate></item><item><title>Unix philosophy and filesystem access makes Claude Code amazing</title><link>https://www.alephic.com/writing/the-magic-of-claude-code</link><description>&lt;doc fingerprint="370009d837378e9b"&gt;
  &lt;main&gt;
    &lt;p&gt;Noah Brier, September 30, 2025&lt;/p&gt;
    &lt;p&gt;If you've talked to me lately about AI, you've almost certainly been subject to a long soliloquy about the wonders of Claude Code. What started as a tool I ran in parallel with other tools to aid coding has turned into my full-fledged agentic operating system, supporting all kinds of workflows.&lt;/p&gt;
    &lt;p&gt;Most notably, Obsidian, the tool I use for note-taking. The difference between Obsidian and Notion or Evernote is that all the files are just plain old Markdown files stored on your computer. You can sync, style, and save them, but ultimately, it's still a text file on your hard drive. A few months ago, I realized that this fact made my Obsidian notes and research a particularly interesting target for AI coding tools. What first started with trying to open my vault in Cursor quickly moved to a sort of note-taking operating system that I grew so reliant on, I ended up standing up a server in my house so I could connect via SSH from my phone into my Claude Code + Obsidian setup and take notes, read notes, and think through things on the go.&lt;/p&gt;
    &lt;p&gt;A few weeks ago, I went on Dan Shipper's AI &amp;amp; I Podcast to wax poetic about my love for this setup. I did a pretty deep dive into the system I use, how it works, why it works, etc. I won't retread all those details—you can read the transcript or listen to the podcast—but I want to talk about a few other things related to Claude Code that I've come to realize since the conversation.&lt;/p&gt;
    &lt;p&gt;I've really struggled to answer this question. I'm also not sure it's better than Cursor for all things, but I do think there are a set of fairly exceptional pieces that work together in concert to make me turn to Claude Code whenever I need to build anything these days. Increasingly, that's not even about applying it to existing codebases as much as it's building entirely new things on top of its functionality (more on that in a bit).&lt;/p&gt;
    &lt;p&gt;So what's the secret? Part of it lies in how Claude Code approaches tools. As a terminal-based application, it trades accessibility for something powerful: native Unix command integration. While I typically avoid long blockquotes, the Unix Philosophy deserves an exception—Doug McIlroy's original formulation captures it perfectly:&lt;/p&gt;
    &lt;p&gt;The Unix philosophy is documented by Doug McIlroy in the Bell System Technical Journal from 1978:&lt;/p&gt;
    &lt;p&gt;It was later summarized by Peter H. Salus in A Quarter-Century of Unix (1994):&lt;/p&gt;
    &lt;p&gt;These fifty-year-old principles are exactly how LLMs want to use tools. If you look at how these models actually use the tools they're given, they are constantly "piping" output to input (albeit using their own fuzziness in between). (As an aside, the Unix | command allows you to string the output from one command into the input of another.) When models fail to weld their tools effectively, it is almost always because the tools are overly complex.&lt;/p&gt;
    &lt;p&gt;So part one of why Claude Code can be so mind-blowing is that the commands that power Unix happen to be perfectly suited for use by LLMs. This is both because they're simple and also incredibly well-documented, meaning the models had ample source material to teach them the literal ins and outs.&lt;/p&gt;
    &lt;p&gt;But that still wasn't the whole thing. The other piece was obviously Claude Code's ability to write code initially and, more recently, prose (for me, at least). But while other applications like ChatGPT and Claude can write output, there was something different going on here. Last week, while reading The Pragmatic Engineer's deep dive into how Claude Code is built. The answer was staring me in the face: filesystem access.&lt;/p&gt;
    &lt;p&gt;The filesystem changes everything. ChatGPT and Claude in the browser have two fatal flaws: no memory between conversations and a cramped context window. A filesystem solves both. Claude Code writes notes to itself, accumulates knowledge, and keeps running tallies. It has state and memory. It can think beyond a single conversation.&lt;/p&gt;
    &lt;p&gt;Back in 2022, when I first played with the GPT-3 API, I said that even if models never got better than they were in that moment, we would still have a decade to discover the use cases. They did get better—reasoning models made tool calling reliable—but the filesystem discovery proves my point.&lt;/p&gt;
    &lt;p&gt;I bring this up because in the Pragmatic Engineer interview, Boris Cherney, who built the initial version of Claude Code, uses it to describe the aha:&lt;/p&gt;
    &lt;p&gt;In AI, we talk about “product overhang”, and this is what we discovered with the prototype. Product overhang means that a model is able to do a specific thing, but the product that the AI runs in isn’t built in a way that captures this capability. What I discovered about Claude exploring the filesystem was pure product overhang. The model could already do this, but there wasn’t a product built around this capability!&lt;/p&gt;
    &lt;p&gt;Again, I'd argue it's filesystem + Unix commands, but the point is that the capability was there in the model just waiting to be woken up, and once it was, we were off to the races. Claude Code works as a blueprint for building reliable agentic systems because it captures model capabilities instead of limiting them through over-engineered interfaces.&lt;/p&gt;
    &lt;p&gt;I talked about my Claude Code + Obsidian setup, and I've actually taken it a step further by open-sourcing "Claudesidian," which pulls in a bunch of the tools and commands I use in my own Claude Code + Obsidian setup. It also goes beyond that and was a fun experimental ground for me. Most notably, I built an initial upgrade tool so that if changes are made centrally, you can pull them into your own Claudesidian, and the AI will help you check to see if you've made changes to the files being updated and, if so, attempt to smartly merge your changes with the new updates. Both projects follow the same Unix philosophy principles—simple, composable tools that do one thing well and work together. This is the kind of stuff that Claude Code makes possible, and why it's so exciting for me as a new way of building applications.&lt;/p&gt;
    &lt;p&gt;Speaking of which, one I'm not quite ready to release, but hopefully will be soon, is something I've been calling "Inbox Magic," though I'll surely come up with a better name. It's a Claude Code repo with access to a set of Gmail tools and a whole bunch of prompts and commands to effectively start operating like your own email EA. Right now, the functionality is fairly simple: it can obviously run searches or send emails on your behalf, but it can also do things like triage and actually run a whole training run on how you sound over email so it can more effectively draft emails for you. While Claude Code and ChatGPT both have access to my emails, they mostly grab one or two at a time. This system, because it can write things out to files and do lots of other fancy tricks, can perform a task like “find every single travel-related email in my inbox and use that to build a profile of my travel habits that I can use as a prompt to help ChatGPT/Claude do travel research that's actually aligned with my preferences.” Anyway, more on this soon, and if it's something you want to try out, ping me with your GitHub username, and as soon as I feel like I have something ready to test, I'll happily share it.&lt;/p&gt;
    &lt;p&gt;While I generally shy away from conclusions, I think there are a few here worth reiterating.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45437893</guid><pubDate>Wed, 01 Oct 2025 14:05:45 +0000</pubDate></item><item><title>Show HN: Autism Simulator</title><link>https://autism-simulator.vercel.app/</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=45438346</guid><pubDate>Wed, 01 Oct 2025 14:48:31 +0000</pubDate></item><item><title>Building the heap: racking 30 petabytes of hard drives for pretraining</title><link>https://si.inc/posts/the-heap/</link><description>&lt;doc fingerprint="2fd91d7735ac7f8d"&gt;
  &lt;main&gt;
    &lt;p&gt;We built a storage cluster in downtown SF to store 90 million hours worth of video data. Why? We’re pretraining models to solve computer use. Compared to text LLMs like LLaMa-405B, which require ~60 TB of text data to train, videos are sufficiently large that we need 500 times more storage. Instead of paying the $12 million / yr it would cost to store all of this on AWS, we rented space from a colocation center in San Francisco to bring that cost down ~40x to $354k per year, including depreciation.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why&lt;/head&gt;
    &lt;p&gt;Our use case for data is unique. Most cloud providers care highly about redundancy, availability, and data integrity, which tends to be unnecessary for ML training data. Since pretraining data is a commodity—we can lose any individual 5% with minimal impact—we can handle relatively large amounts of data corruption compared to enterprises who need guarantees that their user data isn’t going anywhere. In other words, we don’t need AWS’s 13 nines of reliability; 2 is more than enough.&lt;/p&gt;
    &lt;p&gt;Additionally, storage tends to be priced substantially above cost. Most companies use relatively small amounts of storage (even ones like Discord still use under a petabyte for messages), and the companies that use petabytes are so large that storage remains a tiny fraction of their total compute spend.&lt;/p&gt;
    &lt;p&gt;Data is one of our biggest contraints, and would be prohibitively expensive otherwise. As long as the cost predictions work out in favor of a local datacenter, and it would not consume too much of the core team’s time, it would make sense to stack hard drives ourselves. [1] 1. We talked to some engineers at the Internet Archive, which had basically the same problem as us; even after massive friends &amp;amp; family discounts on AWS, it was still 10 times more cost-effective to buy racks and store the data themselves!&lt;/p&gt;
    &lt;head rend="h2"&gt;The Cost Breakdown: Cloud Alternatives vs In-House&lt;/head&gt;
    &lt;p&gt;Internet and electricity total $17.5k as our only recurring expenses (the price of colocation space, cooling, etc were bundled into electricity costs). One-time costs were dominated by hard drive capex. [2] 2. When deciding the datacenter location we had multiple options across the Bay Area, including options in Fremont through Hurricane Electric for around $10k in setup fees and $12.8k per month, saving us $38.5k initially and $4.7k per month, but ended up opting for a datacenter that was only a couple blocks from our office in SF. Though this came at a premium, it was extremely helpful to get the initial nodes setup and for ongoing maintenance. Our team is just 5 people, so any friction in going to the datacenter would come at a noticeable cost to team productivity.&lt;/p&gt;
    &lt;p&gt;Table 1: Cost comparison of cloud alternatives vs in-house. AWS is $1,130,000/month including estimated egress, Cloudflare is $270,000/month (with bulk-discounted pricing), and our datacenter is $29,500/month (including recurring costs and depreciation).&lt;/p&gt;
    &lt;head rend="h3"&gt;Monthly Recurring Costs&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Item&lt;/cell&gt;
        &lt;cell role="head"&gt;Cost&lt;/cell&gt;
        &lt;cell role="head"&gt;Notes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Internet&lt;/cell&gt;
        &lt;cell&gt;$7,500/month&lt;/cell&gt;
        &lt;cell&gt;100Gbps DIA from Zayo, 1yr term.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Electricity&lt;/cell&gt;
        &lt;cell&gt;$10,000/month&lt;/cell&gt;
        &lt;cell&gt;1 kW/PB, $330/kW. Includes cabinet space &amp;amp; cooling. 1yr term.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Total Monthly&lt;/cell&gt;
        &lt;cell&gt;$17,500/month&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;One-Time Costs&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Category&lt;/cell&gt;
        &lt;cell role="head"&gt;Item&lt;/cell&gt;
        &lt;cell role="head"&gt;Cost&lt;/cell&gt;
        &lt;cell role="head"&gt;Details&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Storage&lt;/cell&gt;
        &lt;cell&gt;Hard drives (HDDs)&lt;/cell&gt;
        &lt;cell&gt;$300,000&lt;/cell&gt;
        &lt;cell&gt;2,400 drives. Mostly 12TB used enterprise drives (3/4 SATA, 1/4 SAS). The JBOD DS4246s work for either.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Storage Infrastructure&lt;/cell&gt;
        &lt;cell&gt;NetApp DS4246 chassis&lt;/cell&gt;
        &lt;cell&gt;$35,000&lt;/cell&gt;
        &lt;cell&gt;100 dual SATA/SAS chassis, 4U each&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Compute&lt;/cell&gt;
        &lt;cell&gt;CPU head nodes&lt;/cell&gt;
        &lt;cell&gt;$6,000&lt;/cell&gt;
        &lt;cell&gt;10 Intel RR2000s from eBay&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Datacenter Setup&lt;/cell&gt;
        &lt;cell&gt;Install fee&lt;/cell&gt;
        &lt;cell&gt;$38,500&lt;/cell&gt;
        &lt;cell&gt;One-off datacenter install fee&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Labor&lt;/cell&gt;
        &lt;cell&gt;Contractors&lt;/cell&gt;
        &lt;cell&gt;$27,000&lt;/cell&gt;
        &lt;cell&gt;Contractors to help physically screw in / install racks and wire cables&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Networking &amp;amp; Misc&lt;/cell&gt;
        &lt;cell&gt;Install expenses&lt;/cell&gt;
        &lt;cell&gt;$20,000&lt;/cell&gt;
        &lt;cell&gt;Power cables, 100GbE QSFP CX4 NICs, Arista router, copper jumpers, one-time internet install fee&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Total One-Time&lt;/cell&gt;
        &lt;cell&gt;$426,500&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Our price assuming three-year depreciation (including for the one-off install fees) is $17.5k/month in fixed monthly costs (internet, power, etc.) and $12k/month in depreciation, for $29.5k/month overall.&lt;/p&gt;
    &lt;p&gt;We compare our costs to two main providers: AWS’s public pricing numbers as a baseline, and Cloudflare’s discounted pricing for 30PB of storage. It’s important to note that AWS egress would be substantially lower if we utilized AWS GPUs. This is not reflected on our graph because AWS GPUs are priced at substantially above market prices and large clusters are difficult to attain, untenable at our compute scales.&lt;/p&gt;
    &lt;p&gt;Here are the pricing breakdowns:&lt;/p&gt;
    &lt;head rend="h3"&gt;AWS Pricing Breakdown&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Cost Component&lt;/cell&gt;
        &lt;cell role="head"&gt;Rate&lt;/cell&gt;
        &lt;cell role="head"&gt;Monthly Cost&lt;/cell&gt;
        &lt;cell role="head"&gt;Notes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Storage&lt;/cell&gt;
        &lt;cell&gt;$0.021/GB/month&lt;/cell&gt;
        &lt;cell&gt;$630,000&lt;/cell&gt;
        &lt;cell&gt;For data over 500TB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Egress&lt;/cell&gt;
        &lt;cell&gt;$0.05/GB&lt;/cell&gt;
        &lt;cell&gt;$500,000&lt;/cell&gt;
        &lt;cell&gt;Entire dataset egressed quarterly (10 PB/month)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Total AWS Monthly&lt;/cell&gt;
        &lt;cell&gt;$1,130,000&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Cloudflare R2 Pricing&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Pricing Tier&lt;/cell&gt;
        &lt;cell role="head"&gt;Rate&lt;/cell&gt;
        &lt;cell role="head"&gt;Monthly Cost&lt;/cell&gt;
        &lt;cell role="head"&gt;Notes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Published Rate&lt;/cell&gt;
        &lt;cell&gt;$0.015/GB/month&lt;/cell&gt;
        &lt;cell&gt;$450,000&lt;/cell&gt;
        &lt;cell&gt;No egress fees&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Estimated Private Pricing [3] 3. Cloudflare has a more reasonable estimate for the 30 PB, placing it at an overall monthly cost of $270k without egress fees. We also have bulk-discounted pricing estimates after getting pricing quotes—this was our main point of comparison for the datacenter.&lt;/cell&gt;
        &lt;cell&gt;$0.009/GB/month&lt;/cell&gt;
        &lt;cell&gt;$270,000&lt;/cell&gt;
        &lt;cell&gt;Estimated rate for &amp;gt;20 PB scale&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;That brings monthly costs to $38/TB/month for AWS, $10/TB/month for Cloudflare, and $1/TB/month for our datacenter—about 38x lower and 10x lower respectively. (At the very cheapest end of the spectrum, Backblaze has a $6/TB product that is unsuitable for model training due to egress speed limitations; their $15/TB Overdrive AI-specific storage product is closer to Cloudflare’s in price &amp;amp; performance)&lt;/p&gt;
    &lt;p&gt;While we use Cloudflare as a comparison point, we’ve sometimes done too much load for their R2 servers. In particular, in the past we’ve done enough load during large model training runs that they rate-limited us, later confirming we were saturating their metadata layer and the rate limit wasn’t synthetic. Because our metadata on the heap is so simple, and we have a 100Gbps DIA connection, we haven’t ran into any issues there. [4] 4. We love Cloudflare and use many of their products often; we include this anecdote as a fact about our scale being difficult to handle, not as a dig!&lt;/p&gt;
    &lt;p&gt;This setup was and is necessary for our video data pipelines, and we’re extremely happy that we made this investment. By gathering large scale data at low costs, we can be competitive with frontier labs with billions of dollars in capital.&lt;/p&gt;
    &lt;head rend="h2"&gt;Setup/The Process&lt;/head&gt;
    &lt;p&gt;We cared a lot about getting this built fast, because this kind of project can easily stretch on for months if not careful. Hence Storage Stacking Saturday, or S3. We threw a hard drive stacking party in downtown SF and got our friends to come, offering food and custom-engraved hard drives to all who helped. The hard drive stacking started at 6am and continued for 36 hours (with a break to sleep), and by the end of that time we had 30 PB of functioning hardware racked and wired up. We brought in contractors for additional help and professional installation later on in the event.&lt;/p&gt;
    &lt;p&gt;People at the hard drive stacking party! Cool shots of the servers&lt;/p&gt;
    &lt;p&gt;Our software is 200 lines of Rust code for writing (to determine the drive to write data onto) and a nginx webserver for reading data, with a simple SQLite db for tracking metadata like which heap node each file is on and what data split it belongs to. We kept this obsessively simple instead of using MinIO or Ceph because we didn’t need any of the features they provided; it’s much, much simpler to debug a 200-line program than to debug Ceph, and we weren’t worried about redundancy or sharding. All our drives were formatted with XFS.&lt;/p&gt;
    &lt;p&gt;The storage software landscape offers many options, but every option available comes with drawbacks. People experienced with Ceph strongly warned us to avoid it unless we were willing to hire dedicated Ceph specialists—our research confirmed this advice. Ceph appears far more complex than justified for most use cases, only worthwhile for companies that absolutely need maximum performance and customizability and are prepared to invest heavily in tuning. Minio presents an interesting option if S3 compatibility is essential, but otherwise remains a bit too fancy for us and similar use-cases. Weka and Vast are absurdly expensive at 2k / TB / year or so and are primarily designed for NVMEs, not spinning disks.&lt;/p&gt;
    &lt;head rend="h2"&gt;Post-Mortem&lt;/head&gt;
    &lt;p&gt;Building the datacenter was a large endeavor and we definitely learned lessons, both good and bad.&lt;/p&gt;
    &lt;head rend="h3"&gt;Things That We Got Correct&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;We think the redundancy &amp;amp; capability tradeoffs we made are very reasonable at our disk speeds. We’re able to approximately saturate our 100G network for both read &amp;amp; write.&lt;/item&gt;
      &lt;item&gt;Doing this locally a couple blocks away was well worth it because of the amount of debugging and manual work needed.&lt;/item&gt;
      &lt;item&gt;Ebay is good to find vendors but bad to actually buy things with. After finding vendors, they can often individually supply all the parts we need and provide warranties, which are extremely valuable.&lt;/item&gt;
      &lt;item&gt;100G dedicated internet is pretty important, and much much easier to debug issues with than using cloud products.&lt;/item&gt;
      &lt;item&gt;Having high-quality cable management during the racking process saved us a ton of time debugging in the long run; making it easy to switch up the networking saved us a lot of headache.&lt;/item&gt;
      &lt;item&gt;We had a very strong simplicity prior, and this saved an immense amount of effort. We are quite happy that we didn’t use ceph or minio. Unlike e.g. nginx, they do not work out of the box. We were willing to write a simple Rust script and roughly saturated our network read &amp;amp; write at 100 Gbps without any fancy code.&lt;/item&gt;
      &lt;item&gt;We were basically right about the price and advantages this offered, and did not substantially overestimate the amount of time / effort it would take. While the improvements list is longer than this, most of those are minor; fundamentally we built a cluster rivaling massive clouds for 40x cheaper.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Difficult Bits&lt;/head&gt;
    &lt;p&gt;A map of reality only gets you so far—while setting up the datacenter we ran into a couple problems and unexpected challenges. We’ll include a list:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;We used frontloaders instead of toploaders for our server rack. This meant we had to screw every single individual drive in—tedious for 2.4k HDDs&lt;/item&gt;
      &lt;item&gt;Our storage was not dense—we could have saved 5x the work on physical placement and screwing by having a denser array of hard drives&lt;/item&gt;
      &lt;item&gt;Shortcuts like daisy-chaining are usually a bad idea. We could have gotten substantially higher read/write speeds without daisy chaining networked nodes, giving each chassis its own HBA (Host Bus Adapter, not a significant cost).&lt;/item&gt;
      &lt;item&gt;Compatibility is key—specifically in networking functionally everything is locked to a specific brand. We had many pain points here. Fiber transceivers will ~never work unless used with the right brand, but copper cables are much more forgiving. FS.com is pretty good and well priced (though their speed estimates were pretty inconsistent); Amazon will also often have the parts you need rapidly.&lt;/item&gt;
      &lt;item&gt;Networking came at substantial cost and required experimentation. In general, with our relatively non-sensitive training data, we optimized for convenience and ease of use over all else: we did not use DHCP as our used enterprise switches didn’t support it out of the box, and we didn’t use NAT as we wanted public IPs for the nodes for convenient and performant access from our servers. (We firewalled off unused ports and had basic security with nginx secure_link; we would not be able to do this if handling customer data, but it was fine for our use case.) While this is an area where we would have saved time with a cloud solution, we had our networking up within days and kinks ironed out within ~3 weeks.&lt;/item&gt;
      &lt;item&gt;We were often bottlenecked by easy access to servers via monitor/keyboard; idle crash carts during setup are helpful.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Ideas Worth Trying&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Working KVMs are extremely useful, and you shouldn’t go without them or good IPMI. Physically going to a datacenter is really inconvenient, even if it’s a block away. IPMI is good, but only if you have pretty consistent machines.&lt;/item&gt;
      &lt;item&gt;Think through your management Ethernet network as much as your real network - it’s really nice to be able to SSH into servers while configuring the network, and IPMI is great!&lt;/item&gt;
      &lt;item&gt;Overprovision your network—e.g. if doable it’s worth having 400 Gigabit internally (you can use 100G cards etc for this!)&lt;/item&gt;
      &lt;item&gt;We could have substantially increased density at additional upfront cost by buying 90-drive SuperMicro SuperServers and putting 20TB drives into them. This would allow us to use 2 racks instead of 10, give us about the equivalent of 20 AMD 9654s in total CPU capacity, and use less total power.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;How You Can Build This Yourself&lt;/head&gt;
    &lt;p&gt;Here’s what you need to replicate our setup.&lt;/p&gt;
    &lt;head rend="h3"&gt;Storage&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;10 CPU head nodes.&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;We used Intel Rr2000 with Dual Intel Gold 6148 and 128GB of DDR4 ECC RAM per server (which are incredibly cheap and roughly worked for our use cases) but you have a lot of flexibility in what you use.&lt;/item&gt;
          &lt;item&gt;If you use the above configuration you likely won’t be able to do anything at all CPU-intensive on the servers (like on-device data processing or ZFS data compression / deduplication / etc, which is valuable if you’re storing non-video data).&lt;/item&gt;
          &lt;item&gt;Our CPU nodes cost $600 each—it seems quite reasonable to us to spend up to $3k each if you want ZFS / compression or the abiliy to do data processing on-CPU.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;100 DS4246 chassis—each can hold 24 hard drives.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;2,400 3.5 inch HDDs—need to be all SATA or all SAS in each chassis.&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;We would recommend SAS hard drives if possible [5] 5. if you use SAS drives you’ll need to deal with or disable mulipathing, which is reasonably simple as they roughly double speed over similar SATA drives.&lt;/item&gt;
          &lt;item&gt;We used a mix of 12TB and 14TB drives—basically any size should work, roughly the larger the better holding price constant (density makes stacking easier + in general increases resale value).&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Physical parts to mount the chassis—you’ll need rails or l-brackets. We used l-brackets which worked well, as we haven’t needed to take the chassis out to slot hard drives. If you buy toploaders, you’ll need rails.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Multiple “crash carts” with monitors and keyboards that allow you to physically connect to your CPU head nodes and configure them—this is invaluable when you’re debugging network issues.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Network&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;A 100 GbE switch&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;A used Arista is fine, should be QSFP28, should cost about $1-2k.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;HBAs (Host Bus Adapters), which connect your head nodes to your DS4246 chassis.&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;The best configuration we tried was with Broadcom 9305-16E HBAs, with 3x HBAs per server (make sure your server has physical space for them!) with SFF-8644 to QSFP mini SAS cables.&lt;/item&gt;
          &lt;item&gt;There are 4 slots per HBA, so you can cable each DS4246 chassis directly to the HBA. [6] 6. The option we ended up going with for convenience was putting LSI SAS9207-8e HBAs, which have 2 ports each, into the CPU head nodes- then daisy-chaining the DS4246s together with QSFP+ to QSFP+ DACs.. We deployed this on Storage Stacking Saturday, then while debugging speeds tried the above method on one of the servers and got to ~4 Gbps per chassis-but didn’t find it worth it to swap everything out in pure labor because of the way we had set up some of our head nodes such that they were difficult to take out. Insofar as it is reasonably cheap to just do the above thing to start and we’ve tested it to work, you should probably do as we say, not as we did in this case!&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Network cards (NICs).&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;We used Mellanox ConnectX-4 100GbE. Make sure they come in Ethernet mode and not Infiniband mode for ease of config.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;DAC (Direct Attach Copper) or AOC (Active Optical) cables, to connect the NICs in your head nodes to your switch and therefore the internet. You almost certainly want DACs if your racks are close together, as they are far more compatible with arbitrary networking equipment than AOCs.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We would recommend that you find a supplier to sell you the CPU head nodes with the HBAs and NICs installed—there are a number of used datacenter / enterprise parts suppliers who are willing to do this. This is a substantial positive because it means that you don’t have to spend hours installing the HBAs/NICs yourself and can have a substantially higher degree of confidence in your operations.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Serial cables—you’ll need these to connect to your switch!&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Optional but recommended: an Ethernet management network of some kind. If you can’t easily get ethernet, we’d recommend getting a wifi adapter like this and then a ethernet switch like this —it’s substantially easier to set up than the 100GbE, is a great backup for when that’s not working, and will allow you to do ~everything over SSH from the comfort of the office instead of in the datacenter.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Datacenter Requirements&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;3.5 kW of usable power per cabinet, with 10 4U chassis + 1 2U (cabinets are 42U tall)&lt;/item&gt;
      &lt;item&gt;1 spare cabinet for the 1U or 2U 100GbE switch (you can obviously also just swap out one of the 4U chassis in another cabinet for the switch).&lt;/item&gt;
      &lt;item&gt;1 42U cabinet per 3 PB of storage&lt;/item&gt;
      &lt;item&gt;A dedicated 100G connection (will come in as a fiber pair probably via QSFP28 LR4, but confirm with your datacenter provider before buying parts here!)&lt;/item&gt;
      &lt;item&gt;Ideally physically near your office—there is a lot of value in being able to walk over and debug issues instead of e.g. dealing with remote hands services to get internet to the nodes.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Some setup tips:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Make sure to first properly configure your switch. Depending on your switch model this should be relatively straightforward—you’ll need to physically connect to the switch and then configure the specific port that your 100GbE is connected to (you’ll get a fiber cross-connect from your datacenter that you should plug into a QSFP28 transceiver. Make sure that you get a transceiver that is compatible in form with the ISP, probably LR4, and specifically branded with your switch brand, otherwise it is very unlikely to work). Depending on your ISP you might have to talk to them to make sure that you can get “light” through the fiber cables from both ends, which might involve rolling the fiber and otherwise making sure it’s working properly. &lt;list rend="ul"&gt;&lt;item&gt;If your switch isn’t working / you haven’t configured one before, I’d suggest trying to directly plug the fiber cable from the ISP into one of your 10 heap servers, making sure to buy a transceiver that is compatible with your NIC brand (e.g. Mellanox). Once you get it working from there, move over to your switch and get it working.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Once you can connect to the internet from your switch (simply ping 1.1.1.1 to check) you are ready to set up the netplans for the individual nodes. this is most easily done during the Ubuntu setup process, which will walk you through setting up internet for your CPU head nodes, but is also doable outside of that&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Once you have internet access to your nodes and have properly connected 1 cable to each DS4246, you should format &amp;amp; mount the drives on each node, test that all of them are properly working, and then you are ready to deploy any software you want.&lt;/p&gt;
    &lt;p&gt;If you end up building a similar storage cluster based on this writeup we’d love to hear from you—we’re very curious what can be improved, both in our guidance and in the object-level process. You can reach us at [email protected]&lt;/p&gt;
    &lt;p&gt;If you came away from this post excited about our work, we’d love to chat. We’re a research lab currently focused on pretraining models to use computers, with the long-term goal of building general models that can learn in-context and do arbitrary tasks while aligned with human values; we’re hiring top researchers and engineers to help us train these. If you’re interested in chatting, shoot us an email at [email protected].&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45438496</guid><pubDate>Wed, 01 Oct 2025 15:00:41 +0000</pubDate></item><item><title>Ask HN: Who wants to be hired? (October 2025)</title><link>https://news.ycombinator.com/item?id=45438501</link><description>&lt;doc fingerprint="301182753412e0e7"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Share your information if you are looking for work. Please use this format:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;  Location:
  Remote:
  Willing to relocate:
  Technologies:
  Résumé/CV:
  Email:
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt; Please only post if you are personally looking for work. Agencies, recruiters, job boards, and so on, are off topic here.&lt;/p&gt;
      &lt;p&gt;Readers: please only email these addresses to discuss work opportunities.&lt;/p&gt;
      &lt;p&gt;There's a site for searching these posts at https://www.wantstobehired.com.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45438501</guid><pubDate>Wed, 01 Oct 2025 15:01:06 +0000</pubDate></item><item><title>Ask HN: Who is hiring? (October 2025)</title><link>https://news.ycombinator.com/item?id=45438503</link><description>&lt;doc fingerprint="3651195a341ae364"&gt;
  &lt;main&gt;
    &lt;div&gt;&lt;p&gt;Please state the location and include REMOTE for remote work, REMOTE (US) or similar if the country is restricted, and ONSITE when remote work is &lt;/p&gt;not&lt;p&gt; an option.&lt;/p&gt;&lt;p&gt;Please only post if you personally are part of the hiring company—no recruiting firms or job boards. One post per company. If it isn't a household name, explain what your company does.&lt;/p&gt;&lt;p&gt;Please only post if you are actively filling a position and are committed to responding to applicants.&lt;/p&gt;&lt;p&gt;Commenters: please don't reply to job posts to complain about something. It's off topic here.&lt;/p&gt;&lt;p&gt;Readers: please only email if you are personally interested in the job.&lt;/p&gt;&lt;p&gt;Searchers: try https://dheerajck.github.io/hnwhoishiring/, https://amber-williams.github.io/hackernews-whos-hiring/, http://nchelluri.github.io/hnjobs/, https://hnresumetojobs.com, https://hnhired.fly.dev, https://kennytilton.github.io/whoishiring/, https://hnjobs.emilburzo.com, or this (unofficial) Chrome extension: https://chromewebstore.google.com/detail/hn-hiring-pro/mpfal....&lt;/p&gt;&lt;p&gt;Don't miss these other fine threads:&lt;/p&gt;&lt;p&gt;Who wants to be hired? https://news.ycombinator.com/item?id=45438501&lt;/p&gt;&lt;p&gt;Freelancer? Seeking freelancer? https://news.ycombinator.com/item?id=45438502&lt;/p&gt;&lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45438503</guid><pubDate>Wed, 01 Oct 2025 15:01:06 +0000</pubDate></item><item><title>Long-distance and wide-area detection of gene expression in living bacteria</title><link>https://www.asimov.press/p/hyperspectral</link><description>&lt;doc fingerprint="bec452110dbf0459"&gt;
  &lt;main&gt;
    &lt;p&gt;Nature has evolved a stunning array of biosensors for detecting the physical world.&lt;/p&gt;
    &lt;p&gt;A single E. coli cell, for example, can precisely sense chemical gradients and “swim” toward or away from them. Some bird species, including robins and warblers, can see magnetic fields using cryptochrome proteins embedded in their eyes to guide them during their annual migration. Bogong moths use photons from distant stars as a compass while soaring 1,000 kilometers across southeast Australia. In other words, organisms can sense not only tastes and smells, but also individual molecules, magnetic fields, and infrared or ultraviolet light.&lt;/p&gt;
    &lt;p&gt;Humans have long used other creatures’ senses to aid and extend our own, too. As far back as 1,000 BCE, humans employed pigeons to carry messages across cities and kingdoms, taking advantage of their remarkable homing instinct. Dogs’ superior sense of smell is often used to sniff out disease, truffles, contraband, and explosives. And today, the city of Poznań, in Poland, uses just eight mussels to monitor their water quality.1&lt;/p&gt;
    &lt;p&gt;But increasingly, over the last quarter century, scientists have not only used entire organisms to sense the natural world, but have also taken particular genes from those organisms and adapted them into molecular biosensors. Just as a smoke detector has a sensor that detects particles in the air and a buzzer that then alerts us, all human-made biosensors have two basic components.&lt;/p&gt;
    &lt;p&gt;The first is the sensor itself — an enzyme, antibody, or engineered cell — that physically recognizes a target, whether a pollutant, virus, or rise in temperature. The second is the transducer, which converts that recognition event into a signal we can perceive, such as a glowing light.&lt;/p&gt;
    &lt;p&gt;Although bioengineers have adapted hundreds of biosensors from nature, they have been less successful in making better transducers.2 Nearly every biosensor today still relies on a narrow set of outputs (aka “reporters”), such as green fluorescent protein (GFP), luciferase, or colorful pigments. Most transducers can only be seen from close up with a direct line of sight, usually using a microscope. And almost all man-made reporters fail to work inside the body or at a distance. This is because visible light does not penetrate solid materials, such as human skin, and easily “blends in” with other photons in the environment.3&lt;/p&gt;
    &lt;p&gt;Recently, however, bioengineers have developed transducers that transcend such limitations. To make biosensors that work inside the body, scientists have discovered genetically encoded transducers that can be measured using ultrasound or even MRI machines. And for a recent paper in Nature Biotechnology, scientists have reported — for the first time — a new type of transducer that can even be seen from up to 90 meters away using “hyperspectral” cameras mounted to drones. This new technology makes it feasible to monitor individual molecules, as sensed by engineered bacteria, across entire ecosystems.&lt;/p&gt;
    &lt;head rend="h2"&gt;Hyperspectral Photos&lt;/head&gt;
    &lt;p&gt;The first hyperspectral cameras were developed in the early 1980s by NASA scientists, who wanted to capture information about Earth, including mineral deposits and ocean algal blooms, from the air. Unlike conventional cameras, which record just three bands of light (red, green, blue), hyperspectral cameras split incoming light into hundreds of narrow spectral bands, including ultraviolet and near-infrared wavelengths.&lt;/p&gt;
    &lt;p&gt;Because each type of molecule absorbs and reflects light in a distinct way, the camera can be mounted onto satellites and used to record a full spectrum for every pixel on the ground. In plants, for example, these cameras can quantify shifts in chlorophyll levels because those molecules strongly absorb light in the blue and red regions. For soils, the spectra contain characteristic dips and peaks that correspond to moisture levels.&lt;/p&gt;
    &lt;p&gt;But the idea that these same cameras could be used to detect bacteria required a leap of imagination. It first came to Chris Voigt, professor of biological engineering at MIT, while touring a military facility, where soldiers explained how hyperspectral drones were being used to spot plastic objects from the sky. Foreign militaries sometimes hide explosives or sensors inside plastic casings and disguise them as rocks, but because real rocks reflect light differently than plastic dupes, hyperspectral cameras can distinguish between them.&lt;/p&gt;
    &lt;p&gt;If the military can distinguish plastic from rock, Voigt wondered, why not microbes from soil?&lt;/p&gt;
    &lt;p&gt;The work to answer this question fell to Yonatan Chemla and Itai Levin, a postdoctoral fellow and graduate student in Voigt’s laboratory. Their first challenge was to find molecules that cells make that could produce a distinctive hyperspectral fingerprint visible from a distance. So the duo began by searching through hundreds of thousands of metabolites listed in scientific databases, finding that only about 100 have any recorded absorption spectra.&lt;/p&gt;
    &lt;p&gt;Upon realizing that we don’t understand how the overwhelming majority of biomolecules reflect light, Chemla and Levin decided to investigate themselves. They bought a hyperspectral camera and a large number of purified molecules from online chemical suppliers — such as indigo and porphyrins — and started testing them in the laboratory. They sprayed these molecules onto soils or rocks, took pictures, and then tried to work out which ones produced a clear signal against background noise.&lt;/p&gt;
    &lt;p&gt;The duo also used computational tools to identify candidate molecules that might act as hyperspectral reporters. Together with collaborators at MIT, they ran quantum chemistry simulations on a selection of 20,000 metabolites to predict how each one would respond to light. These simulations calculated which wavelengths of light each chemical would absorb, and how strong those peaks would be.4 After running these computational tests, Chemla and Levin filtered this list down to a few hundred with unusual peaks or that absorbed light in parts of the spectrum where biology is usually quiet, especially near-infrared wavelengths.&lt;/p&gt;
    &lt;p&gt;Finally, they considered which of these molecules would be easiest and most efficient for a microbe to make, favoring ones that could be made by slightly altering natural pathways or requiring the addition of only a few recombinant genes. Since microbes can have very different metabolisms, they also weighed which hosts would be the best for each possible molecule. After this winnowing process, they ended up with just two: biliverdin IXα made by Pseudomonas putida, and bacteriochlorophyll a made by Rhodocyclus gelatinosus.&lt;/p&gt;
    &lt;p&gt;Biliverdin IXα is a green pigment that naturally forms when heme, the molecule carrying oxygen in red blood cells, is broken down and recycled. To make it in P. putida, the team only needed to add two enzymes. Bacteriochlorophyll a, on the other hand, is a photosynthetic pigment found in purple bacteria.5 R. gelatinosus is itself a purple bacterium, meaning that all the team needed to do was amend its existing genome to produce much larger quantities of bacteriochlorophyll a.&lt;/p&gt;
    &lt;p&gt;With these two engineered microbial strains in hand, the researchers traveled to Fort Devens in Massachusetts — alongside two undergraduate students, Anna Johnson and Yueyang Fan — and sprayed the cells onto little patches of soil. They flew a hyperspectral drone overhead and took pictures of one acre, or about 4,000 square meters, across the entire military facility. Using a hyperspectral detection algorithm that separated the molecular signal from background “noise” of soil and dirt, Chemla and Levin could clearly identify the engineered microbes from up to 90 meters away.6&lt;/p&gt;
    &lt;p&gt;Alas, the cells were layered on top of sand, in direct line of sight to the camera. But in many cases, the things we want to sense — like explosives or pathogens invading plant roots — are hidden underground. Chemla is now searching for volatile molecules that diffuse upward through the soil and into the air, creating a spectral signature that a camera can detect from high above (possibly even from outer space.)&lt;/p&gt;
    &lt;head rend="h2"&gt;Environmental Release&lt;/head&gt;
    &lt;p&gt;Despite this scientific breakthrough, it will be difficult to move these biosensors into the real world. Researchers have been testing engineered microbes in field trials for the last four decades, but few have been commercialized.&lt;/p&gt;
    &lt;p&gt;Field trials for genetically-engineered microbes peaked in the early 1990s but have fallen off since then, mainly due to increased regulations and mixed field trial results. In the late 1980s, engineered Agrobacterium radiobacter K1026 was approved in both Australia and the U.S. to fight crown gall disease in trees. (The microbe outcompetes disease-causing bacteria, killing them.)&lt;/p&gt;
    &lt;p&gt;But getting approval to release a microbe into the wild, without containment, can be incredibly arduous. The regulatory pathway is divided across the EPA, USDA, and FDA. Each agency has jurisdiction depending on the intended use; pesticides fall to the EPA, other agricultural products go to the USDA, and ingestible microbes fall under the province of the FDA. Anything that does not easily fit into these categories, including environmental biosensors, is lumped under the EPA’s Toxic Substances Control Act, or TSCA.&lt;/p&gt;
    &lt;p&gt;The TSCA regulates genetically engineered microbes based on their method of engineering, rather than the product itself. This practice is outdated and should be revised, Chemla says. Any microbe containing DNA from another genus — say, moving a gene from Escherichia coli into Pseudomonas putida — is flagged by the TSCA and unlikely to get approval, even if researchers can prove that the product is safe. More than 200 TSCA submissions were filed between 1987 and 2018, but none of those submissions have led to a commercialized product.&lt;/p&gt;
    &lt;p&gt;There are ways to skirt these regulations, though. Pivot Bio sells genetically-engineered microbes that colonize plant roots and convert atmospheric nitrogen (N₂) into ammonia (NH₃), a chemical form that plants can use. This reduces the amount of fertilizer needed for a field, thus decreasing the leaching of fertilizer byproducts into water.7&lt;/p&gt;
    &lt;p&gt;Pivot Bio sidestepped some regulatory hurdles by avoiding the transfer of genes from one species to another; they simply remodeled their organism’s existing genome. The company still must get USDA approval to ship its product across state lines, but that is a simpler and less insurmountable regulatory hurdle.&lt;/p&gt;
    &lt;p&gt;In the case of hyperspectral reporters, there may be similar ways to circumvent the most onerous regulations. Even in this study, the R. gelatinosus strain engineered to make bacteriochlorophyll a did not have any DNA from foreign microbes. It could, in principle, sidestep the TSCA regulations. A startup called Fieldstone Bio has spun out from the Voigt laboratory with the goal of commercializing this hyperspectral technology.&lt;/p&gt;
    &lt;p&gt;Regardless, the barrier to commercializing these biosensors is not scientific feasibility but rather a patchwork of rules written long before anyone imagined microbes capable of broadcasting messages into space.&lt;/p&gt;
    &lt;p&gt;Still, it’s promising to see that synthetic biology is moving past its reliance on visible light toward a broader range of transducers that let us measure biology in places once thought inaccessible, from the molecules inside a tumor to antibiotic resistance genes hidden in soil. The challenge ahead is not discovering what cells can sense, but engineering more reliable ways for them to communicate those impressions back to us.&lt;/p&gt;
    &lt;p&gt;Niko McCarty is a founding editor of Asimov Press.&lt;/p&gt;
    &lt;p&gt;Thanks to Xander Balwit and Ella Watkins-Dulaney for reading drafts of this.&lt;/p&gt;
    &lt;p&gt;Cite: McCarty, Niko. “Seeing Microbes from the Sky.” Asimov Press (2025). https://doi.org/10.62211/23jr-64kt&lt;/p&gt;
    &lt;p&gt;These mussels are used as natural biosensors because they filter large amounts of water and quickly react to pollutants. When they sense harmful chemicals, heavy metals, or sudden changes in water quality, they clamp their shells shut to protect themselves. When they close, a piece of metal hot glued to their shell completes a circuit which alerts the city to check their water system.&lt;/p&gt;
    &lt;p&gt;The synthetic biology community borrows many terms from computer science and electrical engineering. In electrical engineering, a transducer is a part that converts what a sensor has detected into electrical signals. A biological transducer is any sort of read out that signals what a biosensor has detected.&lt;/p&gt;
    &lt;p&gt;Visible wavelengths of light only penetrate about one millimeter into the body, for example. There is a tissue transparency window between 800 and 950 nanometers, though, in which light penetrates about a centimeter.&lt;/p&gt;
    &lt;p&gt;They used three databases, called BKMS, MetaCyc, and Rhea.&lt;/p&gt;
    &lt;p&gt;Bacteriochlorophyll a absorbs infrared light with a wavelength of 860 nanometers.&lt;/p&gt;
    &lt;p&gt;The camera could see the microbes provided there were at least 4 million cells per square centimeter of sand. This is quite a large number of cells, though, as a square centimeter of human skin has between 100 thousand and one million cells. A single gram of soil usually contains hundreds of millions of microbes.&lt;/p&gt;
    &lt;p&gt;The Haber-Bosch process, used to make ammonia, also accounts for between 1-2 percent of all global CO2 emissions.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45438704</guid><pubDate>Wed, 01 Oct 2025 15:15:44 +0000</pubDate></item><item><title>Fossabot: AI code review for Dependabot/Renovate on breaking changes and impacts</title><link>https://fossa.com/blog/fossabot-dependency-upgrade-ai-agent/</link><description>&lt;doc fingerprint="af4799a7096b4451"&gt;
  &lt;main&gt;
    &lt;p&gt;Today we're announcing fossabot, a new AI Agent for making strategic dependency updates, backed by a comprehensive accuracy, consistency, and correctness framework.&lt;/p&gt;
    &lt;p&gt;fossabot is able to deliver completed work just like an engineer, including researching new versions, finding app impact and adapating code if needed. This product fulfills our philosophy for automating dependency updates and EdgeBit acquisition.&lt;/p&gt;
    &lt;p&gt;fossabot is currently available as a public preview, with a focus on the JavaScript and TypeScript ecosystems.&lt;/p&gt;
    &lt;head rend="h3"&gt;Your dependencies are simultaneously moving too fast and too slow&lt;/head&gt;
    &lt;p&gt;For a decade, FOSSA has protected businesses from open source risk in two large categories: compliance and security. We’ve identified a new, third category of risk that is emerging: dependency churn and update stagnation.&lt;/p&gt;
    &lt;p&gt;AI coding agents churning out new repos and dependencies trees faster than we can follow.&lt;/p&gt;
    &lt;p&gt;At the same time, crown jewel apps can’t keep up with the fast pace of upstream development and fall more behind.&lt;/p&gt;
    &lt;p&gt;Neither are good, but fossabot is here to help...as if your best engineer managed updates 24/7.&lt;/p&gt;
    &lt;head rend="h3"&gt;Every dependency update program is broken&lt;/head&gt;
    &lt;p&gt;The root of the problem is that every enterprise dependency update program is broken. Why? Our tools can’t make strategic updates like our engineers are capable of.&lt;/p&gt;
    &lt;p&gt;Instead, enterprises focus is making the smallest update possible to fix an alert, only to do it again next month. No time is devoted to figuring out how to upgrade to the latest version of a package and the benefits it may bring to the app.&lt;/p&gt;
    &lt;p&gt;fossabot, our dependency updating AI agent, is capable of large complexity upgrades – the ones that require a senior engineer because they’re always an unexpected multi-hour research and coding task.&lt;/p&gt;
    &lt;p&gt;Bump lodash from &lt;code&gt;4.17.20&lt;/code&gt; to &lt;code&gt;4.17.21&lt;/code&gt;&lt;/p&gt;
    &lt;head rend="h3"&gt;Summary by fossabot&lt;/head&gt;
    &lt;p&gt;I recommend merging this lodash update from 4.17.20 to 4.17.21. This is a patch release that fixes several security vulnerabilities and includes performance improvements. Your application's usage patterns are compatible with this update.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;•Analyzed 47 files using lodash utilities across components/, utils/, and services/&lt;/item&gt;
      &lt;item&gt;•Verified no deprecated methods or breaking changes affect your codebase&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Change Details&lt;/head&gt;
    &lt;p&gt;1. Fixed prototype pollution vulnerability in merge function&lt;/p&gt;
    &lt;p&gt;2. Improved input validation for template method&lt;/p&gt;
    &lt;p&gt;3. Enhanced sanitization in defaultsDeep&lt;/p&gt;
    &lt;p&gt;fossabot started out as an internal tool and became invaluable to our engineers and trusted testers, so we’re releasing it as a public preview for all to use.&lt;/p&gt;
    &lt;p&gt;fossabot is available as a GitHub app and all users get $15 in free usage credit each month.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why does fossabot work so well?&lt;/head&gt;
    &lt;p&gt;fossabot proposes strategic updates because it can balance risk vs. reward, understand breaking changes in the context of your app, and even adapt code to handle newer paradigms.&lt;/p&gt;
    &lt;p&gt;Existing updaters like Dependabot or Renovate can’t do this reasoning, so they end up being configured to be “dumb,” like patch releases only.&lt;/p&gt;
    &lt;p&gt;Plus, mechanically making the update is not the hard and slow part. It’s the research and understanding of risk to your app that takes forever and ultimately relegates most updates into the backlog forever.&lt;/p&gt;
    &lt;head rend="h3"&gt;Codebase Reasoning&lt;/head&gt;
    &lt;p&gt;fossabot analysis determines the impact of an update to your specific codebase and usage of dependencies instead of making guesses about compatibility, which allows for smart reasoning. Examples of this reasoning include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Use a rewritten React library and update your component to use the more modern syntax&lt;/item&gt;
      &lt;item&gt;Upgrade a major version of a library safely because you use APIs in forward-compatible ways&lt;/item&gt;
      &lt;item&gt;Adapt your code to an undeclared behavior change in a patch update&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here’s a partial excerpt of this reasoning in action:&lt;/p&gt;
    &lt;p&gt;fossabot uses a perfect balance of hard facts from static analysis paired with a scalable and detail-oriented AI.&lt;/p&gt;
    &lt;head rend="h3"&gt;Scale Through AI&lt;/head&gt;
    &lt;p&gt;fossabot outperforms human engineers because it can scale beyond what a reasonable person would do. It researches harder, deeper, and longer, with perfect memory about your first-party code, the dependency code and the library’s release notes, migration guides, and docs.&lt;/p&gt;
    &lt;p&gt;While a human would become fatigued after an hour (or even minutes), fossabot will keep going until every modified function is triaged and every impact is understood throughout your entire codebase.&lt;/p&gt;
    &lt;p&gt;No engineer can hold a full picture of dependency usage, especially when multiple teams are involved. fossabot is able to take in more analysis and relationships that can be mapped out in your brain.&lt;/p&gt;
    &lt;head rend="h3"&gt;Delivers Completed Tasks&lt;/head&gt;
    &lt;p&gt;Customers tell us that understanding the level of effort for a change can be just as hard as the update itself. When fossabot is in charge of your updates, you can skip all of this toil and receive completed tasks, delivered right to a pull request.&lt;/p&gt;
    &lt;p&gt;fossabot understands its limitations and can request assistance to “last-mile” an update across the finish line. Backed by our evaluation framework and ability to classify different types of updates, we’re confident in fossabot’s ability to handle large complexity updates in the JavaScript/TypeScript ecosystem.&lt;/p&gt;
    &lt;head rend="h2"&gt;From Internal Tool to Public Preview&lt;/head&gt;
    &lt;p&gt;Earlier this year, FOSSA engineers hypothesized that with the right context, we could eliminate the toil from dependency updates. We started providing a custom AI framework with details from FOSSA’s dependency metadata scanning, upgrade path guidance, and open source health signals. This grew into a robust breaking change detection engine that continues to surprise us with its detail and accuracy.&lt;/p&gt;
    &lt;p&gt;With breaking changes found, the next challenge was impact detection for each customer’s codebase. Static analysis is the ideal tool for this, which led to a partnership and eventual acquisition of EdgeBit, which pioneered a new type analysis that is designed for dependency update use-cases.&lt;/p&gt;
    &lt;p&gt;Static analysis prevents the AI agent from making silly mistakes, and in our experience, perfectly balances the desired fuzziness you gain from using AI agents and sub agents. fossabot resembles a “focused agent” that resembles a pipeline for determinism but includes agentic steps as well.&lt;/p&gt;
    &lt;head rend="h2"&gt;Accuracy, Consistency, Correctness&lt;/head&gt;
    &lt;p&gt;While iterating on fossabot, we quickly realized that the evaluation framework and ground truth dataset was just as important as the tool itself, and in many ways, just as challenging as writing the code.&lt;/p&gt;
    &lt;p&gt;fossabot continually scores itself on Accuracy, Consistency, Correctness (ACC) against a set of validated dependency updates with varying degrees of breaking changes, changed lines of code and usage of those libraries in real-world apps.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Accuracy, Consistency, Correctness by Group &amp;amp; Complexity&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Group&lt;/cell&gt;
        &lt;cell&gt;Complexity&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;routine_minor_updates&lt;/cell&gt;
        &lt;cell&gt;low&lt;/cell&gt;
        &lt;cell&gt;medium&lt;/cell&gt;
        &lt;cell&gt;high&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;multi_dependency_updates&lt;/cell&gt;
        &lt;cell&gt;low&lt;/cell&gt;
        &lt;cell&gt;medium&lt;/cell&gt;
        &lt;cell&gt;high&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;major_version_upgrades&lt;/cell&gt;
        &lt;cell&gt;low&lt;/cell&gt;
        &lt;cell&gt;medium&lt;/cell&gt;
        &lt;cell&gt;high&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;dev_dependencies&lt;/cell&gt;
        &lt;cell&gt;low&lt;/cell&gt;
        &lt;cell&gt;medium&lt;/cell&gt;
        &lt;cell&gt;high&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;This process quickly highlighted a key learning: the importance of weighted scoring in our evaluation. A false positive (where the tool incorrectly deems a breaking change safe) carries a much higher cost in terms of potential disruption and lost trust than a false negative (where a safe update is flagged for extra scrutiny). This phase also helped us debunk early, overly simplistic assumptions, such as the fallacy that all major version upgrades are inherently breaking.&lt;/p&gt;
    &lt;p&gt;Our public preview is targeted at the JavaScript/TypeScript ecosystem because our ACC dataset is robustly populated — other ecosystems will follow shortly as we build out more ground truth.&lt;/p&gt;
    &lt;p&gt;We believe that several design decisions set at the genesis of fossabot make it a trusted foundation:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Striving for determinism at key steps&lt;/item&gt;
      &lt;item&gt;Smartly using static analysis&lt;/item&gt;
      &lt;item&gt;Use AI to be doggedly persistent and detail oriented&lt;/item&gt;
      &lt;item&gt;Measuring ourselves against the ACC ground truth&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We hope fossabot earns your trust, and we’d love your feedback on analysis that looks great or needs refinement.&lt;/p&gt;
    &lt;head rend="h2"&gt;Try Out fossabot&lt;/head&gt;
    &lt;p&gt;fossabot’s public preview is available as a GitHub app. Every user gets $15 of analysis credit, replenished every month. Let loose the updates!&lt;/p&gt;
    &lt;p&gt;Today, fossabot will auto-analyze Pull Requests opened from Dependabot, Renovate or Snyk. Soon, fossabot will open its own PRs with pre-planning and pre-analysis taken into account.&lt;/p&gt;
    &lt;p&gt;Reach out to get a demo of fossabot and let's figure out how to get your teams caught up on updates.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45439721</guid><pubDate>Wed, 01 Oct 2025 16:30:16 +0000</pubDate></item><item><title>Why Tech Inevitability is Self-Defeating</title><link>https://deviantabstraction.com/2025/09/29/against-the-tech-inevitability/</link><description>&lt;doc fingerprint="2c389bc7ad46364b"&gt;
  &lt;main&gt;
    &lt;p&gt;My dear startup founder friend,&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;You told me that everything is “hyper-determined,” that “life is a big game because of that.” You’re not alone in thinking this: Silicon Valley loves to preach inevitability.&lt;/p&gt;
    &lt;head rend="h2"&gt;A Silicon Valley paradox&lt;/head&gt;
    &lt;p&gt;“Technology has its own agenda.” — Kevin Kelly https://kk.org/thetechnium/the-unabomber-w/&lt;/p&gt;
    &lt;p&gt;“Technology happens because it is possible.” — Sam Altman https://www.nytimes.com/2023/03/31/technology/sam-altman-open-ai-chatgpt.html&lt;/p&gt;
    &lt;p&gt;“Probability that AI exceeds the intelligence of all humans combined by 2030 is ~100%.” — Elon Musk https://x.com/elonmusk/status/1871083864111919134?lang=fr&lt;/p&gt;
    &lt;p&gt;Notice the pattern: inevitability, inevitability, inevitability.&lt;/p&gt;
    &lt;p&gt;Precisely what you believe.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;Yet in reality, they could have gone surfing, lived quietly, done nothing. However, they worked obsessively, raised money, built companies, and reshaped the world. That was their agency, fully deployed.&lt;/p&gt;
    &lt;p&gt;You’ll say, “They got lucky, it had to happen, if not them someone else. That’s why we all work ourselves to the bone.”&lt;/p&gt;
    &lt;p&gt;Now hear me out: I believe they worked hard to achieve this outcome and they deserve the credit. But I also believe that with success comes responsibility — responsibility that evaporates the moment y’all imply inevitability.&lt;/p&gt;
    &lt;p&gt;My goal here is to show you why the trope “it was inevitable” is misleading and it shifts responsibility away from those with power.&lt;/p&gt;
    &lt;p&gt;To be clear, versions of this reasoning have appeared often in “pop science” and academic writing so I’m not claiming originality, but I hope to convince you that treating inevitability as fact comes with important downsides and it’s better to manage your life as if you’d had agency.&lt;/p&gt;
    &lt;head rend="h2"&gt;&lt;lb/&gt;inevitability&lt;/head&gt;
    &lt;p&gt;What Silicon Valley calls inevitability is really a rhetorical posture: a way of talking about the future as if it could not be otherwise. It borrows from different philosophical schools of thought (fatalism, determinism, technological determinism) — each distinct, and often incompatible.&lt;/p&gt;
    &lt;p&gt;This patchwork coalesces around the idea that it is inevitable and I don’t matter as an individual (because someone else would have done it). “This will happen nevertheless (even if I choose to fight it).”&lt;/p&gt;
    &lt;p&gt;This is oddly compatible with their obsessive work: being the one who succeeds is merely a matter of luck and we help luck through hard work.&lt;/p&gt;
    &lt;head rend="h2"&gt;True Prophets and False Predictions&lt;/head&gt;
    &lt;p&gt;Tomorrow the sun will rise is a rock-solid prediction. Objects fall to the ground. Everyone dies. These sentences are so reliable we call them laws. We believe in them.&lt;/p&gt;
    &lt;p&gt;Belief is an act of faith: we trust the future will resemble the past, that the laws won’t suddenly change. Yes, that trust has been earned through millennia of confirmation, but it’s still a belief in regularity. Just ask the turkey on Thanksgiving Day, who believed sunrise meant good food and safety…&lt;/p&gt;
    &lt;p&gt;So even laws of nature require belief, and much more so for “social predictions” that are rooted in much fewer facts. When Sam is saying AI is inevitable, how can I tell if this prediction is a prophecy or an advertisement for his company?&lt;/p&gt;
    &lt;p&gt;More generally, when someone makes a prediction I can’t know if it’s true (otherwise I’d be the one making the prediction). Therefore to act on it, I need to believe in it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Predictions as Communication&lt;/head&gt;
    &lt;p&gt;A prediction is, by definition, a communicative act and communication theory teaches us that these acts are made to influence us. And I should be cognizant and wary of them (i.e. don’t accept them at face value). My prediction about the sunrise could be here to build belief in the scientific method.&lt;/p&gt;
    &lt;p&gt;No prediction is neutral. Because they talk about the future, predictions are always either attempts to reshape the world, genuine forecasts, or a mix of both. And at a fundamental (epistemological) level, I can never know which is which. That is why inevitability talk is so powerful and so dangerous. They present themselves as a law of physics while it’s only an act of communication whose power comes from people believing in it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Self-Fulfilling Prophecy&lt;/head&gt;
    &lt;p&gt;Since I can’t know whether a prediction is persuasion, foresight or a mix of the two, the important question becomes: how should I act?&lt;/p&gt;
    &lt;p&gt;Let’s analyze each position: “things are inevitable” or “if I fight it I might win.”&lt;/p&gt;
    &lt;p&gt;Assuming you think everything is inevitable, you go into a self-fulfilling prophecy and some circular logic:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;“X is inevitable” → Stop trying to change it&lt;/item&gt;
      &lt;item&gt;Stop trying → No agency exercised&lt;/item&gt;
      &lt;item&gt;No agency exercised → X happens&lt;/item&gt;
      &lt;item&gt;Prophecy fulfilled → X was inevitable&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You can replace X by AGI or anything. E.g., there’s an opportunity to win a prize but you won’t work at it because you’re a loser and everything is determined. And it works positively too: e.g., “I am a future champion” → so I become one.&lt;/p&gt;
    &lt;p&gt;The other case is easier (and healthier): if you assume things are not inevitable then you can change the prediction by exercising your agency. You also accept some uncertainty in the process because it might not work out.&lt;/p&gt;
    &lt;p&gt;If you believe in inevitability, it makes sense not to act. And if you don’t, it makes sense to act.&lt;/p&gt;
    &lt;p&gt;Except, except… Using the previous paragraph, I explained it’s impossible for us to know if a prediction is inevitable or not. But if I use my agency, I have a higher chance of getting where I want things to be. Therefore, the only pragmatically rational stance is to embrace our agency.&lt;/p&gt;
    &lt;p&gt;So the informed position is to treat all predictions I disagree with as something I need to fight to change. Not accept them. Especially if they’re inevitable.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Agency’s Wager&lt;/head&gt;
    &lt;p&gt;Inevitability is rhetoric, not truth. Predictions aren’t laws of nature, they are acts of persuasion. And because no one can ever know how much is determined and how much is open, the only rational stance is to live as agents. Supersonic flight once looked inevitable until people stopped it. You, too, have already reshaped the world once; you can do it again. Don’t give away your power.&lt;/p&gt;
    &lt;p&gt;My dear CEO friend, if you want to change the world, the first step is to believe you can. You may win or lose. Success is never guaranteed. But one thing is certain: the surest way to lose is to not fight.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45440212</guid><pubDate>Wed, 01 Oct 2025 17:09:20 +0000</pubDate></item><item><title>Solar leads EU electricity generation as renewables hit 54%</title><link>https://electrek.co/2025/09/30/solar-leads-eu-electricity-generation-as-renewables-hit-54-percent/</link><description>&lt;doc fingerprint="af51f0535bbde596"&gt;
  &lt;main&gt;
    &lt;p&gt;More than half of the European Union’s (EU) electricity came from renewables in the second quarter of 2025, and solar is leading from the front.&lt;/p&gt;
    &lt;p&gt;According to new data from Eurostat, renewable energy sources generated 54% of the EU’s net electricity in Q2 2025, up from 52.7% year-over-year. The growth came mainly from solar, which produced 122,317 gigawatt-hours (GWh) – nearly 20% of the total electricity generation mix.&lt;/p&gt;
    &lt;p&gt;June 2025 was a milestone month: Solar became the EU’s single largest electricity source for the first time ever. It supplied 22% of all power that month, edging out nuclear (21.6%), wind (15.8%), hydro (14.1%), and natural gas (13.8%).&lt;/p&gt;
    &lt;p&gt;Some countries are already nearly 100% renewable. Denmark led with an impressive 94.7% share of renewables in net electricity generated, followed by Latvia (93.4%), Austria (91.8%), Croatia (89.5%), and Portugal (85.6%). At the other end of the spectrum, Slovakia (19.9%), Malta (21.2%), and the Czech Republic (22.1%) lagged behind.&lt;/p&gt;
    &lt;p&gt;In total, 15 EU countries saw their share of renewable generation rise year-over-year. Luxembourg (+13.5 percentage points) and Belgium (+9.1 pp) posted the most significant gains, driven largely by solar power growth.&lt;/p&gt;
    &lt;p&gt;Across the EU, solar made up 36.8% of renewable generation, followed by wind at 29.5%, hydro at 26%, biomass at 7.3%, and geothermal at 0.4%.&lt;/p&gt;
    &lt;p&gt;Read more: EIA: Solar and wind crush coal with 20% more power in 2025&lt;/p&gt;
    &lt;p&gt;The 30% federal solar tax credit is ending this year. If you’ve ever considered going solar, now’s the time to act. To make sure you find a trusted, reliable solar installer near you that offers competitive pricing, check out EnergySage, a free service that makes it easy for you to go solar. It has hundreds of pre-vetted solar installers competing for your business, ensuring you get high-quality solutions and save 20-30% compared to going it alone. Plus, it’s free to use, and you won’t get sales calls until you select an installer and share your phone number with them.&lt;/p&gt;
    &lt;p&gt;Your personalized solar quotes are easy to compare online and you’ll get access to unbiased Energy Advisors to help you every step of the way. Get started here.&lt;/p&gt;
    &lt;p&gt;FTC: We use income earning auto affiliate links. More.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45440387</guid><pubDate>Wed, 01 Oct 2025 17:22:43 +0000</pubDate></item><item><title>OpenTSLM: Language models that understand time series</title><link>https://www.opentslm.com/</link><description>&lt;doc fingerprint="748d54ec193a388b"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;OpenTSLM&lt;/head&gt;
    &lt;p&gt;The Future of AI Delivered on Time&lt;/p&gt;
    &lt;p&gt;AI understands text, images, audio, and video.&lt;lb/&gt;But the real world runs on time.&lt;/p&gt;
    &lt;p&gt;Every heartbeat, price tick, sensor pulse, machine log, and user click is a temporal signal.&lt;lb/&gt;Current models can't reason about them.&lt;/p&gt;
    &lt;p&gt;We're changing that.&lt;/p&gt;
    &lt;head rend="h2"&gt;A New Class of Foundation Models&lt;/head&gt;
    &lt;p&gt;Time-Series Language Models (TSLMs) are multimodal foundation models with time series as a native modality, next to text, enabling direct reasoning, explanation, and forecasting over temporal data in natural language.&lt;/p&gt;
    &lt;p&gt;Our research shows order-of-magnitude gains in temporal reasoning while running on smaller, faster backbones. TSLMs are not an add-on. They're a new modality for AI.&lt;/p&gt;
    &lt;head rend="h2"&gt;Open Core, Frontier Edge&lt;/head&gt;
    &lt;p&gt;OpenTSLM: Lightweight base models trained on public data, released openly. They set the standard for temporal reasoning and power a global developer and research ecosystem.&lt;/p&gt;
    &lt;p&gt;Frontier TSLMs: Advanced proprietary models trained on specialized data, delivering enterprise-grade performance and powering APIs, fine-tuning, and vertical solutions.&lt;/p&gt;
    &lt;head rend="h2"&gt;Our Vision&lt;/head&gt;
    &lt;p&gt;We're building the temporal interface for AI - the layer that connects continuous real-world signals to intelligent decisions and autonomous agents.&lt;/p&gt;
    &lt;p&gt;A universal TSLM will power proactive healthcare, adaptive robotics, resilient infrastructure, and new forms of human-AI collaboration.&lt;/p&gt;
    &lt;head rend="h2"&gt;About Us&lt;/head&gt;
    &lt;p&gt;OpenTSLM is a team of scientists, engineers, and builders from ETH, Stanford, Harvard, Cambridge, TUM, CDTM, Google, Meta, AWS, and beyond. We are the original authors of the OpenTSLM paper.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45440431</guid><pubDate>Wed, 01 Oct 2025 17:25:33 +0000</pubDate></item><item><title>What good workplace politics looks like in practice</title><link>https://terriblesoftware.org/2025/10/01/stop-avoiding-politics/</link><description>&lt;doc fingerprint="25ca331c7e610687"&gt;
  &lt;main&gt;
    &lt;p&gt;Say the word “politics” to most engineers and watch their face scrunch up like they just bit into a lemon. We’ve all been conditioned to believe that workplace politics is this dirty game played by manipulative ladder-climbers while the “real” engineers focus on the code.&lt;/p&gt;
    &lt;p&gt;I used to think the same way. For years as an engineer, I wore my hatred of politics like a badge of honor. I was above all that nonsense. I just wanted to ship. Politics was for those other people, the ones who didn’t have what it takes technically.&lt;/p&gt;
    &lt;p&gt;Now I think the opposite: politics isn’t the problem; bad politics is. And pretending politics doesn’t exist? That’s how bad politics wins.&lt;/p&gt;
    &lt;p&gt;Politics is just how humans coordinate in groups. It’s the invisible network of relationships, influence, and informal power that exists in every organization. You can refuse to participate, but that doesn’t make it go away. It just means decisions get made without you.&lt;/p&gt;
    &lt;p&gt;Think about the last time a terrible technical decision got pushed through at your company. Maybe it was adopting some overcomplicated architecture, or choosing a vendor that everyone knew was wrong, or killing a project that was actually working. I bet if you dig into what happened, you’ll find it wasn’t because the decision-makers were stupid. It’s because the people with the right information weren’t in the room. They “didn’t do politics.”&lt;/p&gt;
    &lt;p&gt;Meanwhile, someone who understood how influence works was in that room, making their case, building coalitions, showing they’d done their homework. And their idea won. Not because it was better, but because they showed up to play while everyone else was “too pure” for politics.&lt;/p&gt;
    &lt;p&gt;Ideas don’t speak. People do. And the people who understand how to navigate organizational dynamics, build relationships, and yes, play politics? Their ideas get heard.&lt;/p&gt;
    &lt;p&gt;When you build strong relationships across teams, understand what motivates different stakeholders, and know how to build consensus, you’re doing politics. When you take time to explain your technical decisions to non-technical stakeholders in language they understand, that’s politics. When you grab coffee with someone from another team to understand their challenges, that’s politics too.&lt;/p&gt;
    &lt;p&gt;Good politics is just being strategic about relationships and influence in the service of good outcomes.&lt;/p&gt;
    &lt;p&gt;The best technical leaders are incredibly political. They just don’t call it that. They call it “stakeholder management” or “building alignment” or “organizational awareness.” But it’s politics, and they’re good at it.&lt;/p&gt;
    &lt;p&gt;The engineers who refuse to engage with politics often complain that their companies make bad technical decisions. But they’re not willing to do what it takes to influence those decisions. They want a world where technical merit alone determines outcomes. That world doesn’t exist and never has.&lt;/p&gt;
    &lt;p&gt;This isn’t about becoming a scheming backstabber. As I wrote in Your Strengths Are Your Weaknesses, the same trait can be positive or negative depending on how you use it. Politics is the same way. You can use political skills to manipulate and self-promote, or you can use them to get good ideas implemented and protect your team from bad decisions.&lt;/p&gt;
    &lt;p&gt;Here’s what good politics looks like in practice:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Building relationships before you need them. That random coffee with someone from the data team? Six months later, they’re your biggest advocate for getting engineering resources for your data pipeline project.&lt;/item&gt;
      &lt;item&gt;Understanding the real incentives. Your VP doesn’t care about your beautiful microservices architecture. They care about shipping features faster. Frame your technical proposals in terms of what they actually care about.&lt;/item&gt;
      &lt;item&gt;Managing up effectively. Your manager is juggling competing priorities you don’t see. Keep them informed about what matters, flag problems early with potential solutions, and help them make good decisions. When they trust you to handle things, they’ll fight for you when it matters&lt;/item&gt;
      &lt;item&gt;Creating win-win situations. Instead of fighting for resources, find ways to help other teams while getting what you need. It doesn’t have to be a zero-sum game.&lt;/item&gt;
      &lt;item&gt;Being visible. If you do great work but nobody knows about it, did it really happen? Share your wins, present at all-hands, write those design docs that everyone will reference later.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The alternative to good politics isn’t no politics. It’s bad politics winning by default. It’s the loud person who’s wrong getting their way because the quiet person who’s right won’t speak up. It’s good projects dying because nobody advocated for them. It’s talented people leaving because they couldn’t navigate the organizational dynamics.&lt;/p&gt;
    &lt;p&gt;Stop pretending you’re above politics. You’re not. Nobody is. The only question is whether you’ll get good at it or keep losing to people who already are.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45440571</guid><pubDate>Wed, 01 Oct 2025 17:36:43 +0000</pubDate></item><item><title>Jane Goodall has died</title><link>https://www.latimes.com/obituaries/story/2025-10-01/jane-goodall-chimpanzees-dead</link><description>&lt;doc fingerprint="764eb86741a32728"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Jane Goodall, who transformed understanding of humankind by studying chimpanzees, dies at 91&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; Share via&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Jane Goodall, the trailblazing naturalist whose intimate observations of chimpanzees in the African wild produced powerful insights that transformed basic conceptions of humankind, has died. She was 91.&lt;/p&gt;
    &lt;p&gt;A tireless advocate of preserving chimpanzees’ natural habitat, Goodall died on Wednesday morning in California of natural causes, the Jane Goodall Institute announced on its Instagram page.&lt;/p&gt;
    &lt;p&gt;“Dr. Goodall’s discoveries as an ethologist revolutionized science,” the Jane Goodall Institute said in a statement.&lt;/p&gt;
    &lt;p&gt;A protege of anthropologist Louis S.B. Leakey, Goodall made history in 1960 when she discovered that chimpanzees, humankind’s closest living ancestors, made and used tools, characteristics that scientists had long thought were exclusive to humans.&lt;/p&gt;
    &lt;p&gt;She also found that chimps hunted prey, ate meat, and were capable of a range of emotions and behaviors similar to those of humans, including filial love, grief and violence bordering on warfare.&lt;/p&gt;
    &lt;p&gt;In the course of establishing one of the world’s longest-running studies of wild animal behavior at what is now Tanzania’s Gombe Stream National Park, she gave her chimp subjects names instead of numbers, a practice that raised eyebrows in the male-dominated field of primate studies in the 1960s. But within a decade, the trim British scientist with the tidy ponytail was a National Geographic heroine, whose books and films educated a worldwide audience with stories of the apes she called David Graybeard, Mr. McGregor, Gilka and Flo.&lt;/p&gt;
    &lt;p&gt;“When we read about a woman who gives funny names to chimpanzees and then follows them into the bush, meticulously recording their every grunt and groom, we are reluctant to admit such activity into the big leagues,” the late biologist Stephen Jay Gould wrote of the scientific world’s initial reaction to Goodall.&lt;/p&gt;
    &lt;p&gt;But Goodall overcame her critics and produced work that Gould later characterized as “one of the Western world’s great scientific achievements.”&lt;/p&gt;
    &lt;p&gt;Tenacious and keenly observant, Goodall paved the way for other women in primatology, including the late gorilla researcher Dian Fossey and orangutan expert Birutė Galdikas. She was honored in 1995 with the National Geographic Society’s Hubbard Medal, which then had been bestowed only 31 times in the previous 90 years to such eminent figures as North Pole explorer Robert E. Peary and aviator Charles Lindbergh.&lt;/p&gt;
    &lt;p&gt;In her 80s she continued to travel 300 days a year to speak to schoolchildren and others about the need to fight deforestation, preserve chimpanzees’ natural habitat and promote sustainable development in Africa. She was in California as part of her speaking tour in the U.S. at the time of her death.&lt;/p&gt;
    &lt;p&gt;Jane Goodall brings “The Book of Hope” to the Los Angeles Times Book Club Feb. 25.&lt;/p&gt;
    &lt;p&gt;Goodall was born April 3, 1934, in London and grew up in the English coastal town of Bournemouth. The daughter of a businessman and a writer who separated when she was a child and later divorced, she was raised in a matriarchal household that included her maternal grandmother, her mother, Vanne, some aunts and her sister, Judy.&lt;/p&gt;
    &lt;p&gt;She demonstrated an affinity for nature from a young age, filling her bedroom with worms and sea snails that she rushed back to their natural homes after her mother told her they would otherwise die.&lt;/p&gt;
    &lt;p&gt;When she was about 5, she disappeared for hours to a dark henhouse to see how chickens laid eggs, so absorbed that she was oblivious to her family’s frantic search for her. She did not abandon her study until she observed the wondrous event.&lt;/p&gt;
    &lt;p&gt;“Suddenly with a plop, the egg landed on the straw. With clucks of pleasure the hen shook her feathers, nudged the egg with her beak, and left,” Goodall wrote almost 60 years later. “It is quite extraordinary how clearly I remember that whole sequence of events.”&lt;/p&gt;
    &lt;p&gt;Wildlife biologist Miguel Ordeñana was passionate about wild animals as a kid. In Jane Goodall, he found hope that would shape his life.&lt;/p&gt;
    &lt;p&gt;When finally she ran out of the henhouse with the exciting news, her mother did not scold her but patiently listened to her daughter’s account of her first scientific observation.&lt;/p&gt;
    &lt;p&gt;Later, she gave Goodall books about animals and adventure — especially the Doctor Dolittle tales and Tarzan. Her daughter became so enchanted with Tarzan’s world that she insisted on doing her homework in a tree.&lt;/p&gt;
    &lt;p&gt;“I was madly in love with the Lord of the Jungle, terribly jealous of his Jane,” Goodall wrote in her 1999 memoir, “Reason for Hope: A Spiritual Journey.” “It was daydreaming about life in the forest with Tarzan that led to my determination to go to Africa, to live with animals and write books about them.”&lt;/p&gt;
    &lt;p&gt;Her opportunity came after she finished high school. A week before Christmas in 1956 she was invited to visit an old school chum’s family farm in Kenya. Goodall saved her earnings from a waitress job until she had enough for a round-trip ticket.&lt;/p&gt;
    &lt;p&gt;She arrived in Kenya in 1957, thrilled to be living in the Africa she had “always felt stirring in my blood.” At a dinner party in Nairobi shortly after her arrival, someone told her that if she was interested in animals, she should meet Leakey, already famous for his discoveries in East Africa of man’s fossil ancestors.&lt;/p&gt;
    &lt;p&gt;She went to see him at what’s now the National Museum of Kenya, where he was curator. He hired her as a secretary and soon had her helping him and his wife, Mary, dig for fossils at Olduvai Gorge, a famous site in the Serengeti Plains in what is now northern Tanzania.&lt;/p&gt;
    &lt;p&gt;Leakey spoke to her of his desire to learn more about all the great apes. He said he had heard of a community of chimpanzees on the rugged eastern shore of Lake Tanganyika where an intrepid researcher might make valuable discoveries.&lt;/p&gt;
    &lt;p&gt;When Goodall told him this was exactly the kind of work she dreamed of doing, Leakey agreed to send her there.&lt;/p&gt;
    &lt;p&gt;It took Leakey two years to find funding, which gave Goodall time to study primate behavior and anatomy in London. She finally landed in Gombe in the summer of 1960.&lt;/p&gt;
    &lt;p&gt;On a rocky outcropping she called the Peak, Goodall made her first important observation. Scientists had thought chimps were docile vegetarians, but on this day about three months after her arrival, Goodall spied a group of the apes feasting on something pink. It turned out to be a baby bush pig.&lt;/p&gt;
    &lt;p&gt;Two weeks later, she made an even more exciting discovery — the one that would establish her reputation. She had begun to recognize individual chimps, and on a rainy October day in 1960, she spotted the one with white hair on his chin. He was sitting beside a mound of red earth, carefully pushing a blade of grass into a hole, then withdrawing it and poking it into his mouth.&lt;/p&gt;
    &lt;p&gt;When he finally ambled off, Goodall hurried over for a closer look. She picked up the abandoned grass stalk, stuck it into the same hole and pulled it out to find it covered with termites. The chimp she later named David Graybeard had been using the stalk to fish for the bugs.&lt;/p&gt;
    &lt;p&gt;“It was hard for me to believe what I had seen,” Goodall later wrote. “It had long been thought that we were the only creatures on earth that used and made tools. ‘Man the Toolmaker’ is how we were defined ...” What Goodall saw challenged man’s uniqueness.&lt;/p&gt;
    &lt;p&gt;When she sent her report to Leakey, he responded: “We must now redefine man, redefine tool, or accept chimpanzees as human!”&lt;/p&gt;
    &lt;p&gt;Goodall’s startling finding, published in Nature in 1964, enabled Leakey to line up funding to extend her stay at Gombe. It also eased Goodall’s admission to Cambridge University to study ethology. In 1965, she became the eighth person in Cambridge history to earn a doctorate without first having a bachelor’s degree.&lt;/p&gt;
    &lt;p&gt;In the meantime, she had met and in 1964 married Hugo Van Lawick, a gifted filmmaker who had traveled to Gombe to make a documentary about her chimp project. They had a child, Hugo Eric Louis — later nicknamed Grub — in 1967.&lt;/p&gt;
    &lt;p&gt;Goodall later said that raising Grub, who lived at Gombe until he was 9, gave her insights into the behavior of chimp mothers. Conversely, she had “no doubt that my observation of the chimpanzees helped me to be a better mother.”&lt;/p&gt;
    &lt;p&gt;“So,” Brett Morgen began, “you’ve been telling your story for so many years.&lt;/p&gt;
    &lt;p&gt;She and Van Lawick were married for 10 years, divorcing in 1974. The following year she married Derek Bryceson, director of Tanzania National Parks. He died of colon cancer four years later.&lt;/p&gt;
    &lt;p&gt;Within a year of arriving at Gombe, Goodall had chimps literally eating out of her hands. Toward the end of her second year there, David Graybeard, who had shown the least fear of her, was the first to allow her physical contact. She touched him lightly and he permitted her to groom him for a full minute before gently pushing her hand away. For an adult male chimpanzee who had grown up in the wild to tolerate physical contact with a human was, she wrote in her 1971 book “In the Shadow of Man,” “a Christmas gift to treasure.”&lt;/p&gt;
    &lt;p&gt;Her studies yielded a trove of other observations on behaviors, including etiquette (such as soliciting a pat on the rump to indicate submission) and the sex lives of chimps. She collected some of the most fascinating information on the latter by watching Flo, an older female with a bulbous nose and an amazing retinue of suitors who was bearing children well into her 40s.&lt;/p&gt;
    &lt;p&gt;Her reports initially caused much skepticism in the scientific community. “I was not taken very seriously by many of the scientists. I was known as a [National] Geographic cover girl,” she recalled in a CBS interview in 2012.&lt;/p&gt;
    &lt;p&gt;Her unorthodox personalizing of the chimps was particularly controversial. The editor of one of her first published papers insisted on crossing out all references to the creatures as “he” or “she” in favor of “it.” Goodall eventually prevailed.&lt;/p&gt;
    &lt;p&gt;Her most disturbing studies came in the mid-1970s, when she and her team of field workers began to record a series of savage attacks.&lt;/p&gt;
    &lt;p&gt;The incidents grew into what Goodall called the four-year war, a period of brutality carried out by a band of male chimpanzees from a region known as the Kasakela Valley. The marauders beat and slashed to death all the males in a neighboring colony and subjugated the breeding females, essentially annihilating an entire community.&lt;/p&gt;
    &lt;p&gt;It was the first time a scientist had witnessed organized aggression by one group of non-human primates against another. Goodall said this “nightmare time” forever changed her view of ape nature.&lt;/p&gt;
    &lt;p&gt;“During the first 10 years of the study I had believed ... that the Gombe chimpanzees were, for the most part, rather nicer than human beings,” she wrote in “Reason for Hope: A Spiritual Journey,” a 1999 book co-authored with Phillip Berman. “Then suddenly we found that the chimpanzees could be brutal — that they, like us, had a dark side to their nature.”&lt;/p&gt;
    &lt;p&gt;Critics tried to dismiss the evidence as merely anecdotal. Others thought she was wrong to publicize the violence, fearing that irresponsible scientists would use the information to “prove” that the tendency to war is innate in humans, a legacy from their ape ancestors. Goodall persisted in talking about the attacks, maintaining that her purpose was not to support or debunk theories about human aggression but to “understand a little better” the nature of chimpanzee aggression.&lt;/p&gt;
    &lt;p&gt;“My question was: How far along our human path, which has led to hatred and evil and full-scale war, have chimpanzees traveled?”&lt;/p&gt;
    &lt;p&gt;Her observations of chimp violence marked a turning point for primate researchers, who had considered it taboo to talk about chimpanzee behavior in human terms. But by the 1980s, much chimp behavior was being interpreted in ways that would have been labeled anthropomorphism — ascribing human traits to non-human entities — decades earlier. Goodall, in removing the barriers, raised primatology to new heights, opening the way for research on subjects ranging from political coalitions among baboons to the use of deception by an array of primates.&lt;/p&gt;
    &lt;p&gt;Chimp change&lt;/p&gt;
    &lt;p&gt;Her concern about protecting chimpanzees in the wild and in captivity led her in 1977 to found the Jane Goodall Institute to advocate for great apes and support research and public education. She also established Roots and Shoots, a program aimed at youths in 130 countries, and TACARE, which involves African villagers in sustainable development.&lt;/p&gt;
    &lt;p&gt;She became an international ambassador for chimps and conservation in 1986 when she saw a film about the mistreatment of laboratory chimps. The secretly taped footage “was like looking into the Holocaust,” she told interviewer Cathleen Rountree in 1998. From that moment, she became a globe-trotting crusader for animal rights.&lt;/p&gt;
    &lt;p&gt;In the 2017 documentary “Jane,” the producer pored through 140 hours of footage of Goodall that had been hidden away in the National Geographic archives. The film won a Los Angeles Film Critics Assn. Award, one of many honors it received.&lt;/p&gt;
    &lt;p&gt;In a ranging 2009 interview with Times columnist Patt Morrison, Goodall mused on topics from traditional zoos — she said most captive environments should be abolished — to climate change, a battle she feared humankind was quickly losing, if not lost already. She also spoke about the power of what one human can accomplish.&lt;/p&gt;
    &lt;p&gt;“I always say, ‘If you would spend just a little bit of time learning about the consequences of the choices you make each day’ — what you buy, what you eat, what you wear, how you interact with people and animals — and start consciously making choices, that would be beneficial rather than harmful.”&lt;/p&gt;
    &lt;p&gt;As the years passed, Goodall continued to track Gombe’s chimps, accumulating enough information to draw the arcs of their lives — from birth through sometimes troubled adolescence, maturity, illness and finally death.&lt;/p&gt;
    &lt;p&gt;She wrote movingly about how she followed Mr. McGregor, an older, somewhat curmudgeonly chimp, through his agonizing death from polio, and how the orphan Gilka survived to lonely adulthood only to have her babies snatched from her by a pair of cannibalistic female chimps.&lt;/p&gt;
    &lt;p&gt;Her reaction in 1972 to the death of Flo, a prolific female known as Gombe’s most devoted mother, suggested the depth of feeling that Goodall had for the animals. Knowing that Flo’s faithful son Flint was nearby and grieving, Goodall watched over the body all night to keep marauding bush pigs from violating her remains.&lt;/p&gt;
    &lt;p&gt;“People say to me, thank you for giving them characters and personalities,” Goodall once told CBS’s “60 Minutes.” “I said I didn’t give them anything. I merely translated them for people.”&lt;/p&gt;
    &lt;p&gt;Woo is a former Times staff writer.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45441069</guid><pubDate>Wed, 01 Oct 2025 18:10:39 +0000</pubDate></item><item><title>Increasing your practice surface area</title><link>https://www.indiehackers.com/post/lifestyle/increasing-your-practice-surface-area-agxYGi9bL0gd1WYYQZAu</link><description>&lt;doc fingerprint="bf92913dbb237681"&gt;
  &lt;main&gt;
    &lt;p&gt;The difference between being good and being great isn’t talent or formal training, but the invisible practice that happens when you're just living life.&lt;/p&gt;
    &lt;p&gt;Budapest. Sometime around 1978. It's past 1am and all the lights in a high-rise apartment are out, except for one. A Hungarian girl — not yet 10 years old — sits on the cold bathroom floor balancing a chessboard on her knees.&lt;/p&gt;
    &lt;p&gt;Her father opens the door and finds her there, crying, "Sofia! Leave the pieces alone!"&lt;/p&gt;
    &lt;p&gt;The girl looks up at him. "Daddy," she says almost desperately, "they won't leave me alone!"&lt;/p&gt;
    &lt;p&gt;If you aren't familiar with this story, the girl is Sofia Polgar. In the years following the above scene in the bathroom, she'd go on to achieve one of the highest-performing ratings in chess history, playing for Hungary in four Chess Olympiads and winning two team gold medals, one team silver, three individual golds, and one individual bronze.&lt;/p&gt;
    &lt;p&gt;A lot has been written about the training regimen that Sofia went through with her two sisters: 5–6 hours of daily chess practice alongside studies in multiple languages and high-level mathematics in an apartment packed with thousands of chess books and detailed filing systems of their opponents' histories.&lt;/p&gt;
    &lt;p&gt;But not much has been written — how could it be? — about all the hidden reps Sofia got in outside of her official sessions. Like most elite performers, she had dissolved the boundaries of what counts as training and become high in something I call "practice surface area." It means what it sounds like: the total volume of time and space in your life where practice can happen.&lt;/p&gt;
    &lt;p&gt;Let's say you and a friend decide to learn something new together. Guitar, chess, coding, whatever. You both sign up for the same class, practice for the same scheduled hour each day, watch the same YouTube tutorials.&lt;/p&gt;
    &lt;p&gt;Six weeks later, they’re proficient and you’re still stuttering through the basics.&lt;/p&gt;
    &lt;p&gt;We all know the standard explanation: talent. They’ve got it, you don’t. Some people are just wired for certain things. Better to cut your losses and find something that comes naturally to you.&lt;/p&gt;
    &lt;p&gt;Right?&lt;/p&gt;
    &lt;p&gt;Maybe! Usually what people mean when they call someone "talented" or a "natural" is that the person is genetically gifted. And genetics is real. But it's also not a very satisfying explanation because it's so nonspecific.&lt;/p&gt;
    &lt;p&gt;So if I may, I think what's actually taking place in most cases is a difference in practice surface area. You and your friend both officially practiced for the same "3 hours per week," but in reality your friend put in closer to 30. And they weren't even aware they were doing it.&lt;/p&gt;
    &lt;p&gt;They started hearing music differently. Every song on their commute became a lesson in chord progressions. Their fingers unconsciously worked through scales during meetings. They fell asleep running through the next day's session. They dreamed in tablature.&lt;/p&gt;
    &lt;p&gt;You began practicing guitar. They began living guitar.&lt;/p&gt;
    &lt;p&gt;I like studying world-class performers, and I can’t think of a single high-level pro who isn’t also high in practice surface area.&lt;/p&gt;
    &lt;p&gt;Take George Orwell. In his essay Why I Write, he reveals something that should have disqualified him from ever becoming a writer: he had a terrible time actually sitting down to write. The physical act of writing was torture for him. By his own admission, he would avoid it whenever possible.&lt;/p&gt;
    &lt;p&gt;So how did this writing-avoidant person become one of the most famous prose stylists of the 20th century?&lt;/p&gt;
    &lt;p&gt;Here’s the secret he buried in that same essay:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;For fifteen years or more, I was carrying out a literary exercise of a quite different kind: this was the making up of a continuous “story” about myself, a sort of diary existing only in the mind… For minutes at a time this kind of thing would be running through my head: ‘He pushed the door open and entered the room. A yellow beam of sunlight, filtering through the muslin curtains, slanted on to the table, where a matchbox, half-open, lay beside the inkpot. With his right hand in his pocket he moved across to the window. Down in the street a tortoiseshell cat was chasing a dead leaf,’ etc. etc.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;From childhood until age twenty-five, Orwell was practicing descriptive prose every waking moment. He wasn’t "writing," he was just existing lol. But his brain was secretly logging thousands of hours of narrative practice.&lt;/p&gt;
    &lt;p&gt;This pattern shows up everywhere once you know to look for it.&lt;/p&gt;
    &lt;p&gt;Richard Feynman didn’t become a legendary teacher by practicing lectures. He became one by explaining physics to imaginary students while walking around campus. He’d work through problems out loud in empty rooms, turning every moment of solitude into a teaching rehearsal.&lt;/p&gt;
    &lt;p&gt;Bobby Fischer carried a pocket chess set everywhere and would analyze positions using ceiling tiles as boards while lying in bed. Insomnia became chess study. Waiting rooms became tournaments. His opponents thought they were facing someone with supernatural talent. They were actually facing someone who’d turned every idle moment into chess.&lt;/p&gt;
    &lt;p&gt;In fact I've found so many examples of high practice surface area that I created a companion piece to this essay filled with nothing but examples.&lt;/p&gt;
    &lt;p&gt;Here it is: The hidden training habits of 21 world-class performers.&lt;/p&gt;
    &lt;p&gt;It should go without saying that the best way to increase your practice surface area in a given field is to be obsessed with that field. Obsession makes quick work of formal and bounded training sessions, and it doesn't need "tips" on how to do so.&lt;/p&gt;
    &lt;p&gt;So the question then becomes, "How do I increase my pracrtice surface area if I'm not already obsessed?"&lt;/p&gt;
    &lt;p&gt;I've got a few ideas:&lt;/p&gt;
    &lt;p&gt;Identify the smallest possible practice unit that requires no equipment, setup, or specific location.&lt;/p&gt;
    &lt;p&gt;Like Bobby Fischer analyzing chess positions on ceiling tiles while lying in bed, you need a version of practice so minimal it can happen anywhere, requiring zero setup or equipment.&lt;/p&gt;
    &lt;p&gt;Waiting periods and dead time are great opportunities for visualization sessions where you mentally simulate perfect performance.&lt;/p&gt;
    &lt;p&gt;Michael Phelps would run “mental movies” of perfect races in waiting rooms and before sleep.&lt;/p&gt;
    &lt;p&gt;Layer your craft directly onto daily activities.&lt;/p&gt;
    &lt;p&gt;Maya Angelou composed entire poems while mopping floors. She claims to have used the rhythm of physical work as a metronome for her words.&lt;/p&gt;
    &lt;p&gt;Develop automatic mental habits that keep your craft running in the background of consciousness throughout the day.&lt;/p&gt;
    &lt;p&gt;Eminem can’t turn off the part of his brain that rhymes everything. Every conversation, interview, even argument becomes inadvertent freestyle practice as he generates rhyme patterns for everything he hears.&lt;/p&gt;
    &lt;p&gt;Convert physical limitations and situational constraints into practice parameters that force innovation.&lt;/p&gt;
    &lt;p&gt;The UFC fighter Anderson Silva would practice his striking combinations disguised as dancing at Brazilian clubs. He'd throw actual combat sequences to the rhythm while everyone thought he was just getting down.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45441222</guid><pubDate>Wed, 01 Oct 2025 18:20:49 +0000</pubDate></item><item><title>Pushing the Boundaries of C64 Graphics with Nuflix</title><link>https://cobbpg.github.io/articles/nuflix.html</link><description>&lt;doc fingerprint="b636d9455c53d398"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Pushing the Boundaries of C64 Graphics with NUFLIX&lt;/head&gt;
    &lt;p&gt;While working on my C64 game projects I started thinking about creating nice full-screen images for the title screens and other transitions. For me it’s a big part of the fun when developing for these old machines to create something that goes beyond the state of the art in some way. When it comes to still images, the best format to date is NUFLI. This format allows the creation of images with the full 320×200 resolution while cramming a lot more colours into small areas than one would think possible. It achieves this by cleverly exploiting various undocumented behaviours of the VIC-II.&lt;/p&gt;
    &lt;p&gt;NUFLI is the result of decades of trial and error, integrating several tricks into a single package. I cannot praise highly enough the genius that went into creating it. However, after picking it apart, I realised that there are some ways I could improve it, despite the fact that it’s over 15 years old! I couldn’t resist the temptation to attack the problem, and a few months later my efforts bore fruit: a system called NUFLI eXtended, aka NUFLIX (thanks to Sebaloz for being the first to suggest this name).&lt;/p&gt;
    &lt;p&gt;NUFLIX images can be created using the tool I built, NUFLIX Studio. Here’s a video showing the workflow:&lt;/p&gt;
    &lt;p&gt;In the rest of this article I’ll explain how NUFLI works, how NUFLIX improves upon it, and wrap up with some ideas for the future.&lt;/p&gt;
    &lt;head rend="h2"&gt;NUFLI&lt;/head&gt;
    &lt;p&gt;NUFLI stands for New Underlayed (sic) Flexible Line Interpretation. If this sounds like a random word soup to you, don’t worry, you’re not alone. In a nutshell, this means that a NUFLI image is a combination of two elements:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;a bitmap with more colour than the hardware would normally allow&lt;/item&gt;
      &lt;item&gt;a layer of hardware sprites that cover the whole screen and also has more colour than what you would usually expect&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Hires Bitmap with FLI&lt;/head&gt;
    &lt;p&gt;Out of the box, the C64 offers a selection of text and bitmap modes with various colour configurations. Fundamentally they are all quite similar to each other: both kinds of screens are built from 40×25 blocks of 8×8 pixels. Each block has a byte associated with it in the screen RAM, which is interpreted differently depending on the mode. Text modes use this byte as an index to the character set, where the actual pixel data is pulled from. Bitmap modes display an 8000-byte section of the memory as one bit per pixel, and this frees up the screen RAM byte to be used as colour information instead.&lt;/p&gt;
    &lt;p&gt;Here’s an example to demonstrate how character codes turn into colours when switching from text to high-resolution (from now on: hires) bitmap mode. The BASIC snippet below generates a simple bit pattern with a loop so the colours are visible, then it switches screen mode and sets the address of the bitmap to point to the freshly filled block. The address of the screen RAM is left unchanged. Note that the second picture has an additional line saying “READY.” that’s printed after the snippet runs.&lt;/p&gt;
    &lt;p&gt;NUFLI, specifically, uses the above hires bitmap mode. Out of the box, this mode is limited to two colours per 8×8 block, an “ink” and a “paper”. Since the C64 has a fixed palette of 16 colours, one byte of colour data allows us to choose both freely for each block. For instance, the space character with code $20 turns into red ink (colour 2) on black paper (colour 0).&lt;/p&gt;
    &lt;p&gt;One might think that it would be possible to get more colours in a block by changing the colour byte while the electron beam is in the process of displaying the row in question, but this doesn’t work on the C64. The reason is that due to bandwidth reasons the VIC-II chip has to store a copy of the 40 bytes of screen RAM that applies to the currently displayed row of blocks. Even worse, to make this copy, it needs to stop the CPU and take over the bus for 40 clock cycles (each clock cycle covers 8 pixels being displayed) so it can perform DMA. This happens on the first scanline of every block, and the C64 demoscene settled on the name “badline” for this phenomenon, since it costs precious CPU time.&lt;/p&gt;
    &lt;p&gt;It didn’t take very long for the scene to figure out how to get more colours in a block anyway. The key to many tricks is the hardware smooth scrolling feature of the VIC-II. It’s possible to define a pixel offset of up to 7 pixels both vertically and horizontally; to move the image any further the contents of the memory need to be moved. The logic for scanning out the blocks line by line and keeping track of where we are within a block is defined by a state machine the hardware implements. It turns out that the condition to trigger a badline is simply to set the smooth vertical scroll value such that a block should be starting on the current raster the electron beam is on. Depending on where the beam is horizontally within the line, the resulting effect can vary.&lt;/p&gt;
    &lt;p&gt;When we trigger a badline in the visible area of a block by modifying the vertical scroll position, it comes as a surprise to the VIC-II. First of all, it has to wait three clock cycles to give the CPU a chance to finish possible write operations. During these three cycles it sees values of $ff on the bus instead of screen RAM. In hires bitmap mode, this results in light grey ink on light grey paper in those blocks. Afterwards, it starts updating the rest of its buffer with the current contents of the RAM. Most importantly, it doesn’t reset the row index within the block. This trick is called Flexible Line Interpretation, i.e. FLI (sometimes pronounced “flee”).&lt;/p&gt;
    &lt;p&gt;For demonstration, I wrote a tiny program that waits until the middle of the 4th row of characters, changes some character colours, then adjusts the vertical scroll in the middle of the scanline to trigger a badline. As soon as the CPU resumes execution, it restores the colours and the scroll position so the process can repeat in the next frame. In text mode, the three-cycle wait manifests as three characters with code $ff (a checkerboard pattern in the default character set) and a colour that depends on the code following the trigger.&lt;/p&gt;
    &lt;p&gt;This means that we can easily get new bitmap colours on every single scanline, effectively shrinking the attribute blocks from 8×8 to 8×1 pixels. All we need to do is write two registers: update the base address for the screen RAM, then update the vertical scroll position at the right moment. Unfortunately there’s no way to prevent grey blocks on the left side of the screen, because trying to trigger the badline earlier will actually reset the block completely and repeat its contents from the beginning. The resulting 24-pixel wide grey area is often referred to as the FLI bug.&lt;/p&gt;
    &lt;p&gt;Here’s what happens if we modify the above program to trigger the badline too early, before it is time for the VIC-II to read the first character in the row:&lt;/p&gt;
    &lt;p&gt;NUFLI uses an attribute block size of 8×2, i.e. it triggers FLI every second line, for reasons that should become clear in the following section. Here’s an example of what that looks like with a bitmap where each byte is just the row number from 0 to 199, and the paper colours are kept constant in each character column:&lt;/p&gt;
    &lt;p&gt;Since the first two scanlines of each 8-pixel section follow a normal badline, there’s no FLI bug, and we can see the bitmap’s own colours. The other three quarters of the block get their colours from the FLI that’s carefully triggered on the exact cycle the first block’s colours need to be read, so they are forced to be grey. The grey pixels are still somewhat usable, as we’ll see below.&lt;/p&gt;
    &lt;head rend="h3"&gt;Full-Screen Sprite Layers&lt;/head&gt;
    &lt;p&gt;While it’s nice to be able to increase the vertical colour resolution, having only two colours within every 8-pixel run is still quite limiting. This is where the hardware sprites of the C64 come into play.&lt;/p&gt;
    &lt;p&gt;The C64 has 8 hardware sprites. Each sprite is 24×21 pixels, so its image neatly fits into 63 bytes at one bit per pixel. Sprites can be reused – multiplexed – several times in the same frame by changing their Y coordinates after they started getting displayed, as long as the new Y value is below the bottom line of the active instance, i.e. at least 21 over the previous value. For instance, if we position a sprite with Y coordinate 100, then as soon as we’re on line 101 we can change its Y to 150, and the sprite will be shown in both positions – we can even reprogram it to have different colour and contents between the two instances. Unfortunately, they cannot be reused horizontally, so we can only ever display 8 sprites within a scanline (technically there’s a trick to show 9 sprites, but with severe limitations that make it impractical for anything other than a demonstration).&lt;/p&gt;
    &lt;p&gt;If we line up all sprites in a row, we can only cover 24×8, i.e. 192 pixels out of the 320. Fortunately, the C64 offers the ability to expand the sprites along both axes by doubling their pixels. Another useful feature of the hardware is to be able to control priority: sprites can be set to appear either in front of or behind the background, i.e. the ink layer of the bitmap. In this case, the latter option is more useful, so we use sprites as underlays. This is where the U in NUFLI comes from.&lt;/p&gt;
    &lt;head rend="h4"&gt;Sprite Configuration&lt;/head&gt;
    &lt;p&gt;NUFLI images use a very specific configuration of sprites to make sure that both the FLI bug and the main section benefit as much as possible. The last 8 pixels are limited to showing the rightmost column of the bitmap, as there’s no sprite left to cover them.&lt;/p&gt;
    &lt;p&gt;Having two sprites in the FLI bug area more than makes up for the loss of useful bitmap layers, and we can still put grey ink pixels in front of the sprites. The lower priority sprite over the FLI bug is set to multicolour mode, which means that its horizontal resolution is halved, but we can use three colours instead of just one. Normally the extra two colours are shared among all sprites, but since all the other sprites are set to hires mode, in this case we get three additional independent colours, albeit at half the resolution.&lt;/p&gt;
    &lt;p&gt;The next 288 pixels of each row can use three colours in every 8×1 region: high-resolution ink in front of the low-resolution sprite-paper mix. They are not all independent from each other, since each sprite spans 48 pixels, or 6 blocks horizontally, but this is still a huge improvement for artistic freedom.&lt;/p&gt;
    &lt;p&gt;It’s difficult to picture how such a system works in the abstract, so let’s look at the layers of an existing NUFLI image. This is the Space Harrier title screen from the Game Art Beyond collection.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;→&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;→&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;→&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Generally speaking, the ink layer is used to add fine detail, and the sprites blend the attribute block boundaries.&lt;/p&gt;
    &lt;head rend="h4"&gt;Covering the Whole Screen&lt;/head&gt;
    &lt;p&gt;NUFLI images have all the sprites expanded vertically to double size, but this doesn’t mean that they cannot use the full vertical resolution. This is thanks to the fact that the pointers that specify the address of the sprite’s contents are also located in the currently active screen RAM. When we change the address of the screen RAM, we not only change the colours of the bitmap, but also the sprite pointers, which are read afresh in every scanline. Since the address changes happen on even lines, while the sprite rows advance on odd lines, every scanline gets a unique combination.&lt;/p&gt;
    &lt;p&gt;Normally, in order to fill the whole screen with Y-expanded sprites, we need to move each one of them downwards three times in the visible area. We can set up the top row during initialisation, then as soon as it starts showing, we can change its Y coordinates before reaching the top of the image. This way we can cover up to 83 scanlines (i.e. 42×2-1) out of the 200 without having to use CPU time over the visible background, then advance the Y coordinates by 42 for each following row.&lt;/p&gt;
    &lt;p&gt;The way NUFLI images are set up, the background spans rasters 48-247. This is a necessity because the video chip cannot generate badlines outside this region. It’s also important to know that sprites with Y coordinate N start showing on scanline N+1 due to the way they’re implemented in hardware. The table below shows a possible schedule for covering all 200 background lines by only having to update Y coordinates three times within the area of the image. Also, the timing of those moves is completely flexible, they can be done anywhere within the respective intervals.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Raster&lt;/cell&gt;
        &lt;cell role="head"&gt;Action&lt;/cell&gt;
        &lt;cell role="head"&gt;Sprite Instance&lt;/cell&gt;
        &lt;cell role="head"&gt;Background&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;0-46&lt;/cell&gt;
        &lt;cell&gt;Move sprites to Y = 46&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;47&lt;/cell&gt;
        &lt;cell&gt;Move sprites to Y = 88&lt;/cell&gt;
        &lt;cell&gt;1st&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;48-88&lt;/cell&gt;
        &lt;cell&gt;1st&lt;/cell&gt;
        &lt;cell&gt;Rows 0-40&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;89-130&lt;/cell&gt;
        &lt;cell&gt;Move sprites to Y = 130&lt;/cell&gt;
        &lt;cell&gt;2nd&lt;/cell&gt;
        &lt;cell&gt;Rows 41-82&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;131-172&lt;/cell&gt;
        &lt;cell&gt;Move sprites to Y = 172&lt;/cell&gt;
        &lt;cell&gt;3rd&lt;/cell&gt;
        &lt;cell&gt;Rows 83-124&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;173-214&lt;/cell&gt;
        &lt;cell&gt;Move sprites to Y = 214&lt;/cell&gt;
        &lt;cell&gt;4th&lt;/cell&gt;
        &lt;cell&gt;Rows 125-166&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;215-247&lt;/cell&gt;
        &lt;cell&gt;5th&lt;/cell&gt;
        &lt;cell&gt;Rows 167-199&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;248-256&lt;/cell&gt;
        &lt;cell&gt;5th&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;257-&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The NUFLI implementation leverages another esoteric bug in the VIC-II called sprite crunching. In a nutshell, by toggling Y-expansion just at the right time the video hardware can be tricked into messing up the current sprite offsets that normally advance by 3 bytes per scanline. This causes the sprite to be displayed three times in a row with different offsets, because the hardware is looking for byte offset 63 to conclude the sprite, which it misses due to the misaligned position, and the 6-bit counter wraps around twice before the process ends.&lt;/p&gt;
    &lt;p&gt;In the case of NUFLI, the scrambled sprites cover the first 123 scanlines, and with the additional move during the initialisation we get 165 rows for free in total. Since at this point we have only 35 rows left, we can cover the full screen with sprites by updating each of them only once within the visible area! We just have to make sure that they are all moved before reaching the end of background row 164 (raster 212). All in all, this trick saves us 16 register updates for the Y positions, and we can use the time to update colours instead.&lt;/p&gt;
    &lt;p&gt;This is how the sprite update schedule works specifically in NUFLI:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Raster&lt;/cell&gt;
        &lt;cell role="head"&gt;Action&lt;/cell&gt;
        &lt;cell role="head"&gt;Sprite Instance&lt;/cell&gt;
        &lt;cell role="head"&gt;Background&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;41&lt;/cell&gt;
        &lt;cell&gt;Trigger first raster interrupt&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;43&lt;/cell&gt;
        &lt;cell&gt;Timing stabilised, move sprites to Y = 43&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;44&lt;/cell&gt;
        &lt;cell&gt;Set inital sprite colours&lt;/cell&gt;
        &lt;cell&gt;1st&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;45&lt;/cell&gt;
        &lt;cell&gt;Move sprites to Y = 170&lt;/cell&gt;
        &lt;cell&gt;1st&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;46&lt;/cell&gt;
        &lt;cell&gt;Crunch sprites on their 3rd line&lt;/cell&gt;
        &lt;cell&gt;1st&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;47&lt;/cell&gt;
        &lt;cell&gt;1st, crunched&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;48-170&lt;/cell&gt;
        &lt;cell&gt;1st, crunched&lt;/cell&gt;
        &lt;cell&gt;Rows 0-122&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;171-212&lt;/cell&gt;
        &lt;cell&gt;Move sprites to Y = 212&lt;/cell&gt;
        &lt;cell&gt;2nd&lt;/cell&gt;
        &lt;cell&gt;Rows 123-164&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;213-247&lt;/cell&gt;
        &lt;cell&gt;3rd&lt;/cell&gt;
        &lt;cell&gt;Rows 165-199&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;248-254&lt;/cell&gt;
        &lt;cell&gt;3rd&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;255-&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;All that’s left to unlock the full power of NUFLI is to update the colours of the sprites as we present the picture line by line.&lt;/p&gt;
    &lt;head rend="h3"&gt;Timing and CPU Budget&lt;/head&gt;
    &lt;p&gt;To understand the limitations and the possibilities, we need to take a closer look at how the video hardware interacts with the CPU. The C64 comes in two flavours supporting major video standards: PAL and NTSC. While there are a few variations in the oldest models, in practice we can assume the following timings:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="6"&gt;
        &lt;cell role="head"&gt;Standard&lt;/cell&gt;
        &lt;cell role="head"&gt;Cycles per Second&lt;/cell&gt;
        &lt;cell role="head"&gt;Number of Scanlines&lt;/cell&gt;
        &lt;cell role="head"&gt;Cycles per Scanline&lt;/cell&gt;
        &lt;cell role="head"&gt;Cycles per Frame&lt;/cell&gt;
        &lt;cell role="head"&gt;Frames per Second&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;PAL&lt;/cell&gt;
        &lt;cell&gt;985248&lt;/cell&gt;
        &lt;cell&gt;312&lt;/cell&gt;
        &lt;cell&gt;63&lt;/cell&gt;
        &lt;cell&gt;19656&lt;/cell&gt;
        &lt;cell&gt;50.12&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;NTSC&lt;/cell&gt;
        &lt;cell&gt;1022727&lt;/cell&gt;
        &lt;cell&gt;263&lt;/cell&gt;
        &lt;cell&gt;65&lt;/cell&gt;
        &lt;cell&gt;17095&lt;/cell&gt;
        &lt;cell&gt;59.83&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Since PAL machines get slightly less cycles per scanline, they are more constrained in how much processing they can do between badlines. Therefore we need to build the system for PAL first, and later we can adapt it for NTSC.&lt;/p&gt;
    &lt;p&gt;Refreshing the bitmap colours is not the only thing that steals CPU time. Every active sprite needs two cycles on every scanline it spans so the video hardware can look up its pointer, and read the three bytes to display on the upcoming line. Sprite DMA takes place in the side border area. Since we have all 8 sprites active throughout the whole screen, there’s basically no time left for the CPU to run when there’s a badline.&lt;/p&gt;
    &lt;p&gt;Let’s look at the breakdown of what happens in each cycle of a scanline on a PAL system in various scenarios relevant to NUFLI. The regions with yellow background are data transfers from RAM to the video hardware, while the grey ones are the three-cycle wait periods where the VIC-II gives the CPU a chance to finish any pending write cycles. In the case of NUFLI there’s really only one write cycle that could be potentially clawed back, which I marked with a ✏️.&lt;/p&gt;
    &lt;p&gt;Every block consists of 8 scanlines. The first one is a normal badline, while the 3rd, 5th and 7th ones are FLI lines. The initial badline leaves exactly one cycle for the CPU in the whole scanline, so we can’t even run a single instruction to completion. As for FLI lines, they have 4 CPU cycles available, which are taken up by exactly one 4-cycle instruction: the write to the vertical scroll register. The moment the write happens is marked by ❌, and the CPU stops in the next cycle.&lt;/p&gt;
    &lt;p&gt;It turns out that with all sprites enabled it is impossible to perform FLI on every line over the full 40-block width of the screen! There’s simply no time to perform the necessary steps: updating the screen RAM address, then updating the vertical scroll, since these operations would normally take 12 cycles altogether, and we’re missing 8. In other words, we’d need to limit the number of active sprites to 4. Or, if we are galaxy brain demosceners, we can play sudoku with some undocumented instructions of the 6510 CPU, and do it with 6 sprites. And, by the way, we’d still need to find time to update the Y positions to be able to cover the bottom section of the screen.&lt;/p&gt;
    &lt;p&gt;Since we’re forced to leave every other scanline with the same bitmap colours, we get at least 44 clock cycles to play with in each two-line section. It takes 6 cycles to write an arbitrary value to an arbitrary location: 2 cycles to store the value in a CPU register, then 4 cycles to write it. Consequently, we can perform 7 updates (not counting the three FLI triggers). One of these is needed for modifying the screen RAM address, so we really only get 6 updates. We have a few cycles to spare, which is used to squeeze in an early update to the vertical scroll register at the end, so the next block can start with a normal badline again.&lt;/p&gt;
    &lt;p&gt;NUFLI images contain a table that specifies 6×101 register update slots, one byte for each. The reason it’s 101 and not 100 is because the first row of the table contains the initial values of the 6 wide underlays (the initial values of the FLI bug colours come from another location). These are the possible values:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Value&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;$0x&lt;/cell&gt;
        &lt;cell&gt;Set the colour of the underlay corresponding to the table column to x&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;$1x&lt;/cell&gt;
        &lt;cell&gt;Update the Y coordinate of sprite x for the last section of the screen&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;$2x&lt;/cell&gt;
        &lt;cell&gt;Set the border colour to x&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;$5x&lt;/cell&gt;
        &lt;cell&gt;Set FLI bug multicolour sprite colour 1 to x&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;$6x&lt;/cell&gt;
        &lt;cell&gt;Set FLI bug multicolour sprite colour 2 to x&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;$7x&lt;/cell&gt;
        &lt;cell&gt;Set FLI bug hires sprite colour to x&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;$ex&lt;/cell&gt;
        &lt;cell&gt;Set FLI bug multicolour sprite colour 3 to x&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;When the image display routine is executed, the program uses this table to generate the FLI code on the fly. The generated code is slightly different depending on whether the machine is PAL or NTSC, thereby supporting both with a single executable. The code is very simple in structure, it just unconditionally writes VIC-II registers for the whole duration of the screen with the correct timing.&lt;/p&gt;
    &lt;p&gt;Due to the way the code generator works, values of $8x–$dx also have an effect: they set the colour of the underlay given by the high nybble minus 7 to x, i.e. $8x updates the colour of the leftmost underlay column regardless of which slot of the table it is in, $9x updates the second column etc. However, this is undocumented behaviour and probably never used.&lt;/p&gt;
    &lt;p&gt;In every row, the updates are performed in the order specified by the table. The timing of the code is set up such that the nth write precedes the nth underlay column, so colour updates always happen on time. Unfortunately, this leads to a very unpleasant property of NUFLI: the colour updates of the underlays are not in sync with the bitmap’s, because they are done on the second line of each bitmap colour block. This makes the format difficult to pixel in manually.&lt;/p&gt;
    &lt;p&gt;It is possible to reorganise the colour updates such that 5 of the 6 underlay columns would actually be aligned with the bitmap (the rightmost one is problematic, because it overlaps with the CPU getting stopped for sprite DMA), but this alternative option got largely forgotten over time due to never having been properly implemented. The justification for the current setup (see post #8 in this thread) is that it allows the artist to have two different sprite colours within the same bitmap colour block, which can be useful sometimes, and of course it’s more regular than one with an odd column.&lt;/p&gt;
    &lt;head rend="h3"&gt;Conversion Process&lt;/head&gt;
    &lt;p&gt;NUFLI images are never created by hand from scratch. Instead, the general workflow is to convert an image to NUFLI using Mufflon, then optionally fix the most glaring issues by hand using the NUFLI Editor, which runs on the C64 itself. The fact that such an editor could be squeezed into the C64’s constraints is nothing short of a miracle!&lt;/p&gt;
    &lt;p&gt;The converter starts by preparing the image, so that it’s limited to the C64’s palette and resolution. The conversion consists of three main phases:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Determine the best bitmap and sprite colours to replicate the image with the least error.&lt;/item&gt;
      &lt;item&gt;Compute the register update table to realise those colour choices as closely as possible.&lt;/item&gt;
      &lt;item&gt;Generate the bitmap and sprite patterns given the final colours such that the error is minimised.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It’s not possible to perform all the potential colour updates that the scheme would theoretically allow. These are the potential register updates within a section:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;6 main section underlay colours&lt;/item&gt;
      &lt;item&gt;4 bug underlay colours&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;On top of that, we need to find the time to update the Y coordinates of each sprite once. As for border colours, the converter doesn’t support them, they are only possible to add in the editor by hand. But still, if we want to update 10 colours in a section, that just cannot happen, and something’s got to give.&lt;/p&gt;
    &lt;p&gt;Since the main area of the picture is the most important, the converter prioritises the 6 underlay columns. Broken down in a bit more detail, the high-level steps are the following:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Determine the best sprite and bitmap colours for the 288-pixel main section with the X-expanded underlays.&lt;/item&gt;
      &lt;item&gt;Build the table and mark its free slots. A slot is free when there’s no colour update required for a given column in a given section.&lt;/item&gt;
      &lt;item&gt;Determine the best sprite and bitmap colours for the 24-pixel FLI bug section, limiting the possible sprite colour changes based on the number of free table slots in any given section. Update slots as needed.&lt;/item&gt;
      &lt;item&gt;Find 8 free slots for the sprite Y coordinate updates in the relevant part of the picture.&lt;/item&gt;
      &lt;item&gt;Generate the bitmap and sprite patterns given the final colours as defined by the table.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Step 1 is performed top to bottom, each of the 6 columns in parallel. The system tries all sprite and bitmap colour combinations for each 48×1-pixel section of the first scanline and picks those that can approximate the input image the best. Then it moves on to the second line, assumes that the bitmap colours stay the same and tries all the potential sprite colours to minimise error. Then it gets to row three, where the sprite colours of the previous row are kept, but new bitmap colours can be chosen. These odd-even rules are applied alternately until the last row of the picture.&lt;/p&gt;
    &lt;p&gt;Step 3 is also performed top to bottom, but two rows at a time, since the colours are aligned between the bitmap and the sprites. This is ensured by not allowing the FLI bug sprite colour changes to take place too early, so they are excluded from the first column’s table slots. The first section can be freely chosen, since its colours are set in the initialisation phase. Then for each additional section the system performs a brute force search of all possibilities that can be reached with the free slots of the update table in that row.&lt;/p&gt;
    &lt;p&gt;In practice, this scheme works nicely, and even the left edge of a image is fairly well reproduced, since usually there’s not that much going on there anyway.&lt;/p&gt;
    &lt;head rend="h2"&gt;NUFLIX&lt;/head&gt;
    &lt;p&gt;After dissecting a few NUFLI pictures, it became very clear to me that a lot of register update slots tend to go unused, because it’s really not necessary to change underlay colours that often. The sprite layer in the Space Harrier example above is quite typical in this regard. This realisation inspired me to ask the question: what if we allowed the main section underlays to potentially change their colour in every scanline? This would bring two crucial advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Artistic freedom: no need to fight the overlapping sections and play whack-a-mole with visual artifacts. If a sprite colour needs to be changed, it won’t affect its vertical neighbours. If some detail needs it, a colour can appear for even just a single scanline in the underlay.&lt;/item&gt;
      &lt;item&gt;Independent blocks: every 48×2 block in the main section is now completely separate from its neighbours, so the search for the best colours can be fully parallelised and implemented on GPUs much more efficiently than before.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This question eventually led to the creation of NUFLIX. The main difference from NUFLI is that instead of building a table to generate code from, we generate the code itself ahead of time. Doing so allows us to be more flexible, since we can use the power of a modern computer to do the hard work.&lt;/p&gt;
    &lt;head rend="h3"&gt;Generalised Updates&lt;/head&gt;
    &lt;p&gt;NUFLIX has to work with the exact same CPU budget as NUFLI. However, it allows more potential updates in each section:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;6×2 underlay colour updates in the main section, as each of them can change colour twice in every two-line block if needed&lt;/item&gt;
      &lt;item&gt;4 FLI bug colour updates&lt;/item&gt;
      &lt;item&gt;1 border colour update&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Besides these, it also needs to deal with the sprite Y coordinates the same way as NUFLI. Now, it would be possible to emit fully NUFLI compliant images by exploiting the undocumented feature to update any sprite colour from any slot by using the $8x–$dx value range. The problem is that due to the hardwired timing of the generated code, we couldn’t handle a lot of combinations that would otherwise fit in the CPU time budget. Instead, I opted to generate more efficient code that takes advantage of identical values being set to different registers (e.g. changing several sprites to the same colour). In practice, we can often fit 8-9 updates in a section with this system.&lt;/p&gt;
    &lt;p&gt;Due to the more flexible nature of handling the register updates, the optimisation process is slightly different from NUFLI. Instead of dealing with the main area and the FLI bug in separate phases, everything is thrown into a bag and sorted out in a single pass. This includes not just the colour updates and the sprite Y positions, but also the two writes necessary for the FLI portion itself: changing the screen address and the vertical scroll position.&lt;/p&gt;
    &lt;p&gt;The overall conversion process consists of the following steps:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Determine the best bitmap and sprite colours to replicate the image with the least error separately for each section (every 48×2 block in the main part and every 24×2 block over the FLI bug).&lt;/item&gt;
      &lt;item&gt;Assign the colours of the FLI bug sprites to the four available slots in a way that minimises the amount of register updates necessary to realise it.&lt;/item&gt;
      &lt;item&gt;Collect all the register updates needed to display the image. Every update includes a target address, a value and timing constraints.&lt;/item&gt;
      &lt;item&gt;Generate the code that executes as many updates as possible such that all the timing constraints are respected. If not all updates fit in the time budget, use some heuristics to skip the least important ones.&lt;/item&gt;
      &lt;item&gt;Generate the bitmap and sprite patterns given the final colours such that the error is minimised.&lt;/item&gt;
      &lt;item&gt;Deal with differences in video standards.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Let’s go through these steps in detail.&lt;/p&gt;
    &lt;head rend="h3"&gt;Finding the Best Colours&lt;/head&gt;
    &lt;p&gt;Allowing every sprite in the main section to change colour on every line means that each block of 48×2 pixels can be checked independently.&lt;/p&gt;
    &lt;p&gt;In each block we have two lines of sprites with 16 possible colours each, hence 16×16 = 256 possible combinations to check. For each combination we compute the choice of bitmap colours – 6 ink-paper combinations – that minimise the error with respect to the input image. We determine the error by assuming the chosen bitmap and sprite colours, then trying all the 7 possible patterns for every pair of pixels:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Pixel 1&lt;/cell&gt;
        &lt;cell role="head"&gt;Pixel 2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Paper&lt;/cell&gt;
        &lt;cell&gt;Paper&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Paper&lt;/cell&gt;
        &lt;cell&gt;Ink&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Ink&lt;/cell&gt;
        &lt;cell&gt;Paper&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Ink&lt;/cell&gt;
        &lt;cell&gt;Ink&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Sprite&lt;/cell&gt;
        &lt;cell&gt;Sprite&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Sprite&lt;/cell&gt;
        &lt;cell&gt;Ink&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Ink&lt;/cell&gt;
        &lt;cell&gt;Sprite&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;In other words, we cannot combine the sprite and the paper colours within any pair of pixels, we have to choose either one or the other. Fortunately the bitmap colours can be evaluated independently for each of the 6 blocks, because the errors are additive.&lt;/p&gt;
    &lt;p&gt;All told, this boils down to a bunch of straightforward nested loops: 256 sprite colours × 6 attribute blocks × 256 bitmap colours × 8 pixel pairs × 7 patterns. Given that there are 6×100 sections in the main area, the innermost loop has to go through over 13 billion iterations.&lt;/p&gt;
    &lt;p&gt;This is the most time-consuming step in the whole conversion process, but thanks to the fact that the NUFLIX converter implements it as a compute shader, it can run in a split second for the whole image. The actual implementation runs in two phases: first it generates the results for the 256 sprite combinations in parallel for every block, i.e. the error metrics and the best bitmap colours that go with them, then it picks out the lowest error option for each block.&lt;/p&gt;
    &lt;p&gt;We can do the same for the FLI bug area, but the parameters are different. Each independent block is 24×2 pixels, and the unknowns we’re looking for are the four sprite colours to replicate the block as closely as possible. While bitmap colours can be freely chosen in every 4th section, I decided to make things simpler by assuming light grey ink and paper everywhere. The four colours are not independent in this case: the hires sprite layer can be anything but light grey (no point in using that for sprites as we can use ink for those pixels), while the multicolor slots should have no repeated colours, as that would be a waste. So there are 15×14×13×12 combinations to check. Another difference is that for each pixel pair we can choose one of three “background” colours and each pixel can also be the ink or the hires sprite. This adds up to 19 different two-pixel patterns to test.&lt;/p&gt;
    &lt;p&gt;Finally, when choosing the best picks for each block, we should give preference to any option that avoids the use of the sprite layer, because that leaves us with less time pressure during code generation.&lt;/p&gt;
    &lt;head rend="h3"&gt;Pre-Optimising Bug Slots&lt;/head&gt;
    &lt;p&gt;The raw optimisation step gives us up to four sprite colours for each section of the FLI bug. However, when building the final output, we have to assign these to concrete registers. The NUFLIX optimiser implements a simple algorithm that shuffles the colours with the aim to minimise the necessary register updates during execution:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Assign the colours to the 4 slots such that any common colours between subsequent sections are kept in the same slots. New colours prefer to replace the old ones that are the last to be used again (if ever) going downwards.&lt;/item&gt;
      &lt;item&gt;Fill out the unused slots with whatever colour is going to be needed next in that slot, but flag them as not yet needed. Slots like this can be updated with very flexible timings.&lt;/item&gt;
      &lt;item&gt;Rearrange slots so if there’s a colour swap between the hires and one of the multicolour slots, then make sure that the same multicolour slot inherits the previous hires colour. This is a very typical scenario, and it allows us to drop two register updates in one go without introducing too much error when needed.&lt;/item&gt;
      &lt;item&gt;Use bitmap colours where available to get rid of rapid changes in sprite colours where possible. The benefits of this step tend to be marginal, but every little bit counts.&lt;/item&gt;
      &lt;item&gt;To conclude, repeat step 2.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Let’s work through an example image to see what this looks like in practice.&lt;/p&gt;
    &lt;p&gt;Most images don’t have a lot of things going on at the edge so this is quite an extreme example! First we take the 24-pixel strip on the left and feed it into the compute shader to extract the optimal colours. Then we perform the five steps outlined above. The last image shows how we can reproduce the original from layers using the final colours (note that the light grey pixels come from the bitmap).&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;→&lt;/cell&gt;
        &lt;cell&gt;→&lt;/cell&gt;
        &lt;cell&gt;→&lt;/cell&gt;
        &lt;cell&gt;→&lt;/cell&gt;
        &lt;cell&gt;→&lt;/cell&gt;
        &lt;cell&gt;→&lt;/cell&gt;
        &lt;cell&gt;→&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The leftmost colour is always the hires sprite, and the other slots define the multicolour entries. Unused entries are marked with a striped pattern. Note that the hires slot is left untouched, since none of these transformations can affect the picture. The purpose of this step is to make it more likely that all the necessary colour changes will fit in our time budget.&lt;/p&gt;
    &lt;head rend="h3"&gt;Scheduling Register Updates&lt;/head&gt;
    &lt;p&gt;Before discussing the code generation process, let’s look at individual updates and their timing constraints.&lt;/p&gt;
    &lt;p&gt;When changing the colour of some element, we need to avoid the time when it’s being displayed. For instance, column 3 is displayed between cycles 32-37, therefore we either need to change it before cycle 32, so we see the effect already on the current line – which is always odd when our code runs –, or after cycle 37, so it only takes effect on the next line.&lt;/p&gt;
    &lt;p&gt;The ❗ marks the moment where we update column 6 to change colour on an even row. It’s one cycle too early, but the CPU is not available in the next cycle due to sprite DMA. As a result, the colours of the even line spill onto the preceding odd line for the last 8 pixels. Since this happens only near the edge of the image, it’s still useful to allow as an option.&lt;/p&gt;
    &lt;p&gt;The FLI trigger write is marked by ❌, and we assume it to be on cycle 58 for scheduling purposes, which is in reality delayed by the sprite DMA to fall exactly on cycle 14 on the next raster. The goal is to generate code that pads out time until cycle 54 or 55 (the latter must be a write cycle) before emitting the trigger.&lt;/p&gt;
    &lt;p&gt;It’s easier to see what’s going on through an example. Let’s take a slice from the image and look at the register updates needed to display it on the C64.&lt;/p&gt;
    &lt;p&gt;This slice is 6 pixels tall, i.e. it includes three sections. We’ll look at just the register updates needed while the middle section is displayed. Note that the first scanline of each section is a badline, so our code can only execute during the second one (line 77). In this particular area of the picture the colours over the FLI bug area don’t need to be updated.&lt;/p&gt;
    &lt;p&gt;These are the colours of the underlay columns under the main area, the ones changing during the relevant section marked bold:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="7"&gt;
        &lt;cell role="head"&gt;Line&lt;/cell&gt;
        &lt;cell role="head"&gt;Column 1&lt;/cell&gt;
        &lt;cell role="head"&gt;Column 2&lt;/cell&gt;
        &lt;cell role="head"&gt;Column 3&lt;/cell&gt;
        &lt;cell role="head"&gt;Column 4&lt;/cell&gt;
        &lt;cell role="head"&gt;Column 5&lt;/cell&gt;
        &lt;cell role="head"&gt;Column 6&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;76&lt;/cell&gt;
        &lt;cell&gt;blue (6)&lt;/cell&gt;
        &lt;cell&gt;blue (6)&lt;/cell&gt;
        &lt;cell&gt;black (0)&lt;/cell&gt;
        &lt;cell&gt;red (2)&lt;/cell&gt;
        &lt;cell&gt;dark grey (11)&lt;/cell&gt;
        &lt;cell&gt;light blue (14)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;77&lt;/cell&gt;
        &lt;cell&gt;blue (6)&lt;/cell&gt;
        &lt;cell&gt;blue (6)&lt;/cell&gt;
        &lt;cell&gt;black (0)&lt;/cell&gt;
        &lt;cell&gt;pink (10)&lt;/cell&gt;
        &lt;cell&gt;dark grey (11)&lt;/cell&gt;
        &lt;cell&gt;light blue (14)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;78&lt;/cell&gt;
        &lt;cell&gt;dark grey (11)&lt;/cell&gt;
        &lt;cell&gt;blue (6)&lt;/cell&gt;
        &lt;cell&gt;black (0)&lt;/cell&gt;
        &lt;cell&gt;black (0)&lt;/cell&gt;
        &lt;cell&gt;black (0)&lt;/cell&gt;
        &lt;cell&gt;light blue (14)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;In this section we need to perform four colour updates plus the two updates needed for the FLI process. We order them by their final deadline, then by their first available cycle:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Register&lt;/cell&gt;
        &lt;cell role="head"&gt;Value&lt;/cell&gt;
        &lt;cell role="head"&gt;Cycle Range&lt;/cell&gt;
        &lt;cell role="head"&gt;Effect&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;$d02b&lt;/cell&gt;
        &lt;cell&gt;$0a&lt;/cell&gt;
        &lt;cell&gt;11-37&lt;/cell&gt;
        &lt;cell&gt;Set column 4 to pink on this scanline (odd).&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;$d018&lt;/cell&gt;
        &lt;cell&gt;$08&lt;/cell&gt;
        &lt;cell&gt;11-55&lt;/cell&gt;
        &lt;cell&gt;Update the screen RAM address. Only affects the next line.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;$d028&lt;/cell&gt;
        &lt;cell&gt;$0b&lt;/cell&gt;
        &lt;cell&gt;26-55&lt;/cell&gt;
        &lt;cell&gt;Set column 1 to dark grey on the next scanline (even).&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;$d02b&lt;/cell&gt;
        &lt;cell&gt;$00&lt;/cell&gt;
        &lt;cell&gt;44-55&lt;/cell&gt;
        &lt;cell&gt;Set column 4 to black on the next scanline (even).&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;$d02c&lt;/cell&gt;
        &lt;cell&gt;$00&lt;/cell&gt;
        &lt;cell&gt;50-55&lt;/cell&gt;
        &lt;cell&gt;Set column 5 to black on the next scanline (even).&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;$d011&lt;/cell&gt;
        &lt;cell&gt;$3e&lt;/cell&gt;
        &lt;cell&gt;“58”&lt;/cell&gt;
        &lt;cell&gt;Trigger FLI on cycle 14 of the next line.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The code snippet that gets generated from this update schedule looks like this:&lt;/p&gt;
    &lt;code&gt;; At this point the CPU registers are set to the following values:
; a=$3c x=$0e y=$0a

          ; first cycle (write cycle)
sty $d02b ; 11 (14)
lda #$08  ; 15
sta $d018 ; 17 (20)
lda #$0b  ; 21
sta $d028 ; 23 (26)
lda #$00  ; 27
ldx #$3e  ; 29
ldy #$78  ; 31, note: preloaded for use in the next section
nop       ; 33
nop       ; 35
nop       ; 37
nop       ; 39
sta $d02b ; 41 (44)
nop       ; 45
sta $d02c ; 47 (50)
lda #$38  ; 51
nop       ; 53
; Sprite DMA starts here and stops the CPU up to cycle 10 on the next line
stx $d011 ; 11 (14)
&lt;/code&gt;
    &lt;head rend="h3"&gt;Dealing with the Time Budget&lt;/head&gt;
    &lt;p&gt;I decided to distinguish between two kinds of updates: immediate and deferred. Immediate updates are required to be performed on a specific scanline, while deferred updates can happen anywhere within a certain interval of scanlines. The distinction is not strictly necessary, but it made implementation easier for me. Examples of deferred updates are colours preceded by unused values for the same register, or sprite Y coordinates.&lt;/p&gt;
    &lt;p&gt;The code generation process starts by making lists of immediate updates needed for each of the 100 sections, and one global list of deferred updates in the order we’ll be needing them as we’re traversing the image top to bottom.&lt;/p&gt;
    &lt;p&gt;The final output of the code generator is a list of 100 snippets, one for each section. We generate these snippets in order from top to bottom by taking the immediate updates as input and trying to add as many of the currently relevant deferred updates as possible into the mix. If we run out of time while generating the current snippet, we remove an update from the input and try again. Every update is categorised by its priority, and the less important it is, the more likely it is to be removed. These are the categories in the order of increasing importance:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Deferred updates that can be executed later.&lt;/item&gt;
      &lt;item&gt;Deferred updates that expire on the current line.&lt;/item&gt;
      &lt;item&gt;Bug colour swap between the hires and one of the multicolour slots. This implies moving the two updates to the next line.&lt;/item&gt;
      &lt;item&gt;The single bug colour update that introduces as little error as possible is moved to the next line.&lt;/item&gt;
      &lt;item&gt;A colour change in the main section whose top and bottom neighbours are identical, i.e. we can remove two updates while only affecting one 48×1-pixel area.&lt;/item&gt;
      &lt;item&gt;One of the colour changes in a main section column that has two changes scheduled.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;All other register updates, i.e. border colour changes and FLI related updates are exempt from this selection process. When we move a bug colour update to the next line, it becomes candidate for inclusion for the next snippet. If there’s already an update scheduled for the same slot on the next line, then we just drop it completely instead.&lt;/p&gt;
    &lt;p&gt;The NUFLIX editor provides feedback about the CPU time used for each snippet as well as any possible changes that were caused by dropped updates.&lt;/p&gt;
    &lt;p&gt;For instance, the above snippet from line 77 has 12 unused cycles, i.e. about 20% of the time still available. On the above images it’s roughly halfway between the first two red lines.&lt;/p&gt;
    &lt;head rend="h3"&gt;Assembling the Final Image&lt;/head&gt;
    &lt;p&gt;After we complete the code generation phase, we know for sure what colours we can use in the sprite layers. To account for any changes, we regenerate the bitmap and sprite layers with this knowledge in mind. If we’re converting an image, then we use that as a reference for the error metrics. On the other hand, if the layers are being directly edited, then we merge them and use the result as a reference image instead.&lt;/p&gt;
    &lt;p&gt;This is the point where we have to consider NTSC machines. NUFLI takes care of this by checking for the video standard during startup, and patching itself so the code generator emits slightly different instructions. In terms of timing, the difference between PAL and NTSC is that the latter adds a cycle before sprite DMA, and another one after. Altogether we get 4 extra cycles during each section (2 per scanline), but not all of them are necessarily additional. For instance, whenever we use cycle 55 for a last write in PAL, we don’t get an extra cycle in NTSC.&lt;/p&gt;
    &lt;p&gt;Taking everything into account, we can modify the PAL code by inserting delays of 1 to 4 cycles in it in various places. Since the fastest instructions take 2 cycles, the way to insert a single cycle is to change an instruction. For instance, an instruction that loads an immediate value into a register can be changed to load a value from the zero page instead. Also, instructions that write the A register can be changed into indexed writes, so they execute in 5 cycles instead of 4. The NUFLIX exporter makes a list of the necessary modifications and saves them into the file. Each modification is described with a single-byte command, and when the displayer routine detects an NTSC machine, it runs a small interpreter over these commands to rewrite the code in place.&lt;/p&gt;
    &lt;head rend="h3"&gt;Workflow Improvements&lt;/head&gt;
    &lt;p&gt;The original goal for this project was to improve the expressiveness of NUFLI, but something interesting happened along the way. Relaxing some of the constraints unlocked the possibility of speeding up the conversion process by orders of magnitude. With Mufflon, the best an artist can do is make some changes to the source image, re-run the converter, and wait at least several seconds to see how it turns out. NUFLIX makes this step basically instantaneous, and offers two additional features for a seamless experience:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The tool can interface with a running instance of the VICE emulator through the binary monitor. After a connection is established, the results of every little change are instantly displayed inside the emulator.&lt;/item&gt;
      &lt;item&gt;It’s possible to watch the input file for changes, and automatically trigger the conversion process when it happens. This allows the artist to work on the image in their preferred editor, and whenever they save, they can see how the final image will look within a fraction of a second.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Even better, when using the built-in editor for placing pixels, it’s possible to optimise the image incrementally. Used together with the VICE bridge, the feedback for the user is continuous, so it’s much easier to develop an intuition for the limitations imposed by the hardware. I’m reminded of Bret Victor’s famous talk, Inventing on Principle, which demonstrated with several examples how speeding up a workflow dramatically can result in a truly qualitative change in perception and lead to a whole new level of understanding. My hope is that NUFLIX will also help some artists in a similar manner.&lt;/p&gt;
    &lt;head rend="h2"&gt;Final Thoughts&lt;/head&gt;
    &lt;p&gt;NUFLIX is a nice improvement over what we had before, but it’s far from the ultimate solution. In the grand scheme of things, I had a fairly easy problem to solve, because brute force was a viable approach. The C64 video hardware has many features that could be combined in order to find the closest representation of an input image. For instance, we could switch both the bitmap or the sprite layers between hires and multicolour throughout the picture. We could allow different sprite configurations that adapt better to the image we’re trying to recreate. We can elect to perform FLI with a different cadence to free up the CPU. The problem is that many of these capabilities lead to non-local effects that would require a much more complex search algorithm to optimise.&lt;/p&gt;
    &lt;p&gt;There are two major directions this system could be developed further:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Expand the algorithm to use as many of the hardware features as possible, and allow processing time to get arbitrarily long in order to find the best possible representation in an unsupervised manner. The final output of this algorithm wouldn’t be possible to fix manually, since it would rely heavily on the order of operations and a very specific memory layout.&lt;/item&gt;
      &lt;item&gt;Add more options, but only as long as the tight feedback loop isn’t compromised. For instance, it would be straightforward to allow the artist to customise the sprite configuration depending on the needs of the piece they’re working on, or e.g. offer the ability to turn off FLI, which would both free up CPU capacity and prevent the grey bug. In short, this is the direction where we’d focus on building an interactive tool that offers a high level of control.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Both of these philosophies could lead to very interesting results and pose exciting technical challenges. With some luck, we won’t have to wait another 15 years for the next improvement. ;)&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45441364</guid><pubDate>Wed, 01 Oct 2025 18:30:36 +0000</pubDate></item><item><title>AI has had zero effect on jobs so far, says Yale study</title><link>https://www.theregister.com/2025/10/01/ai_isnt_taking_people_jobs/</link><description>&lt;doc fingerprint="7d6501bd2aa483cf"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;AI has had zero effect on jobs so far, says Yale study&lt;/head&gt;
    &lt;head rend="h2"&gt;Other studies are finding the same thing&lt;/head&gt;
    &lt;p&gt;Yale researchers say that despite the anxiety about AI taking people's jobs, there's very little evidence of it actually happening.&lt;/p&gt;
    &lt;p&gt;Economists with Yale's Budget Lab, a non-partisan policy research group, took a look at how US employment has changed since the November 2022 debut of ChatGPT and the sequent release of other generative AI models.&lt;/p&gt;
    &lt;p&gt;They saw nothing to be alarmed about.&lt;/p&gt;
    &lt;p&gt;"Overall, our metrics indicate that the broader labor market has not experienced a discernible disruption since ChatGPT’s release 33 months ago, undercutting fears that AI automation is currently eroding the demand for cognitive labor across the economy," said Martha Gimbel, Molly Kinder, Joshua Kendall, and Maddie Lee in a report summary.&lt;/p&gt;
    &lt;p&gt;The leaders of AI companies have been stoking those fears – an effective way to get meetings with lawmakers. In May, Anthropic CEO Dario Amodei expressed concern that within five years, AI could cut the number of entry-level white collar jobs in half. OpenAI CEO Sam Altman has made similar pronouncements.&lt;/p&gt;
    &lt;p&gt;And major companies conducting layoffs like IBM and Salesforce have held themselves up as examples of that narrative, though their employee culls may be more focused on outsourcing than automation. Smaller companies like Fiverr have also cited AI amid layoffs.&lt;/p&gt;
    &lt;p&gt;AI cheerleader Microsoft recently added fuel to the fire with a report on jobs most likely to be affected by AI, only to later distance itself from the blaze by noting, "our study does not draw any conclusions about jobs being eliminated."&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Microsoft moves to the uncanny valley with creepy Copilot avatars that stare at you and say your name&lt;/item&gt;
      &lt;item&gt;Nadella hands Microsoft money machine off to new commercial CEO so he can visioneer the future&lt;/item&gt;
      &lt;item&gt;JetBrains wants to train AI models on your code snippets&lt;/item&gt;
      &lt;item&gt;Google bolts AI into Drive to catch ransomware, but crooks not shaking yet&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In Microsoft's own case, the culls appear to be a way to reduce expenses and mollify investors following its massive capital expenditures on data centers that fuel its AI ambitions.&lt;/p&gt;
    &lt;p&gt;The Yale researchers' nothingburger result has precedent. In 2023, a study by the United Nations International Labour Organization (ILO) concluded that generative AI would probably not replace most workers.&lt;/p&gt;
    &lt;p&gt;A study of Danish workers published in April determined that generative AI had no material impact on wages or jobs. Another such study published in February found "overall employment effects are modest, as reduced demand in exposed occupations is offset by productivity-driven increases in labor demand at AI-adopting firms."&lt;/p&gt;
    &lt;p&gt;There is some contradictory data. A recent Stanford Digital Economy Lab study claims that recent college graduates in occupations most exposed to AI have seen a 13 percent relative decline in employment compared to occupations more insulated from AI.&lt;/p&gt;
    &lt;p&gt;But the consensus appears to be that generative AI has not had a meaningful impact on the labor market so far. Enterprise skepticism of the technology may be one reason for that. ®&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45442743</guid><pubDate>Wed, 01 Oct 2025 20:07:44 +0000</pubDate></item><item><title>Unbound Academy hasn’t replaced teachers with AI</title><link>https://danmeyer.substack.com/p/the-truth-about-2-hour-learning-and</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45443004</guid><pubDate>Wed, 01 Oct 2025 20:25:59 +0000</pubDate></item><item><title>Microsoft declares bring your Copilot to work day, usurping IT authority</title><link>https://www.theregister.com/2025/10/01/microsoft_consumer_copilot_corporate/</link><description>&lt;doc fingerprint="3605bb1ae2dfe8d7"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Microsoft declares bring your Copilot to work day, usurping IT authority&lt;/head&gt;
    &lt;head rend="h2"&gt;Use your home subscription with your work Microsoft 365 account&lt;/head&gt;
    &lt;p&gt;Your job may not support BYOD, but how about BYOC? Microsoft has declared that people can bring their personal Microsoft 365 subscriptions to work to access various Copilot features at companies that fail to provide an AI fix.&lt;/p&gt;
    &lt;p&gt;Redmond has done so unilaterally, effectively endorsing "shadow IT" – the practice of bringing unapproved software and devices into the workplace.&lt;/p&gt;
    &lt;p&gt;Earlier this year, Microsoft said it had adopted a new approach to shadow IT. "While earlier eras of our IT history focused on trying to prevent shadow IT, we are now concentrating on managing it," the biz said in a blog post. By "managing," Microsoft also means "enabling."&lt;/p&gt;
    &lt;p&gt;Samer Baroudi, senior product marketing manager at Microsoft, insists this is for your own good.&lt;/p&gt;
    &lt;p&gt;"This offers a safer alternative to other bring-your-own-AI scenarios, and empowers users with Copilot in their daily jobs while keeping IT firmly in control and all enterprise data protections intact," Baroudi explained in a blog post.&lt;/p&gt;
    &lt;p&gt;Makers of competing AI products might disagree.&lt;/p&gt;
    &lt;p&gt;Microsoft says that employees can sign into Microsoft 365 apps using both personal and work accounts and now can use Copilot features from their personal plan (Personal, Family, or Premium) for business documents – even if their work account lacks a Copilot license.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Raspberry Pi prices hiked as AI gobbles all the memory&lt;/item&gt;
      &lt;item&gt;AI has had zero effect on jobs so far, says Yale study&lt;/item&gt;
      &lt;item&gt;Air Force admits SharePoint privacy issue as reports trickle out of possible breach&lt;/item&gt;
      &lt;item&gt;Hundreds of orgs urge Microsoft: don't kill off free Windows 10 updates&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;IT admins miffed at having their authority usurped by a diktat from Redmond can console themselves with the knowledge that Copilot's level of access "is strictly governed by the user’s work account permissions, ensuring enterprise data remains protected." The user's Entra (work) identity governs file permissions and access controls.&lt;/p&gt;
    &lt;p&gt;Also, "IT retains full control and oversight" – apart from the bit about allowing this to happen in the first place.&lt;/p&gt;
    &lt;p&gt;Admins have the ability to disallow personal Copilot usage on work documents using cloud policy controls. And they can audit personal Copilot interactions and can apply enterprise identity, permission, and compliance policies.&lt;/p&gt;
    &lt;p&gt;Government tenants (GCC/DoD) for some reason don't support this capability, the one that Baroudi insists "does not create new data exposure risks."&lt;/p&gt;
    &lt;p&gt;Meanwhile, employees who decide to fire up their personal Copilot accounts within the workplace should be mindful that their prompts and responses will be captured by their employer.&lt;/p&gt;
    &lt;p&gt;As to why Microsoft would bother, Baroudi provides a hint in the FAQs detailing the bring-your-own-Copilot-to-work initiative that accompanies his post.&lt;/p&gt;
    &lt;quote&gt;Can use of Copilot from personal Microsoft 365 subscriptions help drive AI adoption?&lt;lb/&gt;Yes. It allows users to experience AI productivity benefits while IT retains control.&lt;/quote&gt;
    &lt;p&gt;Of course, when Microsoft next cites enterprise adoption statistics for its AI products, it will be worth asking whether the company is counting personal usage of Copilot. ®&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45443304</guid><pubDate>Wed, 01 Oct 2025 20:48:23 +0000</pubDate></item><item><title>DARPA project for automated translation from C to Rust (2024)</title><link>https://www.darpa.mil/news/2024/memory-safety-vulnerabilities</link><description>&lt;doc fingerprint="eb7bf9db982fa7dc"&gt;
  &lt;main&gt;
    &lt;head rend="h4"&gt;DARPA initiates a new program to automate the translation of the worldâs highly vulnerable legacy C code to the inherently safer Rust programming language&lt;/head&gt;
    &lt;head rend="h4"&gt;Jul 31, 2024&lt;/head&gt;
    &lt;p&gt;Memory safety vulnerabilities are the most prevalent type of disclosed software vulnerability1 and affect a computer's memory in two primary ways. First, programming languages like C allow programmers to manipulate memory directly, making it easy to accidentally introduce errors in their program that would enable a seemingly routine operation to corrupt the state of memory. Second, memory safety issues can arise when a programming language exhibits an âundefined behavior.â Undefined behaviors happen when the programming language standard provides no specification or guidance on how the program should behave under conditions not explicitly defined in the standard.&lt;/p&gt;
    &lt;p&gt;After more than two decades of grappling with memory safety issues in C and C++, the software engineering community has reached a consensus. Relying on bug-finding tools is not enough. Even the Office of the National Cyber Director has called for more proactive approaches to eliminate memory safety vulnerabilities to reduce potential attacks2.&lt;/p&gt;
    &lt;p&gt;While it's been no secret that memory safe programming languages can eliminate memory safety vulnerabilities, the challenge has been rewriting legacy code at scale that matches the vastness of the problem. The C language was created in the 1970s and has become ubiquitous. It has been used to develop applications that run everything from modern smartphones to space vehicles and beyond. And the Department of Defense has long-lived systems that disproportionately depend on programming languages like C.&lt;/p&gt;
    &lt;p&gt;However, in recent years, a cultural shift toward the programming language Rust and recent breakthroughs in machine learning techniques, like large language models (LLMs), have created an environment that may lend itself to a new class of solutions.&lt;/p&gt;
    &lt;p&gt;DARPAâs Translating All C to Rust (TRACTOR) program wants to seize this opportunity by substantially automating the translation of the worldâs legacy C code to Rust.&lt;/p&gt;
    &lt;p&gt;âYou can go to any of the LLM websites, start chatting with one of the AI chatbots, and all you need to say is âhere's some C code, please translate it to safe idiomatic Rust code,â cut, paste, and something comes out, and it's often very good, but not always,â said Dr. Dan Wallach, DARPA program manager for TRACTOR. âThe research challenge is to dramatically improve the automated translation from C to Rust, particularly for program constructs with the most relevance."&lt;/p&gt;
    &lt;p&gt;TRACTOR will strive to create the same quality and style that a skilled Rust developer would produce, thereby eliminating the entire class of memory safety security vulnerabilities in C programs.&lt;/p&gt;
    &lt;p&gt;Wallach anticipates proposals that include novel combinations of software analysis, such as static and dynamic analysis, and large language models. The program will host public competitions throughout the effort to test the capabilities of the LLM-powered solutions.&lt;/p&gt;
    &lt;p&gt;"Rust forces the programmer to get things right,â said Wallach. âIt can feel constraining to deal with all the rules it forces, but when you acclimate to them, the rules give you freedom. They're like guardrails; once you realize they're there to protect you, you'll become free to focus on more important things."&lt;/p&gt;
    &lt;p&gt;DARPA will sponsor a Proposers Day on Aug. 26, 2024, which attendees can attend in person or virtually. Participants must register by Aug. 19, 2024. Details and registration info are available at SAM.Gov.&lt;/p&gt;
    &lt;p&gt;[1]https://www.cisa.gov/sites/default/files/2023-12/The-Case-for-Memory-Safe-Roadmaps-508c.pdf&lt;/p&gt;
    &lt;p&gt;[2]https://www.whitehouse.gov/oncd/briefing-room/2024/02/26/memory-safety-fact-sheet/&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45443368</guid><pubDate>Wed, 01 Oct 2025 20:53:17 +0000</pubDate></item><item><title>Edge264 – Minimalist, high-performance software decoder for H.264/AVC video</title><link>https://github.com/tvlabs/edge264</link><description>&lt;doc fingerprint="2658de598d253911"&gt;
  &lt;main&gt;
    &lt;p&gt;Minimalist software decoder with state-of-the-art performance for the H.264/AVC video format.&lt;/p&gt;
    &lt;p&gt;Please note this is a work in progress and will be ready for use after making GStreamer/VLC plugins.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Supports Progressive High and MVC 3D profiles, up to level 6.2&lt;/item&gt;
      &lt;item&gt;Any resolution up to 8K UHD&lt;/item&gt;
      &lt;item&gt;8-bit 4:2:0 planar YUV output&lt;/item&gt;
      &lt;item&gt;Slices and Arbitrary Slice Order&lt;/item&gt;
      &lt;item&gt;Slice and frame multi-threading&lt;/item&gt;
      &lt;item&gt;Per-slice reference picture list&lt;/item&gt;
      &lt;item&gt;Memory Management Control Operations&lt;/item&gt;
      &lt;item&gt;Long-term reference frames&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Windows: x86, x64&lt;/item&gt;
      &lt;item&gt;Linux: x86, x64, ARM64&lt;/item&gt;
      &lt;item&gt;Mac OS: x64&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;edge264 is entirely developed in C using 128-bit vector extensions and vector intrinsics, and can be compiled with GNU GCC or LLVM Clang. SDL2 runtime library may be used (optional) to enable display with &lt;code&gt;edge264_test&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Here are the &lt;code&gt;make&lt;/code&gt; options for tuning the compiled library file:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;CC&lt;/code&gt;- C compiler used to convert source files to object files (default&lt;code&gt;cc&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;CFLAGS&lt;/code&gt;- additional compilation flags passed to&lt;code&gt;CC&lt;/code&gt;and&lt;code&gt;TARGETCC&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;TARGETCC&lt;/code&gt;- C compiler used to link object files into library file (default&lt;code&gt;CC&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;LDFLAGS&lt;/code&gt;- additional compilation flags passed to&lt;code&gt;TARGETCC&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;TARGETOS&lt;/code&gt;- resulting file naming convention among&lt;code&gt;Windows&lt;/code&gt;|&lt;code&gt;Linux&lt;/code&gt;|&lt;code&gt;Darwin&lt;/code&gt;(default host)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;VARIANTS&lt;/code&gt;- comma-separated list of additional variants included in the library and selected at runtime (default&lt;code&gt;logs&lt;/code&gt;)&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;x86-64-v2&lt;/code&gt;- variant compiled for x86-64 microarchitecture level 2 (SSSE3, SSE4.1 and POPCOUNT)&lt;/item&gt;&lt;item&gt;&lt;code&gt;x86-64-v3&lt;/code&gt;- variant compiled for x86-64 microarchitecture level 3 (AVX2, BMI, LZCNT, MOVBE)&lt;/item&gt;&lt;item&gt;&lt;code&gt;logs&lt;/code&gt;- variant compiled with logging support in YAML format (headers and slices)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;BUILD_TEST&lt;/code&gt;- toggles compilation of&lt;code&gt;edge264_test&lt;/code&gt;(default&lt;code&gt;yes&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;FORCEINTRIN&lt;/code&gt;- enforce the use of intrinsics among&lt;code&gt;x86&lt;/code&gt;|&lt;code&gt;ARM64&lt;/code&gt;(for WebAssembly)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;$ make CFLAGS="-march=x86-64" VARIANTS=x86-64-v2,x86-64-v3 BUILD_TEST=no # example x86 build&lt;/code&gt;
    &lt;p&gt;The automated test program &lt;code&gt;edge264_test&lt;/code&gt; can browse files in a given directory, decoding each &lt;code&gt;&amp;lt;video&amp;gt;.264&lt;/code&gt; file and comparing its output with each sibling file &lt;code&gt;&amp;lt;video&amp;gt;.yuv&lt;/code&gt; if found. On the set of AVCv1, FRExt and MVC conformance bitstreams, 109/224 files are decoded without errors, the rest using yet unsupported features.&lt;/p&gt;
    &lt;code&gt;$ make
$ ./edge264_test --help # prints all options available
$ ffmpeg -i vid.mp4 -vcodec copy -bsf h264_mp4toannexb -an vid.264 # optional, converts from MP4 format
$ ./edge264_test -d vid.264 # replace -d with -b to benchmark instead of display&lt;/code&gt;
    &lt;p&gt;Here is a complete example that opens an input file in Annex B format from command line, and dumps its decoded frames in planar YUV order to standard output. See edge264_test.c for a more complete example which can also display frames.&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;fcntl.h&amp;gt;
#include &amp;lt;unistd.h&amp;gt;
#include &amp;lt;sys/mman.h&amp;gt;
#include &amp;lt;sys/stat.h&amp;gt;
#include &amp;lt;sys/types.h&amp;gt;

#include "edge264.h"

int main(int argc, char *argv[]) {
	int fd = open(argv[1], O_RDONLY);
	struct stat st;
	fstat(fd, &amp;amp;st);
	uint8_t *buf = mmap(NULL, st.st_size, PROT_READ, MAP_SHARED, fd, 0);
	const uint8_t *nal = buf + 3 + (buf[2] == 0); // skip the [0]001 delimiter
	const uint8_t *end = buf + st.st_size;
	// auto threads, no logs, auto allocs
	Edge264Decoder *dec = edge264_alloc(-1, NULL, NULL, 0, NULL, NULL, NULL);
	Edge264Frame frm;
	int res;
	do {
		res = edge264_decode_NAL(dec, nal, end, 0, NULL, NULL, &amp;amp;nal);
		while (!edge264_get_frame(dec, &amp;amp;frm, 0)) {
			for (int y = 0; y &amp;lt; frm.height_Y; y++)
				write(1, frm.samples[0] + y * frm.stride_Y, frm.width_Y);
			for (int y = 0; y &amp;lt; frm.height_C; y++)
				write(1, frm.samples[1] + y * frm.stride_C, frm.width_C);
			for (int y = 0; y &amp;lt; frm.height_C; y++)
				write(1, frm.samples[2] + y * frm.stride_C, frm.width_C);
		}
	} while (res == 0 || res == ENOBUFS);
	edge264_free(&amp;amp;dec);
	munmap(buf, st.st_size);
	close(fd);
	return 0;
}&lt;/code&gt;
    &lt;p&gt;
      &lt;code&gt;const uint8_t * edge264_find_start_code(buf, end, four_byte)&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;Return a pointer to the next three or four byte (0)001 start code prefix, or &lt;code&gt;end&lt;/code&gt; if not found.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;const uint8_t * buf&lt;/code&gt;- first byte of buffer to search into&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;const uint8_t * end&lt;/code&gt;- first invalid byte past the buffer that stops the search&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;int four_byte&lt;/code&gt;- if 0 seek a 001 prefix, otherwise seek a 0001&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;
      &lt;code&gt;Edge264Decoder * edge264_alloc(n_threads, log_cb, log_arg, log_mbs, alloc_cb, free_cb, alloc_arg)&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;Allocate and initialize a decoding context.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;int n_threads&lt;/code&gt;- number of background worker threads, with 0 to disable multithreading and -1 to detect the number of logical cores at runtime&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;void (* log_cb)(const char * str, void * log_arg)&lt;/code&gt;- if not NULL, a&lt;code&gt;fputs&lt;/code&gt;-compatible function pointer that&lt;code&gt;edge264_decode_NAL&lt;/code&gt;will call to log every header, SEI or macroblock (requires the&lt;code&gt;logs&lt;/code&gt;variant otherwise fails at runtime, called from the same thread except macroblocks in multithreaded decoding)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;void * log_arg&lt;/code&gt;- custom value passed to&lt;code&gt;log_cb&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;int log_mbs&lt;/code&gt;- set to 1 to enable logging of macroblocks&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;void (* alloc_cb)(void ** samples, unsigned samples_size, void ** mbs, unsigned mbs_size, int errno_on_fail, void * alloc_arg)&lt;/code&gt;- if not NULL, a function pointer that&lt;code&gt;edge264_decode_NAL&lt;/code&gt;will call (on the same thread) instead of malloc to request allocation of samples and macroblock buffers for a frame (&lt;code&gt;errno_on_fail&lt;/code&gt;is ENOMEM for mandatory allocations, or ENOBUFS for allocations that may be skipped to save memory but reduce playback smoothness)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;void (* free_cb)(void * samples, void * mbs, void * alloc_arg)&lt;/code&gt;- if not NULL, a function pointer that&lt;code&gt;edge264_decode_NAL&lt;/code&gt;and&lt;code&gt;edge264_free&lt;/code&gt;will call (on the same thread) to free buffers allocated through&lt;code&gt;alloc_cb&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;void * alloc_arg&lt;/code&gt;- custom value passed to&lt;code&gt;alloc_cb&lt;/code&gt;and&lt;code&gt;free_cb&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;
      &lt;code&gt;int edge264_decode_NAL(dec, buf, end, non_blocking, free_cb, free_arg, next_NAL)&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;Decode a single NAL unit containing any parameter set or slice.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;Edge264Decoder * dec&lt;/code&gt;- initialized decoding context&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;const uint8_t * buf&lt;/code&gt;- first byte of NAL unit (containing&lt;code&gt;nal_unit_type&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;const uint8_t * end&lt;/code&gt;- first byte past the buffer (max buffer size is 231-1 on 32-bit and 263-1 on 64-bit)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;int non_blocking&lt;/code&gt;- set to 1 if the current thread has other processing thus cannot block here&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;void (* free_cb)(void * free_arg, int ret)&lt;/code&gt;- callback that may be called from another thread when multithreaded, to signal the end of parsing and release the NAL buffer&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;void * free_arg&lt;/code&gt;- custom value that will be passed to&lt;code&gt;free_cb&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;const uint8_t ** next_NAL&lt;/code&gt;- if not NULL and the return code is&lt;code&gt;0&lt;/code&gt;|&lt;code&gt;ENOTSUP&lt;/code&gt;|&lt;code&gt;EBADMSG&lt;/code&gt;, will receive a pointer to the next NAL unit after the next start code in an Annex B stream&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Return codes are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;0&lt;/code&gt;on success&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;ENOTSUP&lt;/code&gt;on unsupported stream (decoding may proceed but could return zero frames)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;EBADMSG&lt;/code&gt;on invalid stream (decoding may proceed but could show visual artefacts, if you can check with another decoder that the stream is actually flawless, please consider filling a bug report 🙏)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;EINVAL&lt;/code&gt;if the function was called with&lt;code&gt;dec == NULL&lt;/code&gt;or&lt;code&gt;dec-&amp;gt;buf == NULL&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;ENODATA&lt;/code&gt;if the function was called while&lt;code&gt;dec-&amp;gt;buf &amp;gt;= dec-&amp;gt;end&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;ENOMEM&lt;/code&gt;if&lt;code&gt;malloc&lt;/code&gt;failed to allocate memory&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;ENOBUFS&lt;/code&gt;if more frames should be consumed with&lt;code&gt;edge264_get_frame&lt;/code&gt;to release a picture slot&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;EWOULDBLOCK&lt;/code&gt;if the non-blocking function would have to wait before a picture slot is available&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;
      &lt;code&gt;int edge264_get_frame(dec, out, borrow)&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;Fetch the next frame ready for output.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;Edge264Decoder * dec&lt;/code&gt;- initialized decoding context&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Edge264Frame *out&lt;/code&gt;- a structure that will be filled with data for the frame returned&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;int borrow&lt;/code&gt;- if 0 the frame may be accessed until the next call to&lt;code&gt;edge264_decode_NAL&lt;/code&gt;, otherwise the frame should be explicitly returned with&lt;code&gt;edge264_return_frame&lt;/code&gt;. Note that access is not exclusive, it may be used concurrently as reference for other frames.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Return codes are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;0&lt;/code&gt;on success (one frame is returned)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;EINVAL&lt;/code&gt;if the function was called with&lt;code&gt;dec == NULL&lt;/code&gt;or&lt;code&gt;out == NULL&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;ENOMSG&lt;/code&gt;if there is no frame to output at the moment&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;While reference frames may be decoded ahead of their actual display (ex. B-Pyramid technique), all frames are buffered for reordering before being released for display:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Decoding a non-reference frame releases it and all frames set to be displayed before it.&lt;/item&gt;
      &lt;item&gt;Decoding a key frame releases all stored frames (but not the key frame itself which might be reordered later).&lt;/item&gt;
      &lt;item&gt;Exceeding the maximum number of frames held for reordering releases the next frame in display order.&lt;/item&gt;
      &lt;item&gt;Lacking an available frame buffer releases the next non-reference frame in display order (to salvage its buffer) and all reference frames displayed before it.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;typedef struct Edge264Frame {
	const uint8_t *samples[3]; // Y/Cb/Cr planes
	const uint8_t *samples_mvc[3]; // second view
	const uint8_t *mb_errors; // probabilities (0..100) for each macroblock to be erroneous, NULL if there are no errors, values are spaced by stride_mb in memory
	int8_t pixel_depth_Y; // 0 for 8-bit, 1 for 16-bit
	int8_t pixel_depth_C;
	int16_t width_Y;
	int16_t width_C;
	int16_t height_Y;
	int16_t height_C;
	int16_t stride_Y;
	int16_t stride_C;
	int16_t stride_mb;
	uint32_t FrameId;
	uint32_t FrameId_mvc; // second view
	int16_t frame_crop_offsets[4]; // {top,right,bottom,left}, useful to derive the original frame with 16x16 macroblocks
	void *return_arg;
} Edge264Frame;&lt;/code&gt;
    &lt;p&gt;
      &lt;code&gt;void edge264_return_frame(dec, return_arg)&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;Give back ownership of the frame if it was borrowed from a previous call to &lt;code&gt;edge264_get_frame&lt;/code&gt;.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;Edge264Decoder * dec&lt;/code&gt;- initialized decoding context&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;void * return_arg&lt;/code&gt;- the value stored inside the frame to return&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;
      &lt;code&gt;void edge264_flush(dec)&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;For use when seeking, stop all background processing, flush all delayed frames while keeping them allocated, and clear the internal decoder state.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;Edge264Decoder * dec&lt;/code&gt;- initialized decoding context&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;
      &lt;code&gt;void edge264_free(pdec)&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;Deallocate the entire decoding context, and unset the pointer.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;Edge264Decoder ** pdec&lt;/code&gt;- pointer to a decoding context, initialized or not&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Stress testing (in progress)&lt;/item&gt;
      &lt;item&gt;Multithreading (in progress)&lt;/item&gt;
      &lt;item&gt;Error recovery (in progress)&lt;/item&gt;
      &lt;item&gt;Integration in VLC/ffmpeg/GStreamer&lt;/item&gt;
      &lt;item&gt;ARM32&lt;/item&gt;
      &lt;item&gt;PAFF and MBAFF&lt;/item&gt;
      &lt;item&gt;4:0:0, 4:2:2 and 4:4:4&lt;/item&gt;
      &lt;item&gt;9-14 bit depths with possibility of different luma/chroma depths&lt;/item&gt;
      &lt;item&gt;Transform-bypass for macroblocks with QP==0&lt;/item&gt;
      &lt;item&gt;SEI messages&lt;/item&gt;
      &lt;item&gt;AVX-2 optimizations&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I use edge264 to experiment on new programming techniques to improve performance and code size over existing decoders, and presented a few of these techniques at FOSDEM'24 and FOSDEM'25.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Single header file - It contains all struct definitions, common constants and enums, SIMD aliases, inline functions and macros, and exported functions for each source file. To understand the code base you should look at this file first.&lt;/item&gt;
      &lt;item&gt;Code blocks instead of functions - The main decoding loop is a forward pipeline designed as a DAG loosely resembling hardware decoders, with nodes being non-inlined functions and edges being tail calls. It helps mutualize code branches wherever possible, thus reduces code size to help fit in L1 cache.&lt;/item&gt;
      &lt;item&gt;Tree branching - Directional intra modes are implemented with a jump table to the leaves of a tree then unconditional jumps down to the trunk. It allows sharing the bottom code among directional modes, to reduce code size.&lt;/item&gt;
      &lt;item&gt;&lt;del rend="overstrike"&gt;Global context register - The pointer to the main structure holding context data is assigned to a register when supported by the compiler (GCC).&lt;/del&gt;This technique was dropped as Clang eventually reached on-par performance, so there is little incentive to maintain this hack.&lt;/item&gt;
      &lt;item&gt;Default neighboring values (search &lt;code&gt;unavail_mb&lt;/code&gt;) - Tests for availability of neighbors are replaced with fake neighboring macroblocks around each frame. It reduces the number of conditional tests inside the main decoding loop, thus reduces code size and branch predictor pressure.&lt;/item&gt;
      &lt;item&gt;Relative neighboring offsets (look for &lt;code&gt;A4x4_int8&lt;/code&gt;and related variables) - Access to left/top macroblock values is done with direct offsets in memory instead of copying their values to a buffer beforehand. It helps to reduce the reads and writes in the main decoding loop.&lt;/item&gt;
      &lt;item&gt;Parsing uneven block shapes (look at function &lt;code&gt;parse_P_sub_mb&lt;/code&gt;) - Each Inter macroblock paving specified with mb_type and sub_mb_type is first converted to a bitmask, then iterated on set bits to fetch the correct number of reference indices and motion vectors. This helps to reduce code size and number of conditional blocks.&lt;/item&gt;
      &lt;item&gt;Using vector extensions - GCC's vector extensions are used along vector intrinsics to write more compact code. All intrinsics from Intel are aliased with shorter names, which also provides an enumeration of all SIMD instructions used in the decoder.&lt;/item&gt;
      &lt;item&gt;Register-saturating SIMD - Some critical SIMD algorithms use more simultaneous vectors than available registers, effectively saturating the register bank and generating stack spills on purpose. In some cases this is more efficient than splitting the algorithm into smaller bits, and has the additional benefit of scaling well with later CPUs.&lt;/item&gt;
      &lt;item&gt;Piston cached bitstream reader - The bitstream bits are read in a size_t[2] intermediate cache with a trailing set bit to keep track of the number of cached bits, giving access to 32/64 bits per read from the cache, and allowing wide refills from memory.&lt;/item&gt;
      &lt;item&gt;On-the-fly SIMD unescaping - The input bitstream is unescaped on the fly using vector code, avoiding a full preprocessing pass to remove escape sequences, and thus reducing memory reads/writes.&lt;/item&gt;
      &lt;item&gt;Multiarch SIMD programming - Using vector extensions along with aliased intrinsics allows supporting both Intel SSE and ARM NEON with around 80% common code and few #if #else blocks, while keeping state-of-the-art performance for both architectures.&lt;/item&gt;
      &lt;item&gt;The Structure of Arrays pattern - The frame buffer is stored with arrays for each distinct field rather than an array of structures, to express operations on frames with bitwise and vector operators (see AoS and SoA). The task buffer for multithreading also relies on it partially.&lt;/item&gt;
      &lt;item&gt;Deferred error checking - Error detection is performed once in each type of NAL unit (search for &lt;code&gt;return&lt;/code&gt;statements), by clamping all input values to their expected ranges, then expecting&lt;code&gt;rbsp_trailing_bit&lt;/code&gt;afterwards (with very high probability of catching an error if the stream is corrupted). This design choice is discussed in A case about parsing errors.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Other yet-to-be-presented bits:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Minimalistic API with FFI-friendly design (7 functions and 1 structure).&lt;/item&gt;
      &lt;item&gt;The bitstream caches for CAVLC and CABAC (search for &lt;code&gt;rbsp_reg&lt;/code&gt;) are stored in two size_t variables each, which may be mapped to Global Register Variables in the future.&lt;/item&gt;
      &lt;item&gt;The decoding of input symbols is interspersed with their parsing (instead of parsing to a &lt;code&gt;struct&lt;/code&gt;then decoding the data). It deduplicates branches and loops that are present in both parsing and decoding, and even eliminates the need to store some symbols (e.g. mb_type, sub_mb_type, mb_qp_delta).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With the help of a custom bitstream writer using the same YAML format edge264 outputs, a set of extensive tests are being created in tools/raw_tests to stress the darkest corners of this decoder. The following table lists them all, along with the files implementing them.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;General tests&lt;/cell&gt;
        &lt;cell role="head"&gt;Expected&lt;/cell&gt;
        &lt;cell role="head"&gt;Test files&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;All supported types of NAL units&lt;/cell&gt;
        &lt;cell&gt;All OK&lt;/cell&gt;
        &lt;cell&gt;supp-nals&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;All unsupported types of NAL units&lt;/cell&gt;
        &lt;cell&gt;All unsupp&lt;/cell&gt;
        &lt;cell&gt;unsupp-nals&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Maximal header log-wise&lt;/cell&gt;
        &lt;cell&gt;All OK&lt;/cell&gt;
        &lt;cell&gt;max-logs&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;All conditions (incl. ignored) for detecting the start of a new frame&lt;/cell&gt;
        &lt;cell&gt;All OK&lt;/cell&gt;
        &lt;cell&gt;finish-frame&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;nal_ref_idc=0 on a IDR&lt;/cell&gt;
        &lt;cell&gt;All OK&lt;/cell&gt;
        &lt;cell&gt;non-ref-idr&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Missing rbsp_trailing_bit for all supported NAL types&lt;/cell&gt;
        &lt;cell&gt;All OK&lt;/cell&gt;
        &lt;cell&gt;no-trailing-bit&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;NAL of less than 11 bytes starting/ending at page boundary&lt;/cell&gt;
        &lt;cell&gt;All OK&lt;/cell&gt;
        &lt;cell&gt;tiny-nal&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;SEI/slice referencing an uninitialized SPS/PPS&lt;/cell&gt;
        &lt;cell&gt;1 OK, 4 errors&lt;/cell&gt;
        &lt;cell&gt;missing-ps&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Two non-ref frames with decreasing POC&lt;/cell&gt;
        &lt;cell&gt;All OK, any order&lt;/cell&gt;
        &lt;cell&gt;non-ref-dec-poc&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Horizontal/vertical cropping leaving zero space&lt;/cell&gt;
        &lt;cell&gt;All OK, 1x1 frames&lt;/cell&gt;
        &lt;cell&gt;zero-cropping&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;P/B slice with nal_unit_type=5 or max_num_ref_frames=0&lt;/cell&gt;
        &lt;cell&gt;4 OK, 2 errors&lt;/cell&gt;
        &lt;cell&gt;no-refs-P-B-slice&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;IDR slice with frame_num&amp;gt;0&lt;/cell&gt;
        &lt;cell&gt;All OK, clamped to 0&lt;/cell&gt;
        &lt;cell&gt;pos-frame-num-idr&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A ref that must bump out higher POCs to enter DPB (C.4.5.2)&lt;/cell&gt;
        &lt;cell&gt;All OK, check output order&lt;/cell&gt;
        &lt;cell&gt;poc-out-of-order&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Two ref frames with the same frame_num but differing POC, then a third frame referencing both&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Gap in frame_num while gaps_in_frame_num_value_allowed_flag=0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Stream starting with non-IDR I frame&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Stream starting with P/B frame&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Ref slice with delta_pic_order_cnt_bottom=-2**31, then a second frame referencing it&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Two frames A/B with intersecting top/bottom POC intervals in all possible intersections&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A 32-bit POC overflow between 2 frames&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A B-frame referencing frames with more than 2**16 POC diff&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;num_ref_idx_active&amp;gt;15 in SPS then no override in slice for L0 and L1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A slice with more ref_pic_list_modifications than num_ref_idx_active/16 for L0 and L1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A slice with ref_pic_list_modifications duplicating a ref then referencing the second one&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A slice with insufficient ref frames with and without override of num_ref_idx_active for L0 and L1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A modification of RefPicList[0/1] to a non-existing short/long term frame, then referencing it in mb&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;33 IDR with long_term_reference_flag=0/1 while max_num_ref_frames=0 (8.2.5.1)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A new reference while max_num_ref_frames are already all long-term&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;All combinations of mmco on all non-existing/short/long refs, with at least twice each mmco&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Two fields of the same frame being assigned different long-term frame indices then referenced&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;While all max_num_ref_frames are long-term, a ref_pic_list_modification that references all of them&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;An IDR picture with POC&amp;gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A picture with mmco=5 decoded after a picture with greater POC (8.2.1)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A P/B frame with zero references before or received with a gap in frame_num equal to max_ref_frames&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A P/B frame referencing a non-existing/erroneous ref&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A B frame with colPic set to a non-existing frame&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A current frame mmco'ed to long-term while all max_num_ref_frames are already long-term&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A mmco marking a non-existing picture to long-term&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;All combinations of IntraNxNPredMode with A/B/C/D unavailability with asserts for out-of-bounds reads&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A direct Inter reference from colPic that is not present in RefPicList0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A residual block with all coeffs at maximum 32-bit values&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Two slices of the same frame separated by a currPic reset (ex. AUD)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Two frames with the same POC yet differing TopFieldOrderCnt/BottomFieldOrderCnt&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Differing mmcos on two slices of the same frame&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Sending 2 IDR, then reaching the lowest possible POC, then getting all frames&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Two slices with mmco=5 yet frame_num&amp;gt;0 (to make it look like a new frame)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;POCs spaced by more than half max bits, such that relying on a stale prevPicOrderCnt yields wrong POC&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Filling the DPB with 16 refs then setting max_num_ref_frames=1 and adding a new ref frame&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Adding a frame cropping after decoding a frame&lt;/cell&gt;
        &lt;cell&gt;Crop should not apply retroactively&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Making a Direct ref_pic be used after it has been unreferenced&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;poc_type=2 and non-ref frame followed by non-ref pic, and the opposite (7.4.2.1.1)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;direct_8x8_inference_flag=1 with frame_mbs_only_flag=0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;checking that a gap in frame_num with poc_type==0 does not insert refs in B slices&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A SPS changing frame format while currPic&amp;gt;=0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;A frame allocator putting all allocs at start/end of a page boundary&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Parameter sets tests&lt;/cell&gt;
        &lt;cell role="head"&gt;Expected&lt;/cell&gt;
        &lt;cell role="head"&gt;Test files&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Invalid profile_idc=0/255&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Highest level_idc=255&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;All unsupported values of chroma_format_idc&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;All unsupported values of bit_depth_luma/chroma&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;qpprime_y_zero_transform_bypass_flag=1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;All scaling lists default/fallback rules and repeated values for all indices, with residual macroblock&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;log2_max_frame_num=4 and a frame referencing another with the same frame_num%4&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;CAVLC tests&lt;/cell&gt;
        &lt;cell role="head"&gt;Expected&lt;/cell&gt;
        &lt;cell role="head"&gt;Test files&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;All valid total_zeros=0-8-prefix+3-bit-suffix for TotalCoeffs in [0;15] for 4x4 and 2x2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Invalid total_zeros=31/63/127-prefix for TotalCoeffs in [0;15] for 4x4 and 2x2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;All valid coeff_token=0-14-prefix+4-bit-suffix for nC=0/2/4, and valid 6-bit-values for nC=8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Invalid coeff_token=31/63/127-prefix for nC=0/2/4, and invalid 6-bit-values for nC=8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;All valid levelCode=25-prefix+suffixLength-bit-suffix for all values of suffixLength&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;All valid run_before for all values of zerosLeft&amp;lt;=7&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Invalid run_before=31/63/127 for zerosLeft=7&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Macroblock of maximal size for all values of mb_type&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;mb_qp_delta=-26/25 that overflows on both sides&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;All valid inferences of nC for all values of nA/nB=unavail/other-slice/0-16&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;All coded_block_pattern=[0;47] for I and P/B slices&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;All combinations of intra_chroma_pred_mode and Intra4x4/8x8/16x16PredMode with A/B-unavailability&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;All values of mb_type+sub_mb_types for I/P/B with ref_idx/mvds different than values from B_Direct&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;mvd=[-32768/0/32767,-32768/0/32767] in a single 16x16 macroblock&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;TotalCoeff=16 for a Intra16x16 AC block&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;A residual block with run_length=14 making zerosLeft negative&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;CABAC tests&lt;/cell&gt;
        &lt;cell role="head"&gt;Expected&lt;/cell&gt;
        &lt;cell role="head"&gt;Test files&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Mixing CAVLC and CABAC in a same frame&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Single slice with at least 8 cabac_zero_word&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;MVC tests&lt;/cell&gt;
        &lt;cell role="head"&gt;Expected&lt;/cell&gt;
        &lt;cell role="head"&gt;Test files&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;All wrong combinations of non_idr_flag with nal_unit_type=1/5 and nal_ref_idc=0/1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;nal_unit_type=14 then filler unit then nal_unit_type=1/5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;An nal_unit_type=5 view paired with a non_idr_flag=0 P view, or a non_idr_flag=1 view&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Missing a base or non-base view&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Receiving a SSPS yet only base views then&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;16 ref base views while non base are non-refs&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A SSPS with different pic_width_in_mbs/pic_height_in_mbs/chroma_format_idc than its SPS&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A SSPS with num_views=1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A non-base view with weighted_bipred_idc=2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A non-base view with its base in RefPicList1[0] and direct_spatial_mv_pred_flag=0 (H.7.4.3)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A slice with num_ref_idx_l0_active&amp;gt;8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;svc_extension_flag=1 on a MVC stream&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;SSPS with additional_extension2_flag=1 and more trailing data&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Gap in frame_num of 16 frames on both views&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Specifying extra_frames=1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Receiving a non-base view before its base&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;A stream sending non-base views after a few frames have been output&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Error recovery tests&lt;/cell&gt;
        &lt;cell role="head"&gt;Expected&lt;/cell&gt;
        &lt;cell role="head"&gt;Test files&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Tests to implement&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A complete frame received twice&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A slice of a frame received twice&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Frame with correct and erroneous slice&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;All combinations erroneous/correct and all interval intersections on 2 slices&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;All failures of malloc&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;All (dis-)allowed bit positions at the end without rbsp_trailing_bit&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45443462</guid><pubDate>Wed, 01 Oct 2025 21:00:18 +0000</pubDate></item></channel></rss>