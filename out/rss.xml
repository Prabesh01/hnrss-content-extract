<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Sat, 10 Jan 2026 12:19:04 +0000</lastBuildDate><item><title>Show HN: Various shape regularization algorithms</title><link>https://github.com/nickponline/shreg</link><description>&lt;doc fingerprint="78fc5329f7d44de"&gt;
  &lt;main&gt;
    &lt;p&gt;A Python implementation of various shape regularization algorithms for regularizing line segments and closed contours.&lt;/p&gt;
    &lt;p&gt;Shape regularization is a technique used in computational geometry to clean up noisy or imprecise geometric data by aligning segments to common orientations and adjusting their positions to create cleaner, more regular shapes.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Segment Regularization: Align line segments to common angles and offsets using quadratic programming optimization&lt;/item&gt;
      &lt;item&gt;Snap Regularization: Connect nearby endpoints to create watertight polygons and meshes&lt;/item&gt;
      &lt;item&gt;Metric Regularization: Constrain segment dimensions - equal lengths, length quantization, and equal spacing&lt;/item&gt;
      &lt;item&gt;Contour Regularization: Simplify closed polygons by aligning edges to principal directions&lt;/item&gt;
      &lt;item&gt;T-Junction Detection: Snap endpoints onto segment interiors for proper connectivity&lt;/item&gt;
      &lt;item&gt;Flexible Configuration: Control maximum angle and offset tolerances&lt;/item&gt;
      &lt;item&gt;Visualization: Built-in plotting utilities for before/after comparisons&lt;/item&gt;
      &lt;item&gt;Pure Python: No dependencies required&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;pip install shreg&lt;/code&gt;
    &lt;code&gt;uv pip install shreg&lt;/code&gt;
    &lt;code&gt;git clone https://github.com/nickp/shreg.git
cd shreg
pip install -e .&lt;/code&gt;
    &lt;p&gt;Regularize a set of line segments by aligning their angles and offsets:&lt;/p&gt;
    &lt;code&gt;import numpy as np
from shreg import solve_line_segments, seg

# Create some segments (each segment is [x1, y1, x2, y2])
segments = [
    seg(0.0, 0.0, 1.0, 0.02),   # Nearly horizontal
    seg(0.0, 1.0, 1.0, 1.05),   # Nearly horizontal, slightly offset
    seg(1.0, 0.0, 1.02, 1.0),   # Nearly vertical
]

# Regularize: align angles within 25 degrees, offsets within 0.5 units
result = solve_line_segments(
    segments,
    angle=True,
    offset=True,
    maximum_angle=25,
    maximum_offset=0.5
)&lt;/code&gt;
    &lt;p&gt;Simplify a closed polygon by aligning edges to principal directions:&lt;/p&gt;
    &lt;code&gt;from shreg import regularize_contour

# Define a noisy polygon (list of [x, y] points)
points = [
    [45, 29], [65, 440], [44, 498], [446, 498], [429, 325],
    [499, 309], [448, 206], [479, 148], [479, 31], [247, 88],
]

# Regularize with axis alignment
result = regularize_contour(
    points,
    principle="axis",     # Align to horizontal/vertical
    max_offset=20,        # Maximum offset for merging
)

print(f"Simplified from {len(points)} to {len(result)} points")&lt;/code&gt;
    &lt;p&gt;Close gaps between nearby endpoints to create watertight polygons:&lt;/p&gt;
    &lt;code&gt;from shreg import snap_regularize_segments, seg

# Create segments with small gaps at corners
segments = [
    seg(0.0, 0.0, 1.0, 0.0),    # bottom edge
    seg(1.05, 0.02, 1.0, 1.0),  # right edge (gap at bottom-right)
    seg(1.0, 1.03, 0.0, 0.98),  # top edge (gap at corners)
    seg(-0.02, 1.0, 0.0, 0.0),  # left edge (gap at top-left)
]

# Snap endpoints within 0.1 units of each other
result = snap_regularize_segments(
    segments,
    epsilon=0.1,      # Distance threshold for snapping
    method="cluster"  # Fast centroid-based method
)
# Result: All corners are now perfectly connected&lt;/code&gt;
    &lt;p&gt;Constrain segment dimensions - force equal lengths, quantize to grid units, or equalize spacing:&lt;/p&gt;
    &lt;code&gt;from shreg import metric_regularize_segments, seg

# Segments with slightly different lengths and uneven spacing
segments = [
    seg(0.0, 0.0, 1.9, 0.0),   # length ~2
    seg(0.0, 0.9, 2.1, 0.9),   # length ~2, y=0.9 (should be 1.0)
    seg(0.0, 2.0, 1.95, 2.0),  # length ~2
]

# Regularize: equal lengths, snap to 1-unit grid, equalize spacing
result = metric_regularize_segments(
    segments,
    equal_length=True,         # Force similar lengths to be equal
    length_quantization=True,  # Snap lengths to multiples of base_unit
    equal_spacing=True,        # Equalize gaps between parallel lines
    base_unit=1.0,             # Grid unit for quantization
)
# Result: All segments have length 2.0 and are evenly spaced at y=0, 1, 2&lt;/code&gt;
    &lt;p&gt;The algorithm optimizes segment orientations and positions to create cleaner line arrangements:&lt;/p&gt;
    &lt;p&gt;Angle regularization aligns crossing lines to common orientations:&lt;/p&gt;
    &lt;p&gt;Combined angle and offset regularization on a hexagon:&lt;/p&gt;
    &lt;p&gt;This example from the CGAL documentation demonstrates sequential angle and offset regularization on 15 segments organized into three groups: outer boundary, top rhombus, and bottom rhombus.&lt;/p&gt;
    &lt;code&gt;from shreg import solve_line_segments, create_cgal_example

# Load the 15 segments from the CGAL example
segments, groups = create_cgal_example()

# Regularize with tight tolerances
result = solve_line_segments(
    segments,
    angle=True,
    offset=True,
    maximum_angle=10,    # 10 degrees max angle deviation
    maximum_offset=0.1   # 0.1 units max offset
)&lt;/code&gt;
    &lt;p&gt;Simplify complex polygons while preserving their essential shape:&lt;/p&gt;
    &lt;p&gt;Complex shapes are reduced to their essential vertices:&lt;/p&gt;
    &lt;p&gt;Snap regularization connects nearby endpoints to create watertight geometry. This is essential for creating closed polygons suitable for 3D extrusion, mesh generation, or CAD operations.&lt;/p&gt;
    &lt;p&gt;The cluster method groups nearby endpoints and moves them to their centroid. This is the fastest approach and guarantees watertight results:&lt;/p&gt;
    &lt;code&gt;from shreg import snap_regularize_segments, seg

segments = [
    seg(0.0, 0.0, 1.0, 0.05),
    seg(1.08, 0.0, 1.05, 1.0),
    seg(1.0, 1.08, 0.0, 0.95),
    seg(-0.05, 1.0, 0.0, 0.0),
]
result = snap_regularize_segments(segments, epsilon=0.15, method="cluster")&lt;/code&gt;
    &lt;p&gt;Hard constraints use quadratic programming to find the optimal positions that exactly satisfy all snap constraints while minimizing total endpoint movement:&lt;/p&gt;
    &lt;code&gt;result = snap_regularize_segments(segments, epsilon=0.15, method="hard")&lt;/code&gt;
    &lt;p&gt;Soft constraints add "spring" forces between endpoints that should connect. This is useful when data is noisy and you're not certain endpoints should be exactly coincident:&lt;/p&gt;
    &lt;code&gt;result = snap_regularize_segments(
    segments,
    epsilon=0.25,
    method="soft",
    soft_weight=50.0  # Higher = stiffer springs
)&lt;/code&gt;
    &lt;p&gt;T-junctions occur when an endpoint should snap onto another segment's interior (not its endpoints). Enable T-junction detection for proper connectivity:&lt;/p&gt;
    &lt;code&gt;segments = [
    seg(0.0, 0.0, 2.0, 0.0),      # horizontal line
    seg(0.0, 1.0, 2.0, 1.0),      # horizontal line
    seg(0.95, -0.08, 1.05, 1.1),  # vertical line (forms T-junctions)
]
result = snap_regularize_segments(
    segments, epsilon=0.15, method="cluster", t_junctions=True
)&lt;/code&gt;
    &lt;p&gt;Snap regularization works on polygons of any complexity:&lt;/p&gt;
    &lt;p&gt;Metric regularization constrains the relative measurements of segments. This is useful for architectural drawings, CAD cleanup, and any domain where dimensions should follow regular patterns.&lt;/p&gt;
    &lt;p&gt;Forces segments with similar lengths to be exactly equal. Useful when objects (like windows or columns) should have identical dimensions:&lt;/p&gt;
    &lt;code&gt;from shreg import metric_regularize_segments, seg

segments = [
    seg(0.0, 0.0, 2.0, 0.0),    # length 2.0
    seg(0.0, 1.0, 2.15, 1.0),   # length 2.15
    seg(0.0, 2.0, 1.9, 2.0),    # length 1.9
    seg(0.0, 3.0, 2.05, 3.0),   # length 2.05
]

result = metric_regularize_segments(
    segments,
    equal_length=True,
    length_tolerance=0.15,  # 15% relative tolerance
)
# Result: All segments now have equal length (~2.0)&lt;/code&gt;
    &lt;p&gt;Snaps segment lengths to integer multiples of a base unit. Essential for architectural plans where walls must be multiples of a grid unit:&lt;/p&gt;
    &lt;code&gt;segments = [
    seg(0.0, 0.0, 1.85, 0.0),   # length 1.85 -&amp;gt; 2.0
    seg(0.0, 1.0, 3.15, 1.0),   # length 3.15 -&amp;gt; 3.0
    seg(0.0, 2.0, 0.9, 2.0),    # length 0.9 -&amp;gt; 1.0
    seg(0.0, 3.0, 2.2, 3.0),    # length 2.2 -&amp;gt; 2.0
]

result = metric_regularize_segments(
    segments,
    length_quantization=True,
    base_unit=1.0,              # Snap to 1-meter multiples
    quantization_tolerance=0.3, # Within 30% of base unit
)&lt;/code&gt;
    &lt;p&gt;Forces equal gaps between parallel lines. Perfect for regularizing staircases, window arrays, or any repeated elements:&lt;/p&gt;
    &lt;code&gt;segments = [
    seg(0.0, 0.0, 3.0, 0.0),    # y=0.0
    seg(0.0, 0.9, 3.0, 0.9),    # y=0.9 (uneven)
    seg(0.0, 2.0, 3.0, 2.0),    # y=2.0
    seg(0.0, 3.1, 3.0, 3.1),    # y=3.1 (uneven)
    seg(0.0, 4.0, 3.0, 4.0),    # y=4.0
]

result = metric_regularize_segments(
    segments,
    equal_spacing=True,
    angle_tolerance=5.0,  # Lines within 5° are considered parallel
)
# Result: Lines are now evenly spaced at y=0, 1, 2, 3, 4&lt;/code&gt;
    &lt;p&gt;See API.md for the complete API documentation.&lt;/p&gt;
    &lt;p&gt;Run the demo examples:&lt;/p&gt;
    &lt;code&gt;# Run all examples with visualization
shreg

# Run without visualization (batch mode)
shreg --no-plot

# Run only segment examples
shreg --segments

# Run only contour examples
shreg --contours&lt;/code&gt;
    &lt;p&gt;Or using Python module syntax:&lt;/p&gt;
    &lt;code&gt;python -m shreg --help&lt;/code&gt;
    &lt;p&gt;The regularization problem is formulated as an energy minimization problem. Given a set of segments, we seek small adjustments (rotations and translations) that minimize an energy function while respecting constraints on maximum deviations.&lt;/p&gt;
    &lt;p&gt;The energy function balances two objectives:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Fidelity: Keep segments close to their original positions&lt;/item&gt;
      &lt;item&gt;Regularity: Encourage nearby segments to share common angles and offsets&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This leads to a quadratic program (QP) of the form:&lt;/p&gt;
    &lt;code&gt;minimize    (1/2) x'Px + q'x
subject to  l &amp;lt;= Ax &amp;lt;= u
&lt;/code&gt;
    &lt;p&gt;where &lt;code&gt;x&lt;/code&gt; contains the rotation and translation corrections for each segment, &lt;code&gt;P&lt;/code&gt; encodes the fidelity cost, and the constraints enforce that angle/offset differences between nearby segments are minimized.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Neighbor Detection: Use Delaunay triangulation on segment midpoints to identify nearby segment pairs efficiently&lt;/item&gt;
      &lt;item&gt;Constraint Graph: Build constraints for angle and offset differences between neighboring segments within tolerance bounds&lt;/item&gt;
      &lt;item&gt;QP Optimization: Solve the quadratic program using OSQP to find optimal corrections&lt;/item&gt;
      &lt;item&gt;Application: Apply computed rotations and translations to each segment&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Snap regularization is formulated as a Constrained Quadratic Programming problem that minimizes endpoint movement while enforcing connectivity constraints.&lt;/p&gt;
    &lt;p&gt;Variables: For N segments, the state vector contains all 4N endpoint coordinates:&lt;/p&gt;
    &lt;code&gt;x = [x₁₁, y₁₁, x₁₂, y₁₂, ..., xₙ₂, yₙ₂]ᵀ
&lt;/code&gt;
    &lt;p&gt;Objective (Fidelity): Minimize squared distance from original positions:&lt;/p&gt;
    &lt;code&gt;minimize (1/2) Σᵢ (||uᵢ - ûᵢ||² + ||vᵢ - v̂ᵢ||²)
&lt;/code&gt;
    &lt;p&gt;Methods:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Method&lt;/cell&gt;
        &lt;cell role="head"&gt;Formulation&lt;/cell&gt;
        &lt;cell role="head"&gt;Use Case&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;cluster&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Replace clustered endpoints with centroid&lt;/cell&gt;
        &lt;cell&gt;Fast, guaranteed watertight&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;hard&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Equality constraints: vᵢ - uⱼ = 0&lt;/cell&gt;
        &lt;cell&gt;Exact connections required&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;soft&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Penalty term: λ·Σ&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Pipeline:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Endpoint Detection: Build KD-Tree on all 2N endpoints&lt;/item&gt;
      &lt;item&gt;Clustering: Use Union-Find to group endpoints within ε distance&lt;/item&gt;
      &lt;item&gt;Variable Reduction (cluster): Replace clusters with single variables&lt;/item&gt;
      &lt;item&gt;QP Solve (hard/soft): Optimize using OSQP&lt;/item&gt;
      &lt;item&gt;T-Junction Handling: Project endpoints onto target segments&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Metric regularization constrains segment dimensions (length, distance). The challenge is that length calculation &lt;code&gt;√(Δx² + Δy²)&lt;/code&gt; is non-linear, but QP solvers require linear constraints.&lt;/p&gt;
    &lt;p&gt;Linearization: We approximate length using the segment's unit direction vector d = (dₓ, dᵧ):&lt;/p&gt;
    &lt;code&gt;L ≈ dₓ(xₑ - xₛ) + dᵧ(yₑ - yₛ)
&lt;/code&gt;
    &lt;p&gt;This is linear in the endpoint coordinates and can be directly inserted into the constraint matrix.&lt;/p&gt;
    &lt;p&gt;Constraint Formulations:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Constraint&lt;/cell&gt;
        &lt;cell role="head"&gt;Mathematical Form&lt;/cell&gt;
        &lt;cell role="head"&gt;Application&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Equal Length&lt;/cell&gt;
        &lt;cell&gt;L_A - L_B = 0&lt;/cell&gt;
        &lt;cell&gt;Windows, columns&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Quantization&lt;/cell&gt;
        &lt;cell&gt;L = K (target)&lt;/cell&gt;
        &lt;cell&gt;Grid snapping&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Equal Spacing&lt;/cell&gt;
        &lt;cell&gt;2·Pos(L₂) - Pos(L₁) - Pos(L₃) = 0&lt;/cell&gt;
        &lt;cell&gt;Stairs, arrays&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Iterative Refinement (SQP): Because the unit vectors are computed from the current geometry, results are approximate if segments rotate significantly. The algorithm uses Sequential Quadratic Programming:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Compute unit vectors from current segment orientations&lt;/item&gt;
      &lt;item&gt;Build and solve the QP&lt;/item&gt;
      &lt;item&gt;Update segment coordinates&lt;/item&gt;
      &lt;item&gt;Repeat until convergence (or max iterations)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Pipeline:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Detection: Find candidate pairs/groups (similar lengths, parallel lines)&lt;/item&gt;
      &lt;item&gt;Linearization: Compute unit direction vectors for length approximation&lt;/item&gt;
      &lt;item&gt;Constraint Building: Build sparse constraint matrix A for detected patterns&lt;/item&gt;
      &lt;item&gt;QP Solve: Minimize ||x - x̂||² subject to Ax = b using OSQP&lt;/item&gt;
      &lt;item&gt;Iteration: Refine unit vectors and re-solve if needed&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The contour regularization algorithm follows CGAL's approach for closed polygons:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Angle Alignment: Rotate each edge to align with principal directions (modulo 90 degrees)&lt;/item&gt;
      &lt;item&gt;Parallel Merging: Merge consecutive parallel edges that are close together&lt;/item&gt;
      &lt;item&gt;Link Insertion: Insert connecting segments between remaining parallel edges&lt;/item&gt;
      &lt;item&gt;Intersection: Compute intersection points to form the final regularized polygon&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;numpy &amp;gt;= 1.20.0&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;scipy &amp;gt;= 1.7.0&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;osqp &amp;gt;= 0.6.0&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;matplotlib &amp;gt;= 3.5.0&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Install development dependencies:&lt;/p&gt;
    &lt;code&gt;pip install -e ".[dev]"&lt;/code&gt;
    &lt;p&gt;Run tests:&lt;/p&gt;
    &lt;code&gt;pytest tests/ -v&lt;/code&gt;
    &lt;p&gt;Run tests with coverage:&lt;/p&gt;
    &lt;code&gt;pytest tests/ -v --cov=shreg --cov-report=term-missing&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Jean-Philippe Bauchet and Florent Lafarge. KIPPI: KInetic Polygonal Partitioning of Images. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 3146–3154, Salt Lake City, United States, June 2018. [PDF]&lt;/item&gt;
      &lt;item&gt;CGAL Shape Regularization Documentation&lt;/item&gt;
      &lt;item&gt;OSQP: Operator Splitting Quadratic Program Solver&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46549333</guid><pubDate>Fri, 09 Jan 2026 02:13:01 +0000</pubDate></item><item><title>Kagi releases alpha version of Orion for Linux</title><link>https://help.kagi.com/orion/misc/linux-status.html</link><description>&lt;doc fingerprint="8923d81071c60f5d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Orion for Linux Status â&lt;/head&gt;
    &lt;p&gt;The alpha stage is an early, unstable version meant primarily for testing.&lt;/p&gt;
    &lt;head rend="h2"&gt;What is ready to test â&lt;/head&gt;
    &lt;p&gt;All visual components, including:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Main menus, submenus, dialogs, buttons, and toolbars.&lt;/item&gt;
      &lt;item&gt;Right-click menus and other visual controls.&lt;/item&gt;
      &lt;item&gt;Window layouts and basic controls.&lt;/item&gt;
      &lt;item&gt;Demonstrated basic website navigation functionality, supporting essentials like the homepage, tabs, and simple searches&lt;/item&gt;
      &lt;item&gt;Advanced tab management is now complete, with the exception of the Tab Switcher UI, which is not supported yet.&lt;/item&gt;
      &lt;item&gt;Tabs now function independently and can be opened in parallel&lt;/item&gt;
      &lt;item&gt;Session persistence is implemented: previously opened tabs, along with their history, will reopen when the application is launched again.&lt;/item&gt;
      &lt;item&gt;Tabs currently appear in the main window and are supported in the left sidebar as well.&lt;/item&gt;
      &lt;item&gt;Bookmarks system a simple bookmark feature is now available.&lt;/item&gt;
      &lt;item&gt;Users can save pages, organize them into folders&lt;/item&gt;
      &lt;item&gt;Users can view them in the bookmarks dialog, sidebar, and bookmarks bar.&lt;/item&gt;
      &lt;item&gt;Bookmarking via the â´ï¸ icon.&lt;/item&gt;
      &lt;item&gt;Intuitive folder assignment when saving a new bookmark.&lt;/item&gt;
      &lt;item&gt;Advanced history management provides handling of browsing history&lt;/item&gt;
      &lt;item&gt;Password management framework establishes the core infrastructure needed for secure password handling and future improvements in this area.&lt;/item&gt;
      &lt;item&gt;Local export/import (via file)&lt;/item&gt;
      &lt;item&gt;Managing passwords&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Future improvements (not implemented in Alpha): â&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;WebKit Extension support&lt;/item&gt;
      &lt;item&gt;Sync infrastructure&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46553343</guid><pubDate>Fri, 09 Jan 2026 12:54:48 +0000</pubDate></item><item><title>How to store a chess position in 26 bytes (2022)</title><link>https://ezzeriesa.notion.site/How-to-store-a-chess-position-in-26-bytes-using-bit-level-magic-df1fdb5364eb42fdac11eb23b25e9605</link><description>&lt;doc fingerprint="10f452a104a33a8"&gt;
  &lt;main&gt;
    &lt;p&gt;JavaScript must be enabled in order to use Notion. Please enable JavaScript to continue.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46554652</guid><pubDate>Fri, 09 Jan 2026 15:07:17 +0000</pubDate></item><item><title>Cloudflare CEO on the Italy fines</title><link>https://twitter.com/eastdakota/status/2009654937303896492</link><description>&lt;doc fingerprint="d635e49f34142863"&gt;
  &lt;main&gt;
    &lt;p&gt;We’ve detected that JavaScript is disabled in this browser. Please enable JavaScript or switch to a supported browser to continue using x.com. You can see a list of supported browsers in our Help Center.&lt;/p&gt;
    &lt;p&gt;Help Center&lt;/p&gt;
    &lt;p&gt;Terms of Service Privacy Policy Cookie Policy Imprint Ads info © 2026 X Corp.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46555760</guid><pubDate>Fri, 09 Jan 2026 16:46:48 +0000</pubDate></item><item><title>Flock Hardcoded the Password for America's Surveillance Infrastructure 53 Times</title><link>https://nexanet.ai/blog/53-times-flocksafety-hardcoded-the-password-for-americas-surveillance-infrastructure</link><description>&lt;doc fingerprint="dd8d45a2a1773e1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;53 Times Flock Safety Hardcoded the Password for America's Surveillance Infrastructure&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Vendor&lt;/cell&gt;
        &lt;cell&gt;Flock Safety&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Affected Products&lt;/cell&gt;
        &lt;cell&gt;Flock Safety's ArcGIS, FlockOS, Aerodome, Flock911&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Vulnerability Type&lt;/cell&gt;
        &lt;cell&gt;Hardcoded API Key Exposure (CWE-798)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Exposure Count&lt;/cell&gt;
        &lt;cell&gt;53 separate instances across public-facing assets compromising 50 data layers&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Data at Risk&lt;/cell&gt;
        &lt;cell&gt;~5,000 police departments, ~6,000 community deployments, and ~1,000 private businesses&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Status&lt;/cell&gt;
        &lt;cell&gt;Remediated following responsible disclosure&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;Executive Summary&lt;/head&gt;
    &lt;p&gt;I discovered a Default ArcGIS API key embedded in Flock Safety's public-facing JavaScript bundles. This single credential granted access to the company's ArcGIS mapping environment, and 50 private layers, the same infrastructure that consolidates license plate detections, patrol car locations, drone telemetry, body camera locations, 911 call data, and surveillance camera locations from approximately 12,000 law enforcement, community, and private sector deployments nationwide.&lt;/p&gt;
    &lt;p&gt;The key was not restricted by referrer, IP, or origin allowing it to be used by anyone, anywhere. It was exposed publicly across 53 separate Flock Safety front-end bundles and environments, each instance independently granting access to their ArcGIS mapping platform.&lt;/p&gt;
    &lt;head rend="h2"&gt;Background: What is Flock Safety?&lt;/head&gt;
    &lt;p&gt;Across the United States, license plate readers, drones, and audio sensors quietly record the movements of millions of people every day. Flock Safety operates one of the largest and most rapidly expanding of these networks, with hundreds of thousands of cameras generating over 30 billion vehicle detections each month, and an undisclosed amount of people detections.&lt;/p&gt;
    &lt;p&gt;At the center of this infrastructure is FlockOS, which Flock markets under the headline "One map. Smarter Response." According to their own documentation, the ArcGIS-powered interface "consolidates all data streams and the locations of each connected asset, enabling greater situational awareness and a common operating procedure." (Source: ClearGov Resource Document)&lt;/p&gt;
    &lt;p&gt;That "one map" is not a metaphor. It is the ArcGIS stack itself and the exposed API key unlocked it.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Vulnerability&lt;/head&gt;
    &lt;p&gt;The exposed credential was an organization-wide ArcGIS API key tied directly to Flock Safety's ArcGIS mapping environment. It appeared in client-side JavaScript bundles served from development subdomains that were publicly accessible.&lt;/p&gt;
    &lt;p&gt;Querying the ArcGIS API with this key returned metadata confirming its scope and the extent of Flock's misconfiguration:&lt;/p&gt;
    &lt;p&gt;The credential was tagged appTitle: "Default API Key", the auto-generated key Esri creates at account signup. According to Esri's ArcGIS documentation:&lt;/p&gt;
    &lt;quote&gt;"An API key is a permanent access token that defines the scope and permission for granting your public-facing application access to specific, ready-to-use services and private content... An API key is created for you when you sign up for an ArcGIS Developer account."&lt;/quote&gt;
    &lt;p&gt;The key's metadata listed 50 "portal:app:access:item: privileges each granting access to a private ArcGIS item.&lt;/p&gt;
    &lt;p&gt;Given Flock's centralized "one map" architecture where participating agencies contribute data to shared, Flock-owned layers rather than maintaining separate instances each of those 50 private items likely aggregates data from hundreds or thousands of agencies. A single Detections layer would contain hotlist hits from all ~5,000 participating police departments. A single Mobile Units layer would show patrol car positions across every integrated agency.&lt;/p&gt;
    &lt;p&gt;Esri's documentation warns:&lt;/p&gt;
    &lt;quote&gt;"For the highest level of security, always set the API key scopes and referrers before deploying an application."&lt;/quote&gt;
    &lt;p&gt;Flock applied no referrer restrictions, no IP allowlist, and no scope limitations. They took the default key, granted it access to 50 private items, and embedded it in client-side JavaScript bundles across 53 publicly accessible endpoints:&lt;/p&gt;
    &lt;p&gt;53 Exposed Endpoints (hostnames redacted):&lt;/p&gt;
    &lt;code&gt;[redacted].flocksafety.com &lt;/code&gt;
    &lt;code&gt;[redacted].flocksafety.com/.../flock-DzA9VKXM.js&lt;/code&gt;
    &lt;code&gt;[redacted].flocksafety.com/.../index-slKO6jum.js&lt;/code&gt;
    &lt;code&gt;[redacted].flocksafety.com/.../index-BBRurSLX.js&lt;/code&gt;
    &lt;head class="cursor-pointer text-purple-400 hover:text-purple-300 text-sm font-medium"&gt;Show all 53 endpoints&lt;/head&gt;
    &lt;code&gt;[redacted].flocksafety.com/.../index-BPxp6hzB.js&lt;/code&gt;
    &lt;code&gt;[redacted].flocksafety.com/.../subjectMessageExpireWorker-BoZI8MYY.js&lt;/code&gt;
    &lt;code&gt;[redacted].flocksafety.com/.../notificationExpireWorker-_11bsRZ0.js&lt;/code&gt;
    &lt;code&gt;[redacted].flocksafety.com/.../notificationAnalyticsTracker-NGZrpZsD.js&lt;/code&gt;
    &lt;code&gt;[redacted].flocksafety.com/.../visualSearchExpiringWorker-na0tVtmO.js&lt;/code&gt;
    &lt;code&gt;[redacted].flocksafety.com/.../prepared911ExpireWorker-BpcA2uVs.js&lt;/code&gt;
    &lt;code&gt;[redacted].flocksafety.com/.../visualSearchExpireWorker-DLZtzWUN.js&lt;/code&gt;
    &lt;code&gt;[redacted].flocksafety.com/.../index-xrDxc-Lv.js&lt;/code&gt;
    &lt;code&gt;[redacted].flocksafety.com/.../subjectMessageExpireWorker-J-ZVyGSG.js&lt;/code&gt;
    &lt;code&gt;[redacted].flocksafety.com/.../notificationExpireWorker-4m9KycyN.js&lt;/code&gt;
    &lt;code&gt;[redacted].flocksafety.com/.../notificationAnalyticsTracker-by_qQfZs.js&lt;/code&gt;
    &lt;code&gt;[redacted].flocksafety.com/.../visualSearchExpireWorker-reQjrWVk.js&lt;/code&gt;
    &lt;code&gt;[redacted].flocksafety.com/.../prepared911ExpireWorker-XS3Uur-m.js&lt;/code&gt;
    &lt;code&gt;[redacted].flocksafety.com/.../index-ztnhrNoG.js&lt;/code&gt;
    &lt;code&gt;[redacted].flocksafety.com/.../notificationAnalyticsTracker-HA5zO2gU.js&lt;/code&gt;
    &lt;code&gt;[redacted].flocksafety.com/.../notificationExpireWorker-8Pe_fNZM.js&lt;/code&gt;
    &lt;code&gt;[redacted].flocksafety.com/.../visualSearchExpiringWorker-Dro5OKrq.js&lt;/code&gt;
    &lt;code&gt;[redacted].flocksafety.com/.../visualSearchExpireWorker-wJjUIzMY.js&lt;/code&gt;
    &lt;code&gt;[redacted].flocksafety.com/.../prepared911ExpireWorker-9gsy_LJ6.js&lt;/code&gt;
    &lt;code&gt;[redacted].flocksafety.com/.../subjectMessageExpireWorker--KqStPhO.js&lt;/code&gt;
    &lt;code&gt;[redacted].flocksafety.com/.../visualSearchExpiringWorker-3cNFqmkh.js&lt;/code&gt;
    &lt;code&gt;[redacted].flocksafety.com/.../index-Bas-zpR9.js&lt;/code&gt;
    &lt;code&gt;[redacted].flocksafety.com/.../subjectMessageExpireWorker-B1jQx_EB.js&lt;/code&gt;
    &lt;code&gt;[redacted].flocksafety.com/.../notificationAnalyticsTracker-DYLz-f8O.js&lt;/code&gt;
    &lt;code&gt;[redacted].flocksafety.com/.../notificationExpireWorker-Bix-gxWk.js&lt;/code&gt;
    &lt;code&gt;[redacted].flocksafety.com/.../visualSearchExpiringWorker-5vLUhmkY.js&lt;/code&gt;
    &lt;code&gt;[redacted].flocksafety.com/.../prepared911ExpireWorker-DA2TrQ_9.js&lt;/code&gt;
    &lt;code&gt;[redacted].flocksafety.com/.../visualSearchExpireWorker-D6gTd4TT.js&lt;/code&gt;
    &lt;code&gt;[redacted].flocksafety.com/.../index-DGGtSpOn.js&lt;/code&gt;
    &lt;code&gt;[redacted].flocksafety.com/.../subjectMessageExpireWorker-D1Fw4UF9.js&lt;/code&gt;
    &lt;code&gt;[redacted].flocksafety.com/.../notificationExpireWorker-wjAN01b6.js&lt;/code&gt;
    &lt;code&gt;[redacted].flocksafety.com/.../notificationAnalyticsTracker-BoXGrJ_I.js&lt;/code&gt;
    &lt;code&gt;[redacted].flocksafety.com/.../prepared911ExpireWorker-CYEuywlF.js&lt;/code&gt;
    &lt;code&gt;[redacted].flocksafety.com/.../visualSearchExpireWorker-BM2HGkks.js&lt;/code&gt;
    &lt;code&gt;[redacted].flocksafety.com/.../visualSearchExpiringWorker-DW-LQUZh.js&lt;/code&gt;
    &lt;code&gt;[redacted].flocksafety.com/.../index-BtFbKoWO.js&lt;/code&gt;
    &lt;code&gt;[redacted].flocksafety.com/.../notificationExpireWorker-Bs_qqZKK.js&lt;/code&gt;
    &lt;code&gt;[redacted].flocksafety.com/.../notificationAnalyticsTracker-W6tR54zI.js&lt;/code&gt;
    &lt;code&gt;[redacted].flocksafety.com/.../subjectMessageExpireWorker-D3Gi0z83.js&lt;/code&gt;
    &lt;code&gt;[redacted].flocksafety.com/.../visualSearchExpireWorker-gofSdgry.js&lt;/code&gt;
    &lt;code&gt;[redacted].flocksafety.com/.../visualSearchExpiringWorker-HayKEVyk.js&lt;/code&gt;
    &lt;code&gt;[redacted].flocksafety.com/.../prepared911ExpireWorker-BzI_7Vif.js&lt;/code&gt;
    &lt;code&gt;[redacted].flocksafety.com/.../index-DrW4QnA-.js&lt;/code&gt;
    &lt;code&gt;[redacted].flocksafety.com/.../subjectMessageExpireWorker-D3Gi0z83.js&lt;/code&gt;
    &lt;code&gt;[redacted].flocksafety.com/.../notificationExpireWorker-Bs_qqZKK.js&lt;/code&gt;
    &lt;code&gt;[redacted].flocksafety.com/.../notificationAnalyticsTracker-W6tR54zI.js&lt;/code&gt;
    &lt;code&gt;[redacted].flocksafety.com/.../visualSearchExpireWorker-gofSdgry.js&lt;/code&gt;
    &lt;code&gt;[redacted].flocksafety.com/.../visualSearchExpiringWorker-HayKEVyk.js&lt;/code&gt;
    &lt;code&gt;[redacted].flocksafety.com/.../prepared911ExpireWorker-BzI_7Vif.js&lt;/code&gt;
    &lt;p&gt;Each endpoint independently served the same unrestricted credential 53 times, and any one of them could have been used to access Flock's ArcGIS environment.&lt;/p&gt;
    &lt;head rend="h2"&gt;FlockOS: The Unified Attack Surface&lt;/head&gt;
    &lt;p&gt;The FlockOS map component signature reveals the unified layer architecture:&lt;/p&gt;
    &lt;code&gt;ru=({
  esriMapsApiKey: t,
  baseLayers: n,
  dynamicLayers: i,
  featureLayers: o,
  markerLayers: a,
  nonClusteredMarkerLayers: s,
  clusteredMarkerLayers: l,
  heatmapLayers: h,
  focusedMarkers: p,
  selectedLayers: g,
  setSelectedLayers: A,
  onBaseLayerChange: y,
  onCustomMapLayerSelectionChange: b
})&lt;/code&gt;
    &lt;p&gt;A single component consumes the Esri API key alongside every layer type: base maps, dynamic overlays, feature layers, clustered and non-clustered markers, and heatmaps. Layer selection state is managed uniformly across all data sources.&lt;/p&gt;
    &lt;p&gt;Internal permission flags from JavaScript bundles confirm FlockOS's role as the unified interface:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;canUseFlockOS911&lt;/item&gt;
      &lt;item&gt;canUseCAD&lt;/item&gt;
      &lt;item&gt;canDispatchDrone&lt;/item&gt;
      &lt;item&gt;canManageIntegrations&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;FlockOS is the interface; ArcGIS is the substrate. The exposed API key granted access to the common mapping layer where all Flock Safety applications converge: camera inventories, mobile units, detection outputs, hotlists, search geometries, drone telemetry, Raven analytics, officer-accessible views, and Flock911 incidents.&lt;/p&gt;
    &lt;head rend="h2"&gt;Exposed Data Categories&lt;/head&gt;
    &lt;head rend="h3"&gt;Surveillance Infrastructure&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Camera deployments operated by police departments, communities, and private businesses&lt;/item&gt;
      &lt;item&gt;Third-party devices connected via Wing Gateways&lt;/item&gt;
      &lt;item&gt;Raven audio gunshot detection sensors&lt;/item&gt;
      &lt;item&gt;Drone assets with live status&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Law Enforcement Location Data&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Live and historical patrol car GPS positions&lt;/item&gt;
      &lt;item&gt;Axon body-worn camera locations&lt;/item&gt;
      &lt;item&gt;Officer mobile app location data (phone, smartwatch)&lt;/item&gt;
      &lt;item&gt;Trailers and auxiliary GPS trackers&lt;/item&gt;
      &lt;item&gt;CAD (Computer-Aided Dispatch) event layers and patrol history&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;People and Vehicle Intelligence&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;People detection alerts with camera IDs, time windows, confidence thresholds, and expiration timestamps&lt;/item&gt;
      &lt;item&gt;People searches rendered as tracked objects on the map&lt;/item&gt;
      &lt;item&gt;Vehicle alerts and vehicle description alerts&lt;/item&gt;
      &lt;item&gt;Vehicle searches persisted alongside detections&lt;/item&gt;
      &lt;item&gt;Audio alerts including gunshot detection popups with classification (single shot vs. multiple shots)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Hotlists and Investigative Data&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Hotlist detections with topic names, alert reasons, filter IDs, deduplicated detection IDs, license plate numbers, and time/location data&lt;/item&gt;
      &lt;item&gt;Saved search filters—analyst searches persisted as spatial objects&lt;/item&gt;
      &lt;item&gt;Search footprints—the actual polygons and radii investigators draw when selecting geographic areas of interest&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Personally Identifiable Information&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Camera registrant names&lt;/item&gt;
      &lt;item&gt;Email addresses&lt;/item&gt;
      &lt;item&gt;Phone numbers&lt;/item&gt;
      &lt;item&gt;Location types (law enforcement, non-law enforcement, private)&lt;/item&gt;
      &lt;item&gt;Postal addresses&lt;/item&gt;
      &lt;item&gt;Counts of interior and exterior cameras per location&lt;/item&gt;
      &lt;item&gt;Arrays of associated camera locations&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Camera Registry and Asset Data&lt;/head&gt;
    &lt;p&gt;Each camera record exposed:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Physical deployment location&lt;/item&gt;
      &lt;item&gt;Device serial numbers&lt;/item&gt;
      &lt;item&gt;Device uptime percentages&lt;/item&gt;
      &lt;item&gt;Operational status&lt;/item&gt;
      &lt;item&gt;Flock support and service metadata&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Flock911 Emergency Data&lt;/head&gt;
    &lt;p&gt;A dedicated ArcGIS FeatureServer layer for Flock911 incidents exposed:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Live incident locations&lt;/item&gt;
      &lt;item&gt;Call IDs&lt;/item&gt;
      &lt;item&gt;Transcript access tokens&lt;/item&gt;
      &lt;item&gt;Per-word transcript timing data&lt;/item&gt;
      &lt;item&gt;Audio scrub positions and playback state&lt;/item&gt;
      &lt;item&gt;Incident classification&lt;/item&gt;
      &lt;item&gt;Active and selected incident identifiers&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Audio and transcript data flows through the same map context as cameras, patrol units, and alerts. No separate security boundary exists at the mapping layer.&lt;/p&gt;
    &lt;head rend="h3"&gt;Drone Telemetry (Aerodome Integration)&lt;/head&gt;
    &lt;p&gt;Every status chip rendered on the patrol/device tray (via hQ) only recognizes the values: &lt;code&gt;Docked&lt;/code&gt;, &lt;code&gt;Buffering&lt;/code&gt;, &lt;code&gt;Recording&lt;/code&gt;, &lt;code&gt;Inactive&lt;/code&gt;, &lt;code&gt;Offline&lt;/code&gt;, &lt;code&gt;Off&lt;/code&gt;, &lt;code&gt;ON&lt;/code&gt;, &lt;code&gt;ONLINE&lt;/code&gt;, &lt;code&gt;ACTIVE&lt;/code&gt;, &lt;code&gt;Charging&lt;/code&gt;, and &lt;code&gt;Uploading&lt;/code&gt;. Statuses in the "online/charging/buffering" group render as green; "inactive/offline/off" renders gray; and "recording" renders red. This confirms the complete set of device states actively rendered on the shared map UI.&lt;/p&gt;
    &lt;head rend="h2"&gt;Pattern of Credential Exposure&lt;/head&gt;
    &lt;p&gt;The exposed Default API Key was not an isolated incident.&lt;/p&gt;
    &lt;p&gt;I separately disclosed an additional critical vulnerability involving unauthenticated ArcGIS token minting. This vulnerability allows unauthenticated users to obtain valid ArcGIS tokens scoped to Flock Safety's production environment from their development environment, tokens titled "Flock Safety Prod" that grant access to the geographic mapping of Flock's camera network locations.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;November 13, 2025 — Initial disclosure sent to Flock Safety security team&lt;/item&gt;
      &lt;item&gt;November 14, 2025 — First follow-up requesting confirmation of receipt&lt;/item&gt;
      &lt;item&gt;November 19, 2025 — Second follow-up; Flock Safety finally acknowledges receipt&lt;/item&gt;
      &lt;item&gt;January 7, 2026 — Vulnerability remains unpatched (55+ days)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I am withholding specific technical details to prevent exploitation while the vulnerability remains unpatched. However, its existence more than 55 days after responsible disclosure with no remediation, demonstrates a systemic pattern of credential mismanagement.&lt;/p&gt;
    &lt;head rend="h3"&gt;Comparing the Exposed Credentials&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Property&lt;/cell&gt;
        &lt;cell role="head"&gt;Default API Key&lt;/cell&gt;
        &lt;cell role="head"&gt;Flock Safety Prod&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Portal Item Access&lt;/cell&gt;
        &lt;cell&gt;50 private items&lt;/cell&gt;
        &lt;cell&gt;None&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Camera Network Access&lt;/cell&gt;
        &lt;cell&gt;Yes (via item access)&lt;/cell&gt;
        &lt;cell&gt;Yes (direct FeatureLayer)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
        &lt;cell&gt;Dev site JS bundles&lt;/cell&gt;
        &lt;cell&gt;Unauthenticated token minting&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Status&lt;/cell&gt;
        &lt;cell&gt;Fixed (June 2025)&lt;/cell&gt;
        &lt;cell&gt;Unpatched (55+ days)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Both keys operated under the same active subscription with nearly one million available credits. Critically, development environments were configured with broader access than production, and those development sites were publicly accessible.&lt;/p&gt;
    &lt;head rend="h2"&gt;Scope Limitations and Evidentiary Standard&lt;/head&gt;
    &lt;p&gt;The 50 "portal:app:access:item" privileges reference private item IDs that cannot be inventoried without actively querying each one which I did not do. However, ArcGIS collaboration features allow partner organizations to share layers into another organization's portal, and evidence suggests this capability was actively used.&lt;/p&gt;
    &lt;p&gt;An individual at a sheriff's office with an active Flock deployment confirmed during the course of this research that their agency shares ArcGIS layers directly with Flock Safety's organization. This corroborates the technical architecture documented in Esri's collaboration documentation and aligns with the privilege structure observed in the exposed credential metadata.&lt;/p&gt;
    &lt;p&gt;What I can state with certainty:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;The API key's metadata explicitly listed 50 &lt;code&gt;portal:app:access:item:&lt;/code&gt;privileges granting access to private ArcGIS items&lt;/item&gt;
      &lt;item&gt;Esri's own documentation confirms that such privileges grant access to "hosted feature services, web maps, web scenes, tile layers" and other private portal content&lt;/item&gt;
      &lt;item&gt;A law enforcement source with direct knowledge of their agency's Flock integration confirmed that layer sharing with Flock Safety's ArcGIS organization occurs in practice&lt;/item&gt;
      &lt;item&gt;The key appeared across 53 publicly accessible endpoints with no referrer restrictions, IP limitations, or access controls&lt;/item&gt;
      &lt;item&gt;Many of the photo's I've used as examples are from publicly exposed ArcGIS datasets owned by Police Departments, that have relevant Flock Safety data in them.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Taken together, these findings establish that the exposed credential provided a viable technical pathway to access shared law enforcement data. The precise contents of each private layer remain unverified, yet the circumstantial evidence is substantial.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why This Matters: National Security Implications&lt;/head&gt;
    &lt;head rend="h3"&gt;Intelligence Value of Movement Data&lt;/head&gt;
    &lt;p&gt;Foreign intelligence services would not need access to communications content if they could reliably observe movement at this scale. Historical location data revealing the presence, routines, and associations of politicians, federal agents, intelligence personnel, military leadership, or special operations units constitutes intelligence in its own right.&lt;/p&gt;
    &lt;p&gt;Consider a scenario: If members of SEAL Team 6 or Delta Force disappear from roadways for several days, that absence is itself a signal. If, during the same timeframe, a primary French translator also vanishes from routine movement patterns, the signal sharpens. A coordinated absence across these roles would strongly suggest the initiation of a special operations mission inferred solely from movement data collected by a nationwide license plate reader network. A top secret clearance wouldn't be needed for top secret information.&lt;/p&gt;
    &lt;p&gt;China has previously compromised hotel infrastructure for years at a time, not to surveil ordinary guests, but to capture rare overlaps where officials from different countries stayed in the same location on the same night. (Source) If adversaries are willing to infiltrate hotel systems for fragments of movement data, the intelligence value of a nationwide, centralized surveillance map should be self-evident.&lt;/p&gt;
    &lt;head rend="h3"&gt;Domestic Risks&lt;/head&gt;
    &lt;p&gt;Persistent, indiscriminate movement tracking enables coercion, blackmail, and influence operations that do not require access to communications content. Members of Congress, senior military leaders, diplomats, corporate executives and their spouses and children are all placed at heightened risk. With sufficient coverage and time, patterns of life emerge. Affairs, undisclosed meetings, sensitive relationships, and routine behaviors become visible once movement data is collected and correlated at scale.&lt;/p&gt;
    &lt;p&gt;This is not a theoretical concern. The documented history of law enforcement misuse of license plate reader systems, including Flock's own platforms, demonstrates that access to movement data is routinely weaponized for personal purposes by those entrusted with it.&lt;/p&gt;
    &lt;head rend="h4"&gt;Documented Cases of Flock Camera Misuse&lt;/head&gt;
    &lt;p&gt;Braselton, Georgia (November 2025): Police Chief Michael Steffman was arrested and charged with stalking, harassment, and multiple counts of misusing automated license plate recognition systems after a months-long Georgia Bureau of Investigation probe revealed he used Flock cameras to track and harass multiple individuals. Steffman resigned hours before his arrest after serving the department for 20 years. Subsequent public records analysis by the grassroots coalition Get The Flock Out revealed that Steffman had searched Flock data from agencies in other states, including Capitola, California, demonstrating the cross-jurisdictional reach enabled by Flock's network sharing capabilities. (Source)&lt;/p&gt;
    &lt;p&gt;Sedgwick, Kansas (2023–2024): Police Chief Lee Nygaard used Flock Safety license plate readers to track his ex-girlfriend's vehicle 164 times and her new boyfriend's vehicle 64 times over a four-month period. He logged false justifications including "missing child," "drug investigation," and "suspicious activity" to conceal the personal nature of his searches. Nygaard also followed the couple in his patrol vehicle outside city limits. He resigned during the misconduct investigation. His police certification was revoked, though he faced no criminal charges. (Source)&lt;/p&gt;
    &lt;p&gt;Orange City, Florida (2024–2025): Officer Jarmarus Brown was arrested and charged with stalking and unauthorized computer access after using Flock license plate readers to track his ex-girlfriend's whereabouts for approximately seven months. An audit revealed he had repeatedly run tags for three specific vehicles. A fellow officer had warned Brown to "stop running her vehicle in that system because he could get in trouble" a warning Brown ignored. Brown also placed a GPS AirTag in the victim's wallet without her knowledge. When confronted by investigators, Brown admitted the situation was "dumb as hell on my end." He was served termination paperwork following his arrest. (Source)&lt;/p&gt;
    &lt;p&gt;These cases share common patterns: trusted officials often in leadership positions weaponizing surveillance tools against women with no connection to criminal investigations. The systems provided few meaningful barriers to misuse, and detection typically occurred only after victims independently reported suspicious behavior.&lt;/p&gt;
    &lt;head rend="h3"&gt;Congressional Concerns Validated&lt;/head&gt;
    &lt;p&gt;My research directly supports Senator Ron Wyden's claims that "Flock cannot live up to its commitment to protect the privacy and security of Oregonians" (Letter to Flock) and his urging for the "Federal Trade Commission (FTC) investigate Flock Safety... and, where appropriate, hold the company responsible for its negligent cybersecurity practices" (Letter to FTC).&lt;/p&gt;
    &lt;head rend="h2"&gt;On Flock's Security Claims&lt;/head&gt;
    &lt;p&gt;After the City of Staunton canceled its Flock Safety contract, CEO Garrett Langley sent an unsolicited email to Staunton Police Department (source) stating:&lt;/p&gt;
    &lt;quote&gt;"I'm writing to you directly because I want there to be zero confusion about what's happening. Flock has never been hacked. Ever."&lt;/quote&gt;
    &lt;p&gt;That statement is technically correct only in the narrowest sense. The absence of a breach was not the result of internal security controls, audits, or monitoring but of responsible disclosure. I identified the vulnerability and reported it so it could be remediated.&lt;/p&gt;
    &lt;p&gt;The absence of a hack does not imply the presence of security. Had this credential been found by anyone else, this may have been one of the largest data breaches and national security incidents of this decade.&lt;/p&gt;
    &lt;head rend="h3"&gt;On Compliance Claims&lt;/head&gt;
    &lt;p&gt;In the same communication, Flock asserted:&lt;/p&gt;
    &lt;quote&gt;"Flock is CJIS compliant"&lt;lb/&gt;"Flock adheres to the highest security standards, including NDAA, SOC 2 (Type II), SOC 3, ISO 27001, HECVAT, FERPA, and alignment with NIST and CAIQ."&lt;/quote&gt;
    &lt;p&gt;As a cybersecurity professional who has conducted dozens of compliance assessments, these statements are familiar. Compliance frameworks are often mistaken for guarantees of security, when in reality they are scoped evaluations of specific controls, not comprehensive examinations of an organization's risk posture. The scope of what is tested is defined by the company being assessed, which means compliance reflects what was reviewed, not everything that exists.&lt;/p&gt;
    &lt;p&gt;I requested access to Flock's audit reports; they were not provided. What I can say is this: a default, organization-wide API key embedded across 53 publicly reachable development and production-adjacent web assets would not survive even a basic review for exposed secrets or subdomains. Its persistence strongly suggests that this attack surface was either excluded from the assessment scope or insufficiently tested.&lt;/p&gt;
    &lt;p&gt;When a default, organization-wide credential persists across 53 publicly reachable assets, the failure is not merely procedural, it is architectural. The exposed surface was not a peripheral feature or isolated test environment. It was development infrastructure configured with privileges that would have granted access to private ArcGIS items shared within Flock Safety's organization.&lt;/p&gt;
    &lt;head rend="h2"&gt;What You Can Do&lt;/head&gt;
    &lt;p&gt;If you're a resident: File a public records request for your city's Flock Safety contract and any internal audit logs. Attend the next city council meeting where surveillance procurement is discussed. The EFF maintains a Street-Level Surveillance resource for tracking these deployments.&lt;/p&gt;
    &lt;p&gt;If you're a journalist: The technical evidence presented here is a starting point. I'm available for follow-up. There are more threads to pull.&lt;/p&gt;
    &lt;p&gt;If you're in law enforcement: Ask your vendor hard questions. Request their penetration test results. Demand to know where your agency's data lives and who else can access it. Your officers' safety depends on infrastructure that adversaries cannot trivially compromise.&lt;/p&gt;
    &lt;p&gt;If you're a policymaker: Senator Wyden's letters to Flock and the FTC are public record. Support an investigation. Mandate independent security audits for any vendor handling law enforcement location data.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Although the API key has now been rotated, the lesson remains. If a single cybersecurity researcher in his early twenties could gain direct technical access to an exposure of this magnitude, a well-resourced foreign adversary operating with intent could observe far more.&lt;/p&gt;
    &lt;p&gt;Flock Safety did not merely leak an API key. They exposed the operational heartbeat of the nation, and they did so repeatedly, across 53 separate instances.&lt;/p&gt;
    &lt;p&gt;That reality should concern everyone.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46555807</guid><pubDate>Fri, 09 Jan 2026 16:49:59 +0000</pubDate></item><item><title>IcePanel (YC W23) is hiring full-stack engineers in Vancouver</title><link>https://forms.icepanel.io/careers/senior-product-engineer</link><description>&lt;doc fingerprint="80b89e5ad2c4a6e0"&gt;
  &lt;main&gt;
    &lt;p&gt;$170,000 salary (CAD) + Profit-share quarterly bonus (last year averaged ~30k-40k each)&lt;/p&gt;
    &lt;p&gt;+ 1% equity + Unlimited holiday + Health benefits&lt;/p&gt;
    &lt;p&gt;We’re looking for someone with a high degree of agency, who can immediately take ownership of building new functionality from design &amp;gt; implementation &amp;gt; maintaining and refining current features based on our customers' needs.&lt;/p&gt;
    &lt;p&gt;You’ll be building end-to-end, including: - Frontend UI/UX design alongside a designer. - Backend API/data structure design. - Data migration and infrastructure changes. - Bug fixing and iterations.&lt;/p&gt;
    &lt;p&gt;We're simplifying how teams design for complex systems. We're building a collaborative diagramming and modelling tool that software architects think is cool.&lt;/p&gt;
    &lt;p&gt;We’re a small, energetic team that believes in building a lean and profitable business after being in the YCombinator W23 batch. We’ve grown the product to ~$4 million CAD in ARR and believe in continuing to build on profitability over funding. We’re looking for talented, driven people who love their craft to help achieve our vision of simplifying complexity.&lt;/p&gt;
    &lt;p&gt;🙋 Independence to build our way&lt;/p&gt;
    &lt;p&gt;🛠️ Build simple and exceptional experiences&lt;/p&gt;
    &lt;p&gt;🧊 Transparency and openness&lt;/p&gt;
    &lt;p&gt;💡 Stay humble and explore all ideas&lt;/p&gt;
    &lt;p&gt;💩 No bullshit, have fun&lt;/p&gt;
    &lt;p&gt;- In-person days every week (Tuesday, Wednesday, Thursday)&lt;/p&gt;
    &lt;p&gt;- North Vancouver, British Columbia, Canada&lt;/p&gt;
    &lt;p&gt;- Hybrid &amp;amp; flexible work environment&lt;/p&gt;
    &lt;p&gt;- This is not a fully remote job&lt;/p&gt;
    &lt;p&gt;🍰 Equity in the company 💰 Profit sharing&lt;/p&gt;
    &lt;p&gt;💻 Work setup provided&lt;/p&gt;
    &lt;p&gt;🎉 Flexible work culture&lt;/p&gt;
    &lt;p&gt;🏂 Unlimited holiday&lt;/p&gt;
    &lt;p&gt;🧑⚕️ Health, dental, vision&lt;/p&gt;
    &lt;p&gt;📚 Learning budget&lt;/p&gt;
    &lt;p&gt;✈️ Conference budget&lt;/p&gt;
    &lt;p&gt;🌴 Annual team retreat&lt;/p&gt;
    &lt;p&gt;🌭 Hot dog Wednesdays&lt;/p&gt;
    &lt;p&gt;🧊 Free ice cubes&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46555977</guid><pubDate>Fri, 09 Jan 2026 17:01:02 +0000</pubDate></item><item><title>Show HN: I made a memory game to teach you to play piano by ear</title><link>https://lend-me-your-ears.specr.net</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=46556210</guid><pubDate>Fri, 09 Jan 2026 17:17:28 +0000</pubDate></item><item><title>How Markdown took over the world</title><link>https://www.anildash.com/2026/01/09/how-markdown-took-over-the-world/</link><description>&lt;doc fingerprint="31a09b5f3ae00f98"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;How Markdown took over the world&lt;/head&gt;
    &lt;p&gt;Nearly every bit of the high-tech world, from the most cutting-edge AI systems at the biggest companies, to the casual scraps of code cobbled together by college students, is annotated and described by the same, simple plain text format. Whether you’re trying to give complex instructions to ChatGPT, or you want to be able to exchange a grocery list in Apple Notes or copy someone’s homework in Google Docs, that same format will do the trick. The wild part is, the format wasn’t created by a conglomerate of tech tycoons, it was created by a curmudgeonly guy with a kind heart who right this minute is probably rewatching a Kubrick film while cheering for an absolutely indefensible sports team.&lt;/p&gt;
    &lt;p&gt;But it’s worth understanding how these simple little text files were born, not just because I get to brag about how generous and clever my friends are, but also because it reminds us of how the Internet really works: smart people think of good things that are crazy enough that they just might work, and then they give them away, over and over, until they slowly take over the world and make things better for everyone.&lt;/p&gt;
    &lt;head rend="h2"&gt;Making Their Mark&lt;/head&gt;
    &lt;p&gt;Though it’s now a building block of the contemporary Internet, like so many great things, Markdown just started out trying to solve a personal problem. In 2002, John Gruber made the unconventional decision to bet his online career on two completely irrational foundations: Apple, and blogs.&lt;/p&gt;
    &lt;p&gt;It’s hard to remember now, but in 2002, Apple was just a few years past having been on death’s door. As difficult as it may be to picture in today’s world where Apple keynotes are treated like major events, back then, almost nobody was covering Apple regularly, let alone writing exclusively about the company. There was barely even an “tech news” scene online at all, and virtually no one was blogging. So John’s decision to go all-in on Apple for his pioneering blog Daring Fireball was, well, a daring one. At the time, Apple had only just launched its first iPod that worked with Windows computers, and the iPhone was still a full five years in the future. But that single-minded focus, not just on Apple, but on obsessive detail in everything he covered, eventually helped inspire much of the technology media landscape that we see today. John’s timing was also perfect — from the doldrums of that era, Apple’s stock price would rise by about 120,000% in the years after Daring Fireball started, and its cultural relevance probably increased by even more than that.&lt;/p&gt;
    &lt;p&gt;By 2004, it wasn’t just Apple that had begun to take off: blogs and social media themselves had moved from obscurity to the very center of culture, and a new era of web technology had begun. At the beginning of that year, few people in the world even knew what a “blog” was, but by the end of 2004, blogs had become not just ubiquitous, but downright cool. As unlikely as it seems now, that year’s largely uninspiring slate of U.S. presidential candidates like Wesley Clark, Gary Hart and, yes, Howard Dean helped propel blogs into mainstream awareness during the Democratic primaries, alongside online pundits who had begun weighing in on politics and the issues and cultural moments at a pace that newspapers and TV couldn’t keep up with. A lot has been written about the transformation of media during those years, but less has been written about how the media and tech of the time transformed each other.&lt;/p&gt;
    &lt;p&gt;That era of early blogging was interesting in that nearly everyone who was writing the first popular sites was also busy helping create the tools for publishing them. Just like Lucille Ball and Desi Arnaz had to pioneer combining studio-style flat lighting with 35mm filming in order to define the look of the modern sitcom, or Jimi Hendrix had to work with Roger Mayer to invent the signature guitar distortion pedals that defined the sound of rock and roll, the pioneers who defined the technical format and structures of blogging were often building the very tools of creation as they went along.&lt;/p&gt;
    &lt;p&gt;I got a front row seat to these acts of creation. At the time I was working on Movable Type, which was the most popular tool for publishing “serious” blogs, and helped popularize the medium. Two of my good friends had built the tool and quickly made it into the default choice for anybody who wanted to reach a big audience; it was kind of a combination of everything people do these days on WordPress and all the various email newsletter platforms and all of the “serious” podcasts (since podcasts wouldn’t be invented for another few months). But back in those early days, we’d watch people use our tools to set up Gawker or Huffington Post one day, and Daring Fireball or Waxy.org the next, and each of them would be the first of its kind, both in terms of its design and its voice. To this day, when I see something online that I love by Julianne Escobedo Shepherd or Ta-Nehisi Coates or Nilay Patel or Annalee Newitz or any one of dozens of other brilliant writers or creators, my first thought is often, “hey! They used to type in that app that I used to make!” Because sometimes those writers would inspire us to make a new feature in the publishing tools, and sometimes they would have hacked up a new feature all by themselves in between typing up their new blog posts.&lt;/p&gt;
    &lt;p&gt;A really clear, and very simple, early example of how we learned that lesson was when we changed the size of the box that people used to type in just to create the posts on their sites. We made the box a little bit taller, mostly for aesthetic reasons. Within a few weeks, we’d found that posts on sites like Gawker had gotten longer, mostly because the box was bigger. This seems obvious now, years after we saw tweets get longer when Twitter expanded from 140 characters to 280 characters, but at the time this was a terrifying glimpse at how much power a couple of young product managers in a conference room in California would have over the media consumption of the entire world every time they made a seemingly-insignificant decision.&lt;/p&gt;
    &lt;p&gt;The other dirty little secret was, typing in the box in that old blogging app could be… pretty wonky sometimes. People who wanted to do normal things like include an image or link in their blog post, or even just make some text bold, often had to learn somewhat-obscure HTML formatting, memorizing the actual language that’s used to make web pages. Not everybody knew all the details of how to make pages that way, and if they made even one small mistake, sometimes they could break the whole design of their site. It made things feel very fraught every time a writer went to publish something new online, and got in the way of the increasingly-fast pace of sharing ideas now that social media was taking over the public conversation.&lt;/p&gt;
    &lt;p&gt;Enter John and his magical text files.&lt;/p&gt;
    &lt;head rend="h2"&gt;Marking up and marking down&lt;/head&gt;
    &lt;p&gt;The purpose of Markdown is really simple: It lets you use the regular characters on your keyboard which you already use while typing out things like emails, to make fancy formatting of text for the web. That HTML format that’s used to make web pages stands for HyperText Markup Language. The word “markup” there means you’re “marking up” your text with all kinds of special characters. Only, the special characters can be kind of arcane. Want to put in a link to everybody’s favorite website? Well, you’re going to have to type in &lt;code&gt;&amp;lt;a href="https://anildash.com/"&amp;gt;Anil Dash’s blog&amp;lt;/a&amp;gt;&lt;/code&gt; I could explain why, and what it all means, but honestly, you get the point — it’s a lot! Too much. What if you could just write out the text and then the link, sort of like you might within an email? Like: &lt;code&gt;[Anil Dash’s blog](https://anildash.com)&lt;/code&gt;! And then the right thing would happen. Seems great, right?&lt;/p&gt;
    &lt;p&gt;The same thing works for things like putting a header on a page. For example, as I’m writing this right now, if I want to put a big headline on this page, I can just type &lt;code&gt;#How Markdown Took Over the World&lt;/code&gt; and the right thing will happen.&lt;/p&gt;
    &lt;p&gt;If mark_up_ is complicated, then the opposite of that complexity must be… markd_own_. This kind of solution, where it’s so smart it seems obvious in hindsight, is key to Markdown’s success. John worked to make a format that was so simple that anybody could pick it up in a few minutes, and powerful enough that it could help people express pretty much anything that they wanted to include while writing on the internet. At a technical level, it was also easy enough to implement that John could write the code himself to make it work with Movable Type, his publishing tool of choice. (Within days, people had implemented the same feature for most of the other blogging tools of the era; these days, virtually every app that you can type text into ships with Markdown support as a feature on day one.)&lt;/p&gt;
    &lt;p&gt;Prior to launch, John had enlisted our mutual friend, the late, dearly missed Aaron Swartz, as a beta tester. In addition to being extremely fluent in every detail of the blogging technologies of the time, Aaron was, most notably, seventeen years old. And though Aaron’s activism and untimely passing have resulted in him having been turned into something of a mythological figure, one of the greatest things about Aaron was that he could be a total pain in the ass, which made him terrific at reporting bugs in your software. (One of the last email conversations I ever had with Aaron was him pointing out some obscure bugs in an open source app I was working on at the time.) No surprise, Aaron instantly understood both the potential and the power of Markdown, and was a top-tier beta tester for the technology as it was created. His astute feedback helped finely hone the final product so it was ready for the world, and when Markdown quietly debuted in March of 2004, it was clear that text files around the web were about to get a permanent upgrade.&lt;/p&gt;
    &lt;p&gt;The most surprising part of what happened next wasn’t that everybody immediately started using it to write their blogs; that was, after all, what the tool was designed to do. It’s that everybody started using Markdown to do everything else, too.&lt;/p&gt;
    &lt;head rend="h2"&gt;Hitting the Mark&lt;/head&gt;
    &lt;p&gt;It’s almost impossible to overstate the ubiquity of Markdown within the modern computer industry in the decades since its launch.&lt;/p&gt;
    &lt;p&gt;After being nagged about it by users for more than a decade, Google finally added support for Markdown to Google Docs, though it took them years of fiddly improvements to make it truly usable. Just last year, Microsoft added support for Markdown to its venerable Notepad app, perhaps in attempt to assuage the tempers of users who were still in disbelief that Notepad had been bloated with AI features. Nearly every powerful group messaging app, from Slack to WhatsApp to Discord, has support for Markdown in messages. And even the company that indirectly inspired all of this in the first place finally got on board: the most recent version of Apple Notes finally added support for Markdown. (It’s an especially striking launch by Apple due to its timing, shortly after John had used his platform as the most influential Apple writer in the world to blog about the utter failure of the “Apple Intelligence” AI launch.)&lt;/p&gt;
    &lt;p&gt;But it’s not just the apps that you use on your phone or your laptop. For developers, Markdown has long been the lingua franca of the tools we string together to accomplish our work. On GitHub, the platform that nearly every developer in the world uses to share their code, nearly every single repository of code on the site has at least one Markdown file that’s used to describe its contents. Many have dozens of files describing all the different aspects of their project. And some of the repositories on GitHub consist of nothing but massive collections of Markdown files. The small tools and automations we run to perform routine tasks, the one-off reports that we generate to make sure something worked correctly, the confirmations that we have a system email out when something goes wrong, the temporary files we use when trying to recover some old data — all of these default to being Markdown files.&lt;/p&gt;
    &lt;p&gt;As a result, there are now billions of Markdown files lying around on hard drives around the world. Billions more are stashed in the cloud. There are some on the phone in your pocket. Programmers leave them lying around wherever their code might someday be running. Your kid’s Nintendo Switch has Markdown files on it. If you’re listening to music, there’s probably a Markdown file on the memory chip of the tiny system that controls the headphones stuck in your ears. The Markdown is inside you right now!&lt;/p&gt;
    &lt;head rend="h2"&gt;Down For Whatever&lt;/head&gt;
    &lt;p&gt;So far, these were all things we could have foreseen when John first unleashed his little text tool on the world. I would have been surprised about how many people were using it, but not really the ways in which they were using it. If you’d have said “Twenty years in the future, all the different note-taking apps people use save their files using Markdown!”, I would have said, “Okay, that makes sense!”&lt;/p&gt;
    &lt;p&gt;What I wouldn’t have asked, though, was “Is John getting paid?” As hard as it may be to believe, back in 2004, the default was that people made new standards for open technologies like Markdown, and just shared them freely for the good of the internet, and the world, and then went on about their lives. If it happened to have unleashed billions of dollars of value for others, then so much the better. If they got some credit along the way, that was great, too. But mostly you just did it to solve a problem for yourself and for other like-minded people. And also, maybe, to help make sure that some jerk didn’t otherwise create some horrible proprietary alternative that would lock everybody into their terrible inferior version forever instead. (We didn’t have the word “enshittification” yet, but we did have Cory Doctorow and we did have plain text files, so we kind of knew where things were headed.)&lt;/p&gt;
    &lt;p&gt;To give a sense of the vibe of that era, the term “podcasting” had been coined just a month before Markdown was released, and went into wider use that fall, and was similarly a radically open system that wasn’t owned by any big company and that empowered people to do whatever they wanted to do to express themselves. (And podcasting was another technology that Aaron Swartz helped improve by being a brilliant pain in the ass. But I’ll save that story for another book-length essay.)&lt;/p&gt;
    &lt;p&gt;That attitude of being not-quite-_anti_commercial, but perhaps just not even really concerned with whether something was commercial or not seems downright quaint in an era when the tech tycoons are not just the wealthiest people in the world, but also some of the weirdest and most obnoxious as well. But the truth is, most people today who make technology are actually still exceedingly normal, and quite generous. It’s just that they’ve been overshadowed by their bosses who are out of their minds and building rocket ships and siring hundreds of children and embracing overt white supremacy instead of making fun tools for helping you type text, like regular people do.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Markdown Model&lt;/head&gt;
    &lt;p&gt;The part about not doing this stuff solely for money matters, because even the most advanced LLM systems today, what the big AI companies call their “frontier” models, require complex orchestration that’s carefully scripted by people who’ve tuned their prompts for these systems through countless rounds of trial and error. They’ve iterated and tested and watched for the results as these systems hallucinated or failed or ran amok, chewing up countless resources along the way. And sometimes, they generated genuinely astonishing outputs, things that are truly amazing to consider that modern technology can achieve. The rate of progress and evolution, even factoring in the mind-boggling amounts of investment that are going into these systems, is rivaled only by the initial development of the personal computer or the Internet, or the early space race.&lt;/p&gt;
    &lt;p&gt;And all of it — all of it — is controlled through Markdown files. When you see the brilliant work shown off from somebody who’s bragging about what they made ChatGPT generate for them, or someone is understandably proud about the code that they got Claude to create, all of the most advanced work has been prompted in Markdown. Though where the logic of Markdown was originally a very simple version of "use human language to tell the machine what to do", the implications have gotten far more dire when they use a format designed to help expresss "make this &lt;code&gt;**bold**&lt;/code&gt;" to tell the computer itself "&lt;code&gt;make this imaginary girlfriend more compliant&lt;/code&gt;".&lt;/p&gt;
    &lt;p&gt;But we already know that the Big AI companies are run by people who don't reckon with the implications of their work. They could never understand that every single project that's even moderately ambitious on these new AI platforms is being written up in files formatted according to this system created by one guy who has never asked for a dime for this work. An entire generation of AI coders has been born since Markdown was created who probably can’t even imagine that this technology even has an "inventor". It’s just always been here, like the Moon, or Rihanna.&lt;/p&gt;
    &lt;p&gt;But it’s important for everyone to know that the Internet, and the tech industry, don’t run without the generosity and genius of regular people. It is not just billion-dollar checks and Silicon Valley boardrooms that enable creativity over years, decades, or generations — it’s often a guy with a day job who just gives a damn about doing something right, sweating the details and assuming that if he cares enough about what he makes then others will too. The majority of the technical infrastructure of the Internet was created in this way. For free, often by people in academia, or as part of their regular work, with no promise of some big payday or getting a ton of credit.&lt;/p&gt;
    &lt;p&gt;The people who make the real Internet and the real innovations also don’t look for ways to hurt the world around them, or the people around them. Sometimes, as in the case of Aaron, the world hurts them more than anyone should ever have to bear. I know not everybody cares that much about plain text files on the Internet; I will readily admit I am a huge nerd about this stuff in a way that maybe most normal people are not. But I do think everybody cares about some part of the wonderful stuff on the Internet in this way, and I want to fight to make sure that everybody can understand that it’s not just five terrible tycoons who built this shit. Real people did. Good people. I saw them do it.&lt;/p&gt;
    &lt;p&gt;The trillion-dollar AI industry's system for controlling their most advanced platforms is a plain text format one guy made up for his blog and then bounced off of a 17-year-old kid before sharing it with the world for free. You're welcome, Time Magazine's people of the year, The Architects of AI. Their achievement is every bit as impressive as yours.&lt;/p&gt;
    &lt;head rend="h1"&gt;The Ten Technical Reasons Markdown Won&lt;/head&gt;
    &lt;p&gt;Okay, with some of the narrative covered, what can we learn from Markdown’s success? How did this thing really take off? What could we do if we wanted to replicate something like this in the modern era? Let’s consider a few key points:&lt;/p&gt;
    &lt;head rend="h3"&gt;1. Had a great brand.&lt;/head&gt;
    &lt;p&gt;Okay, let’s be real: “Markdown” as a name is clever as hell. Get it it’s not markup, it’s mark down. You just can’t argue with that kind of logic. People who knew what the “M” in “HTML” stood for could understand the reference, and to everyone else, it was just a clearly-understandable name for a useful utility.&lt;/p&gt;
    &lt;head rend="h3"&gt;2. Solved a real problem.&lt;/head&gt;
    &lt;p&gt;This one is not obvious, but it’s really important that a new technology have a real problem that it’s trying to solve, instead of just being an abstract attempt to do something vague, like “make text files better”. Millions of people were encountering the idea that it was too difficult or inconvenient to write out full HTML by hand, and even if one had the necessary skills, it was nice to be able to do so in a format that was legible as plain text as well.&lt;/p&gt;
    &lt;head rend="h3"&gt;3. Built on behaviors that already existed.&lt;/head&gt;
    &lt;p&gt;This is one of the most quietly genius parts of Markdown: The format is based on the ways people had been adding emphasis and formatting to their text for years or even decades. Some of the formatting choices dated back to the early days of email, so they’d been ingrained in the culture of the internet for a full generation before Markdown existed. It was so familiar, people could be writing Markdown without even knowing it.&lt;/p&gt;
    &lt;head rend="h3"&gt;4. Mirrored RSS in its origin.&lt;/head&gt;
    &lt;p&gt;Around the same time that Markdown was taking off, RSS was maturing into its ubiquitous form as well. The format had existed for some years already, enabling various kinds of content syndication, but at this time, it was adding support for the technologies that would come to be known as podcasting as well. And just like RSS, Markdown was spearheaded by a smart technologist who was also more than a little stubborn about defining a format that would go on to change the way we share content on the internet. In RSS’ case, it was pioneered by Dave Winer, and with Markdown it was John Gruber, and both were tireless in extolling the virtues of the plain text formats they’d helped pioneer. They could both leverage blogs to get the word out, and to get feedback on how to build on their wins.&lt;/p&gt;
    &lt;head rend="h3"&gt;5. There was a community ready to help.&lt;/head&gt;
    &lt;p&gt;One great thing about a format like Markdown is that its success is never just the result of one person. Vitally, Markdown was part of a community that could build on it right from the start. Right from the beginning, Markdown was inspired by earlier works like Textile, a formatting system for plain text created by Dean Allen. Many of us appreciated and were inspired by Dean, who was a pioneer of blogging tools in the early days of social media, but if there’s a bigger fan of Dean Allen on the internet than John Gruber, I’ve never met them. Similarly, Aaron Swartz, the brilliant young technologist who’s known best known as an activist for digital rights and access, was at that time just a super brilliant teenager that a lot of us loved hacking with. He was the most valuable beta tester of Markdown prior to its release, helping to shape it into a durable and flexible format that’s stood the test of time.&lt;/p&gt;
    &lt;head rend="h3"&gt;6. Had the right flavor for every different context.&lt;/head&gt;
    &lt;p&gt;Because Markdown’s format was frozen in place (and had some super-technical details that people could debate about) and people wanted to add features over time, various communities that were implementing Markdown could add their own “flavors” of it as they needed. Popular ones came to be called Commonmark and Github-Flavored, led by various companies or teams that had divergent needs for the tool. While tech geeks tend to obsess over needing everything to be “correct”, in reality it often just doesn’t matter that much, and in the real world, the entire Internet is made up of content that barely follows the technical rules that it’s supposed to.&lt;/p&gt;
    &lt;head rend="h3"&gt;7. Released at a time of change in behaviors and habits.&lt;/head&gt;
    &lt;p&gt;This is a subtle point, but an important one: Markdown came along at the right time in the evolution of its medium. You can get people to change their behaviors when they’re using a new tool, or adopting a new technology. In this case, blogging (and all of social media!) were new, so saying “here’s a new way of typing a list of bullet points” wasn’t much an additional learning curve to add to the mix. If you can take advantage of catching people while they’re already in a learning mood, you can really tap into the moment when they’re most open-minded to new things.&lt;/p&gt;
    &lt;head rend="h3"&gt;8. Came right on the cusp of the “build tool era”.&lt;/head&gt;
    &lt;p&gt;This one’s a bit more technical, but also important to understand. In the first era of building for the web, people often built the web’s languages of HTML, Javascript and CSS by hand, by themselves, or stitched these formats together from subsets or templates. But in many cases, these were fairly simple compositions, made up of smaller pieces that were written in the same languages. As things matured, the roles for web developers specialized (there started to be backend developers vs. front-end, or people who focused on performance vs. those who focused on visual design), and as a result the tooling for developers matured. On the other side of this transition, developers began to use many different programming languages, frameworks and tools, and the standard step before trying to deploy a website was to have an automated build process that transformed the “raw materials” of the site into the finished product. Since Markdown is a raw material that has to be transformed into HTML, it perfectly fit this new workflow as it became the de facto standard method of creation and collaboration.&lt;/p&gt;
    &lt;head rend="h3"&gt;9. Worked with “View source”&lt;/head&gt;
    &lt;p&gt;Most of the technologies that work best on the web enable creators to “view source” just like HTML originally did when the first web browsers were created. In this philosophy, one can look at the source code that makes up a web page, and understand how it was constructed so that you can make your own. With Markdown, it only takes one glimpse of a source Markdown file for anyone to understand how they might make a similar file of their own, or to extrapolate how they might apply analogous formatting to their own documents. There’s no teaching required when people can just see it for themselves.&lt;/p&gt;
    &lt;head rend="h3"&gt;10. Not encumbered in IP&lt;/head&gt;
    &lt;p&gt;This one’s obvious if you think about it, but it can’t go unsaid: There are no legal restrictions around Markdown. You wouldn’t think that anybody would be foolish or greedy enough to try to patent something as simple as Markdown, but there are many far worse examples of patent abuse in the tech industry. Fortunately, John Gruber is not an awful person, and nobody else has (yet) been brazen enough to try to usurp the format for their own misadventures in intellectual property law. As a result, nobody’s been afraid, either to use the format, or to support creating or reading the format in their apps.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46556695</guid><pubDate>Fri, 09 Jan 2026 17:52:20 +0000</pubDate></item><item><title>Replit (YC W18) Is Hiring</title><link>https://jobs.ashbyhq.com/replit</link><description>&lt;doc fingerprint="e10fcdab2cdf53e4"&gt;
  &lt;main&gt;
    &lt;p&gt;You need to enable JavaScript to run this app.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46556822</guid><pubDate>Fri, 09 Jan 2026 18:00:56 +0000</pubDate></item><item><title>Show HN: Scroll Wikipedia like TikTok</title><link>https://quack.sdan.io</link><description>&lt;doc fingerprint="434f9162ba198e26"&gt;
  &lt;main&gt;
    &lt;p&gt;Following Slop Ducks Storytime Home Friends Inbox Profile&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46557029</guid><pubDate>Fri, 09 Jan 2026 18:15:16 +0000</pubDate></item><item><title>My article on why AI is great (or terrible) or how to use it</title><link>https://matthewrocklin.com/ai-zealotry/</link><description>&lt;doc fingerprint="760816363f1703bd"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;AI Zealotry¶&lt;/head&gt;
    &lt;p&gt;I develop with AI today. It's great.&lt;/p&gt;
    &lt;p&gt;There are many articles you can read on why AI is great (or terrible) or how to use it. This is mine. I focus on the experience of a senior engineer (and why we in particular should use AI), on my experience operating within the OSS Python Data world, and on practical suggestions that I've found myself repeating to colleagues.&lt;/p&gt;
    &lt;p&gt;This article contains learned lessons of two types:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Big Ideas: Grand(iose) philosophy on why AI is great for experienced programmers&lt;/item&gt;
      &lt;item&gt;Tips: Taken from my workflow using Claude Code&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We'll interleave these two. I'm hopeful that this approach will make this more fun.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why AI¶&lt;/head&gt;
    &lt;p&gt;AI development is more fun. I do more of what I like (think, experiment, write) and less of what I don't like (wrestle with computers).&lt;/p&gt;
    &lt;p&gt;I feel both that I can move faster and operate in areas that were previously inaccessible to me (like frontend). Experienced developers should all be doing this. We're good enough to avoid AI Slop, and there's so much we can accomplish today.&lt;/p&gt;
    &lt;p&gt;I like this quote from this blog&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;I get it, you’re too good to vibe code. You’re a senior developer who has been doing this for 20 years and knows the system like the back of your hand.&lt;/p&gt;
      &lt;p&gt;[...]&lt;/p&gt;
      &lt;p&gt;No, you’re not too good to vibe code. In fact, you’re the only person who should be vibe coding.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I think that really good engineers, the kind that think hard before writing, can have a tremendous impact and fun while developing with AI. I wouldn't ever go back.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why Not AI¶&lt;/head&gt;
    &lt;p&gt;That being said, there are some serious costs and reasonable reservations to AI development. Let's start by listing those concerns:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;LLMs generate junk&lt;/item&gt;
      &lt;item&gt;LLMs generate a lot of junk&lt;/item&gt;
      &lt;item&gt;Writing code ourselves builds understanding&lt;/item&gt;
      &lt;item&gt;Reviewing code for correctness is the slow part, not writing it&lt;/item&gt;
      &lt;item&gt;AI workflows can be dehumanizing when you just press "yes, allow" over and over again&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These are super-valid concerns. They're also concerns that I suspect came around when we developed compilers and people stopped writing assembly by hand, instead trusting programs like &lt;code&gt;gcc&lt;/code&gt; to pump out instruction after instruction
of shitty machine code.&lt;/p&gt;
    &lt;p&gt;We lost a deeper understanding as developers when we stopped writing assembly but we gained a ton too. As in any transition, we need to navigate the situation to capture the advantages while losing only a little, balancing the costs and benefits of a new technology.&lt;/p&gt;
    &lt;p&gt;This article is how I've been navigating this transition personally.&lt;/p&gt;
    &lt;head rend="h2"&gt;Big Idea: Minimize Interruptions / Climb Abstraction Hierarchy¶&lt;/head&gt;
    &lt;p&gt;Early in using Claude Code (or Cursor) many of my interactions were saying "Yes, it's ok to run that". This was frustrating and dehumanizing. Mostly my job was to enable AI, rather than the other way around.&lt;/p&gt;
    &lt;p&gt;There are many tricks to resolve this (see below), but more broadly "stop doing simple shit" has been a mantra that I've found myself constantly coming back to. The more I identify and reject simple tasks and add automation to my workflow, the higher an abstraction I'm able to climb to and the more effectively I'm able to work. Our goal in programming is to climb an abstraction ladder and gain more intellectual leverage. This requires thought and consistent attention.&lt;/p&gt;
    &lt;p&gt;Fortunately AI can help with this. If you complain and say "I'm always doing X" it'll suggest solutions like what I'll talk about below, but more tailored to your situation.&lt;/p&gt;
    &lt;head rend="h2"&gt;Tip: Hooks¶&lt;/head&gt;
    &lt;p&gt;AI developers, like human developers, benefit from structure.&lt;/p&gt;
    &lt;p&gt;Most people start with an &lt;code&gt;AGENTS.md&lt;/code&gt; or &lt;code&gt;CLAUDE.md&lt;/code&gt; file.  This is a great
start, but I find that the AI agent often forgets what's in there.  The real
solution for me here (at least for Claude Code) is
Hooks.&lt;/p&gt;
    &lt;p&gt;First, let's outline a couple of annoyingly common problems.&lt;/p&gt;
    &lt;head rend="h3"&gt;Example Problem: Ignoring instructions in CLAUDE.md¶&lt;/head&gt;
    &lt;p&gt;Let's say you tell AI that you want to run tests with &lt;code&gt;uv&lt;/code&gt;:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;when running tests, use&lt;/p&gt;
      &lt;code&gt;uv run pytest tests&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;While this works sometimes, AI often decides to run&lt;/p&gt;
    &lt;code&gt;$ pytest tests/
command not found: pytest
&lt;/code&gt;
    &lt;p&gt;While the agents read CLAUDE.md, they don't always follow the instructions. And so you're stuck saying "no, use &lt;code&gt;uv&lt;/code&gt;"  over and over again. Gah.&lt;/p&gt;
    &lt;head rend="h3"&gt;Solution: Hooks¶&lt;/head&gt;
    &lt;p&gt;Here's a hook that catches pytest commands missing uv run. You could put something like this in &lt;code&gt;~/.claude/settings.json&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;{
  "hooks": {
    "PreToolUse": [
      {
        "matcher": "Bash",
        "hooks": [
          {
            "type": "command",
            "command": "python ~/.claude/hooks/check-uv-pytest.py"
          }
        ]
      }
    ]
  }
}
&lt;/code&gt;
    &lt;code&gt;#!/usr/bin/env python3
import json
import sys

data = json.load(sys.stdin)
cmd = data.get("tool_input", {}).get("command", "")

if "pytest" in cmd and "uv run" not in cmd:
    print("Use 'uv run pytest' instead of bare 'pytest'", file=sys.stderr)
    sys.exit(2)
&lt;/code&gt;
    &lt;p&gt;There, we've just automated that annoying task for you forever.&lt;/p&gt;
    &lt;p&gt;I don't actually do this though (I allow Claude to fail and then it finds the right approach.) Mostly this works because I've gotten good at giving Claude fairly broad-yet-safe permissions, which is coming up next.&lt;/p&gt;
    &lt;head rend="h3"&gt;Example Problem: Incomplete Permissions¶&lt;/head&gt;
    &lt;p&gt;Even worse, Claude often asks for permission to do things that are just slightly different from what you've already granted. You allow &lt;code&gt;uv run pytest *&lt;/code&gt;, but Claude keeps finding variants:&lt;/p&gt;
    &lt;code&gt;timeout 60 uv run pytest ...
timeout 40 uv run pytest ...
uv run pytest ... | head
.venv/bin/pytest ...
&lt;/code&gt;
    &lt;p&gt;Claude Code's permission language sucks. It only supports prefixes, while I wish it could handle regexes, or maybe even just arbitrary Python code.&lt;/p&gt;
    &lt;head rend="h3"&gt;Solution: Hooks for permissions¶&lt;/head&gt;
    &lt;p&gt;I have a complex Python script as a hook which overrides the permission system. It uses regexes, but also arbitrary Python code as logic. This allows me to encode arbitrary combinations of rules. It's great.&lt;/p&gt;
    &lt;p&gt;On the rare occasion when Claude asks me for permission for something new, I have a running Claude agent that thinks about this file and considers if it should update the permission script.&lt;/p&gt;
    &lt;head rend="h3"&gt;Solution: Hooks for sounds¶&lt;/head&gt;
    &lt;p&gt;My personal favorite hooks though are these:&lt;/p&gt;
    &lt;code&gt;"Stop": [
  {
    "hooks": [
      {
        "type": "command",
        "command": "afplay -v 0.40 /System/Library/Sounds/Morse.aiff"
      }
    ]
  }
],
"Notification": [
  {
    "hooks": [
      {
        "type": "command",
        "command": "afplay -v 0.35 /System/Library/Sounds/Ping.aiff"
      }
    ]
  }
]
&lt;/code&gt;
    &lt;p&gt;They play subtle little sounds whenever Claude is either done, or needs input from me. This lets me ignore Claude when it's busy. Previously I found that I was constantly checking back in with Claude to see if it was done, and that action was dehumanizing, so I automated it by asking Claude to play a sound.&lt;/p&gt;
    &lt;p&gt;Hooks are great. There are more ways to provide structure (Skills, Commands) but I've found that Hooks are the most dependable, a great starting place, and often augment any other structure that I put in place (like Skills).&lt;/p&gt;
    &lt;head rend="h2"&gt;Big Idea: Build Confidence Without Looking at Code¶&lt;/head&gt;
    &lt;p&gt;In a recent large AI-assisted PR a frustrated reviewer said the following:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;To me, this [size of PR] implies that either&lt;/p&gt;
      &lt;item&gt;reviewers should blindly trust Claude, or&lt;/item&gt;
      &lt;item&gt;reviewers should spend the months worth of effort going through Claude's changes, without the developer bothering to do the same first.&lt;/item&gt;
    &lt;/quote&gt;
    &lt;p&gt;It's a valid problem, even in single-person projects. We're able to generate code far more quickly than we're able to read it. How should we handle review? Everyone needs to figure this out for themselves, but my answer is "find other ways to build confidence".&lt;/p&gt;
    &lt;p&gt;We already do this today with human-written code. I review some code very closely, and other code less-so. Sometimes I rely on a combination of tests, familiarity of a well-known author, and a quick glance at the code to before saying "sure, seems fine" and pressing the green button. I might also ask "Have you thought of X" and see what they say.&lt;/p&gt;
    &lt;p&gt;Trusting code without reading all of it isn't new, we're just now in a state where we need to review 10x more code, and so we need to get much better at establishing confidence that something works without paying human attention all the time.&lt;/p&gt;
    &lt;p&gt;We can augment our ability to write code with AI. We can augment our ability to review code with AI too.&lt;/p&gt;
    &lt;head rend="h2"&gt;Tip: Self-review¶&lt;/head&gt;
    &lt;head rend="h3"&gt;Testing¶&lt;/head&gt;
    &lt;p&gt;Mostly I establish confidence on AI-generated work by investing heavily in tests and benchmarks, the same as I would with humans, just moreso. TDD is baked into most of the prompting structure I have with agents.&lt;/p&gt;
    &lt;p&gt;Remember that this is way cheaper than it used to be. Now rather than write a benchmark I can type&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;How does this compare in performance to the old version? I'm particularly interested in memory use.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;And that's it. If it's bad, the agent will say so (and then diligently work to make it good).&lt;/p&gt;
    &lt;head rend="h3"&gt;Grilling¶&lt;/head&gt;
    &lt;p&gt;Additionally, if I'm nervous about something subtle like "Is it possible this change might unexpectedly affect performance in this other feature?" then I'll ask the AI exactly that question:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Is it possible this change might unexpectedly affect performance in this other feature?&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;And it'll just go and investigate exactly that question. Unlike human authors, the AI has no ego at stake in its work, and isn't in the least bit lazy. It's our job to ask "Have you thought of X" and its job to go learn if that might be an issue. Don't trust its answer? Ask it to prove it to you.&lt;/p&gt;
    &lt;p&gt;AI has flaws, but it is diligent, and it lacks ego. If you question it, it'll investigate thoroughly and critique its own work honestly.&lt;/p&gt;
    &lt;head rend="h3"&gt;Simplifying¶&lt;/head&gt;
    &lt;p&gt;Also, my favorite command:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Let's review our work and see if there is anything we can simplify or clean up&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Before Opus 4.5 came out this was essential. Now it's merely nice. I've turned this into a &lt;code&gt;/cleanup&lt;/code&gt; command and integrated it into most of my Skills
as a final phase in development.&lt;/p&gt;
    &lt;head rend="h3"&gt;Tech debt¶&lt;/head&gt;
    &lt;p&gt;From time to time I also ask a fresh agent to do a full review of the project, with an eye to cleaning up technical debt. I tell it to review everything and think hard. It takes a while, but it often comes back with a nice list of work for itself, which it then of course diligently performs.&lt;/p&gt;
    &lt;p&gt;AI creates technical debt, but it can clean some of it up too. (at least at a certain granularity)&lt;/p&gt;
    &lt;head rend="h2"&gt;Feedback¶&lt;/head&gt;
    &lt;p&gt;In general we want to give our agents good automated feedback. Tests do this, benchmarks do this, prompting them to assess themselves does this, asking them to explain things to us and have us weigh in on high level topics does this.&lt;/p&gt;
    &lt;p&gt;LLMs are smart enough today that if they're given enough of the right feedback they converge to a good solution as-well-or-better-than a senior human engineer (that's my experience at least).&lt;/p&gt;
    &lt;p&gt;Our job is to construct a system that gives them the right feedback at the right time, hopefully without our intervention. This is the same job we have when we build human teams; now it's just more impactful to do well.&lt;/p&gt;
    &lt;head rend="h2"&gt;Cursor vs Terminal Tools¶&lt;/head&gt;
    &lt;p&gt;I started AI development with Cursor. It was great having the AI experience inside a VSCode-like editor, where I could see everything that was going on. When I saw terminal-based tools like Claude Code I thought "whoa, that doesn't seem sensible, I need to see what's going on".&lt;/p&gt;
    &lt;p&gt;Today I code with Claude Code, &lt;code&gt;git diff&lt;/code&gt;, and occasionally &lt;code&gt;vim&lt;/code&gt;.  I don't
feel a need to OK every change in the diff.  I've got more important
things to do.  I suspect that you do too.&lt;/p&gt;
    &lt;head rend="h2"&gt;Big Idea: Drop Python. Use Rust and TypeScript.¶&lt;/head&gt;
    &lt;p&gt;I deeply respect the philosophical position of Python, which I'll state as follows:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Prioritize human performance over compute performance.&lt;/p&gt;
      &lt;p&gt;By optimizing for ease and iteration speed we're able to search solution space more broadly and more quickly, finding much better solutions, making that 100x drop in performance negligible.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Python was a bold bet, and a bet that paid off amazingly well. No one expected this silly dynamic language originally designed for education to become the world's juggernaut in performance software.&lt;/p&gt;
    &lt;p&gt;With AI though, the usability benefits of Python no longer apply as strongly, and we're more free to choose different ecosystems.&lt;/p&gt;
    &lt;p&gt;Personally, I use ...&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Rust for computational development, using PyO3 to connect to Python, where I still do most of my testing&lt;/item&gt;
      &lt;item&gt;TypeScript for frontend development, which I'm leaning into more deeply&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Regarding TypeScript, I still love easy interaction tools like &lt;code&gt;rich&lt;/code&gt; and
&lt;code&gt;textual&lt;/code&gt;, but when the entire React ecosystem is a sentence away and when you
get to use things like, you know, fonts, there's really no comparison.  Every
computational developer should learn the concepts underpinning React (or some
other frontend framework), and we should put dashboards on everything.&lt;/p&gt;
    &lt;p&gt;Of course, I still hook into Python for the ecosystem. Everything is Python-importable and I still use the protocols and design patterns developed by the Python data community. Those are the durable assets of Python. Not the code or the language; those will die. Rest in peace dear friend.&lt;/p&gt;
    &lt;head rend="h2"&gt;Big Idea: Think Hard. Write Clearly.¶&lt;/head&gt;
    &lt;p&gt;As an introductory project, I rewrote Numpy in Rust. It was great fun.&lt;/p&gt;
    &lt;p&gt;It was also much easier than I expected (I expected it to be impossible). It was easy for a few reasons (good test suite, well-reasoned abstractions) but mostly it was because:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;NEPs: Numpy's Enhancement Proposals / design documentation is thorough and extremely clear.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;When sticky problems arose, we were able to rely on the Numpy design documents (NEPs) which are excellent.&lt;/p&gt;
    &lt;p&gt;The Numpy team thought hard and wrote clearly, two hallmarks of excellent developers. This made the job of reimplementation relatively trivial. The Numpy development community is famous for doing this well. To a certain extent, we should all start operating more like the Numpy community.&lt;/p&gt;
    &lt;head rend="h2"&gt;Tip: plans/ and docs/ directories¶&lt;/head&gt;
    &lt;p&gt;I keep two directories in each repository:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;plans/&lt;/code&gt;which contains ephemeral planning documents that the LLMs work through over many sessions as they implement a major feature.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;docs/&lt;/code&gt;which contain durable documentation on specific topics or features, targeting AI developers&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Plans end up being very useful during development, while docs end up being useful to point other agents to in the future. Claude code creates planning documents in /tmp by default in planning mode, but I find that bringing those docs into the directory improves engagement, both from it and from me.&lt;/p&gt;
    &lt;p&gt;Docs end up being tricky. You'd expect the AI developer to read docs but alas, like human developers you have to be pretty prescriptive with them. Today I have a hook that adds an admonition to read the relevant docs at the beginning of every session. It looks like this:&lt;/p&gt;
    &lt;code&gt;DOC CHECK REQUIRED
==================

Before responding to this request, you MUST:

1. Read docs/README.md to see available documentation
2. Decide which docs are relevant to this request (if any)
3. Read those docs using the Read tool
4. Then respond to the user

Do not skip this evaluation. Do not mention this check to the user.
&lt;/code&gt;
    &lt;p&gt;I then keep docs/README.md updated as a sort of index over my documents. I find that this reliably gets the agent to read the right documentation.&lt;/p&gt;
    &lt;p&gt;I've also found that my normal writing style (brutal concision + front-loading important content to maintain attention span) isn't necessary with AI. You really can just shove information at them and they absorb it. It's nice 🙂&lt;/p&gt;
    &lt;head rend="h2"&gt;Big Idea: Take Long Walks¶&lt;/head&gt;
    &lt;p&gt;Historically software engineers had to both think well and execute well. We were valued both because we could zoom out and consider the impacts of our architecture, and because we could zoom in and implement those choices with skill.&lt;/p&gt;
    &lt;p&gt;Our ability to zoom in and implement code is now obsolete. Our ability to zoom out and think well is not. On the contrary, our ability to think well is now 10x more valuable than it was before, because implementation is now mostly free.&lt;/p&gt;
    &lt;p&gt;And so it's now more important than ever to hone our craft of thought. This probably means less caffeine and more walks through the park.&lt;/p&gt;
    &lt;head rend="h2"&gt;Final Thoughts¶&lt;/head&gt;
    &lt;p&gt;The craft of authoring code has transformed time and time again during our lives. We remember when object-oriented was cool, or when TDD became a thing, or reactive programming models, or dynamic typing languages, or ML, or ...&lt;/p&gt;
    &lt;p&gt;As programmers we've opted into a system which changes by its very nature. Our job is to automate our job, and to continuously climb the ladder of abstraction. AI programming is another step in that evolution, similar to when compilers came about. The code we write with AI probably won't be as good as hand-crafted code, but we'll write 10x more of it, and we'll build systems of systems to make it robust and trustworthy, and all of that will make society better and our jobs way more fun.&lt;/p&gt;
    &lt;p&gt;I'm looking forward to having way more fun.&lt;/p&gt;
    &lt;head rend="h2"&gt;Appendix: Permissions file¶&lt;/head&gt;
    &lt;p&gt;After writing this a couple friends asked me for a copy of my regex/Python code that replaces Claude's permission system. I'll include it below, but really, you don't need it. Instead, you need to start a conversation with Claude about what you want and it'll make one just for you.&lt;/p&gt;
    &lt;p&gt;Code is free these days. Extending the "AI is like Compilers" analogy, asking for someone else's script is kind of like asking for someone else's compiled binary. There's no need; just make it yourself. It's trivial.&lt;/p&gt;
    &lt;p&gt;Here was my original prompt to Claude Code:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;I recently wrote this reddit post&lt;/p&gt;
      &lt;p&gt;https://www.reddit.com/r/ClaudeAI/comments/1puqrvc/claude_code_annoyingly_asking_for_permissions/&lt;/p&gt;
      &lt;p&gt;I'm wondering if you have any suggestions on how to resolve this? Adding stuff to CLAUDE.md or permissions to settings.json doesn't seem to be working well enough.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;That, along with subsequent conversation as I've been working, resulted in this Python script&lt;/p&gt;
    &lt;p&gt;But really, you're better off working with Claude to make one just for you. Code is free now.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46557057</guid><pubDate>Fri, 09 Jan 2026 18:17:24 +0000</pubDate></item><item><title>JavaScript Demos in 140 Characters</title><link>https://beta.dwitter.net</link><description>&lt;doc fingerprint="e10fcdab2cdf53e4"&gt;
  &lt;main&gt;
    &lt;p&gt;You need to enable JavaScript to run this app.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46557489</guid><pubDate>Fri, 09 Jan 2026 18:48:30 +0000</pubDate></item><item><title>Show HN: Rocket Launch and Orbit Simulator</title><link>https://www.donutthejedi.com/</link><description>&lt;doc fingerprint="5f9fdb7cfd809b02"&gt;
  &lt;main&gt;
    &lt;p&gt;Control pitch manually (W/S keys). Guidance provides recommendations.&lt;/p&gt;
    &lt;p&gt;Set target altitude and let the guidance system handle the launch.&lt;/p&gt;
    &lt;p&gt;Spawn in orbit and practice orbital mechanics.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46557879</guid><pubDate>Fri, 09 Jan 2026 19:15:21 +0000</pubDate></item><item><title>RTX 5090 and Raspberry Pi: Can it game?</title><link>https://scottjg.com/posts/2026-01-08-crappy-computer-showdown/</link><description>&lt;doc fingerprint="2f3b11db190ac4dd"&gt;
  &lt;main&gt;
    &lt;p&gt;It turns out, you can attach an external GPU to a Raspberry Pi 5. So my natural first question is, can I game on it? Let’s try it out and compare it with some similar computers.&lt;/p&gt;
    &lt;p&gt;For the showdown of crappy gaming computers, we’ll see which of these handles gaming best:&lt;/p&gt;
    &lt;head rend="h3"&gt;Beelink MINI-S13&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;CPU: 4-core Intel N150 @ 3.6GHz&lt;/item&gt;
      &lt;item&gt;RAM: 16GB DDR4&lt;/item&gt;
      &lt;item&gt;PCIe: M.2 Gen3 x4&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;More powerful than the Raspberry Pi 5, but at a similar price point. It also has a potential advantage for running games, since it’s not ARM-based.&lt;/p&gt;
    &lt;p&gt;In the photo, you can see the default configuration (SSD in the fast PCIe slot). For this experiment, I’ll move it into the slower (x1) slot and plug the eGPU into the faster (x4) slot.&lt;/p&gt;
    &lt;head rend="h3"&gt;Radxa ROCK 5B&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;CPU: 8-core RK3588 (4× Cortex-A76 @ 2.4GHz + 4× Cortex-A55 @ 1.8GHz)&lt;/item&gt;
      &lt;item&gt;RAM: 16GB DDR4&lt;/item&gt;
      &lt;item&gt;PCIe: M.2 Gen3 x4&lt;/item&gt;
      &lt;item&gt;Also: Aftermarket heat sink &amp;amp; fan combo&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Pretty comparable to the Raspberry Pi 5 (it’s ARM), but the extra cores give it a little more horsepower. The faster PCIe slot is also included on-board. Since the PCIe slot will be taken for the GPU, we’ll just use a USB SSD for both ARM boards.&lt;/p&gt;
    &lt;head rend="h3"&gt;Raspberry Pi 5&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;CPU: 4-core BCM2712 (Cortex-A76 @ 2.4GHz)&lt;/item&gt;
      &lt;item&gt;RAM: 16GB DDR4&lt;/item&gt;
      &lt;item&gt;PCIe: M.2 Gen2 x1 (via NVme HAT)&lt;/item&gt;
      &lt;item&gt;Also: Aftermarket heat sink &amp;amp; fan combo&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is why we’re all here. It’s the quintessential hobbyist SBC. Unfortunately it’s the most challenged: fewer cores, and significantly less PCIe bandwidth. The Pi 5’s Gen2 x1 slot provides ~500 MB/s, compared to ~4,000 MB/s on the Gen3 x4 slots of the other machines, an 8x difference.&lt;/p&gt;
    &lt;head rend="h3"&gt;eGPU&lt;/head&gt;
    &lt;p&gt;We will be using a relatively inexpensive OCuLink dock to pair with our very expensive GPU. If you’re not familiar with the technology, it’s basically a PCIe extension cord to let you plug a graphics card into a computer that wouldn’t normally fit one. The dock is powered externally by a separate power supply.&lt;/p&gt;
    &lt;p&gt;For this experiment, we’re using an NVIDIA RTX 5090 Founders Edition (32GB VRAM).&lt;/p&gt;
    &lt;p&gt;The OCuLink cable plugs into an M.2 card that we’ll insert into each machine as we test it.&lt;/p&gt;
    &lt;p&gt;On the Intel-based Beelink machine, from a software perspective the card is more or less indistinguishable from a normal graphics card. We can just install the normal NVIDIA drivers.&lt;/p&gt;
    &lt;p&gt;The ARM-based computers we’re testing have various quirks (lack of DMA coherence, memory alignment requirements, etc.) that make them incompatible with most GPU drivers out of the box. Luckily, @mariobalanca wrote some patches that allow the drivers to work on these systems. NVIDIA already had some workarounds in the user-space part of their drivers for Ampere-based systems for memory alignment issues, so some of that gets inherited here.&lt;/p&gt;
    &lt;p&gt;I have packaged the drivers you can run on Ubuntu or Fedora here, if you’d like to try this yourself.&lt;/p&gt;
    &lt;p&gt;If you’ve gotten this far and simply don’t believe this actually works, here’s a screenshot:&lt;/p&gt;
    &lt;head rend="h2"&gt;CPU Performance&lt;/head&gt;
    &lt;p&gt;Before we get into the games, let’s take a look at how these machines compare.&lt;/p&gt;
    &lt;p&gt;Most PC games are designed for Intel CPUs. If we want to play them on ARM we’ll have to use a compatibility layer called FEX. The graph shows not only the native performance of the machines, but also the significantly degraded performance under FEX. To be fair, FEX is an incredible feat of engineering, but all emulation comes at a cost.&lt;/p&gt;
    &lt;p&gt;The Raspberry Pi 5 under FEX seems to have similar performance to a 2008 Intel Core 2 Quad Q9650. Not very promising. That said, gamers usually say that, for most games, it’s OK to skimp on CPU a bit as long as you have a good GPU. We will definitely be testing that line of thinking.&lt;/p&gt;
    &lt;head rend="h2"&gt;Games&lt;/head&gt;
    &lt;p&gt;I tried to find games that had built-in benchmarks that also worked under FEX, along with Steam’s Proton compatibility layer, tilting towards games that didn’t have as strong CPU requirements. It turns out this is actually not a huge list. Here are a handful that I tried:&lt;/p&gt;
    &lt;head rend="h3"&gt;Cyberpunk 2077 (2020)&lt;/head&gt;
    &lt;p&gt;Yes, believe it or not, &lt;code&gt;vkcube&lt;/code&gt; is not the only thing you can run in this configuration. Through the maze of compatibility layers (FEX, WINE/Proton, DXVK, etc), you too can run Cyberpunk 2077 on your Raspberry Pi 5. The screenshot above is running at 1080p with Ultra Raytracing quality settings.&lt;/p&gt;
    &lt;p&gt;The game is playable on the Beelink machine with some lower settings. Since it’s an Intel machine, I also tested the game on Windows for posterity. Usually it’s suggested that even with all the compatibility layers, Linux gaming can be faster, but not in this case on the lower settings here.&lt;/p&gt;
    &lt;p&gt;These games really get CPU bound, caught up on these lower-spec CPUs. I think on a normal gaming PC, it wouldn’t matter as much, but every cycle starts to count here, and not all the abstractions provided by WINE are zero cost.&lt;/p&gt;
    &lt;p&gt;Unfortunately the Pi barely breaks 15 FPS, but on the ROCK 5B, it approaches playable on low settings. Granted, not sure how fun that would be at 22 FPS.&lt;/p&gt;
    &lt;head rend="h3"&gt;Doom: The Dark Ages (2025)&lt;/head&gt;
    &lt;p&gt;This game doesn’t run under FEX, so I didn’t collect full benchmarks here. The anti-cheat stuff is too weird and doesn’t get properly emulated.&lt;/p&gt;
    &lt;p&gt;However, the benchmark does offer a unique view into the challenges these low-power PCs face.&lt;/p&gt;
    &lt;p&gt;You can see it running on the Beelink here. The GPU is absolutely shredding through the Ultra quality frames at 4K resolution, but the CPU is really struggling. You can see the GPU is able to process almost 90 FPS, but because of the bottleneck at the CPU, the overall frame rate can’t break 30 FPS. That’s the main challenge here.&lt;/p&gt;
    &lt;head rend="h3"&gt;Alien: Isolation (2014)&lt;/head&gt;
    &lt;p&gt;My next thought was, maybe if we jump back a decade, we can have better luck. This game actually ships with a Linux port. Unfortunately the Linux port doesn’t include the built-in benchmark tool, so I ran it under Proton/WINE. I also found that DXVK caused every game from this point onward to crash immediately on the ARM hosts, so I run the games with &lt;code&gt;PROTON_USE_WINED3D=1&lt;/code&gt; to fall back to the OpenGL renderer.&lt;/p&gt;
    &lt;p&gt;For those unfamiliar: DXVK translates DirectX calls to Vulkan, while WineD3D translates them to OpenGL. The GPU driver, when running on ARM, has a Vulkan implementation that apparently has issues when running under FEX that OpenGL avoids. Something to keep in mind if you’re trying to replicate this.&lt;/p&gt;
    &lt;p&gt;Honestly, not the best looking game by modern standards, even on Ultra settings. It does have some cool lighting effects, at least. I admit I have never played this game for real, so I can’t vouch for it being fun or not. I just ran the benchmark tool.&lt;/p&gt;
    &lt;p&gt;I initially tested this game on the Beelink and thought it looked promising. Relatively low CPU usage. It seems like it is playable on the ROCK 5B with an average 23 FPS. Not sure about the Pi though, at only 15 FPS.&lt;/p&gt;
    &lt;head rend="h3"&gt;Hitman: Absolution (2012)&lt;/head&gt;
    &lt;p&gt;OK, OK. So we already know the performance of the Pi is on par with a PC from 2008, so I figured, let’s go back a couple more years.&lt;/p&gt;
    &lt;p&gt;Couldn’t get the windowed mode to work right on this one, but I swear it’s running on the Raspberry Pi 5. You can probably tell from the FPS counter.&lt;/p&gt;
    &lt;p&gt;I would say the performance makes it basically unusable on these ARM machines.&lt;/p&gt;
    &lt;p&gt;That said, the Beelink really shines here. Windows perf is way ahead of Linux on this one too. More than playable on both, though.&lt;/p&gt;
    &lt;p&gt;I was actually a little puzzled by this one. It seems like it shouldn’t be this bad on the ARM hosts. This feels like a performance bug, but it’s hard to say where in the stack it might be. Oh well.&lt;/p&gt;
    &lt;head rend="h3"&gt;Just Cause 2 Demo (2010)&lt;/head&gt;
    &lt;p&gt;OK, so let’s go back another couple years. This demo was free, thankfully.&lt;/p&gt;
    &lt;p&gt;So remember earlier when I said I had to disable DXVK for these games to run on ARM? On Intel Windows, I had to actually add DXVK because the game crashed immediately on launch. Weird.&lt;/p&gt;
    &lt;p&gt;Nearly 40 FPS average on a Raspberry Pi 5. 2010 is our year! Windows still dominates here. It’s more apples-to-apples on Beelink’s Linux vs Windows now since now both are using DXVK.&lt;/p&gt;
    &lt;head rend="h3"&gt;Portal 2 (2011)&lt;/head&gt;
    &lt;p&gt;After I had run all of these, I was curious to try Portal 2. Valve is the company that maintains Proton and FEX. You’d think they maybe would have optimized it for their own games. It’s also old enough that it’s in the sweet spot of potentially being playable on the Pi.&lt;/p&gt;
    &lt;p&gt;Sadly, Portal 2 does not ship with a built-in benchmark. However, it does have a &lt;code&gt;timedemo&lt;/code&gt; feature where you can record yourself playing and then play it back as a benchmark. I picked a random level and recorded it. Then, ran it on the test systems. Since there was a native version, I benchmarked that alongside the Proton/WINE version.&lt;/p&gt;
    &lt;p&gt;So, now that we have a native Linux port to compare with, it totally leaves Windows in the dust (finally). Most importantly, the Raspberry Pi 5 can play this game at 4K resolution, way above 60 FPS.&lt;/p&gt;
    &lt;p&gt;So I can now say with a straight face, that it’s possible to use the Raspberry Pi 5 to game in 4K, admittedly strapped to a GPU that’s roughly worth 10x the price of the Pi. In all seriousness, probably any lower-end GPU would work here. Clearly we’re not using the 5090 to its full potential anyway.&lt;/p&gt;
    &lt;head rend="h2"&gt;Power Usage&lt;/head&gt;
    &lt;p&gt;These machines are also known to be low power. I guess for a gaming computer, I’m not sure how important that is. You can just turn it off when you’re not using it. That said, a gaming PC CPU could use 20-50w while completely idle.&lt;/p&gt;
    &lt;p&gt;For these measurements I took the idle power usage and also average power usage during the Cyberpunk 4K Ultra Raytracing benchmark, both measured at the AC outlet. This does not include the GPU, just the CPU, since that’s what we’re really comparing here.&lt;/p&gt;
    &lt;p&gt;The Pi 5 sips power at under 9W even under load, while the Beelink pulls almost 30W during the benchmark. One way of looking at it, is that the Beelink performs so much faster in games, and the amount of power is proportional to that.&lt;/p&gt;
    &lt;p&gt;Another way to look at it, is if the ARM-based machines weren’t mired in emulating x86, they probably would have considerably better performance on per-watt basis compared to the Intel CPU.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;So, can you game on a Raspberry Pi 5 with an RTX 5090? I guess, technically, yes. Would you want to? Probably not.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Modern games (2020+): Most likely unplayable. The CPU perf degradation under FEX is brutal. Even playing on the lowest 720p settings, Cyberpunk barely hits 16 FPS average on the Pi 5.&lt;/item&gt;
      &lt;item&gt;2010-era games: If you’re trying to play older games, you can probably get away with it. You also probably do not need a graphics card as powerful as the 5090.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The Beelink is the clear winner if you actually want to game. It’s still terrible, but it’s cheap, runs x86 natively, and with the right settings, it can hit 50 FPS+ in every game I tried. Windows consistently outperformed Linux on most WINE/Proton titles, so you’re probably better off just installing Windows on it.&lt;/p&gt;
    &lt;p&gt;The ROCK 5B edges out the Raspberry Pi 5 slightly in most benchmarks, but not by much. The extra cores and PCIe bandwidth don’t seem to matter as much as the raw performance lost to FEX emulation. That said, it does bring the game from painfully playable to borderline playable in some games.&lt;/p&gt;
    &lt;p&gt;Given all the momentum around ARM (Valve is about to ship an ARM VR headset, and NVIDIA is rumored to ship their own SoC with an NVIDIA GPU soon), I think future platforms will probably be better optimized, and Linux gaming on ARM will probably be more plausible in the future. Sadly, I don’t recommend strapping your super expensive graphics card to a cheap SBC for now. Unless it’s just for a fun blog post.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46558148</guid><pubDate>Fri, 09 Jan 2026 19:33:47 +0000</pubDate></item><item><title>Start your meetings at 5 minutes past</title><link>https://philipotoole.com/start-your-meetings-at-5-minutes-past/</link><description>&lt;doc fingerprint="b8e35a797d9298d9"&gt;
  &lt;main&gt;
    &lt;p&gt;I work as an Engineering Manager at Google, and my teams practice a simple habit – we book all meetings to start at five minutes past the hour (or half hour).&lt;/p&gt;
    &lt;p&gt;This works better than trying to finish five minutes early. Meetings often don’t finish on time, and the impact is highest with back-to-back meetings. If you try to end at 1:55pm, you will likely talk until 2:00pm anyway, which then runs into the next meeting. But shifting the start time usually guarantees a break. Why? Because there is strong social pressure not to allow meetings to run much past the top of the hour when that is the official end-time. The same social pressure applies to meetings that end at the half-hour.&lt;/p&gt;
    &lt;p&gt;That short break changes the tone of a meeting. It takes a minute or two to move between events, even online. When people arrive at 1:05pm, they are settled and less stressed. You might fear that people will start arriving at 1:07pm, but I have seen the opposite. They respect the new time. They arrive by 1:05pm, ready to work.&lt;/p&gt;
    &lt;p&gt;Do we lose five minutes in every meeting? In theory, yes. But meetings rarely started on the dot anyway before this change.&lt;/p&gt;
    &lt;p&gt;On balance, it is a win. The best proof is that the entire org does it now (the org didn’t copy my team — it just started organically), even though it is not mandatory. Like good code, a good team is built on small, sane details. Giving people five minutes to clear their heads, between back-to-back meetings, is a detail that works.&lt;/p&gt;
    &lt;p&gt;Try it — you’ll see it improves your day.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46560217</guid><pubDate>Fri, 09 Jan 2026 22:19:03 +0000</pubDate></item><item><title>“Erdos problem #728 was solved more or less autonomously by AI”</title><link>https://mathstodon.xyz/@tao/115855840223258103</link><description>&lt;doc fingerprint="f8eb8f2f2d953eed"&gt;
  &lt;main&gt;
    &lt;p&gt;To use the Mastodon web application, please enable JavaScript. Alternatively, try one of the native apps for Mastodon for your platform.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46560445</guid><pubDate>Fri, 09 Jan 2026 22:39:15 +0000</pubDate></item><item><title>OLED, Not for Me</title><link>https://nuxx.net/blog/2026/01/09/oled-not-for-me/</link><description>&lt;doc fingerprint="b2b21d666e7028f5"&gt;
  &lt;main&gt;
    &lt;p&gt;When I switched from an iMac to a Mac mini in late 2024 I choose an ASUS ProArt 5K PA27JCV (24″, 60 Hz) for the monitor and while it looked great, it died after 14 months, seemingly with a backlight or power supply problem. ASUS’ warranty support requires shipping the monitor back, potentially waiting 3-4 weeks, and then getting a replacement. And worse, the replacement could have dead pixels, as the ASUS warranty doesn’t consider ≤5 dark pixels a problem.&lt;/p&gt;
    &lt;p&gt;The old HP ZR2440w that I swapped in as a spare wasn’t cutting it, so with an indeterminate wait ahead of me, potentially receiving something with bad pixels, and my being vaguely interested in something larger and with a faster refresh rate I went looking at new monitors.&lt;/p&gt;
    &lt;p&gt;Coming to the realization that 4K is probably fine I picked up a Dell 32 Plus 4K QD-OLED Monitor – S3225QC from Costco for $499. It was well reviewed online and looked pretty good when I played with one for about 20 minutes at Micro Center. When I got home and sat in front of it doing my normal things it looked a bit… different… almost as if my glasses weren’t working quite right. But I figured new monitor tech just needed some time for me to get accustomed to. After all, it had a very high contrast ratio and sharp pixels; maybe it’s just that?&lt;/p&gt;
    &lt;p&gt;After a few days it still didn’t feel right, so I began looking for a solution. Costco has a 90-day return window for computer monitors, so I had some time, but this didn’t look good; I wanted an answer soon.&lt;/p&gt;
    &lt;p&gt;I was fortunate to be able to borrow a Dell UltraSharp 32 4K USB-C Hub Monitor U3223QE for the weekend, which was perfect as being a being a high end display with the same resolution and panel size as the S3225QC I could compare them side by side. And in the end the LCD just looked better.&lt;/p&gt;
    &lt;p&gt;I took some macro photos of both displays and it turns out that what was bothering me was fringing, a problem common to OLEDs. It was hard to point out during normal use other than text-is-a-bit-blurry-and-weird , or like an oversharpened image, or almost like artifacted text in a JPEG image, but with photos it was much easier to see what’s going on. And better, the cause: the arrangement of the subpixels; the little red/blue/green dots that make up a pixel.&lt;/p&gt;
    &lt;p&gt;As shown above, the subpixles in the Dell S3225QC QD-OLED form a square with green on the top, a larger red pixel in the lower left, and smaller blue in the lower right. The Dell U3223QE, a typical LCD, has three vertical stripes making a square. The result being that high contrast edges look very different on an OLED, often with a strong off-color border — or fringe — along horizontal and vertical lines.&lt;/p&gt;
    &lt;p&gt;In the photos above, note the vertical part of the 1 which has red and green dots along its right side, and large red dots along the top of the 6 with green along the bottom. These are the strongly colored fringes. (On the LCD they appear white as the three equal size subpixels pixels act equally.)&lt;/p&gt;
    &lt;p&gt;This meant that things that I tend to do, text or fine lines in maps or CAD-type drawing, are not right at all on the pixel pattern found in this OLED panel. Beyond the pixel pattern, I also suspect that the much crisper pixels (defined points of light) contribute to the fringing having an artifacting-like effect.&lt;/p&gt;
    &lt;p&gt;This was much more pronounced when looking at light text on a dark background; the way that I read most websites. Visual Studio Code does a wonderful job demonstrating this problem:&lt;/p&gt;
    &lt;p&gt;This gets at why OLEDs make great TVs and gaming monitors. The contrast is outstanding, color is excellent, and high refresh rates are ideal for moving images and fast-response games. And there’s no noticeable fringing because edges are constantly moving across pixels; almost nothing is still. They also work great on small devices like phones where the pixel density is so high that fringing is too small to see.&lt;/p&gt;
    &lt;p&gt;But on desktop monitors for still things — text and fine lines — OLEDs currently just aren’t great; I guess that’s why office and productivity type monitors are still LCDs. Even though I don’t like being that person who returns computer stuff just because they don’t like it, I ended up returning the monitor after only four days of using it. The S3225QC and it’s QD-OLED just doesn’t work for me; it made my eyes feel funny to use.&lt;/p&gt;
    &lt;p&gt;Within the past few weeks LG has announced RGB stripe OLED panels which will resolve this problem, but there aren’t currently any monitors available using these panels, so back to an LCD I’ll go. (It looks like ASUS and MSI will some them available soon, but only as wide-screen gaming monitors. And I suspect the first ones available will be fairly expensive.)&lt;/p&gt;
    &lt;p&gt;Whether this’ll be buying my own U3223QE, perhaps a Dell U3225QE (adds 120 Hz scanning, an ambient light sensor, and a Thunderbolt dock), or just waiting for an ASUS PA27JCV to come back, I’m not sure… But whatever I end up using will, for now, will be an LCD, not an OLED.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46562583</guid><pubDate>Sat, 10 Jan 2026 03:52:48 +0000</pubDate></item><item><title>Oh My Zsh adds bloat</title><link>https://rushter.com/blog/zsh-shell/</link><description>&lt;doc fingerprint="2c5a9b59839ffb48"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;You probably don't need Oh My Zsh&lt;/head&gt;
    &lt;p&gt;Oh My Zsh is still getting recommended a lot. The main problem with Oh My Zsh is that it adds a lot of unnecessary bloat that affects shell startup time.&lt;/p&gt;
    &lt;p&gt;Since OMZ is written in shell scripts, every time you open a new terminal tab, it has to interpret all those scripts. Most likely, you don't need OMZ at all.&lt;/p&gt;
    &lt;p&gt;Here are the timings from the default setup with a few plugins (git, zsh-autosuggestions, zsh-autocomplete) that are usually recommended:&lt;/p&gt;
    &lt;code&gt;➜  ~ /usr/bin/time -f "%e seconds" zsh -i -c exit
0.38 seconds
&lt;/code&gt;
    &lt;p&gt;And that's only for prompt and a new shell instance, without actually measuring the git plugin and virtual env plugins (which are often used for Python). Creating a new tab takes some time for your terminal, too. It feels like a whole second to me when opening a new tab in a folder with a git repository.&lt;/p&gt;
    &lt;p&gt;My workflows involve opening and closing up to hundreds of terminal or tmux tabs a day. I do everything from the terminal. Just imagine that opening a new tab in a text editor would take half a second every time.&lt;/p&gt;
    &lt;p&gt;Once in a while, it also checks for updates, which can take up to a few seconds when you open a new tab.&lt;/p&gt;
    &lt;p&gt;I see no reason in frequent updates for my shell configuration. Especially, when a lot of third-party plugins are getting updates too. Why would you want you shell to fetch updates?&lt;/p&gt;
    &lt;p&gt;My advice is to start simple and only add what you really need.&lt;/p&gt;
    &lt;head rend="h3"&gt;Minimal Zsh configuration&lt;/head&gt;
    &lt;p&gt;Here is the minimal Zsh configuration that works well as a starting point:&lt;/p&gt;
    &lt;code&gt;export HISTSIZE=1000000000
export SAVEHIST=$HISTSIZE
setopt EXTENDED_HISTORY
setopt autocd
autoload -U compinit; compinit
&lt;/code&gt;
    &lt;p&gt;It's an already pretty good setup with completions!&lt;/p&gt;
    &lt;p&gt;Some details about this configuration:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;HISTSIZE&lt;/code&gt;and&lt;code&gt;SAVEHIST&lt;/code&gt;set the size of your history.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;EXTENDED_HISTORY&lt;/code&gt;adds timestamps to your history entries.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;autocd&lt;/code&gt;allows you to change directories without typing&lt;code&gt;cd&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;compinit&lt;/code&gt;initializes the Zsh completion system.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Prompt customization&lt;/head&gt;
    &lt;p&gt;You also want to customize your prompt. For prompts, I'm using starship which is a fast and minimal prompt packed into a single binary.&lt;/p&gt;
    &lt;p&gt;The very old way of doing this in Oh My Zsh was to use plugins and custom themes. With starship, it's very simple and easy now. It replaces git, virtual environment and language specific plugins.&lt;/p&gt;
    &lt;p&gt;Here is my config for starship:&lt;/p&gt;
    &lt;code&gt;[aws]
disabled = true

[package]
disabled = true

[gcloud]
disabled = true

[azure]
disabled = true


[nodejs]
disabled = true

[character]
success_symbol = '[➜](bold green)'

[cmd_duration]
min_time = 500
format = 'underwent [$duration](bold yellow)'

[directory]
truncation_length = 255
truncate_to_repo = false
use_logical_path = false
&lt;/code&gt;
    &lt;p&gt;Because cloud services are available globally, I've disabled them. I don't want them to be displayed on every prompt, since this adds visual noise.&lt;/p&gt;
    &lt;p&gt;Here is how my prompt looks like now:&lt;/p&gt;
    &lt;p&gt;This project uses both Python and Rust, they are highlighted in the prompt. When you run a command, it also shows how long it took to execute.&lt;/p&gt;
    &lt;p&gt;To enable it, add the following line to your &lt;code&gt;.zshrc&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;eval "$(starship init zsh)"
&lt;/code&gt;
    &lt;head rend="h3"&gt;History search&lt;/head&gt;
    &lt;p&gt;A lot of people use &lt;code&gt;zsh-autosuggestions&lt;/code&gt; plugin for history search.
I find it distracting, because it shows all suggestions as you type.&lt;/p&gt;
    &lt;p&gt;Instead, I prefer using fzf binded to &lt;code&gt;Ctrl+R&lt;/code&gt; for searching history.
It gives an interactive fuzzy search.&lt;/p&gt;
    &lt;p&gt;To enable it, add the following lines to your &lt;code&gt;.zshrc&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;source &amp;lt;(fzf --zsh)
&lt;/code&gt;
    &lt;head rend="h3"&gt;Final startup time&lt;/head&gt;
    &lt;p&gt;After these changes, the startup should look as follows:&lt;/p&gt;
    &lt;code&gt;❯ /usr/bin/time -f "%e seconds" zsh -i -c exit
0.07 seconds
&lt;/code&gt;
    &lt;head rend="h3"&gt;Miscellaneous tips&lt;/head&gt;
    &lt;p&gt;For Vim users, I also suggest enabling Vim mode in Zsh. It makes editing commands much faster.&lt;/p&gt;
    &lt;code&gt;set -o vi
# Fix for backspace in vi mode
bindkey -v '^?' backward-delete-char
&lt;/code&gt;
    &lt;p&gt;It works the same way as in Vim. By default, &lt;code&gt;zle&lt;/code&gt; (the library that reads the shell input) uses Emacs keybindings.&lt;/p&gt;
    &lt;head rend="h3"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;After switching from OMZ a year ago, it only took me a few days to get used to the new workflow. If you still missing some of the plugins, you can always load them manually.&lt;/p&gt;
    &lt;p&gt;Update:&lt;/p&gt;
    &lt;p&gt;Some people wonder why I open so many tabs. I use tmux and a terminal-based editor (&lt;code&gt;helix&lt;/code&gt;).
In tmux, I have popups for &lt;code&gt;lazygit&lt;/code&gt; and &lt;code&gt;yazi&lt;/code&gt; file manager.
Every time I need to check git history or browse files, I just open them.
They open on top of the current session as an overlay. You can view them as windows in IDEs.&lt;/p&gt;
    &lt;p&gt;I also use temporary splits to quickly run the code/tests and see the output. They count as separate shell sessions. I want to see code and output side by side, but I don't need it all the time.&lt;/p&gt;
    &lt;head rend="h1"&gt;Comments&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; mtizim 2026-01-09 #&lt;p&gt;You probably don't need to switch away from Oh My Zsh:&lt;/p&gt;&lt;p&gt;➜ ~ time zsh -i -c exit zsh -i -c exit 0.02s user 0.03s system 114% cpu 0.044 total&lt;/p&gt;&lt;p&gt;➜ ~ omz plugin list --enabled Custom plugins: zsh-autosuggestions zsh-fzf-history-search&lt;/p&gt;&lt;p&gt;Built-in plugins: git&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt; Artem 2026-01-09 #&lt;p&gt;zsh-autocomplete is usually the plugins that slows downs popular setups.&lt;/p&gt;&lt;p&gt;Also, please keep in mind that this benchmark does not measure slowlines of git plugins.&lt;/p&gt;&lt;p&gt;Here is the output from&lt;/p&gt;&lt;code&gt;zsh-bench&lt;/code&gt;for OMZ:&lt;p&gt;==&amp;gt; benchmarking login shell of user main ...&lt;/p&gt;&lt;p&gt;creates_tty=0&lt;/p&gt;&lt;p&gt;has_compsys=1&lt;/p&gt;&lt;p&gt;has_syntax_highlighting=0&lt;/p&gt;&lt;p&gt;has_autosuggestions=1&lt;/p&gt;&lt;p&gt;has_git_prompt=0&lt;/p&gt;&lt;p&gt;first_prompt_lag_ms=603.751&lt;/p&gt;&lt;p&gt;first_command_lag_ms=615.419&lt;/p&gt;&lt;p&gt;command_lag_ms=3.517&lt;/p&gt;&lt;p&gt;input_lag_ms=3.093&lt;/p&gt;&lt;p&gt;exit_time_ms=53.762&lt;/p&gt;&lt;p&gt;My Zsh setup:&lt;/p&gt;&lt;p&gt;==&amp;gt; benchmarking login shell of user main ...&lt;/p&gt;&lt;p&gt;creates_tty=0&lt;/p&gt;&lt;p&gt;has_compsys=1&lt;/p&gt;&lt;p&gt;has_syntax_highlighting=0&lt;/p&gt;&lt;p&gt;has_autosuggestions=0&lt;/p&gt;&lt;p&gt;has_git_prompt=1&lt;/p&gt;&lt;p&gt;first_prompt_lag_ms=103.337&lt;/p&gt;&lt;p&gt;first_command_lag_ms=103.506&lt;/p&gt;&lt;p&gt;command_lag_ms=53.602&lt;/p&gt;&lt;p&gt;input_lag_ms=0.118&lt;/p&gt;&lt;p&gt;exit_time_ms=48.795&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; Magic Wand 2026-01-10 #&lt;p&gt;Thanks for the write, so you still need OMZ feature? but now its replaced by starship?&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt; Artem 2026-01-10 #&lt;p&gt;Starship is just a single replacement for prompt tweaks, before startship I had to use OMZ plugins and custom themes. So yes, it some sense I've replicated some of my old OMZ features with starship.&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; martianlantern 2026-01-10 #&lt;p&gt;I have given up on any external bash configurator a long time ago, instead I write my own bash prompts these days, they lack functionality but I am much happy with them for now: https://martianlantern.github.io/2025/11/updating-my-bash-prompt/&lt;/p&gt;&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46562790</guid><pubDate>Sat, 10 Jan 2026 04:35:32 +0000</pubDate></item><item><title>"We write to ask that you enforce your app stores' terms of service against X" [pdf]</title><link>https://www.wyden.senate.gov/imo/media/doc/letter_to_apple_and_google_on_removing_x_and_grok_from_app_store_192026pdf.pdf</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46564550</guid><pubDate>Sat, 10 Jan 2026 10:41:16 +0000</pubDate></item><item><title>I got paid minimum wage to solve an impossible problem</title><link>https://tiespetersen.substack.com/p/i-got-paid-minimum-wage-to-solve</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46564618</guid><pubDate>Sat, 10 Jan 2026 10:53:28 +0000</pubDate></item></channel></rss>