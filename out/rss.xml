<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Fri, 21 Nov 2025 16:12:41 +0000</lastBuildDate><item><title>Okta's NextJS-0auth troubles</title><link>https://joshua.hu/ai-slop-okta-nextjs-0auth-security-vulnerability</link><description>&lt;doc fingerprint="ae60955bb9606999"&gt;
  &lt;main&gt;
    &lt;p&gt;In October, I reported two security issues to Okta‚Äôs auth0/nextjs-auth0 project, here and here. The latter bug, an oauth parameter injection, allows for a range of types of abuse, like scoping tokens for unintended services, setting &lt;code&gt;redirect_uri&lt;/code&gt; and &lt;code&gt;scope&lt;/code&gt; to arbitrary values to leak tokens, and so on.&lt;/p&gt;
    &lt;p&gt;The patch was simple enough, so I opened a PR:&lt;/p&gt;
    &lt;code&gt;diff --git a/src/server/helpers/with-page-auth-required.ts b/src/server/helpers/with-page-auth-required.ts
index 41af2dfe..f07046b8 100644
--- a/src/server/helpers/with-page-auth-required.ts
+++ b/src/server/helpers/with-page-auth-required.ts
@@ -196,7 +196,7 @@ export const appRouteHandlerFactory =
           : opts.returnTo;
       const { redirect } = await import("next/navigation.js");
       redirect(
-        `${config.loginUrl}${opts.returnTo ? `?returnTo=${returnTo}` : ""}`
+        `${config.loginUrl}${opts.returnTo ? `?returnTo=${encodeURIComponent(returnTo)}` : ""}`
       );
     }
     return handler(params);
&lt;/code&gt;
    &lt;p&gt;All‚Äôs well that ends well, right? Obviously, no.&lt;/p&gt;
    &lt;p&gt;The PR, 3 weeks later, was closed by the maintainer, an auth0 (an Okta company) employee, with the following comment:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;This change is superseded by #2413. This was done to ensure that commits are signed. Orignal contribution history has been preserved. Hence closing this PR now.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Hmm, let‚Äôs take a look at that PR:&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;auth0/nextjs-auth0 #2413&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Hmm. That patch looks familiar. And who is Simen Olsen?&lt;/p&gt;
    &lt;p&gt;Pushing back on the attribution error, I replied:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;history has been preserved&lt;/p&gt;
      &lt;p&gt;no it hasn‚Äôt. I don‚Äôt know who ‚ÄúSimen A. W. Olsen my@simen.io‚Äù is but it isn‚Äôt me and my commit here doesn‚Äôt reference that name or email address at all. Was it ai generated or something?&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Of course, the answer was: yes. It was AI slop. Just like my previous post about gixy-ng (a fun read for anybody dealing with nginx), the developer had used CoPilot to somebow generate their patches:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Hi @MegaManSec I sincerely apologize for this attribution error.&lt;/p&gt;
      &lt;p&gt;Can confirm that an AI workflow was used to created the rebased commit, which got confused with OP details. I‚Äôve added a correction to #2413, and will ensure the changelog is updated.&lt;/p&gt;
      &lt;p&gt;Thank you for calling this out, we‚Äôll make sure this doesn‚Äôt happen again.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Not only did the maintainer state the above, they also used AI to generate the response! In a now-deleted comment, they clearly used some AI to respond to my complaint:&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;auth0/nextjs-auth0 #2413‚Äôs now-deleted comment&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;With the classic ChatGPT ‚Äúyou are absolutely correct‚Äù, it‚Äôs pretty frustrating that this developer used AI to:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Take my report/PR and commit it themselves.&lt;/item&gt;
      &lt;item&gt;Used AI to commit it, removing my attribution.&lt;/item&gt;
      &lt;item&gt;Used AI to ‚Äúapologise‚Äù for using AI, then stated that ‚Äúit won‚Äôt happen again‚Äù ‚Äì (yeah right; please provide a detailed explanation how you‚Äôre going to ensure that, when clearly a 1-line code change is too much for your AI to handle without breaking).&lt;/item&gt;
      &lt;item&gt;Refused to fix the commit to remove the invalid / AI-generated-slop details, and add back mine.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Indeed, asking:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;I would appreciate force-pushing a fix for the commit to properly include my information in the commit.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I was told that they cannot change it. That seems like a copyright infringement to me: taking somebody else‚Äôs code, then changing the author‚Äôs name?&lt;/p&gt;
    &lt;p&gt;What I really find the most interesting is really how this AI slop even came to be. I cannot find any reference to the email address ‚Äúmy@simen.io‚Äù anywhere online. On GitHub, the only reference to this email address is from the nextjs-auth0 PR. Simen Olsen has never contributed to any of the nextjs-auth0 repositories as far as I can tell (searching &lt;code&gt;org:auth0 author:simenandre&lt;/code&gt; on GitHub), and that doesn‚Äôt even seem to be their real email address. so was this some type of ai hallucination? And why? The code change was tiny. I just totally don‚Äôt get it: I have literally never had any AI tooling fail like this and come up with some other person‚Äôs (fake) contact details. It‚Äôs simply absurd; are auth0‚Äôs engineers using some extremely (extremely) low quality local model or something? If ChatGPT failed like this for me even once every thousand times, I would simply never use it again.&lt;/p&gt;
    &lt;p&gt;In the end, at the time of writing this, the auth0/nextjs-auth0 maintainer, Tushar Pandey, who made all of these mistakes, has not fixed attribution mistake in the commit history. In addition to this, that first bug, which allows for arbitrary account hijacking in this software, has been fixed after 3 weeks, with new versions of the nextjs-auth0 software released, but Okta‚Äôs security people stating that ‚Äúunless you create a video abusing this vulnerability, we aren‚Äôt going to accept this as a security issue‚Äù ‚Äì LMAO; ‚Äúyeah, it‚Äôs a vulnerability, we fixed in the code, it can be used to takeover accounts, but you need to create a video‚Äù. Hilarious. That‚Äôs just another case to add to my list of hilarious problems related to reporting security issue, that my next post will document.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45963350</guid><pubDate>Tue, 18 Nov 2025 10:17:20 +0000</pubDate></item><item><title>Free interactive tool that shows you how PCIe lanes work on motherboards</title><link>https://mobomaps.com</link><description>&lt;doc fingerprint="35902abe01d162bd"&gt;
  &lt;main&gt;
    &lt;p&gt;MoboMaps YouTube $upport Magic Edition ‚ú® Boards Available 10 Generate ‚úì Live Filtering Active Reset Filters Socket AM5 Brand MSI ASUS Gigabyte ASRock Chipset X870E X870 B850 Form Factor E-ATX ATX mATX Mini-ITX Features WiFi USB4 No GPU Lane Sharing M.2 Slots Any PCIe Slots Any Price Any üåü Select Filters Choose your preferences above and click Generate to view boards ‚Üê Back to Boards&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45976693</guid><pubDate>Wed, 19 Nov 2025 07:13:00 +0000</pubDate></item><item><title>Adversarial poetry as a universal single-turn jailbreak mechanism in LLMs</title><link>https://arxiv.org/abs/2511.15304</link><description>&lt;doc fingerprint="dc9d341acdd39acb"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Computation and Language&lt;/head&gt;&lt;p&gt; [Submitted on 19 Nov 2025 (v1), last revised 20 Nov 2025 (this version, v2)]&lt;/p&gt;&lt;head rend="h1"&gt;Title:Adversarial Poetry as a Universal Single-Turn Jailbreak Mechanism in Large Language Models&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:We present evidence that adversarial poetry functions as a universal single-turn jailbreak technique for Large Language Models (LLMs). Across 25 frontier proprietary and open-weight models, curated poetic prompts yielded high attack-success rates (ASR), with some providers exceeding 90%. Mapping prompts to MLCommons and EU CoP risk taxonomies shows that poetic attacks transfer across CBRN, manipulation, cyber-offence, and loss-of-control domains. Converting 1,200 MLCommons harmful prompts into verse via a standardized meta-prompt produced ASRs up to 18 times higher than their prose baselines. Outputs are evaluated using an ensemble of 3 open-weight LLM judges, whose binary safety assessments were validated on a stratified human-labeled subset. Poetic framing achieved an average jailbreak success rate of 62% for hand-crafted poems and approximately 43% for meta-prompt conversions (compared to non-poetic baselines), substantially outperforming non-poetic baselines and revealing a systematic vulnerability across model families and safety training approaches. These findings demonstrate that stylistic variation alone can circumvent contemporary safety mechanisms, suggesting fundamental limitations in current alignment methods and evaluation protocols.&lt;/quote&gt;&lt;head rend="h2"&gt;Submission history&lt;/head&gt;From: Matteo Prandi [view email]&lt;p&gt;[v1] Wed, 19 Nov 2025 10:14:08 UTC (31 KB)&lt;/p&gt;&lt;p&gt;[v2] Thu, 20 Nov 2025 03:34:44 UTC (30 KB)&lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45991738</guid><pubDate>Thu, 20 Nov 2025 12:01:26 +0000</pubDate></item><item><title>Nano Banana Pro</title><link>https://blog.google/technology/ai/nano-banana-pro/</link><description>&lt;doc fingerprint="7c1622329c3e4b84"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Introducing Nano Banana Pro&lt;/head&gt;
    &lt;p&gt;Just a few months ago we released Nano Banana, our Gemini 2.5 Flash Image model. From restoring old photos to generating mini figurines, Nano Banana was a big step in image editing that empowered casual creators to express their creativity.&lt;/p&gt;
    &lt;p&gt;Today, we‚Äôre introducing Nano Banana Pro (Gemini 3 Pro Image), our new state-of-the art image generation and editing model. Built on Gemini 3 Pro, Nano Banana Pro uses Gemini‚Äôs state-of-the-art reasoning and real-world knowledge to visualize information better than ever before.&lt;/p&gt;
    &lt;head rend="h2"&gt;How Nano Banana Pro helps you bring any idea or design to life&lt;/head&gt;
    &lt;p&gt;Nano Banana Pro can help you visualize any idea and design anything - from prototypes, to representing data as infographics, to turning handwritten notes into diagrams.&lt;/p&gt;
    &lt;p&gt;With Nano Banana Pro, now you can:&lt;/p&gt;
    &lt;p&gt;Generate more accurate, context-rich visuals based on enhanced reasoning, world knowledge and real-time information&lt;/p&gt;
    &lt;p&gt;With Gemini 3‚Äôs advanced reasoning, Nano Banana Pro doesn‚Äôt just create beautiful images, it also helps you create more helpful content. You can get accurate educational explainers to learn more about a new subject, like context-rich infographics and diagrams based on the content you provide or facts from the real world. Nano Banana Pro can also connect to Google Search's vast knowledge base to help you create a quick snapshot for a recipe or visualize real-time information like weather or sports.&lt;/p&gt;
    &lt;p&gt;An infographic of the common house plant, String of Turtles, with information on origins, care essentials and growth patterns.&lt;/p&gt;
    &lt;p&gt;Prompt: Create an infographic about this plant focusing on interesting information.&lt;/p&gt;
    &lt;p&gt;Step-by-step infographic for making Elaichi Chai (cardamom tea), demonstrating the ability to visualize recipes and real-world information.&lt;/p&gt;
    &lt;p&gt;Prompt: Create an infographic that shows how to make elaichi chai&lt;/p&gt;
    &lt;p&gt;We used Nano Banana Pro to pull in real-time weather via Search grounding to build a pop-art infographic.&lt;/p&gt;
    &lt;p&gt;Generate better visuals with more accurate, legible text directly in the image in multiple languages&lt;/p&gt;
    &lt;p&gt;Nano Banana Pro is the best model for creating images with correctly rendered and legible text directly in the image, whether you‚Äôre looking for a short tagline, or a long paragraph. Gemini 3 is great at understanding depth and nuance, which unlocks a world of possibilities with image editing and generation - especially with text. Now you can create more detailed text in mockups or posters with a wider variety of textures, fonts and calligraphy. With Gemini‚Äôs enhanced multilingual reasoning, you can generate text in multiple languages, or localize and translate your content so you can scale internationally and/or share content more easily with friends and family.&lt;/p&gt;
    &lt;p&gt;A black and white storyboard sketch showing an establishing shot, medium shot, close-up, and POV shot for a film scene.&lt;/p&gt;
    &lt;p&gt;Prompt: Create a storyboard for this scene&lt;/p&gt;
    &lt;p&gt;The word 'BERLIN' integrated into the architecture of a city block, spanning across multiple buildings.&lt;/p&gt;
    &lt;p&gt;Prompt: View of a cozy street in Berlin on a bright sunny day, stark shadows. the old houses are oddly shaped like letters that spell out "BERLIN" Colored in Blue, Red, White and black. The houses still look like houses and the resemblance to letters is subtle.&lt;/p&gt;
    &lt;p&gt;Calligraphy inspired by meaning, showcasing the ability to generate expressive text with a wider variety of textures and fonts.&lt;/p&gt;
    &lt;p&gt;Prompt: make 8 minimalistic logos, each is an expressive word, and make letters convey a message or sound visually to express the meaning of this word in a dramatic way. composition: flat vector rendering of all logos in black on a single white background&lt;/p&gt;
    &lt;p&gt;A beverage campaign concept showcasing accurate translation and rendering of English text into Korean.&lt;/p&gt;
    &lt;p&gt;Prompt: translate all the English text on the three yellow and blue cans into Korean, while keeping everything else the same&lt;/p&gt;
    &lt;p&gt;A graphic design featuring the word 'TYPOGRAPHY' with a retro, screen-printed texture.&lt;/p&gt;
    &lt;p&gt;Prompt: A vibrant, eye-catching "TYPOGRAPHY" design on a textured off-white background. The letters are bold, blocky, extra condensed and create a 3D effect with overlapping layers of bright blue and hot pink, each with a halftone dot pattern, evoking a retro print aesthetic. 16:9 aspect ratio&lt;/p&gt;
    &lt;p&gt;Blending text and texture in a creative way by integrating the phrase into a woodchopping scene.&lt;/p&gt;
    &lt;p&gt;Prompt: Create an image showing the phrase "How much wood would a woodchuck chuck if a woodchuck could chuck wood" made out of wood chucked by a woodchuck.&lt;/p&gt;
    &lt;p&gt;Create high-fidelity visuals with upgraded creative capabilities&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Consistency by design: With Nano Banana Pro, you can blend more elements than ever before, using up to 14 images and maintaining the consistency and resemblance of up to 5 people. Whether turning sketches into products or blueprints into photorealistic 3D structures, you can now bridge the gap between concept and creation. Apply your desired visual look and feel to your mockups with ease, ensuring your branding remains seamless and consistent across every touchpoint.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Maintaining the consistency of up to 14 inputs, including multiple characters, across a complex composition.&lt;/p&gt;
    &lt;p&gt;Prompt: A medium shot of the 14 fluffy characters sitting squeezed together side-by-side on a worn beige fabric sofa and on the floor. They are all facing forwards, watching a vintage, wooden-boxed television set placed on a low wooden table in front of the sofa. The room is dimly lit, with warm light from a window on the left and the glow from the TV illuminating the creatures' faces and fluffy textures. The background is a cozy, slightly cluttered living room with a braided rug, a bookshelf with old books, and rustic kitchen elements in the background. The overall atmosphere is warm, cozy, and amused.&lt;/p&gt;
    &lt;p&gt;Craft lifestyle scenes by combining multiple elements.&lt;/p&gt;
    &lt;p&gt;Prompt: Combine these images into one appropriately arranged cinematic image in 16:9 format and change the dress on the mannequin to the dress in the image&lt;/p&gt;
    &lt;p&gt;Create surreal landscapes by combining multiple input elements.&lt;/p&gt;
    &lt;p&gt;Prompt: Combine these images into one appropriately arranged cinematic image in 16:9 format&lt;/p&gt;
    &lt;p&gt;A high-fashion editorial shot set in a desert landscape that maintains the consistency and resemblance of the people from the 6 input photos.&lt;/p&gt;
    &lt;p&gt;Prompt: Put these five people and this dog into a single image, they should fit into a stunning award-winning shot in the style if [sic] a fashion editorial. The identity of all five people and their attire and the dog must stay consistent throughout but they can and should be seen from different angles and distances in [sic] as is most natural and suitable to the scene. Make the colour and lighting look natural on them all, they look like they naturally fit into this fashion show.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Studio-quality creative controls: With Nano Banana Pro's new capabilities we are putting advanced creative controls directly into your hands. Select, refine and transform any part of an image with improved localized editing. Adjust camera angles, change the focus and apply sophisticated color grading, or even transform scene lighting (e.g. changing day to night or creating a bokeh effect). Your creations are ready for any platform, from social media to print, thanks to a range of available aspect ratios and available 2K and 4K resolution&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Change the look and feel of an image for a range of platforms by adapting the aspect ratio.&lt;/p&gt;
    &lt;p&gt;Prompt: change aspect ratio to 1:1 by reducing background. The character, remains exactly locked in its current position&lt;/p&gt;
    &lt;p&gt;Lighting and focus controls applied to transform a scene from day to night.&lt;/p&gt;
    &lt;p&gt;Prompt: Turn this scene into nighttime&lt;/p&gt;
    &lt;p&gt;Obscure or enlighten a section of your image with lighting controls to achieve specific dramatic effects.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;Prompt: Generate an image with an intense chiaroscuro effect. The man should retain his original features and expression. Introduce harsh, directional light, appearing to come from above and slightly to the left, casting deep, defined shadows across the face. Only slivers of light illuminating his eyes and cheekbones, the rest of the face is in deep shadow.&lt;/p&gt;
    &lt;p&gt;Bring out the details of your composition by adjusting the depth of field or focal point (e.g., focusing on the flowers).&lt;/p&gt;
    &lt;p&gt;Prompt: Focus on the flowers&lt;/p&gt;
    &lt;head rend="h2"&gt;How you can try Nano Banana Pro today&lt;/head&gt;
    &lt;p&gt;Across our products and services, you now have a choice: the original Nano Banana for fast, fun editing, or Nano Banana Pro for complex compositions requiring the highest quality and visually sophisticated results.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Consumers and students: Rolling out globally in the Gemini app when you select ‚ÄòCreate images‚Äô with the ‚ÄòThinking‚Äô model. Our free-tier users will receive limited free quotas, after which they will revert to the original Nano Banana model. Google AI Plus, Pro and Ultra subscribers receive higher quotas. For AI Mode in Search, Nano Banana Pro is available in the U.S. for Google AI Pro and Ultra subscribers. For NotebookLM, Nano Banana Pro is also available for subscribers globally.&lt;/item&gt;
      &lt;item&gt;Professionals: We're upgrading image generation in Google Ads to Nano Banana Pro to put cutting-edge creative and editing power directly into the hands of advertisers globally. It‚Äôs also rolling out starting today to Workspace customers in Google Slides and Vids.&lt;/item&gt;
      &lt;item&gt;Developers and enterprise: Starting to roll out in the Gemini API and Google AI Studio, and in Google Antigravity to create rich UX layouts &amp;amp; mockups; enterprises can start building in Vertex AI for scaled creation today and it‚Äôs coming soon to Gemini Enterprise.&lt;/item&gt;
      &lt;item&gt;Creatives: Starting to roll out to Google AI Ultra subscribers in Flow, our AI filmmaking tool, to give creatives, filmmakers and marketers even more precision and control over their frames and scenes.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;How to identify AI-generated images in the Gemini app&lt;/head&gt;
    &lt;p&gt;We believe it‚Äôs critical to know when an image is AI-generated. This is why all media generated by Google‚Äôs tools are embedded with our imperceptible SynthID digital watermark.&lt;/p&gt;
    &lt;p&gt;Today, we are putting a powerful verification tool directly in consumers‚Äô hands: you can now upload an image into the Gemini app and simply ask if it was generated by Google AI, thanks to SynthID technology. We are starting with images, but will expand to audio and video soon.&lt;/p&gt;
    &lt;p&gt;In addition to SynthID, we will maintain a visible watermark (the Gemini sparkle) on images generated by free and Google AI Pro tier users, to make images even more easy to detect as Google AI-generated.&lt;/p&gt;
    &lt;p&gt;Recognizing the need for a clean visual canvas for professional work, we will remove the visible watermark from images generated by Google AI Ultra subscribers and within the Google AI Studio developer tool.&lt;/p&gt;
    &lt;p&gt;You can find out more about how we‚Äôre increasing transparency in AI content with SynthID in our blog post.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45993296</guid><pubDate>Thu, 20 Nov 2025 15:04:23 +0000</pubDate></item><item><title>Android and iPhone users can now share files, starting with the Pixel 10</title><link>https://blog.google/products/android/quick-share-airdrop/</link><description>&lt;doc fingerprint="afecea04cdf1cf3c"&gt;
  &lt;main&gt;
    &lt;p&gt;When it comes to sharing moments between family and friends, what device you have shouldn‚Äôt matter ‚Äî sharing should just work. But we‚Äôve heard from many people that they want a simpler way to share files between devices.&lt;/p&gt;
    &lt;p&gt;Today, we‚Äôre introducing a way for Quick Share to work with AirDrop. This makes file transfer easier between iPhones and Android devices, and starts rolling out today to the Pixel 10 family.&lt;/p&gt;
    &lt;p&gt;We built this with security at its core, protecting your data with strong safeguards that were tested by independent security experts. It‚Äôs just one more way we‚Äôre bringing better compatibility that people are asking for between operating systems, following our work on RCS and unknown tracker alerts.&lt;/p&gt;
    &lt;p&gt;We‚Äôre looking forward to improving the experience and expanding it to more Android devices. See it in action on the Pixel 10 Pro in this video, and try it out for yourself!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45994854</guid><pubDate>Thu, 20 Nov 2025 17:04:34 +0000</pubDate></item><item><title>The Lions Operating System</title><link>https://lionsos.org</link><description>&lt;doc fingerprint="bccc914e63abc302"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The Lions Operating System #&lt;/head&gt;
    &lt;quote&gt;LionsOS is currently undergoing active research and development, it does not have a concrete verification story yet. It is not expected for LionsOS to be stable at this time, but it is available for others to experiment with.&lt;/quote&gt;
    &lt;p&gt;LionsOS is an operating system based on the seL4 microkernel with the goal of making the achievements of seL4 accessible. That is, to provide performance, security, and reliability.&lt;/p&gt;
    &lt;p&gt;LionsOS is being developed by the Trustworthy Systems research group at UNSW Sydney in Australia.&lt;/p&gt;
    &lt;p&gt;It is not a conventional operating system, but contains composable components for creating custom operating systems that are specific to a particular task. Components are joined together using the Microkit tool.&lt;/p&gt;
    &lt;p&gt;The principles on which a LionsOS system is built are laid out fully in the sDDF design document; but in brief they are:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Components are connected by lock-free queues using an efficient model-checked signalling mechanism.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;As far as is practical, operating systems components do a single thing. Drivers for instance exist solely to convert between a hardware interface and a set of queues to talk to the rest of the system.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Components called virtualisers handle multiplexing and control, and conversion between virtual and IO addresses for drivers.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Information is shared only where necessary, via the queues, or via published information pages.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The system is static: it does not adapt to changing hardware, and does not load components at runtime. There is a mechanism for swapping components of the same type at runtime, to implement policy changes, or to reboot a virtual machine with a new Linux kernel.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To be successful, many more components are needed. Pull requests to the various repositories are welcome. See the page on contributing for more details.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45995816</guid><pubDate>Thu, 20 Nov 2025 18:19:31 +0000</pubDate></item><item><title>NTSB Preliminary Report ‚Äì UPS Boeing MD-11F Crash [pdf]</title><link>https://www.ntsb.gov/Documents/Prelimiary%20Report%20DCA26MA024.pdf</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45995834</guid><pubDate>Thu, 20 Nov 2025 18:20:59 +0000</pubDate></item><item><title>Data-at-Rest Encryption in DuckDB</title><link>https://duckdb.org/2025/11/19/encryption-in-duckdb</link><description>&lt;doc fingerprint="bf9f389225e9b2d0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Data-at-Rest Encryption in DuckDB&lt;/head&gt;
    &lt;p&gt;TL;DR: DuckDB v1.4 ships database encryption capabilities. In this blog post, we dive into the implementation details of the encryption, show how to use it and demonstrate its performance implications.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;If you would like to use encryption in DuckDB, we recommend using the latest stable version, v1.4.2. For more details, see the latest release blog post.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Many years ago, we read the excellent ‚ÄúCode Book‚Äù by Simon Singh. Did you know that Mary, Queen of Scots, used an encryption method harking back to Julius Caesar to encrypt her more saucy letters? But alas: the cipher was broken and the contents of the letters got her executed.&lt;/p&gt;
    &lt;p&gt;These days, strong encryption software and hardware is a commodity. Modern CPUs come with specialized cryptography instructions, and operating systems small and big contain mostly-robust cryptography software like OpenSSL.&lt;/p&gt;
    &lt;p&gt;Databases store arbitrary information, it is clear that many if not most datasets of any value should perhaps not be plainly available to everyone. Even if stored on tightly controlled hardware like a cloud virtual machine, there have been many cases of files being lost through various privilege escalations. Unsurprisingly, compliance frameworks like the common SOC 2 ‚Äúhighly recommend‚Äù encrypting data when stored on storage mediums like hard drives.&lt;/p&gt;
    &lt;p&gt;However, database systems and encryption have a somewhat problematic track record. Even PostgreSQL, the self-proclaimed ‚ÄúThe World's Most Advanced Open Source Relational Database‚Äù has very limited options for data encryption. SQLite, the world‚Äôs ‚ÄúMost Widely Deployed and Used Database Engine‚Äù does not support data encryption out-of-the-box, its encryption extension is a $2000 add-on.&lt;/p&gt;
    &lt;p&gt;DuckDB has supported Parquet Modular Encryption for a while. This feature allows reading and writing Parquet files with encrypted columns. However, while Parquet files are great and reports of their impending death are greatly exaggerated, they cannot ‚Äì for example ‚Äì be updated in place, a pretty basic feature of a database management system.&lt;/p&gt;
    &lt;p&gt;Starting with DuckDB 1.4.0, DuckDB supports transparent data encryption of data-at-rest using industry-standard AES encryption.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;DuckDB's encryption does not yet meet the official NIST requirements.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Some Basics of Encryption&lt;/head&gt;
    &lt;p&gt;There are many different ways to encrypt data, some more secure than others. In database systems and elsewhere, the standard is the Advanced Encryption Standard (AES), which is a block cipher algorithm standardized by US NIST. AES is a symmetric encryption algorithm, meaning that the same key is used for both encryption and decryption of data.&lt;/p&gt;
    &lt;p&gt;For this reason, most systems choose to only support randomized encryption, meaning that identical plaintexts will always yield different ciphertexts (if used correctly!). The most commonly used industry standard and recommended encryption algorithm is AES ‚Äì Galois Counter Mode (AES-GCM). This is because on top of its ability to randomize encryption, it also authenticates data by calculating a tag to ensure data has not been tampered with.&lt;/p&gt;
    &lt;p&gt;DuckDB v1.4 supports encryption at rest using AES-GCM-256 and AES-CTR-256 (counter mode) ciphers. AES-CTR is a simpler and faster version of AES-GCM, but less secure, since it does not provide authentication by calculating a tag. The 256 refers to the size of the key in bits, meaning that DuckDB now only supports GCM with 32-byte keys.&lt;/p&gt;
    &lt;p&gt;GCM and CTR both require as input a (1) plaintext, (2) an initialization vector (IV) and (3) an encryption key. Plaintext is the text that a user wants to encrypt. An IV is a unique bytestream of usually 16 bytes, that ensures that identical plaintexts get encrypted into different ciphertexts. A number used once (nonce) is a bytestream of usually 12 bytes, that together with a 4-byte counter construct the IV. Note that the IV needs to be unique for every encrypted block, but it does not necessarily have to be random. Reuse of the same IV is problematic, since an attacker could XOR the two ciphertexts and extract both messages. The tag in AES-GCM is calculated after all blocks are encrypted, pretty much like a checksum, but it adds an integrity check that securely authenticates the entire ciphertext.&lt;/p&gt;
    &lt;head rend="h2"&gt;Implementation in DuckDB&lt;/head&gt;
    &lt;p&gt;Before diving deeper into how we actually implemented encryption in DuckDB, we‚Äôll explain some things about the DuckDB file format.&lt;/p&gt;
    &lt;p&gt;DuckDB has one main database header which stores data that enables it to correctly load and verify a DuckDB database. At the start of each DuckDB main database header, the magic bytes (‚ÄúDUCKDB‚Äù) are stored and read upon initialization to verify whether the file is a valid DuckDB database file. The magic bytes are followed by four 8-byte of flags that can be set for different purposes.&lt;/p&gt;
    &lt;p&gt;When a database is encrypted in DuckDB, the main database header remains plaintext at all times, since the main header contains no sensitive data about the contents of the database file. Upon initializing an encrypted database, DuckDB sets the first bit in the first flag to indicate that the database is encrypted. After setting this bit, additional metadata is stored that is necessary for encryption. This metadata entails the (1) database identifier, (2) 8 bytes of additional metadata for e.g. the encryption cipher used, and (3) the encrypted canary.&lt;/p&gt;
    &lt;p&gt;The database identifier is used as a ‚Äúsalt‚Äù, and consists of 16 randomly generated bytes created upon initialization of each database. The salt is often used to ensure uniqueness, i.e., it makes sure that identical input keys or passwords are transformed into different derived keys. The 8-bytes of metadata comprise the key derivation function (first byte), usage of additional authenticated data (second byte), the encryption cipher (third byte), and the key length (fifth byte). After the metadata, the main header uses the encrypted canary to check if the input key is correct.&lt;/p&gt;
    &lt;head rend="h3"&gt;Encryption Key Management&lt;/head&gt;
    &lt;p&gt;To encrypt data in DuckDB, you can use practically any plaintext or base64 encoded string, but we recommend using a secure 32-byte base64 key. The user itself is responsible for the key management and thus for using a secure key. Instead of directly using the plain key provided by the user, DuckDB always derives a more secure key by means of a key derivation function (kdf). The kdf is a function that reduces or extends the input key to a 32-byte secure key. If the correctness of the input key is checked by deriving the secure key and decrypting the canary, the derived key is managed in a secure encryption key cache. This cache manages encryption keys for the current DuckDB context and ensures that the derived encryption keys are never swapped to disk by locking its memory. To strengthen security even more, the original input keys are immediately wiped from memory when the input keys are transformed into secure derived keys.&lt;/p&gt;
    &lt;head rend="h3"&gt;DuckDB Block Structure&lt;/head&gt;
    &lt;p&gt;After the main database header, DuckDB stores two 4KB database headers that contain more information about e.g. the block (header) size and the storage version used. After keeping the main database header plaintext, all remaining headers and blocks are encrypted when encryption is used.&lt;/p&gt;
    &lt;p&gt;Blocks in DuckDB are by default 256KB, but their size is configurable. At the start of each plaintext block there is an 8-byte block header, which stores an 8-byte checksum. The checksum is a simple calculation that is often used in database systems to check for any corrupted data.&lt;/p&gt;
    &lt;p&gt;For encrypted blocks however, its block header consists of 40 bytes instead of 8 bytes for the checksum. The block header for encrypted blocks contains a 16-byte nonce/IV and, optionally, a 16-byte tag, depending on which encryption cipher is used. The nonce and tag are stored in plaintext, but the checksum is encrypted for better security. Note that the block header always needs to be 8-bytes aligned to calculate the checksum.&lt;/p&gt;
    &lt;head rend="h3"&gt;Write-Ahead-Log Encryption&lt;/head&gt;
    &lt;p&gt;The write ahead log (WAL) in database systems is a crash recovery mechanism to ensure durability. It is an append-only file that is used in scenarios where the database crashed or is abruptly closed, and when not all changes are written yet to the main database file. The WAL makes sure these changes can be replayed up to the last checkpoint; which is a consistent snapshot of the database at a certain point in time. This means, when a checkpoint is enforced, which happens in DuckDB by either (1) closing the database or (2) reaching a certain threshold for storage, the WAL gets written into the main database file.&lt;/p&gt;
    &lt;p&gt;In DuckDB, you can force the creation of a WAL by setting&lt;/p&gt;
    &lt;code&gt;PRAGMA disable_checkpoint_on_shutdown;
PRAGMA wal_autocheckpoint = '1TB';
&lt;/code&gt;
    &lt;p&gt;This way you‚Äôll disable a checkpointing on closing the database, meaning that the WAL does not get merged into the main database file. In addition, by setting wal_autocheckpoint to a high threshold, this will avoid intermediate checkpoints to happen and the WAL will persist. For example, we can create a persistent WAL file by first setting the above PRAGMAs, then attach an encrypted database, and then create a table where we insert 3 values.&lt;/p&gt;
    &lt;code&gt;ATTACH 'encrypted.db' AS enc (
    ENCRYPTION_KEY 'asdf',
    ENCRYPTION_CIPHER 'GCM'
);
CREATE TABLE enc.test (a INTEGER, b INTEGER);
INSERT INTO enc.test VALUES (11, 22), (13, 22), (12, 21)
&lt;/code&gt;
    &lt;p&gt;If we now close the DuckDB process, we can see that there is a &lt;code&gt;.wal&lt;/code&gt; file shown: &lt;code&gt;encrypted.db.wal&lt;/code&gt;. But how is the WAL created internally?&lt;/p&gt;
    &lt;p&gt;Before writing new entries (inserts, updates, deletes) to the database, these entries are essentially logged and appended to the WAL. Only after logged entries are flushed to disk, a transaction is considered as committed. A plaintext WAL entry has the following structure:&lt;/p&gt;
    &lt;p&gt;Since the WAL is append-only, we encrypt a WAL entry per value. For AES-GCM this means that we append a nonce and a tag to each entry. The structure in which we do this is depicted in below. When we serialize an encrypted entry to the encrypted WAL, we first store the length in plaintext, because we need to know how many bytes we should decrypt. The length is followed by a nonce, which on its turn is followed by the encrypted checksum and the encrypted entry itself. After the entry, a 16-byte tag is stored for verification.&lt;/p&gt;
    &lt;p&gt;Encrypting the WAL is triggered by default when an encryption key is given for any (un)encrypted database.&lt;/p&gt;
    &lt;head rend="h3"&gt;Temporary File Encryption&lt;/head&gt;
    &lt;p&gt;Temporary files are used to store intermediate data that is often necessary for large, out-of-core operations such as sorting, large joins and window functions. This data could contain sensitive information and can, in case of a crash, remain on disk. To protect this leftover data, DuckDB automatically encrypts temporary files too.&lt;/p&gt;
    &lt;head rend="h4"&gt;The Structure of Temporary Files&lt;/head&gt;
    &lt;p&gt;There are three different types of temporary files in DuckDB: (1) temporary files that have the same layout as a regular 256KB block, (2) compressed temporary files and (3) temporary files that exceed the standard 256KB block size. The former two are suffixed with .tmp, while the latter is distinguished by a suffix with .block. To keep track of the size of .block temporary files, they are always prefixed with its length. As opposed to regular database blocks, temporary files do not contain a checksum to check for data corruption, since the calculation of a checksum is somewhat expensive.&lt;/p&gt;
    &lt;head rend="h4"&gt;Encrypting Temporary Files&lt;/head&gt;
    &lt;p&gt;Temporary files are encrypted (1) automatically when you attach an encrypted database or (2) when you use the setting &lt;code&gt;SET temp_file_encryption = true&lt;/code&gt;. In the latter case, the main database file is plaintext, but the temporary files will be encrypted. For the encryption of temporary files DuckDB internally generates temporary keys. This means that when the database crashes, the temporary keys are also lost. Temporary files cannot be decrypted in this case and are then essentially garbage.&lt;/p&gt;
    &lt;p&gt;To force DuckDB to produce temporary files, you can use a simple trick by just setting the memory limit low. This will create temporary files once the memory limit is exceeded. For example, we can create a new encrypted database, load this database with TPC-H data (SF 1), and then set the memory limit to 1 GB. If we then perform a large join, we force DuckDB to spill intermediate data to disk. For example:&lt;/p&gt;
    &lt;code&gt;SET memory_limit = '1GB';
ATTACH 'tpch_encrypted.db' AS enc (
    ENCRYPTION_KEY 'asdf',
    ENCRYPTION_CIPHER 'cipher'
);
USE enc;
CALL dbgen(sf = 1);

ALTER TABLE lineitem
    RENAME TO lineitem1;
CREATE TABLE lineitem2 AS
    FROM lineitem1;
CREATE OR REPLACE TABLE ans AS
    SELECT l1.* , l2.*
    FROM lineitem1 l1
    JOIN lineitem2 l2 USING (l_orderkey , l_linenumber);
&lt;/code&gt;
    &lt;p&gt;This sequence of commands will result in encrypted temporary files being written to disk. Once the query completes or when the DuckDB shell is exited, the temporary files are automatically cleaned up. In case of a crash however, it may happen that temporary files will be left on disk and need to be cleaned up manually.&lt;/p&gt;
    &lt;head rend="h2"&gt;How to Use Encryption in DuckDB&lt;/head&gt;
    &lt;p&gt;In DuckDB, you can (1) encrypt an existing database, (2) initialize a new, empty encrypted database or (3) reencrypt a database. For example, let's create a new database, load this database with TPC-H data of scale factor 1 and then encrypt this database.&lt;/p&gt;
    &lt;code&gt;INSTALL tpch;
LOAD tpch;
ATTACH 'encrypted.duckdb' AS encrypted (ENCRYPTION_KEY 'asdf');
ATTACH 'unencrypted.duckdb' AS unencrypted;
USE unencrypted;
CALL dbgen(sf = 1);
COPY FROM DATABASE unencrypted TO encrypted;
&lt;/code&gt;
    &lt;p&gt;There is not a trivial way to prove that a database is encrypted, but correctly encrypted data should look like random noise and has a high entropy. So, to check whether a database is actually encrypted, we can use tools to calculate the entropy or visualize the binary, such as ent and binocle.&lt;/p&gt;
    &lt;p&gt;When we use ent after executing the above chunk of SQL, i.e., &lt;code&gt;ent encrypted.duckdb&lt;/code&gt;, this will result in an entropy of 7.99999 bits per byte. If we do the same for the plaintext (unencrypted) database, this results in 7.65876 bits per byte. Note that the plaintext database also has a high entropy, but this is due to compression.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs now visualize both the plaintext and encrypted data with binocle. For the visualization we created both a plaintext DuckDB database with scale factor of 0.001 of TPC-H data and an encrypted one:&lt;/p&gt;
    &lt;head&gt;Click here to see the entropy of a plaintext database&lt;/head&gt;
    &lt;head&gt;Click here to see the entropy of an encrypted database&lt;/head&gt;
    &lt;p&gt;In these figures, we can clearly observe that the encrypted database file seems completely random, while the plaintext database file shows some clear structure in its binary data.&lt;/p&gt;
    &lt;p&gt;To decrypt an encrypted database, we can use the following SQL:&lt;/p&gt;
    &lt;code&gt;ATTACH 'encrypted.duckdb' AS encrypted (ENCRYPTION_KEY 'asdf');
ATTACH 'new_unencrypted.duckdb' AS unencrypted;
COPY FROM DATABASE encrypted TO unencrypted;
&lt;/code&gt;
    &lt;p&gt;And to reencrypt an existing database, we can just simply copy the old encrypted database to a new one, like:&lt;/p&gt;
    &lt;code&gt;ATTACH 'encrypted.duckdb' AS encrypted (ENCRYPTION_KEY 'asdf');
ATTACH 'new_encrypted.duckdb' AS new_encrypted (ENCRYPTION_KEY 'xxxx');
COPY FROM DATABASE encrypted TO new_encrypted;
&lt;/code&gt;
    &lt;p&gt;The default encryption algorithm is AES GCM. This is recommended since it also authenticates data by calculating a tag. Depending on the use case, you can also use AES CTR. This is faster than AES GCM since it skips calculating a tag after encrypting all data. You can specify the CTR cipher as follows:&lt;/p&gt;
    &lt;code&gt;ATTACH 'encrypted.duckdb' AS encrypted (
    ENCRYPTION_KEY 'asdf',
    ENCRYPTION_CIPHER 'CTR'
);
&lt;/code&gt;
    &lt;p&gt;To keep track of which databases are encrypted, you can query this by running:&lt;/p&gt;
    &lt;code&gt;FROM duckdb_databases();
&lt;/code&gt;
    &lt;p&gt;This will show which databases are encrypted, and which cipher is used:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="6"&gt;
        &lt;cell role="head"&gt;database_name&lt;/cell&gt;
        &lt;cell role="head"&gt;database_oid&lt;/cell&gt;
        &lt;cell role="head"&gt;path&lt;/cell&gt;
        &lt;cell role="head"&gt;‚Ä¶&lt;/cell&gt;
        &lt;cell role="head"&gt;encrypted&lt;/cell&gt;
        &lt;cell role="head"&gt;cipher&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;encrypted&lt;/cell&gt;
        &lt;cell&gt;2103&lt;/cell&gt;
        &lt;cell&gt;encrypted.duckdb&lt;/cell&gt;
        &lt;cell&gt;‚Ä¶&lt;/cell&gt;
        &lt;cell&gt;true&lt;/cell&gt;
        &lt;cell&gt;GCM&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;unencrypted&lt;/cell&gt;
        &lt;cell&gt;2050&lt;/cell&gt;
        &lt;cell&gt;unencrypted.duckdb&lt;/cell&gt;
        &lt;cell&gt;‚Ä¶&lt;/cell&gt;
        &lt;cell&gt;false&lt;/cell&gt;
        &lt;cell&gt;NULL&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;memory&lt;/cell&gt;
        &lt;cell&gt;592&lt;/cell&gt;
        &lt;cell&gt;NULL&lt;/cell&gt;
        &lt;cell&gt;‚Ä¶&lt;/cell&gt;
        &lt;cell&gt;false&lt;/cell&gt;
        &lt;cell&gt;NULL&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;system&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;NULL&lt;/cell&gt;
        &lt;cell&gt;‚Ä¶&lt;/cell&gt;
        &lt;cell&gt;false&lt;/cell&gt;
        &lt;cell&gt;NULL&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;temp&lt;/cell&gt;
        &lt;cell&gt;1995&lt;/cell&gt;
        &lt;cell&gt;NULL&lt;/cell&gt;
        &lt;cell&gt;‚Ä¶&lt;/cell&gt;
        &lt;cell&gt;false&lt;/cell&gt;
        &lt;cell&gt;NULL&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;5 rows ‚Äî 10 columns (5 shown)&lt;/p&gt;
    &lt;head rend="h2"&gt;Implementation and Performance&lt;/head&gt;
    &lt;p&gt;Here at DuckDB, we strive to achieve a good out-of-the-box experience with zero external dependencies and a small footprint. Encryption and decryption, however, are usually performed by pretty heavy external libraries such as OpenSSL. We would much prefer not to rely on external libraries or statically linking huge codebases just so that people can use encryption in DuckDB without additional steps. This is why we actually implemented encryption twice in DuckDB, once with the (excellent) Mbed TLS library and once with the ubiquitous OpenSSL library.&lt;/p&gt;
    &lt;p&gt;DuckDB already shipped parts of Mbed TLS because we use it to verify RSA extension signatures. However, for maximum compatibility we actually disabled the hardware acceleration of MbedTLS, which has a performance impact. Furthermore, Mbed TLS is not particularly hardened against things like nasty timing attacks. OpenSSL on the other hand contains heavily vetted and hardware-accelerated code to perform AES operations, which is why we can also use it for encryption.&lt;/p&gt;
    &lt;p&gt;In DuckDB Land, OpenSSL is part of the &lt;code&gt;httpfs&lt;/code&gt; extension. Once you load that extension, encryption will automatically switch to using OpenSSL. After we shipped encryption in DuckDB 1.4.0, security experts actually found issues with the random number generator we used in Mbed TLS mode. Even though it would be difficult to actually exploit this, we disabled writing to databases in MbedTLS mode from DuckDB 1.4.1. Instead, DuckDB now (version 1.4.2+) tries to auto-install and auto-load the &lt;code&gt;httpfs&lt;/code&gt; extension whenever a write is attempted. We might be able to revisit this in the future, but for now this seems the safest path forward that still allows high compatibility for reading. In OpenSSL mode, we always used a cryptographically-safe random number generation so that mode is unaffected.&lt;/p&gt;
    &lt;p&gt;Encrypting and decrypting database files is an additional step in writing tables to disk, so we would naturally assume that there is some performance impact. Let‚Äôs investigate the performance impact of DuckDB‚Äôs new encryption feature with a very basic experiment.&lt;/p&gt;
    &lt;p&gt;We first create two DuckDB database files, one encrypted and one unencrypted. We use the TPC-H benchmark generator again to create the table data, particularly the (somewhat tired) &lt;code&gt;lineitem&lt;/code&gt; table.&lt;/p&gt;
    &lt;code&gt;INSTALL httpfs;
INSTALL tpch;
LOAD tpch;

ATTACH 'unencrypted.duckdb' AS unencrypted;
CALL dbgen(sf = 10, catalog = 'unencrypted');

ATTACH 'encrypted.duckdb' AS encrypted (ENCRYPTION_KEY 'asdf');
CREATE TABLE encrypted.lineitem AS FROM unencrypted.lineitem;
&lt;/code&gt;
    &lt;p&gt;Now we use DuckDB‚Äôs neat &lt;code&gt;SUMMARIZE&lt;/code&gt; command three times: once on the unencrypted database, and once on the encrypted database using MbedTLS and once on the encrypted database using OpenSSL. We set a very low memory limit to force more reading and writing from disk.&lt;/p&gt;
    &lt;code&gt;SET memory_limit = '200MB';
.timer on

SUMMARIZE unencrypted.lineitem;
SUMMARIZE encrypted.lineitem;

LOAD httpfs; -- use OpenSSL
SUMMARIZE encrypted.lineitem;
&lt;/code&gt;
    &lt;p&gt;Here are the results on a fairly recent MacBook: &lt;code&gt;SUMMARIZE&lt;/code&gt; on the unencrypted table took ca. 5.4 seconds. Using Mbed TLS, this went up to around 6.2 s. However, when enabling OpenSSL the end-to-end time went straight back to 5.4 s. How is this possible? Is decryption not expensive? Well, there is a lot more happening in query processing than reading blocks from storage. So the impact of decryption is not all that huge, even when using a slow implementation. Secondly, when using hardware acceleration in OpenSSL, the overall overhead of encryption and decryption becomes almost negligible.&lt;/p&gt;
    &lt;p&gt;But just running summarization is overly simplistic. Real‚Ñ¢ database workloads include modifications to data, insertion of new rows, updates of rows, deletion of rows etc. Also, multiple clients will be updating and querying at the same time. So we re-surrected the full TPC-H ‚ÄúPower‚Äù test from our previous blog post ‚ÄúChanging Data with Confidence and ACID‚Äù. We slightly tweaked the benchmark script to enable the new database encryption. For this experiment, we used the OpenSSL encryption implementation due to the issues outlined above. We observe ‚ÄúPower@Size‚Äù and ‚ÄúThroughput@Size‚Äù. The former is raw sequential query performance, while the latter measures multiple parallel query streams in the presence of updates.&lt;/p&gt;
    &lt;p&gt;When running on the same MacBook with DuckDB 1.4.1 and a ‚Äúscale factor‚Äù of 100, we get a Power@Size metric of 624,296 and a Throughput@Size metric of 450,409 without encryption.&lt;/p&gt;
    &lt;p&gt;When we enable encryption, the results are almost unchanged, confirming the observation of the small microbenchmark above. However, the relationship between available memory and the benchmark size means that we‚Äôre not stressing temporary file encryption. So we re-ran everything with an 8GB memory limit. We confirmed constant reading and writing to and from disk in this configuration by observing operating system statistics. For the unencrypted case, the Power@Size metric predictably went down to 591,841 and Throughput@Size went down to 153,690. And finally, we could observe a slight performance decrease with Power@Size of 571,985 and Throughput@Size of 145,353. However, that difference is not very great either and likely not relevant in real operational scenarios.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;With the new encrypted database feature, we can now safely pass around DuckDB database files with all information inside them completely opaque to prying eyes. This allows for some interesting new deployment models for DuckDB, for example, we could now put an encrypted DuckDB database file on a Content Delivery Network (CDN). A fleet of DuckDB instances could attach to this file read-only using the decryption key. This elegantly allows efficient distribution of private background data in a similar way like encrypted Parquet files, but of course with many more features like multi-table storage. When using DuckDB with encrypted storage, we can also simplify threat modeling when ‚Äì for example ‚Äì using DuckDB on cloud providers. While in the past access to DuckDB storage would have been enough to leak data, we can now relax paranoia regarding storage a little, especially since temporary files and WAL are also encrypted. And the best part of all of this, there is almost no performance overhead to using encryption in DuckDB, especially with the OpenSSL implementation.&lt;/p&gt;
    &lt;p&gt;We are very much looking forward to what you are going to do with this feature, and please let us know if you run into any issues.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45996585</guid><pubDate>Thu, 20 Nov 2025 19:26:12 +0000</pubDate></item><item><title>New OS aims to provide (some) compatibility with macOS</title><link>https://github.com/ravynsoft/ravynos</link><description>&lt;doc fingerprint="c7963a51c657afa6"&gt;
  &lt;main&gt;
    &lt;head rend="h3"&gt;Don't speak English? Read this in: Italiano, T√ºrk√ße, Deutsch, Indonesia, ÁÆÄ‰Ωì‰∏≠Êñá, ÁπÅÈ´î‰∏≠Êñá, Portugu√™s do Brasil, ÌïúÍµ≠Ïñ¥, ŸÅÿßÿ±ÿ≥€å, Magyar&lt;/head&gt;
    &lt;p&gt;ravynOS is a new open source OS project that aims to provide a similar experience and some compatibility with macOS on x86-64 (and eventually ARM) systems. It builds on the solid foundations of FreeBSD, existing open source packages in the same space, and new code to fill the gaps.&lt;/p&gt;
    &lt;p&gt;The main design goals are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Source compatibility with macOS applications (i.e. you could compile a Mac application on ravynOS and run it)&lt;/item&gt;
      &lt;item&gt;Similar GUI metaphors and familiar UX (file manager, application launcher, top menu bar that reflects the open application, etc)&lt;/item&gt;
      &lt;item&gt;Compatible with macOS folder layouts (/Library, /System, /Users, /Volumes, etc) and perhaps filesystems (HFS+, APFS) as well as fully supporting ZFS&lt;/item&gt;
      &lt;item&gt;Self-contained applications in App Bundles, AppDirs, and AppImage files - an installer-less experience for /Applications&lt;/item&gt;
      &lt;item&gt;Mostly maintain compatibility with the FreeBSD base system and X11 - a standard Unix environment under the hood&lt;/item&gt;
      &lt;item&gt;Compatible with Linux binaries via FreeBSD's Linux support&lt;/item&gt;
      &lt;item&gt;Eventual compatibility with x86-64/arm64 macOS binaries (Mach-O) and libraries&lt;/item&gt;
      &lt;item&gt;Pleasant to use, secure, stable, and performant&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Please visit ravynos.com for more info: Release Notes | Screenshots | FAQ&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Can you help build the dream? See the current projects/needs in CONTRIBUTING.md!&lt;/item&gt;
      &lt;item&gt;Our Discord server.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;#ravynOS-general:matrix.org&lt;/code&gt;- join via Element.io&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is the top level of the FreeBSD source directory.&lt;/p&gt;
    &lt;p&gt;FreeBSD is an operating system used to power modern servers, desktops, and embedded platforms. A large community has continually developed it for more than thirty years. Its advanced networking, security, and storage features have made FreeBSD the platform of choice for many of the busiest web sites and most pervasive embedded networking and storage devices.&lt;/p&gt;
    &lt;p&gt;For copyright information, please see the file COPYRIGHT in this directory. Additional copyright information also exists for some sources in this tree - please see the specific source directories for more information.&lt;/p&gt;
    &lt;p&gt;The Makefile in this directory supports a number of targets for building components (or all) of the FreeBSD source tree. See build(7), config(8), FreeBSD handbook on building userland, and Handbook for kernels for more information, including setting make(1) variables.&lt;/p&gt;
    &lt;p&gt;For information on the CPU architectures and platforms supported by FreeBSD, see the FreeBSD website's Platforms page.&lt;/p&gt;
    &lt;p&gt;For official FreeBSD bootable images, see the release page.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Directory&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bin&lt;/cell&gt;
        &lt;cell&gt;System/user commands.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;cddl&lt;/cell&gt;
        &lt;cell&gt;Various commands and libraries under the Common Development and Distribution License.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;contrib&lt;/cell&gt;
        &lt;cell&gt;Packages contributed by 3rd parties.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;crypto&lt;/cell&gt;
        &lt;cell&gt;Cryptography stuff (see crypto/README).&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;etc&lt;/cell&gt;
        &lt;cell&gt;Template files for /etc.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;gnu&lt;/cell&gt;
        &lt;cell&gt;Commands and libraries under the GNU General Public License (GPL) or Lesser General Public License (LGPL). Please see gnu/COPYING and gnu/COPYING.LIB for more information.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;include&lt;/cell&gt;
        &lt;cell&gt;System include files.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;kerberos5&lt;/cell&gt;
        &lt;cell&gt;Kerberos5 (Heimdal) package.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;lib&lt;/cell&gt;
        &lt;cell&gt;System libraries.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;libexec&lt;/cell&gt;
        &lt;cell&gt;System daemons.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;release&lt;/cell&gt;
        &lt;cell&gt;Release building Makefile &amp;amp; associated tools.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;rescue&lt;/cell&gt;
        &lt;cell&gt;Build system for statically linked /rescue utilities.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;sbin&lt;/cell&gt;
        &lt;cell&gt;System commands.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;secure&lt;/cell&gt;
        &lt;cell&gt;Cryptographic libraries and commands.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;share&lt;/cell&gt;
        &lt;cell&gt;Shared resources.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;stand&lt;/cell&gt;
        &lt;cell&gt;Boot loader sources.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;sys&lt;/cell&gt;
        &lt;cell&gt;Kernel sources (see sys/README.md).&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;targets&lt;/cell&gt;
        &lt;cell&gt;Support for experimental &lt;code&gt;DIRDEPS_BUILD&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;tests&lt;/cell&gt;
        &lt;cell&gt;Regression tests which can be run by Kyua. See tests/README for additional information.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;tools&lt;/cell&gt;
        &lt;cell&gt;Utilities for regression testing and miscellaneous tasks.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;usr.bin&lt;/cell&gt;
        &lt;cell&gt;User commands.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;usr.sbin&lt;/cell&gt;
        &lt;cell&gt;System administration commands.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;For information on synchronizing your source tree with one or more of the FreeBSD Project's development branches, please see FreeBSD Handbook.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45997212</guid><pubDate>Thu, 20 Nov 2025 20:24:42 +0000</pubDate></item><item><title>Over-regulation is doubling the cost</title><link>https://rein.pk/over-regulation-is-doubling-the-cost</link><description>&lt;doc fingerprint="698010174ea1b620"&gt;
  &lt;main&gt;
    &lt;p&gt;After building a software company to a multi-billion dollar exit, I made the jump to hardware. Now I‚Äôm working on carbon removal + steel at Charm Industrial, and electric long-haul trucking with Revoy. It‚Äôs epically fun to be building in the real world, but little did I expect that more than half the cost of building a hardware company would come from regulatory bottlenecks. Despite a huge push for climate fixes and the bipartisan geopolitical desire to bring industry back to the USA, I‚Äôve been shocked to find that the single biggest barrier‚Äîby far‚Äîis over-regulation from the massive depth of bureaucracy.&lt;/p&gt;
    &lt;p&gt;Hardtech companies of all flavors are being forced to burn through limited capital while they wait for regulatory clarity and/or permits. This creates a constant cycle of cost increases that ultimately flows to consumers, it lowers investment in the US manufacturing and industrial base, it delays innovative new hardware getting into the hands of consumers and businesses, and at the end of the day, it leaves us all worse off, stuck with a quality of life pegged to technology developed decades ago.&lt;/p&gt;
    &lt;p&gt;Regulatory delays and bottlenecks have added millions of pounds of pollutants like PM2.5, NO‚Çì and CO‚ÇÇ to our air from the continuation of business as usual, instead of the deployment of clean technologies from my two hardtech efforts alone. While CO‚ÇÇ is a long-term climate issue, PM2.5 and NO‚Çì are immediate major drivers of asthma and excess morbidity. Both operations have high bipartisan appeal‚Äîand we‚Äôve never been denied a permit‚Äîbecause we‚Äôre fundamentally cleaning up things that matter to everyone: dirty air, wildfires, orphaned oil wells. Revoy is also helping deflate the cost of long-haul freight. But none of that has made getting freedom to operate easy. For creative new technologies the default answer is ‚Äúno‚Äù because there isn‚Äôt a clear path to permitting at all, and figuring out that path itself takes years ‚Äî time that startups can‚Äôt afford to wait.&lt;/p&gt;
    &lt;p&gt;Regulation obviously has a critical role in protecting people and the environment, but the sheer volume, over-specificity and sometimes ambiguity of those same regulations is now actively working against those goals! We‚Äôre unintentionally blocking the very things that would improve our environment. We‚Äôve become a society that blocks all things, and we need to be a society that builds great things every day. The rest of this article gets very specific about the astronomical costs regulations are imposing on us as a society, and the massive positive impact that could be unleashed by cutting back regulation that is working against new, cost-saving, creative technology that could also be making people and the environment healthy again.&lt;/p&gt;
    &lt;p&gt;To make it concrete: both Charm and Revoy are capital-efficient hardtech companies, but Charm will spend low hundreds of millions to get to breakeven, and Revoy will spend tens of millions. In both cases, more than half of the total cost of building each company has gone to counterproductive regulatory burden. I‚Äôm hellbent on pushing through these barriers, but the unspoken reality is that our regulatory morass is the deathbed of thousands of hardtech companies that could be drastically improving our lives. We must unleash them.&lt;/p&gt;
    &lt;head rend="h2"&gt;$300M in Societal Cost &amp;amp; $125M in Burden for Charm&lt;/head&gt;
    &lt;p&gt;Charm produces and delivers verified carbon removal to companies like Google, Microsoft and JPMorgan. Charm‚Äôs breakthrough was realizing that you could take CO‚ÇÇ captured in farm &amp;amp; forestry plant residues, convert it into a carbon-rich, BBQ sauce-like liquid (it‚Äôs literally the smoke flavor in BBQ sauce), and inject it into old oil wells to permanently remove carbon from the atmosphere. This has all kinds of co-benefits like reducing the massive overburden of wildfire fuels, cleaning up &amp;amp; plugging nasty orphaned oil wells, and improving PM2.5 and NO‚Çì air quality by avoiding that biomass being burned instead.&lt;/p&gt;
    &lt;p&gt;And yet‚Ä¶ there was a hangup: what kind of injection well is this? Should it be permitted as a Class I disposal, Class II oilfield disposal, or Class V experimental? This question on permitting path took four years to answer. Four years to decide which path to use, not even the actual permit! It took this long because regulators are structurally faced with no upside, only downside legal risk in taking a formal position on something new. Even when we‚Äôd done an enormous amount of lab and field work with bio-oil to understand its safety and behavior at surface and subsurface conditions. A regulator faces little cost to moving incredibly cautiously, but a major cost if they approve something that triggers activist pushback.&lt;/p&gt;
    &lt;p&gt;In the end, we‚Äôre grateful that‚Äîeventually‚Äîa state regulator took the reins and reviewed, managed, and issued the first-ever Class V bio-oil sequestration permit, through what was still an incredibly complex and detailed 14-month review process.&lt;/p&gt;
    &lt;p&gt;Now imagine that, instead of the 5.5 years from first contact to issued permit, it had only taken the 6 months it actually required to get everyone across the regulatory establishment to agree on a Class V pathway, we would have had 5 additional years operating the well. That‚Äôs the equivalent, from our real supply chain, of sinking at least 30,000 tonnes of carbon per year at $600/tonne. Looking only at this one aspect, this delay came with a $90M price tag for Charm. We‚Äôve also spent untold millions on regulatory affairs at all levels of government, not to mention the missed acceleration in sales, and other direct hard costs spent in R&amp;amp;D and processing bio-oil for inefficient and expensive injection into salt caverns instead.&lt;/p&gt;
    &lt;p&gt;But the public health burden created by this regulatory slowness is where it gets really crazy. This one regulatory delay meant we all got subjected to decreased air quality from an additional 30,000 tonnes per year of pile burning. The resulting particulate emissions alone are estimated to have caused a mindblowing $40m/year in healthcare costs. This is $200M in additional healthcare burden over those five years, mostly borne by Medicare and Medicaid. There are additional costs to NO‚Çì emissions and more that take it to $300M.&lt;/p&gt;
    &lt;p&gt;In total, the total cost to society of this single regulatory delay will be about $400M: $120-150M of unnecessary cost to Charm, and the bulk of it‚Äî$300M or so‚Äîborne by the public in healthcare costs. I‚Äôm not sharing these numbers to complain or make excuses; Charm is still on the path to having a huge impact and we‚Äôre among the lucky few that can survive these delays. What pains me most is the 5 years of lost carbon removal and pollutant reduction, and the compounding effect that has on all our health and healthcare costs. Over-regulation is now working against the very things it‚Äôs intended to protect.&lt;/p&gt;
    &lt;p&gt;Regulators do their absolute best with the system they have, but the combined effects of: (1) extremely detailed and complex regulation, (2) chaotic budgets and understaffing that disrupt an efficient process, and (3) endless lawsuits against regulators since 1970s-era Naderism have created an atmosphere of fear. If we want to solve the climate crisis, build abundance, lower costs, and generate wealth for all, this has to change. We need to delete and simplify reams of regulations. We need to pay regulators well, and we need to trust our regulators to operate quickly and decisively by putting reasonable limits on endless activist legal challenges.&lt;/p&gt;
    &lt;head rend="h2"&gt;&amp;gt;$25M in Unnecessary Burden for Revoy&lt;/head&gt;
    &lt;p&gt;Revoy‚Äôs breakthrough was realizing that you could lower long-haul freight costs and electrify long-haul semi trucks by leaving the diesel tractor in place and dropping an electric powertrain onto the back of the semi. Today, we boost semis from 7 mpg to 120 mpg, driving a 94% reduction in fuel consumption. This slashes emissions that negatively impact both air quality and climate.&lt;/p&gt;
    &lt;p&gt;And yet again‚Ä¶ a hangup: what exactly is this electric doohickey? Is it a truck? A trailer? Something else? It was clear from the regulations that it was a ‚Äúconverter dolly‚Äù. But getting complete alignment on that simple fact across an alphabet soup of government agencies spanning both federal and state‚ÄîNHTSA, FMCSA, FHWA, state transit authorities, air quality management districts, state DMVs, highway patrols and more‚Äîtook years.&lt;/p&gt;
    &lt;p&gt;A ‚Äúpowered converter dolly‚Äù isn‚Äôt even a new thing! Here‚Äôs one from the sixties that ran on diesel to help trucks get over mountain passes:&lt;/p&gt;
    &lt;p&gt;There were some bright spots. The Federal Motor Carrier Safety Administration (FMCSA) and the National Highway Transportation Safety Administration (NHTSA) quickly converged on informal definitional clarity, and then eventually a Highway Patrol Captain who was eager to get innovative electric vehicles on the road pushed it through with a state DMV to register the first four Revoys. But bringing along the rest of the agencies, and the rest of the states, was not fast. It delayed deployments, soaked up hundreds of thousands of dollars of legal and lobbyist time (not to mention all the corresponding time on the government side that all of us taxpayers have to bear), and maybe most importantly‚Ä¶ even with a formal memo from the Federal DOT, it is still not 100% resolved in some states.&lt;/p&gt;
    &lt;p&gt;As one example, one state agency has asked Revoy to do certified engine testing to prove that the Revoy doesn‚Äôt increase emissions of semi trucks. And that Revoy must do this certification across every single truck engine family. It costs $100,000 per certification and there are more than 270 engine families for the 9 engines that our initial partners use. That‚Äôs $27,000,000 for this one regulatory item. And keep in mind that this is to certify that a device‚Äîwhose sole reason for existence is to cut pollution by &amp;gt;90%, and which has demonstrably done so across nearly 100,000 miles of testing and operations‚Äîis not increasing the emissions of the truck. It‚Äôs a complete waste of money for everyone.&lt;/p&gt;
    &lt;p&gt;And that $27M dollar cost doesn‚Äôt include the cost to society. This over-regulation will delay deployment of EV trucks by years, increasing NO‚Çì and PM 2.5 air pollution exposure for many of society‚Äôs least well-off who live near freeways. The delayed deployment will also increase CO‚ÇÇ emissions that threaten the climate and environment. Revoy‚Äôs Founder (Ian Rust) and I actually disagree on what exactly it is about the regulatory environment that needs to change, but we agree it‚Äôs completely broken and hurting both people and the planet.&lt;/p&gt;
    &lt;p&gt;In every interaction I have with regulators, I‚Äôm reminded that they‚Äôre good people doing god‚Äôs work operating in a fundamentally broken system. A regulatory system that structurally insists on legalistic, ultra-extreme caution is bound to generate a massive negative return for society.&lt;/p&gt;
    &lt;p&gt;If we had a regulatory system that could move fast to experiment with creative new technologies, we‚Äôd live in a world where our environment gets cleaned up faster, where awesome new hardware was constantly improving our lives by making things better and cheaper, and where large-scale hardtech innovation happened here at home in the USA, not in China.&lt;/p&gt;
    &lt;p&gt;As we collectively work to build more manufacturing capacity at home and build the next wave of technologies to power the economy, we need to grapple with the real bottlenecks holding us back. I hope other hardtech founders will publicly share more of their stories as well (the stories I‚Äôve heard in private would shock you). Props to Blake Scholl for doing so.&lt;/p&gt;
    &lt;p&gt;We need a come-to-jesus about regulatory limits, timelines, and scope. Yes, we need basic and strong protections for clear harms, but we need to unleash every hardworking American, not just a few companies with massive funding, to invent and build hardware again. We need to combine many approaches to get there: expedited reviews for new technology, freedom to operate by default, permits by right-not-process, deleting as many regulatory steps as possible, and more. CA YIMBY‚Äôs successful push to pass a deluge of housing acceleration laws in the past two years could serve as a model. America building things again is the foundation of a prosperous, powerful, and clean America.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45999038</guid><pubDate>Thu, 20 Nov 2025 22:58:06 +0000</pubDate></item><item><title>Olmo 3: Charting a path through the model flow to lead open-source AI</title><link>https://allenai.org/blog/olmo3</link><description>&lt;doc fingerprint="ad5e3b78241b8f9a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Olmo 3: Charting a path through the model flow to lead open-source AI&lt;/head&gt;
    &lt;p&gt;November 20, 2025&lt;/p&gt;
    &lt;p&gt;Ai2&lt;/p&gt;
    &lt;p&gt;Language models are often treated as snapshots‚Äîbrief captures of a long and carefully curated development process. But sharing only the end result obscures the rich context needed to modify, adapt, and extend a model's capabilities. Many meaningful adjustments require integrating domain-specific knowledge deep within the development pipeline, not merely at the final stage. To truly advance open AI development and research, the entire model flow ‚Äì not just its endpoint ‚Äì should be accessible and customizable. The model flow is the full lifecycle of an LM: every stage, checkpoint, dataset, and dependency required to create and modify it. By exposing this complete process, the goal is to engender greater trust and enable more effective adaptation, collaboration, and innovation.&lt;/p&gt;
    &lt;p&gt;With today's release of Olmo 3, we're empowering the open source community with not only state-of-the-art open models, but the entire model flow and full traceability back to training data.&lt;/p&gt;
    &lt;p&gt;At its center is Olmo 3-Think (32B), the best fully open 32B-scale thinking model that for the first time lets you inspect intermediate reasoning traces and trace those behaviors back to the data and training decisions that produced them. Olmo 3 is a family of compact, dense models at 7 billion and 32 billion parameters that can run on everything from laptops to research clusters.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Olmo 3-Base (7B, 32B) is our most powerful base model yet. When evaluated on our expanded, diverse evaluation suite, Olmo 3-Base delivers the strongest performance among fully open base models ‚Äì where training data, code, and weights are all publicly available, like Stanford's Marin and Swiss AI's Apertus ‚Äì and achieves competitive performance with some of the best open-weights base models of comparable size and architecture, including Qwen 2.5 and Gemma 3. Achieving strong results in programming, reading comprehension, and math problem solving, Olmo 3-Base maintains performance at extended context lengths (~up to 65K tokens)‚Äîproviding a versatile foundation for continued pretraining, targeted fine-tuning, and reinforcement learning and making it easy to build in specialized capabilities like reasoning, tool use (function calling), and instruction following through post-training.&lt;/item&gt;
      &lt;item&gt;Olmo 3-Think (7B, 32B) is our flagship post-trained reasoning set built on Olmo 3-Base. At a time when few organizations are releasing truly open models at this scale, Olmo 3-Think (32B) serves as a workhorse for RL research, long-horizon reasoning, and other advanced experiments that require substantial compute. On our suite of reasoning benchmarks (discussed below), it's the strongest fully open thinking model we're aware of, narrowing the gap to the best open-weight models of similar scale ‚Äì such as Qwen 3 32B ‚Äì while training on roughly 6x fewer tokens. Olmo 3-Think (7B) brings the same design and training approach to an even more efficient form factor, surfacing intermediate thinking steps for complex prompts while making open, inspectable reasoning accessible on more modest hardware.&lt;/item&gt;
      &lt;item&gt;Olmo 3-Instruct (7B) is a chat and quick-response focused post-train of Olmo 3-Base that handles multi-turn, instruction-following, tool use, and more. In our evaluations, it matches or outperforms open-weight models including Qwen 2.5, Gemma 3, and Llama 3.1, and narrows the gap with Qwen 3 model families at a similar scale‚Äîdelivering a strong, fully open alternative for high-quality conversational and tool-using agents.&lt;/item&gt;
      &lt;item&gt;Olmo 3-RL Zero (7B), is a fully open reinforcement learning pathway built on Olmo 3-Base, designed to bootstrap complex reasoning behaviors and enable clear benchmarking of RL algorithms. We release four series of checkpoints from domain-focused training on math, code, instruction following, and general chat, enabling careful study of reinforcement learning with verifiable rewards (RLVR).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Instead of a single set of frozen weights, Olmo 3 offers multiple, fully documented paths through development: the Instruct path for everyday chat and tool use, the RL Zero path for RL experimentation from base models, and the Think/reasoning path for models that leverage inference-time scaling to unlock complex reasoning and agentic behaviors. Each path is a concrete example of how to shape behavior from the same base model, and you‚Äôre free to fork or remix them‚Äîstart with Olmo 3-Base, explore your own supervised fine-tuning (SFT) or direct preference optimization (DPO) recipe for instruct-style use cases, or plug in a new RL objective to probe different tradeoffs. The flow itself becomes a rich, reusable object‚Äînot just a record of how we built Olmo 3, but a scaffold for how you can build your own systems.&lt;/p&gt;
    &lt;p&gt;Explore the Model Flow&lt;/p&gt;
    &lt;p&gt;Click on any stage to learn more about it and download artifacts.&lt;/p&gt;
    &lt;p&gt;The Olmo 3 checkpoints we're releasing represent our initial paths targeting our goals around reasoning, tool use, and general capabilities ‚Äì we have exciting plans for other ways to leverage Olmo 3-Base 32B. But because we're releasing the entire flow, you can intervene at any point: swap in domain-specific data during mid-training, adjust post-training for your use case, or build on an earlier checkpoint that better suits your needs.&lt;/p&gt;
    &lt;p&gt;As with Olmo and Olmo 2, we‚Äôre releasing all components of the Olmo 3 flow ‚Äì data, code, model weights, and checkpoints ‚Äì under permissive open source licenses.&lt;/p&gt;
    &lt;p&gt;Try Olmo 3 | Download the models &amp;amp; data | Read the report&lt;/p&gt;
    &lt;head rend="h3"&gt;Strong performance across the board&lt;/head&gt;
    &lt;p&gt;We run the Olmo 3 checkpoints through a broad, updated benchmark suite, grouping dozens of industry-standard tasks (plus a few new ones we introduce) into several capability clusters. Together, the clustered suite and these held-out tasks give us a capability profile of Olmo 3‚Äîa clear picture of how well it solves math problems, codes, uses tools, answers general-knowledge questions, and more.&lt;/p&gt;
    &lt;p&gt;At a high level, the Olmo 3 family delivers the strongest fully open base and thinking models we‚Äôre aware of. Olmo 3-Base 32B outperforms other fully open base models, and Olmo 3-Think 32B emerges as the strongest fully open thinking model.&lt;/p&gt;
    &lt;p&gt;Our results were made possible by rigorous data curation at every stage of training, a carefully designed training recipe for each model, and a set of new algorithmic and infrastructure advances across data processing, training, and reinforcement learning. We also introduce an enhanced reinforcement learning framework that guides the development of our models and is particularly essential for our thinking models. To design the training recipe and coordinate targeted improvements across a wide range of capabilities at each stage of the model training pipeline, our development framework balances distributed innovation with centralized evaluation.&lt;/p&gt;
    &lt;p&gt;Olmo 3-Base, with a training pipeline that first focuses on broad coverage over diverse text, code, and math, then concentrates on harder distributions to sharpen programming, quantitative reasoning, and reading comprehension, is clearly the strongest set of fully open base models in our evaluations. It‚Äôs also arguably the best 32B model in the entire ecosystem of models with open weights, performing impressively in programming, reading comprehension, math problem solving, and long-context benchmarks like RULER, which tests information retrieval from lengthy texts. Olmo 3-Base (7B) and Olmo 3-Base (32) maintain quality at extended context lengths and integrate cleanly with RL workflows, providing a robust foundation for continued pretraining and post-training.&lt;/p&gt;
    &lt;p&gt;Olmo 3-Think, which turns the Base into a reasoning model by training on multi-step problems spanning math, code, and general problem solving, then running the thinking SFT ‚Üí thinking DPO ‚Üí RLVR model flow to elicit high-quality reasoning traces, competes with or exceeds several open-weight reasoning models of similar sizes. On math benchmarks, Olmo 3-Think (7B) matches Qwen 3 8B on MATH and comes within a few points on AIME 2024 and 2025, and also leads all comparison models on HumanEvalPlus for coding‚Äîperforming strongly on MBPP and LiveCodeBench to demonstrate particular strength in code-intensive reasoning. On broader reasoning tasks like BigBench Hard and AGI Eval English, Olmo 3-Think (7B) remains competitive with Qwen 3 8B reasoning and Qwen 3 VL 8B Thinker while staying fully open and slightly smaller.&lt;/p&gt;
    &lt;p&gt;For the 32B model, Olmo 3-Think scales these trends up and becomes one of the strongest fully open reasoning models in its class. Olmo 3-Think (32B) either wins or sits within roughly two points of the best open-weight model on MATH, OMEGA, BigBenchHard, HumanEvalPlus, PopQA, and IFEval. It ties Qwen 3 VL 32B Thinking for the top score on the OMEGA suite while staying clearly ahead of Gemma 3 27B Instruct and competitive with DeepSeek R1 Distill 32B on math and reasoning. On broader knowledge and QA, Olmo 3-Think (32B) is effectively neck-and-neck with the Qwen 3 models on PopQA. And in instruction following, Olmo 3-Think (32B) tops this subset on IFEval and remains solid on IFBench and AlpacaEval 2 LC‚Äîoffering a strong default for reasoning workloads at the 32B scale.&lt;/p&gt;
    &lt;p&gt;Olmo 3-Instruct, which produces shorter sequences than the corresponding Olmo 3-Think models to improve inference efficiency and is designed to focus on general chat, tool use, and synthetic data generation, outperforms comparably-sized open-weight models. Olmo 3-Instruct ties or surpasses Qwen 2.5, Gemma 3, and Llama 3.1 in our evaluations, and competes with the Qwen 3 family at similar scale, delivering strong function calling performance and instruction-following capabilities in a fully open 7B model.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Olmo 3 architecture and training stages&lt;/head&gt;
    &lt;p&gt;Olmo 3 uses a decoder-only transformer architecture and multi-stage training pipeline. Pretraining runs in three stages‚Äîan initial large-scale training run that builds broad capabilities; a mid-training phase that focuses on harder material like math, code, and reading comprehension; and a final long-context extension stage that trains the model on very long documents. Together with architectural enhancements, this yields a more capable, efficient base for the Olmo 3 family.&lt;/p&gt;
    &lt;p&gt;Post-training then specializes the pretrained model for different use cases. Building on Olmo 2, each pathway follows a three-stage recipe ‚Äì SFT, preference tuning with DPO, and RLVR ‚Äì but in Olmo 3, we expose this as a fully documented model flow with complete customization over each training stage and dataset mix.&lt;/p&gt;
    &lt;p&gt;Instead of releasing only the final weights, we provide checkpoints from each major training milestone: the base pretrained model, the mid-trained model after targeted skill enhancement, the long-context-extended version, plus post-training checkpoints for the Olmo 3-Think, Olmo 3-Instruct, and Olmo 3-RL Zero flows. You can study how capabilities emerge over time, run ablations on specific stages, and fork the model at whatever point best fits your data, compute, and goals.&lt;/p&gt;
    &lt;head rend="h3"&gt;Expanded training data&lt;/head&gt;
    &lt;p&gt;Compared to Olmo 2, we scaled data collection and significantly strengthened our dataset curation methods. Continuing our commitment to full transparency, we‚Äôre releasing several new, higher-quality datasets that cover every stage of base model training and post-training‚Äîfrom initial learning to specialized skills like complex reasoning and long-context understanding. This means anyone can see exactly what data shaped the model‚Äôs capabilities, reproduce our results, and reuse these datasets to train their own AI systems.&lt;/p&gt;
    &lt;p&gt;Olmo 3 is pretrained on Dolma 3, a new ~9.3-trillion-token corpus drawn from web pages, science PDFs processed with olmOCR, codebases, math problems and solutions, and encyclopedic text. From this pool, we construct Dolma 3 Mix, a 5.9-trillion-token (~6T) pretraining mix with a higher proportion of coding and mathematical data than earlier Dolma releases, plus much stronger decontamination via extensive deduplication, quality filtering, and careful control over data mixing. We follow established web standards in collecting training data and don‚Äôt collect from sites that explicitly disallow it, including paywalled content.&lt;/p&gt;
    &lt;p&gt;On top of this, we introduce two Dolma 3-based mixes for later stages of base model training. Dolma 3 Dolmino is our mid-training mix: 100B training tokens sampled from a ~2.2T-token pool of high-quality math, science, code, instruction-following, and reading-comprehension data, including reasoning traces that also enable RL directly on the base model. Dolma 3 Longmino is our long-context mix: ~50B training tokens drawn from a 639B-token pool of long documents combined with mid-training data to teach Olmo 3 to track information over very long inputs (like reports, logs, and multi-chapter documents).&lt;/p&gt;
    &lt;p&gt;We also introduce Dolci, a new post-training data suite tailored specifically for reasoning, tool use, and instruction following. Dolci provides separate mixes for each stage of post-training: SFT, DPO, and RLVR. For SFT, Dolci aggregates state-of-the-art datasets that advance step-by-step reasoning, tool use, and high-quality conversational behavior; for DPO, it supplies high-quality contrastive preference data; and for RL, it includes hard, diverse prompts across math, coding, instruction following, and general chat.&lt;/p&gt;
    &lt;p&gt;Together, Dolma 3 and Dolci give Olmo 3 a fully open data curriculum from first token to final post-trained checkpoint.&lt;/p&gt;
    &lt;head rend="h3"&gt;Efficient training stack&lt;/head&gt;
    &lt;p&gt;We pretrained Olmo 3 on a cluster of up to 1,024 H100 GPUs; we achieved training throughput of 7.7K tokens per device per second for Olmo 3-Base (7B). We mid-trained on 128 H100 GPUs, and post-trained on a set of 256 H100s.&lt;/p&gt;
    &lt;p&gt;For Olmo 3, building on the work we did for Olmo 2, we were able to significantly improve the efficiency of our post-training code. By moving SFT from Open Instruct (our post-training codebase, prioritizing flexibility) to Olmo Core (our pretraining codebase, designed to maximize efficiency), we increased throughput (tokens/second) by 8x. Similarly, by incorporating in-flight weight updates, continuous batching, and a lot of threading improvements, we made our RL training 4x more efficient‚Äîresulting in training runs that are significantly cheaper and faster.&lt;/p&gt;
    &lt;p&gt;A note on our 32B models: We believe 32B sits in a sweet spot for research and tinkering. 32B models are big enough to support strong, competitive performance, but still small enough that a wide audience can fine-tune and deploy them on accessible hardware.&lt;/p&gt;
    &lt;p&gt;For more details, including ablations, please read our technical report.&lt;/p&gt;
    &lt;head rend="h3"&gt;Transparency at the core&lt;/head&gt;
    &lt;p&gt;A core goal of Olmo 3 is not just to open the model flow, but to make it actionable for people who want to understand and improve model behavior. Olmo 3 integrates with OlmoTrace, our tool for tracing model outputs back to training data in real time.&lt;/p&gt;
    &lt;p&gt;For example, in the Ai2 Playground, you can ask Olmo 3-Think (32B) to answer a general-knowledge question, then use OlmoTrace to inspect where and how the model may have learned to generate parts of its response. This closes the gap between training data and model behavior: you can see not only what the model is doing, but why‚Äîand adjust data or training decisions accordingly.&lt;/p&gt;
    &lt;p&gt;To further promote transparency and explainability, we‚Äôre making every training and fine-tuning dataset available for download, all under a permissive license that allows for custom deployment and reuse. The datasets come in a range of mixes to accommodate different storage and hardware constraints, from several billion tokens all the way up to 6 trillion.&lt;/p&gt;
    &lt;p&gt;Our new tooling for data processing allows you to de-contaminate, tokenize, and de-duplicate data in the same way we did for Olmo 3‚Äôs corpora. All the tooling is open source, enabling you to replicate our training curves or run controlled ablations across data mixes and objectives.&lt;/p&gt;
    &lt;p&gt;Our Olmo utilities and software cover the whole development cycle:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Olmo-core is a state-of-the-art framework for distributed model training.&lt;/item&gt;
      &lt;item&gt;Open Instruct is our post-training pipeline.&lt;/item&gt;
      &lt;item&gt;datamap-rs is a pure-Rust toolkit for large-scale cleaning.&lt;/item&gt;
      &lt;item&gt;duplodocus for ultra-efficient fuzzy de-duplication.&lt;/item&gt;
      &lt;item&gt;OLMES is a toolkit for reproducible evals. It includes our brand-new eval collection OlmoBaseEval, which we used for Olmo 3 base model development.&lt;/item&gt;
      &lt;item&gt;decon removes test sets from training data.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Importantly, our tooling allows you to instrument complex tasks and analyze intermediate traces to understand where the models succeed‚Äîor struggle. Because the Olmo 3 data recipes, training pipeline, and checkpoints are open, independent teams can connect model behavior back to measurable properties.&lt;/p&gt;
    &lt;head rend="h3"&gt;Ready to deploy and use&lt;/head&gt;
    &lt;p&gt;Together, the Olmo 3 family makes it easier to build trustworthy features quickly, whether for research, education, or applications. By making every development step available and inspectable, we're enabling entirely new categories of research. You can run experiments on any training phase, understand exactly how different techniques contribute to model capabilities, and build on our work at whatever stage makes sense for your project.&lt;/p&gt;
    &lt;p&gt;For scientists, the fully open flow exposes the model‚Äôs inner workings, so you can instrument experiments across coding, reasoning, RL, and tool use.&lt;/p&gt;
    &lt;p&gt;If you care about AI you can study, audit, and improve, Olmo 3 is for you. Try the demos in the Ai2 Playground, explore the documentation, and build on the released weights and checkpoints. Then tell us what you discover‚Äîwe invite the community to validate, critique, and extend our findings.&lt;/p&gt;
    &lt;p&gt;True openness in AI isn't just about access‚Äîit's about trust, accountability, and shared progress. We believe the models shaping our future should be fully inspectable, not black boxes. Olmo 3 represents a different path: one where anyone can understand, verify, and build upon the AI systems that increasingly influence our world. This is what open-first means‚Äînot just releasing weights, but sharing the complete knowledge needed to advance AI responsibly: the flow.&lt;/p&gt;
    &lt;p&gt;Deep dive with Olmo lead researchers Hanna Hajishirzi and Noah Smith on how ‚Äì and why ‚Äì we built Olmo 3, and what comes next:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46001889</guid><pubDate>Fri, 21 Nov 2025 06:50:14 +0000</pubDate></item><item><title>The Qtile Window Manager: A Python-Powered Tiling Experience</title><link>https://tech.stonecharioteer.com/posts/2025/qtile-window-manager/</link><description>&lt;doc fingerprint="360d3a3f0397bf91"&gt;
  &lt;main&gt;
    &lt;p&gt;I‚Äôve been an avid user of XFCE for a very long time. I‚Äôm fond of its lightweight nature, and I feel productive in it. But when I first discovered tiling window managers, I was mind-blown. I‚Äôve wanted to use one forever.&lt;/p&gt;
    &lt;p&gt;My first experience with one was a few years ago, before I understood how Linux window managers worked. I couldn‚Äôt yet wrap my head around the fact that you could install more than one window manager and choose what you wanted during login. I think I‚Äôve grown since then. I faintly remember trying to install i3wm, the most famous tiling window manager at the time. I think I was taken aback by the black screen, and more so with the mouse pointer which just said &lt;code&gt;X&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;A year or so ago, I came across DistroTube‚Äôs Youtube Channel, where he talks about xmonad, the tiling window manager that‚Äôs written in Haskell. While I‚Äôve been wanting to learn Haskell for a very long time, my career trajectory hasn‚Äôt afforded me the chance to learn it so far.&lt;/p&gt;
    &lt;p&gt;I‚Äôve since moved jobs and completely shifted to Linux everywhere. I no longer want to use a non-linux machine ever again. I‚Äôm sure there‚Äôs a whole blog article about how much of a Linux person I‚Äôve become in the past year or so, somewhere in me.&lt;/p&gt;
    &lt;p&gt;Last week, I came across dt‚Äôs video on Qtile, the tiling window manager written entirely in Python. Now that was truly enticing. I‚Äôm adept enough in Python to be able to manage complex configurations all on my own. And after skimming through the documentation, I spent a day modularizing the default qtile config since the default config gives me goosebumps, and not in a good way.&lt;/p&gt;
    &lt;p&gt;In this article, I‚Äôll describe what I did, and how I went about it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Installing Qtile&lt;/head&gt;
    &lt;p&gt;I decided to abstract away the entire configuration so that it doesn‚Äôt live in my dotfiles repository. I wanted to create a python library for myself so that it would have a bunch of utilities for my own consumption.&lt;/p&gt;
    &lt;p&gt;Additionally, I disagreed with the default way of installing Qtile. As a principle, I never &lt;code&gt;sudo pip install&lt;/code&gt; anything. Instead, I asked my friend Karthikeyan Singaravel, who is a Python core developer, and he recommended using the deadsnakes PPA for Ubuntu to install any version of Python that I chose. I tried compiling python 3.10 myself, installing to &lt;code&gt;/opt/qtile/&lt;/code&gt; using &lt;code&gt;configure --prefix /opt/qtile/&lt;/code&gt; during the configuration stage of the source code. However, I admit that using &lt;code&gt;deadsnakes&lt;/code&gt; is a far better idea since I could create a virtual environment based on &lt;code&gt;python3.10&lt;/code&gt; into &lt;code&gt;/opt/qtile/&lt;/code&gt; instead. I had to change the owner of the folder to my user account. Note that I could store the virtual environment in my home folder and just use that, but I wanted to isolate this outside of my home folder.&lt;/p&gt;
    &lt;p&gt;So, I installed &lt;code&gt;python3.10-full&lt;/code&gt; and &lt;code&gt;python3.10-dev&lt;/code&gt; (the development header files are necessary for building some of the dependencies of &lt;code&gt;qtile&lt;/code&gt;), and I created a virtual environment using the &lt;code&gt;venv&lt;/code&gt; module in &lt;code&gt;/opt/qtile&lt;/code&gt;. Then, I changed the owner of the folder to my regular user account.&lt;/p&gt;
    &lt;p&gt;Then, it was time to install qtile.&lt;/p&gt;
    &lt;p&gt;Since I use the fish shell, I had to &lt;code&gt;source activate /opt/qtile/bin/activate.fish&lt;/code&gt; to activate the virtual environment. And then I followed up by installing &lt;code&gt;qtile&lt;/code&gt;. I didn‚Äôt pick a version right away, I decided to go with the latest version.&lt;/p&gt;
    &lt;p&gt;Qtile doesn‚Äôt setup an entry for your &lt;code&gt;xsessions&lt;/code&gt;, so you need to do that yourself.&lt;/p&gt;
    &lt;p&gt;I created &lt;code&gt;/usr/share/xsessions/qtile.desktop&lt;/code&gt; and filled it with the following:&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;Notice how I used the absolute path for qtile.&lt;/p&gt;
    &lt;p&gt;After this, I logged out of my previous window manager and switched to the new entry for Qtile.&lt;/p&gt;
    &lt;p&gt;On loading qtile for the first time, I was fairly surprised with the default config. It wasn‚Äôt as blank as i3wm and xmonad were. It had a panel, a helpful text field on the panel about how to start the launcher, and it was very easy to use. I was liking it already.&lt;/p&gt;
    &lt;p&gt;But I wanted to configure it so that I could mess with the design.&lt;/p&gt;
    &lt;p&gt;The first thing that bothered me was the lack of a wallpaper. I‚Äôd used nitrogen before, so I installed it and started it up, setting a wallpaper. I restarted qtile and then‚Ä¶ nothing.&lt;/p&gt;
    &lt;p&gt;That was me being silly and forgetting that Explicit is better than Implicit. Like all tiling window managers, Qtile did none of the work for us. You have to ensure that the wallpaper manager loads when Qtile is done loading. That‚Äôs where the &lt;code&gt;.xsessionrc&lt;/code&gt; file comes in.&lt;/p&gt;
    &lt;p&gt;Since nitrogen can restore a wallpaper with ease, all I needed to do was:&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;This went into the &lt;code&gt;~/.xsessionrc&lt;/code&gt; file.&lt;/p&gt;
    &lt;head rend="h2"&gt;Configuring Qtile&lt;/head&gt;
    &lt;p&gt;Qtile‚Äôs config file rests at &lt;code&gt;~/.config/qtile/config.py&lt;/code&gt;. On start, Qtile will read this file. Since this file is just Python code, that also means every single line in this file is executed.&lt;/p&gt;
    &lt;p&gt;When you look at the default config, you will notice:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;It‚Äôs about 130 lines long. Not too big.&lt;/item&gt;
      &lt;item&gt;It‚Äôs just a bunch of variable declarations.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This meant that all you needed to configure Qtile was to ensure you set the values of a few global variables in the config file. And Qtile would take care of the rest.&lt;/p&gt;
    &lt;p&gt;This was useful. All I needed to do was set some variables.&lt;/p&gt;
    &lt;p&gt;The default config constructs all these variables as it sets them, which is something I don‚Äôt recommend. Python‚Äôs error handling will not point out the right place where the error is occurring, and while Python 3.11 seeks to improve this, it‚Äôs generally not a good practice to have a long variable declaration step in your code.&lt;/p&gt;
    &lt;p&gt;For example, where the config does this:&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;If you want to reuse these objects, it‚Äôs better to just construct them separately and then use them in a panel. The same goes for reusing panels.&lt;/p&gt;
    &lt;head rend="h2"&gt;My Current Configuration&lt;/head&gt;
    &lt;p&gt;After months of tweaking and refinement, here‚Äôs what my current Qtile setup looks like. The key principles I‚Äôve followed are:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Modularity: Break down complex structures into functions&lt;/item&gt;
      &lt;item&gt;Adaptive behavior: Detect hardware and adjust accordingly&lt;/item&gt;
      &lt;item&gt;Practical shortcuts: Keybindings that make sense for daily use&lt;/item&gt;
      &lt;item&gt;Visual consistency: A cohesive color scheme and layout&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Color Scheme and Assets&lt;/head&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;I use a consistent color palette and have custom icons for different system components. The straw hat is a personal touch - a nod to One Piece!&lt;/p&gt;
    &lt;head rend="h3"&gt;Smart Mouse Movement Between Monitors&lt;/head&gt;
    &lt;p&gt;One of my favorite custom functions handles multi-monitor setups elegantly:&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;This automatically moves the mouse cursor to the center of the next monitor when I press &lt;code&gt;Super + .&lt;/code&gt;, making multi-monitor workflows much smoother.&lt;/p&gt;
    &lt;head rend="h3"&gt;Key Bindings&lt;/head&gt;
    &lt;p&gt;My keybindings follow a logical pattern:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Super + hjkl: Vim-style window navigation&lt;/item&gt;
      &lt;item&gt;Super + Shift + hjkl: Move windows around&lt;/item&gt;
      &lt;item&gt;Super + Control + hjkl: Resize windows&lt;/item&gt;
      &lt;item&gt;Super + r: Launch rofi application launcher&lt;/item&gt;
      &lt;item&gt;Super + Shift + p: Screenshot utility&lt;/item&gt;
      &lt;item&gt;Super + Shift + l: Lock screen&lt;/item&gt;
      &lt;item&gt;Super + Shift + e: Power menu&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Hardware-Aware Widgets&lt;/head&gt;
    &lt;p&gt;One of the most powerful aspects of a Python-based window manager is the ability to create intelligent, hardware-aware components:&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;These functions automatically detect hardware capabilities and adjust the interface accordingly. The battery widget only appears on laptops, and the IP address widget shows the current network status.&lt;/p&gt;
    &lt;head rend="h3"&gt;AMD GPU Integration&lt;/head&gt;
    &lt;p&gt;Since I run AMD hardware, I‚Äôve integrated &lt;code&gt;amdgpu_top&lt;/code&gt; for real-time GPU monitoring:&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;This provides real-time VRAM usage information directly in the status bar.&lt;/p&gt;
    &lt;head rend="h3"&gt;Dynamic Screen Configuration&lt;/head&gt;
    &lt;p&gt;The screen configuration automatically adapts to the number of connected monitors:&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;The main screen gets additional widgets like system tray and network information, while secondary screens get a simplified layout.&lt;/p&gt;
    &lt;head rend="h3"&gt;Startup Hooks&lt;/head&gt;
    &lt;p&gt;Qtile provides hooks for running scripts at startup:&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;This lets me separate one-time setup (like setting wallpapers) from things that should happen on every reload.&lt;/p&gt;
    &lt;head rend="h2"&gt;Current Setup in Action&lt;/head&gt;
    &lt;p&gt;My current setup includes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Top bar: Shows Linux Mint logo, current layout, groups (workspaces), task list, and system tray&lt;/item&gt;
      &lt;item&gt;Bottom bar: CPU/GPU temperatures, VRAM usage, system resources, battery (if present), IP address, and clock&lt;/item&gt;
      &lt;item&gt;Custom separators: Visual dividers using the ‚Äú‚ãÆ‚Äù character in my accent color&lt;/item&gt;
      &lt;item&gt;JetBrains Mono Nerd Font: For consistent icon rendering across all widgets&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Lessons Learned&lt;/head&gt;
    &lt;p&gt;After using Qtile daily for months, here are the key insights:&lt;/p&gt;
    &lt;head rend="h3"&gt;Python Configuration is Powerful&lt;/head&gt;
    &lt;p&gt;Having your window manager configuration in Python means you can:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Write complex logic for hardware detection&lt;/item&gt;
      &lt;item&gt;Create reusable functions and modules&lt;/item&gt;
      &lt;item&gt;Integrate with system tools seamlessly&lt;/item&gt;
      &lt;item&gt;Debug configuration issues using Python tools&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Start Simple, Iterate&lt;/head&gt;
    &lt;p&gt;Don‚Äôt try to recreate someone else‚Äôs rice immediately. Start with the defaults and gradually customize:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Basic keybindings first&lt;/item&gt;
      &lt;item&gt;Add essential widgets&lt;/item&gt;
      &lt;item&gt;Customize colors and fonts&lt;/item&gt;
      &lt;item&gt;Add advanced features like custom functions&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Hardware Awareness Matters&lt;/head&gt;
    &lt;p&gt;Modern systems vary significantly. Your configuration should adapt to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Number of monitors&lt;/item&gt;
      &lt;item&gt;Battery presence&lt;/item&gt;
      &lt;item&gt;Available sensors&lt;/item&gt;
      &lt;item&gt;Network interfaces&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Performance Considerations&lt;/head&gt;
    &lt;p&gt;Since widgets can run arbitrary Python code, be mindful of:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Update intervals for polling widgets&lt;/item&gt;
      &lt;item&gt;Error handling in custom functions&lt;/item&gt;
      &lt;item&gt;Resource usage of external commands&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Future Plans&lt;/head&gt;
    &lt;p&gt;This configuration is continuously evolving. Some planned improvements:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Custom Widgets:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;One Piece chapter release notifications&lt;/item&gt;
          &lt;item&gt;Gmail filtering widget&lt;/item&gt;
          &lt;item&gt;tmux session manager&lt;/item&gt;
          &lt;item&gt;Kubernetes context indicator&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Better Multi-Monitor Support:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Per-monitor wallpaper management&lt;/item&gt;
          &lt;item&gt;Workspace binding to specific monitors&lt;/item&gt;
          &lt;item&gt;Dynamic layout switching based on monitor configuration&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Integration Improvements:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;NordVPN status widget&lt;/item&gt;
          &lt;item&gt;NAS storage monitoring&lt;/item&gt;
          &lt;item&gt;Better notification management&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Preview&lt;/head&gt;
    &lt;p&gt;Here‚Äôs a look at what my config looks like today.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Qtile has transformed my Linux desktop experience. The ability to configure everything in Python, combined with the logical tiling approach, has made me significantly more productive. The learning curve is gentler than pure configuration-file-based window managers, and the extensibility is unmatched.&lt;/p&gt;
    &lt;p&gt;If you‚Äôre comfortable with Python and want a window manager that grows with your needs, Qtile is an excellent choice. The community is helpful, the documentation is comprehensive, and the possibilities are endless.&lt;/p&gt;
    &lt;p&gt;The configuration I‚Äôve shared represents months of daily use and refinement. It‚Äôs not just about aesthetics (though it does look good!) - it‚Äôs about creating a workspace that adapts to your hardware, workflow, and preferences seamlessly.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46002138</guid><pubDate>Fri, 21 Nov 2025 07:41:15 +0000</pubDate></item><item><title>It's hard to build an oscillator</title><link>https://lcamtuf.substack.com/p/its-hard-to-build-an-oscillator</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46002161</guid><pubDate>Fri, 21 Nov 2025 07:45:53 +0000</pubDate></item><item><title>HP and Dell disable HEVC support built into their laptops' CPUs</title><link>https://arstechnica.com/gadgets/2025/11/hp-and-dell-disable-hevc-support-built-into-their-laptops-cpus/</link><description>&lt;doc fingerprint="fe0a89d46542af41"&gt;
  &lt;main&gt;
    &lt;p&gt;Some Dell and HP laptop owners have been befuddled by their machines‚Äô inability to play HEVC/H.265 content in web browsers, despite their machines‚Äô processors having integrated decoding support.&lt;/p&gt;
    &lt;p&gt;Laptops with sixth-generation Intel Core and later processors have built-in hardware support for HEVC decoding and encoding. AMD has made laptop chips supporting the codec since 2015. However, both Dell and HP have disabled this feature on some of their popular business notebooks.&lt;/p&gt;
    &lt;p&gt;HP discloses this in the data sheets for its affected laptops, which include the HP ProBook 460 G11 [PDF], ProBook 465 G11 [PDF], and EliteBook 665 G11 [PDF].&lt;/p&gt;
    &lt;p&gt;‚ÄúHardware acceleration for CODEC H.265/HEVC (High Efficiency Video Coding) is disabled on this platform,‚Äù the note reads.&lt;/p&gt;
    &lt;p&gt;Despite this notice, it can still be jarring to see a modern laptop‚Äôs web browser eternally load videos that play easily in media players. As a member of a group for system administrators on Reddit recalled recently:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;People with older hardware were not experiencing problems, whereas those with newer machines needed to either have the HEVC codec from the Microsoft Store removed entirely from [Microsoft Media Foundation] or have hardware acceleration disabled in their web browser/web app, which causes a number of other problems / feature [degradations]. For example, no background blurring in conference programs, significantly degraded system performance ‚Ä¶&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Owners of some Dell laptops are also experiencing this, as the OEM has also disabled HEVC hardware decoding in some of its laptops. This information, however, isn‚Äôt that easy to find. For example, the product page for the Dell 16 Plus 2-in-1, which has HEVC hardware decoding disabled, makes no mention of HEVC. There‚Äôs also no mention of HEVC in the ‚ÄúNotes, cautions, and warnings‚Äù or specifications sections of the laptop‚Äôs online owner‚Äôs manual. The most easily identifiable information comes from a general support page that explains that Dell laptops only support HEVC content streaming on computer configurations with:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46002989</guid><pubDate>Fri, 21 Nov 2025 10:01:37 +0000</pubDate></item><item><title>FAWK: LLMs can write a language interpreter</title><link>https://martin.janiczek.cz/2025/11/21/fawk-llms-can-write-a-language-interpreter.html</link><description>&lt;doc fingerprint="5ff690b5ea490edb"&gt;
  &lt;main&gt;
    &lt;p&gt;After reading the book The AWK Programming Language (recommended!), I was planning to try AWK out on this year‚Äôs Advent of Code. Having some time off from work this week, I tried to implement one of the problems in it to get some practice, set up my tooling, see how hard AWK would be, and‚Ä¶ I found I‚Äôm FP-pilled.&lt;/p&gt;
    &lt;p&gt;I knew I‚Äôm addicted to the combination of algebraic data types (tagged unions) and exhaustive pattern matching, but what got me this time was immutability, lexical scope and the basic human right of being allowed to return arrays from functions.&lt;/p&gt;
    &lt;p&gt;Part 1 of the Advent of Code problem was easy enough, but for part 2 (basically a shortest path search with a twist, to not spoil too much), I found myself unable to switch from my usual functional BFS approach to something mutable, and ended up trying to implement my functional approach in AWK.&lt;/p&gt;
    &lt;p&gt;It got hairy very fast: I needed to implement:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;hashing of strings and 2D arrays (by piping to &lt;code&gt;md5sum&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;a global &lt;del rend="overstrike"&gt;set&lt;/del&gt;array of seen states&lt;/item&gt;
      &lt;item&gt;a way to serialize and deserialize a 2D array to/from a string&lt;/item&gt;
      &lt;item&gt;and a few associative arrays for retrieving this serialized array by its hash.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I was very lost by the time I had all this; I spent hours just solving what felt like accidental complexity; things that I‚Äôd take for granted in more modern languages.&lt;/p&gt;
    &lt;p&gt;Now, I know nobody said AWK is modern, or functional, or that it promises any convenience for anything other than one-liners and basic scripts that fit under a handful of lines. I don‚Äôt want to sound like I expect AWK to do any of this; I knew I was stretching the tool when going in. But I couldn‚Äôt shake the feeling that there‚Äôs a beautiful AWK-like language within reach, an iteration on the AWK design (the pattern-action way of thinking is beautiful) that also gives us a few of the things programming language designers have learnt over the 48 years since AWK was born.&lt;/p&gt;
    &lt;head rend="h2"&gt;Dreaming of functional AWK&lt;/head&gt;
    &lt;p&gt;Stopping my attempts to solve the AoC puzzle in pure AWK, I wondered: what am I missing here?&lt;/p&gt;
    &lt;p&gt;What if AWK had first-class arrays?&lt;/p&gt;
    &lt;code&gt;BEGIN {
  # array literals
  normal   = [1, 2, 3]
  nested   = [[1,2], [3,4]]
  assoc    = ["foo" =&amp;gt; "bar", "baz" =&amp;gt; "quux"]
  multidim = [(1,"abc") =&amp;gt; 999]

  five = range(1,5)
  analyze(five)
  print five  # --&amp;gt; still [1, 2, 3, 4, 5]! was passed by value
}

function range(a,b) {
  r = []
  for (i = a; i &amp;lt;= b; i++) {
    r[length(r)] = i
  }
  return r  # arrays can be returned!
}

function analyze(arr) {
  arr[0] = 100
  print arr[0]  # --&amp;gt; 100, only within this function
}
&lt;/code&gt;
    &lt;p&gt;What if AWK had first-class functions and lambdas?&lt;/p&gt;
    &lt;code&gt;BEGIN {
  # construct anonymous functions
  double = (x) =&amp;gt; { x * 2 }
  add = (a, b) =&amp;gt; { c = a + b; return c }

  # functions can be passed as values
  apply = (func, value) =&amp;gt; { func(value) }

  print apply(double,add(1,3))  # --&amp;gt; 8
  print apply(inc,5)  # --&amp;gt; 6
}

function inc(a) { return a + 1 }
&lt;/code&gt;
    &lt;p&gt;What if AWK had lexical scope instead of dynamic scope?&lt;/p&gt;
    &lt;code&gt;# No need for this hack anymore ‚Üì     ‚Üì
#function foo(a, b         ,local1, local2) {
function foo(a, b) {
  local1 = a + b
  local2 = a - b
  return local1 + local2
}

BEGIN {
  c = foo(1,2)
  print(local1)  # --&amp;gt; 0, the local1 from foo() didn't leak!
}
&lt;/code&gt;
    &lt;p&gt;What if AWK had explicit globals, and everything else was local by default?&lt;/p&gt;
    &lt;code&gt;BEGIN { global count }
END {
  foo()
  print count  # --&amp;gt; 1
  print mylocal # --&amp;gt; 0, didn't leak
}
function foo() { count++; mylocal++ }
&lt;/code&gt;
    &lt;p&gt;(This one, admittedly, might make programs a bit more verbose. I‚Äôm willing to pay that cost.)&lt;/p&gt;
    &lt;p&gt;What if AWK had pipelines? (OK, now I‚Äôm reaching for syntax sugar‚Ä¶)&lt;/p&gt;
    &lt;code&gt;BEGIN {
  result = [1, 2, 3, 4, 5] 
      |&amp;gt; filter((x) =&amp;gt; { x % 2 == 0 })
      |&amp;gt; map((x) =&amp;gt; { x * x })
      |&amp;gt; reduce((acc, x) =&amp;gt; { acc + x }, 0)

  print "Result:", result
}
&lt;/code&gt;
    &lt;head rend="h2"&gt;Making it happen&lt;/head&gt;
    &lt;quote&gt;&lt;p&gt;TL;DR:&lt;/p&gt;&lt;code&gt;Janiczek/fawk&lt;/code&gt;on GitHub&lt;/quote&gt;
    &lt;p&gt;Now for the crazy, LLM-related part of the post. I didn‚Äôt want to spend days implementing AWK from scratch or tweaking somebody else‚Äôs implementation. So I tried to use Cursor Agent for a larger task than I usually do (I tend to ask for very small targeted edits), and asked Sonnet 4.5 for a README with code examples, and then a full implementation in Python.&lt;/p&gt;
    &lt;p&gt;And it did it.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Note: I also asked for implementations in C, Haskell and Rust at the same time, not knowing if any of the four would succeed, and they all seem to have produced code that at least compiles/runs. I haven‚Äôt tried to test them or even run them though. The PRs are here.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I was very impressed‚ÄîI still am! I expected the LLM to stumble and flail around and ultimately get nothing done, but it did what I asked it for (gave me an interpreter that could run those specific examples), and over the course of a few chat sessions, I guided it towards implementing more and more of ‚Äúthe rest of AWK‚Äù, together with an excessive amount of end-to-end tests.&lt;/p&gt;
    &lt;p&gt;The only time I could see it struggle was when I asked it to implement arbitrary precision floating point operations without using an external library like &lt;code&gt;mpmath&lt;/code&gt;. It attempted to use Taylor series, but couldn‚Äôt get it right for at
least a few minutes. I chickened out and told it to &lt;code&gt;uv add mpmath&lt;/code&gt; and simplify
the interpreter code. In a moment it was done.&lt;/p&gt;
    &lt;p&gt;Other things that I thought it would choke on, like &lt;code&gt;print&lt;/code&gt; being both a
statement (with &lt;code&gt;&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;gt;&amp;gt;&lt;/code&gt; redirection support) and an expression, or
multi-dimensional arrays, or multi-line records, these were all implemented
correctly. Updating the test suite to also check for backwards compatibility
with GAWK - not an issue. Lexical scoping
and tricky closure environment behaviour - handled that just fine.&lt;/p&gt;
    &lt;head rend="h2"&gt;What now?&lt;/head&gt;
    &lt;p&gt;As the cool kids say, I have to update my priors. The frontier of what the LLMs can do has moved since the last time I tried to vibe-code something. I didn‚Äôt expect to have a working interpreter the same day I dreamt of a new programming language. It now seems possible.&lt;/p&gt;
    &lt;p&gt;The downside of vibe coding the whole interpreter is that I have zero knowledge of the code. I only interacted with the agent by telling it to implement a thing and write tests for it, and I only really reviewed the tests. I reckon this would be an issue in the future when I want to manually make some change in the actual code, because I have no familiarity with it.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;This also opened new questions for me wrt. my other projects where I‚Äôve previously run out of steam, eg. trying to implement a Hindley-Milner type system for my dream forever-WIP programming language Cara. It seems I can now just ask the LLM to do it, and it will? But then, I don‚Äôt want to fall into the trap where I am no longer able to work on the codebase myself. I want to be familiar with and able to tinker on the code. I‚Äôd need to spend my time reviewing and reading code instead of writing everything myself. Perhaps that‚Äôs OK.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Performance of FAWK might be an issue as well, though right now it‚Äôs a non-goal, given my intended use case is throwaway scripts for Advent of Code, nothing user-facing. And who knows, based on what I‚Äôve seen, maybe I can instruct it to rewrite it in Rust and have a decent chance of success?&lt;/p&gt;
    &lt;p&gt;For now, I‚Äôll go dogfood my shiny new vibe-coded black box of a programming language on the Advent of Code problem (and as many of the 2025 puzzles as I can), and see what rough edges I can find. I expect them to be equal parts ‚Äúnot implemented yet‚Äù and ‚Äúunexpected interactions of new PL features with the old ones‚Äù.&lt;/p&gt;
    &lt;p&gt;If you‚Äôre willing to jump through some Python project dependency hoops, you can try to use FAWK too at your own risk, at &lt;code&gt;Janiczek/fawk&lt;/code&gt; on
GitHub.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46003144</guid><pubDate>Fri, 21 Nov 2025 10:28:49 +0000</pubDate></item><item><title>Roundtable (YC S23) Is Hiring Two Sales Development Representatives (SDRs)</title><link>https://www.ycombinator.com/companies/roundtable/jobs/irJTEsg-sales-development-representative</link><description>&lt;doc fingerprint="394598be380aff0c"&gt;
  &lt;main&gt;
    &lt;p&gt;Proof of Human - invisible human verification&lt;/p&gt;
    &lt;p&gt;Backed by YC and founded by two Princeton Ph.D‚Äôs, Roundtable provides frictionless, continual verification for our clients‚Äô platforms. We ensure Proof of Human, tracking and stopping bots and fraud in real time to safeguard the integrity of online insights, traffic, and spend. We‚Äôre looking for an exceptional SDR to join our team.&lt;/p&gt;
    &lt;p&gt;An ideal candidate is driven and competitive, but still humble and professional. Their efforts will be instrumental in igniting the sales funnel and introducing major platforms and research firms to the future of online security. There are huge opportunities for personal and professional growth.&lt;/p&gt;
    &lt;p&gt;What you‚Äôll do:&lt;/p&gt;
    &lt;p&gt;Pipeline Generation: Research and identify key target accounts and prospects.&lt;/p&gt;
    &lt;p&gt;Outbound Engagement: Execute strategic outbound campaigns to generate qualified meetings.&lt;/p&gt;
    &lt;p&gt;Expert Positioning: Articulate the value proposition of Roundtable.&lt;/p&gt;
    &lt;p&gt;Pipeline Management: Diligently track and manage lead activity and progress.&lt;/p&gt;
    &lt;p&gt;Sales Strategy: Work with the rest of our GTM team concurrently.&lt;/p&gt;
    &lt;p&gt;Who you are:&lt;/p&gt;
    &lt;p&gt;If you‚Äôre interested in joining a rapidly growing, cutting-edge AI security company that is protecting the future of online data integrity, we‚Äôd love to chat with you.&lt;/p&gt;
    &lt;p&gt;**Equity contingent on 3 month evaluation&lt;/p&gt;
    &lt;p&gt;Roundtable is a research and deployment company building the proof-of-human layer in digital identity. Roundtable seeks to produce high-impact research and to productize this research in bot detection, fraud prevention, and continuous authentication.&lt;/p&gt;
    &lt;p&gt;Roundtable was founded in 2023 by two Princeton PhD scientists ‚Äì Mayank Agrawal and Matt Hardy. They have published in Science, PNAS, Nature Human Behaviour, and Psychological Review and are backed by Y Combinator and Brickyard Ventures.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46003686</guid><pubDate>Fri, 21 Nov 2025 12:00:02 +0000</pubDate></item><item><title>Making a Small RPG</title><link>https://jslegenddev.substack.com/p/making-a-small-rpg</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46004293</guid><pubDate>Fri, 21 Nov 2025 13:23:16 +0000</pubDate></item><item><title>EXIF orientation info in PNGs isn't used for image-orientation</title><link>https://bugzilla.mozilla.org/show_bug.cgi?id=1627423</link><description>&lt;doc fingerprint="fb52bd5796b2ff66"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;EXIF orientation info in PNGs isn't used for image-orientation: from-image&lt;/head&gt;
    &lt;head rend="h2"&gt;Categories&lt;/head&gt;
    &lt;head rend="h3"&gt;(Core :: Layout: Images, Video, and HTML Frames, defect, P3)&lt;/head&gt;
    &lt;head rend="h2"&gt;Tracking&lt;/head&gt;
    &lt;head rend="h3"&gt;()&lt;/head&gt;
    &lt;head rend="h2"&gt;People&lt;/head&gt;
    &lt;head rend="h3"&gt;(Reporter: e, Unassigned)&lt;/head&gt;
    &lt;head rend="h2"&gt;References&lt;/head&gt;
    &lt;head rend="h3"&gt;(Blocks 2 open bugs)&lt;/head&gt;
    &lt;head rend="h2"&gt;Details&lt;/head&gt;
    &lt;head rend="h3"&gt;(Keywords: parity-chrome, parity-safari, webcompat:platform-bug)&lt;/head&gt;
    &lt;head rend="h2"&gt;User Story&lt;/head&gt;
    &lt;quote&gt;user-impact-score:40&lt;/quote&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Reporter&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;&lt;head&gt;Description&lt;/head&gt;‚Ä¢&lt;p&gt;5 years ago &lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;User Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1 Safari/605.1.15&lt;/p&gt;
    &lt;p&gt;Steps to reproduce:&lt;/p&gt;
    &lt;p&gt;Go to https://ericportis.com/etc/PNG-EXIF-orientation/&lt;/p&gt;
    &lt;p&gt;Actual results:&lt;/p&gt;
    &lt;p&gt;The JPEG and PNG are rotated differently, even though they both have the same EXIF info (Orientation: Rotate 90 CW), and are both set to &lt;code&gt;image-orientation: from-image;&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;Expected results:&lt;/p&gt;
    &lt;p&gt;They should display the same.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Reporter&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;&lt;head&gt;Comment 1&lt;/head&gt;‚Ä¢&lt;p&gt;5 years ago &lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Further findings: neither Safari, Chrome, or Firefox respects exiftool's default output, which appends EXIF to the end of a PNG. This is allowed by the spec, but seems to be incompatible with progressive rendering of partially-downloaded PNGs.&lt;/p&gt;
    &lt;p&gt;Safari does respect EXIF orientation info that appears before the image data, but Firefox and Chrome do not.&lt;/p&gt;
    &lt;p&gt;https://bugs.webkit.org/show_bug.cgi?id=210021#c4&lt;lb/&gt; https://ericportis.com/etc/PNG-EXIF-orientation/shuffling-chunks/&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;&lt;head&gt;Comment 2&lt;/head&gt;‚Ä¢&lt;p&gt;5 years ago &lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;heycam: Will this be covered by any of your follow-up work related to bug 1607667?&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;&lt;head&gt;Comment 3&lt;/head&gt;‚Ä¢&lt;p&gt;5 years ago &lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Huh, I didn't even know that PNG supported orientation data. I found https://ftp-osl.osuosl.org/pub/libpng/documents/pngext-1.5.0.html#C.eXIf which defines the &lt;code&gt;eXif&lt;/code&gt; table.  The patches I'm working on don't add support for this, but it would not be too difficult to do so, at least if the table appears earlier than the image data.  (I don't think our current image loading flow would handle the image size changing as a result of the orientation data later on.)&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;&lt;head&gt;Comment 4&lt;/head&gt;‚Ä¢&lt;p&gt;5 years ago &lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Because this bug's Severity has not been changed from the default since it was filed, and it's Priority is &lt;code&gt;P3&lt;/code&gt; (Backlog,) indicating it has been triaged, the bug's Severity is being updated to &lt;code&gt;S3&lt;/code&gt; (normal.)&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;&lt;head&gt;Updated&lt;/head&gt;‚Ä¢&lt;p&gt;1 year ago &lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;&lt;head&gt;Comment 5&lt;/head&gt;‚Ä¢&lt;p&gt;1 year ago &lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;This is spec'd in PNG-3 https://www.w3.org/TR/2024/CRD-png-3-20240718/#eXIf&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;&lt;head&gt;Updated&lt;/head&gt;‚Ä¢&lt;p&gt;1 year ago &lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;&lt;head&gt;Comment 6&lt;/head&gt;‚Ä¢&lt;p&gt;11 months ago &lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;What is the expected waiting time for the issue to be resolved?&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;&lt;head&gt;Updated&lt;/head&gt;‚Ä¢&lt;p&gt;1 month ago &lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;&lt;head&gt;Comment 7&lt;/head&gt;‚Ä¢&lt;p&gt;1 month ago &lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Should be fixed by bug 1682759. If that is incorrect please re-open.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46004364</guid><pubDate>Fri, 21 Nov 2025 13:29:14 +0000</pubDate></item><item><title>Building a Minimal Viable Armv7 Emulator from Scratch</title><link>https://xnacly.me/posts/2025/building-a-minimal-viable-armv7-emulator/</link><description>&lt;doc fingerprint="aa33d81da505eee8"&gt;
  &lt;main&gt;
    &lt;head rend="h3"&gt;Tip or TLDR - I built a tiny, zero dependency armv7 userspace emulator in Rust&lt;/head&gt;
    &lt;p&gt;I wrote a minimal viable armv7 emulator in 1.3k lines of Rust without any dependencies. It parses and validates a 32-bit arm binary, maps its segments, decodes a subset of arm instructions, translates guest and host memory interactions and forwards arm Linux syscalls into x86-64 System V syscalls.&lt;/p&gt;
    &lt;p&gt;It can run a armv7 hello world binary and does so in 1.9ms (0.015ms for raw emulation without setup), while qemu takes 12.3ms (stinkarm is thus ~100-1000x slower than native armv7 execution).&lt;/p&gt;
    &lt;p&gt;After reading about the process the Linux kernel performs to execute binaries, I thought: I want to write an armv7 emulator - &lt;code&gt;stinkarm&lt;/code&gt;. Mostly to understand the ELF
format, the encoding of arm 32bit instructions, the execution of arm assembly
and how it all fits together (this will help me with the JIT for my programming
language I am currently designing). To fully understand everything: no
dependencies. And of course Rust, since I already have enough C projects going
on.&lt;/p&gt;
    &lt;p&gt;So I wrote the smallest binary I could think of:&lt;/p&gt;
    &lt;code&gt;1    .global _start  @ declare _start as a global
2_start:             @ start is the defacto entry point
3    mov r0, #161    @ first and only argument to the exit syscall
4    mov r7, #1      @ syscall number 1 (exit)
5    svc #0          @ trapping into the kernel (thats US, since we are translating)
&lt;/code&gt;
    &lt;p&gt;To execute this arm assembly on my x86 system, I need to:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Parse the ELF, validate it is armv7 and statically executable (I don‚Äôt want to write a dynamic dependency resolver and loader)&lt;/item&gt;
      &lt;item&gt;Map the segments defined in ELF into the host memory, forward memory access&lt;/item&gt;
      &lt;item&gt;Decode armv7 instructions and convert them into a nice Rust enum&lt;/item&gt;
      &lt;item&gt;Emulate the CPU, its state and registers&lt;/item&gt;
      &lt;item&gt;Execute the instructions and apply their effects to the CPU state&lt;/item&gt;
      &lt;item&gt;Translate and forward syscalls&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Sounds easy? It is!&lt;/p&gt;
    &lt;p&gt;Open below if you want to see me write a build script and a nix flake:&lt;head&gt;Minimalist arm setup and smallest possible arm binary&lt;/head&gt;&lt;/p&gt;
    &lt;p&gt;Before I start parsing ELF I‚Äôll need a binary to emulate, so lets create a build script called &lt;code&gt;bld_exmpl&lt;/code&gt; (so I can write a lot less) and nix flake, so
the asm is converted into armv7 machine code in a armv7 binary on my non armv7
system :^)&lt;/p&gt;
    &lt;code&gt; 1// tools/bld_exmpl
 2use clap::Parser;
 3use std::fs;
 4use std::path::Path;
 5use std::process::Command;
 6
 7/// Build all ARM assembly examples into .elf binaries
 8#[derive(Parser)]
 9struct Args {
10    /// Directory containing .S examples
11    #[arg(long, default_value = "examples")]
12    examples_dir: String,
13}
14
15fn main() -&amp;gt; Result&amp;lt;(), Box&amp;lt;dyn std::error::Error&amp;gt;&amp;gt; {
16    let args = Args::parse();
17    let dir = Path::new(&amp;amp;args.examples_dir);
18
19    for entry in fs::read_dir(dir)? {
20        let entry = entry?;
21        let path = entry.path();
22        if path.extension().and_then(|s| s.to_str()) == Some("S") {
23            let name = path.file_stem().unwrap().to_str().unwrap();
24            let output = dir.join(format!("{}.elf", name));
25            build_asm(&amp;amp;path, &amp;amp;output)?;
26        }
27    }
28
29    Ok(())
30}
31
32fn build_asm(input: &amp;amp;Path, output: &amp;amp;Path) -&amp;gt; Result&amp;lt;(), Box&amp;lt;dyn std::error::Error&amp;gt;&amp;gt; {
33    println!("Building {} -&amp;gt; {}", input.display(), output.display());
34
35    let obj_file = input.with_extension("o");
36
37    let status = Command::new("arm-none-eabi-as")
38        .arg("-march=armv7-a")
39        .arg(input)
40        .arg("-o")
41        .arg(&amp;amp;obj_file)
42        .status()?;
43
44    if !status.success() {
45        return Err(format!("Assembler failed for {}", input.display()).into());
46    }
47
48    let status = Command::new("arm-none-eabi-ld")
49        .arg("-Ttext=0x8000")
50        .arg(&amp;amp;obj_file)
51        .arg("-o")
52        .arg(output)
53        .status()?;
54
55    if !status.success() {
56        return Err(format!("Linker failed for {}", output.display()).into());
57    }
58
59    Ok(fs::remove_file(obj_file)?)
60}&lt;/code&gt;
    &lt;code&gt; 1# Cargo.toml
 2[package]
 3name = "stinkarm"
 4version = "0.1.0"
 5edition = "2024"
 6default-run = "stinkarm"
 7
 8[dependencies]
 9clap = { version = "4.5.51", features = ["derive"] }
10
11[[bin]]
12name = "stinkarm"
13path = "src/main.rs"
14
15[[bin]]
16name = "bld_exmpl"
17path = "tools/bld_exmpl.rs"&lt;/code&gt;
    &lt;code&gt; 1{
 2  description = "stinkarm ‚Äî ARMv7 userspace binary emulator for x86 linux systems";
 3  inputs = {
 4    nixpkgs.url = "github:NixOS/nixpkgs/nixos-unstable";
 5    flake-utils.url = "github:numtide/flake-utils";
 6  };
 7  outputs = { self, nixpkgs, flake-utils, ... }:
 8    flake-utils.lib.eachDefaultSystem (system:
 9      let
10        pkgs = import nixpkgs { inherit system; };
11      in {
12        devShells.default = pkgs.mkShell {
13          buildInputs = with pkgs; [
14            gcc-arm-embedded
15            binutils
16            qemu
17          ];
18        };
19      }
20  );
21}&lt;/code&gt;
    &lt;head rend="h1"&gt;Parsing ELF&lt;/head&gt;
    &lt;p&gt;So there are some resources for parsing ELF, two of them I used a whole lot:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;code&gt;man elf&lt;/code&gt;(remember to&lt;code&gt;export MANPAGER='nvim +Man!'&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;gabi.xinuos.com&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;At a high level, ELF (32bit, for armv7) consists of headers and segments, it holds an Elf header, multiple program headers and the rest I don‚Äôt care about, since this emulator is only for static binaries, no dynamically linked support.&lt;/p&gt;
    &lt;head rend="h2"&gt;Elf32_Ehdr&lt;/head&gt;
    &lt;p&gt;The ELF header is exactly 52 bytes long and holds all data I need to find the program headers and whether I even want to emulate the binary I‚Äôm currently parsing. These criteria are defined as members of the &lt;code&gt;Identifier&lt;/code&gt; at the beg
of the header.&lt;/p&gt;
    &lt;p&gt;In terms of byte layout:&lt;/p&gt;
    &lt;code&gt; 1+------------------------+--------+--------+----------------+----------------+----------------+----------------+----------------+--------+---------+--------+---------+--------+--------+
 2|       identifier       |  type  |machine |    version     |     entry      |     phoff      |     shoff      |     flags      | ehsize |phentsize| phnum  |shentsize| shnum  |shstrndx|
 3|          16B           |   2B   |   2B   |       4B       |       4B       |       4B       |       4B       |       4B       |   2B   |   2B    |   2B   |   2B    |   2B   |   2B   |
 4+------------------------+--------+--------+----------------+----------------+----------------+----------------+----------------+--------+---------+--------+---------+--------+--------+
 5           \|/
 6            |
 7            |
 8            v
 9+----------------+------+------+-------+------+-----------+------------------------+
10|     magic      |class | data |version|os_abi|abi_version|          pad           |
11|       4B       |  1B  |  1B  |  1B   |  1B  |    1B     |           7B           |
12+----------------+------+------+-------+------+-----------+------------------------+&lt;/code&gt;
    &lt;p&gt;Most resources show C based examples, the rust ports are below:&lt;/p&gt;
    &lt;code&gt; 1/// Representing the ELF Object File Format header in memory, equivalent to Elf32_Ehdr in 2. ELF
 2/// header in https://gabi.xinuos.com/elf/02-eheader.html
 3///
 4/// Types are taken from https://gabi.xinuos.com/elf/01-intro.html#data-representation Table 1.1
 5/// 32-Bit Data Types:
 6///
 7/// | Elf32_ | Rust |
 8/// | ------ | ---- |
 9/// | Addr   | u32  |
10/// | Off    | u32  |
11/// | Half   | u16  |
12/// | Word   | u32  |
13/// | Sword  | i32  |
14#[derive(Debug, Clone, Copy, PartialEq, Eq)]
15pub struct Header {
16    /// initial bytes mark the file as an object file and provide machine-independent data with
17    /// which to decode and interpret the file‚Äôs contents
18    pub ident: Identifier,
19    pub r#type: Type,
20    pub machine: Machine,
21    /// identifies the object file version, always EV_CURRENT (1)
22    pub version: u32,
23    /// the virtual address to which the system first transfers control, thus starting
24    /// the process. If the file has no associated entry point, this member holds zero
25    pub entry: u32,
26    /// the program header table‚Äôs file offset in bytes. If the file has no program header table,
27    /// this member holds zero
28    pub phoff: u32,
29    /// the section header table‚Äôs file offset in bytes. If the file has no section header table, this
30    /// member holds zero
31    pub shoff: u32,
32    /// processor-specific flags associated with the file
33    pub flags: u32,
34    /// the ELF header‚Äôs size in bytes
35    pub ehsize: u16,
36    /// the size in bytes of one entry in the file‚Äôs program header table; all entries are the same
37    /// size
38    pub phentsize: u16,
39    /// the number of entries in the program header table. Thus the product of e_phentsize and e_phnum
40    /// gives the table‚Äôs size in bytes. If a file has no program header table, e_phnum holds the value
41    /// zero
42    pub phnum: u16,
43    /// section header‚Äôs size in bytes. A section header is one entry in the section header table; all
44    /// entries are the same size
45    pub shentsize: u16,
46    /// number of entries in the section header table. Thus the product of e_shentsize and e_shnum
47    /// gives the section header table‚Äôs size in bytes. If a file has no section header table,
48    /// e_shnum holds the value zero.
49    pub shnum: u16,
50    /// the section header table index of the entry associated with the section name string table.
51    /// If the file has no section name string table, this member holds the value SHN_UNDEF
52    pub shstrndx: u16,
53}&lt;/code&gt;
    &lt;p&gt;The identifier is 16 bytes long and holds the previously mentioned info so I can check if I want to emulate the binary, for instance the endianness and the bit class, in the &lt;code&gt;TryFrom&lt;/code&gt; implementation I strictly check what is parsed:&lt;/p&gt;
    &lt;code&gt; 1/// 2.2 ELF Identification: https://gabi.xinuos.com/elf/02-eheader.html#elf-identification
 2#[repr(C)]
 3#[derive(Debug, Clone, Copy, PartialEq, Eq)]
 4pub struct Identifier {
 5    /// 0x7F, 'E', 'L', 'F'
 6    pub magic: [u8; 4],
 7    /// file class or capacity
 8    ///
 9    /// | Name          | Value | Meaning       |
10    /// | ------------- | ----- | ------------- |
11    /// | ELFCLASSNONE  | 0     | Invalid class |
12    /// | ELFCLASS32    | 1     | 32-bit        |
13    /// | ELFCLASS64    | 2     | 64-bit        |
14    pub class: u8,
15    /// data encoding, endian
16    ///
17    /// | Name         | Value |
18    /// | ------------ | ----- |
19    /// | ELFDATANONE  | 0     |
20    /// | ELFDATA2LSB  | 1     |
21    /// | ELFDATA2MSB  | 2     |
22    pub data: u8,
23    /// file version, always EV_CURRENT (1)
24    pub version: u8,
25    /// operating system identification
26    ///
27    /// - if no extensions are used: 0
28    /// - meaning depends on e_machine
29    pub os_abi: u8,
30    /// value depends on os_abi
31    pub abi_version: u8,
32    // padding bytes (9-15)
33    _pad: [u8; 7],
34}
35
36impl TryFrom&amp;lt;&amp;amp;[u8]&amp;gt; for Identifier {
37    type Error = &amp;amp;'static str;
38
39    fn try_from(bytes: &amp;amp;[u8]) -&amp;gt; Result&amp;lt;Self, Self::Error&amp;gt; {
40        if bytes.len() &amp;lt; 16 {
41            return Err("e_ident too short for ELF");
42        }
43
44        // I don't want to cast via unsafe as_ptr and as Header because the header could outlive the
45        // source slice, thus we just do it the old plain indexing way
46        let ident = Self {
47            magic: bytes[0..4].try_into().unwrap(),
48            class: bytes[4],
49            data: bytes[5],
50            version: bytes[6],
51            os_abi: bytes[7],
52            abi_version: bytes[8],
53            _pad: bytes[9..16].try_into().unwrap(),
54        };
55
56        if ident.magic != [0x7f, b'E', b'L', b'F'] {
57            return Err("Unexpected EI_MAG0 to EI_MAG3, wanted 0x7f E L F");
58        }
59
60        const ELFCLASS32: u8 = 1;
61        const ELFDATA2LSB: u8 = 1;
62        const EV_CURRENT: u8 = 1;
63
64        if ident.version != EV_CURRENT {
65            return Err("Unsupported EI_VERSION value");
66        }
67
68        if ident.class != ELFCLASS32 {
69            return Err("Unexpected EI_CLASS: ELFCLASS64, wanted ELFCLASS32 (ARMv7)");
70        }
71
72        if ident.data != ELFDATA2LSB {
73            return Err("Unexpected EI_DATA: big-endian, wanted little");
74        }
75
76        Ok(ident)
77    }&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;Type&lt;/code&gt; and &lt;code&gt;Machine&lt;/code&gt; are just enums encoding meaning in the Rust type system:&lt;/p&gt;
    &lt;code&gt; 1#[repr(u16)]
 2#[derive(Debug, Clone, Copy, PartialEq, Eq)]
 3pub enum Type {
 4    None = 0,
 5    Relocatable = 1,
 6    Executable = 2,
 7    SharedObject = 3,
 8    Core = 4,
 9    LoOs = 0xfe00,
10    HiOs = 0xfeff,
11    LoProc = 0xff00,
12    HiProc = 0xffff,
13}
14
15impl TryFrom&amp;lt;u16&amp;gt; for Type {
16    type Error = &amp;amp;'static str;
17
18    fn try_from(value: u16) -&amp;gt; Result&amp;lt;Self, Self::Error&amp;gt; {
19        match value {
20            0 =&amp;gt; Ok(Type::None),
21            1 =&amp;gt; Ok(Type::Relocatable),
22            2 =&amp;gt; Ok(Type::Executable),
23            3 =&amp;gt; Ok(Type::SharedObject),
24            4 =&amp;gt; Ok(Type::Core),
25            0xfe00 =&amp;gt; Ok(Type::LoOs),
26            0xfeff =&amp;gt; Ok(Type::HiOs),
27            0xff00 =&amp;gt; Ok(Type::LoProc),
28            0xffff =&amp;gt; Ok(Type::HiProc),
29            _ =&amp;gt; Err("Invalid u16 value for e_type"),
30        }
31    }
32}
33
34
35#[repr(u16)]
36#[allow(non_camel_case_types)]
37#[derive(Debug, Clone, Copy, PartialEq, Eq)]
38pub enum Machine {
39    EM_ARM = 40,
40}
41
42impl TryFrom&amp;lt;u16&amp;gt; for Machine {
43    type Error = &amp;amp;'static str;
44
45    fn try_from(value: u16) -&amp;gt; Result&amp;lt;Self, Self::Error&amp;gt; {
46        match value {
47            40 =&amp;gt; Ok(Machine::EM_ARM),
48            _ =&amp;gt; Err("Unsupported machine"),
49        }
50    }
51}&lt;/code&gt;
    &lt;p&gt;Since all of &lt;code&gt;Header&lt;/code&gt;‚Äôs members implement &lt;code&gt;TryFrom&lt;/code&gt; we can implement
&lt;code&gt;TryFrom&amp;lt;&amp;amp;[u8]&amp;gt; for Header&lt;/code&gt; and propagate all occurring errors in member parsing
cleanly via &lt;code&gt;?&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt; 1impl TryFrom&amp;lt;&amp;amp;[u8]&amp;gt; for Header {
 2    type Error = &amp;amp;'static str;
 3
 4    fn try_from(b: &amp;amp;[u8]) -&amp;gt; Result&amp;lt;Self, Self::Error&amp;gt; {
 5        if b.len() &amp;lt; 52 {
 6            return Err("not enough bytes for Elf32_Ehdr (ELF header)");
 7        }
 8
 9        let header = Self {
10            ident: b[0..16].try_into()?,
11            r#type: le16!(b[16..18]).try_into()?,
12            machine: le16!(b[18..20]).try_into()?,
13            version: le32!(b[20..24]),
14            entry: le32!(b[24..28]),
15            phoff: le32!(b[28..32]),
16            shoff: le32!(b[32..36]),
17            flags: le32!(b[36..40]),
18            ehsize: le16!(b[40..42]),
19            phentsize: le16!(b[42..44]),
20            phnum: le16!(b[44..46]),
21            shentsize: le16!(b[46..48]),
22            shnum: le16!(b[48..50]),
23            shstrndx: le16!(b[50..52]),
24        };
25
26        match header.r#type {
27            Type::Executable =&amp;gt; (),
28            _ =&amp;gt; {
29                return Err("Unsupported ELF type, only ET_EXEC (static executables) is supported");
30            }
31        }
32
33        Ok(header)
34    }
35}&lt;/code&gt;
    &lt;p&gt;The attentive reader will see me using &lt;code&gt;le16!&lt;/code&gt; and &lt;code&gt;le32!&lt;/code&gt; for parsing bytes
into unsigned integers of different classes (&lt;code&gt;le&lt;/code&gt; is short for little endian):&lt;/p&gt;
    &lt;code&gt; 1#[macro_export]
 2macro_rules! le16 {
 3    ($bytes:expr) =&amp;gt; {{
 4        let b: [u8; 2] = $bytes
 5            .try_into()
 6            .map_err(|_| "Failed to create u16 from 2*u8")?;
 7        u16::from_le_bytes(b)
 8    }};
 9}
10
11#[macro_export]
12macro_rules! le32 {
13    ($bytes:expr) =&amp;gt; {{
14        let b: [u8; 4] = $bytes
15            .try_into()
16            .map_err(|_| "Failed to create u32 from 4*u8")?;
17        u32::from_le_bytes(b)
18    }};
19}&lt;/code&gt;
    &lt;head rend="h2"&gt;Elf32_Phdr&lt;/head&gt;
    &lt;code&gt;1+----------------+----------------+----------------+----------------+----------------+----------------+----------------+----------------+
2|      type      |     offset     |     vaddr      |     paddr      |     filesz     |     memsz      |     flags      |     align      |
3|       4B       |       4B       |       4B       |       4B       |       4B       |       4B       |       4B       |       4B       |
4+----------------+----------------+----------------+----------------+----------------+----------------+----------------+----------------+&lt;/code&gt;
    &lt;p&gt;For me, the most important fields in &lt;code&gt;Header&lt;/code&gt; are &lt;code&gt;phoff&lt;/code&gt; and &lt;code&gt;phentsize&lt;/code&gt;,
since we can use these to index into the binary to locate the program headers (&lt;code&gt;Phdr&lt;/code&gt;).&lt;/p&gt;
    &lt;code&gt; 1/// Phdr, equivalent to Elf32_Phdr, see: https://gabi.xinuos.com/elf/07-pheader.html
 2///
 3/// All of its member are u32, be it Elf32_Word, Elf32_Off or Elf32_Addr
 4#[derive(Debug)]
 5pub struct Pheader {
 6    pub r#type: Type,
 7    pub offset: u32,
 8    pub vaddr: u32,
 9    pub paddr: u32,
10    pub filesz: u32,
11    pub memsz: u32,
12    pub flags: Flags,
13    pub align: u32,
14}
15
16impl Pheader {
17    /// extracts Pheader from raw, starting from offset
18    pub fn from(raw: &amp;amp;[u8], offset: usize) -&amp;gt; Result&amp;lt;Self, String&amp;gt; {
19        let end = offset.checked_add(32).ok_or("Offset overflow")?;
20        if raw.len() &amp;lt; end {
21            return Err("Not enough bytes to parse Elf32_Phdr, need at least 32".into());
22        }
23
24        let p_raw = &amp;amp;raw[offset..end];
25        let r#type = p_raw[0..4].try_into()?;
26        let flags = p_raw[24..28].try_into()?;
27        let align = le32!(p_raw[28..32]);
28
29        if align &amp;gt; 1 &amp;amp;&amp;amp; !align.is_power_of_two() {
30            return Err(format!("Invalid p_align: {}", align));
31        }
32
33        Ok(Self {
34            r#type,
35            offset: le32!(p_raw[4..8]),
36            vaddr: le32!(p_raw[8..12]),
37            paddr: le32!(p_raw[12..16]),
38            filesz: le32!(p_raw[16..20]),
39            memsz: le32!(p_raw[20..24]),
40            flags,
41            align,
42        })
43    }
44}&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;Type&lt;/code&gt; holds info about what type of segment the header defines:&lt;/p&gt;
    &lt;code&gt; 1#[derive(Debug, Clone, Copy, PartialEq, Eq)]
 2#[repr(C)]
 3pub enum Type {
 4    NULL = 0,
 5    LOAD = 1,
 6    DYNAMIC = 2,
 7    INTERP = 3,
 8    NOTE = 4,
 9    SHLIB = 5,
10    PHDR = 6,
11    TLS = 7,
12    LOOS = 0x60000000,
13    HIOS = 0x6fffffff,
14    LOPROC = 0x70000000,
15    HIPROC = 0x7fffffff,
16}&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;Flag&lt;/code&gt; defines the
permission flags the segment should have once it is dumped into memory:&lt;/p&gt;
    &lt;code&gt; 1#[derive(Debug, Clone, Copy, PartialEq, Eq)]
 2#[repr(transparent)]
 3pub struct Flags(u32);
 4
 5impl Flags {
 6    pub const NONE: Self = Flags(0x0);
 7    pub const X: Self = Flags(0x1);
 8    pub const W: Self = Flags(0x2);
 9    pub const R: Self = Flags(0x4);
10}&lt;/code&gt;
    &lt;head rend="h2"&gt;Full ELF parsing&lt;/head&gt;
    &lt;p&gt;Putting &lt;code&gt;Elf32_Ehdr&lt;/code&gt; and &lt;code&gt;Elf32_Phdr&lt;/code&gt; parsing together:&lt;/p&gt;
    &lt;code&gt; 1/// Representing an ELF32 binary in memory
 2///
 3/// This does not include section headers (Elf32_Shdr), but only program headers (Elf32_Phdr), see either `man elf` and/or https://gabi.xinuos.com/elf/03-sheader.html
 4#[derive(Debug)]
 5pub struct Elf {
 6    pub header: header::Header,
 7    pub pheaders: Vec&amp;lt;pheader::Pheader&amp;gt;,
 8}
 9
10impl TryFrom&amp;lt;&amp;amp;[u8]&amp;gt; for Elf {
11    type Error = String;
12
13    fn try_from(b: &amp;amp;[u8]) -&amp;gt; Result&amp;lt;Self, String&amp;gt; {
14        let header = header::Header::try_from(b).map_err(|e| e.to_string())?;
15
16        let mut pheaders = Vec::with_capacity(header.phnum as usize);
17        for i in 0..header.phnum {
18            let offset = header.phoff as usize + i as usize * header.phentsize as usize;
19            let ph = pheader::Pheader::from(b, offset)?;
20            pheaders.push(ph);
21        }
22
23        Ok(Elf { header, pheaders })
24    }
25}&lt;/code&gt;
    &lt;p&gt;The equivalent to &lt;code&gt;readelf -l&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt; 1Elf {
 2    header: Header {
 3        ident: Identifier {
 4            magic: [127, 69, 76, 70],
 5            class: 1,
 6            data: 1,
 7            version: 1,
 8            os_abi: 0,
 9            abi_version: 0,
10            _pad: [0, 0, 0, 0, 0, 0, 0]
11        },
12        type: Executable,
13        machine: EM_ARM,
14        version: 1,
15        entry: 32768,
16        phoff: 52,
17        shoff: 4572,
18        flags: 83886592,
19        ehsize: 52,
20        phentsize: 32,
21        phnum: 1,
22        shentsize: 40,
23        shnum: 8,
24        shstrndx: 7
25    },
26    pheaders: [
27        Pheader {
28            type: LOAD,
29            offset: 4096,
30            vaddr: 32768,
31            paddr: 32768,
32            filesz: 12,
33            memsz: 12,
34            flags: Flags(5),
35            align: 4096
36        }
37    ]
38}&lt;/code&gt;
    &lt;p&gt;Or in the debug output of stinkarm:&lt;/p&gt;
    &lt;code&gt; 1[     0.613ms] opening binary "examples/asm.elf"
 2[     0.721ms] parsing ELF...
 3[     0.744ms] \
 4ELF Header:
 5  Magic:              [7f, 45, 4c, 46]
 6  Class:              ELF32
 7  Data:               Little endian
 8  Type:               Executable
 9  Machine:            EM_ARM
10  Version:            1
11  Entry point:        0x8000
12  Program hdr offset: 52 (32 bytes each)
13  Section hdr offset: 4572
14  Flags:              0x05000200
15  EH size:            52
16  # Program headers:  1
17  # Section headers:  8
18  Str tbl index:      7
19
20Program Headers:
21  Type       Offset   VirtAddr   PhysAddr   FileSz    MemSz  Flags  Align
22  LOAD     0x001000 0x00008000 0x00008000 0x00000c 0x00000c    R|X 0x1000&lt;/code&gt;
    &lt;head rend="h1"&gt;Dumping ELF segments into memory&lt;/head&gt;
    &lt;p&gt;Since the only reason for parsing the elf headers is to know where to put what segment with which permissions, I want to quickly interject on why we have to put said segments at these specific addresses. The main reason is that all pointers, all offsets and pc related decoding has to be done relative to &lt;code&gt;Elf32_Ehdr.entry&lt;/code&gt;, here &lt;code&gt;0x8000&lt;/code&gt;. The linker also generated all instruction
arguments according to this value.&lt;/p&gt;
    &lt;p&gt;Before mapping each segment at its &lt;code&gt;Pheader::vaddr&lt;/code&gt;, we have to understand:
One doesn‚Äôt simply &lt;code&gt;mmap&lt;/code&gt; with &lt;code&gt;MAP_FIXED&lt;/code&gt; or &lt;code&gt;MAP_NOREPLACE&lt;/code&gt; into the virtual
address &lt;code&gt;0x8000&lt;/code&gt;. The Linux kernel won‚Äôt let us, and rightfully so, &lt;code&gt;man mmap&lt;/code&gt;
says:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;If addr is not NULL, then the kernel takes it as a hint about where to place the mapping; on Linux, the kernel will pick a nearby page boundary (but always above or equal to the value specified by /proc/sys/vm/mmap_min_addr) and attempt to create the mapping there.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;And &lt;code&gt;/proc/sys/vm/mmap_min_addr&lt;/code&gt; on my system is &lt;code&gt;u16::MAX&lt;/code&gt; (2^16)-1=65535. So
mapping our segment to &lt;code&gt;0x8000&lt;/code&gt; (32768) is not allowed:&lt;/p&gt;
    &lt;code&gt; 1let segment = sys::mmap::mmap(
 2    // this is only UB if dereferenced, its just a hint, so its safe here
 3    Some(unsafe { std::ptr::NonNull::new_unchecked(0x8000 as *mut u8) }),
 4    4096,
 5    sys::mmap::MmapProt::WRITE,
 6    sys::mmap::MmapFlags::ANONYMOUS
 7        | sys::mmap::MmapFlags::PRIVATE
 8        | sys::mmap::MmapFlags::NOREPLACE,
 9    -1,
10    0,
11)
12.unwrap();&lt;/code&gt;
    &lt;p&gt;Running the above with our &lt;code&gt;vaddr&lt;/code&gt; of &lt;code&gt;0x8000&lt;/code&gt; results in:&lt;/p&gt;
    &lt;code&gt;1thread 'main' panicked at src/main.rs:33:6:
2called `Result::unwrap()` on an `Err` value: "mmap failed (errno 1): Operation not permitted
3(os error 1)"&lt;/code&gt;
    &lt;p&gt;It only works in elevated permission mode, which is something I dont want to run my emulator in.&lt;/p&gt;
    &lt;head rend="h2"&gt;Translating guest memory access to host memory access&lt;/head&gt;
    &lt;p&gt;The obvious fix is to not mmap below &lt;code&gt;u16::MAX&lt;/code&gt; and let the kernel choose where
we dump our segment:&lt;/p&gt;
    &lt;code&gt;1let segment = sys::mmap::mmap(
2    None,
3    4096,
4    MmapProt::WRITE,
5    MmapFlags::ANONYMOUS | MmapFlags::PRIVATE,
6    -1,
7    0,
8).unwrap();&lt;/code&gt;
    &lt;p&gt;But this means the segment of the process to emulate is not at &lt;code&gt;0x8000&lt;/code&gt;, but
anywhere the kernel allows. So we need to add a translation layer between guest
and host memory: (If you‚Äôre familiar with how virtual memory works, its similar
but one more indirection)&lt;/p&gt;
    &lt;code&gt;1+--guest--+
2| 0x80000 | ------------+
3+---------+             |
4                        |
5                    Mem::translate
6                        |
7+------host------+      |
8| 0x7f5b4b8f8000 | &amp;lt;----+
9+----------------+&lt;/code&gt;
    &lt;p&gt;Putting this into rust:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;map_region&lt;/code&gt;registers a region of memory and allows&lt;code&gt;Mem&lt;/code&gt;to take ownership for calling munmap on these segments once it goes out of scope&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;translate&lt;/code&gt;takes a guest addr and translates it to a host addr&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt; 1struct MappedSegment {
 2    host_ptr: *mut u8,
 3    len: u32,
 4}
 5
 6pub struct Mem {
 7    maps: BTreeMap&amp;lt;u32, MappedSegment&amp;gt;,
 8}
 9
10impl Mem {
11    pub fn map_region(&amp;amp;mut self, guest_addr: u32, len: u32, host_ptr: *mut u8) {
12        self.maps
13            .insert(guest_addr, MappedSegment { host_ptr, len });
14    }
15
16    /// translate a guest addr to a host addr we can write and read from
17    pub fn translate(&amp;amp;self, guest_addr: u32) -&amp;gt; Option&amp;lt;*mut u8&amp;gt; {
18        // Find the greatest key &amp;lt;= guest_addr.
19        let (&amp;amp;base, seg) = self.maps.range(..=guest_addr).next_back()?;
20        if guest_addr &amp;lt; base.wrapping_add(seg.len) {
21            let offset = guest_addr.wrapping_sub(base);
22            Some(unsafe { seg.host_ptr.add(offset as usize) })
23        } else {
24            None
25        }
26    }
27
28    pub fn read_u32(&amp;amp;self, guest_addr: u32) -&amp;gt; Option&amp;lt;u32&amp;gt; {
29        let ptr = self.translate(guest_addr)?;
30        unsafe { Some(u32::from_le(*(ptr as *const u32))) }
31    }
32}&lt;/code&gt;
    &lt;p&gt;This fix has the added benfit of allowing us to sandbox guest memory fully, so we can validate each memory access before we allow a guest to host memory interaction.&lt;/p&gt;
    &lt;head rend="h2"&gt;Mapping segments with their permissions&lt;/head&gt;
    &lt;p&gt;The basic idea is similar to the way a JIT compiler works:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;create a &lt;code&gt;mmap&lt;/code&gt;section with&lt;code&gt;W&lt;/code&gt;permissions&lt;/item&gt;
      &lt;item&gt;write bytes from elf into section&lt;/item&gt;
      &lt;item&gt;zero rest of defined size&lt;/item&gt;
      &lt;item&gt;change permission of section with &lt;code&gt;mprotect&lt;/code&gt;to the permissions defined in the&lt;code&gt;Pheader&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt; 1/// mapping applies the configuration of self to the current memory context by creating the
 2/// segments with the corresponding permission bits, vaddr, etc
 3pub fn map(&amp;amp;self, raw: &amp;amp;[u8], guest_mem: &amp;amp;mut mem::Mem) -&amp;gt; Result&amp;lt;(), String&amp;gt; {
 4    // zero memory needed case, no clue if this actually ever happens, but we support it
 5    if self.memsz == 0 {
 6        return Ok(());
 7    }
 8
 9    if self.vaddr == 0 {
10        return Err("program header has a zero virtual address".into());
11    }
12
13    // we need page alignement, so either Elf32_Phdr.p_align or 4096
14    let (start, _end, len) = self.alignments();
15
16    // Instead of mapping at the guest vaddr (Linux doesnt't allow for low addresses),
17    // we allocate memory wherever the host kernel gives us.
18    // This keeps guest memory sandboxed: guest addr != host addr.
19    let segment = mem::mmap::mmap(
20        None,
21        len as usize,
22        MmapProt::WRITE,
23        MmapFlags::ANONYMOUS | MmapFlags::PRIVATE,
24        -1,
25        0,
26    )?;
27
28    let segment_ptr = segment.as_ptr();
29    let segment_slice = unsafe { std::slice::from_raw_parts_mut(segment_ptr, len as usize) };
30
31    let file_slice: &amp;amp;[u8] =
32        &amp;amp;raw[self.offset as usize..(self.offset.wrapping_add(self.filesz)) as usize];
33
34    // compute offset inside the mmapped slice where the segment should start
35    let offset = (self.vaddr - start) as usize;
36
37    // copy the segment contents to the mmaped segment
38    segment_slice[offset..offset + file_slice.len()].copy_from_slice(file_slice);
39
40    // we need to zero the remaining bytes
41    if self.memsz &amp;gt; self.filesz {
42        segment_slice
43            [offset.wrapping_add(file_slice.len())..offset.wrapping_add(self.memsz as usize)]
44            .fill(0);
45    }
46
47    // record mapping in guest memory table, so CPU can translate guest vaddr to host pointer
48    guest_mem.map_region(self.vaddr, len, segment_ptr);
49
50    // we change the permissions for our segment from W to the segments requested bits
51    mem::mmap::mprotect(segment, len as usize, self.flags.into())
52}
53
54/// returns (start, end, len)
55fn alignments(&amp;amp;self) -&amp;gt; (u32, u32, u32) {
56    // we need page alignement, so either Elf32_Phdr.p_align or 4096
57    let align = match self.align {
58        0 =&amp;gt; 0x1000,
59        _ =&amp;gt; self.align,
60    };
61    let start = self.vaddr &amp;amp; !(align - 1);
62    let end = (self.vaddr.wrapping_add(self.memsz).wrapping_add(align) - 1) &amp;amp; !(align - 1);
63    let len = end - start;
64    (start, end, len)
65}&lt;/code&gt;
    &lt;p&gt;Map is called in the emulators entry point:&lt;/p&gt;
    &lt;code&gt;1let elf: elf::Elf = (&amp;amp;buf as &amp;amp;[u8]).try_into().expect("Failed to parse binary");
2let mut mem = mem::Mem::new();
3for phdr in elf.pheaders {
4    if phdr.r#type == elf::pheader::Type::LOAD {
5        phdr.map(&amp;amp;buf, &amp;amp;mut mem)
6            .expect("Mapping program header failed");
7    }
8}&lt;/code&gt;
    &lt;head rend="h1"&gt;Decoding armv7&lt;/head&gt;
    &lt;p&gt;We can now request a word (32bit) from our &lt;code&gt;LOAD&lt;/code&gt; segment which contains
the &lt;code&gt;.text&lt;/code&gt; section bytes one can inspect via &lt;code&gt;objdump&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt; 1$ arm-none-eabi-objdump -d examples/exit.elf
 2
 3examples/exit.elf:     file format elf32-littlearm
 4
 5
 6Disassembly of section .text:
 7
 800008000 &amp;lt;_start&amp;gt;:
 9    8000:       e3a000a1        mov     r0, #161        @ 0xa1
10    8004:       e3a07001        mov     r7, #1
11    8008:       ef000000        svc     0x00000000&lt;/code&gt;
    &lt;p&gt;So we use &lt;code&gt;Mem::read_u32(0x8000)&lt;/code&gt; and get &lt;code&gt;0xe3a000a1&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Decoding armv7 instructions seems doable at a glance, but it is a deeper rabbit-hole than I expected, prepare for a bit shifting, implicit behaviour and intertwined meaning heavy section:&lt;/p&gt;
    &lt;p&gt;Instructions are more or less grouped into four groups:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Branch and control&lt;/item&gt;
      &lt;item&gt;Data processing&lt;/item&gt;
      &lt;item&gt;Load and store&lt;/item&gt;
      &lt;item&gt;Other (syscalls &amp;amp; stuff)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Each armv7 instruction is 32 bit in size, (in general) its layout is as follows:&lt;/p&gt;
    &lt;code&gt;1+--------+------+------+------+------------+---------+
2|  cond  |  op  |  Rn  |  Rd  |  Operand2  |  shamt  |
3|   4b   |  4b  |  4b  |  4b  |     12b    |   4b    |
4+--------+------+------+------+------------+---------+&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;bit range&lt;/cell&gt;
        &lt;cell role="head"&gt;name&lt;/cell&gt;
        &lt;cell role="head"&gt;description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;0..4&lt;/cell&gt;
        &lt;cell&gt;cond&lt;/cell&gt;
        &lt;cell&gt;contains &lt;code&gt;EQ&lt;/code&gt;, &lt;code&gt;NE&lt;/code&gt;, etc&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;4..8&lt;/cell&gt;
        &lt;cell&gt;op&lt;/cell&gt;
        &lt;cell&gt;for instance &lt;code&gt;0b1101&lt;/code&gt; for &lt;code&gt;mov&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;8..12&lt;/cell&gt;
        &lt;cell&gt;rn&lt;/cell&gt;
        &lt;cell&gt;source register&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;12..16&lt;/cell&gt;
        &lt;cell&gt;rd&lt;/cell&gt;
        &lt;cell&gt;destination register&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;16..28&lt;/cell&gt;
        &lt;cell&gt;operand2&lt;/cell&gt;
        &lt;cell&gt;immediate value or shifted register&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;28..32&lt;/cell&gt;
        &lt;cell&gt;shamt&lt;/cell&gt;
        &lt;cell&gt;shift amount&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;Rust representation&lt;/head&gt;
    &lt;p&gt;Since &lt;code&gt;cond&lt;/code&gt; decides whether or not the instruction is
executed, I decided on the following struct to be the decoded
instruction:&lt;/p&gt;
    &lt;code&gt; 1#[derive(Debug, Copy, Clone)]
 2pub struct InstructionContainer {
 3    pub cond: u8,
 4    pub instruction: Instruction,
 5}
 6
 7#[derive(Debug, Copy, Clone)]
 8pub enum Instruction {
 9    MovImm { rd: u8, rhs: u32 },
10    Svc,
11    LdrLiteral { rd: u8, addr: u32 },
12    Unknown(u32),
13}&lt;/code&gt;
    &lt;p&gt;These 4 instructions are enough to support both the minimal binary at the intro and the asm hello world:&lt;/p&gt;
    &lt;code&gt;1    .global _start
2_start:
3    mov r0, #161
4    mov r7, #1
5    svc #0&lt;/code&gt;
    &lt;code&gt; 1    .section .rodata
 2msg:
 3    .asciz "Hello, world!\n"
 4
 5    .section .text
 6    .global _start
 7_start:
 8    ldr r0, =1
 9    ldr r1, =msg
10    mov r2, #14
11    mov r7, #4
12    svc #0
13
14    mov r0, #0
15    mov r7, #1
16    svc #0&lt;/code&gt;
    &lt;head rend="h2"&gt;General instruction detection&lt;/head&gt;
    &lt;p&gt;Our decoder is a function accepting a word, the program counter (we need this later for decoding the offset for &lt;code&gt;ldr&lt;/code&gt;) and returning the
aforementioned instruction container:&lt;/p&gt;
    &lt;code&gt;1pub fn decode_word(word: u32, caddr: u32) -&amp;gt; InstructionContainer&lt;/code&gt;
    &lt;p&gt;Referring to the diagram shown before, I know the first 4 bit are the condition, so I can extract these first. I also take the top 3 bits to identify the instruction class (load and store, branch or data processing immediate):&lt;/p&gt;
    &lt;code&gt;1// ...
2let cond = ((word &amp;gt;&amp;gt; 28) &amp;amp; 0xF) as u8;
3let top = ((word &amp;gt;&amp;gt; 25) &amp;amp; 0x7) as u8;&lt;/code&gt;
    &lt;head rend="h2"&gt;Immediate mov&lt;/head&gt;
    &lt;p&gt;Since there are immediate moves and non immediate moves, both &lt;code&gt;0b000&lt;/code&gt; and
&lt;code&gt;0b001&lt;/code&gt; are valid top values we want to support.&lt;/p&gt;
    &lt;code&gt;1// ...
2if top == 0b000 || top == 0b001 {
3    let i_bit = ((word &amp;gt;&amp;gt; 25) &amp;amp; 0x1) != 0;
4    let opcode = ((word &amp;gt;&amp;gt; 21) &amp;amp; 0xF) as u8;
5    if i_bit {
6        // ...
7    }
8}&lt;/code&gt;
    &lt;p&gt;If the i bit is set, we can extract convert the opcode from its bits into something I can read a lot better:&lt;/p&gt;
    &lt;code&gt; 1#[derive(Debug, Clone, Copy, PartialEq, Eq)]
 2#[repr(u8)]
 3enum Op {
 4    // ...
 5    Mov = 0b1101,
 6}
 7
 8static OP_TABLE: [Op; 16] = [
 9    // ...
10    Op::Mov,
11];
12
13#[inline(always)]
14fn op_from_bits(bits: u8) -&amp;gt; Op {
15    debug_assert!(bits &amp;lt;= 0b1111);
16    unsafe { *OP_TABLE.get_unchecked(bits as usize) }
17}&lt;/code&gt;
    &lt;p&gt;We can now plug this in, match on the only ddi (data processing immediate) we know and extract both the destination register (rd) and the raw immediate value:&lt;/p&gt;
    &lt;code&gt; 1if top == 0b000 || top == 0b001 {
 2    // Data-processing immediate (ddi) (top 0b000 or 0b001 when I==1)
 3    let i_bit = ((word &amp;gt;&amp;gt; 25) &amp;amp; 0x1) != 0;
 4    let opcode = ((word &amp;gt;&amp;gt; 21) &amp;amp; 0xF) as u8;
 5    if i_bit {
 6        match op_from_bits(opcode) {
 7            Op::Mov =&amp;gt; {
 8                let rd = ((word &amp;gt;&amp;gt; 12) &amp;amp; 0xF) as u8;
 9                let imm12 = word &amp;amp; 0xFFF;
10                // ...
11            }
12            _ =&amp;gt; todo!(),
13        }
14    }
15}&lt;/code&gt;
    &lt;p&gt;From the examples before one can see the immediate value is prefixed with &lt;code&gt;#&lt;/code&gt;. To move the value &lt;code&gt;161&lt;/code&gt; into &lt;code&gt;r0&lt;/code&gt; we do:&lt;/p&gt;
    &lt;code&gt;1mov r0, #161&lt;/code&gt;
    &lt;p&gt;Since we know there are only 12 bits available for the immediate the arm engineers came up with rotation of the resulting integer by the remaining 4 bits:&lt;/p&gt;
    &lt;code&gt;1#[inline(always)]
2fn decode_rotated_imm(imm12: u32) -&amp;gt; u32 {
3    let rotate = ((imm12 &amp;gt;&amp;gt; 8) &amp;amp; 0b1111) * 2;
4    (imm12 &amp;amp; 0xff).rotate_right(rotate)
5}&lt;/code&gt;
    &lt;p&gt;Plugging this back in results in us being able to fully decode &lt;code&gt;mov r0,#161&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt; 1if top == 0b000 || top == 0b001 {
 2    let i_bit = ((word &amp;gt;&amp;gt; 25) &amp;amp; 0x1) != 0;
 3    let opcode = ((word &amp;gt;&amp;gt; 21) &amp;amp; 0xF) as u8;
 4    if i_bit {
 5        match op_from_bits(opcode) {
 6            Op::Mov =&amp;gt; {
 7                let rd = ((word &amp;gt;&amp;gt; 12) &amp;amp; 0xF) as u8;
 8                let imm12 = word &amp;amp; 0xFFF;
 9                let rhs = decode_rotated_imm(imm12);
10                return InstructionContainer {
11                    cond,
12                    instruction: Instruction::MovImm { rd, rhs },
13                };
14            }
15            _ =&amp;gt; todo!(),
16        }
17    }
18}&lt;/code&gt;
    &lt;p&gt;As seen when &lt;code&gt;dbg!&lt;/code&gt;-ing the cpu steps:&lt;/p&gt;
    &lt;code&gt;1[src/cpu/mod.rs:114:13] decoder::decode_word(word, self.pc()) =
2InstructionContainer {
3    cond: 14,
4    instruction: MovImm {
5        rd: 0,
6        rhs: 161,
7    },
8}&lt;/code&gt;
    &lt;head rend="h2"&gt;Load and Store&lt;/head&gt;
    &lt;p&gt;&lt;code&gt;ldr&lt;/code&gt; is part of the load and store instruction group and is needed for
the accessing of &lt;code&gt;Hello World!&lt;/code&gt; in &lt;code&gt;.rodata&lt;/code&gt; and putting a ptr to it
into a register.&lt;/p&gt;
    &lt;p&gt;In comparison to immediate mov we have to do a little trick, since we only want to match for load and store that matches:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;single register modification&lt;/item&gt;
      &lt;item&gt;load and store with immediate&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So we only decode:&lt;/p&gt;
    &lt;code&gt;1LDR Rd, [Rn, #imm]
2LDR Rd, [Rn], #imm
3@ etc
&lt;/code&gt;
    &lt;p&gt;Thus we match with &lt;code&gt;(top &amp;gt;&amp;gt; 1) &amp;amp; 0b11 == 0b01&lt;/code&gt; and start extracting a
whole bucket load of bit flags:&lt;/p&gt;
    &lt;code&gt; 1if (top &amp;gt;&amp;gt; 1) &amp;amp; 0b11 == 0b01 {
 2    let p = ((word &amp;gt;&amp;gt; 24) &amp;amp; 1) != 0;
 3    let u = ((word &amp;gt;&amp;gt; 23) &amp;amp; 1) != 0;
 4    let b = ((word &amp;gt;&amp;gt; 22) &amp;amp; 1) != 0;
 5    let w = ((word &amp;gt;&amp;gt; 21) &amp;amp; 1) != 0;
 6    let l = ((word &amp;gt;&amp;gt; 20) &amp;amp; 1) != 0;
 7    let rn = ((word &amp;gt;&amp;gt; 16) &amp;amp; 0xF) as u8;
 8    let rd = ((word &amp;gt;&amp;gt; 12) &amp;amp; 0xF) as u8;
 9    let imm12 = (word &amp;amp; 0xFFF) as u32;
10
11    // Literal‚Äëpool version
12    if l &amp;amp;&amp;amp; rn == 0b1111 &amp;amp;&amp;amp; p &amp;amp;&amp;amp; u &amp;amp;&amp;amp; !w &amp;amp;&amp;amp; !b {
13        let pc_seen = caddr.wrapping_add(8);
14        let literal_addr = pc_seen.wrapping_add(imm12);
15
16        return InstructionContainer {
17            cond,
18            instruction: Instruction::LdrLiteral {
19                rd,
20                addr: literal_addr,
21            },
22        };
23    }
24
25    todo!("only LDR with p&amp;amp;u&amp;amp;!w&amp;amp;!b is implemented")
26}&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;bit&lt;/cell&gt;
        &lt;cell role="head"&gt;description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;p&lt;/cell&gt;
        &lt;cell&gt;pre-indexed addressing, offset added before load&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;u&lt;/cell&gt;
        &lt;cell&gt;add (1) vs subtract (0) offset&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;b&lt;/cell&gt;
        &lt;cell&gt;word (0) or byte (1) sized access&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;w&lt;/cell&gt;
        &lt;cell&gt;(no=0) write back to base&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;l&lt;/cell&gt;
        &lt;cell&gt;load (1), or store (0)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;&lt;code&gt;ldr Rn, &amp;lt;addr&amp;gt;&lt;/code&gt; matches exactly &lt;code&gt;load&lt;/code&gt;, base register is PC (&lt;code&gt;rn==0b1111&lt;/code&gt;), pre-indexed
addressing, added offset, no write back and no byte sized access (&lt;code&gt;l &amp;amp;&amp;amp; rn == 0b1111 &amp;amp;&amp;amp; p &amp;amp;&amp;amp; u &amp;amp;&amp;amp; !w &amp;amp;&amp;amp; !b&lt;/code&gt;).&lt;/p&gt;
    &lt;head rend="h2"&gt;Syscalls&lt;/head&gt;
    &lt;p&gt;Syscalls are the only way to interact with the Linux kernel (as far as I know), so we definitely need to implement both decoding and forwarding. Bits 27-24 are &lt;code&gt;1111&lt;/code&gt; for system calls. The immediate value is
irrelevant for us, since the Linux syscall handler either way discards
the value:&lt;/p&gt;
    &lt;code&gt;1if ((word &amp;gt;&amp;gt; 24) &amp;amp; 0xF) as u8 == 0b1111 {
2    return InstructionContainer {
3        cond,
4        // technically arm says svc has a 24bit immediate but we don't care about it, since the
5        // Linux kernel also doesn't
6        instruction: Instruction::Svc,
7    };
8}&lt;/code&gt;
    &lt;p&gt;We can now fully decode all instructions for both the simple exit and the more advanced hello world binary:&lt;/p&gt;
    &lt;code&gt;1[src/cpu/mod.rs:121:15] instruction = MovImm { rd: 0, rhs: 161, }
2[src/cpu/mod.rs:121:15] instruction = MovImm { rd: 7, rhs: 1, }
3[src/cpu/mod.rs:121:15] instruction = Svc&lt;/code&gt;
    &lt;code&gt;1[src/cpu/mod.rs:121:15] instruction = MovImm { rd: 0, rhs: 1, }
2[src/cpu/mod.rs:121:15] instruction = LdrLiteral { rd: 1, addr: 32800, }
3[src/cpu/mod.rs:121:15] instruction = MovImm { rd: 2, rhs: 14, }
4[src/cpu/mod.rs:121:15] instruction = MovImm { rd: 7, rhs: 4, }
5[src/cpu/mod.rs:121:15] instruction = Svc
6[src/cpu/mod.rs:121:15] instruction = MovImm { rd: 0, rhs: 0, }
7[src/cpu/mod.rs:121:15] instruction = MovImm { rd: 7, rhs: 1, }
8[src/cpu/mod.rs:121:15] instruction = Svc&lt;/code&gt;
    &lt;head rend="h1"&gt;Emulating the CPU&lt;/head&gt;
    &lt;p&gt;This is by FAR the easiest part, I only struggled with the double indirection for &lt;code&gt;ldr&lt;/code&gt; (I simply didn‚Äôt know about it), but each problem
at its time :^).&lt;/p&gt;
    &lt;code&gt; 1pub struct Cpu&amp;lt;'cpu&amp;gt; {
 2    /// r0-r15 (r13=SP, r14=LR, r15=PC)
 3    pub r: [u32; 16],
 4    pub cpsr: u32,
 5    pub mem: &amp;amp;'cpu mut mem::Mem,
 6    /// only set by ArmSyscall::Exit to propagate exit code to the host
 7    pub status: Option&amp;lt;i32&amp;gt;,
 8}
 9
10impl&amp;lt;'cpu&amp;gt; Cpu&amp;lt;'cpu&amp;gt; {
11    pub fn new(mem: &amp;amp;'cpu mut mem::Mem, pc: u32) -&amp;gt; Self {
12        let mut s = Self {
13            r: [0; 16],
14            cpsr: 0x60000010,
15            mem,
16            status: None,
17        };
18        s.r[15] = pc;
19        s
20    }&lt;/code&gt;
    &lt;p&gt;Instantiating the cpu:&lt;/p&gt;
    &lt;code&gt;1let mut cpu = cpu::Cpu::new(&amp;amp;mut mem, elf.header.entry);&lt;/code&gt;
    &lt;head rend="h2"&gt;Conditional Instructions?&lt;/head&gt;
    &lt;p&gt;When writing the decoder I was confused by the 4 conditional bits. I always though one does conditional execution by using a branch to jump over instructions that shouldnt be executed. That was before I learned for arm, both ways are supported (the armv7 reference says this feature should only be used if there arent multiple instructions depending on the same condition, otherwise one should use branches) - so I need to support this too:&lt;/p&gt;
    &lt;code&gt; 1impl&amp;lt;'cpu&amp;gt; Cpu&amp;lt;'cpu&amp;gt; {
 2    #[inline(always)]
 3    fn cond_passes(&amp;amp;self, cond: u8) -&amp;gt; bool {
 4        match cond {
 5            0x0 =&amp;gt; (self.cpsr &amp;gt;&amp;gt; 30) &amp;amp; 1 == 1, // EQ: Z == 1
 6            0x1 =&amp;gt; (self.cpsr &amp;gt;&amp;gt; 30) &amp;amp; 1 == 0, // NE
 7            0xE =&amp;gt; true,                       // AL (always)
 8            0xF =&amp;gt; false,                      // NV (never)
 9            _ =&amp;gt; false,                        // strict false
10        }
11    }
12}&lt;/code&gt;
    &lt;head rend="h2"&gt;Instruction dispatch&lt;/head&gt;
    &lt;p&gt;After implementing the necessary checks and setup for emulating the cpu, the CPU can now check if an instruction is to be executed, match on the decoded instruction and run the associated logic:&lt;/p&gt;
    &lt;code&gt; 1impl&amp;lt;'cpu&amp;gt; Cpu&amp;lt;'cpu&amp;gt; {
 2    #[inline(always)]
 3    fn pc(&amp;amp;self) -&amp;gt; u32 {
 4        self.r[15] &amp;amp; !0b11
 5    }
 6
 7    /// moves pc forward a word
 8    #[inline(always)]
 9    fn advance(&amp;amp;mut self) {
10        self.r[15] = self.r[15].wrapping_add(4);
11    }
12
13    pub fn step(&amp;amp;mut self) -&amp;gt; Result&amp;lt;bool, err::Err&amp;gt; {
14        let Some(word) = self.mem.read_u32(self.pc()) else {
15            return Ok(false);
16        };
17
18        if word == 0 {
19            // zero instruction means we hit zeroed out rest of the page
20            return Ok(false);
21        }
22
23        let InstructionContainer { instruction, cond } = decoder::decode_word(word, self.pc());
24
25        if !self.cond_passes(cond) {
26            self.advance();
27            return Ok(true);
28        }
29
30        match instruction {
31            decoder::Instruction::MovImm { rd, rhs } =&amp;gt; {
32                self.r[rd as usize] = rhs;
33            }
34            decoder::Instruction::Unknown(w) =&amp;gt; {
35                return Err(err::Err::UnknownOrUnsupportedInstruction(w));
36            }
37            i =&amp;gt; {
38                stinkln!(
39                    "found unimplemented instruction, exiting: {:#x}:={:?}",
40                    word,
41                    i
42                );
43                self.status = Some(1);
44            }
45        }
46
47        self.advance();
48
49        Ok(true)
50    }
51}&lt;/code&gt;
    &lt;head rend="h2"&gt;LDR and addresses in literal pools&lt;/head&gt;
    &lt;p&gt;While Translating guest memory access to host memory access goes into depth on translating / forwarding guest memory access to host memory adresses, this chapter will focus on the layout of literals in armv7 and how &lt;code&gt;ldr&lt;/code&gt; indirects
memory access.&lt;/p&gt;
    &lt;p&gt;Lets first take a look at the ldr instruction of our hello world example:&lt;/p&gt;
    &lt;code&gt; 1    .section .rodata
 2    @ define a string with the `msg` label
 3msg:
 4    @ asciz is like asciii but zero terminated
 5    .asciz "Hello world!\n"
 6
 7    .section .text
 8    .global _start
 9_start:
10    @ load the literal pool addr of msg into r1
11    ldr r1, =msg&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;as&lt;/code&gt;
documentation
says:&lt;/p&gt;
    &lt;quote&gt;&lt;code&gt;LDR&lt;/code&gt;ARMASM&lt;code&gt;1ldr &amp;lt;register&amp;gt;, = &amp;lt;expression&amp;gt;&lt;/code&gt;&lt;p&gt;If expression evaluates to a numeric constant then a MOV or MVN instruction will be used in place of the LDR instruction, if the constant can be generated by either of these instructions. Otherwise the constant will be placed into the nearest literal pool (if it not already there) and a PC relative LDR instruction will be generated.&lt;/p&gt;&lt;/quote&gt;
    &lt;p&gt;Now this may not make sense at a first glance, why would &lt;code&gt;=msg&lt;/code&gt; be assembled
into an address to the address of the literal. But an &lt;code&gt;armv7&lt;/code&gt; instruction can
not encode a full address, it is impossible due to the instruction being
restricted to an 8-bit value rotated right by an even number of bits. The ldr
instructions argument points to a literal pool entry, this entry is a 32-bit
value and reading it produces the actual address of &lt;code&gt;msg&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;When decoding we can see ldr points to a memory address (32800 or &lt;code&gt;0x8020&lt;/code&gt;) in
the section we mmaped earlier:&lt;/p&gt;
    &lt;code&gt;1[src/cpu/mod.rs:121:15] instruction = LdrLiteral { rd: 1, addr: 32800 }&lt;/code&gt;
    &lt;p&gt;Before accessing guest memory, we must translate said addr to a host addr:&lt;/p&gt;
    &lt;code&gt; 1+--ldr.addr--+
 2|   0x8020   |
 3+------------+
 4      |
 5      |             +-------------Mem::read_u32(addr)-------------+
 6      |             |                                             |
 7      |             |   +--guest--+                               |
 8      |             |   |  0x8020 | ------------+                 |
 9      |             |   +---------+             |                 |
10      |             |                           |                 |
11      +-----------&amp;gt; |                       Mem::translate        |
12                    |                           |                 |
13                    |   +------host------+      |                 |
14                    |   | 0x7ffff7f87020 | &amp;lt;----+                 |
15                    |   +----------------+                        |
16                    |                                             |
17                    +---------------------------------------------+
18                                           |
19+--literal-ptr--+                          |
20|     0x8024    | &amp;lt;------------------------+
21+---------------+&lt;/code&gt;
    &lt;p&gt;Or in code:&lt;/p&gt;
    &lt;code&gt; 1impl&amp;lt;'cpu&amp;gt; Cpu&amp;lt;'cpu&amp;gt; {
 2    pub fn step(&amp;amp;mut self) -&amp;gt; Result&amp;lt;bool, err::Err&amp;gt; {
 3        // ...
 4        match instruction {
 5            decoder::Instruction::LdrLiteral { rd, addr } =&amp;gt; {
 6                self.r[rd as usize] = self.mem.read_u32(addr).expect("Segfault");
 7            }
 8        }
 9        // ...
10    }
11}&lt;/code&gt;
    &lt;p&gt;Any other instruction using a addr will have to also go through the &lt;code&gt;Mem::translate&lt;/code&gt; indirection.&lt;/p&gt;
    &lt;head rend="h2"&gt;Forwarding Syscalls and other feature flag based logic&lt;/head&gt;
    &lt;p&gt;Since stinkarm has three ways of dealing with syscalls (&lt;code&gt;deny&lt;/code&gt;, &lt;code&gt;sandbox&lt;/code&gt;,
&lt;code&gt;forward&lt;/code&gt;). I decided on handling the selection of the appropriate logic at cpu
creation time via a function pointer attached to the CPU as the
&lt;code&gt;syscall_handler&lt;/code&gt; field:&lt;/p&gt;
    &lt;code&gt; 1type SyscallHandlerFn = fn(&amp;amp;mut Cpu, ArmSyscall) -&amp;gt; i32;
 2
 3pub struct Cpu&amp;lt;'cpu&amp;gt; {
 4    /// r0-r15 (r13=SP, r14=LR, r15=PC)
 5    pub r: [u32; 16],
 6    pub cpsr: u32,
 7    pub mem: &amp;amp;'cpu mut mem::Mem,
 8    syscall_handler: SyscallHandlerFn,
 9    pub status: Option&amp;lt;i32&amp;gt;,
10}
11
12impl&amp;lt;'cpu&amp;gt; Cpu&amp;lt;'cpu&amp;gt; {
13    pub fn new(conf: &amp;amp;'cpu config::Config, mem: &amp;amp;'cpu mut mem::Mem, pc: u32) -&amp;gt; Self {
14        // ... 
15
16        // simplified, in stinkarm this gets wrapped if the user specifies
17        // syscall traces via -lsyscalls or -v
18        s.syscall_handler = match conf.syscalls {
19            SyscallMode::Forward =&amp;gt; translation::syscall_forward,
20            SyscallMode::Sandbox =&amp;gt; sandbox::syscall_sandbox,
21            SyscallMode::Deny =&amp;gt; sandbox::syscall_stub,
22        };
23        // ...
24    }
25}&lt;/code&gt;
    &lt;head rend="h3"&gt;Calling conventions, armv7 vs x86&lt;/head&gt;
    &lt;p&gt;In our examples I obviously used the armv7 syscall calling convention. But this convention differs from the calling convention of our x86 (technically its x86-64 System V AMD64 ABI) host by a lot.&lt;/p&gt;
    &lt;p&gt;While armv7 uses &lt;code&gt;r7&lt;/code&gt; for the syscall number and &lt;code&gt;r0-r5&lt;/code&gt; for the syscall
arguments, x86 uses &lt;code&gt;rax&lt;/code&gt; for the syscall id and &lt;code&gt;rdi&lt;/code&gt;, &lt;code&gt;rsi&lt;/code&gt;, &lt;code&gt;rdx&lt;/code&gt;, &lt;code&gt;r10&lt;/code&gt;,
&lt;code&gt;r8&lt;/code&gt; and &lt;code&gt;r9&lt;/code&gt; for the syscall arguments (&lt;code&gt;rcx&lt;/code&gt; can‚Äôt be used since &lt;code&gt;syscall&lt;/code&gt;
clobbers it, thus Linux goes with &lt;code&gt;r10&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;Also the syscall numbers differ between armv7 and x86, &lt;code&gt;sys_write&lt;/code&gt; is &lt;code&gt;1&lt;/code&gt; on
x86 and &lt;code&gt;4&lt;/code&gt; on armv7. If you are interested in either calling conventions,
syscall ids and documentation, do visit The Chromium Projects- Linux System
Call
Table,
it is generated from Linux headers and fairly readable.&lt;/p&gt;
    &lt;p&gt;Table version:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;usage&lt;/cell&gt;
        &lt;cell role="head"&gt;armv7&lt;/cell&gt;
        &lt;cell role="head"&gt;x86-64&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;syscall id&lt;/cell&gt;
        &lt;cell&gt;r7&lt;/cell&gt;
        &lt;cell&gt;rax&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;return&lt;/cell&gt;
        &lt;cell&gt;r0&lt;/cell&gt;
        &lt;cell&gt;rax&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;arg0&lt;/cell&gt;
        &lt;cell&gt;r0&lt;/cell&gt;
        &lt;cell&gt;rdi&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;arg1&lt;/cell&gt;
        &lt;cell&gt;r1&lt;/cell&gt;
        &lt;cell&gt;rsi&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;arg2&lt;/cell&gt;
        &lt;cell&gt;r2&lt;/cell&gt;
        &lt;cell&gt;rdx&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;arg3&lt;/cell&gt;
        &lt;cell&gt;r3&lt;/cell&gt;
        &lt;cell&gt;r10&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;arg4&lt;/cell&gt;
        &lt;cell&gt;r4&lt;/cell&gt;
        &lt;cell&gt;r8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;arg5&lt;/cell&gt;
        &lt;cell&gt;r5&lt;/cell&gt;
        &lt;cell&gt;r9&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;So something like writing &lt;code&gt;TEXT123&lt;/code&gt; to stdout looks like this on arm:&lt;/p&gt;
    &lt;code&gt; 1    .section .rodata
 2txt:
 3    .asciz "TEXT123\n"
 4
 5    .section .text
 6    .global _start
 7_start:
 8    ldr r0, =1
 9    ldr r1, =txt
10    mov r2, #8
11    mov r7, #4
12    svc #0&lt;/code&gt;
    &lt;p&gt;While it looks like the following on x86:&lt;/p&gt;
    &lt;code&gt; 1    .section .rodata
 2txt:
 3    .string "TEXT123\n"
 4
 5    .section .text
 6    .global _start
 7_start:
 8    movq $1, %rax
 9    movq $1, %rdi
10    leaq txt(%rip), %rsi
11    movq $8, %rdx
12    syscall&lt;/code&gt;
    &lt;head rend="h3"&gt;Hooking the syscall handler up&lt;/head&gt;
    &lt;p&gt;After made the calling convention differences clear, the handling of a syscall is simply to execute this handler and use &lt;code&gt;r7&lt;/code&gt; to convert the armv7 syscall
number to the x86 syscall number:&lt;/p&gt;
    &lt;code&gt; 1impl&amp;lt;'cpu&amp;gt; Cpu&amp;lt;'cpu&amp;gt; {
 2    pub fn step(&amp;amp;mut self) -&amp;gt; Result&amp;lt;bool, err::Err&amp;gt; {
 3        // ...
 4
 5        match instruction {
 6            // ...
 7            decoder::Instruction::Svc =&amp;gt; {
 8                self.r[0] = (self.syscall_handler)(self, ArmSyscall::try_from(self.r[7])?) as u32;
 9            }
10            // ...
11        }
12        // ...
13    }
14}&lt;/code&gt;
    &lt;p&gt;Of course for this to work the syscall has to be implemented and even decodable. At least for the decoding, there is the &lt;code&gt;ArmSyscall&lt;/code&gt; enum:&lt;/p&gt;
    &lt;code&gt; 1#[derive(Debug)]
 2#[allow(non_camel_case_types)]
 3pub enum ArmSyscall {
 4    restart = 0x00,
 5    exit = 0x01,
 6    fork = 0x02,
 7    read = 0x03,
 8    write = 0x04,
 9    open = 0x05,
10    close = 0x06,
11}
12
13impl TryFrom&amp;lt;u32&amp;gt; for ArmSyscall {
14    type Error = err::Err;
15
16    fn try_from(value: u32) -&amp;gt; Result&amp;lt;Self, Self::Error&amp;gt; {
17        Ok(match value {
18            0x00 =&amp;gt; Self::restart,
19            0x01 =&amp;gt; Self::exit,
20            0x02 =&amp;gt; Self::fork,
21            0x03 =&amp;gt; Self::read,
22            0x04 =&amp;gt; Self::write,
23            0x05 =&amp;gt; Self::open,
24            0x06 =&amp;gt; Self::close,
25            _ =&amp;gt; return Err(err::Err::UnknownSyscall(value)),
26        })
27    }
28}&lt;/code&gt;
    &lt;p&gt;By default the sandboxing mode is selected, but I will go into detail on both sandboxing and denying syscalls later, first I want to focus on the implementation of the translation layer from armv7 to x86 syscalls:&lt;/p&gt;
    &lt;code&gt;1pub fn syscall_forward(cpu: &amp;amp;mut super::Cpu, syscall: ArmSyscall) -&amp;gt; i32 {
2    match syscall {
3        // none are implemented, dump debug print
4        c =&amp;gt; todo!("{:?}", c),
5    }
6}&lt;/code&gt;
    &lt;head rend="h3"&gt;Handling the only exception: &lt;code&gt;exit&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;Since exit means the guest wants to exit, we can‚Äôt just forward this to the host system, simply because this would exit the emulator before it would be able to do cleanup and unmap memory regions allocated.&lt;/p&gt;
    &lt;code&gt;1pub fn syscall_forward(cpu: &amp;amp;mut super::Cpu, syscall: ArmSyscall) -&amp;gt; i32 {
2    match syscall {
3        ArmSyscall::exit =&amp;gt; {
4            cpu.status = Some(cpu.r[0] as i32);
5            0
6        }
7        // ...
8    }
9}&lt;/code&gt;
    &lt;p&gt;To both know we hit the exit syscall (we need to, otherwise the emulator executes further) and propagate the exit code to the host system, we set the &lt;code&gt;Cpu::status&lt;/code&gt; field to &lt;code&gt;Some(r0)&lt;/code&gt;, which is the argument to the syscall.&lt;/p&gt;
    &lt;p&gt;This field is then used in the emulator entry point / main loop:&lt;/p&gt;
    &lt;code&gt; 1fn main() {
 2    let mut cpu = cpu::Cpu::new(&amp;amp;conf, &amp;amp;mut mem, elf.header.entry);
 3
 4    loop {
 5        match cpu.step() { /**/ }
 6
 7        // Cpu::status is only some if sys_exit was called, we exit the
 8        // emulation loop
 9        if cpu.status.is_some() {
10            break;
11        }
12    }
13
14    let status = cpu.status.unwrap_or(0);
15    // cleaning up used memory via munmap
16    mem.destroy();
17    // propagating the status code to the host system
18    exit(status);
19}&lt;/code&gt;
    &lt;head rend="h3"&gt;Implementing: &lt;code&gt;sys_write&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;The write syscall is not as spectacular as &lt;code&gt;sys_exit&lt;/code&gt;: writing a &lt;code&gt;buf&lt;/code&gt; of &lt;code&gt;len&lt;/code&gt;
to a file descriptor.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;register&lt;/cell&gt;
        &lt;cell role="head"&gt;description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;rax&lt;/cell&gt;
        &lt;cell&gt;syscall number (1 for write)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;rdi&lt;/cell&gt;
        &lt;cell&gt;file descriptor (0 for stdin, 1 for stdout, 2 for stderr)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;rsi&lt;/cell&gt;
        &lt;cell&gt;a pointer to the buffer&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;rdx&lt;/cell&gt;
        &lt;cell&gt;the length of the buffer &lt;code&gt;rsi&lt;/code&gt; is pointing to&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;It is necessary for doing the O of I/O tho, otherwise there won‚Äôt be any &lt;code&gt;Hello, World!&lt;/code&gt;s on the screen.&lt;/p&gt;
    &lt;code&gt; 1use crate::{cpu, sys};
 2
 3pub fn write(cpu: &amp;amp;mut cpu::Cpu, fd: u32, buf: u32, len: u32) -&amp;gt; i32 {
 4    // fast path for zero length buffer
 5    if len == 0 {
 6        return 0;
 7    }
 8
 9    // Option::None returned from translate indicates invalid memory access
10    let Some(buf_ptr) = cpu.mem.translate(buf) else {
11        // so we return 'Bad Address'
12        return -(sys::Errno::EFAULT as i32);
13    };
14
15    let ret: i64;
16    unsafe {
17        core::arch::asm!(
18            "syscall",
19            // syscall number
20            in("rax") 1_u64,
21            in("rdi") fd as u64,
22            in("rsi") buf_ptr as u64,
23            in("rdx") len as u64,
24            lateout("rax") ret,
25            // we clobber rcx
26            out("rcx") _,
27            // and r11
28            out("r11") _,
29            // we don't modify the stack
30            options(nostack),
31        );
32    }
33
34    ret.try_into().unwrap_or(i32::MAX)
35}&lt;/code&gt;
    &lt;p&gt;Adding it to &lt;code&gt;translation::syscall_forward&lt;/code&gt; with it‚Äôs arguments according to the
calling convention we established before:&lt;/p&gt;
    &lt;code&gt;1pub fn syscall_forward(cpu: &amp;amp;mut super::Cpu, syscall: ArmSyscall) -&amp;gt; i32 {
2    match syscall {
3        // ...
4        ArmSyscall::write =&amp;gt; sys::write(cpu, cpu.r[0], cpu.r[1], cpu.r[2]),
5        // ...
6    }
7}&lt;/code&gt;
    &lt;p&gt;Executing &lt;code&gt;helloWorld.elf&lt;/code&gt; now results in:&lt;/p&gt;
    &lt;code&gt;1$ stinkarm -Cforward example/helloWorld.elf
2Hello, world!
3$ echo $status
40&lt;/code&gt;
    &lt;head rend="h3"&gt;Deny and Sandbox - restricting syscalls&lt;/head&gt;
    &lt;p&gt;The simplest sandboxing mode is to deny, the more complex is to allow some syscall interactions while others are denied. The latter requires checking arguments to syscalls, not just the syscall kind.&lt;/p&gt;
    &lt;p&gt;Lets start with the easier syscall handler: &lt;code&gt;deny&lt;/code&gt;. Deny simply returns
&lt;code&gt;ENOSYS&lt;/code&gt; to all invoked syscalls:&lt;/p&gt;
    &lt;code&gt;1pub fn syscall_deny(cpu: &amp;amp;mut super::Cpu, syscall: ArmSyscall) -&amp;gt; i32 {
2    if let ArmSyscall::exit = syscall {
3        cpu.status = Some(cpu.r[0] as i32)
4    };
5
6    -(sys::Errno::ENOSYS as i32)
7}&lt;/code&gt;
    &lt;p&gt;Thus executing the hello world and enabling syscall logs results in neither &lt;code&gt;sys_write&lt;/code&gt; nor &lt;code&gt;sys_exit&lt;/code&gt; going through and &lt;code&gt;ENOSYS&lt;/code&gt; being returned for both
in &lt;code&gt;r0&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;1$ stinkarm -Cdeny -lsyscalls examples/helloWorld.elf
2148738 write(fd=1, buf=0x8024, len=14) [deny]
3=ENOSYS
4148738 exit(code=0) [deny]
5=ENOSYS&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;sandbox&lt;/code&gt; at a high level is the same as deny, check for conditions before
executing a syscall, if they don‚Äôt match, disallow the syscall:&lt;/p&gt;
    &lt;code&gt; 1pub fn syscall_sandbox(cpu: &amp;amp;mut super::Cpu, syscall: ArmSyscall) -&amp;gt; i32 {
 2    match syscall {
 3        ArmSyscall::exit =&amp;gt; {
 4            cpu.status = Some(cpu.r[0] as i32);
 5            0
 6        }
 7        ArmSyscall::write =&amp;gt; {
 8            let (r0, r1, r2) = (cpu.r[0], cpu.r[1], cpu.r[2]);
 9            // only allow writing to stdout, stderr and stdin
10            if r0 &amp;gt; 2 {
11                return -(sys::Errno::ENOSYS as i32);
12            }
13
14            sys::write(cpu, r0, r1, r2)
15        }
16        _ =&amp;gt; todo!("{:?}", syscall),
17    }
18}&lt;/code&gt;
    &lt;p&gt;For instance we only allow writing to stdin, stdout and stderr, no other file descriptors. One could also add pointer range checks, buffer length checks and other hardening measures here. Emulating the hello world example with this mode (which is the default mode):&lt;/p&gt;
    &lt;code&gt;1$ stinkarm -Csandbox -lsyscalls examples/helloWorld.elf
2150147 write(fd=1, buf=0x8024, len=14) [sandbox]
3Hello, world!
4=14
5150147 exit(code=0) [sandbox]
6=0&lt;/code&gt;
    &lt;head rend="h1"&gt;Fin&lt;/head&gt;
    &lt;p&gt;So there you have it, emulating armv7 in six steps:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;parsing and validating a 32-bit armv7 Elf binary&lt;/item&gt;
      &lt;item&gt;mapping segments into host address space&lt;/item&gt;
      &lt;item&gt;decoding a non-trivial subset of armv7 instructions&lt;/item&gt;
      &lt;item&gt;handling program counter relative literal loads&lt;/item&gt;
      &lt;item&gt;translating memory interactions from guest to host&lt;/item&gt;
      &lt;item&gt;forwarding armv7 Linux syscalls into their x86-64 System V counterparts&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Diving into the Elf and armv7 spec without any previous relevant experience, except the asm module I had in uni, was a bit overwhelming at first. Armv7 decoding was by far the most annoying part of the project and I still don‚Äôt like the bizarre argument ordering for x86-64 syscalls.&lt;/p&gt;
    &lt;p&gt;The whole project is about 1284 lines of Rust, has zero dependencies1 and is as far as I know working correctly2.&lt;/p&gt;
    &lt;head rend="h1"&gt;Microbenchmark Performance&lt;/head&gt;
    &lt;p&gt;It executes a real armv7 hello world binary in ~0.015ms of guest execution-only time, excluding process startup and parsing. The e2e execution with all stages I outlined before, it takes about 2ms.&lt;/p&gt;
    &lt;code&gt; 1$ stinkarm -v examples/helloWorld.elf
 2[     0.070ms] opening binary "examples/helloWorld.elf"
 3[     0.097ms] parsing ELF...
 4[     0.101ms] \
 5ELF Header:
 6  Magic:              [7f, 45, 4c, 46]
 7  Class:              ELF32
 8  Data:               Little endian
 9  Type:               Executable
10  Machine:            EM_ARM
11  Version:            1
12  Entry point:        0x8000
13  Program hdr offset: 52 (32 bytes each)
14  Section hdr offset: 4696
15  Flags:              0x05000200
16  EH size:            52
17  # Program headers:  1
18  # Section headers:  9
19  Str tbl index:      8
20
21Program Headers:
22  Type       Offset   VirtAddr   PhysAddr   FileSz    MemSz  Flags  Align
23  LOAD     0x001000 0x00008000 0x00008000 0x000033 0x000033    R|X 0x1000
24
25[     0.126ms] mapped program header `LOAD` of 51B (G=0x8000 -&amp;gt; H=0x7ffff7f87000)
26[     0.129ms] jumping to entry G=0x8000 at H=0x7ffff7f87000
27[     0.131ms] starting the emulator
28153719 write(fd=1, buf=0x8024, len=14) [sandbox]
29Hello, world!
30=14
31153719 exit(code=0) [sandbox]
32=0
33[     0.149ms] exiting with `0`&lt;/code&gt;
    &lt;p&gt;Comparing the whole pipeline (parsing elf, segment mapping, cpu setup, etc) to &lt;code&gt;qemu&lt;/code&gt; we arrive at the following micro benchmark results. To be fair, qemu
does a whole lot more than stinkarm, it has a jit, a full linux-user runtime, a
dynamic loader, etc.&lt;/p&gt;
    &lt;code&gt;1$ hyperfine "./target/release/stinkarm examples/helloWorld.elf" -N --warmup 10
2Benchmark 1: ./target/release/stinkarm examples/helloWorld.elf
3  Time (mean ¬± œÉ):       1.9 ms ¬±   0.3 ms    [User: 0.2 ms, System: 1.4 ms]
4  Range (min ‚Ä¶ max):     1.6 ms ‚Ä¶   3.4 ms    1641 runs
5
6$ hyperfine "qemu-arm ./examples/helloWorld.elf" -N --warmup 10
7Benchmark 1: qemu-arm ./examples/helloWorld.elf
8  Time (mean ¬± œÉ):      12.3 ms ¬±   1.5 ms    [User: 3.8 ms, System: 8.0 ms]
9  Range (min ‚Ä¶ max):     8.8 ms ‚Ä¶  19.8 ms    226 runs&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46004386</guid><pubDate>Fri, 21 Nov 2025 13:30:36 +0000</pubDate></item><item><title>Is C++26 getting destructive move semantics?</title><link>https://stackoverflow.com/questions/79817124/is-c26-getting-destructive-move-semantics</link><description>&lt;doc fingerprint="ae797a39aca737ac"&gt;
  &lt;main&gt;
    &lt;p&gt;Can I express a function that consumes an object? Meaning that its destructor is not run on the moved-from object?&lt;/p&gt;
    &lt;p&gt;Like the proposed library function trivially_locate_at itself?&lt;/p&gt;
    &lt;code&gt;template &amp;lt;class T&amp;gt;
T* trivially_relocate_at(T* dst, T* src);
&lt;/code&gt;
    &lt;p&gt;Naively, if the library authors can, so should I.&lt;/p&gt;
    &lt;p&gt;Problem: Where is the magic sauce? That function signature does not convey that it effectively destructs an object at &lt;code&gt;src&lt;/code&gt;, or the reverse problem, that it effectively constructs an object at &lt;code&gt;dst&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;I suspect the answer is no: The few examples I have found are avoiding it by doing manual memory management with placement-new and std::destroy_at.&lt;/p&gt;
    &lt;p&gt;Reason for asking: I would like to propose what seems missing: Two new pointer qualifiers to express giving and taking ownership. If you can excuse my reuse of the &lt;code&gt;new&lt;/code&gt; and &lt;code&gt;delete&lt;/code&gt; keywords for a moment (it doesn't have to be those):&lt;/p&gt;
    &lt;code&gt;template &amp;lt;class T&amp;gt;
T* trivially_relocate_at(new T* dst, delete T* src);
&lt;/code&gt;
    &lt;p&gt;This is not about optimizing C++, but salvaging it: In order to have static lifetime analysis (akin to Rust) in C and/or C++, I see no way around adding an ability to express static ownership transfer.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46005130</guid><pubDate>Fri, 21 Nov 2025 14:52:36 +0000</pubDate></item></channel></rss>