<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Sat, 06 Dec 2025 23:09:48 +0000</lastBuildDate><item><title>Abstract Interpretation in the Toy Optimizer</title><link>https://bernsteinbear.com/blog/toy-abstract-interpretation/</link><description>&lt;doc fingerprint="4aa9003c4e611a49"&gt;
  &lt;main&gt;
    &lt;p&gt;CF Bolz-Tereick wrote some excellent posts in which they introduce a small IR and optimizer and extend it with allocation removal. We also did a live stream together in which we did some more heap optimizations.&lt;/p&gt;
    &lt;p&gt;In this blog post, I’m going to write a small abtract interpreter for the Toy IR and then show how we can use it to do some simple optimizations. It assumes that you are familiar with the little IR, which I have reproduced unchanged in a GitHub Gist.&lt;/p&gt;
    &lt;p&gt;Abstract interpretation is a general framework for efficiently computing properties that must be true for all possible executions of a program. It’s a widely used approach both in compiler optimizations as well as offline static analysis for finding bugs. I’m writing this post to pave the way for CF’s next post on proving abstract interpreters correct for range analysis and known bits analysis inside PyPy.&lt;/p&gt;
    &lt;p&gt;Before we begin, I want to note a couple of things:&lt;/p&gt;
    &lt;p&gt;Alright, let’s get started.&lt;/p&gt;
    &lt;p&gt;Abstract interpretation means a couple different things to different people. There’s rigorous mathematical formalism thanks to Patrick and Radhia Cousot, our favorite power couple, and there’s also sketchy hand-wavy stuff like what will follow in this post. In the end, all people are trying to do is reason about program behavior without running it.&lt;/p&gt;
    &lt;p&gt;In particular, abstract interpretation is an over-approximation of the behavior of a program. Correctly implemented abstract interpreters never lie, but they might be a little bit pessimistic. This is because instead of using real values and running the program—which would produce a concrete result and some real-world behavior—we “run” the program with a parallel universe of abstract values. This abstract run gives us information about all possible runs of the program.1&lt;/p&gt;
    &lt;p&gt;Abstract values always represent sets of concrete values. Instead of literally storing a set (in the world of integers, for example, it could get pretty big…there are a lot of integers), we group them into a finite number of named subsets.2&lt;/p&gt;
    &lt;p&gt;Let’s learn a little about abstract interpretation with an example program and example abstract domain. Here’s the example program:&lt;/p&gt;
    &lt;code&gt;v0 = 1
v1 = 2
v2 = add(v0, v1)
&lt;/code&gt;
    &lt;p&gt;And our abstract domain is “is the number positive” (where “positive” means nonnegative, but I wanted to keep the words distinct):&lt;/p&gt;
    &lt;code&gt;       top
    /       \
positive    negative
    \       /
      bottom
&lt;/code&gt;
    &lt;p&gt;The special top value means “I don’t know” and the special bottom value means “empty set” or “unreachable”. The positive and negative values represent the sets of all positive and negative numbers, respectively.&lt;/p&gt;
    &lt;p&gt;We initialize all the variables &lt;code&gt;v0&lt;/code&gt;, &lt;code&gt;v1&lt;/code&gt;, and &lt;code&gt;v2&lt;/code&gt; to bottom and then walk
our IR, updating our knowledge as we go.&lt;/p&gt;
    &lt;code&gt;# here
v0:bottom = 1
v1:bottom = 2
v2:bottom = add(v0, v1)
&lt;/code&gt;
    &lt;p&gt;In order to do that, we have to have transfer functions for each operation. For constants, the transfer function is easy: determine if the constant is positive or negative. For other operations, we have to define a function that takes the abstract values of the operands and returns the abstract value of the result.&lt;/p&gt;
    &lt;p&gt;In order to be correct, transfer functions for operations have to be compatible with the behavior of their corresponding concrete implementations. You can think of them having an implicit universal quantifier forall in front of them.&lt;/p&gt;
    &lt;p&gt;Let’s step through the constants at least:&lt;/p&gt;
    &lt;code&gt;v0:positive = 1
v1:positive = 2
# here
v2:bottom = add(v0, v1)
&lt;/code&gt;
    &lt;p&gt;Now we need to figure out the transfer function for &lt;code&gt;add&lt;/code&gt;. It’s kind of tricky
right now because we haven’t specified our abstract domain very well. I keep
saying “numbers”, but what kinds of numbers? Integers? Real numbers? Floating
point? Some kind of fixed-width bit vector (&lt;code&gt;int8&lt;/code&gt;, &lt;code&gt;uint32&lt;/code&gt;, …) like an
actual machine “integer”?&lt;/p&gt;
    &lt;p&gt;For this post, I am going to use the mathematical definition of integer, which means that the values are not bounded in size and therefore do not overflow. Actual hardware memory constraints aside, this is kind of like a Python &lt;code&gt;int&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;So let’s look at what happens when we add two abstract numbers:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;top&lt;/cell&gt;
        &lt;cell role="head"&gt;positive&lt;/cell&gt;
        &lt;cell role="head"&gt;negative&lt;/cell&gt;
        &lt;cell role="head"&gt;bottom&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;top&lt;/cell&gt;
        &lt;cell&gt;top&lt;/cell&gt;
        &lt;cell&gt;top&lt;/cell&gt;
        &lt;cell&gt;top&lt;/cell&gt;
        &lt;cell&gt;bottom&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;positive&lt;/cell&gt;
        &lt;cell&gt;top&lt;/cell&gt;
        &lt;cell&gt;positive&lt;/cell&gt;
        &lt;cell&gt;top&lt;/cell&gt;
        &lt;cell&gt;bottom&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;negative&lt;/cell&gt;
        &lt;cell&gt;top&lt;/cell&gt;
        &lt;cell&gt;top&lt;/cell&gt;
        &lt;cell&gt;negative&lt;/cell&gt;
        &lt;cell&gt;bottom&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;bottom&lt;/cell&gt;
        &lt;cell&gt;bottom&lt;/cell&gt;
        &lt;cell&gt;bottom&lt;/cell&gt;
        &lt;cell&gt;bottom&lt;/cell&gt;
        &lt;cell&gt;bottom&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;As an example, let’s try to add two numbers &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt;, where &lt;code&gt;a&lt;/code&gt; is positive
and &lt;code&gt;b&lt;/code&gt; is negative. We don’t know anything about their values other than their
signs. They could be &lt;code&gt;5&lt;/code&gt; and &lt;code&gt;-3&lt;/code&gt;, where the result is &lt;code&gt;2&lt;/code&gt;, or they could be
&lt;code&gt;1&lt;/code&gt; and &lt;code&gt;-100&lt;/code&gt;, where the result is &lt;code&gt;-99&lt;/code&gt;. This is why we can’t say anything
about the result of this operation and have to return top.&lt;/p&gt;
    &lt;p&gt;The short of this table is that we only really know the result of an addition if both operands are positive or both operands are negative. Thankfully, in this example, both operands are known positive. So we can learn something about &lt;code&gt;v2&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;v0:positive = 1
v1:positive = 2
v2:positive = add(v0, v1)
# here
&lt;/code&gt;
    &lt;p&gt;This may not seem useful in isolation, but analyzing more complex programs even with this simple domain may be able to remove checks such as &lt;code&gt;if (v2 &amp;lt; 0) { ... }&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Let’s take a look at another example using a sample &lt;code&gt;absval&lt;/code&gt; (absolute value)
IR operation:&lt;/p&gt;
    &lt;code&gt;v0 = getarg(0)
v1 = getarg(1)
v2 = absval(v0)
v3 = absval(v1)
v4 = add(v2, v3)
v5 = absval(v4)
&lt;/code&gt;
    &lt;p&gt;Even though we have no constant/concrete values, we can still learn something about the states of values throughout the program. Since we know that &lt;code&gt;absval&lt;/code&gt;
always returns a positive number, we learn that &lt;code&gt;v2&lt;/code&gt;, &lt;code&gt;v3&lt;/code&gt;, and &lt;code&gt;v4&lt;/code&gt; are all
positive. This means that we can optimize out the &lt;code&gt;absval&lt;/code&gt; operation on &lt;code&gt;v5&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;v0:top = getarg(0)
v1:top = getarg(1)
v2:positive = absval(v0)
v3:positive = absval(v1)
v4:positive = add(v2, v3)
v5:positive = v4
&lt;/code&gt;
    &lt;p&gt;Other interesting lattices include:&lt;/p&gt;
    &lt;p&gt;For the rest of this blog post, we are going to do a very limited version of “known bits”, called parity. This analysis only tracks the least significant bit of a number, which indicates if it is even or odd.&lt;/p&gt;
    &lt;p&gt;The lattice is pretty similar to the positive/negative lattice:&lt;/p&gt;
    &lt;code&gt;    top
  /     \
even    odd
  \     /
   bottom
&lt;/code&gt;
    &lt;p&gt;Let’s define a data structure to represent this in Python code:&lt;/p&gt;
    &lt;code&gt;class Parity:
    def __init__(self, name):
        self.name = name

    def __repr__(self):
        return self.name
&lt;/code&gt;
    &lt;p&gt;And instantiate the members of the lattice:&lt;/p&gt;
    &lt;code&gt;TOP = Parity("top")
EVEN = Parity("even")
ODD = Parity("odd")
BOTTOM = Parity("bottom")
&lt;/code&gt;
    &lt;p&gt;Now let’s write a forward flow analysis of a basic block using this lattice. We’ll do that by assuming that a method on &lt;code&gt;Parity&lt;/code&gt; is defined for each IR
operation. For example, &lt;code&gt;Parity.add&lt;/code&gt;, &lt;code&gt;Parity.lshift&lt;/code&gt;, etc.&lt;/p&gt;
    &lt;code&gt;def analyze(block: Block) -&amp;gt; None:
    parity = {v: BOTTOM for v in block}

    def parity_of(value):
        if isinstance(value, Constant):
            return Parity.const(value)
        return parity[value]

    for op in block:
        transfer = getattr(Parity, op.name)
        args = [parity_of(arg.find()) for arg in op.args]
        parity[op] = transfer(*args)
&lt;/code&gt;
    &lt;p&gt;For every operation, we compute the abstract value—the parity—of the arguments and then call the corresponding method on &lt;code&gt;Parity&lt;/code&gt; to get the
abstract result.&lt;/p&gt;
    &lt;p&gt;We need to special case &lt;code&gt;Constant&lt;/code&gt;s due to a quirk of how the Toy IR is
constructed: the constants don’t appear in the instruction stream and instead
are free-floating.&lt;/p&gt;
    &lt;p&gt;Let’s start by looking at the abstraction function for concrete values—constants:&lt;/p&gt;
    &lt;code&gt;class Parity:
    # ...
    @staticmethod
    def const(value):
        if value.value % 2 == 0:
            return EVEN
        else:
            return ODD
&lt;/code&gt;
    &lt;p&gt;Seems reasonable enough. Let’s pause on operations for a moment and consider an example program:&lt;/p&gt;
    &lt;code&gt;v0 = getarg(0)
v1 = getarg(1)
v2 = lshift(v0, 1)
v3 = lshift(v1, 1)
v4 = add(v2, v3)
v5 = dummy(v4)
&lt;/code&gt;
    &lt;p&gt;This function (which is admittedly a little contrived) takes two inputs, shifts them left by one bit, adds the result, and then checks the least significant bit of the addition result. It then passes that result into a &lt;code&gt;dummy&lt;/code&gt; function,
which you can think of as “return” or “escape”.&lt;/p&gt;
    &lt;p&gt;To do some abstract interpretation on this program, we’ll need to implement the transfer functions for &lt;code&gt;lshift&lt;/code&gt; and &lt;code&gt;add&lt;/code&gt; (&lt;code&gt;dummy&lt;/code&gt; will just always return
&lt;code&gt;TOP&lt;/code&gt;). We’ll start with &lt;code&gt;add&lt;/code&gt;. Remember that adding two even numbers returns
an even number, adding two odd numbers returns an even number, and mixing even
and odd returns an odd number.&lt;/p&gt;
    &lt;code&gt;class Parity:
    # ...
    def add(self, other):
        if self is BOTTOM or other is BOTTOM:
            return BOTTOM
        if self is TOP or other is TOP:
            return TOP
        if self is EVEN and other is EVEN:
            return EVEN
        if self is ODD and other is ODD:
            return EVEN
        return ODD
&lt;/code&gt;
    &lt;p&gt;We also need to fill in the other cases where the operands are top or bottom. In this case, they are both “contagious”; if either operand is bottom, the result is as well. If neither is bottom but either operand is top, the result is as well.&lt;/p&gt;
    &lt;p&gt;Now let’s look at &lt;code&gt;lshift&lt;/code&gt;. Shifting any number left by a non-zero number of
bits will always result in an even number, but we need to be careful about the
zero case! Shifting by zero doesn’t change the number at all. Unfortunately,
since our lattice has no notion of zero, we have to over-approximate here:&lt;/p&gt;
    &lt;code&gt;class Parity:
    # ...
    def lshift(self, other):
        # self &amp;lt;&amp;lt; other
        if other is ODD:
            return EVEN
        return TOP
&lt;/code&gt;
    &lt;p&gt;This means that we will miss some opportunities to optimize, but it’s a tradeoff that’s just part of the game. (We could also add more elements to our lattice, but that’s a topic for another day.)&lt;/p&gt;
    &lt;p&gt;Now, if we run our abstract interpretation, we’ll collect some interesting properties about the program. If we temporarily hack on the internals of &lt;code&gt;bb_to_str&lt;/code&gt;, we can print out parity information alongside the IR operations:&lt;/p&gt;
    &lt;code&gt;v0:top = getarg(0)
v1:top = getarg(1)
v2:even = lshift(v0, 1)
v3:even = lshift(v1, 1)
v4:even = add(v2, v3)
v5:top = dummy(v4)
&lt;/code&gt;
    &lt;p&gt;This is pretty awesome, because we can see that &lt;code&gt;v4&lt;/code&gt;, the result of the
addition, is always even. Maybe we can do something with that information.&lt;/p&gt;
    &lt;p&gt;One way that a program might check if a number is odd is by checking the least significant bit. This is a common pattern in C code, where you might see code like &lt;code&gt;y = x &amp;amp; 1&lt;/code&gt;. Let’s introduce a &lt;code&gt;bitand&lt;/code&gt; IR operation that acts like the
&lt;code&gt;&amp;amp;&lt;/code&gt; operator in C/Python. Here is an example of use of it in our program:&lt;/p&gt;
    &lt;code&gt;v0 = getarg(0)
v1 = getarg(1)
v2 = lshift(v0, 1)
v3 = lshift(v1, 1)
v4 = add(v2, v3)
v5 = bitand(v4, 1)  # new!
v6 = dummy(v5)
&lt;/code&gt;
    &lt;p&gt;We’ll hold off on implementing the transfer function for it—that’s left as an exercise for the reader—and instead do something different.&lt;/p&gt;
    &lt;p&gt;Instead, we’ll see if we can optimize operations of the form &lt;code&gt;bitand(X, 1)&lt;/code&gt;. If
we statically know the parity as a result of abstract interpretation, we can
replace the &lt;code&gt;bitand&lt;/code&gt; with a constant &lt;code&gt;0&lt;/code&gt; or &lt;code&gt;1&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;We’ll first modify the &lt;code&gt;analyze&lt;/code&gt; function (and rename it) to return a new
&lt;code&gt;Block&lt;/code&gt; containing optimized instructions:&lt;/p&gt;
    &lt;code&gt;def simplify(block: Block) -&amp;gt; Block:
    parity = {v: BOTTOM for v in block}

    def parity_of(value):
        if isinstance(value, Constant):
            return Parity.const(value)
        return parity[value]

    result = Block()
    for op in block:
        # TODO: Optimize op
        # Emit
        result.append(op)
        # Analyze
        transfer = getattr(Parity, op.name)
        args = [parity_of(arg.find()) for arg in op.args]
        parity[op] = transfer(*args)
    return result
&lt;/code&gt;
    &lt;p&gt;We’re approaching this the way that PyPy does things under the hood, which is all in roughly a single pass. It tries to optimize an instruction away, and if it can’t, it copies it into the new block.&lt;/p&gt;
    &lt;p&gt;Now let’s add in the &lt;code&gt;bitand&lt;/code&gt; optimization. It’s mostly some gross-looking
pattern matching that checks if the right hand side of a bitwise &lt;code&gt;and&lt;/code&gt;
operation is &lt;code&gt;1&lt;/code&gt; (TODO: the left hand side, too). CF had some neat ideas on how
to make this more ergonomic, which I might save for later.3&lt;/p&gt;
    &lt;p&gt;Then, if we know the parity, optimize the &lt;code&gt;bitand&lt;/code&gt; into a constant.&lt;/p&gt;
    &lt;code&gt;def simplify(block: Block) -&amp;gt; Block:
    parity = {v: BOTTOM for v in block}

    def parity_of(value):
        if isinstance(value, Constant):
            return Parity.const(value)
        return parity[value]

    result = Block()
    for op in block:
        # Try to simplify
        if isinstance(op, Operation) and op.name == "bitand":
            arg = op.arg(0)
            mask = op.arg(1)
            if isinstance(mask, Constant) and mask.value == 1:
                if parity_of(arg) is EVEN:
                    op.make_equal_to(Constant(0))
                    continue
                elif parity_of(arg) is ODD:
                    op.make_equal_to(Constant(1))
                    continue
        # Emit
        result.append(op)
        # Analyze
        transfer = getattr(Parity, op.name)
        args = [parity_of(arg.find()) for arg in op.args]
        parity[op] = transfer(*args)
    return result
&lt;/code&gt;
    &lt;p&gt;Remember: because we use union-find to rewrite instructions in the optimizer (&lt;code&gt;make_equal_to&lt;/code&gt;), later uses of the same instruction get the new
optimized version “for free” (&lt;code&gt;find&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;Let’s see how it works on our IR:&lt;/p&gt;
    &lt;code&gt;v0 = getarg(0)
v1 = getarg(1)
v2 = lshift(v0, 1)
v3 = lshift(v1, 1)
v4 = add(v2, v3)
v6 = dummy(0)
&lt;/code&gt;
    &lt;p&gt;Hey, neat! &lt;code&gt;bitand&lt;/code&gt; disappeared and the argument to &lt;code&gt;dummy&lt;/code&gt; is now the constant
&lt;code&gt;0&lt;/code&gt; because we know the lowest bit.&lt;/p&gt;
    &lt;p&gt;Check out the code in the toy repository.&lt;/p&gt;
    &lt;p&gt;Hopefully you have gained a little bit of an intuitive understanding of abstract interpretation. Last year, being able to write some code made me more comfortable with the math. Now being more comfortable with the math is helping me write the code. It’s nice upward spiral.&lt;/p&gt;
    &lt;p&gt;The two abstract domains we used in this post are simple and not very useful in practice but it’s possible to get very far using slightly more complicated abstract domains. Common domains include: constant propagation, type inference, range analysis, effect inference, liveness, etc. For example, here is a a sample lattice for constant propagation:&lt;/p&gt;
    &lt;p&gt;It has multiple levels to indicate more and less precision. For example, you might learn that a variable is either &lt;code&gt;1&lt;/code&gt; or &lt;code&gt;2&lt;/code&gt; and be able to encode that as
&lt;code&gt;nonnegative&lt;/code&gt; instead of just going straight to &lt;code&gt;top&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Check out some real-world abstract interpretation in open source projects:&lt;/p&gt;
    &lt;p&gt;If you have some readable examples, please share them so I can add.&lt;/p&gt;
    &lt;p&gt;Thank you to CF Bolz-Tereick for the toy optimizer and helping edit this post!&lt;/p&gt;
    &lt;p&gt;In the words of abstract interpretation researchers Vincent Laviron and Francesco Logozzo in their paper Refining Abstract Interpretation-based Static Analyses with Hints (APLAS 2009):&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The three main elements of an abstract interpretation are: (i) the abstract elements (“which properties am I interested in?”); (ii) the abstract transfer functions (“which is the abstract semantics of basic statements?”); and (iii) the abstract operations (“how do I combine the abstract elements?”).&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;We don’t have any of these “abstract operations” in this post because there’s no control flow but you can read about them elsewhere! ↩&lt;/p&gt;
    &lt;p&gt;These abstract values are arranged in a lattice, which is a mathematical structure with some properties but the most important ones are that it has a top, a bottom, a partial order, a meet operation, and values can only move in one direction on the lattice.&lt;/p&gt;
    &lt;p&gt;Using abstract values from a lattice promises two things:&lt;/p&gt;
    &lt;p&gt;Something about &lt;code&gt;__match_args__&lt;/code&gt; and &lt;code&gt;@property&lt;/code&gt;… which
(update) I think I got working in this
commit. ↩&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46142949</guid><pubDate>Thu, 04 Dec 2025 02:09:47 +0000</pubDate></item><item><title>Nook Browser</title><link>https://browsewithnook.com</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46170402</guid><pubDate>Sat, 06 Dec 2025 03:32:56 +0000</pubDate></item><item><title>Linux Instal Fest Belgrade</title><link>https://dmz.rs/lif2025_en</link><description>&lt;doc fingerprint="9dbf395bd9b7b4c4"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Linux Install Fest&lt;/head&gt;
    &lt;head rend="h2"&gt;Where and when&lt;/head&gt;
    &lt;p&gt;Linux Install Fest will be held on December 9, 2025 in the JAG3 classroom of the Faculty of Mathematics, at JagiÄeva 5, Belgrade. Entry to the classroom is possible from 6 pm to 9 pm.&lt;/p&gt;
    &lt;p&gt;JagiÄeva street is located between the Pijaca Äeram station where trams 5, 6, 7L and 14 stop, and the Crveni krst station where buses 21 and 83 stop, as well as trolleybuses 19, 22 and 29.&lt;/p&gt;
    &lt;head rend="h2"&gt;Program schedule&lt;/head&gt;
    &lt;p&gt;The goal of the gathering is to help interested install the Linux operating system on laptops. Several people with working Linux experience will be present at the event. In addition, depending on the interest of those present, short trainings related to the command line, git, web services, C programming, etc. can be held.&lt;/p&gt;
    &lt;p&gt;After 9 p.m., we can continue socializing in one of the nearby bars.&lt;/p&gt;
    &lt;head rend="h2"&gt;Linux distributions&lt;/head&gt;
    &lt;p&gt;Linux is the core of the operating system, on which other programs are installed. All of these together make up a particular Linux distribution. There are many distributions, but we recommend the ones with a long tradition like the following:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The Debian distribution is probably the most suitable for Linux beginners. Known derivatives of Debian are Ubuntu, Mint and Zorin.&lt;/item&gt;
      &lt;item&gt;Fedora is also suitable for Linux beginners. It differs from the Debian distribution by the faster release of new versions, which in practice means that users have newer versions of the program.&lt;/item&gt;
      &lt;item&gt;Arch is a Linux distribution that allows the user to easily configure all parts of the system. This distribution is intended for people with significant Linux experience.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you are a beginner and haven't decided which distribution you want to install, we recommend Fedora or Debian. Regardless of which distribution you have, you will be able to run all programs intended for Linux.&lt;/p&gt;
    &lt;head rend="h2"&gt;End of 10&lt;/head&gt;
    &lt;p&gt;This year's Linux Install Fest is organized as part of the global End of 10 campaign, which promotes the Linux operating system as a replacement for Windows 10.&lt;/p&gt;
    &lt;p&gt;For a long time now, the Windows operating system has become increasingly unfriendly to users. On the contrary, many Linux distributions have improved the user experience to the maximum, and today we can claim that Linux enables significantly more pleasant work, regardless of the user's technical knowledge.&lt;/p&gt;
    &lt;p&gt;Windows imposes on users functionalities that users do not want to use, such as: cloud integrations, AI, advertisements, mandatory accounts, and the like. These functionalities serve above all to increase Microsoft's profits, and have no benefit for most end users. Also, basic programs such as calendars, calculators or text editors have become slow and full of bugs. With useless functionalities, Windows becomes more demanding every year and requires the purchase of better hardware, leading to an increase in electronic waste. Unlike Windows, the latest Linux distributions work very well on computers that are more than a decade old.&lt;/p&gt;
    &lt;p&gt;The choice of an operating system is no longer just a technical decision, but also an environmental attitude.&lt;/p&gt;
    &lt;head rend="h2"&gt;Installation methods&lt;/head&gt;
    &lt;p&gt;We can install Linux in three ways:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Inside a virtual machine on Windows. In this way, the user retains his existing operating system and the data on it. Linux in a virtual machine will be significantly slower than an installation without virtualization.&lt;/item&gt;
      &lt;item&gt;In addition to the existing operating system. If it is possible to shrink one of your partitions and free up at least 10GB of space, you can install a Linux operating system in addition to Windows. When booting the computer, the user will be able to choose whether to boot Windows or Linux. With such an installation, there is a certain risk that one of the subsequent Windows updates will reset the bootloader settings, after which a small intervention is required to make the Linux system accessible again.&lt;/item&gt;
      &lt;item&gt;By completely removing the Windows system. In place of the Windows partition, a new partition with the Linux distribution will be placed. Additional partitions that exist may or may not be removed.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Before arrival&lt;/head&gt;
    &lt;p&gt;In order for the installation to be effective, before coming to the Linux Instal Fest, it is necessary to make a backup of the data from the system partition if you decide on the second or third installation option. If you have two partitions (for example, C and D), move the data from the system partition (C:) that you want to keep to the non-system partition (D:). If you don't have an additional partition, you can use a USB flash drive. Pay attention to the files inside the user directory (Desktop, Downloads, Documents,... ), and export bookmarks and passwords from the browser.&lt;/p&gt;
    &lt;p&gt;Also, before your arrival, you can familiarize yourself with the appearance and way of functioning of various Linux distributions. You can try some Linux distributions through the browser, without any installation, on the DistroSea website (sometimes it is necessary to wait a short time to free up resources on the site). Please note that the operating system on this site is many times slower than the system installed on your computer.&lt;/p&gt;
    &lt;head rend="h2"&gt;Organizer&lt;/head&gt;
    &lt;p&gt;The organizer of the event is Decentrala - a group of enthusiasts gathered around the ideas of decentralization and free dissemination of knowledge. So far, we have organized more than 300 events, and we regularly announce the next events on the Events page.&lt;/p&gt;
    &lt;p&gt;In the following period, two more events for Linux beginners will be held at the same location (classroom JAG3):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tuesday December 16 - Introduction to the Linux command line&lt;/item&gt;
      &lt;item&gt;Tuesday, December 23 - Introduction to Git&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Events start at 6pm.&lt;/p&gt;
    &lt;head rend="h2"&gt;Ponovo&lt;/head&gt;
    &lt;p&gt;You can bring defective devices to the Linux install fest: laptops, phones, desktop computers, monitors... We will deliver them to the organization Ponovo in Kikinda during January. This organization will repair these devices and thereby prevent the increase of electronic waste.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46172167</guid><pubDate>Sat, 06 Dec 2025 10:20:19 +0000</pubDate></item><item><title>Autism's confusing cousins</title><link>https://www.psychiatrymargins.com/p/autisms-confusing-cousins</link><description>&lt;doc fingerprint="9d12f95502d28c97"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Autism’s Confusing Cousins&lt;/head&gt;
    &lt;head rend="h3"&gt;A differential diagnosis for the weird and the awkward&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;“I think that these days what we mean by “autism” is basically “weird person disease.””&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Sorbie Richner, Rich Girl Rehab&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“Accurate diagnosis requires consideration of multiple diagnoses. Sometimes, different diagnoses can overlap with one another and can only be differentiated in subtle and nuanced ways, but particular diagnoses vary considerably in levels of public awareness. As such, an individual may meet the diagnostic criteria for one diagnosis but self-diagnoses with a different diagnosis because it is better known.”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Sam Fellowes, Self-Diagnosis in Psychiatry and the Distribution of Social Resources&lt;/p&gt;
    &lt;p&gt;Unsurprisingly, these days I meet many people in the psychiatric clinic who are convinced that they have autism, or suspect (with various degrees of confidence) that they have autism, or report being diagnosed with autism at some point in their lives by some clinician. And for a fair number of such individuals, I cannot say with reasonable certitude that they have autism. The reasons they give for considering autism vary widely, but tend to be along the lines of…&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;“Eye contact makes me very uncomfortable.”&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;“I suck at small talk.”&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;“I have rigid routines.”&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;“I hyper-focus on my hobbies.”&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;“I am always fidgeting.”&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;“Social interaction exhausts me.”&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;“I really bad at making friends.”&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;“I don’t fit in; people find me weird.”&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;What’s interesting about many of the items above is that the number one diagnostic possibility in my mind is an anxiety disorder of some sort. I remember seeing a woman who was a classic example of someone with high neuroticism, poor self-esteem, and severe social anxiety, and she had believed for much of her life that she was autistic because some random doctor somewhere at some point (she couldn’t even remember who or what sort of assessment this involved) had told her that she had autism, and she believed it because it fit in with her experience of being awkward-shy-weird.&lt;/p&gt;
    &lt;p&gt;It is common for me to meet individuals who think they have autism and find myself thinking, “schizoid,” “obsessive compulsive,” “cluster B,” “social anxiety,” “generalized anxiety,” “trauma,” “socially awkward,”… None of these, however, have the mimetic virality of autism.&lt;/p&gt;
    &lt;p&gt;I don’t want to come across as being skeptical of the reality of autism as a diagnosis or as asserting that most people are misdiagnosed. Autism exists, to the extent that any psychiatric disorder exists. Not everyone is misdiagnosed, perhaps even most people. I am not trying to say, “autism is bullshit.” It’s not. I offer the diagnosis of autism as a clinician perhaps as often as I find myself doubting it.&lt;/p&gt;
    &lt;p&gt;What intrigues me is that people are drawn to autism as a diagnosis because it seems to offer recognition of something they’ve lived with: they may be deeply awkward, terribly shy, or bad with people, they may struggle with social interactions, they may find other people annoying, other people may find them weird, they may have a hard time connecting to others, they may have been bullied, and they may have directed their loneliness or introversion towards peculiar interests or hobbies. Autism seems to them to capture all that. It seems like an apt and appealing narrative. But autism may also be the only relevant diagnosis they’ve heard of or are familiar with. They haven’t seen any cool TikToks about being schizoid. No one’s offering them quizzes about being schizotypal. A random pediatrician or primary care doc is not going to tell them they have an obsessive-compulsive style of personality. So when some professional doubts that they have “autism,” they see it as a dismissal or rejection of their “lived experience.” Of course, I am weird-anxious-awkward. How can you say otherwise? What they don’t know is that the choice is not between autism or nothing, but rather between autism and about a dozen other diagnostic possibilities.&lt;/p&gt;
    &lt;p&gt;So for the sake of our collective sanity, let’s consider a few of them…&lt;/p&gt;
    &lt;p&gt;To be diagnosed with autism spectrum disorder according to DSM-5, a person must have ongoing difficulties in social communication and interaction in all three areas: trouble with back-and-forth social connection, problems with nonverbal communication like eye contact and body language, and difficulty making or keeping friendships. They also must show at least two types of repetitive or restricted behaviors, such as repetitive movements or phrases, needing things to stay the same, having very intense focused interests, or being unusually sensitive (or under-sensitive) to things like sounds, textures, or lights. These patterns must have been present since early childhood (even if they weren’t noticed until later when life got more complicated), lead to substantial impairment in functioning, and can’t simply be explained by intellectual disability (or other psychiatric disorders).&lt;/p&gt;
    &lt;p&gt;To “have” autism is simply to demonstrate this cluster of characteristics at the requisite level of severity and pervasiveness. It doesn’t mean that the person has a specific type of brain attribute or a specific set of genes that differentiates them from non-autistics. No such internal essence exists for the notion as currently conceptualized.&lt;/p&gt;
    &lt;p&gt;Autism spectrum is wide enough to have very different prototypes within it. On one end we have profound autism, representing someone with severe autistic traits who is completely dependent on others for care and has substantial intellectual disability or very limited language ability. At the other end, we have successful nerdy individuals with autistic traits and superior intelligence, often seen in science or academia, à la Sheldon Cooper. (Holden Thorp, editor-in-chief of the Science journals and former UNC chancellor, for example, has publicly disclosed his own autism diagnosis.) This wide range is confusing enough on its own, even without considering other conditions that can present with autism-like features.&lt;/p&gt;
    &lt;p&gt;Autism cannot be identified via medical “tests.” It is identified via clinical information in the form of history, observation, and interaction, and the less information available or the more unreliable the information provided is, the more uncertain we’ll be. To have autism is basically a judgment call that one is a good match to a descriptive prototype. We can get this judgment wrong, and we sometimes do get it wrong. (There is nothing wrong with this fallibility as such, as long as we recognize it. Lives have been built on foundations less sturdy.)&lt;/p&gt;
    &lt;p&gt;Autism as a category or identity has taken on a life of its own. I am aware that not everyone in the neurodiversity crowd accepts the legitimacy of clinician judgments or clinical criteria as outlined in the diagnostic manuals, such as the DSM and ICD. There are other ways to ground the legitimacy of self-diagnoses, in theoretically virtuous accounts or pragmatic uses, which require distinct considerations of their own; I don’t reject that. But here, I am concerned with autism as a clinical diagnosis and the accuracy of autism understood in terms of alignment with clinical diagnosis. Would competent and knowledgeable clinicians with access to all relevant clinical information concur that the person’s presentation meets diagnostic criteria for autism? If you don’t really care about that, this post is not for you.&lt;/p&gt;
    &lt;head rend="h4"&gt;Schizoid Personality&lt;/head&gt;
    &lt;p&gt;Schizoid personality describes people who have little desire for close relationships and prefer solitary activities. Unlike people who are simply shy or socially anxious, individuals with schizoid personality style genuinely don’t find relationships rewarding or necessary. They typically appear emotionally detached or cold, show restricted emotional expression, seem indifferent to praise or criticism, and have few if any close friends or confidants. They often live quietly on the margins of society, pursuing solitary interests or jobs. They keep their inner worlds (which can be quite rich) private and don’t seek emotional intimacy with others.&lt;/p&gt;
    &lt;p&gt;In autism, social difficulties stem from genuine challenges with processing social information: difficulty reading facial expressions, understanding implied meanings, picking up on social cues, knowing unwritten social rules, etc. In schizoid personality, the person typically understands social conventions but simply isn’t motivated to engage with them. They withdraw from genuine disinterest. Schizoid personality also lacks the additional features of autism (repetitive or restricted behaviors, various sensory sensitivities).&lt;/p&gt;
    &lt;head rend="h4"&gt;Schizotypal Personality&lt;/head&gt;
    &lt;p&gt;Schizotypal personality describes people who have odd or eccentric beliefs, unusual perceptual experiences, and difficulties with close relationships. Unlike schizoid personality (which involves simple disinterest in relationships), schizotypal includes strange ways of thinking and perceiving the world. People with schizotypal personality might believe in telepathy, feel they have special powers, think random events have special meaning for them personally, or have unusual perceptual experiences (like feeling a presence in the room or hearing whispers). They typically have few close friends, experience social anxiety that doesn’t improve with familiarity, and may appear paranoid or suspicious of others’ motives. Both schizotypal personality and autism can involve social difficulties and odd or eccentric behavior, but in schizotypal personality, the peculiarity comes from magical thinking, paranoid ideas, and perceptual distortions.&lt;/p&gt;
    &lt;head rend="h4"&gt;Obsessive-Compulsive Personality&lt;/head&gt;
    &lt;p&gt;Obsessive-compulsive personality describes people who are preoccupied with orderliness, perfectionism, and control. These individuals are rigid rule-followers who want things to be done “the right way,” have difficulty delegating tasks, and get caught up in details and lists to the point where they lose sight of the main goal. They tend to be workaholics who neglect leisure and friendships, are inflexible about matters of morality or ethics, and are often stubborn and controlling. Both obsessive-compulsive personality and autism can involve rigid adherence to routines, rules, and specific ways of doing things. In obsessive-compulsive personality, the inflexibility comes from anxiety about loss of control. The person is trying to, consciously or unconsciously, manage anxiety through control and perfectionism. In autism, the need for sameness and routine serves different functions. It provides predictability in a world that feels confusing or it helps with sensory regulation rather than anxiety-driven perfectionism.&lt;/p&gt;
    &lt;head rend="h4"&gt;Social Phobia&lt;/head&gt;
    &lt;p&gt;Severe social anxiety is an intense, persistent fear of social situations where a person might be judged, embarrassed, or humiliated. Social anxiety disorder involves overwhelming fear that interferes with daily life. People with this condition worry excessively about saying something stupid, looking foolish, or being rejected. They often avoid social situations entirely, which can lead to isolation, difficulty maintaining employment, and problems forming relationships. Both social anxiety and autism involve social difficulties and withdrawal. Social anxiety usually improves significantly in comfortable, safe environments (like with close family or friends), while autistic social differences tend to be more consistent across all contexts.&lt;/p&gt;
    &lt;head rend="h4"&gt;Borderline Personality&lt;/head&gt;
    &lt;p&gt;Borderline personality disorder involves intense emotional instability, unstable relationships, fear of abandonment, and a shifting sense of self, with people experiencing rapid mood swings and chaotic relationships that alternate between idealization and devaluation of others. While it can resemble autism through social difficulties, emotional dysregulation, rigid thinking, and feeling different from others, the key distinctions are that borderline centers on intense relationship preoccupations and emotional chaos, whereas autism involves genuine difficulty understanding social cues and communication; borderline features rapidly shifting identity and relationship-triggered mood swings, while autism includes stable self-concept, sensory sensitivities, restricted interests, and literal communication that aren’t present in borderline; and borderline symptoms fluctuate dramatically with relationship stability while autistic traits remain consistent across contexts.&lt;/p&gt;
    &lt;head rend="h4"&gt;Social Communication Disorder&lt;/head&gt;
    &lt;p&gt;Social communication disorder is a condition in DSM-5 where someone has significant, ongoing difficulty using verbal and nonverbal communication appropriately in social contexts. People with social communication disorder struggle with the “pragmatic” aspects of language, that is, knowing how to use language effectively in social situations. They may have trouble understanding when to take turns in conversation, knowing how much detail to give, adjusting their speaking style for different situations, understanding implied meanings or hints, picking up on nonverbal cues like body language and facial expressions, and knowing how to start, maintain, or end conversations naturally. This makes forming friendships and relationships difficult and affects life functioning. The social communication problems in social communication disorder look nearly identical to the “Criterion A” features of autism. However, unlike autism, people with social communication disorder don’t show repetitive behaviors, restricted interests, sensory sensitivities, or the need for sameness and routine.&lt;/p&gt;
    &lt;p&gt;Social communication disorder is rarely diagnosed in favor of autism primarily because autism provides access to critical services, insurance coverage, educational support, and legal protections that social communication disorder does not reliably offer, creating strong practical incentives for families and clinicians to prefer the autism diagnosis. Additionally, autism has an established evidence base, validated assessment tools, clear intervention protocols, and a large supportive community with a neurodiversity-affirming culture, while social communication disorder has none of these. It has no community, minimal research, no specific treatments, and little professional awareness since it was only introduced in the DSM in 2013. Service delivery, insurance, and educational systems are built entirely around autism rather than social communication disorder, and since both conditions require similar interventions for social-communication difficulties, there’s little practical incentive to make the diagnostic distinction, especially when the boundary between them (whether restricted/repetitive behaviors are truly absent or just subtle) is often unclear and clinicians are often unsure the distinction really matters.&lt;/p&gt;
    &lt;head rend="h4"&gt;Trauma-Related Disorders&lt;/head&gt;
    &lt;p&gt;Trauma-related disorders, particularly from early developmental trauma, severe neglect, or disrupted attachment, can mimic autism through social withdrawal and avoidance of eye contact (defensive protection rather than social processing difficulties), communication delays and difficulties (from lack of language exposure or trauma’s impact on brain development), emotional dysregulation and meltdowns (from emotional dysregulation rather than sensory overload), repetitive self-soothing behaviors (anxiety management rather than stimming), sensory sensitivities (hypervigilance rather than sensory processing differences), and rigid need for routine (anxiety-driven safety-seeking rather than cognitive processing style).&lt;/p&gt;
    &lt;p&gt;Severe early deprivation can create “quasi-autistic” patterns that can be genuinely difficult to distinguish. The critical distinctions are that trauma-related difficulties typically improve significantly in safe, nurturing environments and with adequate psychological treatment, show more variability across contexts (worse with triggers), are tied to identifiable adverse experiences rather than present from earliest infancy, and lack the restricted interests and genuine social communication processing deficits of autism.&lt;/p&gt;
    &lt;head rend="h4"&gt;Social Awkwardness&lt;/head&gt;
    &lt;p&gt;Social awkwardness refers to social ineptness without meaningful impairment that falls within what is considered normal or typical human variation. This can be mistaken for autism because both may involve limited friendships, preference for solitude, conversation difficulties, reduced eye contact, and intense interests, particularly fueled by online self-diagnosis culture and broad autism awareness. The key distinctions are that socially awkward individuals understand what they should do socially but find it difficult or uninteresting (versus genuinely not understanding unwritten rules), show significant improvement with practice and maturity, are more comfortable in specific contexts, lack the sensory sensitivities and restricted/repetitive behaviors required for autism diagnosis, and generally achieve life goals despite awkwardness rather than experiencing clinically significant impairment.&lt;/p&gt;
    &lt;head rend="h4"&gt;Other conditions to consider in the differential diagnosis of autism &lt;/head&gt;
    &lt;p&gt;Selective Mutism, Intellectual Disability (without autism), Stereotypic Movement Disorder, Attention-Deficit/Hyperactivity Disorder (ADHD), Schizophrenia Spectrum Disorders, Avoidant Personality Disorder, Attachment Disorders, Generalized Anxiety Disorder, Obsessive-Compulsive Disorder, and Rett Syndrome (a characteristic pattern of developmental regression after initial normal development, typically 6-18 months).&lt;/p&gt;
    &lt;head rend="h4"&gt;Additional Caveats &lt;/head&gt;
    &lt;p&gt;Comorbidity is possible and expected. Someone can be autistic and have maladaptive personality patterns, trauma histories, or anxiety disorders that complicate the presentation. Developmental context, response to relationships, and subjective experiences are all very important in looking beyond the surface presentation to understanding the meaning and functions of behaviors.&lt;/p&gt;
    &lt;p&gt;See also:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46172443</guid><pubDate>Sat, 06 Dec 2025 11:18:40 +0000</pubDate></item><item><title>Touching the Elephant – TPUs</title><link>https://considerthebulldog.com/tte-tpu/</link><description>&lt;doc fingerprint="bfb8d835cc3501c8"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Touching the Elephant - TPUs&lt;/head&gt;&lt;head rend="h4"&gt;Understanding the Tensor Processing Unit&lt;/head&gt;&lt;head rend="h2"&gt;Something New&lt;/head&gt;&lt;p&gt;There is mythological reverence for Google’s Tensor Processing Unit. While the world presently watches NVIDIA’s gravity drag more companies into its orbit, there sits Google, imperial and singular. Lots of companies participate in the “Cambrian-style explosion of new-interesting accelerators”[14] – Groq, Amazon, and Tenstorrent come to mind – but the TPU is the original existence proof. NVIDIA should take credit for the reemergence of deep learning, but the GPU wasn’t designed with deep learning in mind. What’s strange is that the TPU isn’t a secret. This research is indebted to Google’s public chest-thumping, but the devices themselves have long been exclusive to Google’s datacenters. That is over a decade of work on a hardware system sequestered behind their walls. That the TPU is so well documented yet without a true counterpart creates a strange asymmetry. Google is well positioned in the AI race because of their decision over a decade ago to build a hardware accelerator. It is because of the TPU.&lt;/p&gt;&lt;p&gt;On the back of DistBelief Google had gotten neural networks running at scale. In 2013 however they realized that they would need to double their datacenter capacity to meet the growing demand for these new services. “Even if this was economically reasonable, it would still take significant time, as it would involve pouring concrete, striking arrangements for windmill farm contracts, ordering and installing lots of computers, etc.” [14] The race against the clock began, and 15 months later the TPU was born. Fast forward to April of this year when Sundar Pichai announced the 7th generation TPU, Ironwood, at Google Cloud Next. The headline figures were eye-popping. 9,216 chips in a pod, 42.5 Exaflops, 10 MW [21]. In 12 years the TPU went from a research project to a goliath rack-scale system.&lt;/p&gt;&lt;p&gt;Perhaps reverence is warranted. The development of the TPU is set against the backdrop of a changing hardware scaling landscape. It used to be that to get better programs you just had to wait. With each new generation of chip Moore’s Law and Dennard Scaling brought enormous tailwinds in transistor density, power efficiency, and wall clock improvements. But in the aughts and 2010s there was no more sitting and no more waiting. The advancements in chip physics were not producing exponential returns as they once had, and workload demands continued growing.&lt;/p&gt;&lt;p&gt;Casting this as mythology however obscures the details and risks making the TPU seem like magic. The development of the TPU is the story of trade-offs and constraints and co-design. It touches hardware, software, algorithms, systems, network topology, and everything in between. It did not happen by accident, but through the deliberate process of design and iteration. When thinking about the TPU it’s natural to ask:&lt;/p&gt;&lt;p&gt;How did we get here?&lt;/p&gt;&lt;head rend="h2"&gt;Slowing Down&lt;/head&gt;&lt;p&gt;For decades the industry relied on Moore’s Law to pack more transistors into a smaller area and on Dennard Scaling to get more energy efficiency from those transistors. This netted out to smaller, faster, and more efficient devices. You didn’t need to change your software or architecture to realize significant gains, regardless of the domain. CPU performance doubled every 1.5 years from 1985-2003, and every 2 years from 2003-2010. The doubling speed since is closer to every 20 years [14]. The AlexNet moment in 2012 charted a course to the current renaissance in neural networks. Different hardware suddenly opened the door for new questions to be asked. The range of problems that neural networks were suited to solve, along with their appetite for bigger data and bigger models, meant that this algorithmic paradigm was taking off as our scaling paradigms began to languish.&lt;/p&gt;&lt;p&gt;Degradation in the reliability of chip performance scaling under different regimes [14]&lt;/p&gt;&lt;p&gt;The TPU falls into the broad classification of hardware accelerators, of which the marquee distinction is that it is specialized for certain computational domains, hence the name Domain Specific Accelerator. Whereas general purpose devices are designed to accommodate the maximum number of program shapes, specialized designs are defined as much by what they can do as what they can’t. They trade off generality for performance. If we can’t rely on Moore’s Law and Dennard Scaling, and there are new workloads demanding attention, the goal is to optimize for the characteristics of those workloads and to discard everything else. Specialization asks what the optimal way to spend a fixed transistor and energy budget is to squeeze out performance.&lt;/p&gt;&lt;p&gt;Linear algebra is ripe for specialization because a relatively small set of parallelizable operations dominate neural networks. For the TPU that meant a monastic focus on those primitives. Neural networks are simple compositions of Matrix-Vector, Matrix-Matrix, and Elementwise computations over large tensors. Consider that matrix multiplication has cubic complexity. While computationally expensive, this one class of operations is the spine for a large fraction of what is required for a neural network. This narrows the window of optimizations that need to be baked into silicon. Matrix multiplies have the property that as the size of inputs grow, the ratio of compute, O(n^3), to data access, O(n^2), improves [15]. If you can dedicate hardware to speeding up arithmetic and coordinating data movement you can exploit this, and the arithmetic properties are complemented by the runtime properties. Neural networks can be fully specified ahead of time. With clever planning a program can be entirely mapped out before an instruction is issued. There was rarely a need before to design, tape out, and deploy custom silicon. Free performance gains made the economics of simply waiting versus the cost of designing an ASIC a non-starter. The decline of hardware scaling made exploring these realities attractive.&lt;/p&gt;&lt;p&gt;Horowitz Energy per Operation [11]&lt;/p&gt;&lt;p&gt;This opportunity is best exploited in the power budget. Compare the relative cost of arithmetic to control, memory access, and data movement. Horowitz [11] notes that over 50% of processor die energy is dissipated in caches and register files. These inefficiencies exist to mitigate the even greater inefficiency of large memory accesses. In [12] they cite that the energy to fetch and interpret instructions is 10-4000x more expensive than to perform simple operations. Moving and accessing data costs significantly more power, and what is required of deep learning is more arithmetic per unit control. Finding ways to circumvent relative power inefficiencies with specialization means rearchitecting chips to remove that waste.&lt;/p&gt;&lt;head rend="h2"&gt;The Inference Chip&lt;/head&gt;&lt;p&gt;Block diagram of TPUv1 [1]&lt;/p&gt;&lt;p&gt;Datacenter expansions plans are a hell of a drug. To stem the tide of models devouring datacenter capacity, the first ASIC needed to focus on inference. Inference only needs a forward pass through the neural network. A simple neural network layer might look like this:&lt;/p&gt;&lt;p&gt;$$ ReLU( (X \cdot W) + b ) $$&lt;/p&gt;&lt;p&gt;Where X and W are input data and model weights, ReLU is a non-linear activation function, and b is a bias term. A matrix multiply followed by some elementwise addition and an elementwise maximum function. Imagine that chaining a handful of these layers together forms the totality of an inference. This simplified view on early model architectures gives us the general template for designing TPUv1. Matrix multiply, some activation looking functions on that result, feed the results to storage, repeat. To meet the initial deadlines the TPU design exploited this loop-like behavior.&lt;/p&gt;&lt;p&gt;TPUv1 is a single-threaded co-processor connected over PCIe with a 24MiB software-controlled Unified Buffer, an 8-bit integer systolic array, and 8GiB DDR3 DRAM. The device runtime lays out tensors, plans memory transfers with a programmable DMA controller between the host and the Unified Buffer (on-chip SRAM), and tiles compute operands. The host sends 12-bit CISC instructions to the device’s instruction buffer which the in-order sequencer consumes to move data to DRAM and issue MXU ops. The datapath consumes ~2/3 of the die area of the chip [1]. Take care to notice what it is not. It is not a multi-level cache hierarchy. There is no multi-threading or branch prediction or prefetching or TLB. The systolic array executes arithmetic and the runtime eliminates control overhead. TPUv1 is a spartan device aimed at making inference fast.&lt;/p&gt;&lt;p&gt;The heart of the device is the Matrix Multiplication Unit (MXU). It is a 256x256, 2D weight-stationary systolic array of processing elements, in this case MACs. The MXU targets dense GEMMs to maximize arithmetic intensity. The TPU is designed to keep the MXU busy. You can find nice animated demonstrations of data moving through the systolic array here or here.&lt;/p&gt;&lt;p&gt;MXU Cycle Timing&lt;/p&gt;&lt;p&gt;We’ll start with a simplified 4x4 systolic array. Although there are design variations of systolic execution [18][36], we are concerned with the 2D weight-stationary variant. The weights are pre-loaded into the array from the right hand side (the top in this diagram), and the inputs stream in from the left hand side (conveniently on the left). Once the weights are loaded they sit resident in the MACs, one weight per MAC. As the inputs flow from left to right, the MACs compute the product of the resident weight and the streamed input each cycle. The result of that computation is passed downward to the next processing element. If a MAC has one of these partial sums, it adds it to the result of the weight/input product and passes that new sum downward. At the bottom edge of the array there are no more computations and the result is passed to a 4096 row x 256-element bank of 32-bit accumulators.&lt;/p&gt;&lt;p&gt;MXU Double Buffering&lt;/p&gt;&lt;p&gt;Notice that weight pre-loading doesn’t happen all at once. It would waste cycles to wait for each MAC to have a resident weight before streaming in inputs. Weight pre-loading instead happens diagonally, with the left-most part of the systolic array receiving weights first. When the left column of processing elements has weights, the inputs begin streaming diagonally top to bottom. This imposes significant timing coordination for such a simple component. Much of the rest of the chips’ design can be thought of as accommodating these timing needs, and a particular instantiation of that is the liberal use of double buffering.&lt;/p&gt;&lt;p&gt;MXUs can perform immense amounts of arithmetic, but data movement/control stops at the edges of the systolic array. Between processing elements there is only result-passing with chains of two-input adders. If either weight or input data is not where it needs to be, stalls burn cycles that hurt MXU utilization. Spelling it out:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;The MXU holds two 64KiB tiles of weights with one reserved for double buffering&lt;/item&gt;&lt;item&gt;Four 64KiB weight tiles act as a FIFO queue to decouple memory accesses and weight loads between DRAM and the MXU&lt;/item&gt;&lt;item&gt;The Unified Buffer stores intermediate results from the accumulators and prepares new data to feed to the systolic array&lt;/item&gt;&lt;item&gt;The bank of accumulators logically splits 4096 rows into two chunks of 2048 rows, one to feed outputs and one to drain them&lt;/item&gt;&lt;/list&gt;&lt;head&gt;Sizing the MXU&lt;/head&gt;The number of processing elements that touch data before it reaches the accumulators grows quadratically with the array size which affects the speed of the computation. For a 256x256 array that is 65,536 MACs vs. 262,144 MACs in the 512x512 configuration. During fill/drain you pay an O(num_edges) cost to populate the buffers. Fewer edges better amortize this overhead. As arrays shrink they are penalized by wiring constraints. They perform less compute per data access and require running many wires between components. Sizing this device is a delicate balance between compute intensity and layout constraints, which we will see again in later generations.&lt;p&gt;The runtime knows how long each operation it issues should take, so it can intelligently overlap them with one another. During matrix multiplications the UB prepares the next batch of inputs, the fixed activation units operate on the results in the accumulators, and the Weight FIFO banks more weights. Matrix multiplies are relatively long latency, which leaves lots of cycles between when work starts and when work ends. The runtime schedules memory accesses, data movement and computation deterministically to minimize stop-the-world pauses rather than make coordination dependent on the MXU. Hiding latency with overlapping improves parallelism, improves data reuse, and conserves energy otherwise wasted in control flow.&lt;/p&gt;&lt;p&gt;The headline figures from their paper are anachronistic by now, but they help contextualize the accomplishment of the first gen chip. 25x as many MACs and 3.5x the on-chip memory of the K80 GPU. 15-30x the inference speed and 30-80x the perf/W of the K80 and the Haswell CPU [1]. The fixed-latency, software-managed design created a hardware accelerator that eschewed prevailing designs that spent energy in cache hierarchies and control overhead. Maniacal focus on mitigating inference bottlenecks with large SRAM and coordinated data movement proved that TPUv1 worked.&lt;/p&gt;&lt;head rend="h2"&gt;The Training Chip&lt;/head&gt;&lt;p&gt;Neural networks need to be trained before they can be used for inference, and TPUv1 was not designed for training. Requirements include backpropagation to modify weights during execution, gradients with higher precision than int8, and support for diverse activation functions. This costs orders of magnitude more FLOPs [2], and those FLOPs must be distributed over multiple devices while maintaining deterministic execution. TPUv1’s fixed activation units were not flexible enough for experimenting with new algorithms. The memory subsystem was not flexible enough to coordinate work between multiple devices. The UB was not flexible enough to tuck more Matrix-Vector work in behind the MXU. The whole device was too tightly coupled. Adding that flexibility, without reverting to a general-purpose processor, needed a radically different datapath.&lt;/p&gt;&lt;p&gt;TPUv2 Block Diagram [2]&lt;/p&gt;&lt;p&gt;TPUv2 was animated from the bones of TPUv1, but only the MXU feels familiar. TPUv2 is a dual-core chip. Each core pairs a scalar controller with programmable vector units, local SRAM, a 128x128 MXU, and HBM. It adds inter-core interconnects (ICI) to communicate between the memory systems of each core and across chips. Two 128x128 MXUs combine to total the same 256x256 array from TPUv1 but simplify the circuit design. Unequal logic, wire, and SRAM scaling on smaller process nodes made arithmetic improvements comparatively free, enabling the chip designers to focus on the laggard scaling axes [2]. For the second generation MXUs that meant two efficiencies over their predecessor: BrainFloat16 and wire routing.&lt;/p&gt;&lt;p&gt;BF16 floating point format [3]&lt;/p&gt;&lt;p&gt;Dynamic range matters more than precision for neural network training. Gradients represented as integers don’t produce adequate convergence behavior; you need floating point numbers to make fine-grained weight updates. Accessing higher precision numerics however means sacrificing die area. Logic circuits need more adders to handle mantissa bits. Floating point adder arrays scale as (M+1) * (M+1), where M is the size of the mantissa, – 576 adders for fp32 and 121 adders for fp16 [14] – totalling more die area and more energy spent on arithmetic. Notice that although bf16 is the same number of bits as fp16, the proportion of exponent bits to mantissa bits is higher. bf16 only requires 64 adders in the MAC circuitry, and less circuitry means more MACs in the same package and power budget [2][14].&lt;/p&gt;&lt;p&gt;MXU Sizing Considerations [32]&lt;/p&gt;&lt;p&gt;Chip geometry considerations extend beyond individual processing elements. Big cores need long, global wires routed to/from functional units, FIFOs, and control units. Though wire diameters shrink on improved process nodes, their resistance and capacitance scale unevenly. Long wires are chunked into shorter segments connected with repeaters, but this induces signal delay making circuit timings more complex [5]. MXU configurations with multiple smaller cores shorten average wire lengths but need wires routed all over the chip. The trade off is between compute bandwidth and array utilization. Compute utilization scales down quadratically with the array area, but smaller arrays use more energy-efficient wires. Splitting the die into two cores and running fewer, shorter wires to the vector and control units balances wiring scaling with utilization.&lt;/p&gt;&lt;p&gt;TPU Scalar Unit [32]&lt;/p&gt;&lt;p&gt;All those wires have to lead to somewhere. To drive the new datapath, TPUv2 introduces the scalar unit. When a user submits a program, the XLA compiler performs static analysis, lowering the program into 322-bit VLIW instruction bundles. XLA schedules DMAs, vector ops, and MXU work in a deterministic stream. The complexity of organizing program control flow is absorbed by software, keeping the scalar unit relatively simple. It is single-threaded and contains 4KB of scratchpad SRAM (SMEM), small instruction memory (IMEM), and a 32 element, 32-bit register file (SReg) connected to a dual-issue ALU. Sync registers flag when arithmetic and memory blocks are busy to explicitly synchronize execution. The host sends instructions over PCIe to HBM, where they are DMA’d into the Scalar Unit’s IMEM as overlays. Scalar instruction slots execute locally, and the vector/matrix slots are decoded and dispatched to the VPU/MXU [3]. There is no dynamic runtime scheduling, just instruction fetch, decode, and forward.&lt;/p&gt;&lt;p&gt;Two programmable vector processing units (VPU) consolidate the fixed function blocks from TPUv1. The VPU is a 2D SIMD processor designed to increase the ratio of vector operations to matrix operations. Each VPU has 128 vector lanes with 8 sublanes. Each sublane is connected to 32 dual-issue ALUs with lane-local register files (Vregs). The VPU is backed by 16MiB on-chip Vector Memory (VMEM) that mediates data movement to the MXU with pushes/pops onto a Result FIFO [3]. Each core’s VMEM has local access to half of the chip’s HBM, and DMAs to VMEM are strided to fetch contiguous tiles of data rather than issuing many small DMAs. The VPU accesses VMEM with explicit loads/stores to Vregs which remove the need for a cache hierarchy.&lt;/p&gt;&lt;p&gt;The simplicity of describing the rearchitected datapath belies the complexity that the subsystems represent. Whereas general purpose devices use branch predictors, TLBs, Out of Order execution, and a bevy of techniques to shuttle data and instructions, the TPU routes around a cache-centric design with software-managed execution. The aforementioned general purpose mechanisms alleviate runtime dependencies at the expense of more hardware and more energy. Control and caches consume massive amounts of the limited energy budget, so redesigning this subsystem is the difference between an economic chip and a renegotiated contract with power providers. When you know what operations you need, the order you need them in, and the operational characteristics of the hardware, you can move control flow to compile time. The VPU and Scalar Units are co-designed to leverage this operating paradigm, moving program orchestration to software.&lt;/p&gt;&lt;p&gt;Sample VLIW Instructions&lt;/p&gt;&lt;p&gt;VLIW instructions expose this complexity. They contain slots for 2 scalar, 4 vector, 2 matrix, 1 miscellaneous, and 6 immediate instructions [3]. Slots map to scalar/vector/matrix arithmetic, loads and stores, DMAs, synchronization flags, and data literals. Though innocuously named, the miscellaneous slot controls heaven and earth. It is reserved for kernel launches, DMAs, and synchronization guards which we can think of as WAITs. Data dependencies must be carefully sequenced to ensure operation A finishes before operation B uses its results. XLA utilizes the misc slot to keep subsystems working while guarding against illegal instruction sequences. Operational latencies are known constants at compile time, and XLA can use those values to place WAIT instructions at exactly the right point in the VLIW stream to minimize stalls.&lt;/p&gt;&lt;p&gt;Simplified TPU Instruction Overlay&lt;/p&gt;&lt;p&gt;Subsystems operate with different latencies: scalar arithmetic might take single digit cycles, vector arithmetic 10s, and matrix multiplies 100s. DMAs, VMEM loads/stores, FIFO buffer fill/drain, etc. all must be coordinated with precise timing. The MXU might be busy executing a matrix multiply for 128 cycles, meanwhile the VPU is preparing the next tile of weights for the Result FIFO. While DMAs prepare new data for VMEM a DMA_OVERLAY instruction gets inserted to fetch new instructions for IMEM. When the MXU finishes a tile, the hardware sends a signal to clear the MXU_BUSY bit in the scalar unit’s sync registers. When the scalar unit evaluates a WAIT_MXU instruction it sees that the bit is unset and hops to the next instruction for decoding. The scalar unit JUMPs to the new VLIW bundle region and the program continues. Seamlessly overlapping the work of an arbitrary DAG requires extraordinary co-design between the device and the software.&lt;/p&gt;&lt;p&gt;Decoupling the hardware gave software the capacity to drive massive data and instruction level parallelism. VLIW slots can launch 8 operations per cycle. That is 2048 vector ALUs and two 128x128 systolic arrays with minimal control overhead. HBM, VMEM, Vregs, and the MXU all remain busy with the same pipelining and overlap philosophy from TPUv1, only now massively scaled up. XLA wrests power away from control and back into the arithmetic units with coordinated, deterministic execution. Determinism across devices requires explicit communication between chips.&lt;/p&gt;&lt;p&gt;ICI forms the backbone of the training pods. It creates a coherent communication fabric that lets chips operate locally while composing into a mesh of devices acting as one large core. Two on-chip ICI links route data between the HBM and VMEM of each core. Four 496Gbit/s bidirectional off-chip links connect a TPU to its neighbors in the rack with OSFP passive copper. RDMAs over this fabric let chips treat remote HBM as explicitly addressable endpoints. Racks arrange 256 chips as a 16x16 2D torus over ICI to form the full supercomputer pod. ICI removes frequent host communication, skipping the cost of network cards, switches, and communication delays. All this sacrifices 13% of the die area for gains in distributing computations [3].&lt;/p&gt;&lt;p&gt;One dimensional torus wraparound&lt;/p&gt;&lt;p&gt;Let’s imagine that we’re playing a game of telephone. You and 8 friends are arranged in a 3x3 grid, and you can only communicate with your adjacent neighbors. Your goal is to send a message from the person at (0,0) to the person at (2,2) in the fewest messages. Many paths achieve this, but the shortest one is always four. Now imagine that the people on the left edge of the grid can wrap messages around to people on the right edge of the grid. This is logically like mirroring you and all your friends over that wraparound axis. These 3 new connections make our shortest path 3 instead of 4.&lt;/p&gt;&lt;p&gt;Logical mirroring in two dimensional torus wraparound, adapted from [5]&lt;/p&gt;&lt;p&gt;ICI plays this game of telephone in two dimensions. During backpropagation and optimizer state updates intermediate values accumulate across different partitions of the model located on different chips. Results must be broadcast to all the chips participating in the computation for synchronization. Whereas on-chip work is explicitly synchronized with hardware flags, work across chips is implicitly synchronized with MPI-style collectives (All-to-All, AllReduce, etc.). Torus topologies improve communication bandwidth and increase access to different communication patterns during synchronization.&lt;/p&gt;&lt;p&gt;32 wraparound links at 496Gbit/s enable 15.9Tbit/s of bisection bandwidth [3], which tells us how much data can move through the network. In a 4x4 array, a cut down the middle would sever 4 connections. That same cut down the middle of a 2D torus severs 8 connections. Even if each connection carries the same amount of data, there are more paths for data to move through which helps reduce congestion. XLA absorbs the complexity of cross-device scheduling. Software can trust that RDMAs will reach their intended stacks of HBM traveling along the ICI interface.&lt;/p&gt;&lt;p&gt;The same DNA ostensibly runs through TPUv1, yet the chips look and feel utterly different. The microarchitecture, software, and networking each became independently sophisticated parts of a larger system. Subsystems decoupled from one another yet still composed neatly. Where TPUv1 tightly choreographed everything, TPUv2 divided components into independent, asynchronously operating units communicating through explicit queues and synchronization points. TPUv3 was a minor revision in comparison. It has two MXUs per core, an increased clock, double the HBM capacity with 30% higher bus speeds, higher ICI bandwidth, and scales up to a 1024 node liquid-cooled rack. The dies only increased 6% relative to TPUv2 because engineers learned how to better lay the chip out [3]. Scaling the system to meet the continued growth of neural networks pushed future designs into new territory.&lt;/p&gt;&lt;head rend="h2"&gt;Scaling Up&lt;/head&gt;&lt;p&gt;As the footprint of the system grew, so too did the complexity of operating it. Our focus up to now has emphasized chip-local comparisons, e.g. How expensive are these operations relative to one another? How does the memory hierarchy work? How do subsystems A and B communicate on-device? While the TPUs remain the atom of the supercomputer, as we zoom out we observe the crystalline structure of the racks and pods. The fourth generation TPU is better examined thinking about memory as one unified domain. Specialization forces care in the microarchitecture, but the questions change. Where are collectives slow? How are larger tensors handled? Can we scale the racks further? Viewing the world from low altitude we find that TPUv4’s design emphasizes system scaling and energy management.&lt;/p&gt;&lt;p&gt;Peeking behind the accounting curtain for a moment, they note that “most OpEx cost is for provisioning power and not for electricity use, so saving power already provisioned doesn’t improve TCO as much as one might hope” [5]. Total Cost of Ownership (TCO) tries to consider the all in cost of the pods. On the back of a napkin we break this out into CapEx (equipment, installation, etc.) and OpEx (personnel, maintenance, power, etc.). Initially CapEx might dominate ASIC design, but as the platform matures, thinking through operational requirements produces different sets of optimizations. The need for fast, power efficient devices remains but extends out into the unknowable future. As model demands increase, better economics need compositional scalability in an efficient power envelope.&lt;/p&gt;&lt;p&gt;A brief note: TPUv4 is the training design and TPUv4i is the inference design. The impetus was to keep training and inference chips nearly identical so that there weren’t two separate designs awkwardly diverging into separate projects [5]. The relevant change is that the inference chip has one core while the training chip is dual-core.&lt;/p&gt;&lt;p&gt;Simplified Model of TPUv4 Systolic Execution&lt;/p&gt;&lt;p&gt;Fourth generation chips keep TPUv3’s MXU footprint, totaling 4 MXUs per core. In previous MXU designs partial sums moved downwards each cycle through a series of N two-input adders, where N is the size of the array, before reaching the output accumulators. TPUv4 batches groups of four products before passing them to custom 4-input adders. Batching products reduces the length of the adder chain from N to N/4, quartering the operational latency. Above we see 12 PEs bank four multiplies to reduce hops from 12 to 3. The specific implementation of these circuits isn’t clear from the paper, but this should provide enough motivation to understand the change. This circuit design decreases die area 40% and reduces peak power 12% [5].&lt;/p&gt;&lt;p&gt;CMEM Speed Ups [4]&lt;/p&gt;&lt;p&gt;Accessing DRAM is still expensive, and inference workloads underutilize chips. TPUv4 adds 128MiB shared CMEM that is like an L3 cache but with the niceties of software-managed programmability. CMEM helps to keep all 4 MXUs busy with computations at the cost of 28% of the TPUv4 die area. On the 7nm process node, SRAM memory accesses are 20x more energy-efficient than DRAM accesses [5]. CMEM’s memory bandwidth sits in between HBM and VMEM, but unlike HBM it can both read and write data. Expanding the memory hierarchy and keeping data closer to the arithmetic units allows XLA to cut out expensive trips to DRAM. During inference, prefetching model weights into SRAM for multi-tenancy drives higher utilization of chip resources that may otherwise be sitting idle. The ability to swap weights out from SRAM rather than DRAM makes paying the context switching cost feasible. All that die area and upfront CapEx gets amortized over the life of the chip in TCO so long as XLA can effectively leverage it. SparseCore Block Diagram [4] Contrary to the prevailing LLMs-everywhere paradigm, ad serving and recommendation models (DLRMs) run the world. SparseCores (SC) are built to accelerate these models at the cost of 5% die area and power [4]. The key features of these models are their usage of embeddings. Embeddings map data into enormous sparse matrices. Efficiently handling these sparse matrices requires clever strategies to shard tensors across devices and to make looking up the correct slice of data fast. Unstructured sparsity suffers massive memory traffic and imbalances between compute, communication, and data-dependent execution. The MXU is ill-suited to make progress on sparse workloads because they waste cycles on empty computations and don’t directly manage communication. SparseCores address this class of models with a “Sea of Cores” architecture designed to accelerate collectives and memory accesses [4]. SCs are segmented into 16 individual compute elements (tiles) near DRAM that support multiple outstanding memory accesses [4]. Tiles communicate over a data crossbar with one another and over the on-chip fabric to the rest of the device. A stream of CISC instructions enabling data-dependent communication gets issued by the processor’s core sequencer. The Fetch unit (8-wide SIMD vector processor, scVPU) reads data from HBM into 2.5MiB of sparse memory (Spmem), and the Flush Unit writes data out to HBM. Five on-board cross channel units (XPU) perform embedding specific operations. When embeddings are distributed across remote devices SCs leverage the existing ICI bandwidth to access remote memory. The dataflow looks as follows: SCs alleviate the need for the MXU to handle computation and memory traffic on sparse data. They remove the CPU/DRAM bottleneck and shift sparse phases off the MXU path. The cores issue many RDMAs across the global address space of the TPUv4 pods, speeding up embeddings based models 30.1x versus CPUs [4]. Dedicating a small amount of die area to the gather/scatter intensive DLRMs allows the device to be flexible and efficient under multiple algorithmic regimes.&lt;head&gt;SparseCores&lt;/head&gt;&lt;/p&gt;&lt;p&gt;Cores are getting crowded: MXUs, SparseCores, VPUs, HBM, and ICI routers. We see this component management pressure in the VLIW bundles. Driving the additional MXUs and CMEM required the VLIW bundle size to expand ~25% [5]. Adding new subsystems to the microarchitecture adds efficiencies that bubble up to system level performance, but lurking behind each of these changes is the specter of wiring. Fitting more efficient work onto the package with point-to-point connections became too great a tax. Training racks need to be close to one another in the datacenter to amortize the cost of cooling infrastructure, and this physical constraint forces the usage of optical fiber. ICI cabling in TPUv2/v3 coupled rack deployments so that a supercomputer couldn’t go into operation until the full pod was deployed [5]. To realize the TCO and energy wins of the microarchitecture system scaling needed to decouple and compose.&lt;/p&gt;&lt;p&gt;TPUv4i Floorplan [5]&lt;/p&gt;&lt;p&gt;The ICI needed to breathe. Previous revisions of ICI handled both on-chip communication and off-chip communication. More wires needed to be routed to/from the ICI interface as the number of components grew. This circuit layout pressure was complemented by the equally frustrating reality that handling on-chip and off-chip communication increased contention for ICI bandwidth. TPUv4 separates these concerns by adding a dedicated on-chip interconnect (OCI) fabric. The OCI interface handles data movement on-chip so that ICI can solely route traffic across chips. Notice in the fourth generation floorplan how much die area is reserved for OCI [5]. Shorter wires run between components and OCI rather than point-to-point. The OCI interface acts as the mailman. The Scalar Unit drops a message off at the OCI to submit a DMA to DRAM, and the OCI routes it to the memory controller. It tucks subsystem communication behind a unified data exchange interface that shortens wire routes and opens a path to flexible scaling in future designs.&lt;/p&gt;&lt;p&gt;Arbitrating memory accesses between HBM, VMEM, IMEM, SMEM and now CMEM meant maintaining too many sets of independent lanes. OCI uses 512B-wide native data paths segmented into four, 128B-wide groups across the memory hierarchy. Each group serves a quarter of the total HBM bandwidth (153GB/s) so that independent transfers don’t serialize behind one another [5]. Transferring small IMEM overlays shouldn’t have to wait on the completion of a long-latency tensor DMA. This partitioning strategy gives software more flexibility when scheduling work across a device. The full HBM bandwidth is available to each group, but software can schedule multiple concurrent transfers instead of funneling everything through one contested path. XLA plans large transfers to CMEM, CMEM feeds the arithmetic units, OCI handles message passing, and ICI routes and manages RDMAs. OCI and CMEM jointly help to improve spatial locality and reduce trips to HBM. TPUv2/v3 used two-dimensional, relaxed order DMAs to stride along two axes when moving data. This forced XLA to decompose complex tensor reshapes into multiple DMA operations. TPUv4(i) uses four-dimensional DMAs that stride along three axes moving 512-byte chunks [5]. Operations that previously required multiple round-trips to memory now happen in a single DMA. The architecture distributes DMA engines throughout the chip rather than centralizing them. Each engine acts as a co-processor that can decode and execute tensor operations independently. The unified design works identically for on-chip transfers, cross-chip transfers, and host transfers. XLA inserts explicit synchronization, but in exchange it gets predictable performance and the freedom to schedule data movement aggressively. The compiler knows the latency and pipelines around it.&lt;head&gt;4D Tensor (R)DMAs&lt;/head&gt;&lt;/p&gt;&lt;p&gt;TPUv3 had already resorted to optical fiber across racks to enable the full 2D torus, but the 1024 node supercomputer could not expand its physical footprint. Rigid ICI wiring constraints meant individual racks couldn’t be used until each pod was deployed, and the system topology was fixed as configured unless a technician recabled the pod. Rack maintenance brought the whole pod offline with it. Optical Circuit Switching (OCS) infrastructure was the cure. Even though optical solutions are expensive, OCS optical components represent less than five percent of both system and power costs [4][10]. Centralizing cross-rack communications inserted massive programmability into the system. Substituting the cross-rack links with a programmable OCS provided massive gains in “scale, availability, utilization, modularity, deployment, security, power, and performance” [4], unlocking a new scaling paradigm.&lt;/p&gt;&lt;p&gt;OCS Logical Diagram&lt;/p&gt;&lt;p&gt;Each rack in TPUv4 is a 4x4x4 cube, where this cube configuration is chosen to optimize all-to-all communications. Previous pod sizes (16x16 in v2, up to 128x32 in v3) were topology-limited. Devices could communicate between racks over ICI, but the system topology was statically programmed by the cabling. OCS removed these hard limits by centralizing cross-rack communication over an optical switching fiber. OCS offloads link establishment to an array of MEMS mirrors that dynamically configure links between devices in milliseconds [4]. New system topologies can be programmed on the fly by software, placing workloads on idle, non-contiguous machines. Dynamically reconfiguring the OCS improves system availability, tolerating outages in 0.1% - 1.0% of the CPU hosts [6]. TPUv4 pods scale up to 8x8 racks totaling a 4096 node cluster connected over OCS.&lt;/p&gt;&lt;p&gt;The OCSes isolate scaling complexity. Each rack contains 64 chips laid out logically as a cube. With 6 cube faces (+/- X/Y/Z), and 16 (4x4) chips per face, 96 optical links go to the OCS per rack. In the full 64 (8x8) rack pod, that is 6,144 uplinks to the OCS. This requires 48 OCSes that have 128 active ports to connect all the uplinks [4]. Moving cross-rack interconnects to a dedicated optical panel at this scale enabled programmable topologies, eased deployment by decoupling racks, and allowed software to effectively use OCS as a “plugboard” to route around node and link failures. MEMS Mirrors [10] OCSes use micro-electro-mechanical systems (MEMS) mirrors that tilt in three dimensions to steer optical beams. Each OCS contains two arrays of 136 mirrors. Each mirror has four voltage-controlled actuators that rotate it along two axes, steering light from any input port to any output port with sub-degree accuracy. Rather than monitoring each of the 136 mirrors with a dedicated photodetector, OCS uses a single camera per array with an 850nm monitoring laser. Image processing algorithms optimize the high-voltage driver signals to minimize insertion loss across the entire array. Once positioned, each mirror draws 10s of milliwatts to maintain alignment [10]. Circulators [10] Circulators double the OCS’s effective capacity by enabling bidirectional communication. A circulator is a three-port optical device. Light entering port 1 exits port 2, light entering port 2 exits port 3. This cyclic property means a single fiber and a single OCS port can carry traffic in both directions simultaneously halving the required fiber count and OCS ports [10].&lt;head&gt;Mirror, Mirror on the Wall&lt;/head&gt;&lt;/p&gt;&lt;p&gt;Full connectivity of the OCS across the pods meant that the torus topologies of the previous generations could now add a third wraparound dimension. The distance between racks was no longer a constraint, and since the OCS can program chip-to-chip connections on the fly a path to new topologies emerged. Not only could the connections between racks wrap around the z-dimension, they could twist.&lt;/p&gt;&lt;p&gt;Sample 1D Twisted Tori&lt;/p&gt;&lt;p&gt;We’ll make one modification to our previous wraparound topology diagram. Instead of wraparounds connecting only to the other side of their respective row/column, OCS programmability means that these connections can be offset. Adding twists to the wraparounds is an option not a requirement. Having the option to twist the network topology allows for new questions, e.g. given the communication pattern of this model, how should data be sent between participating chips? Twists make algorithmic experimentation and optimization two independently tractable targets and broadens the horizon of available efficiencies. Even without twisted topologies a third wraparound dimension adds bisection bandwidth to the network. The bisection bandwidth of 2D tori scales with the side length of the interconnects, N^(1/2). Adding the additional wraparound dimension scales bisection bandwidth with the area of the interconnects, N^(2/3). More paths in the topology shorten hops between participating nodes and alleviate system congestion along busy routes during synchronization. OCS better utilizes available devices and diversifies achievable topologies.&lt;/p&gt;&lt;p&gt;TPUv4(i) requires our thinking to broaden. We shouldn’t forget the impacts that microarchitecture improvements drive, but we need to consider the economics of the system holistically. Building warehouse scale solutions requires thinking about power provisioning, rack availability, interconnects, network topology, and accounting. Energy efficiency is still the overarching principle, but at datacenter scale. The message is simple: Target TCO over CapEx [5]. Adding CMEM is more expensive now but less expensive over time. Optical interconnects are expensive now but cost &amp;lt;3% of the fully operational system [4]. The duration of the design trade-offs became smeared into the future. All the same apparitions motivating TPUv1 go bump in the night, but they cast shorter shadows. TCO implies a system that requires operation, and the software that keeps the system available is an equal part of TPU’s development.&lt;/p&gt;&lt;head rend="h2"&gt;Island Hopping&lt;/head&gt;&lt;p&gt;Up to now we have enjoyed the quiet refuge of spreadsheet analysis, but the world is imperfect. Hardware dies, electricity spikes, and networks suffer congestion. The triumph of composing the system into decoupled, single responsibility units is not trivial, but infrastructure needs to serve real users. A cast of supporting software must keep chips available. Rock solid hardware relies on software to rationalize TCO obsession. The software is as much a part of the TPU story as the hardware.&lt;/p&gt;&lt;p&gt;We want to train a model. We decide which devices we need, pay rent, and start gawking at loss curves. When we submit our job for execution we don’t worry about the thousands of eager folks just like us. This mass of users vying for a fixed number of TPUs in sporadic intervals presents a problem. As the infrastructure provider what matters is that users don’t experience downtime. Components regularly fail and workloads are hard to predict. Once power has been provisioned every second of idle chip time or suboptimal workload allocation works against your best TCO approximations. Whether by underutilization or oversubscription, wasted resources are the enemy. Outer loop software that manages TPUs coordinates with XLA to find available nodes, check resource health, and configure ICI/OCS [6]. XLA needs to know which TPUs the computation will run on as well as the requested network topology because device placement is part of the program. Optimizing the system for high availability means dealing with the constraints imposed by ahead of time scheduling.&lt;/p&gt;&lt;p&gt;TPU Fragmentation [6]&lt;/p&gt;&lt;p&gt;Slices, Single Program Multiple Data (SPMD), and gang scheduling undergird TPU execution. Most workloads don’t consume an entire pod. Slices are declarations in code that allow developers to request an &amp;lt;X,Y,Z&amp;gt; device mesh which XLA uses to partition and shard models. This abstraction squirrels away both topology size and communication patterns. Pipeline parallelism may want a 2x2x1024 slice while data parallelism wants a 16x16x16 slice. The topology choice optimizes which communications are fast and which are slow. Mapping communications to a slice topology gives developers the freedom to experiment with parallelism strategies.&lt;/p&gt;&lt;p&gt;ICI coupling in TPUv3 meant the scheduler needed to find contiguous, healthy chips for workload placement. OCS lifted that restriction in TPUv4, but in both generations once a set of devices is selected the topology remains static for the duration of the program. A program owns the devices that it runs on until the program exits [8]. Concurrent users submitting unknowable slice sizes makes assigning devices like Tetris. The scheduler must place new jobs onto devices as old jobs pop in and out of existence. It needs mechanisms to rebalance suboptimal device allocations.&lt;/p&gt;&lt;p&gt;A single executable distributed to each participating device runs an identical program. SPMD encapsulates this many devices, single program framework. Developers write models as if they are running on one giant device, and the complexity of managing device-level data placement disappears from view. XLA’s partitioner rewrites every operation in the model to work on local tensor shards, inserting an AllReduce where gradients need to sync, scattering data where it needs to spread, and gathering results where they need to combine [7]. The single logical program becomes thousands of coordinated physical programs each operating on its local slice of data. Control is synchronized explicitly on-device with VLIW barriers and implicitly between devices by collectives. Gang scheduled execution means that each device launches the program all at once, trading off runtime resilience for performance. When a fault crops up during execution the job must be checkpointed and relocated [8]. The hardware stays simple, the software stays deterministic, but the orchestration layer must handle outages, link failures, and maintenance.&lt;/p&gt;&lt;p&gt;TPU Job Lifecycle [6]&lt;/p&gt;&lt;p&gt;Software must anticipate failures to juggle pre-allocated workloads. In [6] they note “To train a model, all TPU processes must be simultaneously up to synchronously update their weights via ICI collectives. A single failed, or interrupted process will interrupt the whole training process.” When a user submits a job, the cluster management client Borg queues it. If resources are fragmented or a job fails, Borg can preempt running workloads to shuffle them to different devices. When a job is ready to be scheduled, Borg selects a subset of devices and publishes an xconnect to the Pod Manager. The PM discovers pending xconnects and sends commands to the appropriate OCSes to connect the requested ICI channels. Once ICI connections stabilize, libtpunet configures the device’s ICI and programs its forwarding tables. XLA consumes the topology built by libtpunet to shard the model. Once execution begins, each device has its compiled program in local memory, knows its neighbors via ICI routing tables, and has its slice of the model weights in HBM. Thousands of devices execute in lockstep, synchronizing through collectives, without a single global runtime controller. The user does not see any of this background orchestration. ICI Interface [6] Packets hop through a path of ICI switches and optical fibers to arbitrary pairs of TPUs determined by libtpunet once during setup. xconnects initiate mirror configuration in the OCS, triggering on-chip device managers to initialize physical connections between ICIs. When libtpunet issues an ICI session start it clears and rightsizes the ICI buffers in the data layer for new RDMAs. Routing is handled by forwarding tables that provide a simple abstraction to locate destination TPUs. XLA emits sets of RDMA operations called transactions for collective communications. On-chip DMA engines read data from HBM and send the data to the ICI’s transaction layer to send over the network [6]. All the required hardware for training drags down MTBF [6], so the system needs to be resilient to outages without bringing everything down. TPU Fault Tolerance [6] The system manages faulty links with fault tolerant routing. An offline integer linear program simulates link outages and frames the route selection as a max flow problem, using an all-to-all collective as the canonical use case. The results from the ILP are cached and accessible by libtpunet. Fault tolerant routing uses Wild First Routing as its heuristic. Packets can take a wild hop around faulty links before reverting to fault free routing. Though using fault tolerant routing may induce network congestion, TPU availability benefits [6].&lt;head&gt;Fault Tolerant Routing&lt;/head&gt;&lt;/p&gt;&lt;p&gt;Getting the whole system to cooperate at scale needs clear boundaries and hand-offs. Borg, PM, and libtpunet bless the configuration of the workload before triggering execution. When TCO skews towards operation, getting these pieces right is as important as systolic arrays and memory hierarchies. But this presentation of how the software works is also subject to the constant evolution of the TPU. Cores communicate over OCI. Chips communicate over ICI. Racks connect remote ICI links over OCS. That leaves us with one final communication frontier: the datacenter network.&lt;/p&gt;&lt;p&gt;Mixture of Experts Routing [38]&lt;/p&gt;&lt;p&gt;SPMD assumes every device can communicate over ICI with predictable latency, which constrains developers to slice sizes that fit on a single pod. Islands of accelerators [8] leave idle capacity stranded across pods, and under contention, jobs struggle to get the right-shaped device allocation. Individual pods also constrain algorithmic flexibility. Unlike traditional transformers, Mixture-of-Experts models include runtime data dependencies. The gating mechanism in MoEs introduces non-deterministic routing during execution. The SPMD model has to be stretched to express the fine-grained, data-dependent control flow these models need. If you want to shard experts across pods there is no natural way to do so. Without the DCN there is no dynamic routing, resource sharing, or use of idle chips across pods.&lt;/p&gt;&lt;p&gt;The datacenter network (DCN) connects islands using Google’s Jupiter fabric [9]. From the TPU’s point of view it is the communication that doesn’t occur over ICI. Extending the many cores, one logical system scaling approach gets complicated by varying latency and bandwidth characteristics. Two solutions emerged from these limitations. Multislice extends SPMD across pod boundaries. It is a conservative but compatible approach with existing code. Pathways abandoned synchronous execution for asynchronous dataflow. It is more complex but necessary for true heterogeneity.&lt;/p&gt;&lt;p&gt;Logical diagram of Multislice over DCN [26]&lt;/p&gt;&lt;p&gt;Multislice extends existing SPMD code across pod boundaries with minimal changes. Pod boundaries are treated as just another level in the communication hierarchy. SPMD still uses gang-scheduled execution, but XLA understands that some collectives happen over ICI and others happen over slower DCN. The familiar declarative slice syntax adds a parameter to select devices across islands. The compiler optimizes collective placement to minimize cross-pod traffic. Multislice expands the number of devices available for training by providing access to resources across pods [26].&lt;/p&gt;&lt;p&gt;Pathways System Overview [8]&lt;/p&gt;&lt;p&gt;Pathways is a plug-in replacement for JAX’s backend that virtualizes the datacenter [8]. Instead of one giant SPMD program running in lockstep, it models execution as a DAG of compiled functions distributed across islands. Gang scheduling still happens within each island, but between islands coordination is asynchronous. There’s no single global runtime controller for the whole job. Mixture-of-Experts models can route activations dynamically to experts on different pods, and pipeline parallel stages can span multiple islands connected over DCN. Multiple programs can time-multiplex accelerators without context-switching overhead. Users request devices and the client compiles programs into a device-agnostic Pathways IR. XLA analyzes the program, the resource manager assigns physical TPUs, and the system inserts data movement operations between shards. Orchestration is complete by the time execution begins. Each device knows its program, its neighbors, and its slice of model weights.&lt;/p&gt;&lt;p&gt;Pathways uses a sharded dataflow model built on Plaque [8]. Each node represents a compiled function executing across thousands of TPU shards. The system uses parallel asynchronous dispatch. Pathways pipelines host side work in parallel instead of waiting for computation A to finish before preparing computation B. A control-plane scheduler per island enforces gang scheduling across programs. Between islands, Pathways uses centralized dispatch to coordinate placement and data movement. Data moves directly between accelerators over ICI within islands and DCN between islands. Pathways matches multi-controller performance by front-loading coordination work, even though cross-island dispatch is mediated by the control plane rather than issued independently by each host. This execution model performs as well as JAX and lifts restrictions on algorithmic expressibility [8].&lt;/p&gt;&lt;p&gt;A dedicated upstart could reproduce the hardware design philosophy, but the software co-design makes the TPU a mammoth. Borg allocates resources and preempts jobs. The Pod Manager configures optical switches. libtpunet knows every ICI routing edge case and manages fault tolerance. XLA compiles with full knowledge of topology and latencies. SPMD partitions models while maintaining the illusion of one giant device. Multislice extends that illusion across pods. Pathways rethinks distributed execution and virtualizes the datacenter as one programmable pool. Schedulers, compilers, and coordination systems all play one long song. Building a TPU competitor needs generations of hard earned experience points. Each new design reconsiders which approaches were dead ends. Admitting you were wrong and doubling back is the game. Thinking about the TPU is thinking about Everything Else.&lt;/p&gt;&lt;head rend="h2"&gt;Ceci n’est pas une TPU&lt;/head&gt;&lt;p&gt;After TPUv4 the well of detailed microarchitecture papers runs dry. You can still find information scattered across the internet, but not in the same succinct, curated way. Maybe more papers will be released publicly and we’ll be able to study these designs in greater detail, but until then we have to cobble together an understanding of our own. TPUv4 and v4i are followed by TPUv5p (performance) and v5e (efficiency), Trillium (v6e), and Ironwood (v7). We know that the inference (e) optimized designs retain a single-core architecture and use 2D tori instead of 3D tori. We know the interconnect and HBM performance numbers for the fifth, sixth, and seventh generation chips. We know that Trillium and Ironwood revert to 256x256 systolic arrays. We know that Ironwood scales up to 9,216 chips for training and 256 for inference with 1.77PB HBM that delivers 42.5 FP8 ExaFlops (6x Perf/W improvement over TPUv4) with a chiplet design for next generation reasoning and MoE workloads [16][20][21][23][24].&lt;/p&gt;&lt;p&gt;And I know that all of this fails to capture the totality of the enhancements since TPUv4. But a spec sheet like the one here or a primer like the one here could have told us that. The subsequent papers have focused on the system, but discussions of the system hide the simple origins of the device behind a hodgepodge of specs and new thundering heights. The essence of the thing becomes a folklorish amalgam of TPU lore. Myths are about meaning. Moore’s Law was never free in the literal sense. It required diligent engineering and enduring frustration, but decade after decade the compounding continued. The idea of Moore’s Law cast a spell that actualized its reality.&lt;/p&gt;&lt;p&gt;By nature the TPU is what it is not. The thrust and posturing of papers, talks, slides, and internet chatter focus on the technical minutiae, but the seams that hold this constellation of facts and figures together are the ordinary and the human. They are long emails and oscilloscopes in equal measure. How many of these choices go unseen? Hand-wringing about the system internals helps us to glimpse the creative act, but we mistake the painting for the paint chemistry. In this new world where nothing is free, every decision comes at an intentional, excruciating cost. The weight of the space of possibilities grows heavier knowing that each decision may foreclose another. Each choice is an act of reinvention in the face of a future that folds onto itself.&lt;/p&gt;&lt;p&gt;The TPU is an artifact born out of the quiet solace of steady hands doing careful engineering. AI DSAs are unlikely to be self-fulfilling in the same infinite feeling way as Moore’s Law. They will be five hundred ordinary decisions that compose into something greater. Can we make it smaller? Can we make it bigger? Can we make it easier to use? When we skim specs like the ones strewn above we notice the changes and feel the weight of what they represent. As new pressures get applied new entities emerge. For a moment we sense each decision branching into some unknown. Our new AI-obsessed world brings with it the demands of new ways of thinking. It is a reminder that the future is always at hand, and that if we participate in the myth-making we find that there are dragons after all.&lt;/p&gt;&lt;head rend="h2"&gt;References:&lt;/head&gt;&lt;p&gt;[1]: In-Datacenter Performance Analysis of a Tensor Processing Unit&lt;/p&gt;&lt;p&gt;[2]: The Design Process for Google’s Training Chips: TPUv2 and TPUv3&lt;/p&gt;&lt;p&gt;[3]: A Domain-Specific Supercomputer for Training Deep Neural Networks&lt;/p&gt;&lt;p&gt;[4]: TPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings&lt;/p&gt;&lt;p&gt;[5]: Ten Lessons From Three Generations Shaped Google’s TPUv4i&lt;/p&gt;&lt;p&gt;[6]: Resiliency at Scale: Managing Google’s TPUv4 Machine Learning Supercomputer&lt;/p&gt;&lt;p&gt;[7]: GSPMD: General and Scalable Parallelization for ML Computation Graphs&lt;/p&gt;&lt;p&gt;[8]: PATHWAYS: ASYNCHRONOUS DISTRIBUTED DATAFLOW FOR ML&lt;/p&gt;&lt;p&gt;[9]: Jupiter Evolving: Transforming Google’s Datacenter Network via Optical Circuit Switches and Software-Defined Networking&lt;/p&gt;&lt;p&gt;[10]: Mission Apollo: Landing Optical Circuit Switching at Datacenter Scale&lt;/p&gt;&lt;p&gt;[11]: Computing’s Energy Problem&lt;/p&gt;&lt;p&gt;[12]: Domain-Specific Hardware Accelerators&lt;/p&gt;&lt;p&gt;[13]: The Accelerator Wall: Limits of Chip Specialization&lt;/p&gt;&lt;p&gt;[14]: The Deep Learning Revolution and Its Implications for Computer Architecture and Chip Design&lt;/p&gt;&lt;p&gt;[15]: Domain specific architectures for AI inference&lt;/p&gt;&lt;p&gt;[16]: How to Think About TPUs – Chapter 2&lt;/p&gt;&lt;p&gt;[17]: TPU Deep Dive&lt;/p&gt;&lt;p&gt;[18]: Understanding Matrix Multiplication on a Weight-Stationary Systolic Architecture&lt;/p&gt;&lt;p&gt;[19]: First in-depth look at Google’s TPU Architecture&lt;/p&gt;&lt;p&gt;[20]: WITH “IRONWOOD” TPU, GOOGLE PUSHES THE AI ACCELERATOR TO THE FLOOR&lt;/p&gt;&lt;p&gt;[21]: Ironwood: The first Google TPU for the age of inference&lt;/p&gt;&lt;p&gt;[22]: TPU Architecture – Google Documentation&lt;/p&gt;&lt;p&gt;[23]: Google Ironwood TPU Swings for Reasoning Model Leadership at Hot Chips 2025&lt;/p&gt;&lt;p&gt;[24]: Announcing Trillium, the sixth generation of Google Cloud TPU&lt;/p&gt;&lt;p&gt;[25]: A deep dive into SparseCore for Large Embedding Models&lt;/p&gt;&lt;p&gt;[26]: How to scale AI training to up to tens of thousands of Cloud TPU chips with Multislice&lt;/p&gt;&lt;p&gt;[27]: A Machine Learning Supercomputer With An Optically Reconfigurable Interconnect and Embeddings Support – HotChips Slides&lt;/p&gt;&lt;p&gt;[28]: Challenges in large scale training of Giant Transformers on Google TPU machines – HotChips Slides&lt;/p&gt;&lt;p&gt;[29]: Exploring Limits of ML Training on Google TPUs – HotChips Slides&lt;/p&gt;&lt;p&gt;[30]: Cloud TPU: Codesigning Architecture and Infrastructure – HotChips Slides&lt;/p&gt;&lt;p&gt;[31]: A DOMAIN-SPECIFIC TPU SUPERCOMPUTER FOR TRAINING DEEP NEURAL NETWORKS – Slides&lt;/p&gt;&lt;p&gt;[32]: Google’s Training Chips Revealed: TPUv2 and TPUv3 – Slides&lt;/p&gt;&lt;p&gt;[33]: A Decade of Machine Learning Accelerators: Lessons Learned and Carbon Footprint – MLSys Slides&lt;/p&gt;&lt;p&gt;[34]: Sparse-TPU: Adapting Systolic Arrays for Sparse Matrices&lt;/p&gt;&lt;p&gt;[35]: Systolic Array For VLSi&lt;/p&gt;&lt;p&gt;[36]: Why Systolic Architectures?&lt;/p&gt;&lt;p&gt;[37]: Doubly Twisted Torus Networks for VLSI Processor Arrays&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46172797</guid><pubDate>Sat, 06 Dec 2025 12:29:28 +0000</pubDate></item><item><title>GrapheneOS is the only Android OS providing full security patches</title><link>https://grapheneos.social/@GrapheneOS/115647408229616018</link><description>&lt;doc fingerprint="f8eb8f2f2d953eed"&gt;
  &lt;main&gt;
    &lt;p&gt;To use the Mastodon web application, please enable JavaScript. Alternatively, try one of the native apps for Mastodon for your platform.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46173407</guid><pubDate>Sat, 06 Dec 2025 13:58:31 +0000</pubDate></item><item><title>Tiny Core Linux: a 23 MB Linux distro with graphical desktop</title><link>http://www.tinycorelinux.net/</link><description>&lt;doc fingerprint="9ddb391819bfbc66"&gt;
  &lt;main&gt;
    &lt;head rend="h3"&gt;Welcome to The Core Project - Tiny Core Linux&lt;/head&gt;
    &lt;p&gt;The Core Project is a highly modular based system with community build extensions.&lt;/p&gt;
    &lt;p&gt;It starts with a recent Linux kernel, vmlinuz, and our root filesystem and start-up scripts packaged with a basic set of kernel modules in core.gz. Core (11MB) is simply the kernel + core.gz - this is the foundation for user created desktops, servers, or appliances. TinyCore is Core + Xvesa.tcz + Xprogs.tcz + aterm.tcz + fltk-1.3.tcz + flwm.tcz + wbar.tcz&lt;/p&gt;
    &lt;p&gt;TinyCore becomes simply an example of what the Core Project can produce, an 16MB FLTK/FLWM desktop.&lt;/p&gt;
    &lt;p&gt;CorePlus ofers a simple way to get started using the Core philosophy with its included community packaged extensions enabling easy embedded frugal or pendrive installation of the user's choice of supported desktop, while maintaining the Core principal of mounted extensions with full package management.&lt;/p&gt;
    &lt;p&gt;It is not a complete desktop nor is all hardware completely supported. It represents only the core needed to boot into a very minimal X desktop typically with wired internet access.&lt;/p&gt;
    &lt;p&gt;The user has complete control over which applications and/or additional hardware to have supported, be it for a desktop, a netbook, an appliance, or server, selectable by the user by installing additional applications from online repositories, or easily compiling most anything you desire using tools provided.&lt;/p&gt;
    &lt;p&gt;The latest version: 16.2&lt;/p&gt;
    &lt;head rend="h3"&gt;News&lt;/head&gt;
    &lt;head rend="h3"&gt;About Our Project&lt;/head&gt;
    &lt;p&gt;Our goal is the creation of a nomadic ultra small graphical desktop operating system capable of booting from cdrom, pendrive, or frugally from a hard drive. The desktop boots extremely fast and is able to support additional applications and hardware of the users choice. While Tiny Core always resides in ram, additional applications extensions can either reside in ram, mounted from a persistent storage device, or installed into a persistent storage device.&lt;/p&gt;
    &lt;p&gt;We invite interested users and developers to explore Tiny Core. Within our forums we have an open developement model. We encourage shared knowledge. We promote community involvement and community built application extensions. Anyone can contribute to our project by packaging their favorite application or hardware support to run in Tiny Core. The Tiny Core Linux Team currently consists of eight members who peruse the forums to assist from answering questions to helping package new extensions.&lt;/p&gt;
    &lt;p&gt;Join us here and on IRC Freenode #tinycorelinux.&lt;/p&gt;
    &lt;p&gt;Learn. Share. Grow your knowledge of Linux.&lt;/p&gt;
    &lt;p&gt;Robert Shingledecker, December 01, 2008&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46173547</guid><pubDate>Sat, 06 Dec 2025 14:18:42 +0000</pubDate></item><item><title>HTML as an Accessible Format for Papers</title><link>https://info.arxiv.org/about/accessible_HTML.html</link><description>&lt;doc fingerprint="e3d1ea5238d7dc4a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;HTML as an accessible format for papers&lt;/head&gt;
    &lt;p&gt;Accessibility barriers in research are not new, but they are urgent. The message we have heard from our community is that arXiv can have the most impact in the shortest time by offering HTML papers alongside the existing PDF.&lt;/p&gt;
    &lt;p&gt;arXiv has successfully launched papers in HTML format. We are gradually backfilling HTML for arXiv's corpus of over 2 million papers over time. Not every paper can be successfully converted, so a small percentage of papers will not have an HTML version. We will work to improve conversion over time.&lt;/p&gt;
    &lt;p&gt;The link to the HTML format will appear on abstract pages below the existing PDF download link. Authors will have the opportunity to preview their paperâs HTML as a part of the submission process.&lt;/p&gt;
    &lt;p&gt;The beta rollout is just the beginning. We have a long way to go to improve HTML papers and will continue to solicit feedback from authors, readers, and the entire arXiv community to improve conversions from LaTeX.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why "experimental" HTML?&lt;/head&gt;
    &lt;p&gt;Did you know that 90% of submissions to arXiv are in TeX format, mostly LaTeX? That poses a unique accessibility challenge: to accurately convert from TeXâa very extensible language used in myriad unique ways by authorsâto HTML, a language that is much more accessible to screen readers and text-to-speech software, screen magnifiers, and mobile devices. In addition to the technical challenges, the conversion must be both rapid and automated in order to maintain arXivâs core service of free and fast dissemination.&lt;/p&gt;
    &lt;p&gt;Because of these challenges we know there will be some conversion and rendering issues. We have decided to launch in beta with âexperimentalâ HTML because:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Accessible papers are needed now. We have talked to the arXiv community, especially researchers with accessibility needs, and they overwhelmingly asked us not to wait.&lt;/item&gt;
      &lt;item&gt;We need your help. The obvious work is done. Reports from the community will help us identify issues we can track back to specific LaTeX packages that are not converting correctly.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Error messages you may see in HTML papers&lt;/head&gt;
    &lt;p&gt;HTML papers on arXiv.org are a work in progress and will sometimes display errors. As we work to improve accessibility we share with you the causes of these errors and what authors can do to help minimize them. Learn more about error messages you may see in HTML papers&lt;/p&gt;
    &lt;head rend="h2"&gt;Ways to help&lt;/head&gt;
    &lt;head rend="h3"&gt;1) Read HTML papers and report issues&lt;/head&gt;
    &lt;p&gt;We encourage the community to try out HTML papers in your field:&lt;/p&gt;
    &lt;head rend="h4"&gt;Report an issue&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Go to the abstract page for a paper you are interested in reading.&lt;/item&gt;
      &lt;item&gt;Look in the section where you find the link to the PDF download, and click the new link for HTML.&lt;/item&gt;
      &lt;item&gt;Report issues by either a) clicking on the Open Issue button b) selecting text and clicking on the Open Issue for Selection button or c) use &lt;code&gt;Ctrl+?&lt;/code&gt;on your keyboard. If you are using a screen reader, use&lt;code&gt;Alt+y&lt;/code&gt;to toggle accessible reporting buttons per paragraph.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Please do not create reports that the HTML paper doesn't look exactly like the PDF paper&lt;/p&gt;
    &lt;p&gt;Our primary goal for this project is to make papers more accessible, so the focus during the beta phase will value function over form. HTML layouts that are incorrect or are illegible are important to report. But we do expect the HTML papers to present differently than the same paper rendered in PDF. Line breaks will occur in different places and there is likely to be more white space. In general, the HTML paper won't present as compactly. Intricate typographic layouts will not be rendered so intricately. This is by design.&lt;/p&gt;
    &lt;p&gt;HTML is a different medium and brings its own advantages versus PDF. In addition to being much more compatible with assistive technologies, HTML does a far better job adapting to the characteristics of the device you are reading on, including mobile devices.&lt;/p&gt;
    &lt;head rend="h3"&gt;2) Help improve the conversion from LaTeX&lt;/head&gt;
    &lt;p&gt;If you are an author you can help us improve conversions to HTML by following our guide to LaTeX Markup Best Practices for Successful HTML Papers.&lt;/p&gt;
    &lt;p&gt;If you are a developer and have free development cycles, help us improve conversions! Our collaborators at LaTeXML maintain a list of issues and welcome feedback and developer contributions.&lt;/p&gt;
    &lt;p&gt;If you are a publisher, member of a society, or conference organizer you can help us improve conversions to HTML by reviewing the .cls files your organization recommends to authors for unsupported packages. Providing .cls files that use supported packages is an easy way to support and sow accessibility in the scientific community.&lt;/p&gt;
    &lt;head rend="h2"&gt;Thank you to our collaborators&lt;/head&gt;
    &lt;p&gt;First, we want to share a special thank you to all the scientists with disabilities who have generously shared their insights, expertise, and guidance throughout this project.&lt;/p&gt;
    &lt;p&gt;We want to thank two organizations without which HTML papers on arXiv would not be possible: The LaTeX Project, and the LaTeXML team from NIST. We deeply thank each member of these teams for their knowledge, incredible work, and commitment to accessibility.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46173825</guid><pubDate>Sat, 06 Dec 2025 14:59:52 +0000</pubDate></item><item><title>Infisical (YC W23) Is Hiring Engineers to Build the Modern OSS Security Stack</title><link>https://www.ycombinator.com/companies/infisical/jobs/2pwGcK9-senior-full-stack-engineer-us-canada</link><description>&lt;doc fingerprint="c51f69845b7e12a"&gt;
  &lt;main&gt;
    &lt;p&gt;Unified platform for secrets, certs, and privileged access management&lt;/p&gt;
    &lt;p&gt;Infisical is looking to hire exceptional talent to join our teams in building the open source security infrastructure stack for the AI era.&lt;/p&gt;
    &lt;p&gt;We're building a generational company with a world-class engineering team. This isn’t a place to coast — but if you want to grow fast, take ownership, and solve tough problems, you’ll be challenged like nowhere else.&lt;/p&gt;
    &lt;p&gt;What We’re Looking For&lt;/p&gt;
    &lt;p&gt;We’re looking for an exceptional Full Stack Engineer to help us build, optimize, and expand the foundation of the platform.&lt;/p&gt;
    &lt;p&gt;We’ve kept our hiring standards exceptionally high since we expect engineers to tackle a broad range of challenges on a day-to-day basis. Examples of past engineering initiatives include developing strategies for secret rotation and dynamic secrets, a gateway to provide secure access to private resources, protocols like EST and KMIP, integrations for syncing secrets across cloud providers, and entire new product lines such as Infisical PKI and Infisical SSH.&lt;/p&gt;
    &lt;p&gt;You’ll be working closely with our CTO and the rest of the engineering team to:&lt;/p&gt;
    &lt;p&gt;Requirements&lt;/p&gt;
    &lt;p&gt;Bonus&lt;/p&gt;
    &lt;p&gt;How You’ll Grow&lt;/p&gt;
    &lt;p&gt;In this role, you’ll play a pivotal part in shaping Infisical’s future—making key technical decisions, establishing foundational processes, and tackling complex scalability challenges. As you gain experience and the team expands, you'll have the opportunity to take full ownership of specific areas of our platform, driving them end-to-end with autonomy and impact.&lt;/p&gt;
    &lt;p&gt;Overall, you’ll be one of the defining pieces of our team as we scale to thousands of customers over the next 18 months.&lt;/p&gt;
    &lt;p&gt;Team, Values &amp;amp; Benefits&lt;/p&gt;
    &lt;p&gt;Our team brings experience from companies like Figma, AWS, and Red Hat. We operate primarily as a remote team but maintain a strong presence in San Francisco, where we have an office. We also get together in person throughout the year for off-sites, conferences, and team gatherings.&lt;/p&gt;
    &lt;p&gt;At Infisical, we offer competitive compensation, including both salary and equity options. Additional benefits, such as a lunch stipend and a work setup budget, are available with more details to be found on our careers page.&lt;/p&gt;
    &lt;p&gt;About Us&lt;/p&gt;
    &lt;p&gt;Infisical is the open source security infrastructure platform that engineers use for secrets management, internal PKI, key management, and SSH workflow orchestration. We help developers and organizations securely manage over 1.5 billion secrets each month including application configuration, database credentials, certificates, and more.&lt;/p&gt;
    &lt;p&gt;We’ve raised $19M from Y Combinator, Google, and Elad Gil, and our customers include Hugging Face, Lucid, and LG.&lt;/p&gt;
    &lt;p&gt;Join us on a mission to make security easier for all developers — starting with secrets management.&lt;/p&gt;
    &lt;p&gt;Infisical is the #1 open source secret management platform – used by tens of thousands of developers.&lt;/p&gt;
    &lt;p&gt;We raised $3M from Y Combinator, Gradient Ventures (Google's VC fund), and awesome angel investors like Elad Gil, Arash Ferdowsi (founder/ex-CTO of Dropbox), Paul Copplestone (founder/CEO of Supabase), James Hawkins (founder/CEO of PostHog), Andrew Miklas (founder/ex-CTO of PagerDuty), Diana Hu (GP at Y Combinator), and more.&lt;/p&gt;
    &lt;p&gt;We are default alive, and have signed many customers ranging from fastest growing startups to post-IPO enterprises.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46174789</guid><pubDate>Sat, 06 Dec 2025 17:01:53 +0000</pubDate></item><item><title>OMSCS Open Courseware</title><link>https://sites.gatech.edu/omscsopencourseware/</link><description>&lt;doc fingerprint="e76af2125443a7a2"&gt;
  &lt;main&gt;
    &lt;p&gt;Georgia Tech’s Online Master of Science in Computer Science (OMSCS) program is proud to make the course content* for many of its courses publicly available through Ed Lessons. Select a course below to view the public content for that course.&lt;/p&gt;
    &lt;p&gt;Note that students enrolled in OMSCS should access their course content through Canvas, as the for-credit versions of these courses may include graded components or recent content updates not available through OMSCS Open Courseware.&lt;/p&gt;
    &lt;p&gt;*Course content typically includes things such as lecture videos and exercises; it will not include things like homeworks, projects quizzes, exams, or other graded assignments.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46175826</guid><pubDate>Sat, 06 Dec 2025 19:14:35 +0000</pubDate></item><item><title>CATL Expects Oceanic Electric Ships in 3 Years</title><link>https://cleantechnica.com/2025/12/05/catl-expects-oceanic-electric-ships-in-3-years/</link><description>&lt;doc fingerprint="f09b004a62d0af21"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;CATL Expects Oceanic Electric Ships in 3 Years&lt;/head&gt;
    &lt;p&gt;Support CleanTechnica's work through a Substack subscription or on Stripe.&lt;/p&gt;
    &lt;p&gt;News in batteries and electrification has been coming fast and furious lately. Recently, Su Yi, the head of CATL’s marine division, stated, “CATL’s marine business already covers inland rivers, lakes, and coastal waters, and is now advancing toward ocean-going applications.”&lt;/p&gt;
    &lt;p&gt;“In the near future — perhaps within the next three years — we will achieve pure-electric vessels navigating the open seas.”&lt;/p&gt;
    &lt;p&gt;CATL is determined to provide zero-carbon marine transportation. CATL has not been content to merely make batteries, but has dedicated efforts towards application in several sectors, including grid storage, passenger vehicles, and ships. CATL’s marine division has been in operation since 2017, expanding efforts in shipping. In 2023, it introduced a comprehensive battery replenishment solution including battery swapping, charging, and a cloud-based system providing shared mobile application of containerized power for optimal efficiency and economics. Those efforts resulted in several major products, containerized mobile power, high-voltage high-power charging systems, and the cloud information platform. This comprehensive system provides a seamless solution for electric ships.&lt;/p&gt;
    &lt;p&gt;Sharp-eyed readers may have noticed CATL’s recent discussion with shipping giant Maersk and that it has been busy collaborating with shipping partners. CATL has supplied batteries for over 900 vessels, including the Yangtze River Three Gorges 1 cruise ship, the world’s first pure electric ocean-going passenger ship, and the Qinggang Tug 1 tugboat.&lt;/p&gt;
    &lt;p&gt;Recent battery price drops indicate that possibilities for long range electric shipping are improving. The expected timeline for electric shipping dovetails with the expected timeline for sodium-ion battery (SIB) volume production and resulting cost reductions. The material costs of SIBs are expected to lower costs significantly, opening up applications and speeding up electrification. While passenger transport has been successfully electrified, with EVs surpassing ICE parity with battery costs well below $100/kWh enabling widespread adoption, ships can increasingly take advantage of lower-cost batteries for expanded electrification. Studies have shown that long-distance electric ships with up to 5,000 km of range can be successfully utilized using today’s battery capabilities, without significant weight and volume. CATL appears to be aware of this. Marine division head Su Yi notes CATL’s “full-spectrum growth” strategy, with goals to electrify maritime and aviation sectors. Sodium-ion technology may remove the last barrier to widespread maritime electrification.&lt;/p&gt;
    &lt;p&gt;Sign up for CleanTechnica's Weekly Substack for Zach and Scott's in-depth analyses and high level summaries, sign up for our daily newsletter, and follow us on Google News!&lt;/p&gt;
    &lt;p&gt;Have a tip for CleanTechnica? Want to advertise? Want to suggest a guest for our CleanTech Talk podcast? Contact us here.&lt;/p&gt;
    &lt;p&gt;Sign up for our daily newsletter for 15 new cleantech stories a day. Or sign up for our weekly one on top stories of the week if daily is too frequent.&lt;/p&gt;
    &lt;p&gt;CleanTechnica uses affiliate links. See our policy here.&lt;/p&gt;
    &lt;p&gt;CleanTechnica's Comment Policy&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46176169</guid><pubDate>Sat, 06 Dec 2025 20:00:17 +0000</pubDate></item><item><title>Zebra-Llama: Towards Efficient Hybrid Models</title><link>https://arxiv.org/abs/2505.17272</link><description>&lt;doc fingerprint="c1d340b445bd08b"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Machine Learning&lt;/head&gt;&lt;p&gt; [Submitted on 22 May 2025]&lt;/p&gt;&lt;head rend="h1"&gt;Title:Zebra-Llama: Towards Extremely Efficient Hybrid Models&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:With the growing demand for deploying large language models (LLMs) across diverse applications, improving their inference efficiency is crucial for sustainable and democratized access. However, retraining LLMs to meet new user-specific requirements is prohibitively expensive and environmentally unsustainable. In this work, we propose a practical and scalable alternative: composing efficient hybrid language models from existing pre-trained models. Our approach, Zebra-Llama, introduces a family of 1B, 3B, and 8B hybrid models by combining State Space Models (SSMs) and Multi-head Latent Attention (MLA) layers, using a refined initialization and post-training pipeline to efficiently transfer knowledge from pre-trained Transformers. Zebra-Llama achieves Transformer-level accuracy with near-SSM efficiency using only 7-11B training tokens (compared to trillions of tokens required for pre-training) and an 8B teacher. Moreover, Zebra-Llama dramatically reduces KV cache size -down to 3.9%, 2%, and 2.73% of the original for the 1B, 3B, and 8B variants, respectively-while preserving 100%, 100%, and &amp;gt;97% of average zero-shot performance on LM Harness tasks. Compared to models like MambaInLLaMA, X-EcoMLA, Minitron, and Llamba, Zebra-Llama consistently delivers competitive or superior accuracy while using significantly fewer tokens, smaller teachers, and vastly reduced KV cache memory. Notably, Zebra-Llama-8B surpasses Minitron-8B in few-shot accuracy by 7% while using 8x fewer training tokens, over 12x smaller KV cache, and a smaller teacher (8B vs. 15B). It also achieves 2.6x-3.8x higher throughput (tokens/s) than MambaInLlama up to a 32k context length. We will release code and model checkpoints upon acceptance.&lt;/quote&gt;&lt;head rend="h2"&gt;Submission history&lt;/head&gt;From: Mehdi Rezagholizadeh [view email]&lt;p&gt;[v1] Thu, 22 May 2025 20:39:57 UTC (12,646 KB)&lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;p&gt; IArxiv Recommender (What is IArxiv?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46176289</guid><pubDate>Sat, 06 Dec 2025 20:15:54 +0000</pubDate></item><item><title>Show HN: Tascli, a command line based (human) task and record manager</title><link>https://github.com/Aperocky/tascli</link><description>&lt;doc fingerprint="e35cd7e9d322ee4"&gt;
  &lt;main&gt;
    &lt;p&gt;A simple, fast, local CLI tool for tracking tasks and records from unix terminal.&lt;/p&gt;
    &lt;p&gt;Installation:&lt;/p&gt;
    &lt;code&gt;cargo install tascli
# or use brew
brew tap Aperocky/tascli
brew install tascli&lt;/code&gt;
    &lt;p&gt;Tasks and records are stored in &lt;code&gt;~/.local/share/tascli/tascli.db&lt;/code&gt; (configurable) with &lt;code&gt;rusqlite&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Create tasks with deadlines:&lt;/p&gt;
    &lt;code&gt;# Basic tasks
tascli task "Create readme" today
tascli task "Publish package" tomorrow
tascli task "Do taxes" 4/15

# With category
tascli task -c work "Read emails" week&lt;/code&gt;
    &lt;p&gt;Create recurring tasks:&lt;/p&gt;
    &lt;code&gt;tascli task "write diary" daily
tascli task "mortgage payment" "monthly 17th"&lt;/code&gt;
    &lt;p&gt;List tasks:&lt;/p&gt;
    &lt;code&gt;# List active tasks
$ tascli list task&lt;/code&gt;
    &lt;p&gt;output:&lt;/p&gt;
    &lt;code&gt;Task List:
----------------------------------------------------------------------------------------------
| Index  | Category            | Content                               | Deadline            |
----------------------------------------------------------------------------------------------
| 1      | life (recurring)    | write diary                           | Today               |
----------------------------------------------------------------------------------------------
| 2      | tascli              | Add pagination capability for tascli  | Sunday              |
|        |                     | list actions                          |                     |
----------------------------------------------------------------------------------------------
| 3      | tascli              | Add readme section on timestring      | Sunday              |
|        |                     | format                                |                     |
----------------------------------------------------------------------------------------------
| 4      | life                | Do state taxes                        | Sunday              |
----------------------------------------------------------------------------------------------
| 5      | tascli              | Sort list output by time instead of   | Sunday              |
|        |                     | internal id                           |                     |
----------------------------------------------------------------------------------------------
| 6      | tascli              | Fix length issue for unicode chars    | Sunday              |
----------------------------------------------------------------------------------------------
| 7      | life                | Two month pictures - follow the lead  | 4/23                |
|        |                     | from the previous one month pictures  |                     |
----------------------------------------------------------------------------------------------
&lt;/code&gt;
    &lt;p&gt;Complete tasks:&lt;/p&gt;
    &lt;code&gt;# Mark index 1 as done
tascli done 1&lt;/code&gt;
    &lt;p&gt;Completing a task or a recurring tasks will generate a corresponding record.&lt;/p&gt;
    &lt;p&gt;Search tasks:&lt;/p&gt;
    &lt;code&gt;tascli list task --search "rust"&lt;/code&gt;
    &lt;p&gt;List all tasks in &lt;code&gt;tascli&lt;/code&gt; category (including completed)&lt;/p&gt;
    &lt;code&gt;tascli list task -s all -c tascli&lt;/code&gt;
    &lt;p&gt;Example output:&lt;/p&gt;
    &lt;code&gt;Task List:
----------------------------------------------------------------------------------------------
| Index  | Category            | Content                               | Deadline            |
----------------------------------------------------------------------------------------------
| 1      | baby (Recurring)    | Mix egg yolk milk for Rowan           | Daily (fulfilled)   |
----------------------------------------------------------------------------------------------
| 2      | tascli              | Fix addition and modification commands| Today (completed)   |
|        |                     | output to have N/A for index          |                     |
----------------------------------------------------------------------------------------------
| 3      | tascli              | Insert guardrail against accidental   | Today (completed)   |
|        |                     | valid syntax like 'task list' that is |                     |
|        |                     | mistakenly made                       |                     |
----------------------------------------------------------------------------------------------
| 4      | tascli              | Create a gif for readme               | Today (completed)   |
----------------------------------------------------------------------------------------------
| 5      | tascli              | Add pagination capability for tascli  | Sunday              |
|        |                     | list actions                          |                     |
----------------------------------------------------------------------------------------------
| 6      | tascli              | Add readme section on timestring      | Sunday              |
|        |                     | format                                |                     |
----------------------------------------------------------------------------------------------
&lt;/code&gt;
    &lt;p&gt;Create records (for tracking events):&lt;/p&gt;
    &lt;code&gt;# With current time
tascli record -c feeding "100ML"

# With specific time
tascli record -c feeding -t 11:20AM "100ML"&lt;/code&gt;
    &lt;p&gt;List records:&lt;/p&gt;
    &lt;code&gt;# -d 1 stand for only get last 1 day of record
tascli list record -d 1&lt;/code&gt;
    &lt;p&gt;Search records:&lt;/p&gt;
    &lt;code&gt;tascli list record --search "secret"&lt;/code&gt;
    &lt;p&gt;Example output:&lt;/p&gt;
    &lt;code&gt;Records List:
----------------------------------------------------------------------------------------------
| Index  | Category            | Content                               | Created At          |
----------------------------------------------------------------------------------------------
| 1      | feeding             | 110ML                                 | Today 1:00AM        |
----------------------------------------------------------------------------------------------
| 2      | feeding             | breastfeeding                         | Today 4:10AM        |
----------------------------------------------------------------------------------------------
| 3      | feeding             | 100ML                                 | Today 7:30AM        |
----------------------------------------------------------------------------------------------
| 3      | life (Recurring)    | write diary                           | Today 10:30AM       |
----------------------------------------------------------------------------------------------
| 4      | feeding             | 110ML                                 | Today 11:20AM       |
----------------------------------------------------------------------------------------------
&lt;/code&gt;
    &lt;p&gt;This application accepts flexible time strings in various formats:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Simple dates: &lt;code&gt;today&lt;/code&gt;,&lt;code&gt;tomorrow&lt;/code&gt;,&lt;code&gt;yesterday&lt;/code&gt;,&lt;code&gt;friday&lt;/code&gt;,&lt;code&gt;eom&lt;/code&gt;(end of month),&lt;code&gt;eoy&lt;/code&gt;(end of year)&lt;/item&gt;
      &lt;item&gt;Date formats: &lt;code&gt;YYYY-MM-DD&lt;/code&gt;,&lt;code&gt;MM/DD/YYYY&lt;/code&gt;,&lt;code&gt;MM/DD&lt;/code&gt;(current year)&lt;/item&gt;
      &lt;item&gt;Time formats: &lt;code&gt;HH:MM&lt;/code&gt;,&lt;code&gt;3:00PM&lt;/code&gt;,&lt;code&gt;3PM&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Combined: &lt;code&gt;2025-03-24 15:30&lt;/code&gt;,&lt;code&gt;tomorrow 3PM&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When only a date is provided, the time defaults to end of day (23:59:59). When only a time is provided, the date defaults to today.&lt;/p&gt;
    &lt;p&gt;Recurring Formats (schedules) are applicable to tasks:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Recurring Formats: &lt;code&gt;daily&lt;/code&gt;,&lt;code&gt;daily 9PM&lt;/code&gt;,&lt;code&gt;weekly&lt;/code&gt;,&lt;code&gt;weekly Friday 9AM&lt;/code&gt;,&lt;code&gt;weekly mon-fri&lt;/code&gt;,&lt;code&gt;monthly 1st&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Recurring Formats (II): &lt;code&gt;every day&lt;/code&gt;,&lt;code&gt;every 9PM&lt;/code&gt;,&lt;code&gt;every monday&lt;/code&gt;,&lt;code&gt;every 9th of the month&lt;/code&gt;,&lt;code&gt;every 2/14&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If storing the db file in location other than &lt;code&gt;~/.local/share/tascli/tascli.db&lt;/code&gt; is preferred, create a config file:&lt;/p&gt;
    &lt;code&gt;{
    "data_dir": "/where/you/want/it"
}
&lt;/code&gt;
    &lt;p&gt;at &lt;code&gt;~/.config/tascli/config.json&lt;/code&gt; to adjust the location of the stored file. Note, if you already have existing tasks, you may want to move/copy the db file there first.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;tascli&lt;/code&gt; uses &lt;code&gt;clap&lt;/code&gt; for argument parsing, use &lt;code&gt;--help&lt;/code&gt; to get help on all levels of this cli:&lt;/p&gt;
    &lt;code&gt;aperocky@~$ tascli -h
Usage: tascli &amp;lt;COMMAND&amp;gt;

Commands:
  task    add task with end time
  record  add record
  done    Finish tasks
  update  Update tasks or records wording/deadlines
  delete  Delete Records or Tasks
  list    list tasks or records
  help    Print this message or the help of the given subcommand(s)

Options:
  -h, --help     Print help
  -V, --version  Print version
aperocky@~$ tascli task -h
add task with end time

Usage: tascli task [OPTIONS] &amp;lt;CONTENT&amp;gt; [TIMESTR]

Arguments:
  &amp;lt;CONTENT&amp;gt;  Description of the task
  [TIMESTR]  Time the task is due, default to EOD

Options:
  -c, --category &amp;lt;CATEGORY&amp;gt;  Category of the task
  -h, --help                 Print help
&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46176533</guid><pubDate>Sat, 06 Dec 2025 20:56:38 +0000</pubDate></item><item><title>Coffee linked to slower biological ageing among those with severe mental illness</title><link>https://www.kcl.ac.uk/news/coffee-linked-to-slower-biological-ageing-among-those-with-severe-mental-illness-up-to-a-limit</link><description>&lt;doc fingerprint="776a119b602981bc"&gt;
  &lt;main&gt;
    &lt;quote&gt;&lt;p&gt;We know that coffee can help slow biological ageing in the general population, but little is known about its effect on people with severe mental illness – a population whose lifespan is already shortened, in part due to age-related diseases. Our study shows that up to four cups of coffee per day is linked to longer telomeres among people with bipolar disorder and schizophrenia. This is comparable to a biological age of five years younger than non-coffee drinkers.&lt;/p&gt;Vid Mlakar, PhD student at King’s College London and first author of the study&lt;/quote&gt;
    &lt;p&gt;26 November 2025&lt;/p&gt;
    &lt;head rend="h2"&gt;Coffee linked to slower biological ageing among those with severe mental illness – up to a limit&lt;/head&gt;
    &lt;p&gt;New research from King’s College London finds that coffee consumption within the NHS recommended limit is linked to longer telomere lengths – a marker of biological ageing – among people with bipolar disorder and schizophrenia. The effect is comparable to roughly five years younger biological age.&lt;/p&gt;
    &lt;p&gt;Telomeres are structures that protect DNA. As people get older, their telomeres shorten as part of the natural human ageing process. This process has been shown to be accelerated among people with severe mental illness, such as bipolar disorder and schizophrenia, who have an average life expectancy 15 years shorter than the general population.&lt;/p&gt;
    &lt;p&gt;Previous research shows that coffee possesses health benefits. It may reduce oxidative stress in the general population, helping slow biological ageing processes like telomere shortening. The new study, published in BMJ Mental Health, explores whether coffee consumption could slow this ageing process among those with severe mental illness.&lt;/p&gt;
    &lt;p&gt;Researchers at the Institute of Psychiatry, Psychology &amp;amp; Neuroscience measured the effects of coffee consumption on telomere length among 436 participants aged 18 to 65 with schizophrenia, bipolar disorder or major depressive disorder with psychosis.&lt;/p&gt;
    &lt;p&gt;They found that coffee consumption of up to four cups per day was linked to longer telomeres, comparable to a biological age five years younger than non-coffee drinkers.&lt;/p&gt;
    &lt;p&gt;The longest telomeres were seen among those who consumed three to four cups per day. Too much coffee reduced this positive effect, with participants who consumed more than four cups having shorter telomeres than those who consumed between three and four cups.&lt;/p&gt;
    &lt;p&gt;These effects remained after accounting for variations in age, sex, ethnicity, medication and tobacco use.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Coffee is a beverage that many people consume daily. On one hand, we know that excessive coffee consumption can have negative effects on health, such as reducing sleep quality. However, our new study suggests that coffee consumption up to a certain point may have benefits for biological ageing. Many of the factors that are known to affect biological ageing, such as genetics and negative stressful life experiences, are beyond our control. Lifestyle factors like coffee consumption are something we can actively modify, making research like this particularly valuable.&lt;/p&gt;Dr Monica Aas, MRC Research Fellow at King’s College London and senior author of the study&lt;/quote&gt;
    &lt;p&gt;Dr Aas added: "Studies such as this also support the idea that we should move away from viewing coffee as simply “good or bad”, and instead consider a more balanced view. Still, these results need to be confirmed in other independent studies and longitudinal research before we can determine if this is a causal effect."&lt;/p&gt;
    &lt;p&gt;Data were from the Norwegian TOP study, collected between 2007 and 2018. The researchers included participants who had available data on mental health diagnosis (assessed using the Structured Clinical Interview for DSM-IV), telomere length (measured by extracting DNA from blood samples) and self-reported coffee consumption.&lt;/p&gt;
    &lt;p&gt;The researchers note that the study did not have information on the type of coffee consumed (instant versus filter) or the caffeine concentration of each cup. The NHS advises limiting caffeine intake to 400 mg/day (approximately four cups of coffee).&lt;/p&gt;
    &lt;p&gt;The study was funded by the Research Council of Norway, the KG Jebsen Stiftelsen and an Medical Research Council Fellowship. The team has recently received funding from the British Medical Association’s Margaret Temple grant to investigate telomere shortening in a longitudinal cohort of patients with psychosis. This project will allow them to explore further how several lifestyle factors, as well as stress, influence the rate of telomere shortening over time.&lt;/p&gt;
    &lt;p&gt;"Coffee intake is associated with telomere length in severe mental disorders" (Vid Mlakar et al.) was published in BMJ Mental Health. DOI: 10.1136/bmjment-2025-301700&lt;/p&gt;
    &lt;p&gt;For more information, please contact Milly Remmington (School of Mental Health &amp;amp; Psychological Sciences Communications Manager).&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46176766</guid><pubDate>Sat, 06 Dec 2025 21:33:03 +0000</pubDate></item><item><title>The past was not that cute</title><link>https://juliawise.net/the-past-was-not-that-cute/</link><description>&lt;doc fingerprint="26e613518ba77ebb"&gt;
  &lt;main&gt;
    &lt;p&gt;I was excited when cottagecore became a thing. Maybe my interest in retro clothes and handicrafts would be less embarrassing now!&lt;/p&gt;
    &lt;p&gt;I still enjoy it. But in spaces focused on old-fashioned vibes, you encounter a lot of people who believe that the past was actually this charming.&lt;/p&gt;
    &lt;p&gt;Laura Ingalls Wilder‘s Little House on the Prairie books are problematic, and also I will always love them. She wrote about the beauty of family and hard work, but she wrote them because she spent her whole life supporting disabled family members. She and her daughter beautified her “pioneer girl” history to make good books. Her daughter describes the reality: “It took seven successive years of complete crop failure, with work, weather and sickness that wrecked [my father’s] health permanently, and interest rates of 36 percent on money borrowed to buy food, to dislodge us from that land.”&lt;/p&gt;
    &lt;p&gt;My own version of this mistake was thinking that people’s personalities were different in the past. I grew up listening to folk music and imagining a past where nice boys would admire a nice quiet girl like me, and I wouldn’t have to figure out dating because everything would just unfold, probably on a May morning. My mother pointed out that a lot of the songs along the lines of “my own true love proved false to me” were about unplanned pregnancies.&lt;/p&gt;
    &lt;p&gt;I also assumed the bonny lasses in these songs would be wholesome and nice. But were popular girls of the past nicer people than they are now?&lt;/p&gt;
    &lt;p&gt;Some of my picture came from growing up in the Anglo-American folk dance and music community: it had a lot of aging hippies with graduate degrees. So I came away imagining a past with a lot of the kind of people who become engineers and English teachers. A more accurate picture would have been “Imagine a small town where the same 19 kids form your entire group of peers and potential partners.”&lt;/p&gt;
    &lt;p&gt;Bookish girls like Belle didn’t really go to live in enchanted castles with huge libraries. They stayed in villages where everyone thought they were weird and their best option was Gaston.&lt;/p&gt;
    &lt;p&gt;Maybe my favorite podcast episode ever is Rachel Laudan on food history: “I did have the extraordinary good fortune to grow up eating what I think the romantic movement dreams of. We had milk fresh from the cow; I never had pasteurized milk until I went to school. We had fish from the river, pheasant from the farm. The food was extremely good. . . . everything was fresh from the garden. So, I do romanticize—some of that because the taste was often extraordinary. And then I tweak myself and I say, ‘Look, Rachel, your mother spent all day, every day gardening or cooking.’ Essentially. As well as doing other chores. And she said to you, ‘Rachel, it’s servitude. I want you to have a life I didn’t have.’ “&lt;/p&gt;
    &lt;p&gt;I love living in a time and place where we get to choose aesthetics. I have bread rising in my kitchen right now, and I’m looking forward to baking it in an electric oven that doesn’t require me stacking wood or putting smoke into my house.&lt;/p&gt;
    &lt;p&gt;So I’ll continue to enjoy retro vibes, and draw on the past for lessons on how to be a human. (For example, making music together is one of life’s great experiences, and it’s a mistake to entirely substitute recorded music for that.) But I’ll enjoy doing so with indoor plumbing, dental care, and a desk job.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46176893</guid><pubDate>Sat, 06 Dec 2025 21:53:35 +0000</pubDate></item><item><title>Copy-Item is 27% slower than File Explorer</title><link>https://til.andrew-quinn.me/posts/copy-item-is-27-percent-slower-than-file-explorer-drag-and-drop-on-windows/</link><description>&lt;doc fingerprint="364cd87578a67eeb"&gt;
  &lt;main&gt;
    &lt;p&gt;In table form:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Tool&lt;/cell&gt;
        &lt;cell role="head"&gt;Speed (MBps)&lt;/cell&gt;
        &lt;cell role="head"&gt;Difference&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Drag and drop&lt;/cell&gt;
        &lt;cell&gt;~112&lt;/cell&gt;
        &lt;cell&gt;—&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;Copy-Item&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;~82&lt;/cell&gt;
        &lt;cell&gt;-27%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;sftp&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;~70&lt;/cell&gt;
        &lt;cell&gt;-37%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;robocopy&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;~25 MBps&lt;/cell&gt;
        &lt;cell&gt;-78%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;&lt;code&gt;rsync&lt;/code&gt; (WSL 2)&lt;/cell&gt;
        &lt;cell&gt;~13 MBps&lt;/cell&gt;
        &lt;cell&gt;-88%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;I feel like I’m losing my mind.&lt;/p&gt;
    &lt;p&gt;It all started innocently enough with a misconfigured NAS. I’ve been making some investments into the homelab recently, both for professional reasons and personal curiosity; by now I consider a couple of long CAT 7 cables and some unmanaged 1 Gbps Ethernet switches to be some of the best money I’ve spent this year. Every device in our home has a cozy data autobahn to rush down to talk to every other device in our home, and what’s more, I’ve actually seen &lt;code&gt;rsync&lt;/code&gt; copy files
from my NAS to my Debian daily driver at 100+ MBps quite
regularly.&lt;/p&gt;
    &lt;p&gt;But even after I un-mis-configured the NAS, I was surprised to find that &lt;code&gt;rsync&lt;/code&gt; on Windows Subsystem for
Linux pulled in data at a paltry 9 MBps, for an ideal use
case (one large file). Removing the &lt;code&gt;-z&lt;/code&gt; compression flag
moved us up by 30-50%, which is commendable, but not the
order of magnitude increase I was doubtfully hoping for.&lt;/p&gt;
    &lt;p&gt;Was something wrong with the connection? Getting up from the strandmon would have required physical movement, so instead I just took that same exact file via the File Explorer network share, and just Ctrl-C Ctrl-V’d it onto my desktop - and I was immediately hitting 111-112 MBps again! There goes that theory. The calls are coming from inside the house.&lt;/p&gt;
    &lt;p&gt;I don’t have much occasion to use these skills these days, but I actually became a somewhat proficient PowerShell user and Windows admin back in my first job. My curiosity was fully piqued by this point so I busted out Old Reliable - robocopy, rsync before rsync. It’s been built in since Windows 7 and handles an absurd number of weird edge cases due to its longevity, so you would expect it to be, well, fast, right. Like this is one of those Laws of Lindy Software that everyone supposedly knows; software that was created to be demanding on system resources 30 years ago is blazing fast to the point of instantaneous today, that’s why everyone loves Vim and Emacs over VS Code, etc.&lt;/p&gt;
    &lt;p&gt;Not so, apparently! &lt;code&gt;robocopy&lt;/code&gt; doesn’t print its progress
in seconds out of the box, but it can restart things from
where they were last left off, so a few &lt;code&gt;date &amp;amp;&amp;amp; robocopy&lt;/code&gt;
and Ctrl+C’s later, I had a pretty reliable estimate
of… 25 MBps.&lt;/p&gt;
    &lt;p&gt;My jaw just about hit the floor, reader. It was here I began to realize that something was rotten in the state of Denmark. You probably shouldn’t be launching 10- or 100-hour long copy jobs between Windows machines in the first place if you have any other options, but on the other hand, civilization advances when we extend the number of important operations which we can perform without thinking about them, and I would sure like to know that the venerable old tool I would script such enormous jobs around wouldn’t do it at one-fifth the speed of doing it by hand. No, I didn’t investigate &lt;code&gt;robocopy&lt;/code&gt;’s
myriad other options in detail, but in this situation I do
not think I have to - we are comparing default behaviors
against default behaviors! This is also why I didn’t go
super deep into the WSL question; I should be able to
assume that WSL comes shipped with sane defaults that don’t
decimate my network connection because I didn’t set the MTU
to 1337 by hand or what have you.&lt;/p&gt;
    &lt;p&gt;These days Windows also comes with a very nice &lt;code&gt;ssh&lt;/code&gt; and
&lt;code&gt;sftp&lt;/code&gt; client out of the box, and this is where I could feel
my heart beating a little faster, because in my day to day
work now &lt;code&gt;sftp&lt;/code&gt; is actually my preferred method of shuttling
files to and from my little corporate laptop. Here, at
long last, we crossed the 50 MBps mark - &lt;code&gt;get same-file.dat&lt;/code&gt;
got a semi-respectable 70 MBps down from the NAS. Only a
37% reduction compared to drag and drop! And here, at least,
we start to get some kind of explanation for why this
reduction might happen - SFTP is an encrypted protocol, so
maybe those CPU cycles add up to a lot of extra work over
time or slowdown. That… shouldn’t feel convincing to anyone
who gives it more than 15 seconds of thought, but we all live
with our eyes wide shut at times.&lt;/p&gt;
    &lt;p&gt;I had one last thing to test, PowerShell’s good old fashioned &lt;code&gt;Copy-Item&lt;/code&gt;. Those of us with a Unix background more or less
assume, without looking into it too deeply, that good old
fashioned &lt;code&gt;cp&lt;/code&gt; is about as fast as you can get for things
like this. You’re edging up to the limits of what the
actual hardware can do. You assume &lt;code&gt;cp&lt;/code&gt; as a binary is laser
focused on, well, making copies as fast as possible. Surely
&lt;code&gt;Copy-Item&lt;/code&gt; would work the same way and be close to parity
with File Explorer, right?&lt;/p&gt;
    &lt;p&gt;Alas. While it was the fastest of the bunch otherwise, &lt;code&gt;Copy-Item&lt;/code&gt; still topped out at an average 82 MBps when it
came to copying the file from the NAS’s network drive to
my local filesystem. I have to say, this was the most
disheartening of the bunch in a way; despite all Microsoft’s
efforts to the contrary, reaching all the way back to the
dawn of PowerShell and the Azure corporate revolution, here
they are, still making poor bedraggled sysadmins pay a
speed tax for trying to do their work in a minimally sane
way.&lt;/p&gt;
    &lt;p&gt;There are, to my knowledge, no scripting options at all on Windows that come close to the fearsome power of File Explorer’s copy and paste out of the box. I hope you never have to use this knowledge, but if you do, may your 100-hour copy job not accidentally be a 130-, 140, 500-, or 1100-hour one instead just because you chose to try to be clever about things.&lt;/p&gt;
    &lt;p&gt;(Also hyvää itsenäisyyspäivää, for those who celebrate!)&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46176904</guid><pubDate>Sat, 06 Dec 2025 21:55:08 +0000</pubDate></item><item><title>Screenshots from developers: 2002 vs. 2015 (2015)</title><link>https://anders.unix.se/2015/12/10/screenshots-from-developers--2002-vs.-2015/</link><description>&lt;doc fingerprint="5e3b5b5dcbbf3eec"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Screenshots from developers: 2002 vs. 2015&lt;/head&gt;
    &lt;p&gt;In 2002 I asked a number of developers/Unix people for screenshots of their desktops. I recently republished them, and, seeing the interest this generated, I thought it’d be fun to ask the same people* again 13 years later. To my delight I managed to reach many of them.&lt;/p&gt;
    &lt;p&gt;* Sans Dennis Ritchie and itojun, who are no longer with us.&lt;/p&gt;
    &lt;p&gt;So, without further ado:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;my desktop is pretty boring, since it consists of xterm windows to whatever unix system i am using at the moment. the machine itself is likely to be running some x-window server like exceed on some flavor of windows, though for many years i just used an x terminal.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;If you thought it was boring last time, check this out!&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;2002:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;I don’t know how to make a screenshot, because I normally use my computer in text-mode. I have X and GNOME installed, but I use them only occasionally.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;2015:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Under X, I use the standard environment of Trisquel, but mostly I type at Emacs in a console.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;Well, my desktop is quite boring. I mostly work with four xterms and a few Netscape windows. The KDE bar hides automatically, you can only see a thin grey line at the bottom.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;Here is the new one. You'll see that, like before, I have lots of xterms where I work on Vim, Zimbu and email. Now using the Chrome browser, showing off the Zimbu homepage. But clearly everything has become bigger!&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;Linux (2.4.20-pre5), Gnome2, vim, Pine.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;Not that much has changed in 13 years. Still using Linux. Still just a browser window and a ton of terminals hiding behind them. The main change is that switched from Pine to Thunderbird for email at some point. The OS on my laptop here is Ubuntu with Unity although there are a lot of Debian packages installed so it is a bit of a hybrid at this point. Oh, and yes, my son Carl is a lot older now.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;Ah, my desktop is pretty boring, I used fvwm 1.24 as my window manager and I try to have no more than 1 or 2 windows open per virtual desktop. I use FreeBSD 4-STABLE as my operating system. I first came across Unix when I got an account on a Pyramid 90x running OSx. This had a dual-universe setup: both AT&amp;amp;T and BSD-style environments, chosen by an environment variable. Initially I was given the AT&amp;amp;T environment, but my friends convinced me to ``come over” to BSD. Since then I’ve been a BSD afficionado.&lt;/p&gt;
      &lt;p&gt;After OSx, SunOS 3.5 and later SunOS releases, until 386BSD 0.1 came out and I started to run BSD at home. Then when 386BSD transmogrified to FreeBSD, I went with FreeBSD.&lt;/p&gt;
      &lt;p&gt;In terms of desktop, I’m a command-line guy, always will be. My favourite editor is vi, my favourite shell is tcsh (but kudos to rc for elegance). So I don’t really feel the need for GUI things like Gnome or KDE :-)&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;How things have (and have not changed). I'm still a command-line junkie with at least two xterm windows open. I'm still using a 3x3 virtual desktop. However, instead of fvwm, it is now LXDE. I've also switched from FreeBSD to Linux and I'm running Lubuntu as my distribution.&lt;/p&gt;
      &lt;p&gt;There are a lot of indispensable GUI tools that I use. These include Firefox, lyx, Gimp, KeepassX, Shutter, viking, dia, Wireshark, calibre, audacity, Handbrake and VLC. But where possible I still prefer to script things. My main development languages are still shell, Perl and C.&lt;/p&gt;
      &lt;p&gt;My shell is now bash. The vi keystrokes are burned into my fingertips and, as long as vim can be ported to new systems, that will be my text editor until I pass on. My mail client is now mutt (definitely not a web client) and my mail is stored locally, not on someone else's server.&lt;/p&gt;
      &lt;p&gt;The only issue I have is that, since a job change, I now have to deal with Windoze things. Thus, I have VirtualBox, libreoffice and Wine to help me do that.&lt;/p&gt;
      &lt;p&gt;I started with Unix on a Pyramid 90x. I now have a smart phone that blows the 90x out of the water on performance, RAM and storage. But I'm so very happy that, somewhere down underneath, there is still a Bourne shell and an operating system that does open(), close(), read(), write(), fork() and exec()!&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;You’ll probably be sad (or perhaps not) to hear that my desktop hasn’t really changed much at all - still OS X, though because OS X has virtual desktops now I have multiple “desktops” (6 of them) where Mail.app runs on one, Safari on another, Calendar, Slack, etc - all on separate desktops. This makes it a bit boring, but here’s the one I probably spend the most time in - the terminal window desktop. :)&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;There we go. Actually, that’s a condensate in one workspace cause I usually use about 4. Some of my favourite apps:&lt;/p&gt;
      &lt;item&gt;http://anjuta.sf.net/ (IDE)&lt;/item&gt;
      &lt;item&gt;http://quirc.org/ (IRC)&lt;/item&gt;
      &lt;item&gt;http://gaim.sf.net/ (IM)&lt;/item&gt;
      &lt;item&gt;http://multignometerm.sf.net/ (Term)&lt;/item&gt;
      &lt;p&gt;not on the shot, but worth noted&lt;/p&gt;
      &lt;item&gt;http://sylpheed.good-day.net/ (Email Client)&lt;/item&gt;
      &lt;p&gt;and of course a shot of RTCW&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;'screenshot as code', I maintain my desktop configuration through saltstack: https://github.com/TTimo/linux-salted/commits/master&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Discussion: Hacker News; reddit: /r/programming, /r/linux&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46176905</guid><pubDate>Sat, 06 Dec 2025 21:55:09 +0000</pubDate></item><item><title>How the 'hypnagogic state' of drowsiness could enhance your creativity</title><link>https://theconversation.com/how-the-hypnagogic-state-of-drowsiness-could-enhance-your-creativity-269724</link><description>&lt;doc fingerprint="27ef1aba0f204dc0"&gt;
  &lt;main&gt;
    &lt;p&gt;The Beatles’ song Yesterday was written in what psychologists refer to as the “hypnagogic state”. This is the twilight zone between sleep and wakefulness, when we drowsily linger in a semi-conscious state, experiencing vivid mental images and sounds.&lt;/p&gt;
    &lt;p&gt;Waking up one morning in early 1965, Paul McCartney became aware of a long complex melody playing inside his head. He jumped straight out of bed, sat down at his piano and picked out the melody on the keys. He quickly found the chords to go with the melody and created some holding phrases (as songwriters call them, before they write proper lyrics) to fit the melody.&lt;/p&gt;
    &lt;p&gt;Finding it difficult to believe that such a beautiful melody could emerge spontaneously, McCartney suspected that he was subconsciously plagiarising another composition. As he recalled: “For about a month I went round to people in the music business and asked them whether they had ever heard it before … I thought if no one claimed after a few weeks, then I could have it.” But it turned out to be original.&lt;/p&gt;
    &lt;p&gt;Many great discoveries and inventions have emerged from the hypnagogic state. The physicist Niels Bohr effectively won the Noble prize while semi-conscious. Drifting off to sleep, he dreamt he saw the nucleus of the atom, with the electrons spinning around it, just like the solar system with the sun and planets – and in this way he “discovered” the structure of the atom.&lt;/p&gt;
    &lt;head rend="h2"&gt;The sweet spot&lt;/head&gt;
    &lt;p&gt;Research has shown that the hypnagogic state is a creative “sweet spot.” For example, in a 2021 study, participants in a hypnagogic state were three times more likely to discover the “hidden rule” that could solve a mathematical problem.&lt;/p&gt;
    &lt;p&gt;Psychologists associate creativity with qualities such as openness to experience and cognitive flexibility. Others have suggested that creativity arises from co-ordination between the cognitive control network of the brain (which deals with planning and problem solving) and the default mode network (which is associated with daydreaming and mind-wandering).&lt;/p&gt;
    &lt;p&gt;However, in my view, one of the most important theories of creativity is one of the oldest, put forward by the early British psychologist Frederic Myers in 1881. According to Myers, ideas and insights come as a sudden “uprush” from a subliminal mind.&lt;/p&gt;
    &lt;p&gt;As Myers saw it, our conscious mind is just a small segment of our overall mind, including not only what Sigmund Freud called the unconscious, but also wider and higher levels of consciousness. Ideas may gestate unconsciously for a long time before they emerge into conscious awareness.&lt;/p&gt;
    &lt;p&gt;This is why it often feels as if ideas come from beyond the mind, as if they are gifted to us. They can come from beyond our conscious mind.&lt;/p&gt;
    &lt;head rend="h2"&gt;The importance of relaxation&lt;/head&gt;
    &lt;p&gt;The hypnagogic state is so creative because, as we hover between sleep and wakefulness, the conscious mind is barely active. For a brief period, our mental boundaries are permeable, and there is a chance creative insights and ideas will flow through from the subliminal mind.&lt;/p&gt;
    &lt;p&gt;In a more general sense, this is why creativity is often associated with relaxation and idleness. When we relax, our conscious minds are usually less active. Often, when we are busy, our minds are full of chattering thoughts, so there is no space for creative insights to flow through.&lt;/p&gt;
    &lt;p&gt;This is also why meditation is strongly associated with creativity. Research shows that meditation promotes general creative qualities such as openness to experience and cognitive flexibility.&lt;/p&gt;
    &lt;p&gt;But perhaps even more importantly, meditation quietens and softens the conscious mind, so that we’re more liable to receive inspiration from beyond it. As I point out in my book The Leap, this is why there is a strong connection between spiritual awakening and creativity.&lt;/p&gt;
    &lt;head rend="h2"&gt;Nurturing the hypnogogic state&lt;/head&gt;
    &lt;p&gt;Research has found that around 80% of people have experienced the hypnagogic state, and that around a quarter of the population experience it regularly. It is slightly more common in women than men.&lt;/p&gt;
    &lt;p&gt;It is most likely to occur at the onset of sleep, but can also occur on waking up, or during the day if we become drowsy and zone out of normal consciousness.&lt;/p&gt;
    &lt;p&gt;Can we use the hypnagogic state to enhance our creativity? It’s certainly possible to linger in the hypnagogic state, as you probably know from Sunday morning lie-ins.&lt;/p&gt;
    &lt;p&gt;However, one of the difficulties is capturing the ideas that arise. In our drowsiness, we may not feel the impulse to record of our ideas. It’s tempting to tell ourselves before falling back to sleep, “This is such a good idea that it will definitely stick in my mind.” But when we wake up some time later, the idea is gone forever.&lt;/p&gt;
    &lt;p&gt;However, through mental training, there is no reason why we can’t build up a habit of recording our hypnagogic ideas. The best practice is to keep a pen and paper right on a bedside table. Or for a more contemporary variant, keep your phone beside the bed, with the recording app open.&lt;/p&gt;
    &lt;p&gt;In fact, this is a practice that Paul McCartney has always followed. He even trained himself to write in the dark for this purpose.&lt;/p&gt;
    &lt;p&gt;We can also use a technique of “conscious napping” to generate ideas. Whenever the great inventor Thomas Edison was stuck for a solution or new idea, he would allow himself to drift into unconsciousness, while holding a metal ball. As he fell asleep, the ball would clatter to the ground and wake him, when he would often find that a new insight had emerged.&lt;/p&gt;
    &lt;p&gt;More generally, we should use idleness as a way of cultivating creativity. Don’t think of napping or relaxing as a waste of time. Far from being unproductive, they may lead to the most inspired ideas and insights of our lives.&lt;/p&gt;
    &lt;p&gt;This article features references to books that have been included for editorial reasons, and may contain links to bookshop.org. If you click on one of the links and go on to buy something from bookshop.org The Conversation UK may earn a commission.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46176908</guid><pubDate>Sat, 06 Dec 2025 21:55:29 +0000</pubDate></item><item><title>Chernobyl protective shield can no longer confine radiation after drone strike</title><link>https://www.cnn.com/2025/12/06/europe/chernobyl-drone-strike-radiation-latam-intl</link><description>&lt;doc fingerprint="110085f7cf2bb4d2"&gt;
  &lt;main&gt;
    &lt;p&gt;The protective shield built around the Chernobyl nuclear disaster site in Ukraine can no longer do its job to confine radioactive waste as a result of a drone strike earlier this year, according to the International Atomic Energy Agency (IAEA).&lt;/p&gt;
    &lt;p&gt;The New Safe Confinement (NSC) at Chernobyl, which was “severely damaged” by the drone strike in February, has “lost its primary safety functions, including the confinement capability,” the IAEA said in a Friday statement.&lt;/p&gt;
    &lt;p&gt;Ukraine accused Russia of carrying out the February 14 strike at Chernobyl, which the Kremlin denied.&lt;/p&gt;
    &lt;p&gt;The strike hit the NSC, sparking a fire and damaging the protective cladding around it, the IAEA said.&lt;/p&gt;
    &lt;p&gt;The nuclear watchdog has recommended a major renovation of the huge steel structure, which was put into place several years ago to enable clean-up operations and ensure the site’s safety nearly four decades on from the worst nuclear power plant accident in history.&lt;/p&gt;
    &lt;p&gt;“Limited temporary repairs have been carried out on the roof, but timely and comprehensive restoration remains essential to prevent further degradation and ensure long-term nuclear safety,” IAEA Director General Rafael Mariano Grossi said.&lt;/p&gt;
    &lt;p&gt;Grossi added that there had been no permanent damage to the NSC’s load-bearing structures or monitoring systems.&lt;/p&gt;
    &lt;p&gt;The IAEA, which has a permanent presence at the site, will “continue to do everything it can to support efforts to fully restore nuclear safety and security,” Grossi said.&lt;/p&gt;
    &lt;p&gt;It’s not the first time that Chernobyl has been in the spotlight over the course of Russia’s near four-year war in Ukraine. Russian forces seized the nuclear plant and its surrounding area in the early days of Moscow’s full-scale invasion, overrunning the plant in February 2022 and holding staff hostage. They left the plant and handed back control to Ukrainian personnel just over a month later.&lt;/p&gt;
    &lt;p&gt;The NSC is a massive, arch-shaped steel structure built at the Chernobyl site to cover the ruined No. 4 reactor and contain its radioactive material.&lt;/p&gt;
    &lt;p&gt;As the world’s largest movable land structure, the colossal hangar is a monumental feat of engineering. Built in 2010 and completed in 2019, it was designed to last 100 years and has played a crucial role in securing the site.&lt;/p&gt;
    &lt;p&gt;The project cost €2.1 billion and was funded by contributions from more than 45 donor countries and organizations through the Chernobyl Shelter Fund, according to the European Bank for Reconstruction and Development, which in 2019 hailed the venture as “the largest international collaboration ever in the field of nuclear safety.”&lt;/p&gt;
    &lt;p&gt;In April 26, 1986 an explosion tore through the No. 4 reactor at Chernobyl, in what was then the Soviet Union, spreading radioactivity across swathes of Ukraine, Belarus, Russia and beyond.&lt;/p&gt;
    &lt;p&gt;More than 30 people were killed in the nearby city of Pripyat, Ukraine, with many others suffering symptoms resulting from radiation exposure since, according to the IAEA and the World Health Organization. Birth defects and cancer rates among residents in the area exposed to radiation are still high.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46176987</guid><pubDate>Sat, 06 Dec 2025 22:06:48 +0000</pubDate></item><item><title>Wave of (Open Street Map) Vandalism in South Korea</title><link>https://www.openstreetmap.org/user/KennyDap/diary/407844</link><description>&lt;doc fingerprint="722999e04011753"&gt;
  &lt;main&gt;
    &lt;p&gt;About a week ago, vandals began defacing the map in South Korea. Over the course of that week, I rolled back hundreds of changes, and with the help of the site’s moderators, I banned over 50 malicious accounts.&lt;/p&gt;
    &lt;p&gt;In fact, the problem arose not even a week ago, but about a month ago. Back then, South Korean media reported that one account had allegedly leaked the locations of all the country’s military bases to the public on Openstreetmap. (link)&lt;/p&gt;
    &lt;p&gt;Even then, this false media claim struck me as disgusting, but it didn’t cause any major problems. Almost simultaneously, another Korean media report about Openstreetmap appeared: allegedly, an error in the domestic mapping services NaverMap and KakaoMap displaying a river in North Korea was linked to Openstreetmap’s activities: (link)&lt;/p&gt;
    &lt;p&gt;It’s hard to say whether this is a deliberate attack on OpenStreetMap or whether the Korean journalists simply lack the basic journalistic training to understand the issue. I’m leaning toward the latter, as if there was a deliberate intent, they would have acted more intelligently. But the result is what matters. This news quickly found its way into the ultra-conservative circles of the Korean right, who are obsessed with conspiracy theories, set up their accounts under the hashtag #YoonAgain, and believe that OpenStreetMap is a creation of Chinese communists and Russian Putinists. They say that yesterday, Russia bombed Ukraine with OpenStreetMap, and tomorrow, North Korea will start bombing the South. So they raided OpenStreetMap with the goal of once again saving (and embarrassing) their country.&lt;/p&gt;
    &lt;p&gt;But what’s funniest about this whole situation for me is that it wasn’t even Korean military bases that were vandalized, but power plants. Apparently, some employee at one of them (or even at a ministry) was such a crazy person that he managed to convince his superiors that there was data somewhere online that needed to be urgently deleted. He didn’t send an official request, didn’t write on the forum, and refuses to engage in dialogue (I tried). He and at least a few other people simply persistently create accounts on OpenStreetMap and try to delete things that I can restore with a single click.&lt;/p&gt;
    &lt;p&gt;That’s how it goes :)&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46177198</guid><pubDate>Sat, 06 Dec 2025 22:34:56 +0000</pubDate></item></channel></rss>