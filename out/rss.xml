<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Wed, 15 Oct 2025 12:21:03 +0000</lastBuildDate><item><title>How AI hears accents: An audible visualization of accent clusters</title><link>https://accent-explorer.boldvoice.com/</link><description>&lt;doc fingerprint="35bba6d935b3db90"&gt;
  &lt;main&gt;
    &lt;p&gt;Today, we’re going to go on a tour of the world's accents in English. Users of BoldVoice, the American accent training app, speak more than 200 different languages, and it is our mission to help them speak English clearly and confidently. While building the accent strength metric we covered in the previous blog post, we needed to understand how our models clustered accents, dialects, native languages, and language families. Today, we will share some of our findings using a 3D latent visualization.&lt;/p&gt;
    &lt;p&gt;To begin, we finetuned HuBERT, a pretrained audio-only foundation model for the task of accent identification using our in-house dataset of non-native English speech and self-reported accents. BoldVoice’s own dataset of accented speech is one of the largest of its kind in the world.&lt;/p&gt;
    &lt;p&gt;This model receives only the raw input audio and associated accent label; it gets neither a text prompt nor a transcript. For this "finetuning", we sampled 30 million speech recordings comprising 25,000 hours of English speech - a small fraction of our total accent dataset. Unlike a traditional finetune, we unfroze all layers of the pretrained base model due to the large size of our dataset. We trained the model for roughly a week on a cluster of A100 GPUs.&lt;/p&gt;
    &lt;p&gt;While the accent identifier performs quite well across the top hundred or so accents (play with it yourself at accentoracle.com), for today, we are less interested in its raw performance, and more interested in the clustering of accents in its latent space.&lt;/p&gt;
    &lt;p&gt;To observe how accents cluster, we've provided an audible latent space visualization for a small subset of recordings. Hover on the points on the graph to see the language labels.&lt;/p&gt;
    &lt;p&gt;The visualization is created by applying the UMAP dimensionality reduction technique to reduce the 768-dimensional latent space to just 3 dimensions.&lt;/p&gt;
    &lt;p&gt;Note that UMAP destroys much of the information in the full-dimensional latent space, but roughly preserves the global structure, including the relative distances between clusters. Each point represents a single recording inferenced by the model after it was fine tuned and the color corresponds to the true accent label.&lt;/p&gt;
    &lt;p&gt;Finally, in order to denoise the clusters, we cherry-pick only those points for which the predicted and target accents match. Remember, the purpose of this visualization is not to help us assess the performance of the model, but to understand where it has placed accents relative to one another.&lt;/p&gt;
    &lt;p&gt;By clicking or tapping on a point, you will hear a standardized version of the corresponding recording. The reason for voice standardization is two-fold: first, it anonymizes the speaker in the original recordings in order to protect their privacy. Second, it allows us to hear each accent projected onto a neutral voice, making it easier to hear the accent differences and ignore extraneous differences like gender, recording quality, and background noise. However, there is no free lunch: it does not perfectly preserve the source accent and introduces some audible phonetic artifacts.&lt;/p&gt;
    &lt;p&gt;This voice standardization model is an in-house accent-preserving voice conversion model.&lt;/p&gt;
    &lt;p&gt;Please explore the latent space visualization. You can click, drag, zoom, and scroll to navigate. You can also isolate accents by double clicking them in the legend to the right (desktop only) – double-clicking again will undo the filter.&lt;/p&gt;
    &lt;p&gt;Meanwhile, think about the following questions: which accents would you expect to be clustered together? Do you expect them to follow the taxonomy of language families or to cluster in other ways?&lt;/p&gt;
    &lt;p&gt;Our team was most surprised to see that geographic proximity, immigration, and colonialism seem to affect this model's learned accent groupings more than language taxonomy. Click the button below to explore our first grouping.&lt;/p&gt;
    &lt;p&gt;For example, the Australian cluster is right next to the Vietnamese cluster despite the fact that English and Vietnamese are not related taxonomically. If you listen to the 10 points that make up a bridge between the two clusters, you hear what sounds like native Vietnamese speakers who speak English with an Australian accent. Perhaps these hybrid accents could explain the overall proximity of these clusters.&lt;/p&gt;
    &lt;p&gt;We see something similar for the French/Nigerian/Ghanaian grouping.&lt;/p&gt;
    &lt;p&gt;It's important to remember that the distances on this map are not an objective measure of the phonetic similarity between accents. They are a byproduct of a model which has successfully learned to distinguish a variety of accents in L2 English speech from audio alone with no knowledge of language or linguistics.&lt;/p&gt;
    &lt;p&gt;Next, take a look at the Indian subcontinent accent cluster. Note that the Telugu, Tamil, and Malayalam accents are grouped together at one end of the cluster, and the Nepali and Bengali accents are at the other. This roughly mirrors geography, where Telugu, Tamil, and Malayalam are widely spoken languages in southern India, and Bengali and Nepali are widely spoken in northwest India and Nepal.&lt;/p&gt;
    &lt;p&gt;Finally, let's scroll to the Mongolian cluster, where the nearest cluster is actually Korean.&lt;/p&gt;
    &lt;p&gt;Experts and non-experts have observed phonetic similarities between Mongolian and Korean. A now-refuted hypothesis called the "Altaic language family" once grouped them together.&lt;/p&gt;
    &lt;p&gt;It is interesting that this model, with no concept of language families, has also picked up on the phonetic similarities even as filtered through a second language (English).&lt;/p&gt;
    &lt;p&gt;What do you think? Is this a meaningless artifact of latent space visualization or evidence of real phonetic features diffusing between Korean and Mongolian?&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45581735</guid><pubDate>Tue, 14 Oct 2025 16:07:37 +0000</pubDate></item><item><title>How bad can a $2.97 ADC be?</title><link>https://excamera.substack.com/p/how-bad-can-a-297-adc-be</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45582462</guid><pubDate>Tue, 14 Oct 2025 17:12:10 +0000</pubDate></item><item><title>SmolBSD – build your own minimal BSD system</title><link>https://smolbsd.org</link><description>&lt;doc fingerprint="19dcff71d549ada5"&gt;
  &lt;main&gt;
    &lt;p&gt;build your own minimal BSD UNIX system&lt;/p&gt;
    &lt;p&gt;smolBSD is a meta-operating system built on top of NetBSD. It lets you compose your own UNIX environment â from a single-purpose microservice system to a fully-custom OS image â in just a few minutes.&lt;/p&gt;
    &lt;p&gt;The smolBSD environment uses the netbsd-MICROVM kernel as its foundation, leveraging the same portable, reliable codebase that powers NetBSD itself. You decide what to include â sshd, httpd, or your own service â and smolBSD builds a coherent, minimal, bootable image ready to run anywhere.&lt;/p&gt;
    &lt;quote&gt;$ bmake SERVICE=bozohttpd build â¡ï¸ starting the builder microvm â¡ï¸ host filesystem mounted on /mnt â¡ï¸ fetching sets â¡ï¸ creating root filesystem (512M) done â image ready: bozohttpd-amd64.img&lt;/quote&gt;
    &lt;p&gt;Build BSD systems like you build software â fast, reproducible, and minimal.&lt;/p&gt;
    &lt;p&gt;Pick only the components you need â from kernel to services.&lt;/p&gt;
    &lt;p&gt;Every build is deterministic, portable, and easy to version-control.&lt;/p&gt;
    &lt;p&gt;Powered by netbsd-MICROVM â boot to service in milliseconds.&lt;/p&gt;
    &lt;p&gt;Runs anywhere QEMU or Firecracker runs â cloud, CI, edge, or laptop.&lt;/p&gt;
    &lt;p&gt;Build and boot your own BSD system in seconds:&lt;/p&gt;
    &lt;quote&gt;$ git clone https://github.com/NetBSDfr/smolBSD $ cd smolBSD $ bmake SERVICE=sshd build â¡ï¸ starting the builder microvm â¡ï¸ fetching sets â¡ï¸ creating root filesystem (512M) done â image ready: sshd-amd64.img â¡ï¸ killing the builder microvm $ ./startnb.sh -f etc/sshd.conf [ 1.0092096] kernel boot time: 14ms Starting sshd. Server listening on :: port 22. Server listening on 0.0.0.0 port 22. $ ssh -p 2022 ssh@localhostDownload&lt;/quote&gt;
    &lt;p&gt; A complete static web server in a few megabytes. smolBSD builds a minimal system with &lt;code&gt;bozohttpd&lt;/code&gt;
preconfigured and ready to serve content immediately on boot.
        &lt;/p&gt;
    &lt;quote&gt;$ bmake SERVICE=bozohttpd build â¡ï¸ starting the builder microvm â¡ï¸ fetching sets â¡ï¸ creating root filesystem (512M) done â image ready: bozohttpd-amd64.img $ ./startnb.sh -f etc/bozohttpd.conf [ 1.001231] kernel boot time: 10ms Starting bozohttpd on :80 listening on 0.0.0.0:80&lt;/quote&gt;
    &lt;p&gt; A lightweight build and image creation service based entirely on NetBSD tools. The &lt;code&gt;nbakery&lt;/code&gt; image gives you a taste of a preconfigured NetBSD environment with all the well known tools.
        &lt;/p&gt;
    &lt;quote&gt;$ bmake SERVICE=nbakery build â¡ï¸ starting the builder microvm â¡ï¸ fetching sets â¡ï¸ creating root filesystem (512M) done â image ready: nbakery-amd64.img $ ./startnb.sh -f etc/nbakery.conf [ 1.008374] kernel boot time: 11ms Welcome to the (n)bakery! ð§ ðª doas&lt;command&gt;to run command as root ð¦ pkgin to manage packages ðª exit to cleanly shutdown, ^a-x to exit qemu ðª you are inside a tmux with prefix ^q&lt;/command&gt;&lt;/quote&gt;
    &lt;p&gt; A minimal secure shell server started with &lt;code&gt;nitro&lt;/code&gt;,
designed to launch instantly and provide remote access with zero unnecessary
services.  
          Ideal for an SSH bouncer.
        &lt;/p&gt;
    &lt;quote&gt;$ bmake SERVICE=nitrosshd build â¡ï¸ starting the builder microvm â¡ï¸ fetching sets â¡ï¸ creating root filesystem (512M) done â image ready: nitrosshd-amd64.img $ ./startnb.sh -f etc/sshd.conf [ 1.011598] kernel boot time: 12ms Created tmpfs /dev (1835008 byte, 3552 inodes) Starting sshd. Server listening on :: port 22. Server listening on 0.0.0.0 port 22.&lt;/quote&gt;
    &lt;p&gt;smolBSD is an independent project built on top of NetBSD. Join us, share your micro-systems, or contribute new services and build recipes.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45582758</guid><pubDate>Tue, 14 Oct 2025 17:43:33 +0000</pubDate></item><item><title>Beliefs that are true for regular software but false when applied to AI</title><link>https://boydkane.com/essays/boss</link><description>&lt;doc fingerprint="ac3c1b1a6d77819d"&gt;
  &lt;main&gt;
    &lt;p&gt;(a note for technical folk)1 | read as pdf | Substack | LessWrong&lt;/p&gt;
    &lt;p&gt;When it comes to understanding the dangers of AI systems, the general public has the worst kind of knowledge: that what you know for sure that just ainât so.&lt;/p&gt;
    &lt;p&gt;After 40 years of persistent badgering, the software industry has convinced the public that bugs can have disastrous consequences. This is great! It is good that people understand that software can result in real-world harm. Not only does the general public mostly understand the dangers, but they mostly understand that bugs can be fixed. It might be expensive, it might be difficult, but it can be done.&lt;/p&gt;
    &lt;p&gt;The problem is that this understanding, when applied to AIs like ChatGPT, is completely wrong. The software that runs AI acts very differently to the software that runs most of your computer or your phone. Good, sensible assumptions about bugs in regular software actually end up being harmful and misleading when you try to apply them to AI.&lt;/p&gt;
    &lt;p&gt;Attempting to apply regular-software assumptions to AI systems leads to confusion, and remarks such as:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;âIf something goes wrong with ChatGPT, canât some boffin just think hard for a bit, find the missing semi-colon or whatever, and then fix the bug?â&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;or&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;âEven if itâs hard for one person to understand everything the AI does, surely still smart people who individually understand small parts of what the AI does?â.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;or&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;âJust because current systems donât work perfectly, thatâs not a problem right? Because eventually weâll iron out all the bugs so the AIs will get more reliable over time, like old software is more reliable than new software.â&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;If you understand how modern AI systems work, these statements are all painfully incorrect. But if youâre used to regular software, theyâre completely reasonable. I believe there is a gap between the experts and the novices in the field:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;the experts donât see the gap because itâs so obvious, so they donât bother explaining the gap&lt;/item&gt;
      &lt;item&gt;the novices donât see the gap because they donât know to look, so they donât realise where their confusion comes from.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This leads to frustration on both sides, because the experts feel like their arguments arenât hitting home, and the novices feel like all arguments have obvious flaws. In reality, the experts and the novices have different, unspoken, assumptions about how AI systems work.&lt;/p&gt;
    &lt;head rend="h1"&gt;Some example false beliefs&lt;/head&gt;
    &lt;p&gt;To make this more concrete, here are some example ideas that are perfectly true when applied to regular software but become harmfully false when applied to modern AIs:&lt;/p&gt;
    &lt;head rend="h2"&gt;Software vulnerabilities are caused by mistakes in the code&lt;/head&gt;
    &lt;p&gt;In regular software, vulnerabilities are caused by mistakes in the lines of code that make up the software. There might be hundreds of thousands of lines of code, but code doesnât take up much space so this is only around 50MB of data, about the size of a small album of photos.&lt;/p&gt;
    &lt;p&gt;But in modern AI systems, vulnerabilities or bugs are usually caused by problems in the data used to train an AI2. It takes thousands of gigabytes of data to train modern AI systems, and bad behaviour isnât caused by any single bad piece of data, but by the combined effects of significant fractions of the dataset. Because these datasets are so large, nobody knows everything that an AI is actually trained on. One popular dataset, FineWeb, is about 11.25 trillion words long3, which, if you were reading at about 250 words per minute, would take you over 85 thousand years to read. Itâs just not possible for any single human (or even a team of humans) to have read everything that an LLM has read during training.&lt;/p&gt;
    &lt;head rend="h2"&gt;Bugs in the code can be found by carefully analysing the code&lt;/head&gt;
    &lt;p&gt;With regular software, if thereâs a bug, itâs possible for smart people to carefully read through the code and logically figure out what must be causing the bug.&lt;/p&gt;
    &lt;p&gt;With AI systems, almost all bad behaviour originates from the data thatâs used to train them2, but itâs basically impossible to look at misbehaving AI and figure out parts of the training data caused that bad behaviour. In practice, itâs rare to even attempt this, researchers will retrain the AI with more data to try and counteract the bad behaviour, or theyâll start over and try to curate the data to not include the bad data.&lt;/p&gt;
    &lt;p&gt;You cannot logically deduce what pieces of data caused the bad behaviour, you can only make good guesses. For example, modern AIs are trained on lots of mathematics proofs and programming tasks, because that seems to make them do better at reasoning and logical thinking tasks. If an AI system makes a logical reasoning mistake, itâs impossible to attribute that mistake to any portion of the training data, the only answer weâve got is to use more data next time.&lt;/p&gt;
    &lt;p&gt;I think I need to emphasise this: With regular software, we can pinpoint mistakes precisely, walk step-by-step through the events leading up to the mistake, and logically understand why that mistake happened. When AIs make mistakes, we donât understand the steps that caused those mistakes. Even the people who made the AIs donât understand why they make mistakes4. Nobody understands where these bugs come from. We sometimes kinda have a rough idea about why they maybe did something unusual. But weâre far, far away from anything that guarantees the AI wonât have any catastrophic failures.&lt;/p&gt;
    &lt;head rend="h2"&gt;Once a bug is fixed, it wonât come back again&lt;/head&gt;
    &lt;p&gt;With regular software, once youâve found the bug, you can fix the bug. And once youâve fixed the bug, it wonât re-appear5. There might be a bug that causes similar problems, but itâs not the same bug as the one you fixed. This means you can, if youâre patient, reduce the number of bugs over time and rest assured that removing new bugs wonât cause old bugs to re-appear.&lt;/p&gt;
    &lt;p&gt;This is not the case with AI. Itâs not really possible to âfixâ a bug in an AI, because even if the AI was behaving weirdly, and you retrained it, and now itâs not behaving weirdly anymore, you canât know for sure that the weird behaviour is gone, just that it doesnât happen for the prompts you tested. Itâs entirely possible that someone can find a prompt you forgot to test, and then the buggy behaviour is back again!&lt;/p&gt;
    &lt;head rend="h2"&gt;Every time you run the code, the same thing happens&lt;/head&gt;
    &lt;p&gt;With regular software, you can run the same piece of code multiple times and itâll behave in the same way. If you give it the same input, itâll give you the same output.&lt;/p&gt;
    &lt;p&gt;Now technically this is still true for AIs, if you give them exactly the prompt theyâll respond in exactly the same way. But practically, itâs very far from the truth6. Even tiny changes to the input of an AI can have dramatic changes in the output. Even innocent changes like adding a question mark at the end of your sentence or forgetting to start your sentence with a capital letter can cause the AI to return something different.&lt;/p&gt;
    &lt;p&gt;Additionally, most AI companies will slightly change the way their AIs respond, so that they say slightly different things to the same prompt. This helps their AIs seem less robotic and more natural.&lt;/p&gt;
    &lt;head rend="h2"&gt;If you give specifications beforehand, you can get software that meets those specifications&lt;/head&gt;
    &lt;p&gt;With regular software, this is true. You can sit with stakeholders to discuss the requirements for some piece of software, and then write code to meet those requirements. The requirements might change, but fundamentally you can write code to serve some specific purpose and have confidence that it will serve that specific purpose.&lt;/p&gt;
    &lt;p&gt;With AI systems, this is more or less false. Or at the very least, the creators of modern AI systems have far far less control about the behaviour the AIs will exhibit. We understand how to get an AI to meet narrow, testable specifications like speaking English and writing code, but we donât know how to get a brand new AI to achieve a certain score on some particular test or to guarantee global behaviour like ânever tells the user to commit a crimeâ. The best AI companies in the world have basically one lever which is âbetterâ, and they can pull that lever to make the AI better, but nobody knows precisely what to do to ensure an AI writes formal emails correctly or summarises text accurately.&lt;/p&gt;
    &lt;p&gt;This means that we donât know what an AI will be capable of before weâve trained it. Itâs very common for AIs to be released to the public for months before a random person on Twitter discovers some ability that the AI has which even its creators didnât know about. So far, these abilities have been mostly just fun, like being good at Geoguessr:&lt;/p&gt;
    &lt;p&gt;Or making photos look like they were from a Studio Ghibli film:&lt;/p&gt;
    &lt;p&gt;But thereâs no reason for these hidden abilities to always be positive. Itâs entirely possible that some dangerous capability is hidden in ChatGPT, but nobodyâs figured out the right prompt just yet.&lt;/p&gt;
    &lt;p&gt;While itâs possible to demonstrate the safety of an AI for a specific test suite or a known threat, itâs impossible for AI creators to definitively say their AI will never act maliciously or dangerously for any prompt it could be given.&lt;/p&gt;
    &lt;head rend="h1"&gt;Where to go from here&lt;/head&gt;
    &lt;p&gt;It is good that most people know the dangers of poorly written or buggy software. But this hard-won knowledge about regular software is misleading the public when it gets applied to AI. Despite the cries of âinscrutable arrays of floating point numbersâ, Iâd be surprised if a majority of people know that modern AI is architecturally different from regular software.&lt;/p&gt;
    &lt;p&gt;AI safety is a complicated and subtle argument. The best we can do is to make sure weâre starting from the same baseline, and that means conveying to our contemporaries that if it all starts to go wrong, we cannot just âpatch the bugâ7.&lt;/p&gt;
    &lt;p&gt;If this essay was the first time you realised AI was fundamentally different from regular software, let me know, and share this with a friend who might also not realise the difference.&lt;/p&gt;
    &lt;p&gt;If you always knew that regular software and AIs are fundamentally different, talk to your family and non-technical friends, or with a stranger at a coffee shop. I think youâll be surprised at how few people know that these two are different.&lt;/p&gt;
    &lt;p&gt;If youâre interested the dynamics between experts and novices, and how gaps between them arise, Iâve written more about the systemic biases encountered by experts (and the difficulties endured by novices) in this essay: Experts have it easy.&lt;/p&gt;
    &lt;p&gt;Thanks to Sam Cross and Caleb for reviewing drafts of this essay.&lt;/p&gt;
    &lt;p&gt;Discuss this essay:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;HackerNews (284 points, 218 comments)&lt;/item&gt;
      &lt;item&gt;r/programming (55 points, 7 comments)&lt;/item&gt;
      &lt;item&gt;/slatestarcodex (17 points, 19 comments)&lt;/item&gt;
      &lt;item&gt;Lobse.rs (12 points, 4 comments)&lt;/item&gt;
      &lt;item&gt;LessWrong (9 points, 2 comments)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Footnotes&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;This article is attempting to bridge a gap between the technical and the non-technical, so Iâm going to be quite lax with the jargon here and there. By âAIâ Iâm referring to 2025 frontier LLMs. Iâm also going to be making some sweeping statements about âhow software worksâ, these claims mostly hold, but they break down when applied to distributed systems, parallel code, or complex interactions between software systems and human processes. Feel free to debate me in the comments if you think this piece discussing how experts struggle to empathise with novices should have had more jargon (: â©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;It can also come from the reward model used during RLHF, but the reward model is still trained from data at the end of the day. It can also come from prompt injections, but those also only work because of the data. â© â©2&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;FineWeb is 15 trillion tokens, each token is about 0.75 words, 11.25 trillion words. â©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Anthropic is doing very good work to try and figure out why AIs think the way they do, but even the state of the art does not have a full understanding of these AIs, and what understanding we do have, is often partial and with significant gaps. â©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;You are writing tests to prevent regressions, right? right?! â©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;see for example, this blog post from Mira Muratiâs Thinking Machines Lab â©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Personally, I think some more empathy is needed when having good faith discussions with non-technical folk. Communication is empirically hard, in that it often goes wrong in practice, even if it feels easy to do. â©&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45583180</guid><pubDate>Tue, 14 Oct 2025 18:26:00 +0000</pubDate></item><item><title>Intel Announces Inference-Optimized Xe3P Graphics Card with 160GB VRAM</title><link>https://www.phoronix.com/review/intel-crescent-island</link><description>&lt;doc fingerprint="a4591a3d988e20a9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Intel Announces "Crescent Island" Inference-Optimized Xe3P Graphics Card With 160GB vRAM&lt;/head&gt;
    &lt;p&gt;Back during the Intel Tech Tour in Arizona, Intel teased a new inference-optimized enterprise GPU would be announced soon. This new product would feature enhanced memory, bandwidth, and enterprise-level AI inference capabilities. Today the embargo expires on talking about this new GPU offering.&lt;/p&gt;
    &lt;p&gt;When Intel was teasing this new inference-optimized GPU a few weeks back in Arizona it sounded like Intel may have had an unexpected trick up its sleeves. What's being announced today is indeed a new enterprise GPU for AI that is interesting from a technology perspective, but it's not shipping until at least H2'2026. So while there was hope that perhaps Intel had managed to innovate some interesting Battlemage / BMG-G31 part for AI or the like with lots of vRAM, what's being announced is a next-gen part but one that is at least one year away still.&lt;/p&gt;
    &lt;p&gt;This new graphics card is codenamed Crescent Island and is built on their next-gen Xe3P Celestial micro-architecture. Xe3P will be optimized around performance-per-Watt and Crescent Island will feature 160GB of LPDDR5x memory to allow for plenty of space for large language models (LLMs).&lt;/p&gt;
    &lt;p&gt;Intel's embargoed announcement also notes that Crescent Island will feature support for a variety of different data types and be an "ideal" solution for tokens-as-a-service providers and inference use cases.&lt;/p&gt;
    &lt;p&gt;In addition to being optimized around performance-per-Watt, Crescent Island will also be air cooled and cost-optimized. Intel is currently working on refining their open-source software stack for Crescent Island via using current-generation Arc Pro B-Series GPUs.&lt;/p&gt;
    &lt;p&gt;Intel's announcement notes that customer sampling of this new data center GPU will begin in the second half of 2026. No official release timeframe was provided if they also hope to squeeze it out next year or if (more than likely) it will actually ship more broadly in 2027 but just noting their customer sampling for H2'2026 in the embargoed news release. No slides or prototype images or anything else to share today on Intel's Crescent Island.&lt;/p&gt;
    &lt;p&gt;Long story short, Intel is announcing Crescent Island today as a Xe3LP + 160GB LPDDR5X offering for H2'2026 or later that will be AI inference optimized around power efficiency and cost. It sounds interesting but technical details beyond those basics were light and it's going to be a long while before we see Crescent Island. Given the timing this will be going up against the AMD Instinct MI450 series and NVIDIA Vera Rubin. It seems like Intel wanted to have something to announce now given the ongoing AI rush albeit not many details today and no short term AI solution.&lt;/p&gt;
    &lt;p&gt;At least this does lead to more weight for the ongoing Project Battlematrix Linux driver improvements and other ongoing Intel Compute Runtime and Intel Xe Linux driver enhancements that are currently ongoing for the Arc Pro B-Series. With confirming Crescent Island now it also opens the door to them beginning to push open-source hardware enablement patches without otherwise spilling the beans on this forthcoming enterprise AI product.&lt;/p&gt;
    &lt;p&gt;Intel is using the OCP Global Summit to announce some additional Gaudi 3 rack-scale reference designs. These new Gaudi 3 rack-scale reference designs will allow up to 64 accelerators per rack with liquid cooling and 8.2TB of high bandwidth memory. Intel hasn't aggressively promoted Gaudi 3 in recent quarters after its launch last year. Gaudi 3 has enjoyed some reprieve since Intel canceled their Falcon Shores AI accelerator chip but still appears to be the end of the road for Gaudi especially with Jaguar Shores still expected and now Crescent Island too. The Gaudi 3 software support has been neglected over the past year with losing multiple rounds of the Habana Labs Linux driver maintainers and only recently seeing new activity to return to working on this AI accelerator Linux driver albeit as of writing for Linux 6.18 there still is no mainline kernel driver support for Gaudi 3.&lt;/p&gt;
    &lt;p&gt;If you enjoyed this article consider joining Phoronix Premium to view this site ad-free, multi-page articles on a single page, and other benefits. PayPal or Stripe tips are also graciously accepted. Thanks for your support.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45583243</guid><pubDate>Tue, 14 Oct 2025 18:30:57 +0000</pubDate></item><item><title>Unpacking Cloudflare Workers CPU Performance Benchmarks</title><link>https://blog.cloudflare.com/unpacking-cloudflare-workers-cpu-performance-benchmarks/</link><description>&lt;doc fingerprint="ef3a174958a2cc9b"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;On October 4, independent developer Theo Browne published a series of benchmarks designed to compare server-side JavaScript execution speed between Cloudflare Workers and Vercel, a competing compute platform built on AWS Lambda. The initial results showed Cloudflare Workers performing worse than Node.js on Vercel at a variety of CPU-intensive tasks, by a factor of as much as 3.5x.&lt;/p&gt;
      &lt;p&gt;We were surprised by the results. The benchmarks were designed to compare JavaScript execution speed in a CPU-intensive workload that never waits on external services. But, Cloudflare Workers and Node.js both use the same underlying JavaScript engine: V8, the open source engine from Google Chrome. Hence, one would expect the benchmarks to be executing essentially identical code in each environment. Physical CPUs can vary in performance, but modern server CPUs do not vary by anywhere near 3.5x.&lt;/p&gt;
      &lt;p&gt;On investigation, we discovered a wide range of small problems that contributed to the disparity, ranging from some bad tuning in our infrastructure, to differences between the JavaScript libraries used on each platform, to some issues with the test itself. We spent the week working on many of these problems, which means over the past week Workers got better and faster for all of our customers. We even fixed some problems that affect other compute providers but not us, such as an issue that made trigonometry functions much slower on Vercel. This post will dig into all the gory details. &lt;/p&gt;
      &lt;p&gt;It's important to note that the original benchmark was not representative of billable CPU usage on Cloudflare, nor did the issues involved impact most typical workloads. Most of the disparity was an artifact of the specific benchmark methodology. Read on to understand why.&lt;/p&gt;
      &lt;p&gt;With our fixes, the results now look much more like we'd expect:&lt;/p&gt;
      &lt;p&gt;There is still work to do, but we're happy to say that after these changes, Cloudflare now performs on par with Vercel in every benchmark case except the one based on Next.js. On that benchmark, the gap has closed considerably, and we expect to be able to eliminate it with further improvements detailed later in this post.&lt;/p&gt;
      &lt;p&gt;We are grateful to Theo for highlighting areas where we could make improvements, which will now benefit all our customers, and even many who aren't our customers.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Our benchmark methodology&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;We wanted to run Theo's test with no major design changes, in order to keep numbers comparable. Benchmark cases are nearly identical to Theo's original test but we made a couple changes in how we ran the test, in the hopes of making the results more accurate:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;
          &lt;p&gt;Theo ran the test client on a laptop connected by a Webpass internet connection in San Francisco, against Vercel instances running in its sfo1 region. In order to make our results easier to reproduce, we chose instead to run our test client directly in AWS's us-east-1 datacenter, invoking Vercel instances running in its iad1 region (which we understand to be in the same building). We felt this would minimize any impact from network latency. Because of this, Vercel's numbers are slightly better in our results than they were in Theo's.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;We chose to use Vercel instances with 1 vCPU instead of 2. All of the benchmarks are single-threaded workloads, meaning they cannot take advantage of a second CPU anyway. Vercel's CTO, Malte Ubl, had stated publicly on X that using single-CPU instances would make no difference in this test, and indeed, we found this to be correct. Using 1 vCPU makes it easier to reason about pricing, since both Vercel and Cloudflare charge for CPU time (&lt;code&gt;$&lt;/code&gt;0.128/hr for Vercel in iad1, and &lt;code&gt;$&lt;/code&gt;0.072/hr for Cloudflare globally).&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;We made some changes to fix bugs in the test, for which we submitted a pull request. More on this below.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Theo's benchmarks covered a variety of frameworks, making it clear that no single JavaScript library could be at fault for the general problem. Clearly, we needed to look first at the Workers Runtime itself. And so we did, and we found two problems â not bugs, but tuning and heuristic choices which interacted poorly with the benchmarks as written.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Sharding and warm isolate routing: A problem of scheduling, not CPU speed&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Over the last year we shipped smarter routing that sends traffic to warm isolates more often. That cuts cold starts for large apps, which matters for frameworks with heavy initialization requirements like Next.js. The original policy optimized for latency and throughput across billions of requests, but was less optimal for heavily CPU-bound workloads for the same reason that such workloads cause performance issues in other platforms like Node.js: When the CPU is busy computing an expensive operation for one request, other requests sent to the same isolate must wait for it to finish before they can proceed.&lt;/p&gt;
      &lt;p&gt;The system uses heuristics to detect when requests are getting blocked behind each other, and automatically spin up more isolates to compensate. However, these heuristics are not precise, and the particular workload generated by Theo's tests â in which a burst of expensive traffic would come from a single client â played poorly with our existing algorithm. As a result, the benchmarks showed much higher latency (and variability in latency) than would normally be expected.&lt;/p&gt;
      &lt;p&gt;It's important to understand that, as a result of this problem, the benchmark was not really measuring CPU time. Pricing on the Workers platform is based on CPU time â that is, time spent actually executing JavaScript code, as opposed to time waiting for things. Time spent waiting for the isolate to become available makes the request take longer, but is not billed as CPU time against the waiting request. So, this problem would not have affected your bill.&lt;/p&gt;
      &lt;p&gt;After analyzing the benchmarks, we updated the algorithm to detect sustained CPU-heavy work earlier, then bias traffic so that new isolates spin up faster. The result is that Workers can more effectively and efficiently autoscale when different workloads are applied. I/O-bound workloads coalesce into individual already warm isolates while CPU-bound are directed so that they do not block each other. This change has already been rolled out globally and is enabled automatically for everyone. It should be pretty clear from the graph when the change was rolled out:&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;V8 garbage collector tuning&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;While this scheduling issue accounted for the majority of the disparity in the benchmark, we did find a minor issue affecting code execution performance during our testing.&lt;/p&gt;
      &lt;p&gt;The range of issues that we uncovered in the framework code in these benchmarks repeatedly pointed at garbage collection and memory management issues as being key contributors to the results. But, we would expect these to be an issue with the same frameworks running in Node.js as well. To see exactly what was going on differently with Workers and why it was causing such a significant degradation in performance, we had to look inwards at our own memory management configuration.&lt;/p&gt;
      &lt;p&gt;The V8 garbage collector has a huge number of knobs that can be tuned that directly impact performance. One of these is the size of the "young generation". This is where newly created objects go initially. It's a memory area that's less compact, but optimized for short-lived objects. When objects have bounced around the "young space" for a few generations they get moved to the old space, which is more compact, but requires more CPU to reclaim.&lt;/p&gt;
      &lt;p&gt;V8 allows the embedding runtime to tune the size of the young generation. And it turns out, we had done so. Way back in June of 2017, just two months after the Workers project kicked off, we â or specifically, I, Kenton, as I was the only engineer on the project at the time â had configured this value according to V8's recommendations at the time for environments with 512MB of memory or less. Since Workers defaults to a limit of 128MB per isolate, this seemed appropriate.&lt;/p&gt;
      &lt;p&gt;V8's entire garbage collector has changed dramatically since 2017. When analyzing the benchmarks, it became apparent that the setting which made sense in 2017 no longer made sense in 2025, and we were now limiting V8's young space too rigidly. Our configuration was causing V8's garbage collection to work harder and more frequently than it otherwise needed to. As a result, we have backed off on the manual tuning and now allow V8 to pick its young space size more freely, based on its internal heuristics. This is already live on Cloudflare Workers, and it has given an approximately 25% boost to the benchmarks with only a small increase in memory usage. Of course, the benchmarks are not the only Workers that benefit: all Workers should now be faster. That said, for most Workers the difference has been much smaller.&lt;/p&gt;
      &lt;p&gt;The platform changes solved most of the problem. Following the changes, our testing showed we were now even on all of the benchmarks save one: Next.js.&lt;/p&gt;
      &lt;p&gt;Next.js is a popular web application framework which, historically, has not had built-in support for hosting on a wide range of platforms. Recently, a project called OpenNext has arisen to fill the gap, making Next.js work well on many platforms, including Cloudflare. On investigation, we found several missing optimizations and other opportunities to improve performance, explaining much of why the benchmark performed poorly on Workers.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Unnecessary allocations and copies&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;When profiling the benchmark code, we noticed that garbage collection was dominating the timeline. From 10-25% of the request processing time was being spent reclaiming memory.&lt;/p&gt;
      &lt;p&gt;So we dug in and discovered that OpenNext, and in some cases Next.js and React itself, will often create unnecessary copies of internal data buffers at some of the worst times during the handling of the process. For instance, there's one &lt;code&gt;pipeThrough()&lt;/code&gt; operation in the rendering pipeline that we saw creating no less than 50 2048-byte &lt;code&gt;Buffer&lt;/code&gt; instances, whether they are actually used or not.&lt;/p&gt;
      &lt;p&gt;We further discovered that on every request, the Cloudflare OpenNext adapter has been needlessly copying every chunk of streamed output data as itâs passed out of the renderer and into the Workers runtime to return to users. Given this benchmark returns a 5 MB result on every request, that's a lot of data being copied!&lt;/p&gt;
      &lt;p&gt;In other places, we found that arrays of internal Buffer instances were being copied and concatenated using &lt;code&gt;Buffer.concat&lt;/code&gt; for no other reason than to get the total number of bytes in the collection. That is, we spotted code of the form &lt;code&gt;getBody().length&lt;/code&gt;. The function &lt;code&gt;getBody()&lt;/code&gt; would concatenate a large number of buffers into a single buffer and return it, without storing the buffer anywhere. So, all that work was being done just to read the overall length. Obviously this was not intended, and fixing it was an easy win.&lt;/p&gt;
      &lt;p&gt;We've started opening a series of pull requests in OpenNext to fix these issues, and others in hot paths, removing some unnecessary allocations and copies:&lt;/p&gt;
      &lt;p&gt;We're not done. We intend to keep iterating through OpenNext code, making improvements wherever theyâre needed â not only in the parts that run on Workers. Many of these improvements apply to other OpenNext platforms. The shared goal of OpenNext is to make NextJS as fast as possible regardless of where you choose to run your code.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;Inefficient Streams Adapters&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Much of the Next.js code was written to use Node.js's APIs for byte streams. Workers, however, prefers the web-standard Streams API, and uses it to represent HTTP request and response bodies. This necessitates using adapters to convert between the two APIs. When investigating the performance bottlenecks, we found a number of examples where inefficient streams adapters are being needlessly applied. For example:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;const stream = Readable.toWeb(Readable.from(res.getBody()))&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;&lt;code&gt;res.getBody()&lt;/code&gt; was performing a &lt;code&gt;Buffer.concat(chunks)&lt;/code&gt; to copy accumulated chunks of data into a new Buffer, which was then passed as an iterable into a Node.js &lt;code&gt;stream.Readable&lt;/code&gt; that was then wrapped by an adapter that returns a &lt;code&gt;ReadableStream&lt;/code&gt;. While these utilities do serve a useful purpose, this becomes a data buffering nightmare since both Node.js streams and Web streams each apply their own internal buffers! Instead we can simply do:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;const stream = ReadableStream.from(chunks);&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;This returns a &lt;code&gt;ReadableStream&lt;/code&gt; directly from the accumulated chunks without additional copies, extraneous buffering, or passing everything through inefficient adaptation layers.&lt;/p&gt;
      &lt;p&gt;In other places we see that Next.js and React make extensive use of &lt;code&gt;ReadableStream&lt;/code&gt; to pass bytes through, but the streams being created are value-oriented rather than byte-oriented! For example,&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;const readable = new ReadableStream({
  pull(controller) {
    controller.enqueue(chunks.shift());
    if (chunks.length === 0) {
      controller.close();
    }
});  // Default highWaterMark is 1!
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;Seems perfectly reasonable. However, there's an issue here. If the chunks are &lt;code&gt;Buffer&lt;/code&gt; or &lt;code&gt;Uint8Array&lt;/code&gt; instances, every instance ends up being a separate read by default. So if the &lt;code&gt;chunk&lt;/code&gt; is only a single byte, or 1000 bytes, that's still always two reads. By converting this to a byte stream with a reasonable high water mark, we can make it possible to read this stream much more efficiently:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;const readable = new ReadableStream({
  type: 'bytes',
  pull(controller) {
    controller.enqueue(chunks.shift());
    if (chunks.length === 0) {
      controller.close();
    }
}, { highWaterMark: 4096 });
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;Now, the stream can be read as a stream of bytes rather than a stream of distinct JavaScript values, and the individual chunks can be coalesced internally into 4096 byte chunks, making it possible to optimize the reads much more efficiently. Rather than reading each individual enqueued chunk one at a time, the ReadableStream will proactively call &lt;code&gt;pull()&lt;/code&gt; repeatedly until the highWaterMark is reached. Reads then do not have to ask the stream for one chunk of data at a time.&lt;/p&gt;
      &lt;p&gt;While it would be best for the rendering pipeline to be using byte streams and paying attention to back pressure signals more, our implementation can still be tuned to better handle cases like this.&lt;/p&gt;
      &lt;p&gt;The bottom line? We've got some work to do! There are a number of improvements to make in the implementation of OpenNext and the adapters that allow it to work on Cloudflare that we will continue to investigate and iterate on. We've made a handful of these fixes already and we're already seeing improvements. Soon we also plan to start submitting patches to Next.js and React to make further improvements upstream that will ideally benefit the entire ecosystem.&lt;/p&gt;
      &lt;p&gt;Aside from buffer allocations and streams, one additional item stood out like a sore thumb in the profiles: &lt;code&gt;JSON.parse()&lt;/code&gt; with a reviver function. This is used in both React and Next.js and in our profiling this was significantly slower than it should be. We built a microbenchmark and found that JSON.parse with a reviver argument recently got even slower when the standard added a third argument to the reviver callback to provide access to the JSON source context.&lt;/p&gt;
      &lt;p&gt;For those unfamiliar with the reviver function, it allows an application to effectively customize how JSON is parsed. But it has drawbacks. The function gets called on every key-value pair included in the JSON structure, including every individual element of an Array that gets serialized. In Theo's NextJS benchmark, in any single request, it ends up being called well over 100,000 times!&lt;/p&gt;
      &lt;p&gt;Even though this problem affects all platforms, not just ours, we decided that we weren't just going to accept it. After all, we have contributors to V8 on the Workers runtime team! We've upstreamed a V8 patch that can speed up &lt;code&gt;JSON.parse()&lt;/code&gt; with revivers by roughly 33 percent. That should be in V8 starting with version 14.3 (Chrome 143) and can help everyone using V8, not just Cloudflare: Node.js, Chrome, Deno, the entire ecosystem.Â  If you are not using Cloudflare Workers or didn't change the syntax of your reviver you are currently suffering under the red performance bar.&lt;/p&gt;
      &lt;p&gt;We will continue to work with framework authors to reduce overhead in hot paths. Some changes belong in the frameworks, some belong in the engine, some in our platform.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;Node.js's trigonometry problem&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;We are engineers, and we like to solve engineering problems â whether our own, or for the broader community.&lt;/p&gt;
      &lt;p&gt;Theo's benchmarks were actually posted in response to a different benchmark by another author which compared Cloudflare Workers against Vercel. The original benchmark focused on calling trigonometry functions (e.g. sine and cosine) in a tight loop. In this benchmark, Cloudflare Workers performed 3x faster than Node.js running on Vercel.&lt;/p&gt;
      &lt;p&gt;The author of the original benchmark offered this as evidence that Cloudflare Workers are just faster. Theo disagreed, and so did we. We expect to be faster, but not by 3x! We don't implement math functions ourselves; these come with V8. We weren't happy to just accept the win, so we dug in.&lt;/p&gt;
      &lt;p&gt;It turns out that Node.js is not using the latest, fastest path for these functions. Node.js can be built with either the clang or gcc compilers, and is written to support a broader range of operating systems and architectures than Workers. This means that Node.js' compilation often ends up using a lowest-common denominator for some things in order to provide support for the broadest range of platforms. V8 includes a compile-time flag that, in some configurations, allows it to use a faster implementation of the trig functions. In Workers, mostly by coincidence, that flag is enabled by default. In Node.js, it is not. We've opened a pull request to enable the flag in Node.js so that everyone benefits, at least on platforms where it can be supported.&lt;/p&gt;
      &lt;p&gt;Assuming that lands, and once AWS Lambda and Vercel are able to pick it up, we expect this specific gap to go away, making these operations faster for everyone. This change won't benefit our customers, since Cloudflare Workers already uses the faster trig functions, but a bug is a bug and we like making everything faster.&lt;/p&gt;
      &lt;p&gt;Even the best benchmarks have bias and tradeoffs. It's difficult to create a benchmark that is truly representative of real-world performance, and all too easy to misinterpret the results of benchmarks that are not. We particularly liked Planetscale's take on this subject.&lt;/p&gt;
      &lt;p&gt;These specific CPU-bound tests are not an ideal choice to represent web applications. Theo even notes this in his video. Most real-world applications on Workers and Vercel are bound by databases, downstream services, network, and page size. End user experience is what matters. CPU is one piece of that picture. That said, if a benchmark shows us slower, we take it seriously.&lt;/p&gt;
      &lt;p&gt;While the benchmarks helped us find and fix many real problems, we also found a few problems with the benchmarks themselves, which contributed to the apparent disparity in speed:&lt;/p&gt;
      &lt;p&gt;The benchmark is designed to be run on your laptop, from which it hits Cloudflare's and Vercel's servers over the Internet. It makes the assumption that latency observed from the client is a close enough approximation of server-side CPU time. The reasons are fair: As Theo notes, Cloudflare does not permit an application to measure its own CPU time, in order to prevent timing side channel attacks. Actual CPU time can be seen in logs after the fact, but gathering those may be a lot of work. It's just easier to measure time from the client.&lt;/p&gt;
      &lt;p&gt;However, as Cloudflare and Vercel are hosted from different data centers, the network latency to each can be a factor in the benchmark, and this can skew the results. Typically, this effect will favor Cloudflare, because Cloudflare can run your Worker in locations spread across 330+ cities worldwide, and will tend to choose the closest one to you. Vercel, on the other hand, usually places compute in a central location, so latency will vary depending on your distance from that location.&lt;/p&gt;
      &lt;p&gt;For our own testing, to minimize this effect, we ran the benchmark client from a VM on AWS located in the same data center as our Vercel instances. Since Cloudflare is well-connected to every AWS location, we think this should have eliminated network latency from the picture. We chose AWS's us-east-1 / Vercel's iad1 for our test as it is widely seen as the default choice; any other choice could draw questions about cherry-picking.&lt;/p&gt;
      &lt;p&gt;Cloudflare's servers aren't all identical. Although we refresh them aggressively, there will always be multiple generations of hardware in production at any particular time. Currently, this includes generations 10, 11, and 12 of our server hardware.&lt;/p&gt;
      &lt;p&gt;Other cloud providers are no different. No cloud provider simply throws away all their old servers every time a new version becomes available.&lt;/p&gt;
      &lt;p&gt;Of course, newer CPUs run faster, even for single-threaded workloads. The differences are not as large as they used to be 20-30 years ago, but they are not nothing. As such, an application may get (a little bit) lucky or unlucky depending on what machine it is assigned to.&lt;/p&gt;
      &lt;p&gt;In cloud environments, even identical CPUs can yield different performance depending on circumstances, due to multitenancy. The server your application is assigned to is running many others as well. In AWS Lambda, a server may be running hundreds of applications; in Cloudflare, with our ultra-efficient runtime, a server may be running thousands. These "noisy neighbors" won't share the same CPU core as your app, but they may share other resources, such as memory bandwidth. As a result, performance can vary.&lt;/p&gt;
      &lt;p&gt;It's important to note that these problems create correlated noise. That is, if you run the test again, the application is likely to remain assigned to the same machines as before â this is true of both Cloudflare and Vercel. So, this noise cannot be corrected by simply running more iterations. To correct for this type of noise on Cloudflare, one would need to initiate requests from a variety of geographic locations, in order to hit different Cloudflare data centers and therefore different machines. But, that is admittedly a lot of work. (We are not familiar with how best to get an application to switch machines on Vercel.)&lt;/p&gt;
      &lt;p&gt;The Cloudflare version of the NextJS benchmark was not configured to use force-dynamic while the Vercel version was. This triggered curious behavior. Our understanding is that pages which are not "dynamic" should normally be rendered statically at build time. With OpenNext, however, it appears the pages are still rendered dynamically, but if multiple requests for the same page are received at the same time, OpenNext will only invoke the rendering once. Before we made the changes to fix our scheduling algorithm to avoid sending too many requests to the same isolate, this behavior may have somewhat counteracted that problem. Theo reports that he had disabled force-dynamic in the Cloudflare version specifically for this reason: with it on, our results were so bad as to appear outright broken, so he intentionally turned it off.&lt;/p&gt;
      &lt;p&gt;Ironically, though, once we fixed the scheduling issue, using "static" rendering (i.e. not enabling force-dynamic) hurt Cloudflare's performance for other reasons. It seems that when OpenNext renders a "cacheable" page, streaming of the response body is inhibited. This interacted poorly with a property of the benchmark client: it measured time-to-first-byte (TTFB), rather than total request/response time. When running in dynamic mode â as the test did on Vercel â the first byte would be returned to the client before the full page had been rendered. The rest of the rendering would happen as bytes streamed out. But with OpenNext in non-dynamic mode, the entire payload was rendered into a giant buffer upfront, before any bytes were returned to the client.&lt;/p&gt;
      &lt;p&gt;Due to the TTFB behavior of the benchmark client, in dynamic mode, the benchmark actually does not measure the time needed to fully render the page. We became suspicious when we noticed that Vercel's observability tools indicated more CPU time had been spent than the benchmark itself had reported.&lt;/p&gt;
      &lt;p&gt;One option would have been to change the benchmarks to use TTLB instead â that is, wait until the last byte is received before stopping the timer. However, this would make the benchmark even more affected by network differences: The responses are quite large, ranging from 2MB to 15MB, and so the results could vary depending on the bandwidth to the provider. Indeed, this would tend to favor Cloudflare, but as the point of the test is to measure CPU speed, not bandwidth, it would be an unfair advantage.&lt;/p&gt;
      &lt;p&gt;Once we changed the Cloudflare version of the test to use force-dynamic as well, matching the Vercel version, the streaming behavior then matched, making the request fair. This means that neither version is actually measuring the cost of rendering the full page to HTML, but at least they are now measuring the same thing.&lt;/p&gt;
      &lt;p&gt;As a side note, the original behavior allowed us to spot that OpenNext has a couple of performance bottlenecks in its implementation of the composable cache it uses to deduplicate rendering requests. While fixes to these aren't going to impact the numbers for this particular set of benchmarks, we're working on improving those pieces also.&lt;/p&gt;
      &lt;p&gt;The React SSR benchmark contained a more basic configuration error. React inspects the environment variable &lt;code&gt;NODE_ENV&lt;/code&gt; to decide whether the environment is "production" or a development environment. Many Node.js-based environments, including Vercel, set this variable automatically in production. Many frameworks, such as OpenNext, automatically set this variable for Workers in production as well. However, the React SSR benchmark was written against lower-level React APIs, not using any framework. In this case, the &lt;code&gt;NODE_ENV&lt;/code&gt; variable wasn't being set at all.&lt;/p&gt;
      &lt;p&gt;And, unfortunately, when &lt;code&gt;NODE_ENV&lt;/code&gt; is not set, React defaults to "dev mode", a mode that contains extra debugging checks and is therefore much slower than production mode. As a result, the numbers for Workers were much worse than they should have been.&lt;/p&gt;
      &lt;p&gt;Arguably, it may make sense for Workers to set this variable automatically for all deployed workers, particularly when Node.js compatibility is enabled. We are looking into doing this in the future, but for now we've updated the test to set it directly.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;What weâre going to do next&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Our improvements to the Workers Runtime are already live for all workers, so you do not need to change anything. Many apps will already see faster, steadier tail latency on compute heavy routes with less jitter during bursts. In places where garbage collection improved, some workloads will also use fewer billed CPU seconds.&lt;/p&gt;
      &lt;p&gt;We also sent Theo a pull request to update OpenNext with our improvements there, and with other test fixes.&lt;/p&gt;
      &lt;p&gt;But we're far from done. We still have work to do to close the gap between OpenNext and Next.js on Vercel â but given the other benchmark results, it's clear we can get there. We also have plans for further improvements to our scheduling algorithm, so that requests almost never block each other. We will continue to improve V8, and even Node.js â the Workers team employs multiple core contributors to each project. Our approach is simple: improve open source infrastructure so that everyone gets faster, then make sure our platform makes the most of those improvements.&lt;/p&gt;
      &lt;p&gt;And, obviously, we'll be writing more benchmarks, to make sure we're catching these kinds of issues ourselves in the future. If you have a benchmark that shows Workers being slower, send it to us with a repro. We will profile it, fix what we can upstream, and share back what we learn!&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45584281</guid><pubDate>Tue, 14 Oct 2025 20:17:44 +0000</pubDate></item><item><title>FSF announces Librephone project</title><link>https://www.fsf.org/news/librephone-project</link><description>&lt;doc fingerprint="e30f1910ee07a1b4"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;FSF announces Librephone project&lt;/head&gt;
    &lt;p&gt;Librephone is a new initiative by the FSF with the goal of bringing full freedom to the mobile computing environment. The vast majority of software users around the world use a mobile phone as their primary computing device. After forty years of advocacy for computing freedom, the FSF will now work to bring the right to study, change, share, and modify the programs users depend on in their daily lives to mobile phones.&lt;/p&gt;
    &lt;p&gt;"Forty years ago, when the FSF was founded, our focus was on providing an operating system people could use on desktop and server computers in freedom. Times have changed, technology has progressed, but our commitment to freedom hasn't," said Zoë Kooyman, executive director of the FSF. "A lot of work has been done in mobile phone freedom over the years that we'll be building on. The FSF is now ready to do what is necessary to bring freedom to cell phone users. Given the complexity of the devices, this work will take time, but we're used to playing the long game."&lt;/p&gt;
    &lt;p&gt;Practically, Librephone aims to close the last gaps between existing distributions of the Android operating system and software freedom. The FSF has hired experienced developer Rob Savoye (DejaGNU, Gnash, OpenStreetMap, and more) to lead the technical project. He is currently investigating the state of device firmware and binary blobs in other mobile phone freedom projects, prioritizing the free software work done by the not entirely free software mobile phone operating system LineageOS.&lt;/p&gt;
    &lt;p&gt;The initial work is funded by a donation from FSF board member John Gilmore, who explained, "I have enjoyed using a mobile phone running LineageOS with MicroG and F-Droid for years, which eliminates the spyware and control that Google embeds in standard Android phones. I later discovered that the LineageOS distribution links in significant proprietary binary modules copied from the firmware of particular phones. Rather than accept this sad situation, I looked for collaborators to reverse-engineer and replace those proprietary modules with fully free software, for at least one modern phone."&lt;/p&gt;
    &lt;p&gt;Triaging existing packages and device compatibility to find a phone with the fewest, most fixable freedom problems is the first step. From there, the FSF and Savoye aim to reverse-engineer and replace the remaining nonfree software. Librephone will serve existing developers and projects who aim to build a fully functioning and free (as in freedom) Android-compatible OS.&lt;/p&gt;
    &lt;p&gt;The FSF has been supporting earlier free software mobile phone projects such as Replicant, and is excited to launch this new effort. Gilmore added: "We were lucky to find Rob Savoye, a great engineer with decades of experience in free software, embedded systems, and project management."&lt;/p&gt;
    &lt;p&gt;When asked to comment on the project, Savoye said: "As a long-time embedded systems engineer who has worked on mobile devices for decades, I'm looking forward to this opportunity to work towards a freedom-supporting phone and help users gain control over their phone hardware."&lt;/p&gt;
    &lt;p&gt;He added: "Making fully free software for a modern commercial phone will not be quick, easy, or cheap, but our project benefits from standing on the shoulders of giants who have done most of the work. Please join us, with your efforts and/or with your donations."&lt;/p&gt;
    &lt;p&gt;Besides the campaign information at https://fsf.org/campaigns/librephone, the project will have its own website at https://librephone.fsf.org and anyone can connect using #librephone irc on irc.libera.chat.&lt;/p&gt;
    &lt;head rend="h4"&gt;About the Free Software Foundation&lt;/head&gt;
    &lt;p&gt;The FSF, founded in 1985, is dedicated to promoting computer users' right to use, study, copy, modify, and redistribute computer programs. The FSF promotes the development and use of free (as in freedom) software -- particularly the GNU operating system and its GNU/Linux variants -- and free documentation for free software. The FSF also helps to spread awareness of the ethical and political issues of freedom in the use of software, and its websites, located at https://www.fsf.org and https://www.gnu.org, are an important source of information about GNU/Linux. Donations to support the FSF's work can be made at https://donate.fsf.org. The FSF is a remote organization, incorporated in Massachusetts, US.&lt;/p&gt;
    &lt;head rend="h4"&gt;MEDIA CONTACT&lt;/head&gt;
    &lt;p&gt;Greg Farough&lt;lb/&gt; Campaigns Manager &lt;lb/&gt; Free Software Foundation &lt;lb/&gt; +1 (617) 542 5942 &lt;lb/&gt; campaigns@fsf.org&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45586339</guid><pubDate>Tue, 14 Oct 2025 23:47:08 +0000</pubDate></item><item><title>Can we know whether a profiler is accurate?</title><link>https://stefan-marr.de/2025/10/can-we-know-whether-a-profiler-is-accurate/</link><description>&lt;doc fingerprint="ae4e1bb93c224701"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Can We Know Whether a Profiler is Accurate?&lt;/head&gt;
    &lt;p&gt;If you have been following the adventures of our hero over the last couple of years, you might remember that we canât really trust sampling profilers for Java, and itâs even worse for Javaâs instrumentation-based profilers.&lt;/p&gt;
    &lt;p&gt;For sampling profilers, the so-called observer effect gets in the way: when we profile a program, the profiling itself can change the programâs performance behavior. This means we canât simply increase the sampling frequency to get a more accurate profile, because the sampling causes inaccuracies. So, how could we possibly know whether a profile correctly reflects an execution?&lt;/p&gt;
    &lt;p&gt;We could try to look at the code and estimate how long each bit takes, and then painstakingly compute what an accurate profile would be. Unfortunately, with the complexity of todayâs processors and language runtimes, this would require a cycle-accurate simulator that needs to model everything, from the processorâs pipeline, over the cache hierarchy, to memory and storage. While there are simulators that do this kind of thing, they are generally too slow to simulate a full JVM with JIT compilation for any interesting program within a practical amount of time. This means that simulation is currently impractical, and it is impractical to determine what a ground truth would be.&lt;/p&gt;
    &lt;p&gt;So, what other approaches might there be to determine whether a profile is accurate?&lt;/p&gt;
    &lt;p&gt;In 2010, Mytkowicz et al. already checked whether Java profilers were actionable by inserting computations at the Java bytecode level. On todayâs VMs, thatâs unfortunately an approach that changes performance in fairly unpredictable ways, because it interacts with the compiler optimizations. However, the idea to check whether a profiler accurately reflects the slowdown of a program is sound. For example, an inaccurate profiler is less likely to correctly identify a change in the distribution of where a program spends its time. Similarly, if we change the overall amount of time a program takes, without changing the distribution of where time is spent, it may attribute run time to the wrong parts of a program.&lt;/p&gt;
    &lt;p&gt;We can detect both of these issues by accurately slowing down a program. And, as you might know from the previous post, we are able to slow down programs fairly accurately. Figure 1 illustrates the idea with a stacked bar chart for a hypothetical distribution of run-time over three methods. This distribution should remain identical, independent of a slowdown observed by the program. So, thereâs a linear relation between the absolute time measured and a constant relation between the percentage of time per method, depending on the slowdown.&lt;/p&gt;
    &lt;p&gt;With this slowdown approach, we can detect whether the profiler is accurate with respect to the predicted time increase. Iâll leave all the technical details to the paper. We can also slow down individual basic blocks accurately to make a particular method take more time. As it turns out, this is a good litmus test for the accuracy of profilers, and we find a number of examples where they fail to attribute the run time correctly. Figure 2 shows an example for the Havlak benchmark. The bar charts show how much change the four profilers detect after we slowed down &lt;code&gt;Vector.hasSome&lt;/code&gt; to the level indicated by the red dashed line.
In this particular example, async-profiler detects the change accurately.
JFR is probably within the margin of error.
However, JProfiler and YourKit are completely off. JProfiler likely canât deal with inlining and attributes the change to the &lt;code&gt;forEach&lt;/code&gt; method that calls &lt;code&gt;hasSome&lt;/code&gt;.
YourKit does not seem to see the change at all.&lt;/p&gt;
    &lt;p&gt;With this slowdown-based approach, we finally have a way to see how accurate sampling profilers are by approximating the ground truth profile. Since we canât measure the ground truth directly, we found a way to sidestep a fundamental problem and found a reasonably practical solution.&lt;/p&gt;
    &lt;p&gt;The paper details how we implement our divining approach, i.e., how we slow down programs accurately. It also has all the methodological details, research questions, benchmarking setup, and lots more numbers, especially in the appendix. So, please give it a read, and let us know what you think.&lt;/p&gt;
    &lt;p&gt;If you happen to attend the SPLASH conference, Humphrey is presenting our work today and on Saturday.&lt;/p&gt;
    &lt;p&gt;Questions, pointers, and suggestions are always welcome, for instance, on Mastodon, BlueSky, or Twitter.&lt;/p&gt;
    &lt;p&gt;Thanks to Octave for feedback on this post.&lt;/p&gt;
    &lt;p&gt;Abstract&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Optimizing performance on top of modern runtime systems with just-in-time (JIT) compilation is a challenge for a wide range of applications from browser-based applications on mobile devices to large-scale server applications. Developers often rely on sampling-based profilers to understand where their code spends its time. Unfortunately, sampling of JIT-compiled programs can give inaccurate and sometimes unreliable results.&lt;/p&gt;
      &lt;p&gt;To assess accuracy of such profilers, we would ideally want to compare their results to a known ground truth. With the complexity of todayâs software and hardware stacks, such ground truth is unfortunately not available. Instead, we propose a novel technique to approximate a ground truth by accurately slowing down a Java program at the machine-code level, preserving its optimization and compilation decisions as well as its execution behavior on modern CPUs.&lt;/p&gt;
      &lt;p&gt;Our experiments demonstrate that we can slow down benchmarks by a specific amount, which is a challenge because of the optimizations in modern CPUs, and we verified with hardware profiling that on a basic-block level, the slowdown is accurate for blocks that dominate the execution. With the benchmarks slowed down to specific speeds, we confirmed that async-profiler, JFR, JProfiler, and YourKit maintain original performance behavior and assign the same percentage of run time to methods. Additionally, we identify cases of inaccuracy caused by missing debug information, which prevents the correct identification of the relevant source code. Finally, we tested the accuracy of sampling profilers by approximating the ground truth by the slowing down of specific basic blocks and found large differences in accuracy between the profilers.&lt;/p&gt;
      &lt;p&gt;We believe, our slowdown-based approach is the first practical methodology to assess the accuracy of sampling profilers for JIT-compiling systems and will enable further work to improve the accuracy of profilers.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Divining Profiler Accuracy: An Approach to Approximate Profiler Accuracy Through Machine Code-Level Slowdown&lt;lb/&gt;H. Burchell, S. Marr; Proceedings of the ACM on Programming Languages, OOPSLA'25, ACM, 2025.&lt;/item&gt;
      &lt;item&gt;Paper: PDF&lt;/item&gt;
      &lt;item&gt;DOI: 10.1145/3763180&lt;/item&gt;
      &lt;item&gt;Appendix: online appendix&lt;/item&gt;
      &lt;item&gt; BibTex: bibtex &lt;quote&gt;@article{Burchell:2025:Divining, abstract = {Optimizing performance on top of modern runtime systems with just-in-time (JIT) compilation is a challenge for a wide range of applications from browser-based applications on mobile devices to large-scale server applications. Developers often rely on sampling-based profilers to understand where their code spends its time. Unfortunately, sampling of JIT-compiled programs can give inaccurate and sometimes unreliable results. To assess accuracy of such profilers, we would ideally want to compare their results to a known ground truth. With the complexity of today's software and hardware stacks, such ground truth is unfortunately not available. Instead, we propose a novel technique to approximate a ground truth by accurately slowing down a Java program at the machine-code level, preserving its optimization and compilation decisions as well as its execution behavior on modern CPUs. Our experiments demonstrate that we can slow down benchmarks by a specific amount, which is a challenge because of the optimizations in modern CPUs, and we verified with hardware profiling that on a basic-block level, the slowdown is accurate for blocks that dominate the execution. With the benchmarks slowed down to specific speeds, we confirmed that async-profiler, JFR, JProfiler, and YourKit maintain original performance behavior and assign the same percentage of run time to methods. Additionally, we identify cases of inaccuracy caused by missing debug information, which prevents the correct identification of the relevant source code. Finally, we tested the accuracy of sampling profilers by approximating the ground truth by the slowing down of specific basic blocks and found large differences in accuracy between the profilers. We believe, our slowdown-based approach is the first practical methodology to assess the accuracy of sampling profilers for JIT-compiling systems and will enable further work to improve the accuracy of profilers.}, acceptancerate = {0.356}, appendix = {https://doi.org/10.5281/zenodo.16911348}, articleno = {402}, author = {Burchell, Humphrey and Marr, Stefan}, blog = {https://stefan-marr.de/2025/10/can-we-know-whether-a-profiler-is-accurate/}, doi = {10.1145/3763180}, issn = {2475-1421}, journal = {Proceedings of the ACM on Programming Languages}, keywords = {Accuracy GroundTruth Java MeMyPublication Profiling Sampling myown}, month = oct, number = {OOPSLAB25}, numpages = {32}, pdf = {https://stefan-marr.de/downloads/oopsla25-burchell-marr-divining-profiler-accuracy.pdf}, publisher = {{ACM}}, series = {OOPSLA'25}, title = {{Divining Profiler Accuracy: An Approach to Approximate Profiler Accuracy Through Machine Code-Level Slowdown}}, year = {2025}, month_numeric = {10} }&lt;/quote&gt;&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45587289</guid><pubDate>Wed, 15 Oct 2025 02:04:26 +0000</pubDate></item><item><title>Pixnapping Attack</title><link>https://www.pixnapping.com/</link><description>&lt;doc fingerprint="bd16f981309e636b"&gt;
  &lt;main&gt;
    &lt;p&gt;Pixnapping is a new class of attacks that allows a malicious Android app to stealthily leak information displayed by other Android apps or arbitrary websites. Pixnapping exploits Android APIs and a hardware side channel that affects nearly all modern Android devices. We have demonstrated Pixnapping attacks on Google and Samsung phones and end-to-end recovery of sensitive data from websites including Gmail and Google Accounts and apps including Signal, Google Authenticator, Venmo, and Google Maps. Notably, our attack against Google Authenticator allows any malicious app to steal 2FA codes in under 30 seconds while hiding the attack from the user.&lt;/p&gt;
    &lt;p&gt;The Pixnapping paper will appear in the 32nd ACM Conference on Computer and Communications Security (Taipei, Taiwan; October 13-17, 2025) with the following title:&lt;/p&gt;
    &lt;p&gt;You can download a preprint of the paper and cite it via this BibTeX citation.&lt;/p&gt;
    &lt;p&gt;The paper is the result of a collaboration between the following researchers:&lt;/p&gt;
    &lt;p&gt;We instantiated Pixnapping on five devices running Android versions 13 to 16 (up until build id BP3A.250905.014): Google Pixel 6, Google Pixel 7, Google Pixel 8, Google Pixel 9, and Samsung Galaxy S25.&lt;/p&gt;
    &lt;p&gt;We have not confirmed if Android devices from other vendors are affected by Pixnapping. However, the core mechanisms enabling the attack are typically available in all Android devices.&lt;/p&gt;
    &lt;p&gt;Any running Android app can mount this attack, even if it does not have any Android permissions (i.e., no permissions are specified in its manifest file).&lt;/p&gt;
    &lt;p&gt;Anything that is visible when the target app is opened can be stolen by the malicious app using Pixnapping. Chat messages, 2FA codes, email messages, etc. are all vulnerable since they are visible.&lt;/p&gt;
    &lt;p&gt;If an app has secret information that is not visible (e.g., it has a secret key that is stored but never shown on the screen), that information cannot be stolen by Pixnapping.&lt;/p&gt;
    &lt;p&gt;We do not know.&lt;/p&gt;
    &lt;p&gt;Make sure to install Android patches as soon as they become available.&lt;/p&gt;
    &lt;p&gt;We are not aware of mitigation strategies to protect apps against Pixnapping. If you have any insights into mitigations, please let us know and we will update this section.&lt;/p&gt;
    &lt;p&gt;The three steps a malicious app can use to mount a Pixnapping attack are:&lt;/p&gt;
    &lt;p&gt;Invoking a target app (e.g., Google Authenticator) to cause sensitive information to be submitted for rendering. This step is described in Section 3.1 of the paper.&lt;/p&gt;
    &lt;p&gt;Inducing graphical operations on individual sensitive pixels rendered by the target app (e.g., the pixels that are part of the screen region where a 2FA character is known to be rendered by Google Authenticator). This step is described in Section 3.2 of the paper.&lt;/p&gt;
    &lt;p&gt;Using a side channel (e.g., GPU.zip) to steal the pixels operated on during Step 2, one pixel at a time. This step is described in Section 3.3 of the paper.&lt;/p&gt;
    &lt;p&gt;Steps 2 and 3 are repeated for as many pixels as needed to run OCR over the recovered pixels and recover the original content. Conceptually, it is as if the malicious app was taking a screenshot of screen contents it should not have access to.&lt;/p&gt;
    &lt;p&gt;Pixnapping forces sensitive pixels into the rendering pipeline and overlays semi-transparent activities on top of those pixels via Android intents. To induce graphical operations on these pixels, our instantiations use Android’s window blur API. To measure rendering time, our instantiations use VSync callbacks. For a more detailed explanation, we refer to the paper.&lt;/p&gt;
    &lt;p&gt;Google has attempted to patch Pixnapping by limiting the number of activities an app can invoke blur on. However, we discovered a workaround to make Pixnapping work despite this patch. The workaround is still under embargo.&lt;/p&gt;
    &lt;p&gt;Pixnapping relies on the GPU.zip side channel to leak pixels.&lt;/p&gt;
    &lt;p&gt;As of October 2025, no GPU vendor has committed to patching GPU.zip.&lt;/p&gt;
    &lt;p&gt;Yes. Pixnapping is tracked under CVE-2025-48561 in the Common Vulnerabilities and Exposures (CVE) system.&lt;/p&gt;
    &lt;p&gt;Android is vulnerable to Pixnapping because it allows an app to:&lt;/p&gt;
    &lt;p&gt;We have not investigated the applicability of these properties on other platforms yet.&lt;/p&gt;
    &lt;p&gt;It is another vulnerability we discovered that an app can use to determine if any other app is installed on the phone. This information can be used to profile users. Note that unlike prior app list bypass tricks (e.g., [1] and [2]), nothing needs to be specified in the malicious app’s manifest file to exploit our app list bypass vulnerability. For a more detailed explanation, we refer to Section 3.1 of the paper.&lt;/p&gt;
    &lt;p&gt;As of October 2025, Google has not committed to patching our app list bypass vulnerability. They resolved our report as “Won’t fix (Infeasible)”.&lt;/p&gt;
    &lt;p&gt;Yes. The Pixnapping logo is free to use under a CC0 license.&lt;/p&gt;
    &lt;p&gt;We will release the source code at this link once patches become available: https://github.com/TAC-UCB/pixnapping&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45588594</guid><pubDate>Wed, 15 Oct 2025 06:05:51 +0000</pubDate></item><item><title>Just Talk to It – The No-Bs Way of Agentic Engineering</title><link>https://steipete.me/posts/just-talk-to-it</link><description>&lt;doc fingerprint="3f41187729a60ada"&gt;
  &lt;main&gt;
    &lt;p&gt;I’ve been more quiet here lately as I’m knee-deep working on my latest project. Agentic engineering has become so good that it now writes pretty much 100% of my code. And yet I see so many folks trying to solve issues and generating these elaborated charades instead of getting sh*t done.&lt;/p&gt;
    &lt;p&gt;This post partly is inspired by the conversations I had at last night’s Claude Code Anonymous in London and partly since it’s been an AI year since my last workflow update. Time for a check-in.&lt;/p&gt;
    &lt;p&gt;All of the basic ideas still apply, so I won’t mention simple things like context management again. Read my Optimal AI Workflow post for a primer.&lt;/p&gt;
    &lt;head rend="h2"&gt;Context &amp;amp; Tech-Stack&lt;/head&gt;
    &lt;p&gt;I work by myself, current project is a ~300k LOC TypeScript React app, a Chrome extension, a cli, a client app in Tauri and a mobile app in Expo. I host on vercel, a PR delivers a new version of my website in ~2 minutes to test. Everything else (apps etc) is not automated.&lt;/p&gt;
    &lt;head rend="h2"&gt;Harness &amp;amp; General Approach&lt;/head&gt;
    &lt;p&gt;I’ve completely moved to &lt;code&gt;codex&lt;/code&gt; cli as daily driver. I run between 3-8 in parallel in a 3x3 terminal grid, most of them in the same folder, some experiments go in separate folders. I experimented with worktrees, PRs but always revert back to this setup as it gets stuff done the fastest.&lt;/p&gt;
    &lt;p&gt;My agents do git atomic commits themselves. In order to maintain a mostly clean commit history, I iterated a lot on my agent file. This makes git ops sharper so each agent commits exactly the files it edited.&lt;/p&gt;
    &lt;p&gt;Yes, with claude you could do hooks and codex doesn’t support them yet, but models are incredibly clever and no hook will stop them if they are determined.&lt;/p&gt;
    &lt;p&gt;I was being ridiculed in the past and called a slop-generator, good to see that running parallel agents slowly gets mainstream.&lt;/p&gt;
    &lt;head rend="h2"&gt;Model Picker&lt;/head&gt;
    &lt;p&gt;I build pretty much everything with gpt-5-codex on mid settings. It’s a great compromise of smart &amp;amp; speed, and dials thinking up/down automatically. I found over-thinking these settings to not yield meaningful results, and it’s nice not having to think about ultrathink.&lt;/p&gt;
    &lt;head rend="h3"&gt;Blast Radius 💥&lt;/head&gt;
    &lt;p&gt;Whenever I work, I think about the “blast radius”. I didn’t come up with that term, I do love it tho. When I think of a change I have a pretty good feeling about how long it’ll take and how many files it will touch. I can throw many small bombs at my codebase or a one “Fat Man” and a few small ones. If you throw multiple large bombs, it’ll be impossible to do isolated commits, much harder to reset if sth goes wrong.&lt;/p&gt;
    &lt;p&gt;This is also a good indicator while I watch my agents. If something takes longer than I anticipated, I just hit escape and ask “what’s the status” to get a status update and then either help the model to find the right direction, abort or continue. Don’t be afraid of stopping models mid-way, file changes are atomic and they are really good at picking up where they stopped.&lt;/p&gt;
    &lt;p&gt;When I am unsure about the impact, I use “give me a few options before making changes” to gauge it.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why not worktrees?&lt;/head&gt;
    &lt;p&gt;I run one dev server, as I evolve my project I click through it and test multiple changes at once. Having a tree/branch per change would make this significantly slower, spawning multiple dev servers would quickly get annoying. I also have limitations for Twitter OAuth, so I can only register some domains for callbacks.&lt;/p&gt;
    &lt;head rend="h3"&gt;What about Claude Code?&lt;/head&gt;
    &lt;p&gt;I used to love Claude Code, these days I can’t stand it anymore (even tho codex is a fan). It’s language, the absolutely right’s, the 100% production ready messages while tests fail - I just can’t anymore. Codex is more like the introverted engineer that chugs along and just gets stuff done. It reads much more files before starting work so even small prompts usually do exactly what I want.&lt;/p&gt;
    &lt;p&gt;There’s broad consensus in my timeline that codex is the way to go.&lt;/p&gt;
    &lt;head rend="h3"&gt;Other benefits of codex&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;~230k usable context vs claude’s 156k. Yes, there’s Sonnet 1Mio if you get lucky or pay API pricing, but realistically Claude gets very silly long before it depletes that context so it’s not realistically something you can use.&lt;/item&gt;
      &lt;item&gt;More efficient token use. Idk what OpenAI does different, but my context fills up far slower than with Claude Code. I used to see Compacting… all the time when using claude, I very rarely manage to exceed the context in codex.&lt;/item&gt;
      &lt;item&gt;Message Queuing. Codex allows to queue messages. Claude had this feature, but a few months ago they changed it so your messages “steer” the model. If I want to steer codex, I just press escape and enter to send the new message. Having the option for both is just far better. I often queue related feature tasks and it just reliably works them off.&lt;/item&gt;
      &lt;item&gt;Speed OpenAI rewrote codex in Rust, and it shows. It’s incredibly fast. With Claude Code I often have multi-second freezes and it’s process blows up to gigabytes of memory. And then there’s the terminal flickering, especially when using Ghostty. Codex has none of that. It feels incredibly lightweight and fast.&lt;/item&gt;
      &lt;item&gt;Language. This really makes a difference to my mental health. I’ve been screaming at claude so many times. I rarely get angry with codex. Even if codex would be a worse model I’d use it for that fact alone. If you use both for a few weeks you will understand.&lt;/item&gt;
      &lt;item&gt;No random markdown files everywhere. IYKYK.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Why not $harness&lt;/head&gt;
    &lt;p&gt;IMO there’s simply not much space between the end user and the model company. I get by far the best deal using a subscription. I currently have 4 OpenAI subs and 1 Anthropic sub, so my overall costs are around 1k/month for basically unlimited tokens. If I’d use API calls, that’d cost my around 10x more. Don’t nail me on this math, I used some token counting tools like ccusage and it’s all somewhat imprecise, but even if it’s just 5x it’s a damn good deal.&lt;/p&gt;
    &lt;p&gt;I like that we have tools like amp or Factory, I just don’t see them surviving long-term. Both codex and claude code are getting better with every release, and they all converge to the same ideas and feature set. Some might have a temporary edge with better todo lists, steering or slight dx features, but I don’t see them significantly out-competing the big AI companies.&lt;/p&gt;
    &lt;p&gt;amp moved away from GPT-5 as driver and now calls it their “oracle”. Meanwhile I use codex and basically constantly work with the smarter model, the oracle. Yes, there are benchmarks, but given the skewed usage numbers, I don’t trust them. codex gets me far better results than amp. I have to give them kudos tho for session sharing, they push some interesting ideas ahead.&lt;/p&gt;
    &lt;p&gt;Factory, unconvinced. Their videos are a bit cringe, I do hear good things in my timeline about it tho, even if images aren’t supported (yet) and they have the signature flicker.&lt;/p&gt;
    &lt;p&gt;Cursor… it’s tab completion model is industry leading, if you still write code yourself. I use VS Code mostly, I do like them pushing things like browser automation and plan mode tho. I did experiment with GPT-5-Pro but Cursor still has the same bugs that annoyed me back in May. I hear that’s being worked on tho, so it stays in my dock.&lt;/p&gt;
    &lt;p&gt;Others like Auggie were a blip on my timeline and nobody ever mentioned them again. In the end they all wrap either GPT-5 and/or Sonnet and are replaceable. RAG might been helpful for Sonnet, but GPT-5 is so good at searching at you don’t need a separate vector index for your code.&lt;/p&gt;
    &lt;p&gt;The most promising candidates are opencode and crush, esp. in combination with open models. You can totally use your OpenAI or Anthropic sub with them as well (thanks to clever hax), but it’s questionable if that is allowed, and what’s the point of using a less capable harness for the model optimized for codex or Claude Code.&lt;/p&gt;
    &lt;head rend="h3"&gt;What about $openmodel&lt;/head&gt;
    &lt;p&gt;I keep an eye on China’s open models, and it’s impressive how quickly they catch up. GLM 4.6 and Kimi K2.1 are strong contenders that slowly reach Sonnet 3.7 quality, I don’t recommend them as daily driver tho.&lt;/p&gt;
    &lt;p&gt;The benchmarks only tell half the story. IMO agentic engineering moved from “this is crap” to “this is good” around May with the release of Sonnet 4.0, and we hit an even bigger leap from good to “this is amazing” with gpt-5-codex.&lt;/p&gt;
    &lt;head rend="h3"&gt;Plan Mode &amp;amp; Approach&lt;/head&gt;
    &lt;p&gt;What benchmarks miss is the strategy that the model+harness pursue when they get a prompt. codex is far FAR more careful and reads much more files in your repo before deciding what to do. It pushes back harder when you make a silly request. Claude/other agents are much more eager and just try something. This can be mitigated with plan mode and rigorous structure docs, to me that feels like working around a broken system.&lt;/p&gt;
    &lt;p&gt;I rarely use big plan files now with codex. codex doesn’t even have a dedicated plan mode - however it’s so much better at adhering to the prompt that I can just write “let’s discuss” or “give me options” and it will diligently wait until I approve it. No harness charade needed. Just talk to it.&lt;/p&gt;
    &lt;head rend="h3"&gt;But Claude Code now has Plugins&lt;/head&gt;
    &lt;p&gt;Do you hear that noise in the distance? It’s me sigh-ing. What a big pile of bs. This one really left me disappointed in Anthropic’s focus. They try to patch over inefficiencies in the model. Yes, maintaining good documents for specific tasks is a good idea. I keep a big list of useful docs in a docs folder as markdown.&lt;/p&gt;
    &lt;head rend="h3"&gt;But but Subagents !!!1!&lt;/head&gt;
    &lt;p&gt;But something has to be said about this whole dance with subagents. Back in May this was called subtasks, and mostly a way to spin out tasks into a separate context when the model doesn’t need the full text - mainly a way to parallelize or to reduce context waste for e.g. noisy build scripts. Later they rebranded and improved this to subagents, so you spin of a task with some instructions, nicely packaged.&lt;/p&gt;
    &lt;p&gt;The use case is the same. What others do with subagents, I usually do with separate windows. If I wanna research sth I might do that in a separate terminal pane and paste it to another one. This gives me complete control and visibility over the context I engineer, unlike subagents who make it harder to view and steer or control what is sent back.&lt;/p&gt;
    &lt;p&gt;And we have to talk about the subagent Anthropic recommends on their blog. Just look at this “AI Engineer” agent. It’s an amalgamation of slop, mentioning GPT-4o and o1 for integration, and overall just seems like an autogenerated soup of words that tries to make sense. There’s no meat in there that would make your agent a better “AI engineer”.&lt;/p&gt;
    &lt;p&gt;What does that even mean? If you want to get better output, telling your model “You are an AI engineer specializing in production-grade LLM applications” will not change that. Giving it documentation, examples and do/don’t helps. I bet that you’d get better result if you ask your agent to “google AI agent building best practices” and let it load some websites than this crap. You could even make the argument that this slop is context poison.&lt;/p&gt;
    &lt;head rend="h2"&gt;How I write prompts&lt;/head&gt;
    &lt;p&gt;Back when using claude, I used to write (ofc not, I speak) very extensive prompts, since this model “gets me” the more context I supply. While this is true with any model, I noticed that my prompts became significantly shorter with codex. Often it’s just 1-2 sentences + an image. The model is incredibly good at reading the codebase and just gets me. I even sometimes go back to typing since codex requires so much less context to understand.&lt;/p&gt;
    &lt;p&gt;Adding images is an amazing trick to provide more context, the model is really good at finding exactly what you show, it finds strings and matches it and directly arrives at the place you mention. I’d say at least 50% of my prompts contain a screenshot. I rarely annotate that, that works even better but is slower. A screenshot takes 2 seconds to drag into the terminal.&lt;/p&gt;
    &lt;p&gt;Wispr Flow with semantic correction is still king.&lt;/p&gt;
    &lt;head rend="h2"&gt;Web-Based Agents&lt;/head&gt;
    &lt;p&gt;Lately I experimented again with web agents: Devin, Cursor and Codex. Google’s Jules looks nice but was really annoying to set up and Gemini 2.5 just isn’t a good model anymore. Things might change soon once we get Gemini 3 Pro. The only one that stuck is codex web. It also is annoying to setup and broken, the terminal currently doesn’t load correctly, but I had an older version of my environment and made it work, with the price of slower wramp-up times.&lt;/p&gt;
    &lt;p&gt;I use codex web as my short-term issue tracker. Whenever I’m on the go and have an idea, I do a one-liner via the iOS app and later review this on my Mac. Sure, I could do way more with my phone and even review/merge this, but I choose not to. My work is already addictive enough as-is, so when I’m out or seeing friends, I don’t wanna be pulled in even more. Heck, I say this as someone who spent almost two months building a tool to make it easier to code on your phone.&lt;/p&gt;
    &lt;p&gt;Codex web didn’t even count towards your usage limits, but these days sadly are numbered.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Agentic Journey&lt;/head&gt;
    &lt;p&gt;Let’s talk about tools. Conductor, Terragon, Sculptor and the 1000 other ones. Some are hobby projects, some are drowning in VC money. I tried so many of them. None stick. IMO they work around current inefficiencies and promote a workflow that just isn’t optimal. Plus, most of them hide the terminal and don’t show everything the model shows.&lt;/p&gt;
    &lt;p&gt;Most are thin wrappers around Anthropic’s SDK + work tree management. There’s no moat. And I question if you even want easier access to coding agents on your phone. The little use case these did for me, codex web fully covers.&lt;/p&gt;
    &lt;p&gt;I do see this pattern tho that almost every engineer goes through a phase of building their own tools, mostly because it’s fun and because it’s so much easier now. And what else to build than tools that (we think) will make it simpler to build more tools?&lt;/p&gt;
    &lt;head rend="h2"&gt;But Claude Code can Background Tasks!&lt;/head&gt;
    &lt;p&gt;True. codex currently lacks a few bells and whistles that claude has. The most painful omission is background task management. While it should have a timeout, I did see it get stuck quite a few times with cli tasks that don’t end, like spinning up a dev server or tests that deadlock.&lt;/p&gt;
    &lt;p&gt;This was one of the reasons I reverted back to claude, but since that model is just so silly in other ways, I now use &lt;code&gt;tmux&lt;/code&gt;. It’s an old tool to run CLIs in persistent sessions in the background and there’s plenty world knowledge in the model, so all you need to do is “run via tmux”. No custom agent md charade needed.&lt;/p&gt;
    &lt;head rend="h2"&gt;What about MCPs&lt;/head&gt;
    &lt;p&gt;Other people wrote plenty about MCPs. IMO most are something for the marketing department to make a checkbox and be proud. Almost all MCPs really should be clis. I say that as someone who wrote 5 MCPs myself.&lt;/p&gt;
    &lt;p&gt;I can just refer to a cli by name. I don’t need any explanation in my agents file. The agent will try $randomcrap on the first call, the cli will present the help menu, context now has full info how this works and from now on we good. I don’t have to pay a price for any tools, unlike MCPs which are a constant cost and garbage in my context. Use GitHub’s MCP and see 23k tokens gone. Heck, they did make it better because it was almost 50.000 tokens when it first launched. Or use the &lt;code&gt;gh&lt;/code&gt; cli which has basically the same feature set, models already know how to use it, and pay zero context tax.&lt;/p&gt;
    &lt;p&gt;I did open source some of my cli tools, like bslog and inngest.&lt;/p&gt;
    &lt;p&gt;I do use &lt;code&gt;chrome-devtools-mcp&lt;/code&gt; these days to close the loop. it replaced Playwright as my to-go MCP for web debugging. I don’t need it lots but when I do, it’s quite useful to close the loop. I designed my website so that I can create api keys that allow my model to query any endpoint via curl, which is faster and more token-efficient in almost all use cases, so even that MCP isn’t something I need daily.&lt;/p&gt;
    &lt;head rend="h2"&gt;But the code is slop!&lt;/head&gt;
    &lt;p&gt;I spend about 20% of my time on refactoring. Ofc all of that is done by agents, I don’t waste my time doing that manually. Refactor days are great when I need less focus or I’m tired, since I can make great progress without the need of too much focus or clear thinking.&lt;/p&gt;
    &lt;p&gt;Typical refactor work is using &lt;code&gt;jscpd&lt;/code&gt; for code duplication, &lt;code&gt;knip&lt;/code&gt; for dead code, running &lt;code&gt;eslint&lt;/code&gt;’s &lt;code&gt;react-compiler&lt;/code&gt; and deprecation plugins, checking if we introduced api routes that can be consolidated, maintaining my docs, breaking apart files that grew too large, adding tests and code comments for tricky parts, updating dependencies, tool upgrades, file restructuring, finding and rewriting slow tests, mentioning modern react patterns and rewriting code (e.g. you might not need &lt;code&gt;useEffect&lt;/code&gt;). There’s always something to do.&lt;/p&gt;
    &lt;p&gt;You could make the argument that this could be done on each commit, I do find these phases of iterating fast and then maintaining and improving the codebase - basically paying back some technical debt, to be far more productive, and overall far more fun.&lt;/p&gt;
    &lt;head rend="h2"&gt;Do you do spec-driven development?&lt;/head&gt;
    &lt;p&gt;I used to back in June. Designing a big spec, then let the model build it, ideally for hours. IMO that’s the old way of thinking about building software.&lt;/p&gt;
    &lt;p&gt;My current approach is usually that I start a discussion with codex, I paste in some websites, some ideas, ask it to read code, and we flesh out a new feature together. If it’s something tricky, I ask it to write everything into a spec, give that to GPT-5-Pro for review (via chatgpt.com) to see if it has better ideas (surprisingly often, this greatly improves my plan!) and then paste back what I think is useful into the main context to update the file.&lt;/p&gt;
    &lt;p&gt;By now I have a good feeling which tasks take how much context, and codex’s context space is quite good, so often I’ll just start building. Some people are religious and always use a new context with the plan - IMO that was useful for Sonnet, but GPT-5 is far better at dealing with larger contexts, and doing that would easily add 10 minutes to everything as the model has to slowly fetch all files needed to build the feature again.&lt;/p&gt;
    &lt;p&gt;The far more fun approach is when I do UI-based work. I often start with sth simple and woefully under-spec my requests, and watch the model build and see the browser update in real time. Then I queue in additional changes and iterate on the feature. Often I don’t fully know how something should look like, and that way I can play with the idea and iterate and see it slowly come to life. I often saw codex build something interesting I didn’t even think of. I don’t reset, I simply iterate and morph the chaos into the shape that feels right.&lt;/p&gt;
    &lt;p&gt;Often I get ideas for related interactions and iterate on other parts as well while I build it, that work I do in a different agent. Usually I work on one main feature and some smaller, tangentially related tasks.&lt;/p&gt;
    &lt;p&gt;As I’m writing this, I build a new Twitter data importer in my Chrome extension, and for that I reshape the graphql importer. Since I’m a bit unsure if that is the right approach, that one is in a separate folder so I can look at the PR and see if that approach makes sense. The main repo does refactoring, so I can focus on writing this article.&lt;/p&gt;
    &lt;head rend="h2"&gt;Show me your slash commands!&lt;/head&gt;
    &lt;p&gt;I only have a few, and I use them rarely:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;/commit&lt;/code&gt;(custom text to explain that multiple agents work in the same folder and to only commit your changes, so I get clean comments and gpt doesn’t freak out about other changes and tries to revert things if linter fails)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;/automerge&lt;/code&gt;(process one PR at a time, react to bot comments, reply, get CI green and squash when green)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;/massageprs&lt;/code&gt;(same as automerge but without the squashing so I can parallelize the process if I have a lot of PRs)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;/review&lt;/code&gt;(built-in, only sometimes since I have review bots on GH, but can be useful)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And even with these, usually I just type “commit”, unless I know that there’s far too many dirty files and the agent might mess up without some guidance. No need for charade/context waste when I’m confident that this is enough. Again, you develop intuition for these. I have yet to see other commands that really are useful.&lt;/p&gt;
    &lt;head rend="h2"&gt;What other tricks do you have?&lt;/head&gt;
    &lt;p&gt;Instead of trying to formulate the perfect prompt to motivate the agent to continue on a long-running task, there’s lazy workarounds. If you do a bigger refactor, codex often stops with a mid-work reply. Queue up continue messages if you wanna go away and just see it done. If codex is done and gets more messages, it happily ignores them.&lt;/p&gt;
    &lt;p&gt;Ask the model to write tests after each feature/fix is done. Use the same context. This will lead to far better tests, and likely uncover a bug in your implementation. If it’s purely a UI tweak, tests likely make less sense, but for anything else, do it. AI generally is bad at writing good tests, it’s still helpful tho, and let’s be honest - are you writing tests for every fix you make?&lt;/p&gt;
    &lt;p&gt;Ask the model to preserve your intent and “add code comments on tricky parts” helps both you and future model runs.&lt;/p&gt;
    &lt;p&gt;When things get hard, prompting and adding some trigger words like “take your time” “comprehensive” “read all code that could be related” “create possible hypothesis” makes codex solve even the trickiest problems.&lt;/p&gt;
    &lt;head rend="h2"&gt;How does your Agents/Claude file look like?&lt;/head&gt;
    &lt;p&gt;I have an Agents.md file with a symlink to claude.md, since Anthropic decided not to standardize. I recognize that this is difficult and sub-optimal, since GPT-5 prefers quite different prompting than Claude. Stop here and read their prompting guide if you haven’t yet.&lt;/p&gt;
    &lt;p&gt;While Claude reacts well to 🚨 SCREAMING ALL-CAPS 🚨 commands that threaten it that it will imply ultimate failure and 100 kittens will die if it runs command X, that freaks out GPT-5. (Rightfully so). So drop all of that and just use words like a human. That also means that these files can’t optimally be shared. Which isn’t a problem to me since I mostly use codex, and accept that the instructions might be too weak for the rare instances where claude gets to play.&lt;/p&gt;
    &lt;p&gt;My Agent file is currently ~800 lines long and feels like a collection of organizational scar tissue. I didn’t write it, codex did, and anytime sth happens I ask it to make a concise note in there. I should clean this up at some point, but despite it being large it works incredibly well, and gpt really mostly honors entries there. At least it does far far more often than Claude ever did. (Sonnet 4.5 got better there, to give them some credit)&lt;/p&gt;
    &lt;p&gt;Next to git instruction it contains an explanation about my product, common naming and API patterns I prefer, notes about React Compiler - often it’s things that are newer than world knowledge because my tech stack is quite bleeding edge. I expect that I can again reduce things in there with model updates. For example, Sonnet 4.0 really needed guidance to understand Tailwind 4, Sonnet 4.5 and GPT-5 are newer and know about that version so I was able to delete all that fluff.&lt;/p&gt;
    &lt;p&gt;Significant blocks are about which React patterns I prefer, database migration management, testing, using and writing ast-grep rules. (If you don’t know or don’t use ast-grep as codebase linter, stop here and ask your model to set this up as a git hook to block commits)&lt;/p&gt;
    &lt;p&gt;I also experimented and started using a text-based “design system” for how things should look, the verdict is still out on that one.&lt;/p&gt;
    &lt;head rend="h2"&gt;So GPT-5-Codex is perfect?&lt;/head&gt;
    &lt;p&gt;Absolutely not. Sometimes it refactors for half an hour and then panics and reverts everything, and you need to re-run and soothen it like a child to tell it that it has enough time. Sometimes it forgets that it can do bash commands and it requires some encouragement. Sometimes it replies in russian or korean. Sometimes the monster slips and sends raw thinking to bash. But overall these are quite rare and it’s just so insanely good in almost everything else that I can look past these flaws. Humans aren’t perfect either.&lt;/p&gt;
    &lt;p&gt;My biggest annoyance with codex is that it “loses” lines, so scrolling up quickly makes parts of the text disappear. I really hope this is on top of OpenAI’s bug roster, as it’s the main reason I sometimes have to slow down, so messages don’t disappear.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Don’t waste your time on stuff like RAG, subagents, Agents 2.0 or other things that are mostly just charade. Just talk to it. Play with it. Develop intuition. The more you work with agents, the better your results will be.&lt;/p&gt;
    &lt;p&gt;Simon Willison’s article makes an excellent point - many of the skills needed to manage agents are similar to what you need when managing engineers - almost all of these are characteristics of senior software engineers.&lt;/p&gt;
    &lt;p&gt;And yes, writing good software is still hard. Just because I don’t write the code anymore doesn’t mean I don’t think hard about architecture, system design, dependencies, features or how to delight users. Using AI simply means that expectations what to ship went up.&lt;/p&gt;
    &lt;p&gt;PS: This post is 100% organic and hand-written. I love AI, I also recognize that some things are just better done the old-fashioned way. Keep the typos, keep my voice. 🚄✌️&lt;/p&gt;
    &lt;p&gt;PPS: Credit for the header graphic goes to Thorsten Ball.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45588689</guid><pubDate>Wed, 15 Oct 2025 06:21:04 +0000</pubDate></item><item><title>Show HN: Firm, a text-based work management system</title><link>https://github.com/42futures/firm</link><description>&lt;doc fingerprint="3a9ba6182815e7b8"&gt;
  &lt;main&gt;
    &lt;p&gt;A text-based work management system for technologists.&lt;/p&gt;
    &lt;p&gt;Modern businesses are natively digital, but lack a unified view. Your data is scattered across SaaS tools you don't control, so you piece together answers by jumping between platforms.&lt;/p&gt;
    &lt;p&gt;Your business is a graph: customers link to projects, projects link to tasks, people link to organizations. Firm lets you define these relationships in plain text files (you own!).&lt;/p&gt;
    &lt;p&gt;Version controlled, locally stored and structured as code with the Firm DSL. This structured representation of your work, business-as-code, makes your business readable to yourself and to the robots that help you run it.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Everything in one place: Organizations, contacts, projects, and how they relate.&lt;/item&gt;
      &lt;item&gt;Own your data: Plain text files and tooling that runs on your machine.&lt;/item&gt;
      &lt;item&gt;Open data model: Tailor to your business with custom schemas.&lt;/item&gt;
      &lt;item&gt;Automate anything: Search, report, integrate, whatever. It's just code.&lt;/item&gt;
      &lt;item&gt;AI-ready: LLMs can read, write, and query your business structure.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The Firm CLI is available to download via Github Releases. Install scripts are provided for desktop platforms to make that process easy.&lt;/p&gt;
    &lt;code&gt;curl -fsSL https://raw.githubusercontent.com/42futures/firm/main/install.sh | sudo bash&lt;/code&gt;
    &lt;code&gt;irm https://raw.githubusercontent.com/42futures/firm/main/install.ps1 | iex&lt;/code&gt;
    &lt;p&gt;Firm operates on a "workspace": a directory containing all your &lt;code&gt;.firm&lt;/code&gt; DSL files. The Firm CLI processes every file in this workspace to build a unified, queryable graph of your business.&lt;/p&gt;
    &lt;p&gt;The first step is to add an entity to your workspace. You can do this either by using the CLI or by writing the DSL yourself.&lt;/p&gt;
    &lt;p&gt;Use &lt;code&gt;firm add&lt;/code&gt; to interactively generate new entities. Out of the box, Firm supports a set of pre-built entity schemas for org mapping, customer relations and work management. The CLI will prompt you for the necessary info and generate corresponding DSL.&lt;/p&gt;
    &lt;code&gt;$ firm add&lt;/code&gt;
    &lt;code&gt;Adding new entity

&amp;gt; Type: organization
&amp;gt; ID: megacorp
&amp;gt; Name: Megacorp Ltd.
&amp;gt; Email: mega@corp.com
&amp;gt; Urls: ["corp.com"]

Writing generated DSL to file my_workspace/generated/organization.firm
&lt;/code&gt;
    &lt;p&gt;Alternatively, you can create a &lt;code&gt;.firm&lt;/code&gt; file and write the DSL yourself.&lt;/p&gt;
    &lt;code&gt;organization megacorp {
  name = "Megacorp Ltd."
  email = "mega@corp.com"
  urls = ["corp.com"]
}
&lt;/code&gt;
    &lt;p&gt;Both of these methods achieve the same result: a new entity defined in your Firm workspace.&lt;/p&gt;
    &lt;p&gt;Once you have entities in your workspace, you can query them using the CLI.&lt;/p&gt;
    &lt;p&gt;Use &lt;code&gt;firm list&lt;/code&gt; to see all entities of a specific type.&lt;/p&gt;
    &lt;code&gt;$ firm list task&lt;/code&gt;
    &lt;code&gt;Found 7 entities with type 'task'

ID: task.design_homepage
Name: Design new homepage
Is completed: false
Assignee ref: person.jane_doe

...
&lt;/code&gt;
    &lt;p&gt;To view the full details of a single entity, use &lt;code&gt;firm get&lt;/code&gt; followed by the entity's type and ID.&lt;/p&gt;
    &lt;code&gt;$ firm get person john_doe&lt;/code&gt;
    &lt;code&gt;Found 'person' entity with ID 'john_doe'

ID: person.john_doe
Name: John Doe
Email: john@doe.com
&lt;/code&gt;
    &lt;p&gt;The power of Firm lies in its ability to travel a graph of your business. Use &lt;code&gt;firm related&lt;/code&gt; to explore connections to/from any entity.&lt;/p&gt;
    &lt;code&gt;$ firm related contact john_doe&lt;/code&gt;
    &lt;code&gt;Found 1 relationships for 'contact' entity with ID 'john_doe'

ID: interaction.megacorp_intro
Type: Call
Subject: Initial discussion about Project X
Interaction date: 2025-09-30 09:45:00 +02:00
Initiator ref: person.jane_smith
Primary contact ref: contact.john_doe
&lt;/code&gt;
    &lt;p&gt;You've seen the basic commands for interacting with a Firm workspace. The project is a work-in-progress, and you can expect to see more sophisticated features added over time, including a more powerful query engine and tools for running business workflows directly from the CLI.&lt;/p&gt;
    &lt;p&gt;Beyond the CLI, you can integrate Firm's core logic directly into your own software using the &lt;code&gt;firm_core&lt;/code&gt; and &lt;code&gt;firm_lang&lt;/code&gt; Rust packages. This allows you to build more powerful automations and integrations on top of Firm.&lt;/p&gt;
    &lt;p&gt;First, add the Firm crates to your &lt;code&gt;Cargo.toml&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;[dependencies]
firm_core = { git = "https://github.com/42futures/firm.git" }
firm_lang = { git = "https://github.com/42futures/firm.git" }&lt;/code&gt;
    &lt;p&gt;You can then load a workspace, build the entity graph, and query it programmatically:&lt;/p&gt;
    &lt;code&gt;use firm_lang::workspace::Workspace;
use firm_core::EntityGraph;

// Load workspace from a directory
let mut workspace = Workspace::new();
workspace.load_directory("./my_workspace")?;
let build = workspace.build()?;

// Build the graph from the workspace entities
let mut graph = EntityGraph::new();
graph.add_entities(build.entities)?;
graph.build();

// Query the graph for a specific entity
let lead = graph.get_entity(&amp;amp;EntityId::new("lead.ai_validation_project"))?;

// Traverse a relationship to another entity
let contact_ref = lead.get_field(FieldId::new("contact_ref"))?;
let contact = contact_ref.resolve_entity_reference(&amp;amp;graph)?;&lt;/code&gt;
    &lt;p&gt;This gives you full access to the underlying data structures, providing a foundation for building custom business automations.&lt;/p&gt;
    &lt;p&gt;Firm is organized as a Rust workspace with three crates:&lt;/p&gt;
    &lt;p&gt;Core data structures and graph operations.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Entity data model&lt;/item&gt;
      &lt;item&gt;Typed fields with references&lt;/item&gt;
      &lt;item&gt;Relationship graph with query capabilities&lt;/item&gt;
      &lt;item&gt;Entity schemas and validation&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;DSL parsing and generation.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tree-sitter-based parser for &lt;code&gt;.firm&lt;/code&gt;files&lt;/item&gt;
      &lt;item&gt;Conversion between DSL and entities&lt;/item&gt;
      &lt;item&gt;Workspace support for multi-file projects&lt;/item&gt;
      &lt;item&gt;DSL generation from entities&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Grammar is defined in tree-sitter-firm.&lt;/p&gt;
    &lt;p&gt;Command-line interface, making the Firm workspace interactive.&lt;/p&gt;
    &lt;p&gt;Firm's data model is built on a few key concepts. Each concept is accessible declaratively through the &lt;code&gt;.firm&lt;/code&gt; DSL for human-readable definitions, and programmatically through the Rust packages for building your own automations.&lt;/p&gt;
    &lt;p&gt;Entities are the fundamental business objects in your workspace, like people, organizations, or projects. Each entity has a unique ID, a type, and a collection of fields.&lt;/p&gt;
    &lt;p&gt;In the DSL, you define an entity with its type and ID, followed by its fields in a block:&lt;/p&gt;
    &lt;code&gt;person john_doe {
    name = "John Doe"
    email = "john@doe.com"
}
&lt;/code&gt;
    &lt;p&gt;In Rust, this corresponds to an &lt;code&gt;Entity&lt;/code&gt; struct:&lt;/p&gt;
    &lt;code&gt;let person = Entity::new(EntityId::new("john_doe"), EntityType::new("person"))
    .with_field(FieldId::new("name"), "John Doe")
    .with_field(FieldId::new("email"), "john@doe.com");&lt;/code&gt;
    &lt;p&gt;Fields are typed key-value pairs attached to an entity. Firm supports a rich set of types:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;String&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;Integer&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;Float&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;Boolean&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;Currency&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;DateTime&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;code&gt;List&lt;/code&gt;of other values&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Reference&lt;/code&gt;to other fields or entities&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Path&lt;/code&gt;to a local file&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In the DSL, the syntax maps directly to these types:&lt;/p&gt;
    &lt;code&gt;my_task design_homepage {
    title = "Design new homepage"        // String
    priority = 1                         // Integer
    completed = false                    // Boolean
    budget = 5000.00 USD                 // Currency
    due_date = 2024-12-01 at 17:00 UTC   // DateTime
    tags = ["ui", "ux"]                  // List
    assignee = person.jane_doe           // Reference
    deliverable = path"./homepage.zip"   // Path
}
&lt;/code&gt;
    &lt;p&gt;In Rust, these are represented by the &lt;code&gt;FieldValue&lt;/code&gt; enum:&lt;/p&gt;
    &lt;code&gt;let value = FieldValue::Integer(42);&lt;/code&gt;
    &lt;p&gt;The power of Firm comes from connecting entities. You create relationships using &lt;code&gt;Reference&lt;/code&gt; fields.&lt;/p&gt;
    &lt;p&gt;When Firm processes your workspace, it builds the entity graph representing of all your entities (as nodes) and their relationships (as directed edges). This graph is what allows for traversal and querying.&lt;/p&gt;
    &lt;p&gt;In the DSL, creating a relationship is as simple as referencing another entity's ID.&lt;/p&gt;
    &lt;code&gt;contact john_at_acme {
    person_ref = person.john_doe
    organization_ref = organization.acme_corp
}
&lt;/code&gt;
    &lt;p&gt;In Rust, you build the graph by loading entities and calling the &lt;code&gt;.build()&lt;/code&gt; method, which resolves all references into queryable links.&lt;/p&gt;
    &lt;code&gt;let mut graph = EntityGraph::new();
graph.add_entities(workspace.build()?.entities)?;
graph.build(); // Builds relationships from references

// Now you can traverse the graph
let contact = graph.get_entity(&amp;amp;EntityId::new("contact.john_at_acme"))?;
let person_ref = contact.get_field(FieldId::new("person_ref"))?;
let person = person_ref.resolve_entity_reference(&amp;amp;graph)?;&lt;/code&gt;
    &lt;p&gt;Schemas allow you to define and enforce a structure for your entities, ensuring data consistency. You can specify which fields are required or optional and what their types should be.&lt;/p&gt;
    &lt;p&gt;In the DSL, you can define a schema that other entities can adhere to:&lt;/p&gt;
    &lt;code&gt;schema custom_project {
    field {
        name = "title"
        type = "string"
        required = true
    }
    field {
        name = "budget"
        type = "currency"
        required = false
    }
}

custom_project my_project {
    title  = "My custom project"
    budget = 42000 EUR
}
&lt;/code&gt;
    &lt;p&gt;In Rust, you can define schemas programmatically to validate entities.&lt;/p&gt;
    &lt;code&gt;let schema = EntitySchema::new(EntityType::new("project"))
    .with_required_field(FieldId::new("title"), FieldType::String)
    .with_optional_field(FieldId::new("budget"), FieldType::Currency);

schema.validate(&amp;amp;some_project_entity)?;&lt;/code&gt;
    &lt;p&gt;Firm includes schemas for a range of built-in entities like Person, Organization, and Industry.&lt;/p&gt;
    &lt;p&gt;Firm's entity taxonomy is built on the REA model (Resources, Events, Agents) with inspiration from Schema.org, designed for flexible composition and efficient queries.&lt;/p&gt;
    &lt;p&gt;Every entity maps to a Resource (thing with value), an Event (thing that happens), or an Agent (thing that acts).&lt;/p&gt;
    &lt;p&gt;We separate objective reality from business relationships:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Fundamental entities represent things that exist independently (&lt;code&gt;Person&lt;/code&gt;,&lt;code&gt;Organization&lt;/code&gt;,&lt;code&gt;Document&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Contextual entities represent your business relationships and processes (&lt;code&gt;Contact&lt;/code&gt;,&lt;code&gt;Lead&lt;/code&gt;,&lt;code&gt;Project&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Entities reference each other rather than extending. One &lt;code&gt;Person&lt;/code&gt; can be referenced by multiple &lt;code&gt;Contact&lt;/code&gt;, &lt;code&gt;Employee&lt;/code&gt;, and &lt;code&gt;Partner&lt;/code&gt; entities simultaneously.&lt;/p&gt;
    &lt;p&gt;When the entity graph is built, all &lt;code&gt;Reference&lt;/code&gt; values automatically create directed edges between entities. This enables traversal queries like "find all Tasks for Opportunities whose Contacts work at Organization X" without complex joins.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45588959</guid><pubDate>Wed, 15 Oct 2025 07:01:53 +0000</pubDate></item><item><title>Britain has wasted £1,112,293,718 switching off wind turbines in 2025</title><link>https://wastedwind.energy/</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=45590236</guid><pubDate>Wed, 15 Oct 2025 10:10:20 +0000</pubDate></item><item><title>CVE-2025-55315: Asp.net Security Feature Bypass Vulnerability [9.9 Critical]</title><link>https://nvd.nist.gov/vuln/detail/CVE-2025-55315</link><description>&lt;doc fingerprint="eeb0cb52b8bf2f47"&gt;
  &lt;main&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;&lt;head&gt;CVE-2025-55315 Detail&lt;/head&gt;&lt;p&gt; Awaiting Analysis &lt;/p&gt;&lt;p&gt;This CVE record has been marked for NVD enrichment efforts.&lt;/p&gt;&lt;head&gt;Description&lt;/head&gt;&lt;p&gt;Inconsistent interpretation of http requests ('http request/response smuggling') in ASP.NET Core allows an authorized attacker to bypass a security feature over a network.&lt;/p&gt;&lt;head&gt;Metrics&lt;/head&gt;&lt;p&gt; NVD enrichment efforts reference publicly available information to associate vector strings. CVSS information contributed by other sources is also displayed. &lt;/p&gt;&lt;p&gt; CVSS 4.0 Severity and Vector Strings: &lt;/p&gt;&lt;head&gt;References to Advisories, Solutions, and Tools&lt;/head&gt;&lt;p&gt;By selecting these links, you will be leaving NIST webspace. We have provided these links to other web sites because they may have information that would be of interest to you. No inferences should be drawn on account of other sites being referenced, or not, from this page. There may be other web sites that are more appropriate for your purpose. NIST does not necessarily endorse the views expressed, or concur with the facts presented on these sites. Further, NIST does not endorse any commercial products that may be mentioned on these sites. Please address comments about this page to [email protected].&lt;/p&gt;&lt;head&gt;Weakness Enumeration&lt;/head&gt;&lt;head&gt;Quick Info&lt;/head&gt;CVE Dictionary Entry:&lt;p&gt;CVE-2025-55315&lt;/p&gt;&lt;p&gt;NVD Published Date:&lt;/p&gt;&lt;p&gt;10/14/2025&lt;/p&gt;&lt;p&gt;NVD Last Modified:&lt;/p&gt;&lt;p&gt;10/14/2025&lt;/p&gt;&lt;p&gt;Source:&lt;/p&gt;&lt;p&gt;Microsoft Corporation&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45590302</guid><pubDate>Wed, 15 Oct 2025 10:19:21 +0000</pubDate></item><item><title>I analyzed 200 e-commerce sites and found 73% of their traffic is fake</title><link>https://joindatacops.com/resources/how-73-of-your-e-commerce-visitors-could-be-fake</link><description>&lt;doc fingerprint="acf3d92879304fe2"&gt;
  &lt;main&gt;
    &lt;p&gt;Make confident, data-driven decisions with actionable ad spend insights.&lt;/p&gt;
    &lt;p&gt;9 min read&lt;/p&gt;
    &lt;p&gt;A conversion rate of less than 0.1%. That was the moment I realized something was fundamentally broken with the way we measure success on the internet.&lt;/p&gt;
    &lt;p&gt;Simul Sarker&lt;/p&gt;
    &lt;p&gt;CEO of DataCops&lt;/p&gt;
    &lt;p&gt;Last Updated&lt;/p&gt;
    &lt;p&gt;October 15, 2025&lt;/p&gt;
    &lt;p&gt;It all started with a simple, devastating problem. My client’s e-commerce website registered 50,000 visitors last February but made only 47 sales. A conversion rate of less than 0.1%. That was the moment I realized something was fundamentally broken with the way we measure success on the internet.&lt;/p&gt;
    &lt;p&gt;As the head of a digital marketing agency, I am no stranger to confusing analytics. But this was different. An e-commerce client approached me last April, completely bewildered. They were pouring $4,000 a month into Facebook ads, their Google Analytics reports were glowing with green arrows pointing up, yet their business was barely breaking even. The numbers told a story of booming growth, but the bank account told a story of stagnation.&lt;/p&gt;
    &lt;p&gt;My first thought was blunt. "Maybe your products are the problem?" I suggested, half-jokingly. They did not appreciate the feedback.&lt;/p&gt;
    &lt;p&gt;But then I dove deep into their website traffic data, and a strange, unsettling feeling crept in. It was like walking into your own home and sensing that something is out of place, even if you cannot immediately identify what has moved. I should have probably left it alone.&lt;/p&gt;
    &lt;p&gt;Instead, I went down a rabbit hole that would change how I view the entire digital economy.&lt;/p&gt;
    &lt;p&gt;Driven by this discrepancy, I built a simple tracking script. It was not a sophisticated piece of software, just a tool designed to observe how "users" actually interacted with the website. I was not just counting clicks; I was watching behavior.&lt;/p&gt;
    &lt;p&gt;In short, I was looking for the small, imperfect, and unpredictable actions that separate a real human from a bot pretending to be one. With the client's permission, I installed the script. Within a single week, the results were both clarifying and horrifying.&lt;/p&gt;
    &lt;p&gt;A staggering 68% of their website traffic was non-human traffic. This was not the obvious spam that gets filtered out. This was sophisticated bot traffic designed to fool standard analytics platforms.&lt;/p&gt;
    &lt;p&gt;This discovery became an obsession. I started reaching out to other e-commerce owners in private marketing forums and Discord groups. I posed a simple question: "Do your traffic numbers seem weirdly disconnected from your sales?"&lt;/p&gt;
    &lt;p&gt;The response was a deluge. A flood of messages came in, all echoing the same anxious sentiment: "I thought it was just me."&lt;/p&gt;
    &lt;p&gt;Over the next six months, I received permission to install my tracking script on over 200 websites, mostly small to medium sized e-commerce businesses. The results were consistent and shocking. Across this diverse sample, the average level of fake traffic was 73%. This was a systemic issue, a phantom epidemic haunting the digital storefronts of countless entrepreneurs.&lt;/p&gt;
    &lt;p&gt;The bots operating today are disturbingly good. They are not just hitting your site and leaving; they are programmed to mimic engagement, making your marketing ROI calculations dangerously inaccurate. I began to categorize them.&lt;/p&gt;
    &lt;p&gt;These bots are designed to make analytics reports look good. They perform actions that signal a "quality visitor." They scroll down pages, hover their cursors over products, and click on different internal links. But their perfection is their fatal flaw. A human might spend 15 seconds on a product description, or they might spend two minutes. These bots spent between 11 and 13 seconds on every single one. Their scrolling speed was a perfectly constant 3.2 pages per second. Humans are messy; these bots were clinically precise.&lt;/p&gt;
    &lt;p&gt;One of the most bizarre patterns I witnessed was a bot that would add the same $47 item to the shopping cart, wait exactly four minutes, and then abandon it. It repeated this exact process 30 times a day from different IP addresses and user sessions. Why? The purpose is likely to manipulate e-commerce metrics, perhaps to influence a site's internal recommendation algorithms or to make cart abandonment rates look normal amidst a sea of other non-purchasing bots.&lt;/p&gt;
    &lt;p&gt;Your analytics might proudly report a visitor from Instagram or TikTok. However, my investigation revealed that approximately 64% of this referral traffic would land on a page, wait exactly 1.8 seconds without any scrolling or clicking, and then bounce. This still registers as a "visitor from social media," a vanity metric that deceives marketers trying to measure the effectiveness of their campaigns. It is a key component of ad fraud, allowing sellers of fake engagement to "prove" they sent traffic.&lt;/p&gt;
    &lt;p&gt;During my investigation, a source from the e-commerce data industry provided a crucial piece of the puzzle. He explained that his former company was responsible for scraping 70 million retailer web pages every single day. This is a legitimate and massive source of automated traffic.&lt;/p&gt;
    &lt;p&gt;Why do they do this? For vital business intelligence. Major retailers like Amazon do not always notify vendors when they run out of stock. So, brands pay for data scraping services to monitor their own products. These "good bots" check inventory levels, see who is winning the "buy box," ensure product descriptions are correct, and track search result rankings. They even scrape from different locations and mobile device profiles to analyze what banner ads are being shown to different audiences.&lt;/p&gt;
    &lt;p&gt;This confirms that a massive portion of the web is automated. A recent Kurzgesagt video even stated that nearly 50% of all internet traffic is now bots. While some of this is for legitimate competitive analysis and price monitoring, a huge portion is the fraudulent traffic that is draining advertising budgets worldwide.&lt;/p&gt;
    &lt;p&gt;The financial implications of this phantom traffic are staggering. I had one client spending $12,000 per month on Google Ads. After we implemented advanced bot traffic detection and filtering, their reported traffic plummeted by 71%. Their CFO was initially horrified.&lt;/p&gt;
    &lt;p&gt;But then the sales report came in. Their actual sales went up by 34%.&lt;/p&gt;
    &lt;p&gt;Their real conversion rate optimization (CRO) efforts had been working all along, but the results were buried under an avalanche of fake clicks. They were not bad at marketing; they were just spending thousands of dollars advertising to robots programmed never to buy anything. Their marketing ROI went from "terrible" to "excellent" overnight.&lt;/p&gt;
    &lt;p&gt;When I tried to bring this up with a few major ad platforms, the conversation always followed a predictable script. The sales reps were incredibly friendly until I mentioned click fraud or bot traffic. Then, the tone shifted instantly to corporate-speak: "Our AI detection is industry leading" and "We take ad fraud very seriously." It was a polite but firm wall, a clear signal to stop asking questions.&lt;/p&gt;
    &lt;p&gt;One rep I had known for years finally admitted the truth off the record. "Dude, we know," he said. "Everyone knows. But if we filtered it all out properly, our revenue would drop 40% overnight, and investors would have a meltdown."&lt;/p&gt;
    &lt;p&gt;The conflict of interest is immense. Ad platforms get paid per click or impression, regardless of whether that click comes from a potential customer or a server in a click farm.&lt;/p&gt;
    &lt;p&gt;You do not need a custom script to start looking for red flags. Open your Google Analytics or other platform right now and conduct a sanity check.&lt;/p&gt;
    &lt;p&gt;The deeper I dug, the more unsettling the landscape became. I spoke to a startup founder who raised $2 million in funding based on "user growth" metrics that he later discovered were 80% bots. He is now trapped, forced to pretend everything is fine because admitting the truth could jeopardize his company and his relationship with his investors.&lt;/p&gt;
    &lt;p&gt;This is the hidden bot economy. Ad platforms are selling impressions to bots. Businesses are buying fake traffic to inflate their metrics. Analytics companies are dutifully reporting on this bot activity. And the entire industry seems to be nodding along, complicit in a collective charade because admitting the truth would cause the fragile system to collapse.&lt;/p&gt;
    &lt;p&gt;I am now convinced that well over half of the internet is a facade, a digital stage play performed by bots for an audience of other bots. And that percentage is growing every day as AI and automation become more sophisticated.&lt;/p&gt;
    &lt;p&gt;The question is no longer whether your business is affected. The question is, what happens when this digital house of cards finally comes tumbling down?&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45590681</guid><pubDate>Wed, 15 Oct 2025 11:11:41 +0000</pubDate></item><item><title>Why We're Leaving Serverless</title><link>https://www.unkey.com/blog/serverless-exit</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45590756</guid><pubDate>Wed, 15 Oct 2025 11:20:35 +0000</pubDate></item><item><title>Esports scholarship at Deutsche Bahn (German railways)</title><link>https://db.jobs/de-de/esports-11092734</link><description>&lt;doc fingerprint="f19030afd78ed14"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Artikel: Unsere Mission: Mitspieler:innen finden!&lt;/head&gt;
    &lt;p&gt;Du hast deinen Schulabschluss (bald) in der Tasche und möchtest ins Berufsleben starten? Wir bieten dir eine sichere Ausbildung und einen zukunftsorientierten Job bei der Deutschen Bahn. Und das ist nicht alles: Gleichzeitig wollen wir deine Leidenschaft fürs Gaming und deine Esports-Träume fördern. Join uns und werde Teil unserer wachsenden Gaming-Community – wir suchen immer nach neuen Mitspieler:innen.&lt;/p&gt;
    &lt;head rend="h2"&gt;Wähle deine Quest&lt;/head&gt;
    &lt;head rend="h2"&gt;Gaming@DB&lt;/head&gt;
    &lt;p&gt;Gaming ist das Herzstück unserer Community. Für viele unserer Mitarbeitenden ist es ein Hobby, um nach der Arbeit abzuschalten und trotzdem eine gute Zeit mit Kolleg:innen zu haben. Wir unterstützen unsere Mitarbeitenden dabei und integrieren ihr Hobby in unsere Unternehmenskultur.&lt;/p&gt;
    &lt;head rend="h3"&gt;Drei Gründe, warum die Deutsche Bahn für dich der perfekte Premade ist!&lt;/head&gt;
    &lt;head rend="h2"&gt;Esports-Stipendium&lt;/head&gt;
    &lt;p&gt;Du willst Esports-Profi werden? Wir sind die einzige Arbeitgeberin in Deutschland, die ein ESports-Stipendium anbietet in Kombination mit einer Ausbildung bei der DB. Egal ob in League of Legends, Valorant, Fifa, Counter Strike 2 oder Brawl Stars – werde mit uns zum Profi-Esportler!&lt;/p&gt;
    &lt;head rend="h3"&gt;Drei Gründe, warum du mit der Deutsche Bahn zur Pro-Gamer:in wirst!&lt;/head&gt;
    &lt;head rend="h2"&gt;Unsere Roadmap für dein DB Esports-Stipendium&lt;/head&gt;
    &lt;p&gt;Unter allen Auszubildenen und Dualstudierenden vergeben wir jährlich Stipendien an die talentiertesten Spieler:innen. Der Auswahlprozess gestaltet sich wie folgt:&lt;/p&gt;
    &lt;p&gt;Bewerbung um einen Ausbildungsplatz oder Duales Studium bei der DB InfraGO AG für das Ausbildungsjahr 2025 oder 2026.&lt;/p&gt;
    &lt;p&gt;Nach mündlicher Einstellungszusage hast du die Möglichkeit, deine Bewerbung für ein einjähriges ESports-Stipendium an unseren Partner, die esports player foundation (epf), zu schicken.&lt;/p&gt;
    &lt;p&gt;Auswahl der talentiertesten Bewerber:innen anhand der Ingame-Statistiken der geförderten Spiele.&lt;/p&gt;
    &lt;p&gt;Einladung zu einem virtuellen Auswahlevent mit Expert:innen der esports player foundation. Die besten Teilnehmer:innen erhalten jeweils ab März ein einjähriges Stipendium.&lt;/p&gt;
    &lt;head rend="h2"&gt;Das sagen unsere Esports-Talente&lt;/head&gt;
    &lt;head rend="h2"&gt;Unsere Champion-Builds&lt;/head&gt;
    &lt;head rend="h2"&gt;Zugverkehrssteuerer (w/m/d)&lt;/head&gt;
    &lt;head rend="h2"&gt;Gleisbauer:in&lt;/head&gt;
    &lt;head rend="h2"&gt;Elektroniker:in für Betriebstechnik&lt;/head&gt;
    &lt;head rend="h2"&gt;Mechatroniker:in&lt;/head&gt;
    &lt;head rend="h2"&gt;Kaufmann für Verkehrsservice (w/m/d)&lt;/head&gt;
    &lt;p&gt;In deiner Ausbildung als Kaufmann für Verkehrsservice (w/m/d) bist du entweder im Bahnhof oder im Zug Ansprechpartner:in für unsere Kund:innen und hilfst ihnen bei Fragen weiter. Außerdem gewinnst du ein Verständnis für wichtige Abläufe im Bahnbetrieb, um einen reibungslosen Betrieb zu gewährleisten. Du erwirbst praktische Fähigkeiten, die dir helfen, professionell und sicher zu arbeiten.&lt;/p&gt;
    &lt;head rend="h2"&gt;Duales Studium Bauingenieurwesen&lt;/head&gt;
    &lt;head rend="h2"&gt;Duales Studium Elektrotechnik&lt;/head&gt;
    &lt;head rend="h2"&gt;Duales Studium Wirtschaftsingenieurwesen&lt;/head&gt;
    &lt;head rend="h2"&gt;Deine Perks&lt;/head&gt;
    &lt;head rend="h2"&gt;Eindrücke&lt;/head&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45590800</guid><pubDate>Wed, 15 Oct 2025 11:26:41 +0000</pubDate></item><item><title>Ireland Is Making Basic Income for Artists Program Permanent</title><link>https://www.artnews.com/art-news/news/ireland-basic-income-artists-program-permanent-1234756981/</link><description>&lt;doc fingerprint="7fea01d9548b374e"&gt;
  &lt;main&gt;
    &lt;p&gt;Several years after launching a trial, Ireland is set to make its basic income for artists program permanent starting in 2026.&lt;/p&gt;
    &lt;p&gt;Under the program, selected artists receive a weekly payment of approximately $375, or about $1,500 per month. There are 2,000 spots available, with applications set to open in September 2026; eligibility criteria have not yet been announced. The government may expand the program to additional applicants in the future, should more funding become available, according to Irish broadcaster RTÉ.&lt;/p&gt;
    &lt;p&gt; The current program, which began in 2022 and is set to end in February after a six-month extension agreed to earlier this year, was launched to support the arts sector following the pandemic. Many artists suffered disproportionate income losses during that time due to the cancelation of live performances and events.&lt;lb/&gt;For the pilot, applicants could apply under visual arts, theater, literature, music, dance, opera, film, circuses, and architecture. They were required to submit two pieces of evidence proving that they were professional cultural workers, such as proof of income from art sales, membership in a professional body, or reviews. At the time, the New York Times reported that more than 9,000 people applied, with 8,200 deemed eligible and 2,000 randomly selected to receive payments. Another 1,000 eligible applicants were placed in a control group to be monitored but not receive funds. &lt;/p&gt;
    &lt;p&gt;The announcement follows the release of an external report by UK-based consultants Alma Economics, which found that the pilot cost €72 million to date but generated nearly €80 million in total benefits to the Irish economy. The report also found that recipients’ arts-related income increased by more than €500 per month on average, income from non-arts work decreased by around €280, and reliance on other social programs declined, with participants receiving €100 less per month on average.&lt;/p&gt;
    &lt;p&gt;“The economic return on this investment in Ireland’s artists and creative arts workers is having an immediate positive impact on the sector and the economy overall,” Patrick O’Donovan, minister for culture, communications, and sport, said in a statement.&lt;/p&gt;
    &lt;p&gt;The report further estimated that a permanent, “scaled-up” program would likely result in artists producing 22 percent more work, while lowering the average cost of art to consumers by 9 to 25 percent.&lt;/p&gt;
    &lt;p&gt; In October, the government released the results of a public survey on the scheme, which found that 97 percent of respondents support the program. However, 47 percent of the 17,000 respondents said artists should be selected based on economic need, while 37.5 percent favored selection by merit. Only 14 percent preferred random selection.&lt;lb/&gt;Ireland’s BIA program is a form of universal basic income, a policy that grants all citizens a recurring payment regardless of socioeconomic status or other factors. Such programs have grown increasingly mainstream—if not widely implemented—in recent years, as fears rise over the effects of artificial intelligence and other technology-driven job losses. Many UBI advocates have cited Ireland’s program as evidence that the model works. &lt;/p&gt;
    &lt;p&gt;“As the pilot shows, basic income works and people need a UBI now to face and deal with the many social, economic, and ecological crises of our world. The Network will continue to help demonstrate basic income within communities and show how it is a sustainable policy,” the UBI Lab Network said in a statement calling for a nationwide program.&lt;/p&gt;
    &lt;p&gt;“We need no further pilots. People need a UBI now to face and deal with the many social, economic, and ecological crises of our world,” Reinhard Huss, organizer of UBI Lab Leeds, told Business Insider in June.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45590900</guid><pubDate>Wed, 15 Oct 2025 11:40:09 +0000</pubDate></item><item><title>Show HN: Halloy – the modern IRC client I hope will outlive me</title><link>https://github.com/squidowl/halloy</link><description>&lt;doc fingerprint="3f7a5c1966185a9a"&gt;
  &lt;main&gt;
    &lt;p&gt;Halloy is an open-source IRC client written in Rust, with the Iced GUI library. It aims to provide a simple and fast client for Mac, Windows, and Linux platforms.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Documentation for latest release: https://halloy.chat.&lt;/item&gt;
      &lt;item&gt;Documentation for main branch (when building from source): https://unstable.halloy.chat.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Join #halloy on libera.chat if you have questions or looking for help.&lt;/p&gt;
    &lt;p&gt;Halloy is also available from Flathub and Snap Store.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;IRCv3.2 capabilities&lt;/item&gt;
      &lt;item&gt;SASL support&lt;/item&gt;
      &lt;item&gt;DCC Send&lt;/item&gt;
      &lt;item&gt;Keyboard shortcuts&lt;/item&gt;
      &lt;item&gt;Auto-completion for nicknames, commands, and channels&lt;/item&gt;
      &lt;item&gt;Notifications support&lt;/item&gt;
      &lt;item&gt;Multiple channels at the same time across servers&lt;/item&gt;
      &lt;item&gt;Command bar for for quick actions&lt;/item&gt;
      &lt;item&gt;Custom themes&lt;/item&gt;
      &lt;item&gt;Portable mode&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Halloy is released under the GPL-3.0 License. For more details, see the LICENSE file.&lt;/p&gt;
    &lt;p&gt;For any questions, suggestions, or issues, please open an issue on the GitHub repository.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45590949</guid><pubDate>Wed, 15 Oct 2025 11:45:47 +0000</pubDate></item><item><title>Helpcare AI (YC F24) Is Hiring</title><link>https://news.ycombinator.com/item?id=45591082</link><description>&lt;doc fingerprint="439da654fe15b36e"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Ideal Candidate:&lt;/p&gt;
      &lt;p&gt;- Worked at an early-stage startup (pre-seed, seed, series A)&lt;/p&gt;
      &lt;p&gt;- 7yrs of full-stack experience. (python, react preferred, open to other)&lt;/p&gt;
      &lt;p&gt;- Some experience with LLM / Agentic Applications.&lt;/p&gt;
      &lt;p&gt;- Some experience with Data Ingestion.&lt;/p&gt;
      &lt;p&gt;- Bonus 2x / yr.&lt;/p&gt;
      &lt;p&gt;- $120,000+ / yr. with generous equity.&lt;/p&gt;
      &lt;p&gt;Tech Stack:&lt;/p&gt;
      &lt;p&gt;Python, React, Fast API, Supabase&lt;/p&gt;
      &lt;p&gt;Who we are:&lt;/p&gt;
      &lt;p&gt;- Mission to improve the world's capacity for care&lt;/p&gt;
      &lt;p&gt;- Respectful, intelligent, positive team&lt;/p&gt;
      &lt;p&gt;- A-players that are excited to build the best&lt;/p&gt;
      &lt;p&gt;- Well funded, YC- Company with incredible customer demand&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45591082</guid><pubDate>Wed, 15 Oct 2025 12:00:35 +0000</pubDate></item><item><title>Irish privacy regulator picks ex-Meta lobbyist as third commissioner</title><link>https://www.euractiv.com/news/irish-privacy-regulator-picks-ex-meta-lobbyist-as-third-commissioner/</link><description>&lt;doc fingerprint="a6d8fb488a905e1f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Irish privacy regulator picks ex-Meta lobbyist as third commissioner&lt;/head&gt;
    &lt;p&gt;Meta's lead privacy watchdog hires Niamh Sweeney, who previously led public policy for Meta-owned Facebook Ireland and WhatsApp in EMEA&lt;/p&gt;
    &lt;p&gt;The Irish Data Protection Commission (DPC), which is Meta’s lead oversight authority in the EU, has appointed Niamh Sweeney, an ex-WhatsApp, Facebook and Stripe lobbyist, as third commissioner to lead its work.&lt;/p&gt;
    &lt;p&gt;The DPC is in charge of enforcing the EU’s privacy rulebook, the General Data Protection Regulation (GDPR), for all firms established in the country – which includes many tech giants, Meta, Google, Apple, and Microsoft among them. This gives it an outsized role in EU oversight of big tech.&lt;/p&gt;
    &lt;p&gt;Sweeney was appointed to the DPC by the Irish government. During her time at Meta she led public policy for WhatsApp in Europe, Africa, and the Middle East, and also lobbying for Facebook in Ireland. Most recently, she was director for the public affairs firm Milltown Partners in Ireland.&lt;/p&gt;
    &lt;p&gt;The appointment comes as the Irish government is planning to increase the DPC’s resources.&lt;/p&gt;
    &lt;p&gt;While the regulator operated for many years with just a single commissioner, in September 2023 it began recruitment for two more. Sweeney’s arrival completes the trio of commissioners now heading the watchdog – after Des Hogan and Dale Sunderland were appointed in February 2024, replacing the previous (sole) commissioner, Helen Dixon.&lt;/p&gt;
    &lt;p&gt;Dixon faced a barrage of criticism over the DPC’s approach to enforcing the GDPR on big tech. And while Hogan and Sunderland vowed to take a tougher stance when they were first appointed, many still doubt the watchdog’s willingness to follow through.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45591085</guid><pubDate>Wed, 15 Oct 2025 12:01:05 +0000</pubDate></item></channel></rss>