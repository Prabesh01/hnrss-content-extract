<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Wed, 26 Nov 2025 20:36:52 +0000</lastBuildDate><item><title>Show HN: We built an open source, zero webhooks payment processor</title><link>https://github.com/flowglad/flowglad</link><description>&lt;doc fingerprint="f056b3782f0b3458"&gt;
  &lt;main&gt;
    &lt;p&gt; The easiest way to make internet money. &lt;lb/&gt; Get Started &lt;lb/&gt; · Quickstart · Website · Issues · Discord &lt;/p&gt;
    &lt;p&gt;Infinite pricing models, one source of truth, zero webhooks.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Default Stateless Say goodbye to webhooks, &lt;code&gt;"subscriptions"&lt;/code&gt;db tables,&lt;code&gt;customer_id&lt;/code&gt;columns,&lt;code&gt;PRICE_ID&lt;/code&gt;env variables, or manually mapping your plans to prices to features and back.&lt;/item&gt;
      &lt;item&gt;Single Source of Truth: Read your latest customer billing state from Flowglad, including feature access and usage meter credits&lt;/item&gt;
      &lt;item&gt;Access Data Using Your Ids: Query customer state by your auth's user ids. Refer to prices, features, and usage meters via slugs you define.&lt;/item&gt;
      &lt;item&gt;Full-Stack SDK: Access your customer's data on the backend using &lt;code&gt;flowgladServer.getBilling()&lt;/code&gt;, or in your React frontend using our&lt;code&gt;useBilling()&lt;/code&gt;hook&lt;/item&gt;
      &lt;item&gt;Adaptable: Iterate on new pricing models in testmode, and push them to prod in a click. Seamlessly rotate pricing models in your app without any redeployment.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;First, install the packages necessary Flowglad packages based on your project setup:&lt;/p&gt;
    &lt;code&gt;# Next.js Projects
bun add @flowglad/nextjs

# React + Express projects:
bun add @flowglad/react @flowglad/express

# All other React + Node Projects
bun add @flowglad/react @flowglad/server&lt;/code&gt;
    &lt;p&gt;Flowglad integrates seamlessly with your authentication system and requires only a few lines of code to get started in your Next.js app. Setup typically takes under a minute:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Configure Your Flowglad Server Client&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Create a utility to generate your Flowglad server instance. Pass your own customer/user/organization IDs—Flowglad never requires its own customer IDs to be managed in your app:&lt;/p&gt;
    &lt;code&gt;// utils/flowglad.ts
import { FlowgladServer } from '@flowglad/nextjs/server'

export const flowglad = (customerExternalId: string) =&amp;gt; {
  return new FlowgladServer({
    customerExternalId,
    getCustomerDetails: async (externalId) =&amp;gt; {
      // e.g. Fetch user info from your DB using your user/org/team ID
      const user = await db.users.findOne({ id: externalId })
      if (!user) throw new Error('User not found')
      return { email: user.email, name: user.name }
    },
  })
}&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Expose the Flowglad API Handler&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Add an API route so the Flowglad client can communicate securely with your backend:&lt;/p&gt;
    &lt;code&gt;// app/api/flowglad/[...path]/route.ts
import { nextRouteHandler } from '@flowglad/nextjs/server'
import { flowglad } from '@/utils/flowglad'

export const { GET, POST } = nextRouteHandler({
  flowglad,
  getCustomerExternalId: async (req) =&amp;gt; {
    // Extract your user/org/team ID from session/auth.
    // For B2C: return user.id from your DB
    // For B2B: return organization.id or team.id
    const userId = await getUserIdFromRequest(req)
    if (!userId) throw new Error('User not authenticated')
    return userId
  },
})&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Wrap Your App with the Provider&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In your root layout (App Router) or _app (Pages Router):&lt;/p&gt;
    &lt;code&gt;import { FlowgladProvider } from '@flowglad/nextjs'

// App Router example (app/layout.tsx)
export default function RootLayout({ children }) {
  return (
    &amp;lt;html&amp;gt;
      &amp;lt;body&amp;gt;
        &amp;lt;FlowgladProvider loadBilling={true}&amp;gt;
          {children}
        &amp;lt;/FlowgladProvider&amp;gt;
      &amp;lt;/body&amp;gt;
    &amp;lt;/html&amp;gt;
  )
}&lt;/code&gt;
    &lt;p&gt;That’s it—Flowglad will use your app’s internal user IDs for all billing logic and integrate billing status into your frontend in real time.&lt;/p&gt;
    &lt;p&gt;B2C apps: Use &lt;code&gt;user.id&lt;/code&gt; as the customer ID.&lt;lb/&gt; B2B apps: Use &lt;code&gt;organization.id&lt;/code&gt; or &lt;code&gt;team.id&lt;/code&gt; as the customer ID.&lt;/p&gt;
    &lt;p&gt;Flowglad does not require you to change your authentication system or manage Flowglad customer IDs. Just pass your own!&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Use &lt;code&gt;useBilling&lt;/code&gt;on your frontend, and&lt;code&gt;flowglad(userId).getBilling()&lt;/code&gt;on your backend&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;'use client'

import { useBilling } from '@flowglad/nextjs'

export function FeatureGate({ featureSlug, children }) {
  const { loaded, errors, checkFeatureAccess } = useBilling()

  if (!loaded || !checkFeatureAccess) {
    return &amp;lt;p&amp;gt;Loading billing state…&amp;lt;/p&amp;gt;
  }

  if (errors?.length) {
    return &amp;lt;p&amp;gt;Unable to load billing data right now.&amp;lt;/p&amp;gt;
  }

  return checkFeatureAccess(featureSlug)
    ? children
    : &amp;lt;p&amp;gt;You need to upgrade to unlock this feature.&amp;lt;/p&amp;gt;
}&lt;/code&gt;
    &lt;code&gt;import { useBilling } from '@flowglad/nextjs'

export function UsageBalanceIndicator({ usageMeterSlug }) {
  const { loaded, errors, checkUsageBalance, createCheckoutSession } = useBilling()

  if (!loaded || !checkUsageBalance) {
    return &amp;lt;p&amp;gt;Loading usage…&amp;lt;/p&amp;gt;
  }

  const usage = checkUsageBalance(usageMeterSlug)

  return (
    &amp;lt;div&amp;gt;
      &amp;lt;h3&amp;gt;Usage Balance&amp;lt;/h3&amp;gt;
      &amp;lt;p&amp;gt;
        Remaining:{' '}
        {usage ? `${usage.availableBalance} credits available` : &amp;lt;button onClick={() =&amp;gt; createCheckoutSession({ 
            priceSlug: 'pro_plan',
            autoRedirect: true
          })}
        /&amp;gt;}
      &amp;lt;/p&amp;gt;
    &amp;lt;/div&amp;gt;
  )
}&lt;/code&gt;
    &lt;code&gt;import { NextResponse } from 'next/server'
import { flowglad } from '@/utils/flowglad'

const hasFastGenerations = async () =&amp;gt; {
  // ...
  const user = await getUser()

  const billing = await flowglad(user.id).getBilling()
  const hasAccess = billing.checkFeatureAccess('fast_generations')
  if (hasAccess) {
    // run fast generations
  } else {
    // fall back to normal generations
  }
}&lt;/code&gt;
    &lt;code&gt;import { flowglad } from '@/utils/flowglad'

const processChatMessage = async (params: { chat: string }) =&amp;gt; {
  // Extract your app's user/org/team ID,
  // whichever corresponds to your customer
  const user = await getUser()

  const billing = await flowglad(user.id).getBilling()
  const usage = billing.checkUsageBalance('chat_messages')
  if (usage.availableBalance &amp;gt; 0) {
    // run chat request
  } else {
    throw Error(`User ${user.id} does not have sufficient usage credits`)
  }
}&lt;/code&gt;
    &lt;p&gt;First, set up a pricing model. You can do so in the dashboard in just a few clicks using a template, that you can then customize to suit your specific needs.&lt;/p&gt;
    &lt;p&gt;We currently have templates for the following pricing models:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Usage-limit + Subscription Hybrid (like Cursor)&lt;/item&gt;
      &lt;item&gt;Unlimited Usage (like ChatGPT consumer)&lt;/item&gt;
      &lt;item&gt;Tiered Access and Usage Credits (like Midjourney)&lt;/item&gt;
      &lt;item&gt;Feature-Gated Subscription (like Linear)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And more on the way. If you don't see a pricing model from our templates that suits you, you can always make one from scratch.&lt;/p&gt;
    &lt;p&gt;In the last 15 years, the market has given developers more options than ever for every single part of their stack. But when it comes to payments, there have been virtually zero new entrants. The existing options are slim, and almost all of them require us to talk to sales to even set up an account. When it comes to self-serve payments, there are even fewer options.&lt;/p&gt;
    &lt;p&gt;The result? The developer experience and cost of payments has barely improved in that time. Best in class DX in payments feels eerily suspended in 2015. Meanwhile, we've enjoyed constant improvements in auth, compute, hosting, and practically everything else.&lt;/p&gt;
    &lt;p&gt;Flowglad wants to change that.&lt;/p&gt;
    &lt;p&gt;We're building a payments layer that lets you:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Think about billing and payments as little as possible&lt;/item&gt;
      &lt;item&gt;Spend as little time on integration and maintenance as possible&lt;/item&gt;
      &lt;item&gt;Get as much out of your single integration as possible&lt;/item&gt;
      &lt;item&gt;Unlock more payment providers from a single integration&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Achieving this mission will take time. It will be hard. It might even make some people unhappy. But with AI bringing more and more developers on line and exploding the complexity of startup billing, the need is more urgent than ever.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46048252</guid><pubDate>Tue, 25 Nov 2025 17:33:50 +0000</pubDate></item><item><title>A new bridge links the math of infinity to computer science</title><link>https://www.quantamagazine.org/a-new-bridge-links-the-strange-math-of-infinity-to-computer-science-20251121/</link><description>&lt;doc fingerprint="b72c931205918bb9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;A New Bridge Links the Strange Math of Infinity to Computer Science&lt;/head&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;All of modern mathematics is built on the foundation of set theory, the study of how to organize abstract collections of objects. But in general, research mathematicians don’t need to think about it when they’re solving their problems. They can take it for granted that sets behave the way they’d expect, and carry on with their work.&lt;/p&gt;
    &lt;p&gt;Descriptive set theorists are an exception. This small community of mathematicians never stopped studying the fundamental nature of sets — particularly the strange infinite ones that other mathematicians ignore.&lt;/p&gt;
    &lt;p&gt;Their field just got a lot less lonely. In 2023, a mathematician named Anton Bernshteyn published a deep and surprising connection between the remote mathematical frontier of descriptive set theory and modern computer science.&lt;/p&gt;
    &lt;p&gt;He showed that all problems about certain kinds of infinite sets can be rewritten as problems about how networks of computers communicate. The bridge connecting the disciplines surprised researchers on both sides. Set theorists use the language of logic, computer scientists the language of algorithms. Set theory deals with the infinite, computer science with the finite. There’s no reason why their problems should be related, much less equivalent.&lt;/p&gt;
    &lt;p&gt;“This is something really weird,” said Václav Rozhoň, a computer scientist at Charles University in Prague. “Like, you are not supposed to have this.”&lt;/p&gt;
    &lt;p&gt;Since Bernshteyn’s result, his peers have been exploring how to move back and forth across the bridge to prove new theorems on either side, and how to extend that bridge to new classes of problems. Some descriptive set theorists are even starting to apply insights from the computer science side to reorganize the landscape of their entire field, and to rethink the way they understand infinity.&lt;/p&gt;
    &lt;p&gt;“This whole time we’ve been working on very similar problems without directly talking to each other,” said Clinton Conley, a descriptive set theorist at Carnegie Mellon University. “It just opens the doors to all these new collaborations.”&lt;/p&gt;
    &lt;head rend="h2"&gt;Broken Sets&lt;/head&gt;
    &lt;p&gt;Bernshteyn was an undergraduate when he first heard of descriptive set theory — as an example of a field that had once mattered, then decayed to nothing. More than a year would pass before he found out the professor had been wrong.&lt;/p&gt;
    &lt;p&gt;In 2014, as a first-year graduate student at the University of Illinois, Bernshteyn took a logic course with Anush Tserunyan, who would later become one of his advisers. She corrected the misconception. “She should take all the credit for me being in this field,” he said. “She really made it seem that logic and set theory is this glue that connects all different parts of math.”&lt;/p&gt;
    &lt;p&gt;Descriptive set theory dates back to Georg Cantor, who proved in 1874 that there are different sizes of infinity. The set of whole numbers (0, 1, 2, 3, …), for instance, is the same size as the set of all fractions, but smaller than the set of all real numbers.&lt;/p&gt;
    &lt;p&gt;At the time, mathematicians were deeply uncomfortable with this menagerie of different infinities. “It’s hard to wrap your head around,” said Bernshteyn, who is now at the University of California, Los Angeles.&lt;/p&gt;
    &lt;p&gt;Partly in response to that discomfort, mathematicians developed a different notion of size — one that described, say, how much length or area or volume a set might occupy, rather than the number of elements it contained. This notion of size is known as a set’s “measure” (in contrast to Cantor’s notion of size, which is a set’s “cardinality”). One of the simplest types of measure — the Lebesgue measure — quantifies a set’s length. While the set of real numbers between zero and 1 and the set of real numbers between zero and 10 are both infinite and have the same cardinality, the first has a Lebesgue measure of 1 and the second a Lebesgue measure of 10.&lt;/p&gt;
    &lt;p&gt;To study more complicated sets, mathematicians use other types of measures. The uglier a set is, the fewer ways there are to measure it. Descriptive set theorists ask questions about which sets can be measured according to different definitions of “measure.” They then arrange them in a hierarchy based on the answers to those questions. At the top are sets that can be constructed easily and studied using any notion of measure you want. At the bottom are “unmeasurable” sets, which are so complicated they can’t be measured at all. “The word people often use is ‘pathological,’” Bernshteyn said. “Nonmeasurable sets are really bad. They’re counterintuitive, and they don’t behave well.”&lt;/p&gt;
    &lt;p&gt;This hierarchy doesn’t just help set theorists map out the landscape of their field; it also gives them insights into what tools they can use to tackle more typical problems in other areas of math. Mathematicians in some fields, such as dynamical systems, group theory and probability theory, need information about the size of the sets they’re using. A set’s position in the hierarchy determines what tools they can use to solve their problem.&lt;/p&gt;
    &lt;p&gt;Descriptive set theorists are thus like librarians, tending to a massive bookshelf of different kinds of infinite sets (and the different ways of measuring them). Their job is to take a problem, determine how complicated a set its solution requires, and place it on the proper shelf, so that other mathematicians can take note.&lt;/p&gt;
    &lt;head rend="h2"&gt;Making a Choice&lt;/head&gt;
    &lt;p&gt;Bernshteyn belongs to a group of librarians who sort problems about infinite sets of nodes connected by edges, called graphs. In particular, he studies graphs that have infinitely many separate pieces, each containing infinitely many nodes. Most graph theorists don’t study these kinds of graphs; they focus on finite ones instead. But such infinite graphs can represent and provide information about dynamical systems and other important kinds of sets, making them a major area of interest for descriptive set theorists.&lt;/p&gt;
    &lt;p&gt;Here’s an example of the kind of infinite graph that Bernshteyn and his colleagues might study. Start with a circle, which contains infinitely many points. Pick one point: This will be your first node. Then move a fixed distance around the circle’s circumference. This gives you a second node. For example, you might move one-fifth of the way around the circle. Connect the two nodes with an edge. Move the same distance to a third node, and connect it to the previous one. And so on.&lt;/p&gt;
    &lt;p&gt;If you move one-fifth of the way around the circle each time, it’ll take five steps to get back where you started. In general, if you move any distance that can be written as a fraction, the nodes will form a closed loop. But if the distance can’t be written as a fraction, the process will go on forever. You’ll get an infinite number of connected nodes.&lt;/p&gt;
    &lt;p&gt;But that’s not all: This infinitely long sequence forms only the first piece of your graph. Even though it contains infinitely many nodes, it doesn’t contain all the points on the circle. To generate the other pieces of the graph, start at one of those other points. Now move the same distance at each step as you did in the first piece. You’ll end up building a second infinite sequence of connected nodes, totally disconnected from the first.&lt;/p&gt;
    &lt;p&gt;Do this for every possible new starting point on the circle. You’ll get a graph consisting of infinitely many separate pieces, with each piece made of an infinite number of nodes.&lt;/p&gt;
    &lt;p&gt;Mathematicians can then ask whether it’s possible to color the nodes in this graph so that they obey certain rules. Using just two colors, for instance, can you color every node in the graph so that no two connected nodes are the same color? The solution might seem straightforward. Look at the first piece of your graph, pick a node, and color it blue. Then color the rest of the piece’s nodes in an alternating pattern: yellow, blue, yellow, blue. Do the same for every piece in your graph: Pick a node, color it blue, then alternate colors. Ultimately, you’ll use just two colors to achieve your task.&lt;/p&gt;
    &lt;p&gt;But to accomplish this coloring, you had to rely on a hidden assumption that set theorists call the axiom of choice. It’s one of the nine fundamental building blocks from which all mathematical statements are constructed. According to this axiom, if you start with a bunch of sets, you can choose one item from each of those sets to create a new set — even if you have infinitely many sets to choose from. This axiom is useful, in that it allows mathematicians to prove all sorts of statements of interest. But it also leads to strange paradoxes. Descriptive set theorists avoid it.&lt;/p&gt;
    &lt;p&gt;Your graph had infinitely many pieces. This corresponds to having infinitely many sets. You chose one item from each set — the first point you decided to color blue in each of the pieces. All those blue points formed a new set. You used the axiom of choice.&lt;/p&gt;
    &lt;p&gt;Which leads to a problem when you color the rest of the nodes in alternating patterns of blue and yellow. You’ve colored each node (which has zero length) separately, without any understanding of how nodes relate to one another when they come from different pieces of the graph. This means that you can’t describe the set of all the graph’s blue nodes, or the set of all its yellow nodes, in terms of length either. In other words, these sets are unmeasurable. Mathematicians can’t say anything useful about them.&lt;/p&gt;
    &lt;p&gt;To descriptive set theorists, this is unsatisfying. And so they want to figure out a way to color the graph in a continuous way — a way that doesn’t use the axiom of choice, and that gives them measurable sets.&lt;/p&gt;
    &lt;p&gt;To do this, remember how you built the first piece of your graph: You picked a node on a circle and connected it to a second node some distance away. Now color the first node blue, the second yellow, and the entire arc between them blue. Similarly, color the arc between the second and third nodes yellow. Color the third arc blue. And so on.&lt;/p&gt;
    &lt;p&gt;Soon, you’ll have made it almost completely around the circle — meaning that you’ve assigned a color to all the nodes in your graph except for the ones that fall in a small, leftover segment. Say the last arc you colored was yellow. How do you color this final, smaller segment? You can’t use blue, because these nodes will connect to nodes in the original arc you colored blue. But you also can’t use yellow, because these nodes connect back to yellow ones from the previous arc.&lt;/p&gt;
    &lt;p&gt;You have to use a third color — say, green — to complete your coloring.&lt;/p&gt;
    &lt;p&gt;Still, the sets of blue, yellow and green nodes you end up with are all just pieces of the circle’s circumference, rather than the scatterings of points you ended up with when you used the axiom of choice. You can calculate the lengths of these sets. They’re measurable.&lt;/p&gt;
    &lt;p&gt;Descriptive set theorists therefore place the two-color version of the problem on the lowest shelf in their hierarchy (for unmeasurable sets), while the three-color problem goes on a much higher shelf of problems — ones where lots of notions of measure can be applied.&lt;/p&gt;
    &lt;p&gt;Bernshteyn spent his years in graduate school studying such coloring problems, shelving them one by one. Then, shortly after he finished his degree, he stumbled on a potential way to shelve them all at once — and to show that these problems have a much deeper and more mathematically relevant structure than anyone had realized.&lt;/p&gt;
    &lt;head rend="h2"&gt;Round by Round&lt;/head&gt;
    &lt;p&gt;From time to time, Bernshteyn enjoys going to computer science talks, where graphs are finite and represent networks of computers.&lt;/p&gt;
    &lt;p&gt;In 2019, one of those talks changed the course of his career. It was about “distributed algorithms” — sets of instructions that run simultaneously on multiple computers in a network to accomplish a task without a central coordinator.&lt;/p&gt;
    &lt;p&gt;Say you have a bunch of Wi-Fi routers in a building. Nearby routers can interfere with each other if they use the same communication frequency channel. So each router needs to choose a different channel from the ones used by its immediate neighbors.&lt;/p&gt;
    &lt;p&gt;Computer scientists can reframe this as a coloring problem on a graph: Represent each router as a node, and connect nearby ones with edges. Using just two colors (representing two different frequency channels), find a way to color each node so that no two connected nodes are the same color.&lt;/p&gt;
    &lt;p&gt;But there’s a catch: Nodes can only communicate with their immediate neighbors, using so-called local algorithms. First, each node runs the same algorithm and assigns itself a color. It then communicates with its neighbors to learn how other nodes are colored in a small region around it. Then it runs the algorithm again to decide whether to keep its color or switch it. It repeats this step until the whole network has a proper coloring.&lt;/p&gt;
    &lt;p&gt;Computer scientists want to know how many steps a given algorithm requires. For example, any local algorithm that can solve the router problem with only two colors must be incredibly inefficient, but it’s possible to find a very efficient local algorithm if you’re allowed to use three.&lt;/p&gt;
    &lt;p&gt;At the talk Bernshteyn was attending, the speaker discussed these thresholds for different kinds of problems. One of the thresholds, he realized, sounded a lot like a threshold that existed in the world of descriptive set theory — about the number of colors required to color certain infinite graphs in a measurable way.&lt;/p&gt;
    &lt;p&gt;To Bernshteyn, it felt like more than a coincidence. It wasn’t just that computer scientists are like librarians too, shelving problems based on how efficiently their algorithms work. It wasn’t just that these problems could also be written in terms of graphs and colorings.&lt;/p&gt;
    &lt;p&gt;Perhaps, he thought, the two bookshelves had more in common than that. Perhaps the connection between these two fields went much, much deeper.&lt;/p&gt;
    &lt;p&gt;Perhaps all the books, and their shelves, were identical, just written in different languages — and in need of a translator.&lt;/p&gt;
    &lt;head rend="h2"&gt;Opening the Door&lt;/head&gt;
    &lt;p&gt;Bernshteyn set out to make this connection explicit. He wanted to show that every efficient local algorithm can be turned into a Lebesgue-measurable way of coloring an infinite graph (that satisfies some additional important properties). That is, one of computer science’s most important shelves is equivalent to one of set theory’s most important shelves (high up in the hierarchy).&lt;/p&gt;
    &lt;p&gt;He began with the class of network problems from the computer science lecture, focusing on their overarching rule — that any given node’s algorithm uses information about just its local neighborhood, whether the graph has a thousand nodes or a billion.&lt;/p&gt;
    &lt;p&gt;To run properly, all the algorithm has to do is label each node in a given neighborhood with a unique number, so that it can log information about nearby nodes and give instructions about them. That’s easy enough to do in a finite graph: Just give every node in the graph a different number.&lt;/p&gt;
    &lt;p&gt;If Bernshteyn could run the same algorithm on an infinite graph, it meant he could color the graph in a measurable way — solving a graph-coloring question on the set theory side. But there was a problem: These infinite graphs are “uncountably” infinite. There’s no way to uniquely label all their nodes.&lt;/p&gt;
    &lt;p&gt;Bernshteyn’s challenge was to find a cleverer way to label the graphs.&lt;/p&gt;
    &lt;p&gt;He knew that he’d have to reuse labels. But that was fine so long as nearby nodes were labeled differently. Was there a way to assign labels without accidentally reusing one in the same neighborhood?&lt;/p&gt;
    &lt;p&gt;Bernshteyn showed that there is always a way — no matter how many labels you decide to use, and no matter how many nodes your local neighborhood has. This means that you can always safely extend the algorithm from the computer science side to the set theory side. “Any algorithm in our setup corresponds to a way of measurably coloring any graph in the descriptive set theory setup,” Rozhoň said.&lt;/p&gt;
    &lt;p&gt;The proof came as a surprise to mathematicians. It demonstrated a deep link between computation and definability, and between algorithms and measurable sets. Mathematicians are now exploring how to take advantage of Bernshteyn’s discovery. In a paper published this year, for instance, Rozhoň and his colleagues figured out that it’s possible to color special graphs called trees by looking at the same problem in the computer science context. The result also illuminated which tools mathematicians might use to study the trees’ corresponding dynamical systems. “This is a very interesting experience, trying to prove results in a field where I don’t understand even the basic definitions,” Rozhoň said.&lt;/p&gt;
    &lt;p&gt;Mathematicians have also been working to translate problems in the other direction. In one case, they used set theory to prove a new estimate of how hard a certain class of problems is to solve.&lt;/p&gt;
    &lt;p&gt;Bernshteyn’s bridge isn’t just about having a new tool kit for solving individual problems. It has also allowed set theorists to gain a clearer view of their field. There were lots of problems that they had no idea how to classify. In many cases, that’s now changed, because set theorists have computer scientists’ more organized bookshelves to guide them.&lt;/p&gt;
    &lt;p&gt;Bernshteyn hopes this growing area of research will change how the working mathematician views set theorists’ work — that they’ll no longer see it as remote and disconnected from the real mathematical world. “I’m trying to change this,” he said. “I want people to get used to thinking about infinity.”&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46049932</guid><pubDate>Tue, 25 Nov 2025 19:53:20 +0000</pubDate></item><item><title>Show HN: KiDoom – Running DOOM on PCB Traces</title><link>https://www.mikeayles.com/#kidoom</link><description>&lt;doc fingerprint="562395acea28f504"&gt;
  &lt;main&gt;
    &lt;p&gt;3 ECUs Developed 10+ Years Exp. 28.5M+ Miles Driven Selected Projects Private Tools ×&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46051449</guid><pubDate>Tue, 25 Nov 2025 22:13:35 +0000</pubDate></item><item><title>CS234: Reinforcement Learning Winter 2025</title><link>https://web.stanford.edu/class/cs234/</link><description>&lt;doc fingerprint="db6129c8929d1c49"&gt;
  &lt;main&gt;
    &lt;table&gt;
      &lt;row span="9"&gt;
        &lt;cell role="head"&gt;Monday&lt;/cell&gt;
        &lt;cell role="head"&gt;Tuesday&lt;/cell&gt;
        &lt;cell role="head"&gt;Wednesday&lt;/cell&gt;
        &lt;cell role="head"&gt;Thursday&lt;/cell&gt;
        &lt;cell role="head"&gt;Friday&lt;/cell&gt;
        &lt;cell role="head"&gt;Saturday&lt;/cell&gt;
        &lt;cell role="head"&gt;Sunday&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Week 1&lt;/cell&gt;
        &lt;cell&gt;Jan 6&lt;/cell&gt;
        &lt;cell&gt;Jan 7&lt;/cell&gt;
        &lt;cell&gt;Jan 8&lt;/cell&gt;
        &lt;cell&gt;Jan 9&lt;/cell&gt;
        &lt;cell&gt;Jan 10&lt;/cell&gt;
        &lt;cell&gt;Jan 11&lt;/cell&gt;
        &lt;cell&gt;Jan 12&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt; Lecture Materials &lt;/cell&gt;
        &lt;cell&gt; Introduction to Reinforcement Learning &lt;p&gt;1:30pm-2:50pm&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt; Tabular MDP Planning &lt;p&gt;1:30pm-2:50pm&lt;/p&gt;&lt;p&gt;[Assignment 1 Released]&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Week 2&lt;/cell&gt;
        &lt;cell&gt;Jan 13&lt;/cell&gt;
        &lt;cell&gt;Jan 14&lt;/cell&gt;
        &lt;cell&gt;Jan 15&lt;/cell&gt;
        &lt;cell&gt;Jan 16&lt;/cell&gt;
        &lt;cell&gt;Jan 17&lt;/cell&gt;
        &lt;cell&gt;Jan 18&lt;/cell&gt;
        &lt;cell&gt;Jan 19&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt; Lecture Materials &lt;/cell&gt;
        &lt;cell&gt; Policy Evaluation &lt;p&gt;1:30pm-2:50pm&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt; Q-Learning and Function Approximation &lt;p&gt;1:30pm-2:50pm&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt; Assignment 1 Due at 6pm&lt;p&gt;[Assignment 2 Released]&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Week 3&lt;/cell&gt;
        &lt;cell&gt;Jan 20&lt;/cell&gt;
        &lt;cell&gt;Jan 21&lt;/cell&gt;
        &lt;cell&gt;Jan 22&lt;/cell&gt;
        &lt;cell&gt;Jan 23&lt;/cell&gt;
        &lt;cell&gt;Jan 24&lt;/cell&gt;
        &lt;cell&gt;Jan 25&lt;/cell&gt;
        &lt;cell&gt;Jan 26&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt; Lecture Materials &lt;/cell&gt;
        &lt;cell&gt; Policy Search 1 &lt;p&gt;1:30pm-2:50pm&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt; Policy Search 2 &lt;p&gt;1:30pm-2:50pm&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Week 4&lt;/cell&gt;
        &lt;cell&gt;Jan 27&lt;/cell&gt;
        &lt;cell&gt;Jan 28&lt;/cell&gt;
        &lt;cell&gt;Jan 29&lt;/cell&gt;
        &lt;cell&gt;Jan 30&lt;/cell&gt;
        &lt;cell&gt;Jan 31&lt;/cell&gt;
        &lt;cell&gt;Feb 1&lt;/cell&gt;
        &lt;cell&gt;Feb 2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt; Lecture Materials &lt;/cell&gt;
        &lt;cell&gt; Policy Search 3 &lt;p&gt;1:30pm-2:50pm&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt; Offline RL 1 / Imitation learning &lt;p&gt;1:30pm-2:50pm&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;Assignment 2 Due at 6pm&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Week 5&lt;/cell&gt;
        &lt;cell&gt;Feb 3&lt;/cell&gt;
        &lt;cell&gt;Feb 4&lt;/cell&gt;
        &lt;cell&gt;Feb 5&lt;/cell&gt;
        &lt;cell&gt;Feb 6&lt;/cell&gt;
        &lt;cell&gt;Feb 7&lt;/cell&gt;
        &lt;cell&gt;Feb 8&lt;/cell&gt;
        &lt;cell&gt;Feb 9&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Lecture Materials&lt;/cell&gt;
        &lt;cell&gt; Offline RL 2 / DPO &lt;p&gt;1:30pm-2:50pm&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;Midterm (in class)&lt;/cell&gt;
        &lt;cell&gt;[Assignment 3 released]&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Week 6&lt;/cell&gt;
        &lt;cell&gt;Feb 10&lt;/cell&gt;
        &lt;cell&gt;Feb 11&lt;/cell&gt;
        &lt;cell&gt;Feb 12&lt;/cell&gt;
        &lt;cell&gt;Feb 13&lt;/cell&gt;
        &lt;cell&gt;Feb 14&lt;/cell&gt;
        &lt;cell&gt;Feb 15&lt;/cell&gt;
        &lt;cell&gt;Feb 16&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Lecture Materials&lt;/cell&gt;
        &lt;cell&gt; Offline RL 3 &lt;p&gt;1:30pm-2:50pm&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt; Exploration 1 &lt;p&gt;1:30pm-2:50pm&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Week 7&lt;/cell&gt;
        &lt;cell&gt;Feb 17&lt;/cell&gt;
        &lt;cell&gt;Feb 18&lt;/cell&gt;
        &lt;cell&gt;Feb 19&lt;/cell&gt;
        &lt;cell&gt;Feb 20&lt;/cell&gt;
        &lt;cell&gt;Feb 21&lt;/cell&gt;
        &lt;cell&gt;Feb 22&lt;/cell&gt;
        &lt;cell&gt;Feb 23&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Lecture Materials&lt;/cell&gt;
        &lt;cell&gt; Exploration 2 &lt;p&gt;1:30pm-2:50pm&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt; Exploration 3 &lt;p&gt;1:30pm-2:50pm&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;Assignment 3 Due at 6pm&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Week 8&lt;/cell&gt;
        &lt;cell&gt;Feb 24&lt;/cell&gt;
        &lt;cell&gt;Feb 25&lt;/cell&gt;
        &lt;cell&gt;Feb 26&lt;/cell&gt;
        &lt;cell&gt;Feb 27&lt;/cell&gt;
        &lt;cell&gt;Feb 28&lt;/cell&gt;
        &lt;cell&gt;Mar 1&lt;/cell&gt;
        &lt;cell&gt;Mar 2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Lecture Materials&lt;/cell&gt;
        &lt;cell&gt; Exploration 4 &lt;p&gt;1:30pm-2:50pm&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;Guest lecture&lt;/cell&gt;
        &lt;cell&gt; Project Milestone &lt;p&gt;Due at 6pm&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Week 9&lt;/cell&gt;
        &lt;cell&gt;Mar 3&lt;/cell&gt;
        &lt;cell&gt;Mar 4&lt;/cell&gt;
        &lt;cell&gt;Mar 5&lt;/cell&gt;
        &lt;cell&gt;Mar 6&lt;/cell&gt;
        &lt;cell&gt;Mar 7&lt;/cell&gt;
        &lt;cell&gt;Mar 8&lt;/cell&gt;
        &lt;cell&gt;Mar 9&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Lecture Materials&lt;/cell&gt;
        &lt;cell&gt; Monte Carlo Tree Search / AlphaGo &lt;p&gt;1:30pm-2:50pm&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt; Quiz (in class) &lt;p&gt;1:30pm-2:50pm&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Week 10&lt;/cell&gt;
        &lt;cell&gt;Mar 10&lt;/cell&gt;
        &lt;cell&gt;Mar 11&lt;/cell&gt;
        &lt;cell&gt;Mar 12&lt;/cell&gt;
        &lt;cell&gt;Mar 13&lt;/cell&gt;
        &lt;cell&gt;Mar 14&lt;/cell&gt;
        &lt;cell&gt;Mar 15&lt;/cell&gt;
        &lt;cell&gt;Mar 16&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Lecture Materials&lt;/cell&gt;
        &lt;cell&gt;Guest Lecture and Wrap Up&lt;/cell&gt;
        &lt;cell&gt;Final Project Poster Session&lt;p&gt;1:30pm-4:30pm&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Week 11&lt;/cell&gt;
        &lt;cell&gt;Mar 17&lt;/cell&gt;
        &lt;cell&gt;Mar 18&lt;/cell&gt;
        &lt;cell&gt;Mar 19&lt;/cell&gt;
        &lt;cell&gt;Mar 20&lt;/cell&gt;
        &lt;cell&gt;Mar 21&lt;/cell&gt;
        &lt;cell&gt;Mar 22&lt;/cell&gt;
        &lt;cell&gt;Mar 23&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Lecture Materials&lt;/cell&gt;
        &lt;cell&gt;Final Project Writeup Due at 6pm&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46052685</guid><pubDate>Wed, 26 Nov 2025 00:33:29 +0000</pubDate></item><item><title>Image Diffusion Models Exhibit Emergent Temporal Propagation in Videos</title><link>https://arxiv.org/abs/2511.19936</link><description>&lt;doc fingerprint="949db60014f5ba86"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Computer Vision and Pattern Recognition&lt;/head&gt;&lt;p&gt; [Submitted on 25 Nov 2025]&lt;/p&gt;&lt;head rend="h1"&gt;Title:Image Diffusion Models Exhibit Emergent Temporal Propagation in Videos&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:Image diffusion models, though originally developed for image generation, implicitly capture rich semantic structures that enable various recognition and localization tasks beyond synthesis. In this work, we investigate their self-attention maps can be reinterpreted as semantic label propagation kernels, providing robust pixel-level correspondences between relevant image regions. Extending this mechanism across frames yields a temporal propagation kernel that enables zero-shot object tracking via segmentation in videos. We further demonstrate the effectiveness of test-time optimization strategies-DDIM inversion, textual inversion, and adaptive head weighting-in adapting diffusion features for robust and consistent label propagation. Building on these findings, we introduce DRIFT, a framework for object tracking in videos leveraging a pretrained image diffusion model with SAM-guided mask refinement, achieving state-of-the-art zero-shot performance on standard video object segmentation benchmarks.&lt;/quote&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46055177</guid><pubDate>Wed, 26 Nov 2025 07:55:49 +0000</pubDate></item><item><title>Statistical Process Control in Python</title><link>https://timothyfraser.com/sigma/statistical-process-control-in-python.html</link><description>&lt;doc fingerprint="d2780f32a8fc6cd2"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;16 Statistical Process Control in &lt;code&gt;Python&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;In this workshop, we will learn how to perform statistical process control in Python, using statistical tools and &lt;code&gt;plotnine&lt;/code&gt; visualizations! Statistical Process Control refers to using statistics to (1) measure variation in product quality over time and (2) identify benchmarks to know when intervention is needed. Let’s get started!&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting Started&lt;/head&gt;
    &lt;head rend="h3"&gt;Packages&lt;/head&gt;
    &lt;code&gt;# Remember to install these packages using a terminal, if you haven't already!
!pip install pandas plotnine scipy&lt;/code&gt;
    &lt;p&gt;We’ll be using &lt;code&gt;pandas&lt;/code&gt; for data manipulation, &lt;code&gt;plotnine&lt;/code&gt; for visualization, and &lt;code&gt;scipy&lt;/code&gt; for statistical functions.&lt;/p&gt;
    &lt;head rend="h3"&gt;Custom Functions&lt;/head&gt;
    &lt;p&gt;This workshop uses custom functions from the &lt;code&gt;functions/&lt;/code&gt; directory. You may need both:
- &lt;code&gt;functions_distributions.py&lt;/code&gt; - for reliability and distribution functions
- &lt;code&gt;functions_process_control.py&lt;/code&gt; - for statistical process control functions&lt;/p&gt;
    &lt;p&gt;To use these functions, you need to acquire them from the repository at github.com/timothyfraser/sigma/tree/main/functions.&lt;/p&gt;
    &lt;p&gt;Add the functions directory to your Python path&lt;/p&gt;
    &lt;code&gt;import sys
import os
# Add the functions directory to Python path
sys.path.append('functions')  # or path to wherever you placed the functions folder&lt;/code&gt;
    &lt;p&gt;Once you have the functions available, you can import them:&lt;/p&gt;
    &lt;head rend="h3"&gt;Our Case&lt;/head&gt;
    &lt;p&gt;For today’s workshop, we’re going to think about why quality control matters in a local economy, by examining the case of the Japanese Hot Springs bath economy! Hot springs, or onsen, are a major source of tourism and recreation for families in Japan, bringing residents from across the country every year to often rural communities where the right geological conditions have brought on naturally occurring hot springs. Restaurants, taxi and bus companies, and many service sector firms rely on their local onsen to bring in a steady stream (pun intended) of tourists to the local economy. So, it’s often in the best interest of onsen operators to keep an eye on the temperature, minerals, or other aspects of their hot springs baths to ensure quality control, to keep up their firm (and town’s!) reputation for quality rest and relaxation!&lt;/p&gt;
    &lt;p&gt;Onsen-goers often seek out specific types of hot springs, so it’s important for an onsen to actually provide what it advertises! Serbulea and Payyappallimana (2012) describe some of these benchmarks.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Temperature: Onsen are divided into “Extra Hot Springs” (&lt;/p&gt;&lt;code&gt;&amp;gt;42°C&lt;/code&gt;), “Hot Springs” (&lt;code&gt;41~34°C&lt;/code&gt;), and “Warm Springs” (&lt;code&gt;33~25°C&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;pH: Onsen are classified into “Acidic” (&lt;/p&gt;&lt;code&gt;pH &amp;lt; 3&lt;/code&gt;), “Mildly Acidic” (&lt;code&gt;pH 3~6&lt;/code&gt;), “Neutral” (&lt;code&gt;pH 6~7.5&lt;/code&gt;), “Mildly alkaline” (&lt;code&gt;pH 7.5~8.5&lt;/code&gt;), and “Alkaline” (&lt;code&gt;pH &amp;gt; 8.5&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Sulfur: Sulfur onsen typically have about 2mg of sulfur per 1kg of hot spring water; sulfur levels must exceed 1 mg to count as a Sulfur onsen. (It smells like rotten eggs!)&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These are decent examples of quality control metrics that onsen operators might want to keep tabs on!&lt;/p&gt;
    &lt;head rend="h3"&gt;Our Data&lt;/head&gt;
    &lt;p&gt;You’ve been hired to evaluate quality control at a local onsen in sunny Kagoshima prefecture! Every month, for 15 months, you systematically took 20 random samples of hot spring water and recorded its temperature, pH, and sulfur levels. How might you determine if this onsen is at risk of slipping out of one sector of the market (eg. Extra Hot!) and into another (just normal Hot Springs?).&lt;/p&gt;
    &lt;p&gt;Let’s read in our data from &lt;code&gt;workshops/onsen.csv&lt;/code&gt;!&lt;/p&gt;
    &lt;code&gt;# Add functions directory to path if not already there
import sys
if 'functions' not in sys.path:
    sys.path.append('functions')

from functions_distributions import density, tidy_density, approxfun

water = pd.read_csv('workshops/onsen.csv')
water.head(3)&lt;/code&gt;
    &lt;code&gt;##    id  time  temp   ph  sulfur
## 0   1     1  43.2  5.1     0.0
## 1   2     1  45.3  4.8     0.4
## 2   3     1  45.5  6.2     0.9&lt;/code&gt;
    &lt;head rend="h2"&gt;16.1 Process Descriptive Statistics&lt;/head&gt;
    &lt;p&gt;First, let’s get a sense of our process by calculating some basic descriptive statistics. We’ll create a simple function to calculate the mean and standard deviation, which are fundamental to evaluating process variation.&lt;/p&gt;
    &lt;code&gt;from pandas import Series
def describe(x: Series):
  x = Series(x)
  out = pd.DataFrame({
    'mean': [x.mean()],
    'sd': [x.std()],
  })
  out['caption'] = ("Process Mean: " + out['mean'].round(2).astype(str) +
                    " | SD: " + out['sd'].round(2).astype(str))
  return out

tab = describe(water['temp'])
tab&lt;/code&gt;
    &lt;code&gt;##     mean        sd                         caption
## 0  44.85  1.989501  Process Mean: 44.85 | SD: 1.99&lt;/code&gt;
    &lt;p&gt;Now let’s apply this to our temperature data to see the overall process mean and variation.&lt;/p&gt;
    &lt;head rend="h2"&gt;16.2 Process Overview Visual&lt;/head&gt;
    &lt;p&gt;The process overview chart is one of the most important tools in SPC. It shows us how our process behaves over time, helping us identify patterns, trends, and potential issues. We’ll create a visualization that shows individual measurements, subgroup means, and the overall process average.&lt;/p&gt;
    &lt;code&gt;g1 = (ggplot(water, aes(x='time', y='temp', group='time')) +
  geom_hline(aes(yintercept=water['temp'].mean()), color='lightgrey', size=3) +
  geom_jitter(height=0, width=0.25) +
  geom_boxplot() +
  labs(x='Time (Subgroup)', y='Temperature (Celsius)', subtitle='Process Overview', caption=tab['caption'][0]))

# Save the plot
g1.save('images/05_process_overview.png', width=8, height=6, dpi=100)&lt;/code&gt;
    &lt;code&gt;g2 = (ggplot(water, aes(x='temp')) + geom_histogram(bins=15, color='white', fill='grey') + theme_void() + coord_flip())

# Save the plot
g2.save('images/05_process_histogram.png', width=8, height=6, dpi=100)&lt;/code&gt;
    &lt;p&gt;The histogram shows us the distribution of all temperature measurements, giving us insight into the overall process variation. This helps us understand if our process is centered and how much variation we’re seeing.&lt;/p&gt;
    &lt;head rend="h2"&gt;16.3 Subgroup (Within-Group) Statistics&lt;/head&gt;
    &lt;p&gt;In SPC, we often work with subgroups - small samples taken at regular intervals. This allows us to distinguish between common cause variation (inherent to the process) and special cause variation (due to specific events). Let’s calculate statistics for each subgroup to see how the process behaves over time.&lt;/p&gt;
    &lt;code&gt;stat_s = (water.groupby('time').apply(lambda d: pd.Series({
  'xbar': d['temp'].mean(),
  'r': d['temp'].max() - d['temp'].min(),
  'sd': d['temp'].std(),
  'nw': len(d)
})).reset_index())
stat_s['df'] = stat_s['nw'] - 1
stat_s['sigma_s'] = ( (stat_s['df'] * (stat_s['sd']**2)).sum() / stat_s['df'].sum() )**0.5
stat_s['se'] = stat_s['sigma_s'] / (stat_s['nw']**0.5)
stat_s['upper'] = stat_s['xbar'].mean() + 3*stat_s['se']
stat_s['lower'] = stat_s['xbar'].mean() - 3*stat_s['se']
stat_s.head(3)&lt;/code&gt;
    &lt;code&gt;##    time    xbar    r        sd    nw    df   sigma_s        se      upper      lower
## 0     1  44.635  4.2  1.342533  20.0  19.0  1.986174  0.444122  46.182366  43.517634
## 1     3  45.305  7.9  2.001440  20.0  19.0  1.986174  0.444122  46.182366  43.517634
## 2     5  44.765  5.9  1.628133  20.0  19.0  1.986174  0.444122  46.182366  43.517634&lt;/code&gt;
    &lt;p&gt;Here we’ve calculated key statistics for each subgroup:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;xbar: The mean of each subgroup&lt;/item&gt;
      &lt;item&gt;r: The range (max - min) within each subgroup&lt;/item&gt;
      &lt;item&gt;sd: The standard deviation within each subgroup&lt;/item&gt;
      &lt;item&gt;sigma_s: The pooled within-subgroup standard deviation&lt;/item&gt;
      &lt;item&gt;se: The standard error for each subgroup mean&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;16.3.1 Total Statistics (Between Groups)&lt;/head&gt;
    &lt;p&gt;Now let’s calculate the overall process statistics that summarize the behavior across all subgroups:&lt;/p&gt;
    &lt;code&gt;stat_t = pd.DataFrame({
  'xbbar': [stat_s['xbar'].mean()],
  'rbar': [stat_s['r'].mean()],
  'sdbar': [stat_s['sd'].mean()],
  'sigma_s': [(stat_s['sd']**2).mean()**0.5],
  'sigma_t': [water['temp'].std()]
})
stat_t&lt;/code&gt;
    &lt;code&gt;##    xbbar    rbar    sdbar   sigma_s   sigma_t
## 0  44.85  7.2625  1.93619  1.986174  1.989501&lt;/code&gt;
    &lt;p&gt;These statistics give us:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;xbbar: The grand mean (average of all subgroup means)&lt;/item&gt;
      &lt;item&gt;rbar: The average range across subgroups&lt;/item&gt;
      &lt;item&gt;sdbar: The average standard deviation across subgroups&lt;/item&gt;
      &lt;item&gt;sigma_s: The pooled within-subgroup standard deviation&lt;/item&gt;
      &lt;item&gt;sigma_t: The total process standard deviation&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;16.3.2 Average and Standard Deviation Charts&lt;/head&gt;
    &lt;p&gt;Control charts are the heart of SPC. They help us monitor process stability over time and detect when the process is out of control. We’ll create charts for both the subgroup means (X-bar chart) and standard deviations (S chart).&lt;/p&gt;
    &lt;code&gt;labels = pd.DataFrame({
  'time': [stat_s['time'].max()]*3,
  'type': ['xbbar','upper','lower'],
  'name': ['mean','+3 s','-3 s'],
  'value': [stat_s['xbar'].mean(), stat_s['upper'].iloc[0], stat_s['lower'].iloc[0]]
})

control_chart = (ggplot(stat_s, aes(x='time', y='xbar')) +
  geom_hline(aes(yintercept=stat_s['xbar'].mean()), color='lightgrey', size=3) +
  geom_ribbon(aes(ymin='lower', ymax='upper'), fill='steelblue', alpha=0.2) +
  geom_line(size=1) + geom_point(size=5) +
  geom_label(data=labels, mapping=aes(x='time', y='value', label='name'), ha='right') +
  labs(x='Time (Subgroups)', y='Average', subtitle='Average and Standard Deviation Chart'))

# Save the plot
control_chart.save('images/05_control_chart.png', width=8, height=6, dpi=100)&lt;/code&gt;
    &lt;p&gt;This control chart shows:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Center line: The grand mean (xbbar)&lt;/item&gt;
      &lt;item&gt;Control limits: Upper and lower 3-sigma limits based on the standard error&lt;/item&gt;
      &lt;item&gt;Individual points: Each subgroup mean plotted over time&lt;/item&gt;
      &lt;item&gt;Shaded area: The control limits region&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Points outside the control limits or showing non-random patterns indicate the process may be out of control and requires investigation.&lt;/p&gt;
    &lt;head rend="h2"&gt;Learning Check 1&lt;/head&gt;
    &lt;p&gt;Question&lt;/p&gt;
    &lt;p&gt;Produce the same process overview chart for &lt;code&gt;pH&lt;/code&gt;.&lt;/p&gt;
    &lt;head&gt;[View Answer!]&lt;/head&gt;
    &lt;code&gt;def ggprocess(x, y, xlab='Subgroup', ylab='Metric'):
  import pandas as pd
  from plotnine import ggplot, aes, geom_hline, geom_jitter, geom_boxplot, labs
  d = pd.DataFrame({'x': x, 'y': y})
  g = (ggplot(d, aes(x='x', y='y', group='x')) +
       geom_hline(aes(yintercept=d['y'].mean()), color='lightgrey', size=3) +
       geom_jitter(height=0, width=0.25) +
       geom_boxplot() +
       labs(x=xlab, y=ylab, subtitle='Process Overview'))
  return g

ph_chart = ggprocess(water['time'], water['ph'])

# Save the plot
ph_chart.save('images/05_ph_chart.png', width=8, height=6, dpi=100)&lt;/code&gt;
    &lt;head rend="h2"&gt;16.4 Moving Range Charts (n=1)&lt;/head&gt;
    &lt;p&gt;When we have individual measurements rather than subgroups, we use moving range charts. The moving range is the absolute difference between consecutive measurements, which helps us estimate process variation when we can’t calculate within-subgroup statistics.&lt;/p&gt;
    &lt;code&gt;indiv = water.iloc[[0,20,40,60,80,100,120,140]]
mr = (indiv['temp'].diff().abs().dropna())
mrbar = mr.mean()
import numpy as np
d2 = np.mean(np.abs(np.diff(np.random.normal(0,1,10000))))
sigma_s = mrbar / d2
se = sigma_s / (1**0.5)
upper = mrbar + 3*se
lower = 0&lt;/code&gt;
    &lt;code&gt;istat = pd.DataFrame({'time': indiv['time'].iloc[1:], 'mr': mr, 'mrbar': mrbar, 'upper': upper, 'lower': lower})
mr_chart = (ggplot(istat, aes(x='time', y='mr')) +
  geom_ribbon(aes(ymin='lower', ymax='upper'), fill='steelblue', alpha=0.25) +
  geom_hline(aes(yintercept=mr.mean()), size=3, color='darkgrey') +
  geom_line(size=1) + geom_point(size=5) +
  labs(x='Time (Subgroup)', y='Moving Range', subtitle='Moving Range Chart'))

# Save the plot
mr_chart.save('images/05_moving_range_chart.png', width=8, height=6, dpi=100)&lt;/code&gt;
    &lt;p&gt;The moving range chart shows:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Center line: The average moving range (mrbar)&lt;/item&gt;
      &lt;item&gt;Upper control limit: Based on the estimated process standard deviation&lt;/item&gt;
      &lt;item&gt;Lower control limit: Set to 0 (moving ranges can’t be negative)&lt;/item&gt;
      &lt;item&gt;Individual points: Each moving range value&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This chart helps us monitor process variation when we have individual measurements rather than subgroups.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46055421</guid><pubDate>Wed, 26 Nov 2025 08:40:29 +0000</pubDate></item><item><title>A cell so minimal that it challenges definitions of life</title><link>https://www.quantamagazine.org/a-cell-so-minimal-that-it-challenges-definitions-of-life-20251124/</link><description>&lt;doc fingerprint="b629199a712103d4"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;A Cell So Minimal That It Challenges Definitions of Life&lt;/head&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;Life’s fundamental structure is the cell, and so the main things that a cell does — processing biomolecules, growing, replicating its genetic material and producing a new body — are considered hallmarks of life. But earlier this year, scientists discovered a cell so severely stripped of essential functions that it challenges biologists’ definitions of what counts as a living thing.&lt;/p&gt;
    &lt;p&gt;The species is a single-celled organism known only by the mysterious sequence of its genetic code. Its genome is fantastically small: Along the organism’s evolutionary journey, it seems to have gotten rid of most of it. According to the shocked researchers who published the discovery in a preprint uploaded to biorxiv.org in May, the lost genes include those central to cell metabolism, meaning it can neither process nutrients nor grow on its own.&lt;/p&gt;
    &lt;p&gt;Other cells with highly reduced genomes still encode proteins to create amino acids, break down carbohydrates for energy or synthesize vitamins. All this appears to be absent from the cell, which seems to be a parasite entirely dependent on a host or cellular community to meet its nutritional needs. Until now, these genetic pathways were considered fundamental for the survival of any cell.&lt;/p&gt;
    &lt;p&gt;The organism’s “replicative core” — the genetic components needed to reproduce itself — remains, making up more than half of its genome.&lt;/p&gt;
    &lt;p&gt;“Metabolism is one of the key components of how we often define life,” said Takuro Nakayama, an evolutionary microbiologist at the University of Tsukuba in Japan who led the team. The cell’s discovery “challenges this by suggesting a cell can exist almost entirely without its own. It demonstrates that the diversity of cellular life is far greater than we knew and that organisms do not always follow our definitions.”&lt;/p&gt;
    &lt;p&gt;While this form of life is new to science, it’s possible that organisms like it are common. A huge proportion of microbial biodiversity may be hiding in recursive interrelationships between parasitic and host microbes, said Puri López-García, a microbial ecologist at the French National Center for Scientific Research in Paris who was not involved in the study.&lt;/p&gt;
    &lt;p&gt;“The diversity of archaea and bacteria that appear to belong to these supergroups of parasitic organisms is very, very large,” she said. For bacteria, it may be between 25% and 50% of the group’s total share of species, she suggested.&lt;/p&gt;
    &lt;p&gt;The discovery pushes the boundaries of our knowledge of just how small and simple cellular life can become, as it evolves even into forms that are barely alive.&lt;/p&gt;
    &lt;head rend="h2"&gt;An Extraordinary Discovery&lt;/head&gt;
    &lt;p&gt;Nakayama has built a scientific career out of looking more closely than other researchers typically do. He considers an already tiny cell and wonders: Are there even smaller cells that make a home there?&lt;/p&gt;
    &lt;p&gt;“The difference [in size between parasitic and host cells] can sometimes be like that between a human and Godzilla,” Nakayama said. He is fascinated by the potentially vast amount of undiscovered biodiversity these relationships might contain, and his lab looks for such relationships in seawater. The ocean is a nutrient-poor environment that incentivizes cells to form trading partnerships. Sometimes they float along together, loosely tethered, exchanging rare nutrients and energy. Other times their arrangements are more organized.&lt;/p&gt;
    &lt;p&gt;Citharistes regius is a globally widespread single-celled dinoflagellate that has a walled, pouchlike external chamber for housing symbiotic cyanobacteria. Nakayama and his team searched for the alga by scooping seawater samples from the Pacific Ocean using a fine-mesh net. A common technique is to sequence whatever DNA can be found in the soup of such a sample, an approach called metagenomics.&lt;/p&gt;
    &lt;p&gt;“That method is incredibly powerful for capturing a broad overview,” Nakayama said. “However, with such data, it is often difficult to maintain the link between a sequence and the specific cell it came from, and rare organisms can be easily missed.” His team’s more targeted approach involves microscopically identifying and physically isolating a single target cell from that mixed sample.&lt;/p&gt;
    &lt;p&gt;Back on shore in the Tsukuba lab, after the researchers confirmed they had C. regius, they sequenced every genome associated with that one cell. As expected, they found DNA from its symbiotic cyanobacteria, but they found something else, too: sequences that belong to an archaeon, a member of the domain of life thought to have given rise to eukaryotes like us.&lt;/p&gt;
    &lt;p&gt;At first, Nakayama and his colleagues thought they had made a mistake. The archaeal genome is tiny: just 238,000 base pairs end to end. In comparison, humans have a few billion base pairs, and even E. coli bacteria work with several million. (C. regius’ symbiotic cyanobacteria have 1.9 million base pairs.) Previously, the smallest known archaeal genome was the one belonging to Nanoarchaeum equitans — at 490,000 base pairs, it is more than twice as long as the new one the researchers found. They initially figured that this tiny genome — too large to be merely statistical noise — was an abbreviated piece of a much larger genome, erroneously compiled by their software.&lt;/p&gt;
    &lt;p&gt;“At first, we suspected it might be an artifact of the genome-assembly process,” Nakayama recalled. To check, the team sequenced the genome using different technologies and ran the data through multiple computer programs that assemble fragments of DNA sequences into a full genome. The various approaches all reconstructed the exact same 238,000-base-pair circular genome. “This consistency is what convinced us it was the real, complete genome,” he said.&lt;/p&gt;
    &lt;p&gt;This meant that Nakayama and his team had a new organism on their hands. They named the microbe Candidatus Sukunaarchaeum mirabile (hereafter referred to as Sukunaarchaeum) for its remarkably tiny genome — after Sukuna-biko-na, a Shinto deity notable for his short stature, plus a Latin word for “extraordinary.”&lt;/p&gt;
    &lt;head rend="h2"&gt;The Spectrum of Quasi-Life&lt;/head&gt;
    &lt;p&gt;When the team consulted databases of known genes to analyze the archaeon, they found its small size was the result of a whole lot that was missing.&lt;/p&gt;
    &lt;p&gt;Sukunaarchaeum encodes the barest minimum of proteins for its own replication, and that’s about all. Most strangely, its genome is missing any hints of the genes required to process and build molecules, outside of those needed to reproduce. Lacking those metabolic components, the organism must outsource the processes for growth and maintenance to another cell, a host upon which the microbe is entirely dependent.&lt;/p&gt;
    &lt;p&gt;Other symbiotic microbes have scrapped much of their genomes, including Sukunaarchaeum’s evolutionary relatives. The researchers’ analysis suggested that the microbe is part of the DPANN archaea, sometimes called nanoarchaea or ultra-small archaea, which are characterized by small size and small genomes. DPANN archaea are generally thought to be symbiotes that cling to the outside of larger prokaryotic microbes, and plenty of them have substantially reduced genomes to match that lifestyle. But until now, none of the DPANN species had genomes quite this pared back. And Sukunaarchaeum branched off the DPANN lineage early, suggesting that it had taken its own evolutionary journey.&lt;/p&gt;
    &lt;p&gt;“This realm of the archaea is pretty mysterious in general,” said Brett Baker, a microbial ecologist at the University of Texas, Austin who was not involved in the work. “[DPANN archaea are] obviously limited in their metabolic capabilities.”&lt;/p&gt;
    &lt;p&gt;While Sukunaarchaeum may provide some undetermined benefit for its host — which could be C. regius, the symbiotic cyanobacteria or another cell entirely — it’s probably a self-absorbed parasite. “Its genome reduction is driven by entirely selfish motives, consistent with a parasitic lifestyle,” said Tim Williams, a microbiologist at the University of Technology Sydney who was not involved in the study. It cannot contribute metabolic products, so the relationship between Sukunaarchaeum and any other cell would likely be a one-way street.&lt;/p&gt;
    &lt;p&gt;Other microbes have evolved similarly extreme, streamlined forms. For instance, the bacterium Carsonella ruddii, which lives as a symbiont within the guts of sap-feeding insects, has an even smaller genome than Sukunaarchaeum, at around 159,000 base pairs. However, these and other super-small bacteria have metabolic genes to produce nutrients, such as amino acids and vitamins, for their hosts. Instead, their genome has cast off much of their ability to reproduce on their own.&lt;/p&gt;
    &lt;p&gt;“They are on the way to becoming organelles. This is the way mitochondria and chloroplasts are thought to have evolved,” Williams said. “But Sukunaarchaeum has gone in the opposite direction: The genome retains genes required for its own propagation, but lost most, if not all, of its metabolic genes.”&lt;/p&gt;
    &lt;p&gt;Soon after Nakayama’s team posted their results online, they got a big response. “When we saw the preprint, this was really quite exciting in the lab,” said Thijs Ettema, an evolutionary microbiologist and expert on archaeal genomics at Wageningen University &amp;amp; Research in the Netherlands, who was not involved in the work. “These types of organisms [with reduced genomes] have been found before, but not as extreme as this.”&lt;/p&gt;
    &lt;p&gt;Some news reports went so far as to imply that Sukunaarchaeum is on its way to evolving into a virus. However, while both Sukunaarchaeum and viruses are reliant on a host cell for very basic biological functions, viruses can’t reproduce on their own.&lt;/p&gt;
    &lt;p&gt;“There is a fundamental gap between Sukunaarchaeum and viruses,” Nakayama said. “Sukunaarchaeum retains its own core machinery for gene expression, including ribosomes, albeit in a simplified form. This is in stark contrast to viruses, which lack ribosomes and must hijack the host’s cellular systems to replicate.”&lt;/p&gt;
    &lt;p&gt;The findings fit into a larger discussion about how we define life, Ettema said, since nature routinely evolves exceptions that defy simple categorization. “Most likely it cannot live independently,” he said. “You could say the same of bacterial symbionts. And what do we call organelles like mitochondria and plastids? … At what point should we call things alive?”&lt;/p&gt;
    &lt;head rend="h2"&gt;A Minimalist Lifestyle&lt;/head&gt;
    &lt;p&gt;Many questions about Sukunaarchaeum remain unresolved. For one, a large portion of its genome is made up of genes that don’t match any known sequences. They seem to encode large proteins, which is uncommon in such radically reduced organisms.&lt;/p&gt;
    &lt;p&gt;Nakayama and his colleagues think these large proteins are employed on the cell membrane and somehow support interactions between the archaeon and its host. That would fit with the lifestyles of other studied DPANN archaea as well, Ettema said, which are generally thought to be ectosymbionts, adhering to the outside of comparatively immense hosts.&lt;/p&gt;
    &lt;p&gt;Although Sukunaarchaeum was found in association with the dinoflagellate C. regius, its true host’s identity is unknown. C. regius is a eukaryote, but DPANN archaea generally associate with other archaea. Also up for debate: Is it attaching to the outside of a host cell, like other DPANN archaea, or is it living internally — or both? Answering these questions would require setting human eyes on the archaeon for the first time; at this point it’s only known from a curious string of genetic data.&lt;/p&gt;
    &lt;p&gt;There is also a slim possibility that these genes are the “lost” metabolic genes after all, López-García said, if they have evolved so far from their original sequences as to be unrecognizable. “Because the genome is so fast-evolving, maybe some of these functions correspond to metabolic functions, but the divergence is so much that we cannot identify the [gene] homologue [in the database],” she said.&lt;/p&gt;
    &lt;p&gt;Even stranger minimalist lifestyles or more reduced genomes may be out there, but researchers may miss them, Ettema said. Traditional analytical approaches for surveying the genomes of microbial samples could flag their tiny genomes as incomplete or low quality and discard them, or skip them entirely, he said. “[The DNA] might have been present in the samples, but it was removed after sequencing, and hence overlooked.”&lt;/p&gt;
    &lt;p&gt;When Nakayama and his colleagues searched a database of marine environmental sequence data from the world’s oceans to see if the new microbe popped up anywhere else, they didn’t find any matches. But they did detect many very similar sequences from what are likely to be close relatives. Sukunaarchaeum may be the tip of a very large microbial iceberg, one floating in a vast ocean of microbial diversity: tiny microbes clinging to slightly less tiny microbes, perhaps inside other microbes, the stories of their ancient relationships only beginning to be revealed.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46055935</guid><pubDate>Wed, 26 Nov 2025 10:06:41 +0000</pubDate></item><item><title>Qiskit open-source SDK for working with quantum computers</title><link>https://github.com/Qiskit/qiskit</link><description>&lt;doc fingerprint="7133719a8c8455c8"&gt;
  &lt;main&gt;
    &lt;p&gt;Qiskit is an open-source SDK for working with quantum computers at the level of extended quantum circuits, operators, and primitives.&lt;/p&gt;
    &lt;p&gt;This library is the core component of Qiskit, which contains the building blocks for creating and working with quantum circuits, quantum operators, and primitive functions (Sampler and Estimator). It also contains a transpiler that supports optimizing quantum circuits, and a quantum information toolbox for creating advanced operators.&lt;/p&gt;
    &lt;p&gt;For more details on how to use Qiskit, refer to the documentation located here:&lt;/p&gt;
    &lt;p&gt;https://quantum.cloud.ibm.com/docs/&lt;/p&gt;
    &lt;p&gt;We encourage installing Qiskit via &lt;code&gt;pip&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;pip install qiskit&lt;/code&gt;
    &lt;p&gt;Pip will handle all dependencies automatically and you will always install the latest (and well-tested) version.&lt;/p&gt;
    &lt;p&gt;To install from source, follow the instructions in the documentation.&lt;/p&gt;
    &lt;p&gt;Now that Qiskit is installed, it's time to begin working with Qiskit. The essential parts of a quantum program are:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Define and build a quantum circuit that represents the quantum state&lt;/item&gt;
      &lt;item&gt;Define the classical output by measurements or a set of observable operators&lt;/item&gt;
      &lt;item&gt;Depending on the output, use the Sampler primitive to sample outcomes or the Estimator primitive to estimate expectation values.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Create an example quantum circuit using the &lt;code&gt;QuantumCircuit&lt;/code&gt; class:&lt;/p&gt;
    &lt;code&gt;import numpy as np
from qiskit import QuantumCircuit

# 1. A quantum circuit for preparing the quantum state |000&amp;gt; + i |111&amp;gt; / √2
qc = QuantumCircuit(3)
qc.h(0)             # generate superposition
qc.p(np.pi / 2, 0)  # add quantum phase
qc.cx(0, 1)         # 0th-qubit-Controlled-NOT gate on 1st qubit
qc.cx(0, 2)         # 0th-qubit-Controlled-NOT gate on 2nd qubit&lt;/code&gt;
    &lt;p&gt;This simple example creates an entangled state known as a GHZ state &lt;code&gt;h&lt;/code&gt;), Phase gate (&lt;code&gt;p&lt;/code&gt;), and CNOT gate (&lt;code&gt;cx&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;Once you've made your first quantum circuit, choose which primitive you will use. Starting with the Sampler, we use &lt;code&gt;measure_all(inplace=False)&lt;/code&gt; to get a copy of the circuit in which all the qubits are measured:&lt;/p&gt;
    &lt;code&gt;# 2. Add the classical output in the form of measurement of all qubits
qc_measured = qc.measure_all(inplace=False)

# 3. Execute using the Sampler primitive
from qiskit.primitives import StatevectorSampler
sampler = StatevectorSampler()
job = sampler.run([qc_measured], shots=1000)
result = job.result()
print(f" &amp;gt; Counts: {result[0].data['meas'].get_counts()}")&lt;/code&gt;
    &lt;p&gt;Running this will give an outcome similar to &lt;code&gt;{'000': 497, '111': 503}&lt;/code&gt; which is &lt;code&gt;000&lt;/code&gt; 50% of the time and &lt;code&gt;111&lt;/code&gt; 50% of the time up to statistical fluctuations.
To illustrate the power of the Estimator, we now use the quantum information toolbox to create the operator &lt;code&gt;run()&lt;/code&gt; function, along with our quantum circuit. Note that the Estimator requires a circuit without measurements, so we use the &lt;code&gt;qc&lt;/code&gt; circuit we created earlier.&lt;/p&gt;
    &lt;code&gt;# 2. Define the observable to be measured 
from qiskit.quantum_info import SparsePauliOp
operator = SparsePauliOp.from_list([("XXY", 1), ("XYX", 1), ("YXX", 1), ("YYY", -1)])

# 3. Execute using the Estimator primitive
from qiskit.primitives import StatevectorEstimator
estimator = StatevectorEstimator()
job = estimator.run([(qc, operator)], precision=1e-3)
result = job.result()
print(f" &amp;gt; Expectation values: {result[0].data.evs}")&lt;/code&gt;
    &lt;p&gt;Running this will give the outcome &lt;code&gt;4&lt;/code&gt;. For fun, try to assign a value of +/- 1 to each single-qubit operator X and Y
and see if you can achieve this outcome. (Spoiler alert: this is not possible!)&lt;/p&gt;
    &lt;p&gt;Using the Qiskit-provided &lt;code&gt;qiskit.primitives.StatevectorSampler&lt;/code&gt; and &lt;code&gt;qiskit.primitives.StatevectorEstimator&lt;/code&gt; will not take you very far.
The power of quantum computing cannot be simulated on classical computers and you need to use real quantum hardware to scale to larger quantum circuits.
However, running a quantum circuit on hardware requires rewriting to the basis gates and connectivity of the quantum hardware.
The tool that does this is the transpiler, and Qiskit includes transpiler passes for synthesis, optimization, mapping, and scheduling.
However, it also includes a default compiler, which works very well in most examples.
The following code will map the example circuit to the &lt;code&gt;basis_gates = ["cz", "sx", "rz"]&lt;/code&gt; and a
bidirectional linear chain of qubits &lt;code&gt;coupling_map = [[0, 1], [1, 0], [1, 2], [2, 1]]&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;from qiskit import transpile
from qiskit.transpiler import Target, CouplingMap
target = Target.from_configuration(
    basis_gates=["cz", "sx", "rz"],
    coupling_map=CouplingMap.from_line(3),
)
qc_transpiled = transpile(qc, target=target)&lt;/code&gt;
    &lt;p&gt;Qiskit provides an abstraction layer that lets users run quantum circuits on hardware from any vendor that provides a compatible interface. The best way to use Qiskit is with a runtime environment that provides optimized implementations of Sampler and Estimator for a given hardware platform. This runtime may involve using pre- and post-processing, such as optimized transpiler passes with error suppression, error mitigation, and, eventually, error correction built in. A runtime implements &lt;code&gt;qiskit.primitives.BaseSamplerV2&lt;/code&gt; and &lt;code&gt;qiskit.primitives.BaseEstimatorV2&lt;/code&gt; interfaces. For example,
some packages that provide implementations of a runtime primitive implementation are:&lt;/p&gt;
    &lt;p&gt;Qiskit also provides a lower-level abstract interface for describing quantum backends. This interface, located in &lt;code&gt;qiskit.providers&lt;/code&gt;, defines an abstract &lt;code&gt;BackendV2&lt;/code&gt; class that providers can implement to represent their
hardware or simulators to Qiskit. The backend class includes a common interface for executing circuits on the backends; however, in this interface each provider may perform different types of pre- and post-processing and return outcomes that are vendor-defined. Some examples of published provider packages that interface with real hardware are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;https://github.com/qiskit-community/qiskit-ionq&lt;/item&gt;
      &lt;item&gt;https://github.com/qiskit-community/qiskit-aqt-provider&lt;/item&gt;
      &lt;item&gt;https://github.com/qiskit-community/qiskit-braket-provider&lt;/item&gt;
      &lt;item&gt;https://github.com/qiskit-community/qiskit-quantinuum-provider&lt;/item&gt;
      &lt;item&gt;https://github.com/rigetti/qiskit-rigetti&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You can refer to the documentation of these packages for further instructions on how to get access and use these systems.&lt;/p&gt;
    &lt;p&gt;If you'd like to contribute to Qiskit, please take a look at our contribution guidelines. By participating, you are expected to uphold our code of conduct.&lt;/p&gt;
    &lt;p&gt;We use GitHub issues for tracking requests and bugs. Please join the Qiskit Slack community for discussion, comments, and questions. For questions related to running or using Qiskit, Stack Overflow has a &lt;code&gt;qiskit&lt;/code&gt;.
For questions on quantum computing with Qiskit, use the &lt;code&gt;qiskit&lt;/code&gt; tag in the Quantum Computing Stack Exchange (please, read first the guidelines on how to ask in that forum).&lt;/p&gt;
    &lt;p&gt;Qiskit is the work of many people who contribute to the project at different levels. If you use Qiskit, please cite as per the included BibTeX file.&lt;/p&gt;
    &lt;p&gt;The changelog for a particular release is dynamically generated and gets written to the release page on Github for each release. For example, you can find the page for the &lt;code&gt;1.2.0&lt;/code&gt; release here:&lt;/p&gt;
    &lt;p&gt;https://github.com/Qiskit/qiskit/releases/tag/1.2.0&lt;/p&gt;
    &lt;p&gt;The changelog for the current release can be found in the releases tab: The changelog provides a quick overview of notable changes for a given release.&lt;/p&gt;
    &lt;p&gt;Additionally, as part of each release, detailed release notes are written to document in detail what has changed as part of a release. This includes any documentation on potential breaking changes on upgrade and new features. See all release notes here.&lt;/p&gt;
    &lt;p&gt;We acknowledge partial support for Qiskit development from the DOE Office of Science National Quantum Information Science Research Centers, Co-design Center for Quantum Advantage (C2QA) under contract number DE-SC0012704.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46056757</guid><pubDate>Wed, 26 Nov 2025 12:26:49 +0000</pubDate></item><item><title>Voyager 1 Is About to Reach One Light-Day from Earth</title><link>https://scienceclock.com/voyager-1-is-about-to-reach-one-light-day-from-earth/</link><description>&lt;doc fingerprint="6da2c57b3816873f"&gt;
  &lt;main&gt;
    &lt;p&gt;After nearly 50 years in space, NASA’s Voyager 1 is about to hit a historic milestone. By November 15, 2026, it will be 16.1 billion miles (25.9 billion km) away, meaning a radio signal will take a full 24 hours—a full light-day—to reach it. For context, a light-year is the distance light travels in a year, about 5.88 trillion miles (9.46 trillion km), so one light-day is just a tiny fraction of that.&lt;/p&gt;
    &lt;p&gt;Launched in 1977 to explore Jupiter and Saturn, Voyager 1 entered interstellar space in 2012, becoming the most distant human-made object ever. Traveling at around 11 miles per second (17.7 km/s), it adds roughly 3.5 astronomical units (the distance from Earth to the Sun) each year. Even after decades in the harsh environment of space, Voyager 1 keeps sending data thanks to its radioisotope thermoelectric generators, which will last into the 2030s.&lt;/p&gt;
    &lt;p&gt;Communicating with Voyager 1 is slow. Commands now take about a day to arrive, with another day for confirmation. Compare that to the Moon (1.3 seconds), Mars (up to 4 minutes), and Pluto (nearly 7 hours). The probe’s distance makes every instruction a patient exercise in deep-space operations. To reach our closest star, Proxima Centauri, even at light speed, would take over four years—showing just how tiny a light-day is in cosmic terms.&lt;/p&gt;
    &lt;p&gt;Voyager 1’s journey is more than a record for distance. From its planetary flybys to the iconic ‘Pale Blue Dot’ image, it reminds us of the vast scale of the solar system and the incredible endurance of a spacecraft designed to keep exploring, even without return.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46057488</guid><pubDate>Wed, 26 Nov 2025 14:02:46 +0000</pubDate></item><item><title>OpenAI needs to raise at least $207B by 2030</title><link>https://ft.com/content/23e54a28-6f63-4533-ab96-3756d9c88bad</link><description>&lt;doc fingerprint="923f1c7588aac3e9"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;FT Alphaville&lt;/head&gt;&lt;p&gt;Register to unlock this article&lt;/p&gt;&lt;head rend="h1"&gt;&lt;quote&gt;OpenAI needs to raise at least $207bn by 2030 so it can continue to lose money, HSBC estimates&lt;/quote&gt;&lt;/head&gt;&lt;p&gt;FT Alphaville is free&lt;/p&gt;&lt;p&gt;Register to keep reading&lt;/p&gt;&lt;p&gt;Want a deeper look?&lt;/p&gt;Explore our recommended subscriptions&lt;head rend="h2"&gt;Explore more offers.&lt;/head&gt;&lt;head rend="h3"&gt;Trial&lt;/head&gt;&lt;p&gt;$1 for 4 weeks&lt;/p&gt;&lt;p&gt;Then $75 per month. Complete digital access to quality FT journalism on any device. Cancel or change your plan anytime during your trial.&lt;/p&gt;&lt;head rend="h3"&gt;Standard Digital&lt;/head&gt;&lt;p&gt;$45 per month&lt;/p&gt;&lt;p&gt;Get essential digital access to quality FT journalism on any device. Pay a year upfront and save 20%&lt;/p&gt;&lt;head rend="h3"&gt;Premium Digital&lt;/head&gt;&lt;p&gt;Complete coverage&lt;/p&gt;&lt;p&gt;$75 per month&lt;/p&gt;&lt;p&gt;Complete digital access to quality FT journalism with expert analysis from industry leaders. Pay a year upfront and save 20%.&lt;/p&gt;&lt;p&gt;Check whether you already have access via your university or organisation.&lt;/p&gt;&lt;p&gt;Terms &amp;amp; Conditions apply&lt;/p&gt;&lt;head rend="h2"&gt;Explore our full range of subscriptions.&lt;/head&gt;&lt;head rend="h3"&gt;For individuals&lt;/head&gt;&lt;p&gt;Discover all the plans currently available in your country&lt;/p&gt;&lt;head rend="h3"&gt;For multiple readers&lt;/head&gt;&lt;p&gt;Digital access for organisations. Includes exclusive features and content.&lt;/p&gt;&lt;head rend="h2"&gt;Why the FT?&lt;/head&gt;&lt;p&gt;See why over a million readers pay to read the Financial Times.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46058065</guid><pubDate>Wed, 26 Nov 2025 15:06:37 +0000</pubDate></item><item><title>From blood sugar to brain relief: GLP-1 therapy slashes migraine frequency</title><link>https://www.medlink.com/news/from-blood-sugar-to-brain-relief-glp-1-therapy-slashes-migraine-frequency</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46058600</guid><pubDate>Wed, 26 Nov 2025 15:49:11 +0000</pubDate></item><item><title>Slop Detective – Fight the Slop Syndicate</title><link>https://slopdetective.kagi.com/</link><description>&lt;doc fingerprint="517fd5a9bcf28ee0"&gt;
  &lt;main&gt;
    &lt;p&gt;Slop Detective Streak: | Cases Solved: Please enable JavaScript to play Slop Detective.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46059069</guid><pubDate>Wed, 26 Nov 2025 16:24:29 +0000</pubDate></item><item><title>Optery (YC W22) Hiring CISO, Release Manager, Tech Lead (Node), Full Stack Eng</title><link>https://www.optery.com/careers/</link><description>&lt;doc fingerprint="ca6bb85f43b74372"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Careers&lt;/head&gt;
    &lt;p&gt;💡Page not loading? Optery’s Career page uses Cookies to display the full page content. If you’re not seeing anything, try opening the cookie banner (cookie icon in the bottom left corner) and Accept Personalization cookies.&lt;/p&gt;
    &lt;p&gt;💡Page not loading? Optery’s Career page uses Cookies to display the full page content. If you’re not seeing anything, try opening the cookie banner (cookie icon in the bottom left corner) and Accept Personalization cookies.&lt;/p&gt;
    &lt;p&gt;Ready to safeguard your personal data?&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46059620</guid><pubDate>Wed, 26 Nov 2025 17:03:21 +0000</pubDate></item><item><title>DRAM prices are spiking, but I don't trust the industry's why</title><link>https://www.xda-developers.com/dram-prices-spiking-dont-trust-industry-reasons/</link><description>&lt;doc fingerprint="3826959d8cb231a8"&gt;
  &lt;main&gt;
    &lt;p&gt;RAM prices have skyrocketed globally in 2025, with industry officials pointing to explosive demand from AI data centers as the primary cause. Mainstream DDR5 memory modules now cost at least twice what they did in mid-2025, and overall DRAM contract prices were a stunning 171.8% higher in the third quarter of 2025 compared to a year prior. For context, these increases have even outpaced the surge in gold prices over the same period, which has also seen a surge in price given economic fears and overall uncertainty. Manufacturers and analysts are now warning us that we're at the start of a major DRAM bull market, with shortages expected to continue into 2026.&lt;/p&gt;
    &lt;p&gt;With all of that said, the memory industry has a bit of history with regard to price-fixing. While I'm sure that there are perfectly natural market forces at play, here, there's a lot of room for skepticism, too.&lt;/p&gt;
    &lt;head rend="h2"&gt;A global surge in memory pricing&lt;/head&gt;
    &lt;head rend="h3"&gt;Some context, first&lt;/head&gt;
    &lt;p&gt;The price spike in memory (especially DRAM) is being felt worldwide. Contract prices for server and PC memory have climbed steeply through 2025, and these increases are now trickling down to retail. For example, a standard 32GB Corsair RAM kit I found on Amazon, specifically 6000 MHz DDR5, cost $110 at the start of this year. Now it costs a whopping $442 after a long period of time where it was out of stock. That quadrupling in price in less than a year highlights just how quickly the market has turned. In other regions, prices have jumped at a similar scale, and there have been reports of retailers even rationing sales of memory modules due to limited supply. In Japan, certain shops have even capped the quantity of HDDs, SSDs, and RAM that a customer can buy because deliveries are so scarce, and memory kit launches are being delayed, too.&lt;/p&gt;
    &lt;p&gt;This isn't just a DRAM story, either. NAND flash memory and hard drive prices are rising in tandem, all caught in the same squeeze of demand that's affecting everything. Back in September, both DRAM and NAND flash contract prices were climbing by 15 to 20%, and that trend has accelerated as we enter the final quarter of the year. Major cloud providers have reportedly agreed to pay up to 50% higher prices for memory chips than they did in the previous quarter. Plus, even despite those premiums, some companies report only receiving approximately 70% of the server memory that they ordered, and smaller buyers are receiving even less. Memory is being allocated to those with deeper pockets first, and it's affecting everything.&lt;/p&gt;
    &lt;p&gt;Anyone looking to upgrade or build a PC will have noticed the crazy-high RAM prices that are now taking hold. The cost savings from cheaper CPUs or GPUs this year are being wiped out by memory kit price hikes. Even now, desktop DDR5 memory modules cost roughly double what they did just a few months ago, and this adds significant expense to any build. And if you thought you could go to older DDR4 modules instead, then think again. Those are also getting pricier as they become scarcer, and that's because DDR4 is being phased out of production. Companies like Samsung, SK Hynix, and Micron are extending how long they're producing it for, but production was supposed to have stopped by now, and it's unlikely that they're producing it at the same rate they used to.&lt;/p&gt;
    &lt;p&gt;It's not just consumers and AI companies that are feeling the pressure; companies that produce devices like laptops, smartphones, and graphics cards are feeling the squeeze, too. Major PC OEMs and system integrators have started panic-buying and stockpiling RAM to secure supply, and Asus has said that it only has about two months of inventory left for production. Embedded devices aren't exempt either, and the Raspberry Pi Foundation, which had stockpiled memory ahead of time, was forced to increase the prices of its 4GB and 8GB models by $5 and $10, respectively, because memory now costs "roughly 120% more than it did a year ago," according to Raspberry Pi Holdings CEO Eben Upton.&lt;/p&gt;
    &lt;p&gt;Even data centers aren't safe from the chaos they've created, and some analysts estimate the world's largest memory maker, Samsung, have imposed such steep price hikes that they could push AI server costs up by 10% to 25% for cloud operators. If things worsen to the point of not being able to get any stock at all, hyperscalers may have to slow down their AI data center deployments because they simply can't get enough memory to actually build out their data centers. Plus, because AI servers are devouring both DRAM and storage, it's causing a cascading effect. High-capacity HDDs (which are used for data center storage) are on backorder for a year or more, and with disk drives scarce, cloud companies are turning to flash storage (SSDs) in roles traditionally filled by disks. This simultaneous strain on NAND flash and HDDs is unprecedented, as when one was constrained, the other often acted as a fallback.&lt;/p&gt;
    &lt;head rend="h2"&gt;AI is the official explanation&lt;/head&gt;
    &lt;head rend="h3"&gt;An absurd amount of DRAM is going to just a few companies&lt;/head&gt;
    &lt;p&gt;The explanation from industry leaders is centered around a perfect storm of both booming demand and constrained supply, and the narrative from most companies in the space backs that assertion up. Generative AI requires a huge amount of both memory and storage, as training and running models require data centers filled with GPUs; GPUs that can have hundreds of gigabytes of DRAM paired with multiple terabytes of flash storage. For example, OpenAI's new "Stargate" project reportedly signed deals with Samsung and SK Hynix for up to 900,000 wafers of DRAM per month to feed its AI clusters, which is an amount close to 40% of total global DRAM output if it's ever met. That's an absurd amount of DRAM. Similarly, cloud providers are pre-buying years' worth of memory. Micron, as another example, has already presold essentially all of its HBM (High Bandwidth Memory) chip output essentially through 2026, and Samsung's next-gen V9 NAND flash is nearly fully booked by enterprise customers before launch, though that's a problem that technically started in September 2024.&lt;/p&gt;
    &lt;p&gt;On the supply side, only a few companies produce the vast majority of the world's memory chips, and they were clearly not prepared for this surge. The DRAM industry is an oligopoly of basically three major players; Samsung, SK Hynix, and Micron. Over the past decades, the memory market's brutal boom and bust cycles drove many competitors out, leaving just these few big suppliers. This matters because with so few producers, any strategic choices they make (or don't make) have outsized impact on supply. In this cycle, manufacturers had cut back production and investment during the last downturn (2022), and they've been slow to ramp back up. To make matters worse, with a suspected AI bubble that could pop at any moment, it seems that memory makers have no plans to significantly increase overall DRAM production as a result of potential market volatility. In an interview with Taiwanese CommonWealth Magazine, Pua Khein-Seng, CEO of Phison, said the following:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;NAND will face severe shortages next year. I think supply will be tight for the next ten years. In the past, every time flash makers invested more, prices collapsed, and they never recouped their investments. So companies slowed spending starting around 2019–2020. Then in 2023, Micron and SK Hynix redirected huge CapEx into HBM because the margins were so attractive, leaving even less investment for flash.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Another factor limiting supply of standard RAM is that memory firms are diverting their limited manufacturing capacity to the most lucrative products. Specifically, there's a gold rush for HBM, which is a special kind of memory used by AI accelerator GPUs, because HBM commands far higher prices and profit margins than commodity DRAM. Every wafer a company allocates to making HBM for GPUs is one not used to make standard PC or server DRAM, so from the point of view of a major company, why bother investing in the consumer-grade or regular data-center hardware when the same resources can be used to make something with a much higher return? These companies ran out of HBM last year, and if production was shifted towards HBM, then they can gain a higher return on each wafer while also increasing the price of regular DRAM, too.&lt;/p&gt;
    &lt;p&gt;When it comes to DDR4, given those phase-out timelines given by the major producers, some smaller Chinese companies (like CXMT and Jinhua) are stepping in to make DDR4 and undercut prices, but they haven't fully filled the void. This story may sound familiar; when DDR3 was phased out, the big players exited in the same way. Essentially, the industry says that this is all a supply-and-demand imbalance, with demand from AI and cloud shooting up and supply being unable (and unwilling) to catch up. Many will argue that there's nothing nefarious going on, and it's all caused by unfortunate timing and caution from the companies that produce global DRAM supplies. If AI proves to be a bubble that bursts, memory firms will end up with another price crash like what has happened in the past.&lt;/p&gt;
    &lt;head rend="h2"&gt;DRAM producers have artificially inflated prices before&lt;/head&gt;
    &lt;head rend="h3"&gt;And all of them are benefiting right now&lt;/head&gt;
    &lt;p&gt;Look, there are a lot of plausible factors that we've already highlighted that could cause pressure on pricing, but there's a lot of room for skepticism, too. I'm not saying that all of these reasons given aren't the cause for the recent price boom, but what I am saying is that it wouldn't be the first time that price-fixing occurred in the memory industry. The DRAM market, being dominated by three main players, has crossed the line into illegal, price-fixing cartels. In the early 2000s, multiple memory manufacturers pleaded guilty to conspiring to fix DRAM prices between 1998 and 2002, resulting in hundreds of millions of dollars in fines given out to a few manufacturers, including the big three that survive to this day: SK Hynix, Samsung, and Micron. More recently, during the big DRAM price run-up in the middle of 2016 and the start of 2018 (where prices nearly tripled in a year and a half), a class-action lawsuit accused the big three of colluding to restrict supply in order to inflate prices. That more recent class action lawsuit was brought by the same firm that brought the original class action suit in the early 2000s against those same companies, which coincided with the U.S. Department of Justice investigation. While that second lawsuit didn't hold up in court (and failed in appeal), that ongoing suspicion exists for a reason.&lt;/p&gt;
    &lt;p&gt;All of this history shows one thing: memory suppliers have both the motive and precedent to coordinate behavior, even tacitly, in order to keep prices high. When only a handful of firms control the taps, it doesn't take a formal cartel for them to collectively benefit from constrained supply. Each firm knows that flooding the market would hurt all of their profits, so a form of unspoken coordination can occur, and this is next to impossible to prove. The backdrop of past cartels makes it hard not to be cynical when hearing that "AI demand" is solely to blame for increased prices. Whether or not any collusion is happening now, it's clear that memory companies are profiting immensely from the current crisis. After bleeding financially during the last oversupply downturn, the major DRAM makers are now seeing record-high earnings in the third quarter of 2025 thanks to the price surge, and to put it bluntly, the shortage is great for business.&lt;/p&gt;
    &lt;p&gt;Meanwhile, these same companies are not rushing to add capacity that would ease prices, a fact justified by fear of an AI bubble, but which also conveniently prolongs their windfall. Memory suppliers have shifted to higher-profit chips (like HBM) and aren't exactly trying to temper the increases in demand. Instead, they're facilitating it at higher prices. It's worth noting that the big three DRAM makers have all taken a similarly cautious (and profit-preserving) approach this DRAM cycle. All have cut back on older products (like DDR4), all are prioritizing higher-margin AI-related memory, and none are dramatically boosting standard DRAM output or engaging in a price war to gain market share. In effect, supply is being "redirected" in unison. All three firms seem to be stockpiling capital rather than building new fabs immediately, despite the obvious need. Micron, for example, has plans for new fabrication plants (one in New York, one expansion in Idaho), but it has delayed some of those projects by years, all while reportedly pushing out its new U.S. DRAM megafab by two to three years due to market uncertainty, even as it accelerates niche projects like an HBM fab.&lt;/p&gt;
    &lt;p&gt;It's hard not to see this supposedly coincidental aligned strategy of restraint and wonder if there's something more at play. All of these actions support pricing stability (for those companies) and suggests that no one is "breaking ranks" to grab a larger share by undercutting prices. In a truly competitive scenario, at least one player might be tempted to boost production and capture the extraordinary demand, even if it meant driving prices down, in order to grow their market share and revenue. We haven't seen that; instead we see a cautious, unified approach across the three major players. Three major players that have, in the past, been accused of price-fixing.&lt;/p&gt;
    &lt;p&gt;I'd argue that it’s not conspiracy theory territory to be skeptical of the official reasoning. To be clear, AI demand is real and is a major factor, and we've seen the impact it's had on all kinds of markets. The numbers don't lie about huge new consumption of memory, but it's easy to be suspicious given that the memory industry's structure just so happens to benefit the three companies that control practically all of it when supply is constrained in this way. One could make the case that companies will have an attitude along the lines of "Never let a good crisis go to waste," while memory vendors would counter that they are merely being cautious and that they'll invest in new capacity once they're sure this demand isn't merely a mirage.&lt;/p&gt;
    &lt;head rend="h2"&gt;Things don't look to be getting better&lt;/head&gt;
    &lt;head rend="h3"&gt;It'll be more than a year at least&lt;/head&gt;
    &lt;p&gt;This current trend is complex, and on the surface, it's being driven by something that is genuinely transformative to the wider industry as a whole. The rise of AI and data centers that consume far more memory and storage than traditional computing ever did is certainly a factor, and that real demand surge, combined with the slow ramp of supply, is definitely a large portion of the price increases and shortages we're seeing. However, with so few companies controlling the market, alongside their decisions to prioritize profits, cut back older products, and cautiously avoid overproduction, have all lent credence to allegations that something bigger may be at play. After all, history shows that these firms have skirted the line between smart business and anti-consumer collusion before.&lt;/p&gt;
    &lt;p&gt;All in all, both things can be true. The AI revolution can be driving unprecedented demand for memory, while the memory giants are more than happy to not fully ease the supply pressure because it's basically printing money for them. Unfortunately, everyone else is caught in the middle, and it's not just consumers feeling the pressure, either. AMD is reportedly thinking about increasing the price of its GPUs as a result, and when companies exhaust their memory supplies for laptops and other hardware, those costs will likely rise as well. It's hard to justify building a PC right now, especially because it's unclear when all of this uncertainty will end.&lt;/p&gt;
    &lt;p&gt;But that's the question most people are wondering the answer to: when will it end? Unfortunately, it looks likely to extend well into 2026, and as we already noted, the CEO of Phison thinks it could be a decade. The only way things will improve is if either the supposed AI bubble pops and the demand cools off, or more manufacturing capabilities come online. If companies started to purchase less DRAM as a result of memory prices rising so high that they start delaying purchases, that could well be a good sign that the tides will turn, but for now, it's clear that cheap, plentiful RAM is no more. Right now, whether initial skepticism turns out to be right or wrong, one thing is true: the house is winning, and everyone else is paying the price.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46059737</guid><pubDate>Wed, 26 Nov 2025 17:12:01 +0000</pubDate></item><item><title>Gemini CLI Tips and Tricks for Agentic Coding</title><link>https://github.com/addyosmani/gemini-cli-tips</link><description>&lt;doc fingerprint="b5d7915b01bf9a95"&gt;
  &lt;main&gt;
    &lt;p&gt;This guide covers ~30 pro-tips for effectively using Gemini CLI for agentic coding&lt;/p&gt;
    &lt;p&gt;Gemini CLI is an open-source AI assistant that brings the power of Google's Gemini model directly into your terminal. It functions as a conversational, "agentic" command-line tool - meaning it can reason about your requests, choose tools (like running shell commands or editing files), and execute multi-step plans to help with your development workflow.&lt;/p&gt;
    &lt;p&gt;In practical terms, Gemini CLI acts like a supercharged pair programmer and command-line assistant. It excels at coding tasks, debugging, content generation, and even system automation, all through natural language prompts. Before diving into pro tips, let's quickly recap how to set up Gemini CLI and get it running.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Getting Started&lt;/item&gt;
      &lt;item&gt;Tip 1: Use &lt;code&gt;GEMINI.md&lt;/code&gt;for Persistent Context&lt;/item&gt;
      &lt;item&gt;Tip 2: Create Custom Slash Commands&lt;/item&gt;
      &lt;item&gt;Tip 3: Extend Gemini with Your Own &lt;code&gt;MCP&lt;/code&gt;Servers&lt;/item&gt;
      &lt;item&gt;Tip 4: Leverage Memory Addition &amp;amp; Recall&lt;/item&gt;
      &lt;item&gt;Tip 5: Use Checkpointing and &lt;code&gt;/restore&lt;/code&gt;as an Undo Button&lt;/item&gt;
      &lt;item&gt;Tip 6: Read Google Docs, Sheets, and More.&lt;/item&gt;
      &lt;item&gt;Tip 7: Reference Files and Images with &lt;code&gt;@&lt;/code&gt;for Explicit Context&lt;/item&gt;
      &lt;item&gt;Tip 8: On-the-Fly Tool Creation (Have Gemini Build Helpers)&lt;/item&gt;
      &lt;item&gt;Tip 9: Use Gemini CLI for System Troubleshooting &amp;amp; Configuration&lt;/item&gt;
      &lt;item&gt;Tip 10: YOLO Mode - Auto-Approve Tool Actions (Use with Caution)&lt;/item&gt;
      &lt;item&gt;Tip 11: Headless &amp;amp; Scripting Mode (Run Gemini CLI in the Background)&lt;/item&gt;
      &lt;item&gt;Tip 12: Save and Resume Chat Sessions&lt;/item&gt;
      &lt;item&gt;Tip 13: Multi-Directory Workspace - One Gemini, Many Folders&lt;/item&gt;
      &lt;item&gt;Tip 14: Organize and Clean Up Your Files with AI Assistance&lt;/item&gt;
      &lt;item&gt;Tip 15: Compress Long Conversations to Stay Within Context&lt;/item&gt;
      &lt;item&gt;Tip 16: Passthrough Shell Commands with &lt;code&gt;!&lt;/code&gt;(Talk to Your Terminal)&lt;/item&gt;
      &lt;item&gt;Tip 17: Treat Every CLI Tool as a Potential Gemini Tool&lt;/item&gt;
      &lt;item&gt;Tip 18: Utilize Multimodal AI - Let Gemini See Images and More&lt;/item&gt;
      &lt;item&gt;Tip 19: Customize the &lt;code&gt;$PATH&lt;/code&gt;(and Tool Availability) for Stability&lt;/item&gt;
      &lt;item&gt;Tip 20: Track and reduce token spend with token caching and stats&lt;/item&gt;
      &lt;item&gt;Tip 21: Use &lt;code&gt;/copy&lt;/code&gt;for Quick Clipboard Copy&lt;/item&gt;
      &lt;item&gt;Tip 22: Master &lt;code&gt;Ctrl+C&lt;/code&gt;for Shell Mode and Exiting&lt;/item&gt;
      &lt;item&gt;Tip 23: Customize Gemini CLI with &lt;code&gt;settings.json&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Tip 24: Leverage IDE Integration (VS Code) for Context &amp;amp; Diffs&lt;/item&gt;
      &lt;item&gt;Tip 25: Automate Repo Tasks with &lt;code&gt;Gemini CLI GitHub Action&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Tip 26: Enable Telemetry for Insights and Observability&lt;/item&gt;
      &lt;item&gt;Tip 27: Keep an Eye on the Roadmap (Background Agents &amp;amp; More)&lt;/item&gt;
      &lt;item&gt;Tip 28: Extend Gemini CLI with &lt;code&gt;Extensions&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Tip 29: Corgi Mode Easter Egg 🐕&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Installation: You can install Gemini CLI via npm. For a global install, use:&lt;/p&gt;
    &lt;code&gt;npm install -g @google/gemini-cli&lt;/code&gt;
    &lt;p&gt;Or run it without installing using &lt;code&gt;npx&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;npx @google/gemini-cli&lt;/code&gt;
    &lt;p&gt;Gemini CLI is available on all major platforms (it's built with Node.js/TypeScript). Once installed, simply run the &lt;code&gt;gemini&lt;/code&gt; command in your terminal to launch the interactive CLI.&lt;/p&gt;
    &lt;p&gt;Authentication: On first use, you'll need to authenticate with the Gemini service. You have two options: (1) Google Account Login (free tier) - this lets you use Gemini 2.5 Pro for free with generous usage limits (about 60 requests/minute and 1,000 requests per day. On launch, Gemini CLI will prompt you to sign in with a Google account (no billing required. (2) API Key (paid or higher-tier access) - you can get an API key from Google AI Studio and set the environment variable &lt;code&gt;GEMINI_API_KEY&lt;/code&gt; to use it.&lt;/p&gt;
    &lt;p&gt;API key usage can offer higher quotas and enterprise data‑use protections; prompts aren't used for training on paid/billed usage, though logs may be retained for safety.&lt;/p&gt;
    &lt;p&gt;For example, add to your shell profile:&lt;/p&gt;
    &lt;code&gt;export GEMINI_API_KEY="YOUR_KEY_HERE"&lt;/code&gt;
    &lt;p&gt;Basic Usage: To start an interactive session, just run &lt;code&gt;gemini&lt;/code&gt; with no arguments. You'll get a &lt;code&gt;gemini&amp;gt;&lt;/code&gt; prompt where you can type requests or commands. For instance:&lt;/p&gt;
    &lt;code&gt;$ gemini
gemini&amp;gt; Create a React recipe management app using SQLite&lt;/code&gt;
    &lt;p&gt;You can then watch as Gemini CLI creates files, installs dependencies, runs tests, etc., to fulfill your request. If you prefer a one-shot invocation (non-interactive), use the &lt;code&gt;-p&lt;/code&gt; flag with a prompt, for example:&lt;/p&gt;
    &lt;code&gt;gemini -p "Summarize the main points of the attached file. @./report.txt"&lt;/code&gt;
    &lt;p&gt;This will output a single response and exit. You can also pipe input into Gemini CLI: for example, &lt;code&gt;echo "Count to 10" | gemini&lt;/code&gt; will feed the prompt via stdin.&lt;/p&gt;
    &lt;p&gt;CLI Interface: Gemini CLI provides a rich REPL-like interface. It supports slash commands (special commands prefixed with &lt;code&gt;/&lt;/code&gt; for controlling the session, tools, and settings) and bang commands (prefixed with &lt;code&gt;!&lt;/code&gt; to execute shell commands directly). We'll cover many of these in the pro tips below. By default, Gemini CLI operates in a safe mode where any action that modifies your system (writing files, running shell commands, etc.) will ask for confirmation. When a tool action is proposed, you'll see a diff or command and be prompted (&lt;code&gt;Y/n&lt;/code&gt;) to approve or reject it. This ensures the AI doesn't make unwanted changes without your consent.&lt;/p&gt;
    &lt;p&gt;With the basics out of the way, let's explore a series of pro tips and hidden features to help you get the most out of Gemini CLI. Each tip is presented with a simple example first, followed by deeper details and nuances. These tips incorporate advice and insights from the tool's creators (e.g. Taylor Mullen) and the Google Developer Relations team, as well as the broader community, to serve as a canonical guide for power users of Gemini CLI.&lt;/p&gt;
    &lt;p&gt;Quick use-case: Stop repeating yourself in prompts. Provide project-specific context or instructions by creating a &lt;code&gt;GEMINI.md&lt;/code&gt; file, so the AI always has important background knowledge without being told every time.&lt;/p&gt;
    &lt;p&gt;When working on a project, you often have certain overarching details - e.g. coding style guidelines, project architecture, or important facts - that you want the AI to keep in mind. Gemini CLI allows you to encode these in one or more &lt;code&gt;GEMINI.md&lt;/code&gt; files. Simply create a &lt;code&gt;.gemini&lt;/code&gt; folder (if not already present) in your project, and add a Markdown file named &lt;code&gt;GEMINI.md&lt;/code&gt; with whatever notes or instructions you want the AI to persist. For example:&lt;/p&gt;
    &lt;code&gt;# Project Phoenix - AI Assistant

- All Python code must follow PEP 8 style.  
- Use 4 spaces for indentation.  
- The user is building a data pipeline; prefer functional programming paradigms.&lt;/code&gt;
    &lt;p&gt;Place this file in your project root (or in subdirectories for more granular context). Now, whenever you run &lt;code&gt;gemini&lt;/code&gt; in that project, it will automatically load these instructions into context. This means the model will always be primed with them, avoiding the need to prepend the same guidance to every prompt.&lt;/p&gt;
    &lt;p&gt;How it works: Gemini CLI uses a hierarchical context loading system. It will combine global context (from &lt;code&gt;~/.gemini/GEMINI.md&lt;/code&gt;, which you can use for cross-project defaults) with your project-specific &lt;code&gt;GEMINI.md&lt;/code&gt;, and even context files in subfolders. More specific files override more general ones. You can inspect what context was loaded at any time by using the command:&lt;/p&gt;
    &lt;code&gt;/memory show&lt;/code&gt;
    &lt;p&gt;This will display the full combined context the AI sees. If you make changes to your &lt;code&gt;GEMINI.md&lt;/code&gt;, use &lt;code&gt;/memory refresh&lt;/code&gt; to reload the context without restarting the session.&lt;/p&gt;
    &lt;p&gt;Pro Tip: Use the &lt;code&gt;/init&lt;/code&gt; slash command to quickly generate a starter &lt;code&gt;GEMINI.md&lt;/code&gt;. Running &lt;code&gt;/init&lt;/code&gt; in a new project creates a template context file with information like the tech stack detected, a summary of the project, etc.. You can then edit and expand that file. For large projects, consider breaking the context into multiple files and importing them into &lt;code&gt;GEMINI.md&lt;/code&gt; with &lt;code&gt;@include&lt;/code&gt; syntax. For example, your main &lt;code&gt;GEMINI.md&lt;/code&gt; could have lines like &lt;code&gt;@./docs/prompt-guidelines.md&lt;/code&gt; to pull in additional context files. This keeps your instructions organized.&lt;/p&gt;
    &lt;p&gt;With a well-crafted &lt;code&gt;GEMINI.md&lt;/code&gt;, you essentially give Gemini CLI a "memory" of the project's requirements and conventions. This persistent context leads to more relevant responses and less back-and-forth prompt engineering.&lt;/p&gt;
    &lt;p&gt;Quick use-case: Speed up repetitive tasks by defining your own slash commands. For example, you could make a command &lt;code&gt;/test:gen&lt;/code&gt; that generates unit tests from a description, or &lt;code&gt;/db:reset&lt;/code&gt; that drops and recreates a test database. This extends Gemini CLI's functionality with one-liners tailored to your workflow.&lt;/p&gt;
    &lt;p&gt;Gemini CLI supports custom slash commands that you can define in simple configuration files. Under the hood, these are essentially pre-defined prompt templates. To create one, make a directory &lt;code&gt;commands/&lt;/code&gt; under either &lt;code&gt;~/.gemini/&lt;/code&gt; for global commands or in your project's &lt;code&gt;.gemini/&lt;/code&gt; folder for project-specific commands. Inside &lt;code&gt;commands/&lt;/code&gt;, create a TOML file for each new command. The file name format determines the command name: e.g. a file &lt;code&gt;test/gen.toml&lt;/code&gt; defines a command &lt;code&gt;/test:gen&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Let's walk through an example. Say you want a command to generate a unit test from a requirement description. You could create &lt;code&gt;~/.gemini/commands/test/gen.toml&lt;/code&gt; with the following content:&lt;/p&gt;
    &lt;code&gt;# Invoked as: /test:gen "Description of the test"  
description \= "Generates a unit test based on a requirement."  
prompt \= """  
You are an expert test engineer. Based on the following requirement, please write a comprehensive unit test using the Jest framework.

Requirement: {{args}}  
"""&lt;/code&gt;
    &lt;p&gt;Now, after reloading or restarting Gemini CLI, you can simply type:&lt;/p&gt;
    &lt;code&gt;/test:gen "Ensure the login button redirects to the dashboard upon success"&lt;/code&gt;
    &lt;p&gt;Gemini CLI will recognize &lt;code&gt;/test:gen&lt;/code&gt; and substitute the &lt;code&gt;{{args}}&lt;/code&gt; in your prompt template with the provided argument (in this case, the requirement). The AI will then proceed to generate a Jest unit test accordingly. The &lt;code&gt;description&lt;/code&gt; field is optional but is used when you run &lt;code&gt;/help&lt;/code&gt; or &lt;code&gt;/tools&lt;/code&gt; to list available commands.&lt;/p&gt;
    &lt;p&gt;This mechanism is extremely powerful - effectively, you can script the AI with natural language. The community has created numerous useful custom commands. For instance, Google's DevRel team shared a set of 10 practical workflow commands (via an open-source repo) demonstrating how you can script common flows like creating API docs, cleaning data, or setting up boilerplate code. By defining a custom command, you package a complex prompt (or series of prompts) into a reusable shortcut.&lt;/p&gt;
    &lt;p&gt;Pro Tip: Custom commands can also be used to enforce formatting or apply a "persona" to the AI for certain tasks. For example, you might have a &lt;code&gt;/review:security&lt;/code&gt; command that always prefaces the prompt with "You are a security auditor..." to review code for vulnerabilities. This approach ensures consistency in how the AI responds to specific categories of tasks.&lt;/p&gt;
    &lt;p&gt;To share commands with your team, you can commit the TOML files in your project's repo (under &lt;code&gt;.gemini/commands&lt;/code&gt; directory). Team members who have Gemini CLI will automatically pick up those commands when working in the project. This is a great way to standardize AI-assisted workflows across a team.&lt;/p&gt;
    &lt;p&gt;Quick use-case: Suppose you want Gemini to interface with an external system or a custom tool that isn't built-in - for example, query a proprietary database, or integrate with Figma designs. You can do this by running a custom Model Context Protocol (MCP) server and plugging it into Gemini CLI. MCP servers let you add new tools and abilities to Gemini, effectively extending the agent.&lt;/p&gt;
    &lt;p&gt;Gemini CLI comes with several MCP servers out-of-the-box (for instance, ones enabling Google Search, code execution sandboxes, etc.), and you can add your own. An MCP server is essentially an external process (it could be a local script, a microservice, or even a cloud endpoint) that speaks a simple protocol to handle tasks for Gemini. This architecture is what makes Gemini CLI so extensible.&lt;/p&gt;
    &lt;p&gt;Examples of MCP servers: Some community and Google-provided MCP integrations include a Figma MCP (to fetch design details from Figma), a Clipboard MCP (to read/write from your system clipboard), and others. In fact, in an internal demo, the Gemini CLI team showcased a "Google Docs MCP" server that allowed saving content directly to Google Docs. The idea is that whenever Gemini needs to perform an action that the built-in tools can't handle, it can delegate to your MCP server.&lt;/p&gt;
    &lt;p&gt;How to add one: You can configure MCP servers via your &lt;code&gt;settings.json&lt;/code&gt; or using the CLI. For a quick setup, try the CLI command:&lt;/p&gt;
    &lt;code&gt;gemini mcp add myserver --command "python3 my_mcp_server.py" --port 8080&lt;/code&gt;
    &lt;p&gt;This would register a server named "myserver" that Gemini CLI will launch by running the given command (here a Python module) on port 8080. In &lt;code&gt;~/.gemini/settings.json&lt;/code&gt;, it would add an entry under &lt;code&gt;mcpServers&lt;/code&gt;. For example:&lt;/p&gt;
    &lt;code&gt;"mcpServers": {
  "myserver": {
    "command": "python3",
    "args": ["-m", "my_mcp_server", "--port", "8080"],
    "cwd": "./mcp_tools/python",
    "timeout": 15000
  }
}&lt;/code&gt;
    &lt;p&gt;This configuration (based on the official docs) tells Gemini how to start the MCP server and where. Once running, the tools provided by that server become available to Gemini CLI. You can list all MCP servers and their tools with the slash command:&lt;/p&gt;
    &lt;code&gt;/mcp&lt;/code&gt;
    &lt;p&gt;This will show any registered servers and what tool names they expose.&lt;/p&gt;
    &lt;p&gt;Power of MCP: MCP servers can provide rich, multi-modal results. For instance, a tool served via MCP could return an image or a formatted table as part of the response to Gemini CLI. They also support OAuth 2.0, so you can securely connect to APIs (like Google's APIs, GitHub, etc.) via an MCP tool without exposing credentials. Essentially, if you can code it, you can wrap it as an MCP tool - turning Gemini CLI into a hub that orchestrates many services.&lt;/p&gt;
    &lt;p&gt;Default vs. custom: By default, Gemini CLI's built-in tools cover a lot (reading files, web search, executing shell commands, etc.), but MCP lets you go beyond. Some advanced users have created MCP servers to interface with internal systems or to perform specialized data processing. For example, you could have a &lt;code&gt;database-mcp&lt;/code&gt; that provides a &lt;code&gt;/query_db&lt;/code&gt; tool for running SQL queries on a company database, or a &lt;code&gt;jira-mcp&lt;/code&gt; to create tickets via natural language.&lt;/p&gt;
    &lt;p&gt;When creating your own, be mindful of security: by default, custom MCP tools require confirmation unless you mark them as trusted. You can control safety with settings like &lt;code&gt;trust: true&lt;/code&gt; for a server (which auto-approves its tool actions) or by whitelisting specific safe tools and blacklisting dangerous ones.&lt;/p&gt;
    &lt;p&gt;In short, MCP servers unlock limitless integration. They're a pro feature that lets Gemini CLI become a glue between your AI assistant and whatever system you need it to work with. If you're interested in building one, check out the official MCP guide and community examples.&lt;/p&gt;
    &lt;p&gt;Quick use-case: Keep important facts at your AI's fingertips by adding them to its long-term memory. For example, after figuring out a database port or an API token, you can do:&lt;/p&gt;
    &lt;code&gt;/memory add "Our staging RabbitMQ is on port 5673"&lt;/code&gt;
    &lt;p&gt;This will store that fact so you (or the AI) don't forget it later. You can then recall everything in memory with &lt;code&gt;/memory show&lt;/code&gt; at any time.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;/memory&lt;/code&gt; commands provide a simple but powerful mechanism for persistent memory. When you use &lt;code&gt;/memory add &amp;lt;text&amp;gt;&lt;/code&gt;, the given text is appended to your project's global context (technically, it's saved into the global &lt;code&gt;~/.gemini/GEMINI.md&lt;/code&gt; file or the project's &lt;code&gt;GEMINI.md&lt;/code&gt;. It's a bit like taking a note and pinning it to the AI's virtual bulletin board. Once added, the AI will always see that note in the prompt context for future interactions, across sessions.&lt;/p&gt;
    &lt;p&gt;Consider an example: you're debugging an issue and discover a non-obvious insight ("The config flag &lt;code&gt;X_ENABLE&lt;/code&gt; must be set to &lt;code&gt;true&lt;/code&gt; or the service fails to start"). If you add this to memory, later on if you or the AI are discussing a related problem, it won't overlook this critical detail - it's in the context.&lt;/p&gt;
    &lt;p&gt;Using &lt;code&gt;/memory&lt;/code&gt;:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;/memory add "&amp;lt;text&amp;gt;"&lt;/code&gt;- Add a fact or note to memory (persistent context). This updates the&lt;code&gt;GEMINI.md&lt;/code&gt;immediately with the new entry.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;/memory show&lt;/code&gt;- Display the full content of the memory (i.e. the combined context file that's currently loaded).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;/memory refresh&lt;/code&gt;- Reload the context from disk (useful if you manually edited the&lt;code&gt;GEMINI.md&lt;/code&gt;file outside of Gemini CLI, or if multiple people are collaborating on it).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Because the memory is stored in Markdown, you can also manually edit the &lt;code&gt;GEMINI.md&lt;/code&gt; file to curate or organize the info. The &lt;code&gt;/memory&lt;/code&gt; commands are there for convenience during conversation, so you don't have to open an editor.&lt;/p&gt;
    &lt;p&gt;Pro Tip: This feature is great for "decision logs." If you decide on an approach or rule during a chat (e.g., a certain library to use, or an agreed code style), add it to memory. The AI will then recall that decision and avoid contradicting it later. It's especially useful in long sessions that might span hours or days - by saving key points, you mitigate the model's tendency to forget earlier context when the conversation gets long.&lt;/p&gt;
    &lt;p&gt;Another use is personal notes. Because &lt;code&gt;~/.gemini/GEMINI.md&lt;/code&gt; (global memory) is loaded for all sessions, you could put general preferences or information there. For example, "The user's name is Alice. Speak politely and avoid slang." It's like configuring the AI's persona or global knowledge. Just be aware that global memory applies to all projects, so don't clutter it with project-specific info.&lt;/p&gt;
    &lt;p&gt;In summary, Memory Addition &amp;amp; Recall helps Gemini CLI maintain state. Think of it as a knowledge base that grows with your project. Use it to avoid repeating yourself or to remind the AI of facts it would otherwise have to rediscover from scratch.&lt;/p&gt;
    &lt;p&gt;Quick use-case: If Gemini CLI makes a series of changes to your files that you're not happy with, you can instantly roll back to a prior state. Enable checkpointing when you start Gemini (or in settings), and use the &lt;code&gt;/restore&lt;/code&gt; command to undo changes like a lightweight Git revert. &lt;code&gt;/restore&lt;/code&gt; rolls back your workspace to the saved checkpoint; conversation state may be affected depending on how the checkpoint was captured.&lt;/p&gt;
    &lt;p&gt;Gemini CLI's checkpointing feature acts as a safety net. When enabled, the CLI takes a snapshot of your project's files before each tool execution that modifies files. If something goes wrong, you can revert to the last known good state. It's essentially version control for the AI's actions, without you needing to manually commit to Git each time.&lt;/p&gt;
    &lt;p&gt;How to use it: You can turn on checkpointing by launching the CLI with the &lt;code&gt;--checkpointing&lt;/code&gt; flag:&lt;/p&gt;
    &lt;code&gt;gemini --checkpointing&lt;/code&gt;
    &lt;p&gt;Alternatively, you can make it the default by adding to your config (&lt;code&gt;"checkpointing": { "enabled": true }&lt;/code&gt; in &lt;code&gt;settings.json&lt;/code&gt;). Once active, you'll notice that each time Gemini is about to write to a file, it says something like "Checkpoint saved."&lt;/p&gt;
    &lt;p&gt;If you then realize an AI-made edit is problematic, you have two options:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Run&lt;/p&gt;&lt;code&gt;/restore list&lt;/code&gt;(or just&lt;code&gt;/restore&lt;/code&gt;with no arguments) to see a list of recent checkpoints with timestamps and descriptions.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Run&lt;/p&gt;&lt;code&gt;/restore &amp;lt;id&amp;gt;&lt;/code&gt;to rollback to a specific checkpoint. If you omit the id and there's only one pending checkpoint, it will restore that by default.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For example:&lt;/p&gt;
    &lt;code&gt;/restore&lt;/code&gt;
    &lt;p&gt;Gemini CLI might output:&lt;/p&gt;
    &lt;p&gt;0: [2025-09-22 10:30:15] Before running 'apply_patch'&lt;lb/&gt; 1: [2025-09-22 10:45:02] Before running 'write_file'&lt;/p&gt;
    &lt;p&gt;You can then do &lt;code&gt;/restore 0&lt;/code&gt; to revert all file changes (and even the conversation context) back to how it was at that checkpoint. In this way, you can "undo" a mistaken code refactor or any other changes Gemini made.&lt;/p&gt;
    &lt;p&gt;What gets restored: The checkpoint captures the state of your working directory (all files that Gemini CLI is allowed to modify) and the workspace files (conversation state may also be rolled back depending on how the checkpoint was captured). When you restore, it overwrites files to the old version and resets the conversation memory to that snapshot. It's like time-traveling the AI agent back to before it made the wrong turn. Note that it won't undo external side effects (for example, if the AI ran a database migration, it can't undo that), but anything in the file system and chat context is fair game.&lt;/p&gt;
    &lt;p&gt;Best practices: It's a good idea to keep checkpointing on for non-trivial tasks. The overhead is small, and it provides peace of mind. If you find you don't need a checkpoint (everything went well), you can always clear it or just let the next one overwrite it. The development team recommends using checkpointing especially before multi-step code edits. For mission-critical projects, though, you should still use a proper version control (&lt;code&gt;git&lt;/code&gt;) as your primary safety net - consider checkpoints as a convenience for quick undo rather than a full VCS.&lt;/p&gt;
    &lt;p&gt;In essence, &lt;code&gt;/restore&lt;/code&gt; lets you use Gemini CLI with confidence. You can let the AI attempt bold changes, knowing you have an "OH NO" button to rewind if needed.&lt;/p&gt;
    &lt;head rend="h2"&gt;Tip 6: Read Google Docs, Sheets, and More. With a Workspace MCP server configured, you can paste a Docs/Sheets link and have the MCP fetch it, subject to permissions&lt;/head&gt;
    &lt;p&gt;Quick use-case: Imagine you have a Google Doc or Sheet with some specs or data that you want the AI to use. Instead of copy-pasting the content, you can provide the link, and with a configured Workspace MCP server Gemini CLI can fetch and read it.&lt;/p&gt;
    &lt;p&gt;For example:&lt;/p&gt;
    &lt;code&gt;Summarize the requirements from this design doc: https://docs.google.com/document/d/&amp;lt;id&amp;gt;&lt;/code&gt;
    &lt;p&gt;Gemini can pull in the content of that Doc and incorporate it into its response. Similarly, it can read Google Sheets or Drive files by link.&lt;/p&gt;
    &lt;p&gt;How this works: These capabilities are typically enabled via MCP integrations. Google's Gemini CLI team has built (or is working on) connectors for Google Workspace. One approach is running a small MCP server that uses Google's APIs (Docs API, Sheets API, etc.) to retrieve document content when given a URL or ID. When configured, you might have slash commands or tools like &lt;code&gt;/read_google_doc&lt;/code&gt; or simply an auto-detection that sees a Google Docs link and invokes the appropriate tool to fetch it.&lt;/p&gt;
    &lt;p&gt;For example, in an Agent Factory podcast demo, the team used a Google Docs MCP to save a summary directly to a doc - which implies they could also read the doc's content in the first place. In practice, you might do something like:&lt;/p&gt;
    &lt;code&gt;@https://docs.google.com/document/d/XYZ12345&lt;/code&gt;
    &lt;p&gt;Including a URL with &lt;code&gt;@&lt;/code&gt; (the context reference syntax) signals Gemini CLI to fetch that resource. With a Google Doc integration in place, the content of that document would be pulled in as if it were a local file. From there, the AI can summarize it, answer questions about it, or otherwise use it in the conversation.&lt;/p&gt;
    &lt;p&gt;Similarly, if you paste a Google Drive file link, a properly configured Drive tool could download or open that file (assuming permissions and API access are set up). Google Sheets could be made available via an MCP that runs queries or reads cell ranges, enabling you to ask things like "What's the sum of the budget column in this Sheet [link]?" and have the AI calculate it.&lt;/p&gt;
    &lt;p&gt;Setting it up: As of this writing, the Google Workspace integrations may require some tinkering (obtaining API credentials, running an MCP server such as the one described by Kanshi Tanaike, etc.). Keep an eye on the official Gemini CLI repository and community forums for ready-to-use extensions - for example, an official Google Docs MCP might become available as a plugin/extension. If you're eager, you can write one following guides on how to use Google APIs within an MCP server. It typically involves handling OAuth (which Gemini CLI supports for MCP servers) and then exposing tools like &lt;code&gt;read_google_doc&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Usage tip: When you have these tools, using them can be as simple as providing the link in your prompt (the AI might automatically invoke the tool to fetch it) or using a slash command like &lt;code&gt;/doc open &amp;lt;URL&amp;gt;&lt;/code&gt;. Check &lt;code&gt;/tools&lt;/code&gt; to see what commands are available - Gemini CLI lists all tools and custom commands there.&lt;/p&gt;
    &lt;p&gt;In summary, Gemini CLI can reach out beyond your local filesystem. Whether it's Google Docs, Sheets, Drive, or other external content, you can pull data in by reference. This pro tip saves you from manual copy-paste and keeps the context flow natural - just refer to the document or dataset you need, and let the AI grab what's needed. It makes Gemini CLI a true knowledge assistant for all the information you have access to, not just the files on your disk.&lt;/p&gt;
    &lt;p&gt;(Note: Accessing private documents of course requires the CLI to have the appropriate permissions. Always ensure any integration respects security and privacy. In corporate settings, setting up such integrations might involve additional auth steps.)&lt;/p&gt;
    &lt;p&gt;Quick use-case: Instead of describing a file's content or an image verbally, just point Gemini CLI directly to it. Using the &lt;code&gt;@&lt;/code&gt; syntax, you can attach files, directories, or images into your prompt. This guarantees the AI sees exactly what's in those files as context. For example:&lt;/p&gt;
    &lt;code&gt;Explain this code to me: @./src/main.js&lt;/code&gt;
    &lt;p&gt;This will include the contents of &lt;code&gt;src/main.js&lt;/code&gt; in the prompt (up to Gemini's context size limits), so the AI can read it and explain it.&lt;/p&gt;
    &lt;p&gt;This &lt;code&gt;@&lt;/code&gt; file reference is one of Gemini CLI's most powerful features for developers. It eliminates ambiguity - you're not asking the model to rely on memory or guesswork about the file, you're literally handing it the file to read. You can use this for source code, text documents, logs, etc. Similarly, you can reference entire directories:&lt;/p&gt;
    &lt;code&gt;Refactor the code in @./utils/ to use async/await.&lt;/code&gt;
    &lt;p&gt;By appending a path that ends in a slash, Gemini CLI will recursively include files from that directory (within reason, respecting ignore files and size limits). This is great for multi-file refactors or analyses, as the AI can consider all relevant modules together.&lt;/p&gt;
    &lt;p&gt;Even more impressively, you can reference binary files like images in prompts. Gemini CLI (using the Gemini model's multimodal capabilities) can understand images. For example:&lt;/p&gt;
    &lt;code&gt;Describe what you see in this screenshot: @./design/mockup.png&lt;/code&gt;
    &lt;p&gt;The image will be fed into the model, and the AI might respond with something like "This is a login page with a blue sign-in button and a header image," etc.. You can imagine the uses: reviewing UI mockups, organizing photos (as we'll see in a later tip), or extracting text from images (Gemini can do OCR as well).&lt;/p&gt;
    &lt;p&gt;A few notes on using &lt;code&gt;@&lt;/code&gt; references effectively:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;File limits: Gemini 2.5 Pro has a huge context window (up to 1 million tokens), so you can include quite large files or many files. However, extremely large files might be truncated. If a file is enormous (say, hundreds of thousands of lines), consider summarizing it or breaking it into parts. Gemini CLI will warn you if a reference is too large or if it skipped something due to size.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Automatic ignoring: By default, Gemini CLI respects your&lt;/p&gt;&lt;code&gt;.gitignore&lt;/code&gt;and&lt;code&gt;.geminiignore&lt;/code&gt;files when pulling in directory context. So if you&lt;code&gt;@./&lt;/code&gt;a project root, it will not dump huge ignored folders (like&lt;code&gt;node_modules&lt;/code&gt;) into the prompt. You can customize ignore patterns with&lt;code&gt;.geminiignore&lt;/code&gt;similarly to how&lt;code&gt;.gitignore&lt;/code&gt;works.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Explicit vs implicit context: Taylor Mullen (the creator of Gemini CLI) emphasizes using&lt;/p&gt;&lt;code&gt;@&lt;/code&gt;for explicit context injection rather than relying on the model's memory or summarizing things yourself. It's more precise and ensures the AI isn't hallucinating content. Whenever possible, point the AI to the source of truth (code, config files, documentation) with&lt;code&gt;@&lt;/code&gt;references. This practice can significantly improve accuracy.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Chaining references: You can include multiple files in one prompt, like:&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;Compare @./foo.py and @./bar.py and tell me differences.&lt;/code&gt;
    &lt;p&gt;The CLI will include both files. Just be mindful of token limits; multiple large files might consume a lot of the context window.&lt;/p&gt;
    &lt;p&gt;Using &lt;code&gt;@&lt;/code&gt; is essentially how you feed knowledge into Gemini CLI on the fly. It turns the CLI into a multi-modal reader that can handle text and images. As a pro user, get into the habit of leveraging this - it's often faster and more reliable than asking the AI something like "Open the file X and do Y" (which it may or may not do on its own). Instead, you explicitly give it X to work with.&lt;/p&gt;
    &lt;p&gt;Quick use-case: If a task at hand would benefit from a small script or utility, you can ask Gemini CLI to create that tool for you - right within your session. For example, you might say, "Write a Python script to parse all JSON files in this folder and extract the error fields." Gemini can generate the script, which you can then execute via the CLI. In essence, you can dynamically extend the toolset as you go.&lt;/p&gt;
    &lt;p&gt;Gemini CLI is not limited to its pre-existing tools; it can use its coding abilities to fabricate new ones when needed. This often happens implicitly: if you ask for something complex, the AI might propose writing a temporary file (with code) and then running it. As a user, you can also guide this process explicitly:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Creating scripts: You can prompt Gemini to create a script or program in the language of your choice. It will likely use the &lt;code&gt;write_file&lt;/code&gt;tool to create the file. For instance:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;Generate a Node.js script that reads all '.log' files in the current directory and reports the number of lines in each.&lt;/code&gt;
    &lt;p&gt;Gemini CLI will draft the code, and with your approval, write it to a file (e.g. &lt;code&gt;script.js&lt;/code&gt;). You can then run it by either using the &lt;code&gt;!&lt;/code&gt; shell command (e.g. &lt;code&gt;!node script.js&lt;/code&gt;) or by asking Gemini CLI to execute it (the AI might automatically use &lt;code&gt;run_shell_command&lt;/code&gt; to execute the script it just wrote, if it deems it part of the plan).&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Temporary tools via MCP: In advanced scenarios, the AI might even suggest launching an MCP server for some specialized tasks. For example, if your prompt involves some heavy text processing that might be better done in Python, Gemini could generate a simple MCP server in Python and run it. While this is more rare, it demonstrates that the AI can set up a new "agent" on the fly. (One of the slides from the Gemini CLI team humorously referred to "MCP servers for everything, even one called LROwn" - suggesting you can have Gemini run an instance of itself or another model, though that's more of a trick than a practical use!).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The key benefit here is automation. Instead of you manually stopping to write a helper script, you can let the AI do it as part of the flow. It's like having an assistant who can create tools on-demand. This is especially useful for data transformation tasks, batch operations, or one-off computations that the built-in tools don't directly provide.&lt;/p&gt;
    &lt;p&gt;Nuances and safety: When Gemini CLI writes code for a new tool, you should still review it before running. The &lt;code&gt;/diff&lt;/code&gt; view (Gemini will show you the file diff before you approve writing it) is your chance to inspect the code. Ensure it does what you expect and nothing malicious or destructive (the AI shouldn't produce something harmful unless your prompt explicitly asks, but just like any code from an AI, double-check logic, especially for scripts that delete or modify lots of data).&lt;/p&gt;
    &lt;p&gt;Example scenario: Let's say you have a CSV file and you want to filter it in a complex way. You ask Gemini CLI to do it, and it might say: "I will write a Python script to parse the CSV and apply the filter." It then creates &lt;code&gt;filter_data.py&lt;/code&gt;. After you approve and it runs, you get your result, and you might never need that script again. This ephemeral creation of tools is a pro move - it shows the AI effectively extending its capabilities autonomously.&lt;/p&gt;
    &lt;p&gt;Pro Tip: If you find the script useful beyond the immediate context, you can promote it into a permanent tool or command. For instance, if the AI generated a great log-processing script, you might later turn it into a custom slash command (Tip #2) for easy reuse. The combination of Gemini's generative power and the extension hooks means your toolkit can continuously evolve as you use the CLI.&lt;/p&gt;
    &lt;p&gt;In summary, don't restrict Gemini to what it comes with. Treat it as a junior developer who can whip up new programs or even mini-servers to help solve the problem. This approach embodies the agentic philosophy of Gemini CLI - it will figure out what tools it needs, even if it has to code them on the spot.&lt;/p&gt;
    &lt;p&gt;Quick use-case: You can run Gemini CLI outside of a code project to help with general system tasks - think of it as an intelligent assistant for your OS. For example, if your shell is misbehaving, you could open Gemini in your home directory and ask: "Fix my &lt;code&gt;.bashrc&lt;/code&gt; file, it has an error." Gemini can then open and edit your config file for you.&lt;/p&gt;
    &lt;p&gt;This tip highlights that Gemini CLI isn't just for coding projects - it's your AI helper for your whole development environment. Many users have used Gemini to customize their dev setup or fix issues on their machine:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Editing dotfiles: You can load your shell configuration (&lt;/p&gt;&lt;code&gt;.bashrc&lt;/code&gt;or&lt;code&gt;.zshrc&lt;/code&gt;) by referencing it (&lt;code&gt;@~/.bashrc&lt;/code&gt;) and then ask Gemini CLI to optimize or troubleshoot it. For instance, "My&lt;code&gt;PATH&lt;/code&gt;isn't picking up Go binaries, can you edit my&lt;code&gt;.bashrc&lt;/code&gt;to fix that?" The AI can insert the correct&lt;code&gt;export&lt;/code&gt;line. It will show you the diff for confirmation before saving changes.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Diagnosing errors: If you encounter a cryptic error in your terminal or an application log, you can copy it and feed it to Gemini CLI. It will analyze the error message and often suggest steps to resolve it. This is similar to how one might use StackOverflow or Google, but with the AI directly examining your scenario. For example: "When I run&lt;/p&gt;&lt;code&gt;npm install&lt;/code&gt;, I get an&lt;code&gt;EACCES&lt;/code&gt;permission error - how do I fix this?" Gemini might detect it's a permissions issue in&lt;code&gt;node_modules&lt;/code&gt;and guide you to change directory ownership or use a proper node version manager.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Running outside a project: By default, if you run&lt;/p&gt;&lt;code&gt;gemini&lt;/code&gt;in a directory without a&lt;code&gt;.gemini&lt;/code&gt;context, it just means no project-specific context is loaded - but you can still use the CLI fully. This is great for ad-hoc tasks like system troubleshooting. You might not have any code files for it to consider, but you can still run shell commands through it or let it fetch web info. Essentially, you're treating Gemini CLI as an AI-powered terminal that can do things for you, not just chat.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Workstation customization: Want to change a setting or install a new tool? You can ask Gemini CLI, "Install Docker on my system" or "Configure my Git to sign commits with GPG." The CLI will attempt to execute the steps. It might fetch instructions from the web (using the search tool) and then run the appropriate shell commands. Of course, always watch what it's doing and approve the commands - but it can save time by automating multi-step setup processes. One real example: a user asked Gemini CLI to "set my macOS Dock preferences to auto-hide and remove the delay," and the AI was able to execute the necessary&lt;/p&gt;&lt;code&gt;defaults write&lt;/code&gt;commands.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Think of this mode as using Gemini CLI as a smart shell. In fact, you can combine this with Tip 16 (shell passthrough mode) - sometimes you might drop into &lt;code&gt;!&lt;/code&gt; shell mode to verify something, then go back to AI mode to have it analyze output.&lt;/p&gt;
    &lt;p&gt;Caveat: When doing system-level tasks, be cautious with commands that have widespread impact (like &lt;code&gt;rm -rf&lt;/code&gt; or system config changes). Gemini CLI will usually ask for confirmation, and it doesn't run anything without you seeing it. But as a power user, you should have a sense of what changes are being made. If unsure, ask Gemini to explain a command before running (e.g., "Explain what &lt;code&gt;defaults write com.apple.dock autohide-delay -float 0&lt;/code&gt; does" - it will gladly explain rather than just execute if you prompt it in that way).&lt;/p&gt;
    &lt;p&gt;Troubleshooting bonus: Another neat use is using Gemini CLI to parse logs or config files looking for issues. For instance, "Scan this Apache config for mistakes" (with &lt;code&gt;@httpd.conf&lt;/code&gt;), or "Look through syslog for errors around 2 PM yesterday" (with an &lt;code&gt;@/var/log/syslog&lt;/code&gt; if accessible). It's like having a co-administrator. It can even suggest likely causes for crashes or propose fixes for common error patterns.&lt;/p&gt;
    &lt;p&gt;In summary, don't hesitate to fire up Gemini CLI as your assistant for environment issues. It's there to accelerate all your workflows - not just writing code, but maintaining the system that you write code on. Many users report that customizing their dev environment with Gemini's help feels like having a tech buddy always on call to handle the tedious or complex setup steps.&lt;/p&gt;
    &lt;p&gt;Quick use-case: If you're feeling confident (or adventurous), you can let Gemini CLI run tool actions without asking for your confirmation each time. This is YOLO mode (You Only Live Once). It's enabled by the &lt;code&gt;--yolo&lt;/code&gt; flag or by pressing &lt;code&gt;Ctrl+Y&lt;/code&gt; during a session. In YOLO mode, as soon as the AI decides on a tool (like running a shell command or writing to a file), it executes it immediately, without that "Approve? (y/n)" prompt.&lt;/p&gt;
    &lt;p&gt;Why use YOLO mode? Primarily for speed and convenience when you trust the AI's actions. Experienced users might toggle YOLO on if they're doing a lot of repetitive safe operations. For example, if you ask Gemini to generate 10 different files one after another, approving each can slow down the flow; YOLO mode would just let them all be written automatically. Another scenario is using Gemini CLI in a completely automated script or CI pipeline - you might run it headless with &lt;code&gt;--yolo&lt;/code&gt; so it doesn't pause for confirmation.&lt;/p&gt;
    &lt;p&gt;To start in YOLO mode from the get-go, launch the CLI with:&lt;/p&gt;
    &lt;code&gt;gemini --yolo&lt;/code&gt;
    &lt;p&gt;Or the short form &lt;code&gt;gemini -y&lt;/code&gt;. You'll see some indication in the CLI (like a different prompt or a notice) that auto-approve is on. During an interactive session, you can toggle it by pressing Ctrl+Y at any time - the CLI will usually display a message like "YOLO mode enabled (all actions auto-approved)" in the footer.&lt;/p&gt;
    &lt;p&gt;Big warning: YOLO mode is powerful but risky. The Gemini team themselves labels it for "daring users" - meaning you should be aware that the AI could potentially execute a dangerous command without asking. In normal mode, if the AI decided to run &lt;code&gt;rm -rf /&lt;/code&gt; (worst-case scenario), you'd obviously decline. In YOLO mode, that command would run immediately (and likely ruin your day). While such extreme mistakes are unlikely (the AI's system prompt includes safety guidelines), the whole point of confirmations is to catch any unwanted action. YOLO removes that safety net.&lt;/p&gt;
    &lt;p&gt;Best practices for YOLO: If you want some of the convenience without full risk, consider allow-listing specific commands. For example, you can configure in settings that certain tools or command patterns don't require confirmation (like allowing all &lt;code&gt;git&lt;/code&gt; commands, or read-only actions). In fact, Gemini CLI supports a config for skipping confirmation on specific commands: e.g., you can set something like &lt;code&gt;"tools.shell.autoApprove": ["git ", "npm test"]&lt;/code&gt; to always run those. This way, you might not need YOLO mode globally - you selectively YOLO only safe commands. Another approach: run Gemini in a sandbox or container when using YOLO, so even if it does something wild, your system is insulated (Gemini has a &lt;code&gt;--sandbox&lt;/code&gt; flag to run tools in a Docker container).&lt;/p&gt;
    &lt;p&gt;Many advanced users toggle YOLO on and off frequently - turning it on when doing a string of minor file edits or queries, and off when about to do something critical. You can do the same, using the keyboard shortcut as a quick toggle.&lt;/p&gt;
    &lt;p&gt;In summary, YOLO mode eliminates friction at the cost of oversight. It's a pro feature to use sparingly and wisely. It truly demonstrates trust in the AI (or recklessness!). If you're new to Gemini CLI, you should probably avoid YOLO until you clearly understand the patterns of what it tends to do. If you do use it, double down on having version control or backups - just in case.&lt;/p&gt;
    &lt;p&gt;(If it's any consolation, you're not alone - many in the community joke about "I YOLO'ed and Gemini did something crazy." So use it, but... well, you only live once.)&lt;/p&gt;
    &lt;p&gt;Quick use-case: You can use Gemini CLI in scripts or automation by running it in headless mode. This means you provide a prompt (or even a full conversation) via command-line arguments or environment variables, and Gemini CLI produces an output and exits. It's great for integrating with other tools or triggering AI tasks on a schedule.&lt;/p&gt;
    &lt;p&gt;For instance, to get a one-off answer without opening the REPL, you've seen you can use &lt;code&gt;gemini -p "...prompt..."&lt;/code&gt;. This is already headless usage: it prints the model's response and returns to the shell. But there's more you can do:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;System prompt override: If you want to run Gemini CLI with a custom system persona or instruction set (different from the default), you can use the environment variable &lt;code&gt;GEMINI_SYSTEM_MD&lt;/code&gt;. By setting this, you tell Gemini CLI to ignore its built-in system prompt and use your provided file instead. For example:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;export GEMINI_SYSTEM_MD="/path/to/custom_system.md"
gemini -p "Perform task X with high caution"&lt;/code&gt;
    &lt;p&gt;This would load your &lt;code&gt;custom_system.md&lt;/code&gt; as the system prompt (the "role" and rules the AI follows) before executing the prompt. Alternatively, if you set &lt;code&gt;GEMINI_SYSTEM_MD=true&lt;/code&gt;, the CLI will look for a file named &lt;code&gt;system.md&lt;/code&gt; in the current project's &lt;code&gt;.gemini&lt;/code&gt; directory. This feature is very advanced - it essentially allows you to replace the built-in brain of the CLI with your own instructions, which some users do for specialized workflows (like simulating a specific persona or enforcing ultra-strict policies). Use it carefully, as replacing the core prompt can affect tool usage (the core prompt contains important directions for how the AI selects and uses tools).&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Direct prompt via CLI: Aside from&lt;/p&gt;&lt;code&gt;-p&lt;/code&gt;, there's also&lt;code&gt;-i&lt;/code&gt;(interactive prompt) which starts a session with an initial prompt, and then keeps it open. For example:&lt;code&gt;gemini -i "Hello, let's debug something"&lt;/code&gt;will open the REPL and already have said hello to the model. This is useful if you want the first question to be asked immediately when starting.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Scripting with shell pipes: You can pipe not just text but also files or command outputs into Gemini. For example:&lt;/p&gt;&lt;code&gt;gemini -p "Summarize this log:" &amp;lt; big_log.txt&lt;/code&gt;will feed the content of&lt;code&gt;big_log.txt&lt;/code&gt;into the prompt (after the phrase "Summarize this log:"). Or you might do&lt;code&gt;some_command | gemini -p "Given the above output, what went wrong?"&lt;/code&gt;. This technique allows you to compose Unix tools with AI analysis. It's headless in the sense that it's a single-pass operation.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Running in CI/CD: You could incorporate Gemini CLI into build processes. For instance, a CI pipeline might run a test and then use Gemini CLI to automatically analyze failing test output and post a comment. Using the&lt;/p&gt;&lt;code&gt;-p&lt;/code&gt;flag and environment auth, this can be scripted. (Of course, ensure the environment has the API key or auth needed.)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;One more headless trick: the &lt;code&gt;--format=json&lt;/code&gt; flag (or config setting). Gemini CLI can output responses in JSON format instead of the human-readable text if you configure it. This is useful for programmatic consumption - your script can parse the JSON to get the answer or any tool actions details.&lt;/p&gt;
    &lt;p&gt;Why headless mode matters: It transforms Gemini CLI from an interactive assistant into a backend service or utility that other programs can call. You could schedule a cronjob that runs a Gemini CLI prompt nightly (imagine generating a report or cleaning up something with AI logic). You could wire up a button in an IDE that triggers a headless Gemini run for a specific task.&lt;/p&gt;
    &lt;p&gt;Example: Let's say you want a daily summary of a news website. You could have a script:&lt;/p&gt;
    &lt;code&gt;gemini -p "Web-fetch \"https://news.site/top-stories\" and extract the headlines, then write them to headlines.txt"&lt;/code&gt;
    &lt;p&gt;With &lt;code&gt;--yolo&lt;/code&gt; perhaps, so it won't ask confirmation to write the file. This would use the web fetch tool to get the page and the file write tool to save the headlines. All automatically, no human in the loop. The possibilities are endless once you treat Gemini CLI as a scriptable component.&lt;/p&gt;
    &lt;p&gt;In summary, Headless Mode enables automation. It's the bridge between Gemini CLI and other systems. Mastering it means you can scale up your AI usage - not just when you're typing in the terminal, but even when you aren't around, your AI agent can do work for you.&lt;/p&gt;
    &lt;p&gt;(Tip: For truly long-running non-interactive tasks, you might also look into Gemini CLI's "Plan" mode or how it can generate multi-step plans without intervention. However, those are advanced topics beyond this scope. In most cases, a well-crafted single prompt via headless mode can achieve a lot.)&lt;/p&gt;
    &lt;p&gt;Quick use-case: If you've been debugging an issue with Gemini CLI for an hour and need to stop, you don't have to lose the conversation context. Use &lt;code&gt;/chat save &amp;lt;name&amp;gt;&lt;/code&gt; to save the session. Later (even after restarting the CLI), you can use &lt;code&gt;/chat resume &amp;lt;name&amp;gt;&lt;/code&gt; to pick up where you left off. This way, long-running conversations can be paused and continued seamlessly.&lt;/p&gt;
    &lt;p&gt;Gemini CLI essentially has a built-in chat session manager. The commands to know are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;/chat save &amp;lt;tag&amp;gt;&lt;/code&gt;- Saves the current conversation state under a tag/name you provide. The tag is like a filename or key for that session. Save often if you want, it will overwrite the tag if it exists. (Using a descriptive name is helpful - e.g.,&lt;code&gt;chat save fix-docker-issue&lt;/code&gt;.)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;/chat list&lt;/code&gt;- Lists all your saved sessions (the tags you've used. This helps you remember what you named previous saves.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;/chat resume &amp;lt;tag&amp;gt;&lt;/code&gt;- Resumes the session with that tag, restoring the entire conversation context and history to how it was when saved. It's like you never left. You can then continue chatting from that point.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;/chat share&lt;/code&gt;- (saves to file) This is useful as you can share the entire chat with someone else who can continue the session. Almost collaboration-like.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Under the hood, these sessions are stored likely in &lt;code&gt;~/.gemini/chats/&lt;/code&gt; or a similar location. They include the conversation messages and any relevant state. This feature is super useful for cases such as:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Long debugging sessions: Sometimes debugging with an AI can be a long back-and-forth. If you can't solve it in one go, save it and come back later (maybe with a fresh mind). The AI will still "remember" everything from before, because the whole context is reloaded.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Multi-day tasks: If you're using Gemini CLI as an assistant for a project, you might have one chat session for "Refactor module X" that spans multiple days. You can resume that specific chat each day so the context doesn't reset daily. Meanwhile, you might have another session for "Write documentation" saved separately. Switching contexts is just a matter of saving one and resuming the other.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Team hand-off: This is more experimental, but in theory, you could share the content of a saved chat with a colleague (the saved files are likely portable). If they put it in their&lt;/p&gt;&lt;code&gt;.gemini&lt;/code&gt;directory and resume, they could see the same context. The practical simpler approach for collaboration is just copying the relevant Q&amp;amp;A from the log and using a shared&lt;code&gt;GEMINI.md&lt;/code&gt;or prompt, but it's interesting to note that the session data is yours to keep.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Usage example:&lt;/p&gt;
    &lt;code&gt;/chat save api-upgrade&lt;/code&gt;
    &lt;p&gt;(Session saved as "api-upgrade")&lt;/p&gt;
    &lt;code&gt;/quit&lt;/code&gt;
    &lt;p&gt;(Later, reopen CLI)&lt;/p&gt;
    &lt;code&gt;$ gemini
gemini&amp;gt; /chat list&lt;/code&gt;
    &lt;p&gt;(Shows: api-upgrade)&lt;/p&gt;
    &lt;code&gt;gemini&amp;gt; /chat resume api-upgrade&lt;/code&gt;
    &lt;p&gt;Now the model greets you with the last exchange's state ready. You can confirm by scrolling up that all your previous messages are present.&lt;/p&gt;
    &lt;p&gt;Pro Tip: Use meaningful tags when saving chats. Instead of &lt;code&gt;/chat save session1&lt;/code&gt;, give it a name related to the topic (e.g. &lt;code&gt;/chat save memory-leak-bug&lt;/code&gt;). This will help you find the right one later via &lt;code&gt;/chat list&lt;/code&gt;. There is no strict limit announced on how many sessions you can save, but cleaning up old ones occasionally might be wise just for organization.&lt;/p&gt;
    &lt;p&gt;This feature turns Gemini CLI into a persistent advisor. You don't lose knowledge gained in a conversation; you can always pause and resume. It's a differentiator compared to some other AI interfaces that forget context when closed. For power users, it means you can maintain parallel threads of work with the AI. Just like you'd have multiple terminal tabs for different tasks, you can have multiple chat sessions saved and resume the one you need at any given time.&lt;/p&gt;
    &lt;p&gt;Quick use-case: Do you have a project split across multiple repositories or directories? You can launch Gemini CLI with access to all of them at once, so it sees a unified workspace. For example, if your frontend and backend are separate folders, you can include both so that Gemini can edit or reference files in both.&lt;/p&gt;
    &lt;p&gt;There are two ways to use multi-directory mode:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Launch flag: Use the &lt;code&gt;--include-directories&lt;/code&gt;(or&lt;code&gt;-I&lt;/code&gt;) flag when starting Gemini CLI. For example:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;gemini --include-directories "../backend:../frontend"&lt;/code&gt;
    &lt;p&gt;This assumes you run the command from, say, a &lt;code&gt;scripts&lt;/code&gt; directory and want to include two sibling folders. You provide a colon-separated list of paths. Gemini CLI will then treat all those directories as part of one big workspace.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Persistent setting: In your &lt;code&gt;settings.json&lt;/code&gt;, you can define&lt;code&gt;"includeDirectories": ["path1", "path2", [...]](https://www.philschmid.de/gemini-cli-cheatsheet#:~:text=,61AFEF%22%2C%20%22AccentPurple)&lt;/code&gt;. This is useful if you always want certain common directories loaded (e.g., a shared library folder that multiple projects use). The paths can be relative or absolute. Environment variables in the paths (like&lt;code&gt;~/common-utils&lt;/code&gt;) are allowed.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When multi-dir mode is active, the CLI's context and tools consider files across all included locations. The &lt;code&gt;&amp;gt; /directory show&lt;/code&gt; command will list which directories are in the current workspace. You can also dynamically add directories during a session with &lt;code&gt;/directory add [&amp;lt;path&amp;gt;](https://medium.com/@ferreradaniel/gemini-cli-free-ai-tool-upgrade-5-new-features-you-need-right-now-04cfefac5e93#:~:text=How%20to%20add%20multiple%20directories,step)&lt;/code&gt; - it will then load that on the fly (potentially scanning it for context like it does on startup).&lt;/p&gt;
    &lt;p&gt;Why use multi-directory mode? In microservice architectures or modular codebases, it's common that one piece of code lives in one repo and another piece in a different repo. If you only ran Gemini in one, it wouldn't "see" the others. By combining them, you enable cross-project reasoning. For example, you could ask, "Update the API client in the frontend to match the backend's new API endpoints" - Gemini can open the backend folder to see the API definitions and simultaneously open the frontend code to modify it accordingly. Without multi-dir, you'd have to do one side at a time and manually carry info over.&lt;/p&gt;
    &lt;p&gt;Example: Let's say you have &lt;code&gt;client/&lt;/code&gt; and &lt;code&gt;server/&lt;/code&gt;. You start:&lt;/p&gt;
    &lt;code&gt;cd client
gemini --include-directories "../server"&lt;/code&gt;
    &lt;p&gt;Now at the &lt;code&gt;gemini&amp;gt;&lt;/code&gt; prompt, if you do &lt;code&gt;&amp;gt; !ls&lt;/code&gt;, you'll see it can list files in both &lt;code&gt;client&lt;/code&gt; and &lt;code&gt;server&lt;/code&gt; (it might show them as separate paths). You could do:&lt;/p&gt;
    &lt;code&gt;Open server/routes/api.py and client/src/api.js side by side to compare function names.&lt;/code&gt;
    &lt;p&gt;The AI will have access to both files. Or you might say:&lt;/p&gt;
    &lt;code&gt;The API changed: the endpoint "/users/create" is now "/users/register". Update both backend and frontend accordingly.&lt;/code&gt;
    &lt;p&gt;It can simultaneously create a patch in the backend route and adjust the frontend fetch call.&lt;/p&gt;
    &lt;p&gt;Under the hood, Gemini merges the file index of those directories. There might be some performance considerations if each directory is huge, but generally it handles multiple small-medium projects fine. The cheat sheet notes that this effectively creates one workspace with multiple roots.&lt;/p&gt;
    &lt;p&gt;Tip within a tip: Even if you don't use multi-dir all the time, know that you can still reference files across the filesystem by absolute path in prompts (&lt;code&gt;@/path/to/file&lt;/code&gt;). However, without multi-dir, Gemini might not have permission to edit those or know to load context from them proactively. Multi-dir formally includes them in scope so it's aware of all files for tasks like search or code generation across the whole set.&lt;/p&gt;
    &lt;p&gt;Remove directories: If needed, &lt;code&gt;/directory remove &amp;lt;path&amp;gt;&lt;/code&gt; (or a similar command) can drop a directory from the workspace. This is less common, but maybe if you included something accidentally, you can remove it.&lt;/p&gt;
    &lt;p&gt;In summary, multi-directory mode unifies your context. It's a must-have for polyrepo projects or any situation where code is split up. It makes Gemini CLI act more like an IDE that has your entire solution open. As a pro user, this means no part of your project is out of the AI's reach.&lt;/p&gt;
    &lt;p&gt;Quick use-case: Tired of a messy &lt;code&gt;Downloads&lt;/code&gt; folder or disorganized project assets? You can enlist Gemini CLI to act as a smart organizer. By providing it an overview of a directory, it can classify files and even move them into subfolders (with your approval). For instance, "Clean up my &lt;code&gt;Downloads&lt;/code&gt;: move images to an &lt;code&gt;Images&lt;/code&gt; folder, PDFs to &lt;code&gt;Documents&lt;/code&gt;, and delete temporary files."&lt;/p&gt;
    &lt;p&gt;Because Gemini CLI can read file names, sizes, and even peek into file contents, it can make informed decisions about file organization. One community-created tool dubbed "Janitor AI" showcases this: it runs via Gemini CLI to categorize files as important vs junk, and groups them accordingly. The process involved scanning the directory, using Gemini's reasoning on filenames and metadata (and content if needed), then moving files into categories. Notably, it didn't automatically delete junk - rather, it moved them to a &lt;code&gt;Trash&lt;/code&gt; folder for review.&lt;/p&gt;
    &lt;p&gt;Here's how you might replicate such a workflow with Gemini CLI manually:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Survey the directory: Use a prompt to have Gemini list and categorize. For example:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;List all files in the current directory and categorize them as "images", "videos", "documents", "archives", or "others".&lt;/code&gt;
    &lt;p&gt;Gemini might use &lt;code&gt;!ls&lt;/code&gt; or similar to get the file list, then analyze the names/extensions to produce categories.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Plan the organization: Ask Gemini how it would like to reorganize. For example:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;Propose a new folder structure for these files. I want to separate by type (Images, Videos, Documents, etc.). Also identify any files that seem like duplicates or unnecessary.&lt;/code&gt;
    &lt;p&gt;The AI might respond with a plan: e.g., "Create folders: &lt;code&gt;Images/&lt;/code&gt;, &lt;code&gt;Videos/&lt;/code&gt;, &lt;code&gt;Documents/&lt;/code&gt;, &lt;code&gt;Archives/&lt;/code&gt;. Move &lt;code&gt;X.png&lt;/code&gt;, &lt;code&gt;Y.jpg&lt;/code&gt; to &lt;code&gt;Images/&lt;/code&gt;; move &lt;code&gt;A.mp4&lt;/code&gt; to &lt;code&gt;Videos/&lt;/code&gt;; etc. The file &lt;code&gt;temp.txt&lt;/code&gt; looks unnecessary (maybe a temp file)."&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Execute moves with confirmation: You can then instruct it to carry out the plan. It may use shell commands like &lt;code&gt;mv&lt;/code&gt;for each file. Since this modifies your filesystem, you'll get confirmation prompts for each (unless you YOLO it). Carefully approve the moves. After completion, your directory will be neatly organized as suggested.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Throughout, Gemini's natural language understanding is key. It can reason, for instance, that &lt;code&gt;IMG_001.png&lt;/code&gt; is an image or that &lt;code&gt;presentation.pdf&lt;/code&gt; is a document, even if not explicitly stated. It can even open an image (using its vision capability) to see what's in it - e.g., differentiating between a screenshot vs a photo vs an icon - and name or sort it accordingly.&lt;/p&gt;
    &lt;p&gt;Renaming files by content: A particularly magical use is having Gemini rename files to be more descriptive. The Dev Community article "7 Insane Gemini CLI Tips" describes how Gemini can scan images and automatically rename them based on their content. For example, a file named &lt;code&gt;IMG_1234.jpg&lt;/code&gt; might be renamed to &lt;code&gt;login_screen.jpg&lt;/code&gt; if the AI sees it's a screenshot of a login screen. To do this, you could prompt:&lt;/p&gt;
    &lt;code&gt;For each .png image here, look at its content and rename it to something descriptive.&lt;/code&gt;
    &lt;p&gt;Gemini will open each image (via vision tool), get a description, then propose a &lt;code&gt;mv IMG_1234.png login_screen.png&lt;/code&gt; action. This can dramatically improve the organization of assets, especially in design or photo folders.&lt;/p&gt;
    &lt;p&gt;Two-pass approach: The Janitor AI discussion noted a two-step process: first broad categorization (important vs junk vs other), then refining groups. You can emulate this: first separate files that likely can be deleted (maybe large installer &lt;code&gt;.dmg&lt;/code&gt; files or duplicates) from those to keep. Then focus on organizing the keepers. Always double-check what the AI flags as junk; its guess might not always be right, so manual oversight is needed.&lt;/p&gt;
    &lt;p&gt;Safety tip: When letting the AI loose on file moves or deletions, have backups or at least be ready to undo (with &lt;code&gt;/restore&lt;/code&gt; or your own backup). It's wise to do a dry-run: ask Gemini to print the commands it would run to organize, without executing them, so you can review. For instance: "List the &lt;code&gt;mv&lt;/code&gt; and &lt;code&gt;mkdir&lt;/code&gt; commands needed for this plan, but don't execute them yet." Once you review the list, you can either copy-paste execute them, or instruct Gemini to proceed.&lt;/p&gt;
    &lt;p&gt;This is a prime example of using Gemini CLI for "non-obvious" tasks - it's not just writing code, it's doing system housekeeping with AI smarts. It can save time and bring a bit of order to chaos. After all, as developers we accumulate clutter (logs, old scripts, downloads), and an AI janitor can be quite handy.&lt;/p&gt;
    &lt;p&gt;Quick use-case: If you've been chatting with Gemini CLI for a long time, you might hit the model's context length limit or just find the session getting unwieldy. Use the &lt;code&gt;/compress&lt;/code&gt; command to summarize the conversation so far, replacing the full history with a concise summary. This frees up space for more discussion without starting from scratch.&lt;/p&gt;
    &lt;p&gt;Large language models have a fixed context window (Gemini 2.5 Pro's is very large, but not infinite). If you exceed it, the model may start forgetting earlier messages or lose coherence. The &lt;code&gt;/compress&lt;/code&gt; feature is essentially an AI-generated tl;dr of your session that keeps important points.&lt;/p&gt;
    &lt;p&gt;How it works: When you type &lt;code&gt;/compress&lt;/code&gt;, Gemini CLI will take the entire conversation (except system context) and produce a summary. It then replaces the chat history with that summary as a single system or assistant message, preserving essential details but dropping minute-by-minute dialogue. It will indicate that compression happened. For example, after &lt;code&gt;/compress&lt;/code&gt;, you might see something like:&lt;/p&gt;
    &lt;p&gt;--- Conversation compressed ---&lt;lb/&gt; Summary of discussion: The user and assistant have been debugging a memory leak in an application. Key points: The issue is likely in &lt;code&gt;DataProcessor.js&lt;/code&gt;, where objects aren't being freed. The assistant suggested adding logging and identified a possible infinite loop. The user is about to test a fix.&lt;lb/&gt; --- End of summary ---&lt;/p&gt;
    &lt;p&gt;From that point on, the model only has that summary (plus new messages) as context for what happened before. This usually is enough if the summary captured the salient info.&lt;/p&gt;
    &lt;p&gt;When to compress: Ideally before you hit the limit. If you notice the session is getting lengthy (several hundred turns or a lot of code in context), compress proactively. The cheat sheet mentions an automatic compression setting (e.g., compress when context exceeds 60% of max). If you enable that, Gemini might auto-compress and let you know. Otherwise, manual &lt;code&gt;/compress&lt;/code&gt; is in your toolkit.&lt;/p&gt;
    &lt;p&gt;After compressing: You can continue the conversation normally. If needed, you can compress multiple times in a very long session. Each time, you lose some granularity, so don't compress too frequently for no reason - you might end up with an overly brief remembrance of a complex discussion. But generally the model's own summarization is pretty good at keeping the key facts (and you can always restate anything critical yourself).&lt;/p&gt;
    &lt;p&gt;Context window example: Let's illustrate. Suppose you fed in a large codebase by referencing many files and had a 1M token context (the max). If you then want to shift to a different part of the project, rather than starting a new session (losing all that understanding), you could compress. The summary will condense the knowledge gleaned from the code (like "We loaded modules A, B, C. A has these functions... B interacts with C in these ways..."). Now you can proceed to ask about new things with that knowledge retained abstractly.&lt;/p&gt;
    &lt;p&gt;Memory vs Compression: Note that compression doesn't save to long-term memory, it's local to the conversation. If you have facts you never want lost, consider Tip 4 (adding to &lt;code&gt;/memory&lt;/code&gt;) - because memory entries will survive compression (they'll just be reinserted anyway since they are in &lt;code&gt;GEMINI.md&lt;/code&gt; context). Compression is more about ephemeral chat content.&lt;/p&gt;
    &lt;p&gt;A minor caution: after compression, the AI's style might slightly change because it's effectively seeing a "fresh" conversation with a summary. It might reintroduce itself or change tone. You can instruct it like "Continue from here... (we compressed)" to smooth it out. In practice, it often continues fine.&lt;/p&gt;
    &lt;p&gt;To summarize (pun intended), use &lt;code&gt;/compress&lt;/code&gt; as your session grows long to maintain performance and relevance. It helps Gemini CLI focus on the bigger picture instead of every detail of the conversation's history. This way, you can have marathon debugging sessions or extensive design discussions without running out of the "mental paper" the AI is writing on.&lt;/p&gt;
    &lt;p&gt;Quick use-case: At any point in a Gemini CLI session, you can run actual shell commands by prefixing them with &lt;code&gt;!&lt;/code&gt;. For example, if you want to check the git status, just type &lt;code&gt;!git status&lt;/code&gt; and it will execute in your terminal. This saves you from switching windows or context - you're still in the Gemini CLI, but you're essentially telling it "let me run this command real quick."&lt;/p&gt;
    &lt;p&gt;This tip is about Shell Mode in Gemini CLI. There are two ways to use it:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Single command: Just put &lt;code&gt;!&lt;/code&gt;at the start of your prompt, followed by any command and arguments. This will execute that command in the current working directory and display the output in-line. For example:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;!ls -lh src/&lt;/code&gt;
    &lt;p&gt;will list the files in the &lt;code&gt;src&lt;/code&gt; directory, outputting something like you'd see in a normal terminal. After the output, the Gemini prompt returns so you can continue chatting or issue more commands.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Persistent shell mode: If you enter &lt;code&gt;!&lt;/code&gt;alone and hit Enter, Gemini CLI switches into a sub-mode where you get a shell prompt (often it looks like&lt;code&gt;shell&amp;gt;&lt;/code&gt;or similar. Now you can type multiple shell commands interactively. It's basically a mini-shell within the CLI. You exit this mode by typing&lt;code&gt;!&lt;/code&gt;on an empty line again (or&lt;code&gt;exit&lt;/code&gt;). For instance:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;!
shell&amp;gt; pwd
/home/alice/project
shell&amp;gt; python --version
Python 3.x.x
shell&amp;gt; !&lt;/code&gt;
    &lt;p&gt;After the final &lt;code&gt;!&lt;/code&gt;, you're back to the normal Gemini prompt.&lt;/p&gt;
    &lt;p&gt;Why is this useful? Because development is a mix of actions and inquiries. You might be discussing something with the AI and realize you need to compile the code or run tests to see something. Instead of leaving the conversation, you can quickly do it and feed the result back into the chat. In fact, Gemini CLI often does this for you as part of its tool usage (it might automatically run &lt;code&gt;!pytest&lt;/code&gt; when you ask to fix tests, for example). But as the user, you have full control to do it manually too.&lt;/p&gt;
    &lt;p&gt;Examples:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;After Gemini suggests a fix in code, you can do&lt;/p&gt;&lt;code&gt;!npm run build&lt;/code&gt;to see if it compiles, then copy any errors and ask Gemini to help with those.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;If you want to open a file in&lt;/p&gt;&lt;code&gt;vim&lt;/code&gt;or&lt;code&gt;nano&lt;/code&gt;, you could even launch it via&lt;code&gt;!nano filename&lt;/code&gt;(though note that since Gemini CLI has its own interface, using an interactive editor inside it might be a bit awkward - better to use the built-in editor integration or copy to your editor).&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;You can use shell commands to gather info for the AI: e.g.,&lt;/p&gt;&lt;code&gt;!grep TODO -R .&lt;/code&gt;to find all TODOs in the project, then you might ask Gemini to help address those TODOs.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Or simply use it for environment tasks:&lt;/p&gt;&lt;code&gt;!pip install some-package&lt;/code&gt;if needed, etc., without leaving the CLI.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Seamless interplay: One cool aspect is how the conversation can refer to outputs. For example, you could do &lt;code&gt;!curl http://example.com&lt;/code&gt; to fetch some data, see the output, then immediately say to Gemini, "Format the above output as JSON" - since the output was printed in the chat, the AI has it in context to work with (provided it's not too large).&lt;/p&gt;
    &lt;p&gt;Terminal as a default shell: If you find yourself always prefacing commands with &lt;code&gt;!&lt;/code&gt;, you can actually make the shell mode persistent by default. One way is launching Gemini CLI with a specific tool mode (there's a concept of default tool). But easier: just drop into shell mode (&lt;code&gt;!&lt;/code&gt; with nothing) at session start if you plan to run a lot of manual commands and only occasionally talk to AI. Then you can exit shell mode whenever you want to ask a question. It's almost like turning Gemini CLI into your normal terminal that happens to have an AI readily available.&lt;/p&gt;
    &lt;p&gt;Integration with AI planning: Sometimes Gemini CLI itself will propose to run a shell command. If you approve, it effectively does the same as &lt;code&gt;!command&lt;/code&gt;. Understanding that, you know you can always intervene. If Gemini is stuck or you want to try something, you don't have to wait for it to suggest - you can just do it and then continue.&lt;/p&gt;
    &lt;p&gt;In summary, the &lt;code&gt;!&lt;/code&gt; passthrough means you don't have to leave Gemini CLI for shell tasks. It collapses the boundary between chatting with the AI and executing commands on your system. As a pro user, this is fantastic for efficiency - your AI and your terminal become one continuous environment.&lt;/p&gt;
    &lt;p&gt;Quick use-case: Realize that Gemini CLI can leverage any command-line tool installed on your system as part of its problem-solving. The AI has access to the shell, so if you have &lt;code&gt;cURL&lt;/code&gt;, &lt;code&gt;ImageMagick&lt;/code&gt;, &lt;code&gt;git&lt;/code&gt;, &lt;code&gt;Docker&lt;/code&gt;, or any other tool, Gemini can invoke it when appropriate. In other words, your entire &lt;code&gt;$PATH&lt;/code&gt; is the AI's toolkit. This greatly expands what it can do - far beyond its built-in tools.&lt;/p&gt;
    &lt;p&gt;For example, say you ask: "Convert all PNG images in this folder to WebP format." If you have ImageMagick's &lt;code&gt;convert&lt;/code&gt; utility installed, Gemini CLI might plan something like: use a shell loop with &lt;code&gt;convert&lt;/code&gt; command for each file. Indeed, one of the earlier examples from a blog showed exactly this, where the user prompted to batch-convert images, and Gemini executed a shell one-liner with the &lt;code&gt;convert&lt;/code&gt; tool.&lt;/p&gt;
    &lt;p&gt;Another scenario: "Deploy my app to Docker." If &lt;code&gt;Docker CLI&lt;/code&gt; is present, the AI could call &lt;code&gt;docker build&lt;/code&gt; and &lt;code&gt;docker run&lt;/code&gt; steps as needed. Or "Use FFmpeg to extract audio from &lt;code&gt;video.mp4&lt;/code&gt;" - it can construct the &lt;code&gt;ffmpeg&lt;/code&gt; command.&lt;/p&gt;
    &lt;p&gt;This tip is about mindset: Gemini isn't limited to what's coded into it (which is already extensive). It can figure out how to use other programs available to achieve a goal. It knows common syntax and can read help texts if needed (it could call &lt;code&gt;--help&lt;/code&gt; on a tool). The only limitation is safety: by default, it will ask confirmation for any &lt;code&gt;run_shell_command&lt;/code&gt; it comes up with. But as you become comfortable, you might allow certain benign commands automatically (see YOLO or allowed-tools config).&lt;/p&gt;
    &lt;p&gt;Be mindful of the environment: "With great power comes great responsibility." Since every shell tool is fair game, you should ensure that your &lt;code&gt;$PATH&lt;/code&gt; doesn't include anything you wouldn't want the AI to run inadvertently. This is where Tip 19 (custom PATH) comes in - some users create a restricted &lt;code&gt;$PATH&lt;/code&gt; for Gemini, so it can't, say, directly call system destructive commands or maybe not call &lt;code&gt;gemini&lt;/code&gt; recursively (to avoid loops). The point is, by default if &lt;code&gt;gcc&lt;/code&gt; or &lt;code&gt;terraform&lt;/code&gt; or anything is in &lt;code&gt;$PATH&lt;/code&gt;, Gemini could invoke it. It doesn't mean it will randomly do so - only if the task calls for it - but it's possible.&lt;/p&gt;
    &lt;p&gt;Train of thought example: Imagine you ask Gemini CLI: "Set up a basic HTTP server that serves the current directory." The AI might think: "I can use Python's built-in server for this." It then issues &lt;code&gt;!python3 -m http.server 8000&lt;/code&gt;. Now it just used a system tool (Python) to launch a server. That's an innocuous example. Another: "Check the memory usage on this Linux system." The AI might use the &lt;code&gt;free -h&lt;/code&gt; command or read from &lt;code&gt;/proc/meminfo&lt;/code&gt;. It's effectively doing what a sysadmin would do, by using available commands.&lt;/p&gt;
    &lt;p&gt;All tools are extensions of the AI: This is somewhat futuristic, but consider that any command-line program can be seen as a "function" the AI can call to extend its capability. Need to solve a math problem? It could call &lt;code&gt;bc&lt;/code&gt; (calculator). Need to manipulate an image? It could call an image processing tool. Need to query a database? If the CLI client is installed and credentials are there, it can use it. The possibilities are expansive. In other AI agent frameworks, this is known as tool use, and Gemini CLI is designed with a lot of trust in its agent to decide the right tool.&lt;/p&gt;
    &lt;p&gt;When it goes wrong: The flip side is if the AI misunderstands a tool or has a hallucination about one. It might try to call a command that doesn't exist, or use wrong flags, resulting in errors. This isn't a big deal - you'll see the error and can correct or clarify. In fact, the system prompt of Gemini CLI likely guides it to first do a dry-run (just propose the command) rather than executing blindly. So you often get a chance to catch these. Over time, the developers are improving the tool selection logic to reduce these missteps.&lt;/p&gt;
    &lt;p&gt;The main takeaway is to think of Gemini CLI as having a very large Swiss Army knife - not just the built-in blades, but every tool in your OS. You don't have to instruct it on how to use them if it's something standard; usually it knows or can find out. This significantly amplifies what you can accomplish. It's like having a junior dev or devops engineer who knows how to run pretty much any program you have installed.&lt;/p&gt;
    &lt;p&gt;As a pro user, you can even install additional CLI tools specifically to give Gemini more powers. For example, if you install a CLI for a cloud service (AWS CLI, GCloud CLI, etc.), in theory Gemini can utilize it to manage cloud resources if prompted to. Always ensure you understand and trust the commands run, especially with powerful tools (you wouldn't want it spinning up huge cloud instances accidentally). But used wisely, this concept - everything is a Gemini tool - is what makes it exponentially more capable as you integrate it into your environment.&lt;/p&gt;
    &lt;p&gt;Quick use-case: Gemini CLI isn't limited to text - it's multimodal. This means it can analyze images, diagrams, or even PDFs if given. Use this to your advantage. For instance, you could say "Here's a screenshot of an error dialog, &lt;code&gt;@./error.png&lt;/code&gt; - help me troubleshoot this." The AI will "see" the image and respond accordingly.&lt;/p&gt;
    &lt;p&gt;One of the standout features of Google's Gemini model (and its precursor PaLM2 in Codey form) is image understanding. In Gemini CLI, if you reference an image with &lt;code&gt;@&lt;/code&gt;, the model receives the image data. It can output descriptions, classifications, or reason about the image's content. We already discussed renaming images by content (Tip 14) and describing screenshots (Tip 7). But let's consider other creative uses:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;UI/UX feedback: If you're a developer working with designers, you can drop a UI image and ask Gemini for feedback or to generate code. "Look at this UI mockup&lt;/p&gt;&lt;code&gt;@mockup.png&lt;/code&gt;and produce a React component structure for it." It could identify elements in the image (header, buttons, etc.) and outline code.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Organizing images: Beyond renaming, you might have a folder of mixed images and want to sort by content. "Sort the images in&lt;/p&gt;&lt;code&gt;./photos/&lt;/code&gt;into subfolders by theme (e.g., sunsets, mountains, people)." The AI can look at each photo and categorize it (this is similar to what some photo apps do with AI - now you can do it with your own script via Gemini).&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;OCR and data extraction: If you have a screenshot of error text or a photo of a document, Gemini can often read the text from it. For example, "Extract the text from&lt;/p&gt;&lt;code&gt;invoice.png&lt;/code&gt;and put it into a structured format." As shown in a Google Cloud blog example, Gemini CLI can process a set of invoice images and output a table of their info. It basically did OCR + understanding to get invoice numbers, dates, amounts from pictures of invoices. That's an advanced use-case but entirely possible with the multimodal model under the hood.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Understanding graphs or charts: If you have a graph screenshot, you could ask "Explain this chart's key insights&lt;/p&gt;&lt;code&gt;@chart.png&lt;/code&gt;." It might interpret the axes and trends. Accuracy can vary, but it's a nifty try.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To make this practical: when you &lt;code&gt;@image.png&lt;/code&gt;, ensure the image isn't too huge (though the model can handle reasonably large images). The CLI will likely encode it and send it to the model. The response might include descriptions or further actions. You can mix text and image references in one prompt too.&lt;/p&gt;
    &lt;p&gt;Non-image modalities: The CLI and model potentially can handle PDFs and audio too, by converting them via tools. For example, if you &lt;code&gt;@report.pdf&lt;/code&gt;, Gemini CLI might use a PDF-to-text tool under the hood to extract text and then summarize. If you &lt;code&gt;@audio.mp3&lt;/code&gt; and ask for a transcript, it might use an audio-to-text tool (like a speech recognition function). The cheat sheet suggests referencing PDFs, audio, video files is supported, presumably by invoking appropriate internal tools or APIs. So, "transcribe this interview audio: &lt;code&gt;@interview.wav&lt;/code&gt;" could actually work (if not now, likely soon, since underlying Google APIs for speech-to-text could be plugged in).&lt;/p&gt;
    &lt;p&gt;Rich outputs: Multimodal also means the AI can return images in responses if integrated (though in CLI it usually won't display them directly, but it could save an image file or output ASCII art, etc.). The MCP capability mentioned that tools can return images. For instance, an AI drawing tool could generate an image and Gemini CLI could present it (maybe by opening it or giving a link).&lt;/p&gt;
    &lt;p&gt;Important: The CLI itself is text-based, so you won't see the image in the terminal (unless it's capable of ASCII previews). You'll just get the analysis. So this is mostly about reading images, not displaying them. If you're in VS Code integration, it might show images in the chat view.&lt;/p&gt;
    &lt;p&gt;In summary, don't forget the "I" in GUI when using Gemini CLI - it can handle the visual just as well as the textual in many cases. This opens up workflows like visual debugging, design help, data extraction from screenshots, etc., all under the same tool. It's a differentiator that some other CLI tools may not have yet. And as models improve, this multimodal support will only get more powerful, so it's a future-proof skill to exploit.&lt;/p&gt;
    &lt;p&gt;Quick use-case: If you ever find Gemini CLI getting confused or invoking the wrong programs, consider running it with a tailored &lt;code&gt;$PATH&lt;/code&gt;. By limiting or ordering the available executables, you can prevent the AI from, say, calling a similarly named script that you didn't intend. Essentially, you sandbox its tool access to known-good tools.&lt;/p&gt;
    &lt;p&gt;For most users, this isn't an issue, but for pro users with lots of custom scripts or multiple versions of tools, it can be helpful. One reason mentioned by the developers is avoiding infinite loops or weird behavior. For example, if &lt;code&gt;gemini&lt;/code&gt; itself is in &lt;code&gt;$PATH&lt;/code&gt;, an AI gone awry might recursively call &lt;code&gt;gemini&lt;/code&gt; from within Gemini (a strange scenario, but theoretically possible). Or perhaps you have a command named &lt;code&gt;test&lt;/code&gt; that conflicts with something - the AI might call the wrong one.&lt;/p&gt;
    &lt;p&gt;How to set PATH for Gemini: Easiest is inline on launch:&lt;/p&gt;
    &lt;code&gt;PATH=/usr/bin:/usr/local/bin gemini&lt;/code&gt;
    &lt;p&gt;This runs Gemini CLI with a restricted &lt;code&gt;$PATH&lt;/code&gt; of just those directories. You might exclude directories where experimental or dangerous scripts lie. Alternatively, create a small shell script wrapper that purges or adjusts &lt;code&gt;$PATH&lt;/code&gt; then exec's &lt;code&gt;gemini&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Another approach is using environment or config to explicitly disable certain tools. For instance, if you absolutely never want the AI to use &lt;code&gt;rm&lt;/code&gt; or some destructive tool, you could technically create an alias or dummy &lt;code&gt;rm&lt;/code&gt; in a safe &lt;code&gt;$PATH&lt;/code&gt; that does nothing (though this could interfere with normal operations, so maybe not that one). A better method is the exclude list in settings. In an extension or &lt;code&gt;settings.json&lt;/code&gt;, you can exclude tool names. E.g.,&lt;/p&gt;
    &lt;code&gt;"excludeTools": ["run_shell_command"]&lt;/code&gt;
    &lt;p&gt;This extreme example would stop all shell commands from running (making Gemini effectively read-only). More granular, there was mention of skipping confirmation for some; similarly you might configure something like:&lt;/p&gt;
    &lt;code&gt;"tools": {
  "exclude": ["apt-get", "shutdown"]
}&lt;/code&gt;
    &lt;p&gt;(This syntax is illustrative; consult docs for exact usage.)&lt;/p&gt;
    &lt;p&gt;The principle is, by controlling the environment, you reduce risk of the AI doing something dumb with a tool it shouldn't. It's akin to child-proofing the house.&lt;/p&gt;
    &lt;p&gt;Prevent infinite loops: One user scenario was a loop where Gemini kept reading its own output or re-reading files repeatedly. Custom &lt;code&gt;$PATH&lt;/code&gt; can't directly fix logic loops, but one cause could be if the AI calls a command that triggers itself. Ensuring it can't accidentally spawn another AI instance (like calling &lt;code&gt;bard&lt;/code&gt; or &lt;code&gt;gemini&lt;/code&gt; command, if it thought to do so) is good. Removing those from &lt;code&gt;$PATH&lt;/code&gt; (or renaming them for that session) helps.&lt;/p&gt;
    &lt;p&gt;Isolation via sandbox: Another alternative to messing with &lt;code&gt;$PATH&lt;/code&gt; is using &lt;code&gt;--sandbox&lt;/code&gt; mode (which uses Docker or Podman to run tools in an isolated environment). In that case, the AI's actions are contained and have only the tools that sandbox image provides. You could supply a Docker image with a curated set of tools. This is heavy-handed but very safe.&lt;/p&gt;
    &lt;p&gt;Custom PATH for specific tasks: You might have different &lt;code&gt;$PATH&lt;/code&gt; setups for different projects. For example, in one project you want it to use a specific version of Node or a local toolchain. Launching &lt;code&gt;gemini&lt;/code&gt; with the &lt;code&gt;$PATH&lt;/code&gt; that points to those versions will ensure the AI uses the right one. Essentially, treat Gemini CLI like any user - it uses whatever environment you give it. So if you need it to pick &lt;code&gt;gcc-10&lt;/code&gt; vs &lt;code&gt;gcc-12&lt;/code&gt;, adjust &lt;code&gt;$PATH&lt;/code&gt; or &lt;code&gt;CC&lt;/code&gt; env var accordingly.&lt;/p&gt;
    &lt;p&gt;In summary: Guard rails. As a power user, you have the ability to fine-tune the operating conditions of the AI. If you ever find a pattern of undesirable behavior tied to tool usage, tweaking &lt;code&gt;$PATH&lt;/code&gt; is a quick remedy. For everyday use, you likely won't need this, but it's a pro tip to keep in mind if you integrate Gemini CLI into automation or CI: give it a controlled environment. That way, you know exactly what it can and cannot do, which increases reliability.&lt;/p&gt;
    &lt;p&gt;If you run long chats or repeatedly attach the same big files, you can cut cost and latency by turning on token caching and monitoring usage. With an API key or Vertex AI auth, Gemini CLI automatically reuses previously sent system instructions and context, so follow‑up requests are cheaper. You can see the savings live in the CLI.&lt;/p&gt;
    &lt;p&gt;How to use it&lt;/p&gt;
    &lt;p&gt;Use an auth mode that enables caching. Token caching is available when you authenticate with a Gemini API key or Vertex AI. It is not available with OAuth login today. Google Gemini&lt;/p&gt;
    &lt;p&gt;Inspect your usage and cache hits. Run the &lt;code&gt;stats&lt;/code&gt; command during a session. It shows total tokens and a &lt;code&gt;cached&lt;/code&gt; field when caching is active.&lt;/p&gt;
    &lt;code&gt;/stats&lt;/code&gt;
    &lt;p&gt;The command's description and cached reporting behavior are documented in the commands reference and FAQ. Google Gemini+1&lt;/p&gt;
    &lt;p&gt;Capture metrics in scripts. When running headless, output JSON and parse the &lt;code&gt;stats&lt;/code&gt; block, which includes &lt;code&gt;tokens.cached&lt;/code&gt; for each model:&lt;/p&gt;
    &lt;code&gt;gemini -p "Summarize README" --output-format json&lt;/code&gt;
    &lt;p&gt;The headless guide documents the JSON schema with cached token counts. Google Gemini&lt;/p&gt;
    &lt;p&gt;Save a session summary to file: For CI or budget tracking, write a JSON session summary to disk.&lt;/p&gt;
    &lt;code&gt;gemini -p "Analyze logs" --session-summary usage.json&lt;/code&gt;
    &lt;p&gt;This flag is listed in the changelog. Google Gemini&lt;/p&gt;
    &lt;p&gt;With API key or Vertex auth, the CLI automatically reuses previously sent context so later turns send fewer tokens. Keeping &lt;code&gt;GEMINI.md&lt;/code&gt; and large file references stable across turns increases cache hits; you'll see that reflected in stats as cached tokens.&lt;/p&gt;
    &lt;p&gt;Quick use-case: Instantly copy the latest answer or code snippet from Gemini CLI to your system clipboard, without any extraneous formatting or line numbers. This is perfect for quickly pasting AI-generated code into your editor or sharing a result with a teammate.&lt;/p&gt;
    &lt;p&gt;When Gemini CLI provides an answer (especially a multi-line code block), you often want to reuse it elsewhere. The &lt;code&gt;/copy&lt;/code&gt; slash command makes this effortless by copying the last output produced by the CLI directly to your clipboard. Unlike manual selection (which can grab line numbers or prompt text), &lt;code&gt;/copy&lt;/code&gt; grabs only the raw response content. For example, if Gemini just generated a 50-line Python script, simply typing &lt;code&gt;/copy&lt;/code&gt; will put that entire script into your clipboard, ready to paste - no need to scroll and select text. Under the hood, Gemini CLI uses the appropriate clipboard utility for your platform (e.g. &lt;code&gt;pbcopy&lt;/code&gt; on macOS, &lt;code&gt;clip&lt;/code&gt; on Windows. Once you run the command, you'll typically see a confirmation message, and then you can paste the copied text wherever you need it.&lt;/p&gt;
    &lt;p&gt;How it works: The &lt;code&gt;/copy&lt;/code&gt; command requires that your system has a clipboard tool available. On macOS and Windows, the required tools (&lt;code&gt;pbcopy&lt;/code&gt; and &lt;code&gt;clip&lt;/code&gt; respectively) are usually pre-installed. On Linux, you may need to install &lt;code&gt;xclip&lt;/code&gt; or &lt;code&gt;xsel&lt;/code&gt; for &lt;code&gt;/copy&lt;/code&gt; to function. After ensuring that, you can use &lt;code&gt;/copy&lt;/code&gt; anytime after Gemini CLI prints an answer. It will capture the entire last response (even if it's long) and omit any internal numbering or formatting the CLI may show on-screen. This saves you from dealing with unwanted artifacts when transferring the content. It's a small feature, but a huge time-saver when you're iterating on code or compiling a report generated by the AI.&lt;/p&gt;
    &lt;p&gt;Pro Tip: If you find the &lt;code&gt;/copy&lt;/code&gt; command isn't working, double-check that your clipboard utilities are installed and accessible. For instance, Ubuntu users should run &lt;code&gt;sudo apt install xclip&lt;/code&gt; to enable clipboard copying. Once set up, &lt;code&gt;/copy&lt;/code&gt; lets you share Gemini's outputs with zero friction - copy, paste, and you're done.&lt;/p&gt;
    &lt;p&gt;Quick use-case: Cleanly interrupt Gemini CLI or exit shell mode with a single keypress - and quit the CLI entirely with a quick double-tap - thanks to the versatile Ctrl+C shortcut. This gives you immediate control when you need to stop or exit.&lt;/p&gt;
    &lt;p&gt;Gemini CLI operates like a REPL, and knowing how to break out of operations is essential. Pressing Ctrl+C once will cancel the current action or clear any input you've started typing, essentially acting as an "abort" command. For example, if the AI is generating a lengthy answer and you've seen enough, hit &lt;code&gt;Ctrl+C&lt;/code&gt; - the generation stops immediately. If you had started typing a prompt but want to discard it, &lt;code&gt;Ctrl+C&lt;/code&gt; will wipe the input line so you can start fresh. Additionally, if you are in shell mode (activated by typing &lt;code&gt;!&lt;/code&gt; to run shell commands), a single &lt;code&gt;Ctrl+C&lt;/code&gt; will exit shell mode and return you to the normal Gemini prompt (it sends an interrupt to the shell process running. This is extremely handy if a shell command is hanging or you simply want to get back to AI mode.&lt;/p&gt;
    &lt;p&gt;Pressing Ctrl+C twice in a row is the shortcut to exit Gemini CLI entirely. Think of it as "&lt;code&gt;Ctrl+C&lt;/code&gt; to cancel, and &lt;code&gt;Ctrl+C&lt;/code&gt; again to quit." This double-tap signals the CLI to terminate the session (you'll see a goodbye message or the program will close). It's a faster alternative to typing &lt;code&gt;/quit&lt;/code&gt; or closing the terminal window, allowing you to gracefully shut down the CLI from the keyboard. Do note that a single &lt;code&gt;Ctrl+C&lt;/code&gt; will not quit if there's input to clear or an operation to interrupt - it requires that second press (when the prompt is idle) to fully exit. This design prevents accidentally closing the session when you only meant to stop the current output.&lt;/p&gt;
    &lt;p&gt;Pro Tip: In shell mode, you can also press the Esc key to leave shell mode and return to Gemini's chat mode without terminating the CLI. And if you prefer a more formal exit, the &lt;code&gt;/quit&lt;/code&gt; command is always available to cleanly end the session. Lastly, Unix users can use Ctrl+D (EOF) at an empty prompt to exit as well - Gemini CLI will prompt for confirmation if needed. But for most cases, mastering the single- and double-tap of &lt;code&gt;Ctrl+C&lt;/code&gt; is the quickest way to stay in control.&lt;/p&gt;
    &lt;p&gt;Quick use-case: Adapt the CLI's behavior and appearance to your preferences or project conventions by editing the &lt;code&gt;settings.json&lt;/code&gt; config file, instead of sticking with one-size-fits-all defaults. This lets you enforce things like theme, tool usage rules, or editor mode across all your sessions.&lt;/p&gt;
    &lt;p&gt;Gemini CLI is highly configurable. In your home directory (&lt;code&gt;~/.gemini/&lt;/code&gt;) or project folder (&lt;code&gt;.gemini/&lt;/code&gt; within your repo), you can create a &lt;code&gt;settings.json&lt;/code&gt; file to override default settings. Nearly every aspect of the CLI can be tuned here - from visual theme to tool permissions. The CLI merges settings from multiple levels: system-wide defaults, your user settings, and project-specific settings (project settings override user settings. For example, you might have a global preference for a dark theme, but a particular project might require stricter tool sandboxing; you can handle this via different &lt;code&gt;settings.json&lt;/code&gt; files at each level.&lt;/p&gt;
    &lt;p&gt;Inside &lt;code&gt;settings.json&lt;/code&gt;, options are specified as JSON key-value pairs. Here's a snippet illustrating some useful customizations:&lt;/p&gt;
    &lt;code&gt;{
"theme": "GitHub",
"autoAccept": false,
"vimMode": true,
"sandbox": "docker",
"includeDirectories": ["../shared-library", "~/common-utils"],
"usageStatisticsEnabled": true
}&lt;/code&gt;
    &lt;p&gt;In this example, we set the theme to "GitHub" (a popular color scheme), disable &lt;code&gt;autoAccept&lt;/code&gt; (so the CLI will always ask before running potentially altering tools), enable Vim keybindings for the input editor, and enforce using Docker for tool sandboxing. We also added some directories to the workspace context (&lt;code&gt;includeDirectories&lt;/code&gt;) so Gemini can see code in shared paths by default. Finally, we kept &lt;code&gt;usageStatisticsEnabled&lt;/code&gt; true to collect basic usage stats (which feeds into telemetry, if enabled. There are many more settings available - like defining custom color themes, adjusting token limits, or whitelisting/blacklisting specific tools - all documented in the configuration guide. By tailoring these, you ensure Gemini CLI behaves optimally for your workflow (for instance, some developers always want &lt;code&gt;vimMode&lt;/code&gt; on for efficiency, while others might prefer the default editor).&lt;/p&gt;
    &lt;p&gt;One convenient way to edit settings is via the built-in settings UI. Run the command &lt;code&gt;/settings&lt;/code&gt; in Gemini CLI, and it will open an interactive editor for your configuration. This interface lets you browse and search settings with descriptions, and prevents JSON syntax errors by validating inputs. You can tweak colors, toggle features like &lt;code&gt;yolo&lt;/code&gt; (auto-approval), adjust checkpointing (file save/restore behavior), and more through a friendly menu. Changes are saved to your &lt;code&gt;settings.json&lt;/code&gt;, and some take effect immediately (others might require restarting the CLI).&lt;/p&gt;
    &lt;p&gt;Pro Tip: Maintain separate project-specific &lt;code&gt;settings.json&lt;/code&gt; files for different needs. For example, on a team project you might set &lt;code&gt;"sandbox": "docker"&lt;/code&gt; and &lt;code&gt;"excludeTools": ["run_shell_command"]&lt;/code&gt; to lock down dangerous operations, while your personal projects might allow direct shell commands. Gemini CLI will automatically pick up the nearest &lt;code&gt;.gemini/settings.json&lt;/code&gt; in your project directory tree and merge it with your global &lt;code&gt;~/.gemini/settings.json&lt;/code&gt;. Also, don't forget you can quickly adjust visual preferences: try &lt;code&gt;/theme&lt;/code&gt; to interactively switch themes without editing the file, which is great for finding a comfortable look. Once you find one, put it in &lt;code&gt;settings.json&lt;/code&gt; to make it permanent.&lt;/p&gt;
    &lt;p&gt;Quick use-case: Supercharge Gemini CLI by hooking it into VS Code - the CLI will automatically know which files you're working on and even open AI-proposed code changes in VS Code's diff editor for you. This creates a seamless loop between AI assistant and your coding workspace.&lt;/p&gt;
    &lt;p&gt;One of Gemini CLI's powerful features is its IDE integration with Visual Studio Code. By installing the official Gemini CLI Companion extension in VS Code and connecting it, you allow Gemini CLI to become "context-aware" of your editor. What does this mean in practice? When connected, Gemini knows about the files you have open, your current cursor location, and any text you've selected in VS Code. All that information is fed into the AI's context. So if you ask, "Explain this function," Gemini CLI can see the exact function you've highlighted and give a relevant answer, without you needing to copy-paste code into the prompt. The integration shares up to your 10 most recently opened files, plus selection and cursor info, giving the model a rich understanding of your workspace.&lt;/p&gt;
    &lt;p&gt;Another huge benefit is native diffing of code changes. When Gemini CLI suggests modifications to your code (for example, "refactor this function" and it produces a patch), it can open those changes in VS Code's diff viewer automatically. You'll see a side-by-side diff in VS Code showing the proposed edits. You can then use VS Code's familiar interface to review the changes, make any manual tweaks, and even accept the patch with a click. The CLI and editor stay in sync - if you accept the diff in VS Code, Gemini CLI knows and continues the session with those changes applied. This tight loop means you no longer have to copy code from the terminal to your editor; the AI's suggestions flow straight into your development environment.&lt;/p&gt;
    &lt;p&gt;How to set it up: If you start Gemini CLI inside VS Code's integrated terminal, it will detect VS Code and usually prompt you to install/connect the extension automatically. You can agree and it will run the necessary &lt;code&gt;/ide install&lt;/code&gt; step. If you don't see a prompt (or you're enabling it later), simply open Gemini CLI and run the command: &lt;code&gt;/ide install&lt;/code&gt;. This will fetch and install the "Gemini CLI Companion" extension into VS Code for you. Next, run &lt;code&gt;/ide enable&lt;/code&gt; to establish the connection - the CLI will then indicate it's linked to VS Code. You can verify at any time with &lt;code&gt;/ide status&lt;/code&gt;, which will show if it's connected and list which editor and files are being tracked. From then on, Gemini CLI will automatically receive context from VS Code (open files, selections) and will open diffs in VS Code when needed. It essentially turns Gemini CLI into an AI pair programmer that lives in your terminal but operates with full awareness of your IDE.&lt;/p&gt;
    &lt;p&gt;Currently, VS Code is the primary supported editor for this integration. (Other editors that support VS Code extensions, like VSCodium or some JetBrains via a plugin, may work via the same extension, but officially it's VS Code for now.) The design is open though - there's an IDE Companion Spec for developing similar integrations with other editors. So down the road we might see first-class support for IDEs like IntelliJ or Vim via community extensions.&lt;/p&gt;
    &lt;p&gt;Pro Tip: Once connected, you can use VS Code's Command Palette to control Gemini CLI without leaving the editor. For example, press Ctrl+Shift+P (Cmd+Shift+P on Mac) and try commands like "Gemini CLI: Run" (to launch a new CLI session in the terminal), "Gemini CLI: Accept Diff" (to approve and apply an open diff), or "Gemini CLI: Close Diff Editor" (to reject changes. These shortcuts can streamline your workflow even further. And remember, you don't always have to start the CLI manually - if you enable the integration, Gemini CLI essentially becomes an AI co-developer inside VS Code, watching context and ready to help as you work on code.&lt;/p&gt;
    &lt;p&gt;Quick use-case: Put Gemini to work on GitHub - use the Gemini CLI GitHub Action to autonomously triage new issues and review pull requests in your repository, acting as an AI teammate that handles routine dev tasks.&lt;/p&gt;
    &lt;p&gt;Gemini CLI isn't just for interactive terminal sessions; it can also run in CI/CD pipelines via GitHub Actions. Google has provided a ready-made Gemini CLI GitHub Action (currently in beta) that integrates into your repo's workflows. This effectively deploys an AI agent into your project on GitHub. It runs in the background, triggered by repository events. For example, when someone opens a new issue, the Gemini Action can automatically analyze the issue description, apply relevant labels, and even prioritize it or suggest duplicates (this is the "intelligent issue triage" workflow. When a pull request is opened, the Action kicks in to provide an AI code review - it will comment on the PR with insights about code quality, potential bugs, or stylistic improvements. This gives maintainers immediate feedback on the PR before any human even looks at it. Perhaps the coolest feature is on-demand collaboration: team members can mention &lt;code&gt;@gemini-cli&lt;/code&gt; in an issue or PR comment and give it an instruction, like "&lt;code&gt;@gemini-cli&lt;/code&gt; please write unit tests for this". The Action will pick that up and Gemini CLI will attempt to fulfill the request (adding a commit with new tests, for instance. It's like having an AI assistant living in your repo, ready to do chores when asked.&lt;/p&gt;
    &lt;p&gt;Setting up the Gemini CLI GitHub Action is straightforward. First, ensure you have Gemini CLI version 0.1.18 or later installed locally (this ensures compatibility with the Action. Then, in Gemini CLI run the special command: &lt;code&gt;/setup-github&lt;/code&gt;. This command generates the necessary workflow files in your repository (it will guide you through authentication if needed). Specifically, it adds YAML workflow files (for issue triage, PR review, etc.) under &lt;code&gt;.github/workflows/&lt;/code&gt;. You will need to add your Gemini API key to the repo's secrets (as &lt;code&gt;GEMINI_API_KEY&lt;/code&gt;) so the Action can use the Gemini API. Once that's done and the workflows are committed, the GitHub Action springs to life - from that point on, Gemini CLI will autonomously respond to new issues and PRs according to those workflows.&lt;/p&gt;
    &lt;p&gt;Because this Action is essentially running Gemini CLI in an automated way, you can customize it just like you would your CLI. The default setup comes with three workflows (issue triage, PR review, and a general mention-triggered assistant) which are **fully open-source and editable**. You can tweak the YAML to adjust what the AI does, or even add new workflows. For instance, you might create a nightly workflow that uses Gemini CLI to scan your repository for outdated dependencies or to update a README based on recent code changes - the possibilities are endless. The key benefit here is offloading mundane or time-consuming tasks to an AI agent so that human developers can focus on harder problems. And since it runs on GitHub's infrastructure, it doesn't require your intervention - it's truly a "set and forget" AI helper.&lt;/p&gt;
    &lt;p&gt;Pro Tip: Keep an eye on the Action's output in the GitHub Actions logs for transparency. The Gemini CLI Action logs will show what prompts it ran and what changes it made or suggested. This can both build trust and help you refine its behavior. Also, the team has built enterprise-grade safeguards into the Action - e.g., you can require that all shell commands the AI tries to run in a workflow are allow-listed by you. So don't hesitate to use it even on serious projects. And if you come up with a cool custom workflow using Gemini CLI, consider contributing it back to the community - the project welcomes new ideas in their repo!&lt;/p&gt;
    &lt;p&gt;Quick use-case: Gain deeper insight into how Gemini CLI is being used and performing by turning on its built-in OpenTelemetry instrumentation - monitor metrics, logs, and traces of your AI sessions to analyze usage patterns or troubleshoot issues.&lt;/p&gt;
    &lt;p&gt;For developers who like to measure and optimize, Gemini CLI offers an observability feature that exposes what's happening under the hood. By leveraging OpenTelemetry (OTEL), Gemini CLI can emit structured telemetry data about your sessions. This includes things like metrics (e.g. how many tokens used, response latency), logs of actions taken, and even traces of tool calls. With telemetry enabled, you can answer questions like: Which custom command do I use most often? How many times did the AI edit files in this project this week? What's the average response time when I ask the CLI to run tests? Such data is invaluable for understanding usage patterns and performance. Teams can use it to see how developers are interacting with the AI assistant and where bottlenecks might be.&lt;/p&gt;
    &lt;p&gt;By default, telemetry is off (Gemini respects privacy and performance). You can opt-in by setting &lt;code&gt;"telemetry.enabled": true&lt;/code&gt; in your &lt;code&gt;settings.json&lt;/code&gt; or by starting Gemini CLI with the flag &lt;code&gt;--telemetry&lt;/code&gt;. Additionally, you choose the target for the telemetry data: it can be logged locally or sent to a backend like Google Cloud. For a quick start, you might set &lt;code&gt;"telemetry.target": "local"&lt;/code&gt; - with this, Gemini will simply write telemetry data to a local file (by default) or to a custom path you specify via &lt;code&gt;["outfile"](https://google-gemini.github.io/gemini-cli/docs/cli/telemetry.html#:~:text=disable%20telemetry%20,file%20path)&lt;/code&gt;. The local telemetry includes JSON logs you can parse or feed into tools. For more robust monitoring, set &lt;code&gt;"target": "gcp"&lt;/code&gt; (Google Cloud) or even integrate with other OpenTelemetry-compatible systems like Jaeger or Datadog. In fact, Gemini CLI's OTEL support is vendor-neutral - you can export data to just about any observability stack you prefer (Google Cloud Operations, Prometheus, etc.. Google provides a streamlined path for Cloud: if you point to GCP, the CLI can send data directly to Cloud Logging and Cloud Monitoring in your project, where you can use the usual dashboards and alerting tools.&lt;/p&gt;
    &lt;p&gt;What kind of insights can you get? The telemetry captures events like tool executions, errors, and important milestones. It also records metrics such as prompt processing time and token counts per prompt. For usage analytics, you might aggregate how many times each slash command is used across your team, or how often code generation is invoked. For performance monitoring, you could track if responses have gotten slower, which might indicate hitting API rate limits or model changes. And for debugging, you can see errors or exceptions thrown by tools (e.g., a &lt;code&gt;run_shell_command&lt;/code&gt; failure) logged with context. All this data can be visualized if you send it to a platform like Google Cloud's Monitoring - for example, you can create a dashboard of "tokens used per day" or "error rate of tool X". It essentially gives you a window into the AI's "brain" and your usage, which is especially helpful in enterprise settings to ensure everything runs smoothly.&lt;/p&gt;
    &lt;p&gt;Enabling telemetry does introduce some overhead (extra data processing), so you might not keep it on 100% of the time for personal use. However, it's fantastic for debugging sessions or for intermittent health checks. One approach is to enable it on a CI server or in your team's shared environment to collect stats, while leaving it off locally unless needed. Remember, you can always toggle it on the fly: update settings and use &lt;code&gt;/memory refresh&lt;/code&gt; if needed to reload, or restart Gemini CLI with &lt;code&gt;--telemetry&lt;/code&gt; flag. Also, all telemetry is under your control - it respects your environment variables for endpoint and credentials, so data goes only where you intend it to. This feature turns Gemini CLI from a black box into an observatory, shining light on how the AI agent interacts with your world, so you can continuously improve that interaction.&lt;/p&gt;
    &lt;p&gt;Pro Tip: If you just want a quick view of your current session's stats (without full telemetry), use the &lt;code&gt;/stats&lt;/code&gt; command. It will output metrics like token usage and session length right in the CLI. This is a lightweight way to see immediate numbers. But for long-term or multi-session analysis, telemetry is the way to go. And if you're sending telemetry to a cloud project, consider setting up dashboards or alerts (e.g., alert if error rate spikes or token usage hits a threshold) - this can proactively catch issues in how Gemini CLI is being used in your team.&lt;/p&gt;
    &lt;p&gt;Quick use-case: Stay informed about upcoming Gemini CLI features - by following the public Gemini CLI roadmap, you'll know about major planned enhancements (like background agents for long-running tasks) before they arrive, allowing you to plan and give feedback.&lt;/p&gt;
    &lt;p&gt;Gemini CLI is evolving rapidly, with new releases coming out frequently, so it's wise to track what's on the horizon. Google maintains a public roadmap for Gemini CLI on GitHub, detailing the key focus areas and features targeted for the near future. This is essentially a living document (and set of issues) where you can see what the developers are working on and what's in the pipeline. For instance, one exciting item on the roadmap is support for background agents - the ability to spawn autonomous agents that run in the background to handle tasks continuously or asynchronously. According to the roadmap discussion, these background agents would let you delegate long-running processes to Gemini CLI without tying up your interactive session. You could, say, start a background agent that monitors your project for certain events or periodically executes tasks, either on your local machine or even by deploying to a service like Cloud Run. This feature aims to "enable long-running, autonomous tasks and proactive assistance" right from the CLI, essentially extending Gemini CLI's usefulness beyond just on-demand queries.&lt;/p&gt;
    &lt;p&gt;By keeping tabs on the roadmap, you'll also learn about other planned features. These could include new tool integrations, support for additional Gemini model versions, UI/UX improvements, and more. The roadmap is usually organized by "areas" (for example, Extensibility, Model, Background, etc.) and often tagged with milestones (like a target quarter for delivery]. It's not a guarantee of when something will land, but it gives a good idea of the team's priorities. Since the project is open-source, you can even dive into the linked GitHub issues for each roadmap item to see design proposals and progress. For developers who rely on Gemini CLI, this transparency means you can anticipate changes - maybe an API is adding a feature you need, or a breaking change might be coming that you want to prepare for.&lt;/p&gt;
    &lt;p&gt;Following the roadmap can be as simple as bookmarking the GitHub project board or issue labeled "Roadmap" and checking periodically. Some major updates (like the introduction of Extensions or the IDE integration) were hinted at in the roadmap before they were officially announced, so you get a sneak peek. Additionally, the Gemini CLI team often encourages community feedback on those future features. If you have ideas or use cases for something like background agents, you can usually comment on the issue or discussion thread to influence its development.&lt;/p&gt;
    &lt;p&gt;Pro Tip: Since Gemini CLI is open source (Apache 2.0 licensed), you can do more than just watch the roadmap - you can participate! The maintainers welcome contributions, especially for items aligned with the roadmap. If there's a feature you really care about, consider contributing code or testing once it's in preview. At the very least, you can open a feature request if something you need isn't on the roadmap yet. The roadmap page itself provides guidance on how to propose changes. Engaging with the project not only keeps you in the loop but also lets you shape the tool that you use. After all, Gemini CLI is built with community involvement in mind, and many recent features (like certain extensions and tools) started as community suggestions.&lt;/p&gt;
    &lt;p&gt;Quick use-case: Add new capabilities to Gemini CLI by installing plug-and-play extensions - for example, integrate with your favorite database or cloud service - expanding the AI's toolset without any heavy lifting on your part. It's like installing apps for your CLI to teach it new tricks.&lt;/p&gt;
    &lt;p&gt;Extensions are a game-changer introduced in late 2025: they allow you to customize and expand Gemini CLI's functionality in a modular way. An extension is essentially a bundle of configurations (and optionally code) that connects Gemini CLI to an external tool or service. For instance, Google released a suite of extensions for Google Cloud - there's one that helps deploy apps to Cloud Run, one for managing BigQuery, one for analyzing application security, and more. Partners and community developers have built extensions for all sorts of things: Dynatrace (monitoring), Elastic (search analytics), Figma (design assets), Shopify, Snyk (security scans), Stripe (payments), and the list is growing. By installing an appropriate extension, you instantly grant Gemini CLI the ability to use new domain-specific tools. The beauty is that these extensions come with a pre-defined "playbook" that teaches the AI how to use the new tools effectively. That means once installed, you can ask Gemini CLI to perform tasks with those services and it will know the proper APIs or commands to invoke, as if it had that knowledge built-in.&lt;/p&gt;
    &lt;p&gt;Using extensions is very straightforward. The CLI has a command to manage them: &lt;code&gt;gemini extensions install &amp;lt;URL&amp;gt;&lt;/code&gt;. Typically, you provide the URL of the extension's GitHub repo or a local path, and the CLI will fetch and install it. For example, to install an official extension, you might run: &lt;code&gt;gemini extensions install https://github.com/google-gemini/gemini-cli-extension-cloud-run&lt;/code&gt;. Within seconds, the extension is added to your environment (stored under &lt;code&gt;~/.gemini/extensions/&lt;/code&gt; or your project's &lt;code&gt;.gemini/extensions/&lt;/code&gt; folder). You can then see it by running &lt;code&gt;/extensions&lt;/code&gt; in the CLI, which lists active extensions. From that point on, the AI has new tools at its disposal. If it's a Cloud Run extension, you could say "Deploy my app to Cloud Run," and Gemini CLI will actually be able to execute that (by calling the underlying &lt;code&gt;gcloud&lt;/code&gt; commands through the extension's tools). Essentially, extensions function as first-class expansions of Gemini CLI's capabilities, but you opt-in to the ones you need.&lt;/p&gt;
    &lt;p&gt;There's an open ecosystem around extensions. Google has an official Extensions page listing available extensions, and because the framework is open, anyone can create and share their own. If you have a particular internal API or workflow, you can build an extension for it so that Gemini CLI can assist with it. Writing an extension is easier than it sounds: you typically create a directory (say, &lt;code&gt;my-extension/&lt;/code&gt;) with a file &lt;code&gt;gemini-extension.json&lt;/code&gt; describing what tools or context to add. You might define new slash commands or specify remote APIs the AI can call. No need to modify Gemini CLI's core - just drop in your extension. The CLI is designed to load these at runtime. Many extensions consist of adding custom MCP tools (Model Context Protocol servers or functions) that the AI can use. For example, an extension could add a &lt;code&gt;/translate&lt;/code&gt; command by hooking into an external translation API; once installed, the AI knows how to use &lt;code&gt;/translate&lt;/code&gt;. The key benefit is modularity: you install only the extensions you want, keeping the CLI lightweight, but you have the option to integrate virtually anything.&lt;/p&gt;
    &lt;p&gt;To manage extensions, besides the &lt;code&gt;install&lt;/code&gt; command, you can update or remove them via similar CLI commands (&lt;code&gt;gemini extensions update&lt;/code&gt; or just by removing the folder). It's wise to occasionally check for updates on extensions you use, as they may receive improvements. The CLI might introduce an "extensions marketplace" style interface in the future, but for now, exploring the GitHub repositories and official catalog is the way to discover new ones. Some popular ones at launch include the GenAI Genkit extension (for building generative AI apps), and a variety of Google Cloud extensions that cover CI/CD, database admin, and more.&lt;/p&gt;
    &lt;p&gt;Pro Tip: If you're building your own extension, start by looking at existing ones for examples. The official documentation provides an Extensions Guide with the schema and capabilities. A simple way to create a private extension is to use the &lt;code&gt;@include&lt;/code&gt; functionality in &lt;code&gt;GEMINI.md&lt;/code&gt; to inject scripts or context, but a full extension gives you more power (like packaging tools). Also, since extensions can include context files, you can use them to preload domain knowledge. Imagine an extension for your company's internal API that includes a summary of the API and a tool to call it - the AI would then know how to handle requests related to that API. In short, extensions open up a new world where Gemini CLI can interface with anything. Keep an eye on the extensions marketplace for new additions, and don't hesitate to share any useful extension you create with the community - you might just help thousands of other developers.&lt;/p&gt;
    &lt;p&gt;Lastly, not a productivity tip but a delightful easter egg - try the command &lt;code&gt;*/corgi*&lt;/code&gt; in Gemini CLI. This toggles "corgi mode", which makes a cute corgi animation run across your terminal! It doesn't help you code any better, but it can certainly lighten the mood during a long coding session. You'll see an ASCII art corgi dashing in the CLI interface. To turn it off, just run &lt;code&gt;/corgi&lt;/code&gt; again.&lt;/p&gt;
    &lt;p&gt;This is a purely for-fun feature the team added (and yes, there's even a tongue-in-cheek debate about spending dev time on corgi mode). It shows that the creators hide some whimsy in the tool. So when you need a quick break or a smile, give &lt;code&gt;/corgi&lt;/code&gt; a try. 🐕🎉&lt;/p&gt;
    &lt;p&gt;(Rumor has it there might be other easter eggs or modes - who knows? Perhaps a "/partyparrot" or similar. The cheat sheet or help command lists &lt;code&gt;/corgi&lt;/code&gt;, so it's not a secret, just underused. Now you're in on the joke!)&lt;/p&gt;
    &lt;p&gt;Conclusion:&lt;/p&gt;
    &lt;p&gt;We've covered a comprehensive list of pro tips and features for Gemini CLI. From setting up persistent context with &lt;code&gt;GEMINI.md&lt;/code&gt;, to writing custom commands and using advanced tools like MCP servers, to leveraging multi-modal inputs and automating workflows, there's a lot this AI command-line assistant can do. As an external developer, you can integrate Gemini CLI into your daily routine - it's like a powerful ally in your terminal that can handle tedious tasks, provide insights, and even troubleshoot your environment.&lt;/p&gt;
    &lt;p&gt;Gemini CLI is evolving rapidly (being open-source with community contributions), so new features and improvements are constantly on the horizon. By mastering the pro tips in this guide, you'll be well-positioned to harness the full potential of this tool. It's not just about using an AI model - it's about integrating AI deeply into how you develop and manage software.&lt;/p&gt;
    &lt;p&gt;Happy coding with Gemini CLI, and have fun exploring just how far your "AI agent in the terminal" can take you.&lt;/p&gt;
    &lt;p&gt;You now have a Swiss-army knife of AI at your fingertips - use it wisely, and it will make you a more productive (and perhaps happier) developer!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46060508</guid><pubDate>Wed, 26 Nov 2025 18:08:02 +0000</pubDate></item><item><title>China Has Three Reusable Rockets Ready for Their Debut Flights</title><link>https://www.china-in-space.com/p/china-has-three-reusable-rockets</link><description>&lt;doc fingerprint="33bccaf3aed4249a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;China Has Three Reusable Rockets Ready for Their Debut Flights&lt;/head&gt;
    &lt;head rend="h3"&gt;One of the three launch vehicles will be China’s first and the first outside the United States.&lt;/head&gt;
    &lt;p&gt;Three of China’s space enterprises are near the debut flights of their partially reusable rockets, expected to liftoff before the end of the year.&lt;/p&gt;
    &lt;p&gt;Around November 25th, the Shanghai Academy of Spaceflight Technology’s Long March 12A partially reusable launch vehicle1 was spotted heading for its launch pad the the Jiuquan Satellite Launch Center, for its first public appearance of a full vehicle. The liquid methane and liquid oxygen burning rocket has two 3.8-meter wide stages, with the first equipped with seven Longyun engines from Jiuzhou Yunjian (九州云箭) and the second with a single vacuum optimized YF-209, to carry up to 12,000 kilograms. First-stage reuse will be achieved by an engine performing a landing burn to touchdown on four legs, with grid fins guiding it before that.&lt;/p&gt;
    &lt;p&gt;Details on development for the Long March 12A have been hard to come by as few have been released. In January, a largely successful high-altitude hop test occurred, succumbing to software glitches during splashdown. Around August, a second-stage static fire was completed in Haiyang (海阳市). Lastly in November, the rockets transporter-erector was delivered. What has been trackable is Jiuzhou Yunjian’s efforts on verifying its engines for reusable operation.&lt;/p&gt;
    &lt;p&gt;Due to the opaque nature of the Long March 12A’s development, it is unknown if the launch vehicle at Jiuquan will wrap up the overall development campaign, possibly with a static fire, before a debut flight later in December.&lt;/p&gt;
    &lt;p&gt;Meanwhile, LandSpace’s 66-meter-tall, 4.5-meter-wide Zhuque-3 is on its Jiuquan launch pad too, following delivery in October. Like the Long March 12A, the rocket burns liquid methane and liquid oxygen, but has two more engines, LandSpace’s TQ-12A, on its first-stage and one vacuum-optimized TQ-15A engine on the second-stage, to deliver up to 11,800 kilograms in its ‘block one’ configuration. Similar to the Shanghai Academy’s rocket, Zhuque-3’s first-stage will touchdown on four landing legs following an engine burn, with four grid fins guiding it through the atmosphere.&lt;/p&gt;
    &lt;p&gt;Zhuque-3 has had a highly successful test campaign during its just over two-year-long development process. In September 2024, the launch vehicle’s in-atmosphere hop-testing campaign was completed with a 10-kilometer flight that saw an engine relight for touchdown. That was followed by a 45-second static fire in June, later matched by flight hardware performing a similar static fire with a second-stage on top. Hardware has also been flown with the company’s Zhuque-2 and Zhuque-2E launch vehicles as well.&lt;/p&gt;
    &lt;p&gt;Along with the two methane-fueled rockets, Space Pioneer’s Tianlong-3 is also at Jiuquan, having arrived sometime in November. The two-stage 72-meter-tall, 3.8-meter-wide launch burns rocket-grade kerosene and liquid oxygen to carry up to 17,000 kilograms to low Earth orbit, with nine TH-12 engines on the first-stage and a single vacuum-optimized one on the second-stage. Tianlong-3's first-stage is planned to land on four landing legs, guided by four grid fins, with an engine burn providing the soft touchdown needed.&lt;/p&gt;
    &lt;p&gt;In the lead-up to launch, Tianlong-3 conducted its first wholly successful static fire in September and skipped a second-stage firing, having confidence in the singular engine powering it following its development campaign. At the moment, the launch vehicle is on its dedicated launchpad at the launch site for integrated testing with ground systems. Notably, no reuse hardware has been installed yet, and mounting points appear to be missing.&lt;/p&gt;
    &lt;p&gt;Out of the Long March 12A, Zhuque-3, and Tianlong-3, LandSpace may fly China’s first reusable rocket. Despite a current lack of hazard notices, news outlets are saying November 29th is the first targeted date. LandSpace has vaguely denied that date, asking enthusiasts to do diligent research. As for the other two rockets, Space Pioneer and the Shanghai Academy of Spaceflight Technology are yet to share relevant information2.&lt;/p&gt;
    &lt;p&gt;First-stage booster landing sites have been completed for both Zhuque-3 and the Long March 12A in previous months. Those sites are expected to have systems for safing the boosters following touchdown as well as fire suppression systems in the event of an anomaly. LandSpace and the Shanghai Academy are eyeing first-stage landings during the debut flights. Whichever lands first will be the third globally and the first outside of the United States, following SpaceX’s Falcon 9 in 2015 and Blue Origin’s New Glenn on November 13th 2025.&lt;/p&gt;
    &lt;p&gt;No major Jiuquan-side holdups are expected to slow the debut flights of the three rockets. During the past month, the China Manned Space Agency had priority use of the site for the launch of the Shenzhou-21 mission, return of the Shenzhou-20 crew, and ‘emergency response’ launch of the Shenzhou-22 spacecraft.&lt;/p&gt;
    &lt;p&gt;When the three rockets do debut, they will be a boon to the deployment efforts of China’s various mega-constellations, as reuse will allow for cheaper and more frequent launch missions. Back in August, Shanghai Spacesail Technologies, the operator of the Qianfan (千帆) constellation, awarded contracts to LandSpace and Space Pioneer to prove they can launch satellite batches with their partially reusable rockets, with Tianlong-3 looking to deliver larger satellite groups.&lt;/p&gt;
    &lt;p&gt;Loosely based upon the thrice successful Long March 12 launch vehicle, also from the Shanghai Academy of Spaceflight Technology.&lt;/p&gt;
    &lt;p&gt;As a state-owned enterprise, the Shanghai Academy of Spaceflight Technology may be given priority use of Jiuquan’s resources, thus launching before the other two.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46060935</guid><pubDate>Wed, 26 Nov 2025 18:45:43 +0000</pubDate></item><item><title>API that auto-routes to the cheapest AI provider (OpenAI/Anthropic/Gemini)</title><link>https://tokensaver.org/</link><description>&lt;doc fingerprint="b64b20c105bbb8b1"&gt;
  &lt;main&gt;
    &lt;p&gt;Automatically route your AI requests to the cheapest provider. OpenAI, Anthropic, or Google Gemini. Real-time pricing. Zero lock-in.&lt;/p&gt;
    &lt;p&gt;Stop overpaying for AI. Our routing engine finds the best price in real-time.&lt;/p&gt;
    &lt;p&gt;Automatically routes to the cheapest provider. Save 90-99% compared to using premium models directly. Your budget goes further.&lt;/p&gt;
    &lt;p&gt;Automatic fallback if one provider fails. Your app stays online even when individual AI services go down.&lt;/p&gt;
    &lt;p&gt;One simple API works with all providers. We handle the routing logic, SDK differences, and price monitoring.&lt;/p&gt;
    &lt;p&gt;See exactly which provider was used, token counts, and costs for every request. No hidden fees or surprises.&lt;/p&gt;
    &lt;p&gt;We automatically select the best option for each request.&lt;/p&gt;
    &lt;p&gt;No subscriptions. No minimums. No commitments.&lt;/p&gt;
    &lt;p&gt;Billed per request via Stripe. View your usage anytime in the customer dashboard.&lt;/p&gt;
    &lt;p&gt;Choose your language and start making requests.&lt;/p&gt;
    &lt;p&gt;Enterprise-grade security for your peace of mind.&lt;/p&gt;
    &lt;p&gt;All payments processed by Stripe, a PCI-DSS Level 1 certified provider. We never see your card details.&lt;/p&gt;
    &lt;p&gt;All data encrypted in transit (TLS 1.3) and at rest (AES-256). Hosted on enterprise infrastructure.&lt;/p&gt;
    &lt;p&gt;Your API requests are processed and immediately forwarded. We never store or log conversation content.&lt;/p&gt;
    &lt;p&gt;We only store your email and usage records. Nothing else. Your data stays yours.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46061232</guid><pubDate>Wed, 26 Nov 2025 19:12:26 +0000</pubDate></item><item><title>S&amp;box is now an open source game engine</title><link>https://sbox.game/news/update-25-11-26</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46061682</guid><pubDate>Wed, 26 Nov 2025 19:58:27 +0000</pubDate></item><item><title>The most male and female reasons to end up hospital</title><link>https://leobenedictus.substack.com/p/the-most-male-and-female-reasons</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46061714</guid><pubDate>Wed, 26 Nov 2025 20:01:41 +0000</pubDate></item><item><title>Crews Claim Boring Company Failed to Pay Workers and Snubbed OSHA Concerns</title><link>https://nashvillebanner.com/2025/11/25/boring-company-nashville-shane-trucking-and-excavating/</link><description>&lt;doc fingerprint="ba291128beb06f6"&gt;
  &lt;main&gt;
    &lt;p&gt;Willie Shane broke the asphalt on Elon Musk’s Music City Loop project this summer. Seven of his crew had been the sole excavators, fabricators and dump trucking company on The Boring Company’s proposed tunnel through Nashville for months.&lt;/p&gt;
    &lt;p&gt;Then came Monday night, when they walked off the site.&lt;/p&gt;
    &lt;p&gt;“I moved the equipment myself,” Shane said in an interview with the Banner on Tuesday.&lt;/p&gt;
    &lt;p&gt;“We were really skeptical from the beginning, and then since then, things pretty much just went downhill,” he added.&lt;/p&gt;
    &lt;p&gt;Musk’s company has a spotty record of completing similar tunnels in other cities, often snagging on government regulations and contractual issues. When Shane’s company, Shane Trucking and Excavating, which works with major local clients like the Grand Ole Opry and the Nashville International Airport, was approached by The Boring Company, he said he had some reservations.&lt;/p&gt;
    &lt;p&gt;“I told them very bluntly — and I don’t want this to come across like egotistical — but I told them, ‘Hey, my dad worked really hard to build a reputation in Nashville, and my brother and I work very hard to keep that reputation,’” Shane said. “If you guys are actually serious about doing this, you need to be 100 percent serious, because this is going to be our reputation as part of this too.”&lt;/p&gt;
    &lt;p&gt;After being reassured, Shane’s team took the job in July.&lt;/p&gt;
    &lt;p&gt;He and his crew left the state-owned property on Rosa L Parks Boulevard, where they had been working on the proposed 9-mile tunnel from the state capitol to the airport after months of safety and financial issues with Musk’s company.&lt;/p&gt;
    &lt;p&gt;It started about a month in with a change in pay.&lt;/p&gt;
    &lt;p&gt;“We were supposed to be paid every 15 days. And then they switched accounting firms, and then it went from 15 days to 60,” Shane said. Now it’s been 123 days since they started digging, and Shane says The Boring Company has only paid out about five percent of what he’s owed.&lt;/p&gt;
    &lt;p&gt;According to Shane, he has still been able to pay his employees on time, but the local trucking company is left holding the bag for money unpaid by The Boring Company. Other subcontractors, he says, have also severed ties due to nonpayment on the project.&lt;/p&gt;
    &lt;p&gt;The final straw that caused Shane to pull his crew from the site was when multiple employees reported that a representative of The Boring Company was soliciting them to bail on Shane and work directly for TBC on Monday.&lt;/p&gt;
    &lt;p&gt;“One of their head guys texts two of my welders, offering them a job for $45 an hour from his work phone,” Shane described, noting that the same TBC employee denied sending the texts when confronted with screenshots. “That’s actually a breach of contract.”&lt;/p&gt;
    &lt;p&gt;Shane also says he and other vendors have filed multiple OSHA safety complaints since working on the site but have gotten no response. His biggest concerns have been Boring employees on the jobsite not wearing proper personal protective equipment, such as hard hats, and unsafe shoring, which he says he’s repeatedly complained about to the Boring Company.&lt;/p&gt;
    &lt;p&gt;“Where we’re digging, we’re so far down, there should be concrete and different structures like that to hold the slope back from falling on you while you’re working,” Shane explained. “Where most people use concrete, they currently have — I’m not even kidding — they currently have wood. They had us install wood 2x12s.”&lt;/p&gt;
    &lt;p&gt;The safety concerns are why Shane says he decided to make the issue public.&lt;/p&gt;
    &lt;p&gt;“We’re not coming forward in like a vindictive way,” Shane said. “I just don’t want someone to get hurt, sure, and then, in the future, I have to be like, ‘Dang, I worked on there, and I turned a blind eye to it.’”&lt;/p&gt;
    &lt;p&gt;In the meantime, Shane said that the amount of backpay owed to his company is in the six figures and that he has retained a lawyer.&lt;/p&gt;
    &lt;head rend="h3"&gt;Boring Company response&lt;/head&gt;
    &lt;p&gt;After the Banner contacted The Boring Company about Shane’s claims, Vice President David Buss said he connected with Shane and would make good on the outstanding invoices by the end of the day Wednesday and would do a “full audit” on the error.&lt;/p&gt;
    &lt;p&gt;“It does look like we had some invoicing errors on that,” Buss told the Banner. “It was, you know, unfortunately, too common of a thing, but I assured them that we are going to make sure that invoices are wired tomorrow.”&lt;/p&gt;
    &lt;p&gt;Buss later clarified that he does not believe The Boring Company has a “common” practice of missing payments to vendors, but rather missed payments happen sometimes during “the normal course of business.”&lt;/p&gt;
    &lt;p&gt;“You hate to have an unhappy vendor. We certainly aim to have great relationships,” Buss said. “And so my goal will be to figure out what happened in this incident and then make sure that that’s not extrapolated to any other incidents.”&lt;/p&gt;
    &lt;p&gt;Buss also said he was looking into Shane’s claims about The Boring Company trying to hire contractors.&lt;/p&gt;
    &lt;p&gt;“It is definitely not our practice to try to poach anybody, so I understand the frustrations on their side,” Buss said. “Hopefully it’s something where we’re able to smooth that over and correct some of the things that happened on site and that led to this.”&lt;/p&gt;
    &lt;p&gt;Asked about the safety complaints, Buss said Shane did not raise any concerns on their call Tuesday and said he was unaware of any OSHA complaints, but would look into it.&lt;/p&gt;
    &lt;p&gt;“Safety is existential to our company,” Buss said. “We thankfully have a long history of seven years of tunneling in Las Vegas, and we’ve had one construction-related injury that was not the company’s fault in a violation.”&lt;/p&gt;
    &lt;head rend="h3"&gt;Hiring headaches&lt;/head&gt;
    &lt;p&gt;According to Buss, the projected timeline had not changed, and work had not been slowed by the crews’ departure from the site. Shane, however, painted a different picture.&lt;/p&gt;
    &lt;p&gt;“Actually, we were the crew that was building the tunnel boring machine. So there’s nobody building the tunnel boring machine right now, and the Boring Company has been trying to hire welders, but they haven’t been able to secure any help,” Shane said Tuesday, noting that many prospective employees won’t work on the project because of Musk’s reputation.&lt;/p&gt;
    &lt;p&gt;“A lot of people don’t like Elon and their payment terms; the way that they pay their employees, is not traditional,” Shane said.&lt;/p&gt;
    &lt;p&gt;Buss denied any hiring trouble.&lt;/p&gt;
    &lt;p&gt;“We’ve had zero issues finding great talent thus far in Nashville,” Buss said. “I think we’ve hired about 14 people now, and we’re going to start to grow the team as we begin mining operations.”&lt;/p&gt;
    &lt;p&gt;Instability and safety have been pervasive concerns around the project since its hurried public rollout this summer, in which little-to-no public input was received by the state before approving a lease of the state-owned property where digging is taking place.&lt;/p&gt;
    &lt;p&gt;As reports of a second Boring tunnel under Broadway and West End surfaced, Boring Company CEO Steve Davis hosted a two-hour live update session on X, the social media website also owned by Musk Monday evening, in which he touted progress on the Music City Loop and described the project as smoothly underway, with boring set to begin around January after the proper permits are secured.&lt;/p&gt;
    &lt;p&gt;An hour later, Shane’s team left the site.&lt;/p&gt;
    &lt;p&gt;During Davis’ virtual meeting, members of the public could submit questions, some of which were answered by Boring Company leadership. Many of those questions came from State Sen. Heidi Campbell (D-Nashville), who represents the area and has been a vocal critic of the project since it was announced.&lt;/p&gt;
    &lt;p&gt;“I would say the promotional session that they had last night on on Twitter was disingenuous at best, if not dishonest, because it was, it sounded like a utopian project and then, lo and behold, the very next day, we find out that there are people leaving the site because they’re not getting paid and they’re not being treated well,” Campbell told the Banner.&lt;/p&gt;
    &lt;p&gt;In addition to her concerns about irreparable damage to the site and whether the project would even be completed, Campbell said she was concerned about the state’s liability if there were unsafe working conditions on the leased property and whether there was any way for lawmakers to stop the process.&lt;/p&gt;
    &lt;p&gt;“There is nothing to hold The Boring Company accountable for any of these things,” Campbell said of the lease. “They’ve already dug a big hole. But then on top of it, if they move forward, forward in any capacity, they have not proven that they are reliable to take care of the damage that they cause.”&lt;/p&gt;
    &lt;p&gt;When Shane first spoke to the Banner, he said he did not intend to return to the job even if they received payment, noting that his employees had expressed discomfort “because they didn’t feel the management there was very good.”&lt;/p&gt;
    &lt;p&gt;Hours later, after hearing from Buss, Shane said he would consider returning “if they correct the situation on their end.”&lt;/p&gt;
    &lt;p&gt;Demetria Kalodimos contributed to this report.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46061840</guid><pubDate>Wed, 26 Nov 2025 20:14:17 +0000</pubDate></item></channel></rss>