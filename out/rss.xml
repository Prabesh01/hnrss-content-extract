<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Sun, 28 Dec 2025 23:37:43 +0000</lastBuildDate><item><title>Learn computer graphics from scratch and for free</title><link>https://www.scratchapixel.com</link><description>&lt;doc fingerprint="94021bf895f5cbe4"&gt;
  &lt;main&gt;
    &lt;p&gt;These lessons are structured to introduce 3D rendering concepts in a beginner-friendly order. Unlike most resources, we start with hands-on results before diving into theory.&lt;/p&gt;
    &lt;p&gt;This section is dedicated to explaining the mathematical theories and tools used in creating images and simulations with a computer. It's not intended as a starting point, but rather as a reference to be consulted when these topics are mentioned in lessons from other sections.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46410210</guid><pubDate>Sun, 28 Dec 2025 11:08:22 +0000</pubDate></item><item><title>Building a macOS app to know when my Mac is thermal throttling</title><link>https://stanislas.blog/2025/12/macos-thermal-throttling-app/</link><description>&lt;doc fingerprint="25dc742d3b2ac9ec"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Building a macOS app to know when my Mac is thermal throttling&lt;/head&gt;
    &lt;head class="block cursor-pointer bg-neutral-100 py-1 ps-5 text-lg font-semibold text-neutral-800 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden"&gt;Table of Contents&lt;/head&gt;
    &lt;p&gt;This is the story about how I built MacThrottle.&lt;/p&gt;
    &lt;p&gt;I‚Äôve been very happy with my M2 MacBook Air for the past few years. However, when using an external display, especially a very demanding one like a 4K 120Hz display, I‚Äôve noticed it started struggling more. Since it lacks fans, you can‚Äôt hear it struggling, but you can feel it as everything becomes very slow or unresponsive: that‚Äôs when thermal throttling kicks in.&lt;/p&gt;
    &lt;p&gt;I know it‚Äôs thermal throttling because I can see in iStat Menus that my CPU usage is 100% while the power usage in watts goes down.&lt;/p&gt;
    &lt;p&gt;It‚Äôs even more obvious with MX Power Gadget: You can see the power usage and frequency of the performance core dropping, as usage keeps being 100%:&lt;/p&gt;
    &lt;p&gt;I‚Äôve also hit thermal throttling with my work MacBook Pro. It‚Äôs the 14" M4 Max variant, which is the worst variant because the thermal envelope of the 14" is too small for the max output for the M4 Max. On my previous 14" M1 Pro MacBook Pro, I‚Äôve never even heard the fans in 3 years‚Ä¶&lt;/p&gt;
    &lt;p&gt;That being said, I still love Apple Silicon for the performance and power usage, it‚Äôs still a dramatic improvement over the Intel days. ü´∂&lt;/p&gt;
    &lt;p&gt;Anyway, I wanted to know: is there a way to tell if the Apple Silicon SoC is thermal throttling, that is not based on heuristics like in my screenshot?&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting the thermal state programmatically #&lt;/head&gt;
    &lt;p&gt;This was a wilder ride than I expected. It‚Äôs possible to know programmatically if the Mac is throttled, because macOS exposes this in various but inconsistent ways.&lt;/p&gt;
    &lt;p&gt;The approach that Apple recommends is to use &lt;code&gt;ProcessInfo.thermalState&lt;/code&gt; from &lt;code&gt;Foundation&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;‚ûú  ~ swift -e 'import Foundation; print(["nominal", "fair", "serious", "critical"][ProcessInfo.processInfo.thermalState.rawValue])'
nominal
&lt;/code&gt;
    &lt;p&gt;Sounds good, right? However, I knew that another tool could provide this information, though it needed root: &lt;code&gt;powermetrics&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;‚ûú  ~ sudo powermetrics -s thermal

Password:
Machine model: Mac14,2
OS version: 25B78
Boot arguments:
Boot time: Sun Nov 23 10:19:29 2025



*** Sampled system activity (Wed Dec 17 09:48:34 2025 +0100) (5001.07ms elapsed) ***



**** Thermal pressure ****

Current pressure level: Nominal


*** Sampled system activity (Wed Dec 17 09:48:39 2025 +0100) (5001.25ms elapsed) ***



**** Thermal pressure ****

Current pressure level: Nominal
&lt;/code&gt;
    &lt;p&gt;(yes the output has that many newlines)&lt;/p&gt;
    &lt;p&gt;Both report the pressure level to be ‚Äúnominal‚Äù, they must be the same‚Ä¶right?&lt;/p&gt;
    &lt;p&gt;After running a few stress tests &lt;code&gt;stress-ng --cpu 0 -t 600&lt;/code&gt;, I started to see the two values diverge!&lt;/p&gt;
    &lt;p&gt;For some reason, the granularity is different between &lt;code&gt;ProcessInfo.thermalState&lt;/code&gt; and &lt;code&gt;powermetrics&lt;/code&gt;. They have a different amount of possible states and they don‚Äôt line up.&lt;/p&gt;
    &lt;p&gt;Here is my empirical experience:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;
          &lt;code&gt;ProcessInfo.thermalState&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell role="head"&gt;
          &lt;code&gt;powermetrics&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;nominal&lt;/cell&gt;
        &lt;cell&gt;nominal&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;fair&lt;/cell&gt;
        &lt;cell&gt;moderate&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;fair&lt;/cell&gt;
        &lt;cell&gt;heavy&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;I never managed to hit these states, so I don‚Äôt know if they match, but they‚Äôre technically defined:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;
          &lt;code&gt;ProcessInfo.thermalState&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell role="head"&gt;
          &lt;code&gt;powermetrics&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;serious&lt;/cell&gt;
        &lt;cell&gt;trapping&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;critical&lt;/cell&gt;
        &lt;cell&gt;sleeping&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;In practice, when my Mac starts getting hot, from the &lt;code&gt;powermetrics&lt;/code&gt; perspective it goes into &lt;code&gt;moderate&lt;/code&gt;, and when it starts throttling, it goes into &lt;code&gt;heavy&lt;/code&gt;. The problem is that with &lt;code&gt;ProcessInfo&lt;/code&gt;, both are covered by the &lt;code&gt;fair&lt;/code&gt; state, so it‚Äôs not really useful to know when the Mac is actually throttling. ‚òπÔ∏è&lt;/p&gt;
    &lt;p&gt;I thought maybe this was an iOS vs macOS thing? But Apple references it in the macOS docs as well. Maybe it was more consistent on Intel Macs?&lt;/p&gt;
    &lt;p&gt;I stumbled upon this article from Dave MacLachlan, a Googler working on Apple stuff, from 2020. I learned that there are other CLI tools to get thermal data, but they don‚Äôt seem to work on my Apple Silicon MacBook:&lt;/p&gt;
    &lt;code&gt;‚ûú sudo thermal levels
Thermal levels are unsupported on this machine.
&lt;/code&gt;
    &lt;code&gt;‚ûú sudo pmset -g thermlog
Note: No thermal warning level has been recorded
Note: No performance warning level has been recorded
Note: No CPU power status has been recorded
^C
&lt;/code&gt;
    &lt;p&gt;But the most interesting thing I learned is that the data &lt;code&gt;powermetrics&lt;/code&gt; shows is actually coming from &lt;code&gt;thermald&lt;/code&gt;. And &lt;code&gt;thermald&lt;/code&gt; writes the current thermal pressure to the Darwin notification system (&lt;code&gt;notifyd&lt;/code&gt;)!&lt;/p&gt;
    &lt;code&gt;‚ûú notifyutil -g com.apple.system.thermalpressurelevel

com.apple.system.thermalpressurelevel 0
&lt;/code&gt;
    &lt;p&gt;The various levels are defined in &lt;code&gt;OSThermalNotification.h&lt;/code&gt; according to the article. Indeed:&lt;/p&gt;
    &lt;code&gt;‚ûú  ~ xcrun --sdk macosx --show-sdk-path
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX26.2.sdk
‚ûú  ~ SDK="$(xcrun --sdk macosx --show-sdk-path)"
‚ûú  ~ find "$SDK/usr/include" -name 'OSThermalNotification.h'
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX26.2.sdk/usr/include/libkern/OSThermalNotification.h
&lt;/code&gt;
    &lt;code&gt;‚ûú  ~ head -n 52 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX26.2.sdk/usr/include/libkern/OSThermalNotification.h"
/*
 * Copyright (c) 2007 Apple Inc. All rights reserved.
 *
 * @APPLE_LICENSE_HEADER_START@
 *
 * This file contains Original Code and/or Modifications of Original Code
 * as defined in and that are subject to the Apple Public Source License
 * Version 2.0 (the 'License'). You may not use this file except in
 * compliance with the License. Please obtain a copy of the License at
 * http://www.opensource.apple.com/apsl/ and read it before using this
 * file.
 *
 * The Original Code and all software distributed under the License are
 * distributed on an 'AS IS' basis, WITHOUT WARRANTY OF ANY KIND, EITHER
 * EXPRESS OR IMPLIED, AND APPLE HEREBY DISCLAIMS ALL SUCH WARRANTIES,
 * INCLUDING WITHOUT LIMITATION, ANY WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE, QUIET ENJOYMENT OR NON-INFRINGEMENT.
 * Please see the License for the specific language governing rights and
 * limitations under the License.
 *
 * @APPLE_LICENSE_HEADER_END@
 */

#ifndef _OSTHERMALNOTIFICATION_H_
#define _OSTHERMALNOTIFICATION_H_

#include &amp;lt;_bounds.h&amp;gt;
#include &amp;lt;sys/cdefs.h&amp;gt;
#include &amp;lt;Availability.h&amp;gt;
#include &amp;lt;TargetConditionals.h&amp;gt;

_LIBC_SINGLE_BY_DEFAULT()

/*
**  OSThermalNotification.h
**
**  Notification mechanism to alert registered tasks when device thermal conditions
**  reach certain thresholds. Notifications are triggered in both directions
**  so clients can manage their memory usage more and less aggressively.
**
*/

__BEGIN_DECLS

/* Define pressure levels usable by OSThermalPressureLevel */
typedef enum {
#if TARGET_OS_OSX || TARGET_OS_MACCATALYST
        kOSThermalPressureLevelNominal = 0,
        kOSThermalPressureLevelModerate,
        kOSThermalPressureLevelHeavy,
        kOSThermalPressureLevelTrapping,
        kOSThermalPressureLevelSleeping
&lt;/code&gt;
    &lt;p&gt;The funny thing is that &lt;code&gt;OSThermalNotification.h&lt;/code&gt; is barely referenced anywhere, there are only three pages of Google results. It seems to be used in Bazel for example. That post was a big help.&lt;/p&gt;
    &lt;p&gt;What‚Äôs great about this approach is that it doesn‚Äôt require root! I can subscribe to the notification system for the &lt;code&gt;com.apple.system.thermalpressurelevel&lt;/code&gt; event to get the (good) thermal state!&lt;/p&gt;
    &lt;p&gt;Here is a snippet to get it in Swift:&lt;/p&gt;
    &lt;code&gt;import Foundation

@_silgen_name("notify_register_check")
private func notify_register_check(
  _ name: UnsafePointer&amp;lt;CChar&amp;gt;, _ token: UnsafeMutablePointer&amp;lt;Int32&amp;gt;
) -&amp;gt; UInt32
@_silgen_name("notify_get_state")
private func notify_get_state(_ token: Int32, _ state: UnsafeMutablePointer&amp;lt;UInt64&amp;gt;) -&amp;gt; UInt32
@_silgen_name("notify_cancel")
private func notify_cancel(_ token: Int32) -&amp;gt; UInt32

let notifyOK: UInt32 = 0
let name = "com.apple.system.thermalpressurelevel"

var token: Int32 = 0
let reg = name.withCString { notify_register_check($0, &amp;amp;token) }
guard reg == notifyOK else { fatalError("notify_register_check failed: \(reg)") }
defer { _ = notify_cancel(token) }

var state: UInt64 = 0
let got = notify_get_state(token, &amp;amp;state)
guard got == notifyOK else { fatalError("notify_get_state failed: \(got)") }

let label =
  switch state {
  case 0: "nominal"
  case 1: "moderate"
  case 2: "heavy"
  case 3: "trapping"
  case 4: "sleeping"
  default: "unknown(\(state))"
  }

print("\(state) \(label)")
&lt;/code&gt;
    &lt;p&gt;Prints:&lt;/p&gt;
    &lt;code&gt;‚ûú  ~ swift thermal.swift
0 nominal
&lt;/code&gt;
    &lt;p&gt;Now that I had a useful value to work with, it was time to build the app.&lt;/p&gt;
    &lt;head rend="h2"&gt;Building MacThrottle #&lt;/head&gt;
    &lt;p&gt;Armed with Opus 4.5, I set out to build a little menu bar app where I could see, at a glance, if my Apple Silicon die was trying to save itself from crossing 110¬∞C. I called it MacThrottle.&lt;/p&gt;
    &lt;p&gt;I built a simple SwiftUI app for the menu bar that shows me the status in a superbly original thermometer icon. The thermometer is filled depending on the thermal state, and its color changes from green to red. I have like 20 menu bar icons and they‚Äôre all monochromatic, so the color in the thermometer is very subtle to keep things consistent.&lt;/p&gt;
    &lt;p&gt;The app is a simple SwiftUI app. Apple provides a scene called MenuBarExtra to render a menu bar control. It was simpler than I expected! To make it a pure menu bar app with no dock icon, you just need to set &lt;code&gt;LSUIElement&lt;/code&gt; to &lt;code&gt;true&lt;/code&gt; in &lt;code&gt;Info.plist&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h3"&gt;First approach: root helper for &lt;code&gt;powermetrics&lt;/code&gt; #&lt;/head&gt;
    &lt;p&gt;I explained the various approaches to get the thermal pressure level in the previous section. But when I was building the app, I discovered later on that &lt;code&gt;thermald&lt;/code&gt; was publishing the thermal state to &lt;code&gt;notifyd&lt;/code&gt;. So at first, I thought I had to use &lt;code&gt;powermetrics&lt;/code&gt; to get useful thermal state changes. Since that unfortunately requires root access, the app needed root access too.&lt;/p&gt;
    &lt;p&gt;To reduce the scope of what runs as root, I did not run the app itself as root. Instead, the app does not work by default, but it gives you the option to install a helper. It does this through an AppleScript &lt;code&gt;with administrator privileges&lt;/code&gt; to prompt for access.&lt;/p&gt;
    &lt;p&gt;The helper is just a bash script run as a launchd daemon:&lt;/p&gt;
    &lt;code&gt;‚ûú  ~ cat /Library/LaunchDaemons/com.macthrottle.thermal-monitor.plist
&amp;lt;?xml version="1.0" encoding="UTF-8"?&amp;gt;
&amp;lt;!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd"&amp;gt;
&amp;lt;plist version="1.0"&amp;gt;
&amp;lt;dict&amp;gt;
    &amp;lt;key&amp;gt;Label&amp;lt;/key&amp;gt;
    &amp;lt;string&amp;gt;com.macthrottle.thermal-monitor&amp;lt;/string&amp;gt;
    &amp;lt;key&amp;gt;ProgramArguments&amp;lt;/key&amp;gt;
    &amp;lt;array&amp;gt;
        &amp;lt;string&amp;gt;/usr/local/bin/mac-throttle-thermal-monitor&amp;lt;/string&amp;gt;
    &amp;lt;/array&amp;gt;
    &amp;lt;key&amp;gt;RunAtLoad&amp;lt;/key&amp;gt;
    &amp;lt;true/&amp;gt;
    &amp;lt;key&amp;gt;KeepAlive&amp;lt;/key&amp;gt;
    &amp;lt;true/&amp;gt;
&amp;lt;/dict&amp;gt;
&amp;lt;/plist&amp;gt;
&lt;/code&gt;
    &lt;code&gt;‚ûú  ~ cat /usr/local/bin/mac-throttle-thermal-monitor
#!/bin/bash
OUTPUT_FILE="/tmp/mac-throttle-thermal-state"

while true; do
    THERMAL_OUTPUT=$(powermetrics -s thermal -n 1 -i 1 2&amp;gt;/dev/null | grep -i "Current pressure level")

    if echo "$THERMAL_OUTPUT" | grep -qi "sleeping"; then
        PRESSURE="sleeping"
    elif echo "$THERMAL_OUTPUT" | grep -qi "trapping"; then
        PRESSURE="trapping"
    elif echo "$THERMAL_OUTPUT" | grep -qi "heavy"; then
        PRESSURE="heavy"
    elif echo "$THERMAL_OUTPUT" | grep -qi "moderate"; then
        PRESSURE="moderate"
    elif echo "$THERMAL_OUTPUT" | grep -qi "nominal"; then
        PRESSURE="nominal"
    else
        PRESSURE="unknown"
    fi

    echo "{\"pressure\":\"$PRESSURE\",\"timestamp\":$(date +%s)}" &amp;gt; "$OUTPUT_FILE"
    chmod 644 "$OUTPUT_FILE"
    sleep 10
done
&lt;/code&gt;
    &lt;p&gt;The bash script writes the thermal state to a file every few seconds and the app reads it every few seconds!&lt;/p&gt;
    &lt;head rend="h3"&gt;Using the &lt;code&gt;thermald&lt;/code&gt; IPC notifications #&lt;/head&gt;
    &lt;p&gt;Once I discovered I could use the notification system without elevated privileges, I replaced the helper by code in the app to read the value from the notification system directly. Much simpler üéâ&lt;/p&gt;
    &lt;head rend="h3"&gt;Temperature and fans #&lt;/head&gt;
    &lt;p&gt;I wanted to show the temperature and fan speed (when supported) in a little graph in the menu bar app. This would allow me to correlate the thermal state with increased temperature, for example.&lt;/p&gt;
    &lt;p&gt;Again, there are multiple APIs to read the temperature. First, I started using an undocumented API from IOKit, but I realised I was getting ~80¬∫C max, while iStat Menus or MX Power Gadget would show &amp;gt;100¬∫C.&lt;/p&gt;
    &lt;p&gt;Stats, the open source alternative to iStat Menus, helped me use the SMC instead and get the correct values. But the SMC is a much more unstable API because each SoC has different keys to access the temperature data:&lt;/p&gt;
    &lt;code&gt;private let m1Keys = ["Tp01", "Tp05", "Tp09", "Tp0D", "Tp0H", "Tp0L", "Tp0P", "Tp0X", "Tp0b"]
private let m2Keys = ["Tp01", "Tp05", "Tp09", "Tp0D", "Tp0X", "Tp0b", "Tp0f", "Tp0j"]
private let m3Keys = ["Tf04", "Tf09", "Tf0A", "Tf0B", "Tf0D", "Tf0E", "Tf44", "Tf49", "Tf4A", "Tf4B"]
&lt;/code&gt;
    &lt;p&gt;Though the M3 keys seem to work on my M4 Max work MacBook Pro‚Ä¶&lt;/p&gt;
    &lt;p&gt;I ended up using SMC first to get the accurate temperature and fall back to IOKit if SMC doesn‚Äôt work.&lt;/p&gt;
    &lt;head rend="h3"&gt;Graph in the menu bar #&lt;/head&gt;
    &lt;p&gt;For the graph, I wanted a compact visualization that would show me the thermal history at a glance.&lt;/p&gt;
    &lt;p&gt;The graph packs three layers of information:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Colored background segments for each thermal state (green for nominal, yellow for moderate, orange for heavy, red for critical)&lt;/item&gt;
      &lt;item&gt;A solid line for CPU temperature with a dynamic Y-axis that adjusts to actual values&lt;/item&gt;
      &lt;item&gt;A dashed cyan line for fan speed percentage (on Macs that have fans)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I didn‚Äôt want to spend too much time making a super fancy graph system. Since it polls every two seconds, the graph gets very busy after a while. So I decided to keep it down to 10 minutes, since the thermal state history is mostly interesting short-term.&lt;/p&gt;
    &lt;p&gt;I also added hover tooltips using &lt;code&gt;onContinuousHover&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;When the system was under load, I noticed the graph hovering was not very smooth on my 120Hz display. I found out I can add &lt;code&gt;.drawingGroup&lt;/code&gt; to my SwiftUI canvas to use GPU rendering!. Indeed, I added it, and it was smooth again.&lt;/p&gt;
    &lt;head rend="h3"&gt;Adding macOS notifications #&lt;/head&gt;
    &lt;p&gt;I also added notifications so I get alerted when the state changes, in case I miss the menu bar icon. It can alert on specific state transitions, and optionally on recovery. This is useful to know when it‚Äôs time to kill a VS Code instance or a Docker container!&lt;/p&gt;
    &lt;p&gt;It‚Äôs true that I usually already notice when the Mac is getting slow, but sometimes the Mac gets slow when it‚Äôs swapping heavily. At least now I know when it‚Äôs just too hot.&lt;/p&gt;
    &lt;head rend="h3"&gt;Launching the app at Login #&lt;/head&gt;
    &lt;p&gt;Of course, I want the app to start automatically now, since it works so well!&lt;/p&gt;
    &lt;p&gt;I expected that I would need to write &lt;code&gt;.plist&lt;/code&gt; again, but no, it‚Äôs extremely easy to prompt the user to add a ‚Äúlogin item‚Äù as macOS calls it, using &lt;code&gt;SMAppService&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;SMAppService.mainApp.register()    // enable auto-start
SMAppService.mainApp.unregister()  // disable auto-start
SMAppService.mainApp.status == .enabled  // check current state
&lt;/code&gt;
    &lt;head rend="h2"&gt;How to use it #&lt;/head&gt;
    &lt;p&gt;Since I don‚Äôt have an Apple Developer account, I can‚Äôt notarize the app, so installing it from the releases is going to require a few extra clicks in &lt;code&gt;Privacy and Security&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;And for Macs that disallow it entirely, building from source with Xcode is the only way. I added instructions in the README.&lt;/p&gt;
    &lt;p&gt;Hope this is useful to someone else!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46410402</guid><pubDate>Sun, 28 Dec 2025 11:51:42 +0000</pubDate></item><item><title>Langfuse (YC W23) Is Hiring in Berlin, Germany</title><link>https://langfuse.com/careers</link><description>&lt;doc fingerprint="a8cdb519cd720a6d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Building Langfuse&lt;/head&gt;
    &lt;p&gt;Join the team building the leading open-source LLM engineering platform&lt;/p&gt;
    &lt;p&gt;View Open Positions ‚Üí&lt;/p&gt;
    &lt;p&gt;While LLMs improve a lot, we don‚Äôt see enough applications in production. Building these applications requires a new workflow of continuous monitoring and evaluation that we enable with Langfuse (learn more about our mission).&lt;/p&gt;
    &lt;p&gt;We are seeing strong traction (see metrics below), thus it is the right time to grow the team to build out our backend systems, product, and how we communicate with developers.&lt;/p&gt;
    &lt;p&gt;We are backed by Lightspeed, General Catalyst, Y Combinator, and angels. We are growing fast (see metrics below) and work with some of the best AI teams such as Samsara, Twilio, Khan Academy, and Rocket Money.&lt;/p&gt;
    &lt;p&gt;If complex technical problems &amp;amp; great developer experiences excite you, we‚Äôd love to hear from you.&lt;/p&gt;
    &lt;p&gt;‚Äì Marc, Clemens, Max and the Langfuse team&lt;/p&gt;
    &lt;head rend="h2"&gt;Team&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Marc Klingen, @marcklingen, GitHub, Linkedin&lt;/item&gt;
      &lt;item&gt;Max Deichmann, @maxdeichmann, GitHub, Linkedin&lt;/item&gt;
      &lt;item&gt;Clemens Rawert, @rawert, GitHub, Linkedin&lt;/item&gt;
      &lt;item&gt;Marlies Mayerhofer, @marliessophie, GitHub, LinkedIn&lt;/item&gt;
      &lt;item&gt;Hassieb Pakzad, @hassiebpakzad, GitHub, LinkedIn&lt;/item&gt;
      &lt;item&gt;Steffen Schmitz, GitHub, LinkedIn&lt;/item&gt;
      &lt;item&gt;Jannik Maierh√∂fer, @jmaierhoefer, GitHub, LinkedIn&lt;/item&gt;
      &lt;item&gt;Felix Krauth, @felixkrrr, GitHub, LinkedIn&lt;/item&gt;
      &lt;item&gt;Akio Nuernberger, @AkioNuernberger, LinkedIn&lt;/item&gt;
      &lt;item&gt;Nimar Blume, @nimarblu, GitHub, LinkedIn&lt;/item&gt;
      &lt;item&gt;Michael Froehlich, GitHub, LinkedIn&lt;/item&gt;
      &lt;item&gt;Valeriy Meleshkin, GitHub, LinkedIn&lt;/item&gt;
      &lt;item&gt;Lotte Verheyden, GitHub, LinkedIn&lt;/item&gt;
      &lt;item&gt;Leonard Wolters, GitHub, LinkedIn&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Curious to build with us?&lt;/head&gt;
    &lt;p&gt;If you are excited about delivering exceptional open-source developer experiences alongside an insanely motivated team that ships, reach out!&lt;/p&gt;
    &lt;head rend="h2"&gt;Read our Handbook&lt;/head&gt;
    &lt;p&gt;We publicly document our core principles and processes at Langfuse to align as a team, maintain transparency with our community, and help you determine if you‚Äôd enjoy working here.&lt;/p&gt;
    &lt;head rend="h3"&gt;Main Chapters&lt;/head&gt;
    &lt;head rend="h3"&gt;More Links&lt;/head&gt;
    &lt;p&gt;The handbook contains many more resources that define how we do things. These might be interesting to you:&lt;/p&gt;
    &lt;head rend="h2"&gt;Open Source&lt;/head&gt;
    &lt;p&gt;Almost everything we do is public. Get a glimpse of our work here:&lt;/p&gt;
    &lt;head rend="h2"&gt;Videos/Podcasts&lt;/head&gt;
    &lt;p&gt;If you prefer watching videos or listening to podcasts to get an impression, here are some suggestions:&lt;/p&gt;
    &lt;head rend="h2"&gt;Public Metrics&lt;/head&gt;
    &lt;p&gt;Langfuse is the most widely adopted LLM Engineering platform with 19,719 GitHub stars, 23.1M+ SDK installs per month, and 6M+ Docker pulls. Trusted by 19 of the Fortune 50 and 63 of the Fortune 500 companies.&lt;/p&gt;
    &lt;head rend="h2"&gt;Work with us&lt;/head&gt;
    &lt;head rend="h3"&gt;Curious to build with us?&lt;/head&gt;
    &lt;p&gt;If you are excited about delivering exceptional open-source developer experiences alongside an insanely motivated team that ships, reach out!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46410449</guid><pubDate>Sun, 28 Dec 2025 12:00:06 +0000</pubDate></item><item><title>Designing Predictable LLM-Verifier Systems for Formal Method Guarantee</title><link>https://arxiv.org/abs/2512.02080</link><description>&lt;doc fingerprint="e89c2c0b05c79c88"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Artificial Intelligence&lt;/head&gt;&lt;p&gt; [Submitted on 30 Nov 2025 (v1), last revised 16 Dec 2025 (this version, v2)]&lt;/p&gt;&lt;head rend="h1"&gt;Title:The 4/$Œ¥$ Bound: Designing Predictable LLM-Verifier Systems for Formal Method Guarantee&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:The integration of Formal Verification tools with Large Language Models (LLMs) offers a path to scale software verification beyond manual workflows. However, current methods remain unreliable: without a solid theoretical footing, the refinement process acts as a black box that may oscillate, loop, or diverge. This work bridges this critical gap by developing an LLM-Verifier Convergence Theorem, providing the first formal framework with provable guarantees for termination in multi-stage verification pipelines. We model the interaction not as a generic loop, but as a sequential absorbing Markov Chain comprising four essential engineering stages: \texttt{CodeGen}, \texttt{Compilation}, \texttt{InvariantSynth}, and \texttt{SMTSolving}. We prove that for any non-zero stage success probability ($\delta &amp;gt; 0$), the system reaches the \texttt{Verified} state almost surely. Furthermore, because of the sequential nature of the pipeline, we derive a precise latency bound of $\mathbb{E}[n] \leq 4/\delta$. We stress-tested this prediction in an extensive empirical campaign comprising over 90,000 trials. The results match the theory with striking consistency: every run reached verification, and the empirical convergence factor clustered tightly around $C_f\approx 1.0$, confirming that the $4/\delta$ bound accurately mirrors system behavior rather than serving as a loose buffer. Based on this data, we identify three distinct operating zones -- marginal, practical, and high-performance -- and propose a dynamic calibration strategy to handle parameter drift in real-world environments. Together, these contributions replace heuristic guesswork with a rigorous architectural foundation, enabling predictable resource planning and performance budgeting for safety-critical software.&lt;/quote&gt;&lt;head rend="h2"&gt;Submission history&lt;/head&gt;From: Pierre Dantas [view email]&lt;p&gt;[v1] Sun, 30 Nov 2025 22:19:09 UTC (459 KB)&lt;/p&gt;&lt;p&gt;[v2] Tue, 16 Dec 2025 22:28:30 UTC (491 KB)&lt;/p&gt;&lt;p&gt; Current browse context: &lt;/p&gt;&lt;p&gt;cs.AI&lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46411539</guid><pubDate>Sun, 28 Dec 2025 15:02:07 +0000</pubDate></item><item><title>tc-ematch(8) extended matches for use with "basic", "cgroup" or "flow" filters</title><link>https://man7.org/linux/man-pages/man8/tc-ematch.8.html</link><description>&lt;doc fingerprint="a601e349e52939e4"&gt;
  &lt;main&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;p&gt;NAME | SYNOPSIS | MATCHES | CAVEATS | EXAMPLE &amp;amp; USAGE | AUTHOR | COLOPHON&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;code&gt;
ematch(8)                         Linux                         ematch(8)
&lt;/code&gt;
    &lt;quote&gt;ematch - extended matches for use with "basic", "cgroup" or "flow" filters&lt;/quote&gt;
    &lt;quote&gt;tc filter add .. basic match EXPR .. flowid .. EXPR := TERM [ { and | or } EXPR ] TERM := [ not ] { MATCH | '(' EXPR ')' } MATCH := module '(' ARGS ')' ARGS := ARG1 ARG2 ..&lt;/quote&gt;
    &lt;quote&gt;cmp Simple comparison ematch: arithmetic compare of packet data to a given value. cmp( ALIGN at OFFSET [ ATTRS ] { eq | lt | gt } VALUE ) ALIGN := { u8 | u16 | u32 } ATTRS := [ layer LAYER ] [ mask MASK ] [ trans ] LAYER := { link | network | transport | 0..2 } meta Metadata ematch meta( OBJECT { eq | lt |gt } OBJECT ) OBJECT := { META_ID | VALUE } META_ID := id [ shift SHIFT ] [ mask MASK ] meta attributes: random 32 bit random value loadavg_1 Load average in last 5 minutes nf_mark Netfilter mark vlan Vlan tag sk_rcvbuf Receive buffer size sk_snd_queue Send queue length A full list of meta attributes can be obtained via # tc filter add dev eth1 basic match 'meta(list)' nbyte match packet data byte sequence nbyte( NEEDLE at OFFSET [ layer LAYER ] ) NEEDLE := { string | c-escape-sequence } OFFSET := int LAYER := { link | network | transport | 0..2 } u32 u32 ematch u32( ALIGN VALUE MASK at [ nexthdr+ ] OFFSET ) ALIGN := { u8 | u16 | u32 } ipset test packet against ipset membership ipset( SETNAME FLAGS ) SETNAME := string FLAGS := { FLAG [, FLAGS] } The flag options are the same as those used by the iptables "set" match. When using the ipset ematch with the "ip_set_hash:net,iface" set type, the interface can be queried using "src,dst (source ip address, outgoing interface) or "src,src" (source ip address, incoming interface) syntax. ipt test packet against xtables matches ipt( [-6] -m MATCH_NAME FLAGS ) MATCH_NAME := string FLAGS := { FLAG [, FLAGS] } The flag options are the same as those used by the xtable match used. canid ematch rule to match CAN frames canid( IDLIST ) IDLIST := IDSPEC[IDLIST] IDSPEC := { √¢sff√¢ CANID | √¢eff√¢ CANID } CANID := ID[:MASK] ID, MASK := hexadecimal number (i.e. 0x123)&lt;/quote&gt;
    &lt;quote&gt;The ematch syntax uses '(' and ')' to group expressions. All braces need to be escaped properly to prevent shell commandline from interpreting these directly. When using the ipset ematch with the "ifb" device, the outgoing device will be the ifb device itself, e.g. "ifb0". The original interface (i.e. the device the packet arrived on) is treated as the incoming interface.&lt;/quote&gt;
    &lt;quote&gt;# tc filter add .. basic match ... # 'cmp(u16 at 3 layer 2 mask 0xff00 gt 20)' # 'meta(nfmark gt 24)' and 'meta(tcindex mask 0xf0 eq 0xf0)' # 'nbyte("ababa" at 12 layer 1)' # 'u32(u16 0x1122 0xffff at nexthdr+4)' Check if packet source ip address is member of set named bulk: # 'ipset(bulk src)' Check if packet source ip and the interface the packet arrived on is member of "hash:net,iface" set named interactive: # 'ipset(interactive src,src)' Check if packet matches an IPSec state with reqid 1: # 'ipt(-m policy --dir in --pol ipsec --reqid 1)'&lt;/quote&gt;
    &lt;quote&gt;The extended match infrastructure was added by Thomas Graf.&lt;/quote&gt;
    &lt;code&gt;
       This page is part of the iproute2 (utilities for controlling
       TCP/IP networking and traffic) project.  Information about the
       project can be found at 
       √¢¬®http://www.linuxfoundation.org/collaborate/workgroups/networking/iproute2√¢¬©.
       If you have a bug report for this manual page, send it to
       netdev@vger.kernel.org, shemminger@osdl.org.  This page was
       obtained from the project's upstream Git repository
       √¢¬®https://git.kernel.org/pub/scm/network/iproute2/iproute2.git√¢¬© on
       2025-08-11.  (At that time, the date of the most recent commit
       that was found in the repository was 2025-08-08.)  If you discover
       any rendering problems in this HTML version of the page, or you
       believe there is a better or more up-to-date source for the page,
       or you have corrections or improvements to the information in this
       COLOPHON (which is not part of the original manual page), send a
       mail to man-pages@man7.org

iproute2                      6 August 2012                     ematch(8)
&lt;/code&gt;
    &lt;p&gt;Pages that refer to this page: tc(8), tc-basic(8), tc-bpf(8), tc-cgroup(8), tc-flow(8)&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46412327</guid><pubDate>Sun, 28 Dec 2025 16:43:14 +0000</pubDate></item><item><title>Show HN: Pion SCTP with RACK is 70% faster with 30% less latency</title><link>https://pion.ly/blog/sctp-and-rack/</link><description>&lt;doc fingerprint="21e41095e1799bd5"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;What is SCTP?&lt;/head&gt;
    &lt;p&gt;SCTP stands for Stream Control Transmission Protocol. At a basic level, SCTP is designed to be reliable, handle de-duplication of packets, and support packets that may be delivered in order or out of order. Beyond transporting messages, SCTP can also set up a connection between users. On a deeper level, SCTP includes native support for multiplexing: multiple applications can take advantage of a single transport connection. SCTP also supports multi-homing, which enables automatic failover from a primary connection to a secondary one.&lt;/p&gt;
    &lt;p&gt;At the most basic level, it lets you reliably send information from one computer to another without any complications.&lt;/p&gt;
    &lt;head rend="h3"&gt;What is SCTP used for?&lt;/head&gt;
    &lt;p&gt;SCTP‚Äôs uses can generally fit into two cases:&lt;/p&gt;
    &lt;head rend="h4"&gt;1. Sending some amount of data.&lt;/head&gt;
    &lt;p&gt;Imagine a scenario where two people are texting when one person remembers a picture that they want to send. As they text back and forth, an image gets uploaded, which takes some time to get sent. SCTP can handle multiple things going on at the same time and doesn‚Äôt delay any messages from being sent just because an image is being uploaded! Thanks to SCTP, text messages can be safely delivered to each person and nothing in their conversation is lost in transit or delayed just because something else is being transferred at the same time as their messages.&lt;/p&gt;
    &lt;p&gt;Building on this idea, users can share larger files with each other. This includes anything: birthday videos, audio recordings, even boring paperwork; anything that‚Äôs a file can be sent!&lt;/p&gt;
    &lt;head rend="h4"&gt;2. Sending small amounts of data with a purpose.&lt;/head&gt;
    &lt;p&gt;In a new scenario, imagine two people who are texting back and forth when one person gets hungry. They send a message saying, ‚ÄúI want a pizza!‚Äù When the other person receives the text, they think, ‚ÄúMaybe I should do something about that!‚Äù The recipient can choose to do something useful for the sender with that information.&lt;/p&gt;
    &lt;p&gt;This is the blueprint for many awesome technologies today, as it opens up the possibility of controlling one computer from a different computer. Consider a surgeon who performs an operation involving a remote-controlled device that needs to respond with as little latency as possible. Similarly, real-time navigation systems also need to respond to changes in traffic conditions quickly in order to avoid congested or unsafe areas due to accidents or weather conditions.&lt;/p&gt;
    &lt;head rend="h4"&gt;Other uses:&lt;/head&gt;
    &lt;p&gt;SCTP can be used for online multiplayer games where every frame counts, including first-person shooters and fighting games. Taking the remote surgery example in this direction leads to the idea of cloud gaming, as players can have their inputs sent to a different device than the one that they‚Äôre using while still being able to play the game!&lt;/p&gt;
    &lt;p&gt;SCTP is also used inside web browsers via WebRTC and has found use in AI applications and cryptocurrency-related technologies. Additionally, payment verification can similarly benefit from secure and fast communication.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why SCTP for WebRTC?&lt;/head&gt;
    &lt;p&gt;SCTP is used for WebRTC because of its ability to send information via reliable and unreliable datachannels. For example, you can send messages or files in a chat with SCTP. Other uses include being able to know when users toggle their microphone or video in a video call. In some special cases, SCTP can even be used to transmit video between users, but that‚Äôs significantly less common.&lt;/p&gt;
    &lt;p&gt;In WebRTC, the ICE protocol connects users and the DTLS protocol establishes a secure connection, at which point SCTP is then used to securely transfer data. In an ideal setup, data that‚Äôs sent should ‚Äújust work‚Äù. Unfortunately, that isn‚Äôt how things tend to pan out, as issues eventually crop up. Packets get dropped, the network jitters, the computer stutters, or the coffee machine doesn‚Äôt start when you thought it had. That‚Äôs why it‚Äôs important to have a backup plan for when things go wrong.&lt;/p&gt;
    &lt;head rend="h2"&gt;How SCTP Deals With Loss&lt;/head&gt;
    &lt;p&gt;SCTP was designed with this in mind and has two built-in recovery strategies for when networking goes wrong.&lt;/p&gt;
    &lt;p&gt;The first is called ‚Äúfast retransmission.‚Äù The receiver detects if a chunk of data is missing in the transmission. If so, the receiver notifies the sender that a specific chunk ID is missing. If the sender receives three reports of a missing chunk where all three reports are referring to the same chunk ID, then the sender will assume that the chunk has been lost and resend it.&lt;/p&gt;
    &lt;p&gt;The second is a timer-based retransmission. This happens if the receiver doesn‚Äôt acknowledge that it has received all the packets within a specific window of time. If the receiver doesn‚Äôt acknowledge that all the packets have been received, then the sender is prompted to retransmit the unacknowledged data.&lt;/p&gt;
    &lt;p&gt;Both of these loss recovery strategies are used by SCTP to try to ensure that any lost data is detected and retransmitted as quickly as possible. At the time of writing, Pion‚Äôs implementation of Pion‚Äôs implementation of SCTP uses these two mechanisms for loss recovery.&lt;/p&gt;
    &lt;p&gt;These strategies are also used by TCP, which has prompted engineers to see if there‚Äôs an even better strategy to detect and mitigate lost data.&lt;/p&gt;
    &lt;head rend="h2"&gt;Introducing RACK&lt;/head&gt;
    &lt;p&gt;In February 2021, RFC 8985: The RACK-TLP Loss Detection Algorithm for TCP was published. This was a completely new loss detection algorithm that focused on actively keeping track of network statistics and using timer-based signals in order to remain adaptive to ever-changing network conditions. RACK‚Äôs improvements over SACK and fast retransmission in TCP were enticing enough for Linux, Windows, and FreeBSD to all implement it in TCP.&lt;/p&gt;
    &lt;p&gt;While RACK was originally intended to be implemented for TCP, it is noted in the RFC that it can be implemented in other transport protocols, including SCTP.&lt;/p&gt;
    &lt;p&gt;The implementation for SCTP was formally analyzed in Felix Weinrank‚Äôs Dissertation and other publications. Weinrank‚Äôs deep dive provides an extremely comprehensive review of SCTP and improvements regarding usage in various scenarios, including WebRTC. At the moment, we‚Äôre more concerned with Weinrank‚Äôs analysis and implementation notes regarding RACK in SCTP. In Chapter 7 of the dissertation, Weinrank goes over how SCTP handles loss and how RACK can be implemented for SCTP, including extra details regarding how the implementation interacts with various SCTP extensions.&lt;/p&gt;
    &lt;head rend="h2"&gt;RACK‚Äôs motivations:&lt;/head&gt;
    &lt;p&gt;The authors of RACK in RFC 8985 provides examples of situations where RACK improves SCTP and TCP during loss recovery.&lt;/p&gt;
    &lt;head rend="h3"&gt;How RACK‚Äôs Tail Loss Probing (TLP) Works&lt;/head&gt;
    &lt;quote&gt;sequenceDiagram participant S as Sender participant R as Receiver Note over S,R: The two sends are normally&amp;lt;br&amp;gt;combined but are separated&amp;lt;br&amp;gt;here for visual clarity. S-&amp;gt;&amp;gt;R: Send üçé S--&amp;gt;&amp;gt;R: Send üçå, ü•ï, ü•î Note right of R: Only üçé arrived, so it's ACK'd. R-&amp;gt;&amp;gt;S: ACK üçé Note over S,R: 2 RTTs later, TLP fires S-&amp;gt;&amp;gt;R: TLP triggers a retransmit ü•î Note right of R: ü•î arrived, so it's SACK'd. R-&amp;gt;&amp;gt;S: SACK ü•î Note left of S: Mark üçå and ü•ï as lost Note over S,R: The two sends are normally&amp;lt;br&amp;gt;combined but are separated&amp;lt;br&amp;gt;here for visual clarity. S--&amp;gt;&amp;gt;R: Retransmit üçå S-&amp;gt;&amp;gt;R: Retransmit ü•ï Note right of R: Only ü•ï arrived, so it's SACK'd&amp;lt;br&amp;gt;alongside ü•î. R--&amp;gt;&amp;gt;S: SACK ü•ï and ü•î Note left of S: üçå retransmission marked as lost.&amp;lt;br&amp;gt;Note that this is the second time&amp;lt;br/&amp;gt;that üçå has been marked as lost. S-&amp;gt;&amp;gt;R: Retransmit üçå again Note right of R: üçå finally arrived! R--&amp;gt;&amp;gt;S: ACK ü•î Note over S,R: The ACK ü•î is a cumulative ACK&amp;lt;br&amp;gt;for the üçéüçåü•ïü•î sequence&lt;/quote&gt;
    &lt;p&gt;In the above scenario, Tail Loss Probing enables the sender to quickly know if üçéüçåü•ïü•î were successfully received. Since the sender only receives an acknowledgment (ACK) for üçé from the receiver, the TLP timer eventually triggers because it didn‚Äôt receive an ACK for üçå, ü•ï, and ü•î. The TLP can then resend the last packet in the segment as an efficient way to:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Retransmit data that the receiver would have to receive down the line anyway. The alternative would be to send an empty packet, receive an ACK, then send a missing packet. This handles both at once and can potentially save an RTT if only the last packet is missing in a segment.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Allow the receiver to ACK any earlier missing packets in the sequence if there were other issues due to networking, temporary stutters or freezes, etc.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Check receiver responsiveness and detect if there‚Äôs a network issue. Note that this is different from (2), as the receiver could potentially never respond with an ACK.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The receiver then confirms that it has ü•î by sending a selective acknowledgment (SACK) to the sender, which tells the sender that it‚Äôs received one of the packets from the segment but not all of them. At this point, the sender has an ACK for üçé and a SACK for ü•î, which means that it can determine that üçå and ü•ï must be missing. The sender then notes that üçå and ü•ï have been lost once. It then retransmits üçå and ü•ï. In the example, üçå happens to get dropped by the network whereas ü•ï is sent and received successfully. The receiver then replies with a SACK for ü•ï and ü•î, at which point the sender can determine that üçå was lost a second time. Finally, the sender retransmits üçå, which fortunately doesn‚Äôt get dropped, and the receiver sends an ACK for ü•î (note the ACK is for the last packet in the segment, which implies that all previous packets have been received).&lt;/p&gt;
    &lt;p&gt;Side note: keeping track of the number of times that a packet has been lost is important as the cubic congestion control algorithm described in RFC 9438 (which is mentioned in RFC 8985) relies on this information.&lt;/p&gt;
    &lt;head rend="h3"&gt;SCTP RTO vs RACK RTO&lt;/head&gt;
    &lt;p&gt;In this example, the authors of RFC 8985 show how, without RACK, SCTP and TCP can suffer from spurious retransmissions when there are retransmission timeouts (RTOs). In this case, each food icon represents an entire segment instead of a packet.&lt;/p&gt;
    &lt;quote&gt;sequenceDiagram participant S as Sender participant R as Receiver S-&amp;gt;&amp;gt;R: Send ü•ê Note over S,R: Then right before&amp;lt;br&amp;gt;the end of the RTO... Note right of R: The receiver has a network hiccup! S-&amp;gt;&amp;gt;R: Send üç≥ü•≠ Note right of R: Received ü•ê Note over S,R: End of the RTO R-&amp;gt;&amp;gt;S: ACK ü•ê&lt;/quote&gt;
    &lt;p&gt;In this scenario, ü•ê is sent, and right before the end of the RTO, üç≥ and ü•≠ are sent. The receiver gets ü•ê, üç≥, and ü•≠, but only manages to send an ACK for ü•ê right after the RTO. Let‚Äôs see how SCTP handles this without RACK versus with RACK.&lt;/p&gt;
    &lt;quote&gt;sequenceDiagram participant S as Sender participant R as Receiver S-&amp;gt;&amp;gt;R: Send ü•ê Note over S,R: Then right before&amp;lt;br&amp;gt;the end of the RTO... Note right of R: The receiver has a network hiccup! S-&amp;gt;&amp;gt;R: Send üç≥ü•≠ Note right of R: Received ü•ê Note over S,R: End of the RTO R-&amp;gt;&amp;gt;S: ACK ü•ê Note over S,R: Without RACK... %% without rack Note left of S: Mark ü•êüç≥ü•≠ as lost&amp;lt;br&amp;gt;since RTO expired Note right of R: Received üç≥ü•≠ Note over S,R: The Sender incorrectly&amp;lt;br&amp;gt;ignores the ack for üç≥ü•≠! Note left of S: Prepare to retransmit ü•êüç≥ü•≠ S-&amp;gt;&amp;gt;R: Retransmit ü•êüç≥ü•≠&lt;/quote&gt;
    &lt;p&gt;We can see here that the receiver eventually sends an ACK for üç≥ and ü•≠, but the sender ignores it and believes that ü•ê, üç≥, and ü•≠ are all missing instead of just ü•ê. While it‚Äôs reasonable to assume that ü•ê is missing, it‚Äôs a little overzealous in retransmitting packets, which can increase network traffic during recovery, especially when it‚Äôs completely possible for the sender to wait for the acknowledgments of üç≥ and ü•≠ from the receiver.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs see what RACK does!&lt;/p&gt;
    &lt;quote&gt;sequenceDiagram participant S as Sender participant R as Receiver S-&amp;gt;&amp;gt;R: Send ü•ê Note over S,R: Then right before&amp;lt;br&amp;gt;the end of the RTO... Note right of R: The receiver has a network hiccup! S-&amp;gt;&amp;gt;R: Send üç≥ü•≠ Note right of R: Received ü•ê Note over S,R: End of the RTO R-&amp;gt;&amp;gt;S: ACK ü•ê Note over S,R: With RACK... %% with rack Note left of S: Mark ü•ê as lost&amp;lt;br&amp;gt;since RTO expired Note right of R: Received üç≥ü•≠ Note left of S: Prepare to retransmit ü•ê S-&amp;gt;&amp;gt;R: Retransmit ü•ê&lt;/quote&gt;
    &lt;p&gt;Here, RACK makes it so only ü•ê is marked as lost when the RTO expires. üç≥ and ü•≠ aren‚Äôt marked as lost because their own RTOs have not yet expired by the time their ACKs are received. Therefore, only ü•ê is retransmitted, as the timers for üç≥ and ü•≠ would be re-armed if an ACK is received for the retransmitted ü•ê.&lt;/p&gt;
    &lt;p&gt;In this example, even though both non-RACK and RACK end up retransmitting ü•ê despite the receiver already having it, the focus is on minimizing spurious retransmissions. This can save on the amount of data sent over the network, which naturally speeds up any retransmissions that might occur.&lt;/p&gt;
    &lt;head rend="h3"&gt;RACK‚Äôs strategy&lt;/head&gt;
    &lt;p&gt;In summary, RACK‚Äôs strategy generally has two main parts:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Detect packet losses as quickly as possible by utilizing time-based acknowledgments of segmented data and inferences from network statistics.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Use Tail Loss Probing (TLP), which sends sample data to gather more network statistics.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The combination of these two strategies allows it to quickly determine issues and properly rectify them once identified. It also provides better resilience for some tricky edge cases! If you‚Äôre interested in seeing how RACK could perform in SCTP and other SCTP-specific improvements, check out chapter 7 of Felix Weinrank‚Äôs thesis.&lt;/p&gt;
    &lt;head rend="h2"&gt;A quick look at the results (why this matters if you don‚Äôt live in SCTP land)&lt;/head&gt;
    &lt;p&gt;SCP is a Go test harness that runs two Pion SCTP stacks against each other inside a deterministic, in-process ‚Äúvirtual network‚Äù (from Pion/transport). It pins exact commits on each side, replays scenarios with a fixed seed, validates packet on the wire (CRC32c + basic SCTP parsing), and writes artifacts (&lt;code&gt;results.json&lt;/code&gt;, packet logs, and pprof) so you can reproduce the numbers.&lt;/p&gt;
    &lt;head rend="h3"&gt;The headline (max-burst): more throughput, less CPU, lower latency&lt;/head&gt;
    &lt;p&gt;This is the cleanest microbench in the suite: no loss, no delay, no jitter. Just a burst of messages in both directions. Comparing non-rack&amp;lt;-&amp;gt;non-rack vs rack&amp;lt;-&amp;gt;rack (There are similar improvements even when comparing rack&amp;lt;-&amp;gt;non-rack):&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;metric&lt;/cell&gt;
        &lt;cell role="head"&gt;main (baseline)&lt;/cell&gt;
        &lt;cell role="head"&gt;RACK&lt;/cell&gt;
        &lt;cell role="head"&gt;delta&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;goodput&lt;/cell&gt;
        &lt;cell&gt;234.55 Mbps&lt;/cell&gt;
        &lt;cell&gt;316.42 Mbps&lt;/cell&gt;
        &lt;cell&gt;+34.9%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;CPU time (&lt;code&gt;cpu_seconds&lt;/code&gt;)&lt;/cell&gt;
        &lt;cell&gt;0.0560 s&lt;/cell&gt;
        &lt;cell&gt;0.0441 s&lt;/cell&gt;
        &lt;cell&gt;‚àí21.3%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;goodput / CPU-second&lt;/cell&gt;
        &lt;cell&gt;4,189&lt;/cell&gt;
        &lt;cell&gt;7,177&lt;/cell&gt;
        &lt;cell&gt;+71.3%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;latency p50&lt;/cell&gt;
        &lt;cell&gt;16.37 ms&lt;/cell&gt;
        &lt;cell&gt;11.86 ms&lt;/cell&gt;
        &lt;cell&gt;‚àí27.5%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;latency p99&lt;/cell&gt;
        &lt;cell&gt;36.95 ms&lt;/cell&gt;
        &lt;cell&gt;27.84 ms&lt;/cell&gt;
        &lt;cell&gt;‚àí24.6%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;That +71% throughput-per-CPU is simply the goodput measured (Mbps) divided by the run‚Äôs &lt;code&gt;cpu_seconds&lt;/code&gt;. Non-rack cruised at ~234 Mbps using ~0.056 CPU seconds (~4,189 Mbps/CPU-s), while RACK sustained 316 Mbps with ~0.044 CPU seconds (~7,177 Mbps/CPU-s). That gap is the proof that rack delivers ~71% more work per unit of CPU.&lt;/p&gt;
    &lt;head rend="h3"&gt;Test setup&lt;/head&gt;
    &lt;p&gt;To test RACK, we ran these test profiles to compare how RACK performs against main (baseline):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;max-burst - ‚Äúhow fast can we go‚Äù with no delay, loss, or reordering; it targets the raw transport path:&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Goodput jumps +34.9% (234 -&amp;gt;316 Mbps) while CPU seconds drop by 21% (0.056 -&amp;gt;0.044 s) and p50/p99 both fall by ~25%. RACK now delivers ~71% more Mbps per CPU-second.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;handshake - same burst pattern, this time including the COOKIE/SHUTDOWN handshake, so we exercise setup timers:&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Goodput climbs +15% (237 -&amp;gt;272 Mbps) while latency stays basically flat (15.65 -&amp;gt;15.99 ms for p50, 35.27 -&amp;gt;33.25 ms for p99), which confirms that the faster throughput comes without slower ACK paths.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;unordered-late-low-rtt - minor delay/jitter (10 ms) but unordered delivery to simulate packet trains with mild disorder:&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Minor latency and throughput noise. Both branches still pass but RACK keeps the delivery steady despite small unordered bursts.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;unordered-late-high-rtt - large RTT/jitter (180 ms / 60 ms) with unordered delivery, so we can watch how the stack copes with latency spikes:&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Very high latency due to the profile, but RACK keeps throughput comparable while completely avoiding any regressions.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;unordered-late-dynamic-rtt - fluctuating RTT (40 ms base ¬±180 ms jitter) with unordered delivery to mimic burst-y network dynamics:&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Both branches pass with no noticeable regressions from RACK, which shows that each branch handles jitter swings fine.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;congestion - ordered delivery with 2% loss and modest delay/jitter to stress-tests congestion control and SACK-driven recovery:&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The loss-handling path stays green and RACK doesn‚Äôt use extra CPU compared to master which shows the +35% clean-case gain doesn‚Äôt cost the loss profile.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;retransmission - ordered with 5% loss and 20 ms jitter to force fast-retransmit/TLP scenario:&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The fault case still hits retries and RACK‚Äôs CPU profile actually shows more JSON/packet-logging work but that‚Äôs what we expect during retransmit storms.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;reorder-low - unordered with 1.5% loss plus deliberate reordering to exercise scheduler/queue behavior under lossÔºãreorder:&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Goodput improves +44% (1.79 Mbps -&amp;gt; 2.58 Mbps) and the run finishes faster (~3.70 s vs 5.34 s), so RACK dominates the low-rate reordering scenario.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;burst-loss - unordered with 4% loss and 50 ms jitter to push retransmit/recovery under heavy loss bursts.&lt;/item&gt;
      &lt;item&gt;fragmentation - oversized payloads require chunk fragmentation/reassembly to verify large-message handling:&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Nothing improved or got worse.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;media-hevc - A real-world use case with video: one-way stream, paced HEVC frames (~25 fps), 3% loss, ~1200-byte max payload, across a ~13-14 Mbps link (taken from a real-world use case of sending DRM media over WebRTC datachannels) to ensure sustained media delivery works.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;RACK hits 12.90 Mbps goodput in 2.14 s (100% delivery) while the main branch streaming to the RACK branch sits at 11.34 Mbps in 4.66 s. That‚Äôs a 2x faster finish!&lt;/p&gt;
    &lt;p&gt;We also have 3 negative tests to ensure that any corruptions or errors are still being caught:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;fault-checksum - corrupts every 7th DATA chunk‚Äôs checksum so receivers must drop it and log the error.&lt;/item&gt;
      &lt;item&gt;fault-bad-chunk-len - mangles the chunk length field every 7th chunk to validate length checks/parsing.&lt;/item&gt;
      &lt;item&gt;fault-nonzero-padding - corrupts padding bytes every 7th chunk so padding validation and chunk isolation logic are exercised.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In both branches, these cases fail (as desired), which confirms that both branches detect the corruption and that there is no regression in behavior.&lt;/p&gt;
    &lt;head rend="h3"&gt;CPU flamegraphs&lt;/head&gt;
    &lt;p&gt;The flamegraphs below show the CPU profiles for max-burst runs. You can see in the metadata that the RACK profile captured 20ms of samples (9.95% of 201.08ms duration) versus the master profile‚Äôs 40ms of samples (19.86% of 201.45ms duration) ‚Äì exactly half the sample count for a similar duration, which directly supports the efficiency claims.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why RACK behaves better (not just ‚Äúgoes faster‚Äù)&lt;/head&gt;
    &lt;p&gt;RACK changes how SCTP decides that something is lost and when it sends probes, so it wastes less work fixing problems that never really happened:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Instead of keying almost everything off ‚Äúthree missing reports or an RTO fired‚Äù, RACK uses time-based loss detection: it looks at when chunks were last SACKed/ACKed and infers loss from elapsed time and the pattern of tail acknowledgments.&lt;/item&gt;
      &lt;item&gt;Tail Loss Probes (TLP) send a cheap ‚Äúsample‚Äù chunk at the end of a burst to flush out late ACKs. If the receiver really did get the data, it answers and the sender avoids a full retransmission storm, otherwise, the probe doubles as the retransmission you needed anyway.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In practical terms that‚Äôs what the profiles and metrics are showing:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;We still see the same hot stack (&lt;code&gt;vnet.(*chunkUDP).UserData&lt;/code&gt;,&lt;code&gt;runtime.memmove&lt;/code&gt;, a thin layer of runtime/type helpers) in both master and RACK. It‚Äôs the normal packet I/O path.&lt;/item&gt;
      &lt;item&gt;With RACK, that stack is exercised fewer times per unit of useful data because there are fewer spurious retransmits and fewer ‚Äújust in case‚Äù timer expirations.&lt;/item&gt;
      &lt;item&gt;That‚Äôs exactly how we get more goodput, lower latency, and smaller CPU profiles at the same time: RACK spends less CPU ‚Äúarguing with the network‚Äù and more CPU pushing real user data through SCTP.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Spec-aligned ACK behavior and testing&lt;/head&gt;
    &lt;p&gt;Using SCP testing tool we were able to find some issues includes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;In the initial RACK implementation, handling of transitions from high to low RTT was suboptimal due to the implementation using a global minimum for recent RTT measurements instead of a windowed minimum (the latter approach is only a ‚ÄúSHOULD‚Äù in RFC 8985 section 6.2.1). Atsushi Watanabe quickly identified it and we resolved the issue.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The earlier version of RACK implementation also handled packet reordering poorly and consumed more CPU than non-RACK. This was corrected by implementing improved active RTT measurement, following the approach described in Weinrank‚Äôs work, see p. 120.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;A minor bug was discovered (and fixed) in the initial RACK implementation where the latest RTT was not measured for every packet.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;We also found that Pion SCTP did not send a SACK immediately after a TSN gap, causing RACK to perform worse under moderate reordering. After fixing this behavior to align with RFC 4960 section 6.7 (surprisingly only a ‚ÄúSHOULD‚Äù), reordering test cases showed a ~30% improvement.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Looking forward&lt;/head&gt;
    &lt;p&gt;Keep an eye out for even more improvements and benchmarks from our improved SCTP implementation using real-world data, as well as how we‚Äôre doing it in an upcoming blog post!&lt;/p&gt;
    &lt;p&gt;Thanks for reading!&lt;/p&gt;
    &lt;head rend="h2"&gt;Credits&lt;/head&gt;
    &lt;p&gt;Huge thanks to the following for making this possible:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Joe Turki for introducing me to Pion, making SCP, answering countless questions, and so much more.&lt;/item&gt;
      &lt;item&gt;Sean DuBois for making Pion, finding Felix Weinrank‚Äôs thesis, and endless encouragement.&lt;/item&gt;
      &lt;item&gt;Srayan Jana for helping to bounce around many ideas.&lt;/item&gt;
      &lt;item&gt;Atsushi Watanabe for reviewing and catching the global minimum vs windowed minimum issue in the RACK PR.&lt;/item&gt;
      &lt;item&gt;And many more people along the way!&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46413053</guid><pubDate>Sun, 28 Dec 2025 18:05:13 +0000</pubDate></item><item><title>Remembering Lou Gerstner</title><link>https://newsroom.ibm.com/2025-12-28-Remembering-Lou-Gerstner</link><description>&lt;doc fingerprint="3f8dd935902cb38d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;All press releases&lt;/head&gt;
    &lt;head rend="h1"&gt;Remembering Lou Gerstner&lt;/head&gt;
    &lt;p&gt;The following is the text of an email sent today to all IBM employees by Chairman and CEO Arvind Krishna:&lt;/p&gt;
    &lt;p&gt;IBMers,&lt;/p&gt;
    &lt;p&gt;I am saddened to share that Lou Gerstner, IBM‚Äôs Chairman and CEO from 1993 to 2002, passed away yesterday.&lt;/p&gt;
    &lt;p&gt;Lou arrived at IBM at a moment when the company‚Äôs future was genuinely uncertain. The industry was changing rapidly, our business was under pressure, and there was serious debate about whether IBM should even remain whole. His leadership during that period reshaped the company. Not by looking backward, but by focusing relentlessly on what our clients would need next.&lt;/p&gt;
    &lt;p&gt;One of Lou‚Äôs earliest signals as CEO has become part of IBM lore. Early on, he stopped a long internal presentation and said, simply, ‚ÄúLet‚Äôs just talk.‚Äù The message was clear: less inward focus, more real discussion, and much closer attention to customers. That mindset would define his tenure.&lt;/p&gt;
    &lt;p&gt;Lou believed one of IBM‚Äôs central problems was that we had become optimized around our own processes, debates, and structures rather than around client outcomes. As he later put it, the company had lost sight of a basic truth of business: understanding the customer and delivering what the customer actually values.&lt;/p&gt;
    &lt;p&gt;That insight drove real change. Meetings became more direct. Decisions were grounded more in facts and client impact than in hierarchy or tradition. Innovation mattered if it could translate into something clients would come to rely on. Execution in the quarter and the year mattered, but always in service of longer-term relevance.&lt;/p&gt;
    &lt;p&gt;Lou made what may have been the most consequential decision in IBM‚Äôs modern history: to keep IBM together. At the time, the company was organized into many separate businesses, each pursuing its own path. Lou understood that clients didn‚Äôt want fragmented technology‚Äîthey wanted integrated solutions. That conviction shaped IBM‚Äôs evolution and reestablished our relevance for many of the world‚Äôs largest enterprises.&lt;/p&gt;
    &lt;p&gt;Lou also understood that strategy alone would not be enough. He believed lasting change required a shift in culture‚Äîin how people behave when no one is watching. What mattered was what IBMers valued, how honestly they confronted reality, and how willing they were to challenge themselves and each other. Rather than discard IBM‚Äôs long-standing values, he pushed the company to renew them to meet the demands of a very different era.&lt;/p&gt;
    &lt;p&gt;I have my own memory of Lou from the mid-1990s, at a small town hall with a few hundred people. What stood out was his intensity and focus. He had an ability to hold the short term and the long term in his head at the same time. He pushed hard on delivery, but he was equally focused on innovation: doing work that clients would remember, not just consume.&lt;/p&gt;
    &lt;p&gt;Lou stayed engaged with IBM long after his tenure ended. From my first days as CEO, he was generous with advice‚Äîbut always careful in how he gave it. He would offer perspective, then say, ‚ÄúI‚Äôve been gone a long time‚ÄîI‚Äôm here if you need me.‚Äù He listened closely to what others were saying about IBM and reflected it back candidly.&lt;/p&gt;
    &lt;p&gt;That neutral, experienced voice mattered to me, and I was fortunate to learn from Lou on a regular basis.&lt;/p&gt;
    &lt;p&gt;Lou was direct. He expected preparation. He challenged assumptions. But he was deeply committed to building a company that could adapt‚Äîculturally as much as strategically‚Äîwithout losing its core values.&lt;/p&gt;
    &lt;p&gt;Lou‚Äôs impact extended well beyond IBM. Before joining the company, he had already built an extraordinary career‚Äîbecoming one of the youngest partners at McKinsey &amp;amp; Company, later serving as president of American Express and CEO of RJR Nabisco. After IBM, he went on to chair The Carlyle Group and devoted significant time and resources to philanthropy, particularly in education and biomedical research. A native of Long Island, NY, Lou earned his undergraduate degree from Dartmouth and an MBA from Harvard, and he remained deeply devoted to his family throughout his life. Lou was preceded in death by his son Louis Gerstner III.&lt;/p&gt;
    &lt;p&gt;We will hold a celebration in the new year to reflect on Lou‚Äôs legacy and what his leadership enabled at IBM.&lt;/p&gt;
    &lt;p&gt;My thoughts are with Lou‚Äôs wife Robin, his daughter Elizabeth, his grandchildren and extended family, as well as his many friends, colleagues, and people around the world who were shaped by his leadership and his work.&lt;lb/&gt; Media contact:&lt;lb/&gt; IBM Press Room&lt;lb/&gt; ibmpress@us.ibm.com &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46413365</guid><pubDate>Sun, 28 Dec 2025 18:43:54 +0000</pubDate></item><item><title>PySDR: A Guide to SDR and DSP Using Python</title><link>https://pysdr.org/content/intro.html</link><description>&lt;doc fingerprint="7c6211f0c4f3e3a0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;1. Introduction¬∂&lt;/head&gt;
    &lt;head rend="h2"&gt;Purpose and Target Audience¬∂&lt;/head&gt;
    &lt;p&gt;First and foremost, a couple important terms:&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Software-Defined Radio (SDR):&lt;/item&gt;
      &lt;item rend="dd-1"&gt;
        &lt;p&gt;As a concept it refers to using software to perform signal processing tasks that were traditionally performed by hardware, specific to radio/RF applications. This software can be run on a general-purpose computer (CPU), FPGA, or even GPU, and it can be used for real-time applications or offline processing of recorded signals. Analogous terms include ‚Äúsoftware radio‚Äù and ‚ÄúRF digital signal processing‚Äù.&lt;/p&gt;
        &lt;p&gt;As a thing (e.g., ‚Äúan SDR‚Äù) it typically refers to a device that you can plug an antenna into and receive RF signals, with the digitized RF samples being sent to a computer for processing or recording (e.g., over USB, Ethernet, PCI). Many SDRs also have transmit capabilities, allowing the computer to send samples to the SDR which then transmits the signal at a specified RF frequency. Some embedded-style SDRs include an onboard computer.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-2"&gt;Digital Signal Processing (DSP):&lt;/item&gt;
      &lt;item rend="dd-2"&gt;The digital processing of signals; in our case, RF signals.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This textbook acts as a hands-on introduction to the areas of DSP, SDR, and wireless communications. It is designed for someone who is:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Interested in using SDRs to do cool stuff&lt;/item&gt;
      &lt;item&gt;Good with Python&lt;/item&gt;
      &lt;item&gt;Relatively new to DSP, wireless communications, and SDR&lt;/item&gt;
      &lt;item&gt;A visual learner, preferring animations over equations&lt;/item&gt;
      &lt;item&gt;Better at understanding equations after learning the concepts&lt;/item&gt;
      &lt;item&gt;Looking for concise explanations, not a 1,000 page textbook&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;An example is a Computer Science student interested in a job involving wireless communications after graduation, although it can be used by anyone itching to learn about SDR who has programming experience. As such, it covers the necessary theory to understand DSP techniques without the intense math that is usually included in DSP courses. Instead of burying ourselves in equations, an abundance of images and animations are used to help convey the concepts, such as the Fourier series complex plane animation below. I believe that equations are best understood after learning the concepts through visuals and practical exercises. The heavy use of animations is why PySDR will never have a hard copy version being sold on Amazon.&lt;/p&gt;
    &lt;p&gt;This textbook is meant to introduce concepts quickly and smoothly, enabling the reader to perform DSP and use SDRs intelligently. It‚Äôs not meant to be a reference textbook for all DSP/SDR topics; there are plenty of great textbooks already out there, such as Analog Device‚Äôs SDR textbook and dspguide.com. You can always use Google to recall trig identities or the Shannon limit. Think of this textbook like a gateway into the world of DSP and SDR: it‚Äôs lighter and less of a time and monetary commitment, when compared to more traditional courses and textbooks.&lt;/p&gt;
    &lt;p&gt;To cover foundational DSP theory, an entire semester of ‚ÄúSignals and Systems‚Äù, a typical course within electrical engineering, is condensed into a few chapters. Once the DSP fundamentals are covered, we launch into SDRs, although DSP and wireless communications concepts continue to come up throughout the textbook.&lt;/p&gt;
    &lt;p&gt;Code examples are provided in Python. They utilize NumPy, which is Python‚Äôs standard library for arrays and high-level math. The examples also rely upon Matplotlib, which is a Python plotting library that provides an easy way to visualize signals, arrays, and complex numbers. Note that while Python is ‚Äúslower‚Äù than C++ in general, most math functions within Python/NumPy are implemented in C/C++ and heavily optimized. Likewise, the SDR API we use is simply a set of Python bindings for C/C++ functions/classes. Those who have little Python experience yet a solid foundation in MATLAB, Ruby, or Perl will likely be fine after familiarizing themselves with Python‚Äôs syntax.&lt;/p&gt;
    &lt;head rend="h2"&gt;Contributing¬∂&lt;/head&gt;
    &lt;p&gt;If you got value from PySDR, please share it with colleagues, students, and other lifelong learners who may be interested in the material. You can also donate through the PySDR Patreon as a way to say thanks and get your name on the left of every page below the chapter list.&lt;/p&gt;
    &lt;p&gt;If you get through any amount of this textbook and email me at marc@pysdr.org with questions/comments/suggestions, then congratulations, you will have contributed to this textbook! You can also edit the source material directly on the textbook‚Äôs GitHub page (your change will start a new pull request). Feel free to submit an issue or even a Pull Request (PR) with fixes or improvements. Those who submit valuable feedback/fixes will be permanently added to the acknowledgments section below. Not good at Git but have changes to suggest? Feel free to email me at marc@pysdr.org.&lt;/p&gt;
    &lt;head rend="h2"&gt;Acknowledgements¬∂&lt;/head&gt;
    &lt;p&gt;Thank you to anyone who has read any portion of this textbook and provided feedback, and especially to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Barry Duggan&lt;/item&gt;
      &lt;item&gt;Matthew Hannon&lt;/item&gt;
      &lt;item&gt;James Hayek&lt;/item&gt;
      &lt;item&gt;Deidre Stuffer&lt;/item&gt;
      &lt;item&gt;Tarik Benaddi for translating PySDR to French&lt;/item&gt;
      &lt;item&gt;Daniel Versluis for translating PySDR to Dutch&lt;/item&gt;
      &lt;item&gt;mrbloom for translating PySDR to Ukrainian&lt;/item&gt;
      &lt;item&gt;Yimin Zhao for translating PySDR to Simplified Chinese&lt;/item&gt;
      &lt;item&gt;Eduardo Chancay for translating PySDR to Spanish&lt;/item&gt;
      &lt;item&gt;John Marcovici&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As well as all PySDR Patreon supporters!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46413975</guid><pubDate>Sun, 28 Dec 2025 20:02:50 +0000</pubDate></item><item><title>Stepping down as Mockito maintainer after 10 years</title><link>https://github.com/mockito/mockito/issues/3777</link><description>&lt;doc fingerprint="6702905447ee068a"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; Notifications &lt;tool-tip&gt;You must be signed in to change notification settings&lt;/tool-tip&gt;&lt;/item&gt;
      &lt;item&gt;Fork 2.6k&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Description&lt;/head&gt;
    &lt;p&gt;In March 2026, I will be Mockito maintainer for 10 years (nearly a third of my whole life). Looking ahead, I decided that a decade milestone is a good moment to pass on maintainership to other folks. In the coming months until March, I will spend time ensuring a smooth transition in maintainership.&lt;/p&gt;
    &lt;p&gt;In this issue I list several considerations why I made the decision. Communication and discussion of plans for future maintainership will be somewhere else, most likely in a separate GitHub issue. Stay tuned for that.&lt;/p&gt;
    &lt;head rend="h2"&gt;Energy drain because of JVM agent change&lt;/head&gt;
    &lt;p&gt;As you might know, Mockito 5 shipped a breaking change where its main artifact is now an agent. That's because starting JVM 22, the previous so-called "dynamic attachment of agents" is put behind a flag. This change makes sense from a security point-of-view and I support it.&lt;/p&gt;
    &lt;p&gt;However, the way this was put forward to Mockito maintainers was energy draining to say the least. Mockito is probably the biggest user of such an agent and is often looked at for inspiration by other projects. As such, Mockito often pioneers on supporting JVM features, built on a solid foundation with ByteBuddy. Modules was such a feature that took months of hard work by Rafael to figure out, including providing feedback to JVM maintainers.&lt;/p&gt;
    &lt;p&gt;Unfortunately such a collaborative way of working was not the case when discussing agents. To me, it felt like the feature was presented as a done deal because of security. While dynamic attachment is problematic in many ways, no alternative solutions were proposed. That's okay, as Mockito pioneers on these solutions, yet in this case I felt we were left alone.&lt;/p&gt;
    &lt;p&gt;My personal take is that folks involved with the change severely underestimated the societal impact that it had. The fact that proper build support is non-existent to this day shows that agents are not a priority. That's okay if it isn't a priority, but when it was communicated with Mockito I perceived it as "Mockito is holding the JVM ecosystem back by using dynamic attachment, please switch immediately and figure it out on your own".&lt;/p&gt;
    &lt;p&gt;Here, the fact that I (and others) are volunteers doing their best for the project, is important to understand the societal impact. When you put individuals under pressure, who do this work in their own time out of goodwill, things crumble. It's commonly joked about with XKCD's on the fact that the whole open source world relies on a couple of individuals. That couldn't be more true in this situation, where the collaborative system collapses when too much pressure is put on individual folks.&lt;/p&gt;
    &lt;p&gt;This saga planted the seed to reconsider my position as maintainer.&lt;/p&gt;
    &lt;head rend="h2"&gt;Kotlin as the future and odd one out&lt;/head&gt;
    &lt;p&gt;It's undeniable that Kotlin as a language has grown in popularity in recent years. While Mockito maintains several flavors for JVM languages, these packages typically include sugar that makes integration nicer. In all cases, mockito-core remains the place where functionality is implemented.&lt;/p&gt;
    &lt;p&gt;Unfortunately, this model doesn't nicely apply to Kotlin. Where almost all JVM languages work similarly under the hood, Kotlin often does things differently. This means that in several places in mockito-core, there are separate flows dedicated to Kotlin. Most often that's a direct result of Kotlin doing (in my opinion) shenanigans on the JVM that the JVM never intended to support, yet was able to.&lt;/p&gt;
    &lt;p&gt;Even within Kotlin itself, features don't work consistently. Suspend functions are the most well-known example. As such, Mockito code becomes more spaghetti, it's API sometimes fully duplicated just to support a core Kotlin language feature and overall less maintainable.&lt;/p&gt;
    &lt;p&gt;While I fully understand the reasons that developers enjoy the feature richness of Kotlin as a programming language, its underlying implementation has significant downsides for projects like Mockito. Quite frankly, it's not fun to deal with.&lt;/p&gt;
    &lt;p&gt;To me, a future where Kotlin becomes more predominant is not a future that makes me hopeful I can keep on dedicating energy to Mockito.&lt;/p&gt;
    &lt;head rend="h2"&gt;Alternative open source activities&lt;/head&gt;
    &lt;p&gt;I have always been a fan of open source work and have contributed to hundreds of projects in all these years. Mockito is my most important project, but I have also consistently worked on others. In recent months, I have rediscovered the joy of programming by working on Servo. It's a web engine written in Rust.&lt;/p&gt;
    &lt;p&gt;When I need to choose how I want to spend my 2 hours of evening time in a given week, I rarely preferred Mockito in the last year. In the past, Mockito was my go-to and I enjoyed it a lot. Nowadays, Servo and related projects provide significantly more enjoyment.&lt;/p&gt;
    &lt;p&gt;Justifying why I needed to work on Mockito becomes difficult when (because of the above reasons) it feels like a chore. Volunteering work shouldn't feel like a chore, at least not for a long time.&lt;/p&gt;
    &lt;head rend="h2"&gt;Summing it up&lt;/head&gt;
    &lt;p&gt;As you have read, these three factors combined led me to the decision. The first point explains why I started to doubt my position, the second point why I am not hopeful for things to change in a good way and the third point how I found enjoyment in a different way.&lt;/p&gt;
    &lt;p&gt;While these points had impact on me as maintainer, my hypothesis is that it doesn't apply to others in the same way. I know others are eager to work on Kotlin support for example. That's why I concluded that a decade is enough time to have helped Mockito forward. Now it's time for somebody else to take over, as I believe that's in the best interest of Mockito as a project. Because ultimately that's why I chose to become maintainer in the first place: I believed that with my work, I could improve Mockito for millions of software engineers.&lt;/p&gt;
    &lt;p&gt;For those wondering: yes I wholeheartedly advise everyone to take on a volunteering task such as maintaining an open source project. It was an honour and privilege to do so and I thank those that I enjoyed working with.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46414078</guid><pubDate>Sun, 28 Dec 2025 20:14:58 +0000</pubDate></item><item><title>Show HN: Phantas ‚Äì A browser-based binaural strobe engine (Web Audio API)</title><link>https://phantas.io</link><description>&lt;doc fingerprint="c7bc1d2388b9f1ff"&gt;
  &lt;main&gt;
    &lt;head rend="h3"&gt;Operating Protocol&lt;/head&gt;
    &lt;p&gt;STEREO AUDIO: Headphones essential. Frequencies split left/right to create the binaural beat.&lt;/p&gt;
    &lt;p&gt;OPTICAL DRIVING: Set brightness to 100%. Close eyes. Position screen to fill your entire visual field with light.&lt;/p&gt;
    &lt;p&gt;FEEDBACK LOOP: Log duration and pre/post state below to track efficiency.&lt;/p&gt;
    &lt;p&gt;STANDBY&lt;/p&gt;
    &lt;p&gt;Screen Lock&lt;/p&gt;
    &lt;head rend="h4"&gt;ALPHA (10HZ) - PASSIVE OBSERVATION&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Best For: Routine Coding, "Autopilot" Work.&lt;/item&gt;
      &lt;item&gt;How To Use: Don't force thoughts. Let your mind wander. Good for stability.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;10.0&lt;/p&gt;
    &lt;p&gt;15&lt;/p&gt;
    &lt;p&gt;30&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46414258</guid><pubDate>Sun, 28 Dec 2025 20:38:09 +0000</pubDate></item><item><title>Loss of moist broadleaf forest in Africa has turned a carbon sink into source</title><link>https://www.nature.com/articles/s41598-025-27462-3</link><description>&lt;doc fingerprint="177840d35d162283"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Abstract&lt;/head&gt;
    &lt;p&gt;Africa‚Äôs forests and woody savannas have historically acted as a carbon sink, removing atmospheric carbon and storing it as biomass. However, our novel analysis reveals a critical transition from a carbon sink to a carbon source between 2010 and 2017. Using new high-resolution satellite-derived biomass maps, validated with field plots and machine learning techniques, we quantified the aboveground biomass stocks across African biomes over a decade. Between 2007 and 2010, the continent gained 439 ¬± 66 Tg yr‚Åª1 of aboveground biomass, but from 2010 to 2015 biomass declined by ‚àí 132 ¬± 20 Tg yr-1 and from 2015 to 2017 this decline continued with a loss of ‚àí 41 ¬± 6 Tg yr-1, primarily driven by deforestation in tropical moist broadleaf forests. Gains in savanna biomass partially offset these losses, likely due to shrub encroachment. Our findings underline the urgent need for implementing policies to halt global deforestation as required by the Glasgow Leaders Declaration to close the global emissions gap. The current ongoing revisions of Nationally Determined Contributions to the Paris Agreement need to be even more ambitious to compensate for the ongoing loss of natural carbon sinks.&lt;/p&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;Africa‚Äôs ecosystems play a pivotal role in the global carbon cycle, contributing approximately 20% of global carbon removals through terrestrial net primary production, 40% of the world‚Äôs carbon emissions from biomass burning and 20% of emissions from deforestation and forest degradation1. Carbon removals occur in forests and woody savannas through photosynthesis, while carbon emissions are largely driven by fire during forest cover loss, shifting cultivation, agricultural burning and fuelwood burning (~0.4 Pg C y-1 for Africa)2. Biomass burning in savannas mainly burns perennial herbaceous plant matter, which does not add significantly to net carbon emissions but is thought to accelerate the carbon cycle by reducing carbon turnover times3.&lt;/p&gt;
    &lt;p&gt;Despite their critical importance, Africa‚Äôs forests and savannas face increasing pressures from anthropogenic and natural disturbances, leading to a decline in their carbon sequestration potential. Understanding these dynamics is essential for addressing the goals of the Paris Agreement and devising effective climate mitigation strategies.&lt;/p&gt;
    &lt;p&gt;While previous studies have mapped African biomass and carbon stocks, they are limited by coarse spatial resolutions, temporal gaps, or methodological inconsistencies. Notably, the sparse availability of field-based data has hampered precise assessments of biomass changes at fine spatial scales. Recent advancements in satellite remote sensing, such as the GEDI LiDAR and ALOS PALSAR radar systems, combined with more powerful machine learning models, offer an unprecedented opportunity to bridge these knowledge gaps.&lt;/p&gt;
    &lt;p&gt;Previous studies have estimated the extent of African forests, including mosaics of forests and woodlands, as between 638.2 and 836.8 million ha4. These forests contain large carbon stocks in the form of aboveground woody biomass, in the range from 85 to 129 Pg5,6,7,8,9. African tropical forests have an average aboveground biomass density over 396 Mg ha-1, but this can be as high as 429 Mg ha-1 in some areas of the Congo Basin10. This far exceeds the carbon stock density in savannas with an average aboveground biomass density of 45 Mg ha-111, although savannas cover a much larger total area4, making forests and savannas the most relevant biomes for understanding the total carbon stocks stored in aboveground woody biomass (Fig. 1).&lt;/p&gt;
    &lt;p&gt;The quantitative nature of carbon dynamics of African ecosystems remains subject to scientific debate due to the sparse observation network1,3. Bombelli et al.3 estimated that sub-Saharan Africa is a net carbon sink of between 0.16 - 1 Pg C yr-1, depending on the data source used. Bombelli et al.3 also concluded that biogeochemical process models give an ‚Äúunrealistically large sink‚Äù between 1.3 and 3.9 Pg C yr-1 (average of 3.23 Pg C yr-1). Even the direction of the carbon fluxes in Africa is controversial. Some recent studies indicate that the African carbon sink is declining16, while other studies conclude that it is in a near-zero steady-state2, or even that it has already transitioned to a carbon source17,18. A more recent assessment of the African greenhouse gas budget in REgional Carbon Cycle Assessment and Processes 1 (RECCAP1)19 concluded that Africa‚Äôs carbon sink capacity is decreasing and net ecosystem exchange switched from a small sink of -0.61 ¬± 0.58 PgC yr-1 to a small source in RECCAP2 at +0.16 (CI=[-0.52;1.36]) PgC yr-1. In this context, the role of forest degradation20, i.e. the partial loss of aboveground tree biomass from a forest, and total forest cover loss, as well as their interannual variability are major uncertainties in the African carbon cycle1.&lt;/p&gt;
    &lt;p&gt;The rationale of this study was to analyse new satellite-based maps of aboveground biomass stocks at unprecedented high spatial resolution and to use the biomass dynamics inferred from these maps to test whether forest biomass stocks in Africa have switched from a net sink into a net source.&lt;/p&gt;
    &lt;p&gt;Earth observation from satellites combined with field-plot-based national forest inventories provides spatially explicit estimates of key terrestrial carbon pools and their changes over time. In Africa, field plots are scarce. Because of the correlation between forest canopy height, aboveground woody biomass and aboveground carbon stock, satellite-derived forest canopy height maps are the best option for large-scale mapping. Spaceborne Light Detection and Ranging (LiDAR) transmits polarised light from a pulsed laser to measure the distance to the Earth‚Äôs surface. Spaceborne LiDAR missions such as ICESAT21 and GEDI22 allow the estimation of canopy height from the vertical profile of returned laser light to the sensor. These datasets can be used to train empirical and machine learning models, in combination with other datasets such as optical multispectral imagery, digital elevation models, or maps of radar backscatter, in order to map aboveground biomass density across the tropics, including tropical Africa, e.g. for a single year5,6 and for different years to estimate biomass (and carbon) gains and losses using coarse spatial resolution images9,17,23,24,25. However, the resolution of these aboveground biomass density maps is too coarse to relate them to drivers of disturbances, such as forest cover loss and forest degradation26.&lt;/p&gt;
    &lt;p&gt;Because deforestation and forest degradation occur on relatively fine spatial scales, more reliable estimates of aboveground biomass density across Africa and its annual gains and losses can only be quantified at high spatial resolution. This is the route to improving our quantitative knowledge of the African terrestrial carbon cycle and estimating carbon emissions and removals more reliably.&lt;/p&gt;
    &lt;head rend="h2"&gt;Results&lt;/head&gt;
    &lt;p&gt;Our analysis of aboveground woody biomass density derived from Earth observation over a time period of 11 years reveals the spatial distribution and temporal trends of biomass gains and losses for all forests and woody savannas in Africa. Fig. 1a shows a map of aboveground woody biomass density for the year 2017. Tropical Grasslands, Savannas and Shrublands are by far the largest biome by area with an average aboveground biomass density of 32 Mg ha-1, while Tropical Moist Broadleaf Forests cover a much smaller area with much higher biomass density (Fig. 1b,c). An accuracy assessment against an extensive set of field plot measurements gave a coefficient of determination (R2) of 0.70, a Root Mean Square Difference (RMSD) of 74.2 Mg ha-1, a Relative Root Mean Square Difference (Rel. RMSD) of 33%, and a Mean Bias Difference (MBD) of 8 Mg ha-1, but the higher aboveground biomass densities tended to be overestimated (Fig. 1d). A further comparison with airborne LiDAR-based biomass estimates for 2015 and 2016 (Fig. 1e) showed that the aboveground woody biomass map is unbiased even at higher biomass levels (R2 = 0.85, RMSD = 47.7 Mg ha-1, Rel. RMSD = 48%, MBD = 0 Mg ha-1), though these results may be overoptimistic, due to the spatial proximity to the training data. While both validation approaches suggest there may be potential underestimations beyond 350 Mg/ha (Table S4a), this is a common issue in remote sensing derived AGBD maps. Quantifying AGBD in dense forests is still challenging and AGBD remote sensing retrieval algorithms suffer from large uncertainties at high AGBD27.&lt;/p&gt;
    &lt;head rend="h3"&gt;Aboveground woody biomass gains and losses across Africa&lt;/head&gt;
    &lt;p&gt;From 2007 to 2010, Africa gained 439 ¬± 66 Tg yr-1 (0.37% yr-1) net aboveground woody biomass. However, between 2010 and 2015 the continent lost -132 ¬± 20 Tg yr-1 or -0.11% yr-1 biomass. Between 2015 and 2017 the average net aboveground biomass change decreased to a loss rate of -41 ¬± 6 Tg yr-1 or -0.03% yr-1 (Fig. 2a). These results indicate that Africa‚Äôs forests have transitioned from a net sink of aboveground biomass to a net source over that time period. We now investigate where that switch has taken place.&lt;/p&gt;
    &lt;p&gt;The recent aboveground biomass losses in Africa are mostly driven by losses in the Tropical Moist Broadleaf Forests biome over this period. This is partly offset by biomass gains in the savanna biome from 2015 to 2017 (Fig. 2b). These gains are plausibly explained by enhanced shrub encroachment due to the carbon fertilisation effect of increasing atmospheric carbon dioxide, which has been shown to alter the competition between trees and grasses in savannas in favour of trees28. The Tropical Moist Broadleaf Forests biome gained +192 Tg yr-1 aboveground biomass in the period 2007 to 2010. This changed to a biomass loss of -70 Tg yr-1 between 2010‚Äì2015, and an even greater biomass loss of ‚àí 154 Tg yr-1 between 2015 and 2017. The observed biomass losses for 2007 to 2017 can be explained by a significant increase in forest cover loss rates during the last decade in Democratic Republic of Congo, Madagascar and some West African countries29.&lt;/p&gt;
    &lt;p&gt;The largest cumulative aboveground biomass gains took place in the Tropical Moist Broadleaf Forests biome of the Congo basin in areas within Equatorial Guinea, Gabon and the Republic of Congo (Fig. 2). The majority of the Tropical Grasslands, Savannas and Shrublands biome shows cumulative aboveground biomass gains across Africa, although between 2010 and 2015, it exhibited a cumulative loss.&lt;/p&gt;
    &lt;p&gt;These results provide fresh evidence from the new aboveground biomass dataset presented here that Africa‚Äôs forests have switched from a net sink of carbon to a net source after 2010.&lt;/p&gt;
    &lt;head rend="h2"&gt;Discussion&lt;/head&gt;
    &lt;p&gt;Previous studies have led to contradictory insights into whether Africa‚Äôs ecosystems are a net sink or a net source of carbon. This study provides the first continent-wide, high-resolution assessment of aboveground woody biomass changes in Africa over a decade, revealing a significant transition from a carbon sink to a source between 2010 and 2017. Our analysis, based on satellite-derived aboveground biomass maps validated with field data, highlights the escalating impact of deforestation in tropical moist broadleaf forests as a primary driver of biomass loss. We acknowledge that satellite-derived AGBD has some limitations due to the saturation at high AGBD ranges. However, there are two facts that give us confidence in our conclusions. First, we have adopted a rigorous uncertainty analysis approach and the 95% confidence intervals of the net changes in AGBD shown in Fig. 2 indicate that we can claim with high confidence that the transition from a carbon sink to a source is real. The confidence intervals do not overlap. Second, Table 1 and Figure S7 show that the AGBD values in our study are reasonably similar to independent country-level statistics reported by FAO. For example, for the Democratic Republic of Congo (DRC), which contains by far the most of the African AGB, the deviation of our estimate from the FAO estimate is only 11%. Furthermore, as most deforestation occurs in the dense moist broadleaf forests where biomass is high, the underestimations we detect beyond 350 Mg/ha suggest that our analysis may not reflect the full impacts of forest loss on carbon emissions and that the reversal from sink to source we suggest may be even more pronounced than what we report here. While gains in savanna regions, potentially due to shrub encroachment, partially mitigated these losses, they are insufficient to reverse the overall trend.&lt;/p&gt;
    &lt;p&gt;Our study focuses on aboveground woody biomass stock and its changes, as this is the only pool that is persistent over long time periods and that we can realistically quantify using remote sensing techniques. Although also persistent, soil carbon stocks cannot be estimated with any confidence from remote sensing. At continental and biome scale, we consider carbon fluxes from woody debris and litter as short-term (annual turnover time), with no substantial dampening or enhancing effects on ecosystem-level carbon fluxes.&lt;/p&gt;
    &lt;p&gt;The implications of this shift are profound. Africa‚Äôs forests and woodlands have historically served as a carbon sink. Now, they are contributing to widening the global greenhouse gas emissions gap that needs to be filled to stay within the goals of the Paris Agreement. The significance of this finding to global climate policies and the current revision of the Nationally Determined Contributions to the Paris Agreement is that even more greenhouse gas emission reductions are needed than was the case before this natural carbon sink shut down. Our findings underline the urgent need for strengthened conservation policies, improved forest governance, and targeted restoration initiatives, such as REDD+. The fine-scale spatial and temporal insights provided in this study can support policymakers and scientists in prioritizing interventions to halt biomass loss and enhance carbon sequestration.&lt;/p&gt;
    &lt;p&gt;New insights into the geographical regions where significant biomass changes have occurred are also presented. The biomass maps at continental and biome scales were validated with a large independent dataset of field plot data and LiDAR-based estimates of aboveground biomass density (Supplementary Material). The fine spatial resolution allowed us to account for the trends in biomass loss associated with forest disturbances as small as one hectare, which was not possible until recently9,17,31,32.&lt;/p&gt;
    &lt;p&gt;Total aboveground biomass stocks of African forests and woodlands are estimated at of 118 Pg biomass, which equates to approximately 59 Pg carbon. These overall values are consistent with previous estimates using independent datasets, e.g. 113 Pg5, 111 Pg9, 116 Pg8, and 129 Pg6 biomass stocks, but it is higher than the estimates provided by Santoro et al.31 and Avitabile et al.7 (85 Pg and 96 Pg, respectively). The latter two studies excluded some areas with woody aboveground biomass by applying a spatial forest mask to exclude several woody vegetated areas, such as savannas, that did not fit their specific forest definitions, while our estimates include all woody vegetation in the tropical forest and savanna biomes with a percentage tree cover above 1%.&lt;/p&gt;
    &lt;p&gt;The aboveground biomass estimates presented here, and other Earth observation-based studies disagree with official biomass statistics for some countries at national level as reported by the United Nations Food and Agriculture Organisation (FAO) (Table 1). These differences are most likely also the consequence of using different national forest definitions33,34 leading to substantial differences in the reported total national forest area.&lt;/p&gt;
    &lt;p&gt;The pantropical and continental AGBD maps products derived by Saatchi et al.5, Baccini et al.6, Avitabile et al.7, Bouvet et al.35 present spatial resolutions from 25 m to 1 km, and reported Rel. RMSD = 33%, RMSD = 39 Mg/ha, RMSD = 87‚Äì90 Mg/ha, and RMSD = 17‚Äì19 Mg/ha for Africa, respectively. However, the later African AGBD 25m pixel map is constrained to AGBD levels below 100 Mg ha-1. Only the global map from Santoro et al.31 presents the same spatial resolution as the current study (i.e. 100m pixel size) but reports larger errors with Rel. RMSD = 57‚Äì73% for tropical and subtropical regions compared to this study‚Äôs Rel. RMSD = 33‚Äì40%.&lt;/p&gt;
    &lt;p&gt;The mean annual net aboveground biomass change shows gains of +57 ¬± 9 Tg yr-1 for the period 2007 to 2017. However, analysing the gains and losses for different time periods, an annual net gain of +439 ¬± 66 Tg yr-1 is found in the earlier time period from 2007 to 2010, followed by a net loss of -106 ¬± 16 Tg yr-1 from 2011 to 2017. This indicates a rapid acceleration of aboveground biomass losses in Africa, which is driven by increasing biomass loss rates in the Tropical Moist Broadleaf Forests.&lt;/p&gt;
    &lt;p&gt;The evidence presented here suggests that Africa‚Äôs forests and woodlands have switched from a net carbon sink into a net source because of increased biomass losses due to human activities and natural disturbances. This finding is consistent with growing forest loss rates in Africa from 2010 onwards as reported by FAO36, and the increasing forest cover loss rates based on satellite observations from 2012 to 2013 onwards by Hansen et al.29. FAO36 statistics on increasing annual harvested roundwood from 277 million m3 in 1961 up to 768 million m3 by 2017 also confirm these forest losses. The observed trends may be further exacerbated in the future by population growth in Africa37, the increasing export demand particularly from Asia38, and the resulting pressure on natural resources (agricultural expansion for commodity crop, timber and fuelwood). The long-term persistence of these trends will depend on local governance and whether resources are used sustainably.&lt;/p&gt;
    &lt;p&gt;The results provide further independent evidence of a shift in forest functioning from a carbon sink to a source around this time period, which is consistent with recent studies of all pantropical regions19,35,39,40.&lt;/p&gt;
    &lt;p&gt;A comprehensive assessment of the terrestrial greenhouse gas budget of Africa found that most studies agree that Africa is a small sink of carbon on an annual scale, with an average value of ‚àí 0.61 ¬± 0.58 Pg C yr‚àí14. The net loss of aboveground woody biomass from 2011 to 2017 of ‚àí 106 ¬± 16 Tg yr-1 observed here (weighted mean) may well tip the overall carbon budget into a net source overall when considering all carbon pools and fluxes.&lt;/p&gt;
    &lt;p&gt;The world needs to step up efforts to protect the substantial carbon stocks in Africa‚Äôs aboveground woody biomass and restore lost forest areas to counter the climate crisis, as agreed in the Glasgow Leaders Declaration on Forests and Land Use41 at the 26th Conference of the Parties (COP26) to halt net deforestation by 2030. The world otherwise risks losing an important carbon sink needed to achieve the goals of the Paris Agreement. Reversing biomass losses in Africa requires actions in the political, economic and societal spheres, to promote capacity building42, improve forest governance43, implement financial incentives through the REDD+ initiative44, and facilitate technological infrastructure, such as satellite-enabled forest alert systems to halt illegal logging such as those deployed in Kenya45. Restoration initiatives such as AFR10046 and Restor47 will also be needed.&lt;/p&gt;
    &lt;p&gt;Future research should explore the underlying drivers of regional variability in biomass dynamics and assess the potential of emerging technologies for near-real-time monitoring of forest disturbances and strengthening forest governance.&lt;/p&gt;
    &lt;head rend="h2"&gt;Methods&lt;/head&gt;
    &lt;p&gt;Assessing forest aboveground biomass dynamics over long time periods and at continental to global scale requires reference observations such as forest inventory field plots in combination with satellite Earth observation. To overcome the low availability and quality of reference data, we used the spaceborne Geoscience Laser Altimeter System (GLAS) onboard the Ice, Cloud, and land Elevation Satellite (ICESat)48, which operated from 2003 to 2010 and acquired millions of Light Detection and Ranging (LiDAR) footprints, providing measurements of canopy height and other biophysical metrics relating to canopy structure that are highly correlated with aboveground biomass49. The Global Ecosystem Dynamics Investigation (GEDI) LiDAR instrument22 onboard the International Space Station that commenced operation in 2019 uses similar technology to the GLAS/ICESat instrument but with smaller footprints and much denser coverage (narrower spacing between footprint locations along the orbit), providing a dense network of training and validation data on forest canopy height.&lt;/p&gt;
    &lt;p&gt;Based on a machine learning algorithm, L-band Synthetic Aperture Radar (SAR) backscatter image mosaics acquired by ALOS PALSAR-1/PALSAR-248 and optical multispectral Landsat-derived percent tree cover maps29 were used as joint predictors to extend the canopy height obtained from GEDI to canopy height maps across the whole of Africa. Regional maps of aboveground biomass density derived from airborne LiDAR over a range of biomes in Africa were then used to derive an empirical model that estimates aboveground biomass density as a function of canopy height. The LiDAR-based aboveground biomass footprint estimates from LiDAR were used to train the machine learning model. The model was then used to produce annual Africa-wide maps of aboveground biomass density and its standard deviation (SD) at 100 m pixel spacing for the period 2007 to 2017. The maps were validated using a large independent dataset of field plot measurements across the continent. The aboveground biomass density maps and associated standard deviations were used to estimate the African aboveground woody biomass stock and its annual changes (with confidence intervals based on the uncertainty characterization described in Supplementary Materials) with the aim of contributing to improvement of carbon inventories, understanding trends, and testing whether the aboveground biomass change rate has increased, reduced or changed sign over the period 2007-2017.&lt;/p&gt;
    &lt;head rend="h3"&gt;Datasets&lt;/head&gt;
    &lt;p&gt;Spaceborne LiDAR from GEDI. The GEDI L2B Canopy Cover and Vertical Profile Metrics product (version 1), available from the NASA/USGS Land Processes Distributed Active Archive Center50, was collected from April 2019 to June 2019 (Fig. 3a). The LiDAR metrics estimated by this product are representative of a 25 m diameter footprint on the ground. Version 1 of the product has a geolocation error of approximately 15-20 m, so can be difficult to use with moderate spatial resolution imagery such as Sentinel-1/-2 or Landsat (i.e. 10-30 m spatial resolution pixels).&lt;/p&gt;
    &lt;p&gt;Airborne laser scanning (ALS). This study uses gridded LiDAR-derived aboveground biomass density maps based on airborne LiDAR acquired in 2016 at 4 different sites in Gabon (Lope, Mabounie, Mondah, and Rabi)51,52, and in 2015-16 at 2 sites in Kenya (Taita Hills and Maktau)53,54. The canopy height to AGBD model was developed using 50% of the pixels from these datasets, while the remaining 50% were used to validate the aboveground biomass product.&lt;/p&gt;
    &lt;p&gt;Field plot data of aboveground biomass density. We collated a dataset consisting of 10,837 aboveground biomass density reference field plots across Africa (see Table S2 in supplementary material) to be used as an independent validation dataset for the 2017 map (Fig. 3). The assessment was limited to 2017 due to the lack of re-measured plot data. Field plots were measured in different years, mainly between 2000 and 2017, with the vast majority before 2010. The plot data are from different national forest inventories and research projects, so have various plot designs, sizes and shapes. Since most of the plots do not have accurate spatial coordinates due to licensing restrictions, we cannot use them directly to validate the 100 m resolution pixel maps. Instead, we followed the approach described by Santoro et al.31 and Araza et al.15 in which the plot values were first adjusted to minimize the temporal and areal mismatches between the plot and map estimates of aboveground biomass density. The adjustment was necessary because of the uneven spatial distribution of the reference samples, the variety of plot sizes used, variations in field survey methods, and used allometric equations to estimate biomass of the plots55. The plots were mostly smaller than 1 ha and often represented only a small fraction of the area covered by a 1 ha biomass map pixel. To reduce the effect of random errors caused by different resolutions of the reference dataset and the biomass map, we aggregated the map and the plot data to 0.1¬∞ grid cells15. This procedure yielded 463 grid cells with reference aboveground biomass density values for validation (Fig. 3b). The standard deviation associated with each of these values was estimated by accounting for the principal plot measurement error sources (as described in Araza et al.15).&lt;/p&gt;
    &lt;p&gt;ALOS PALSAR/ALOS-2 PALSAR-2 radar image mosaics. JAXA‚Äôs annual mosaics of L-band SAR HH and HV polarised backscatter (Œ≥0) were based on ALOS PALSAR from 2007 to 2010 and ALOS-2 PALSAR-2 from 2015 to 201756,57. No mosaics are available for 2011 to 2014. The PALSAR-2 mosaics for 2018 to 2020 are available, but the pre-processing and geolocation approaches are different from the previous mosaics, so they were excluded from this analysis. The mosaics are a calibrated, 16-look, re-projected, orthorectified and slope-corrected product with 25 m pixel spacing, to which a de-striping process has been applied22,31,33. The PALSAR and PALSAR-2 mosaics were normalised to reduce artefacts and to ensure temporal consistency of the radar backscatter signal, allowing the same trained model to be used for the whole time series. Artefacts in the mosaics usually result from changes in moisture conditions between image acquisitions, which affects the backscatter, or appear in the pre-processing due to inadequate calibration and/or topographic corrections. We followed a similar approach to that described in58, but instead of superpixels used a circular moving window 100 pixels in diameter (~2,000 ha). This normalises the PALSAR/PALSAR-2 imagery to a common baseline based on the mean and standard deviation of backscatter of the PALSAR mosaics (2007-2010). Implicit in this procedure is the assumption that continuous changes with scale larger than 2,000 hectares did not occur from year to year. Normalizing the images at this large scale also tends to ensure that local changes due to disturbances and vegetation growth are preserved. The analysis used both HH and HV polarisations and two additional metrics, the Cross-polarisation Ratio (CpR = HH‚ÅÑHV) and the Radar Forest Degradation Index (RFDI = (HH‚ÄìHV)‚ÅÑ(HH+HV))59.&lt;/p&gt;
    &lt;p&gt;Percent tree cover data. A 30 m Landsat-based map product of percent tree canopy cover for the year 2000 and annual tree cover loss estimates for the period from 2000 to 201729 was used to generate annual percent tree cover maps for each year. For the year 2007, all pixels detected as forest cover loss were set as 0% percent tree cover, while pixels detected as having forest cover loss in previous years (i.e. from 2000 to 2006) were set to ‚Äúno data‚Äù, as we have no information on regrowth after the disturbance. The canopy height and aboveground biomass predictions for these ‚Äúno data‚Äù pixels were performed using only PALSAR as a predictor variable (see Modelling Framework). We repeated this process for all the years within our study period (2007-2010 and 2015-2017). PALSAR data and Landsat percent tree cover datasets were mosaicked and co-registered to generate two stacks of predictor datasets, with 50 m and 100 m pixel spacing, respectively. Woody vegetated areas were defined as pixels with equal or above 1% tree cover.&lt;/p&gt;
    &lt;head rend="h3"&gt;Modelling framework&lt;/head&gt;
    &lt;p&gt;GEDI footprint selection and clustering. We used the maximum footprint height in a footprint, provided by the GEDI L2B footprint product, as reference canopy height metric, but performed a filtering process to select only the highest quality footprints for training and validation purposes. Coverage footprints were excluded due to their lack of laser light penetration in dense forests and only footprints acquired by the full power beams were used. Only night acquisitions (solar elevation &amp;lt; 0¬∞) with a beam sensitivity greater than 95% were retained. Low quality footprints, as indicated by the L2B quality assurance layers, were discarded, as were footprints with canopy height above the 99.9th percentile of the initial set. The Copernicus Global Land Cover dataset60 was used to exclude footprints in non-vegetated classes. We used the 11 forest classes and the shrubland class for this purpose60. After the filtering, approximately 1.8 million footprints were available, distributed across the African continent (Fig. 3). The GEDI footprints were grouped into 4-footprint clusters along the track direction, in each of which the top canopy height values (RH100) were averaged to correct for sampling and geolocation errors. This provided the main reference data for training and validating our canopy height model. The average by cluster increases the sampled area of our reference unit from 0.05 ha (1 footprint) to 0.2 ha (4 footprints) and has been demonstrated to increase accuracy when training models with spaceborne LiDAR footprints6. The larger sampled area helps to average out various errors typical of small sampling units (e.g. small inventory plots), such as sampling error and geolocation error. Only clusters with 4 consecutive footprints were used.&lt;/p&gt;
    &lt;p&gt;Canopy height modelling. We used a non-parametric machine learning Random Forests (RF) regression algorithm61, following the same framework as in62 and 63, to generate a canopy height model (CHM) and its associated error at pixel level using PALSAR/PALSAR-2 radar backscatter and Landsat Percent Tree Cover for the same years as predictors. We used 100 trees for each RF model run. Generation of the canopy height model uses two different resolutions (Fig. 4): (i) Remote sensing signatures were extracted from the four 50 m pixels that overlap the 0.2 ha area corresponding to each cluster of four GEDI footprints and averaged; these four pixels are equivalent to the area of 1 ha. (ii) The model training and the prediction output are at 1 ha pixel size, i.e. 100 m by 100 m.&lt;/p&gt;
    &lt;p&gt;The conversion from 50 m to 100 m was needed because geolocation errors in the GEDI footprint products (15‚Äì20 m for version 1 and around 10 m for version 2) prevent small (&amp;lt; 50 m) pixels being adequately matched to single GEDI footprints. Additionally, as with small forest inventory plots, a large tree within the footprint biases the canopy height and makes the value unrepresentative of the canopy height within 1 ha. This two-scale scheme follows Saatchi et al.5 and Baccini et al.6, and aims to average out these errors by combining several footprints. We assume that 4 footprints located within 1 ha (four 50m x 50m pixels) are more representative of the actual canopy height than 1 or 2 footprints located within a 100 m pixel.&lt;/p&gt;
    &lt;p&gt;The GEDI canopy height dataset was randomly partitioned into 2 datasets: 10% for training the CHM model (36,944 GEDI clusters) and the remaining 90% (328,243 clusters) for independent validation. Training was performed within a jack-knife/k-fold framework in which the training reference data (10% of the original dataset) were spatially partitioned into k subsamples with k = 10. The jack-knife / k-fold allocation was applied spatially to avoid over-optimistic accuracy metrics due to the possible spatial autocorrelation of the training data64. We used a very large dataset for independent validation (i.e., 90% of the original dataset) to avoid a large proportion of the validation dataset being autocorrelated with the training data. Hence, we divided our training dataset (36,944 GEDI clusters) over the whole of Africa into 10 regions with approximately the same number of footprint clusters. A single spatial subsample was kept aside as the validation data for testing the RF model, while the remaining k-1 spatial subsamples were used as training data. The cross-validation process was then repeated k times. Thus, 10 canopy height maps were generated. The mean value of all 10 predictions for each pixel was then used as the final canopy height estimate, and their standard deviation as the prediction error. We also propagated the GEDI footprint height measurement error, the sampling error and the error arising from the temporal difference between GEDI footprints and the satellite imagery to estimate the total error of our canopy height maps (see Supplementary Material).&lt;/p&gt;
    &lt;p&gt;We trained 2 different random forest (RF) models to predict canopy height. The first one (main model) used annual percent tree cover and SAR backscatter data, while the second one only used SAR backscatter data. Our main canopy height model uses L-band PALSAR/PALSAR-2 radar imagery together with the Landsat percent tree cover product. This model was applied for all pixels from the year 2000 to the given year that were undisturbed according to Hansen et al.29. However, for the pixels detected as disturbed (i.e., forest cover loss), a second canopy height model based only on PALSAR/PALSAR-2 was fitted and used for the years after the disturbance, as this is better suited to detecting any recovery (the Landsat Percent Tree Cover product assumes no regrowth after a forest loss event).&lt;/p&gt;
    &lt;p&gt;Conversion of canopy height to aboveground woody biomass density. We developed a single empirical linear model to estimate the square root of aboveground biomass density as a function of canopy height. The model was based on six airborne LiDAR aboveground biomass density maps covering closed-canopy moist tropical forest, mangrove, montane forest, drylands and open savannas (see supplementary material).&lt;/p&gt;
    &lt;p&gt;The square root of AGBD is derived to reduce heteroscedasticity65. Conversion to AGBD requires correction of the inherent negative bias in the square root transformation66 using the ratio estimator proposed by Snowdon et al.67:&lt;/p&gt;
    &lt;p&gt;Boveground biomass density change analysis. For a given pixel, we identified a significant biomass loss between times t1 and t2 if AGBDt1‚ÄìSDt1 &amp;gt; AGBDt2 + SDt2, while there is a significant biomass gain if AGBDt1 + SDt1 &amp;lt; AGBDt2‚ÄìSDt2, where the AGBD values at t1 and t2 are AGBDt1 and AGBDt2 respectively, and their corresponding SD values are SDt1 and SDt2. Changes between consecutive years were considered significant when the error bounds on the estimates of AGBD at the two times did not overlap, otherwise the measured change was considered insignificant. For pixels showing a significant annual gain or loss, we calculated the cumulative significant change over the time series. If this was negative at the end of the period (i.e. 2017), we accepted it as significant biomass loss, and assign this loss value to the year with the highest loss (when significant loss is detected in two or more consecutive pairs of images). Biomass losses usually result from forest cover loss events, but could also arise from other causes, such as natural or anthropogenic fires. A similar approach was taken to biomass gains. However, small gains and losses are hard to detect using this method, and biomass gains were very scarce in forest areas with high aboveground biomass density (i.e. mature forest). Either change is negligible in such areas due to a balance between mortality and growth, or it simply cannot be detected due to insufficient sensitivity of the Earth observation signal when biomass is high.&lt;/p&gt;
    &lt;p&gt;For pixels that were undisturbed (i.e. with positive cumulative significant biomass change or non-significant biomass change), the slope of the temporal linear regression for the given time period was used to represent the average rate of vegetation growth or progressive loss. We calculated the long-term Sen‚Äôs slope68 and only accepted significant slopes (p &amp;lt; 0.05). Additionally, we followed Xu et al.24 in assuming that biomass gain cannot be properly measured once aboveground biomass density exceeds a certain level, as evidenced by the steep increase in the standard deviation of our estimates for high aboveground biomass density (Fig. S5). Thus, for undisturbed pixels with aboveground biomass density above 50 Mg ha-1 we used the IPCC aboveground biomass density growth factors for mature forest69.&lt;/p&gt;
    &lt;p&gt;Once all the significant changes across the time-series were identified, we generated a new aboveground biomass density dataset using the aboveground biomass density map from 2017 as reference (the year used to train the model, equation1), and then rolled only the significant changes back to 2007 to generate a consistent temporal dataset.&lt;/p&gt;
    &lt;p&gt;Biome-level AGB quantification: We used terrestrial biomes as spatial units to calculate regional AGB statistics. Biomes are not strictly defined by the physical land cover types (e.g. forests, shrublands etc.), but rather a complex combination of climate and vegetation conditions70. While biome extents may be dynamic, due to gains and losses in woody vegetation across landscapes, alongside changes in regional climatic conditions, to our knowledge, there is no data product that accounts for land cover dynamics to delineate biomes on an annual basis. We therefore used the most up-to-date static product available, the Ecoregions2017 Resolve biome map14 and assumed that biome extents have not significantly changed over the study period. Quantifying woody aboveground biomass losses / gains is itself an indicator of biomass changes in forests and shrublands.&lt;/p&gt;
    &lt;head rend="h2"&gt;Data availability&lt;/head&gt;
    &lt;p&gt;The datasets used and/or analysed during the current study will be available from the corresponding author on reasonable request.&lt;/p&gt;
    &lt;head rend="h2"&gt;References&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Ciais, P. et al. The carbon balance of Africa: Synthesis of recent research studies. Philos. Trans. R. Soc. A Math. Phys. Eng. Sci. 369, 2038‚Äì2057 (2011).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Williams, C. A. et al. Africa and the global carbon cycle. Carbon Balance Manage. 2, 1‚Äì13 (2007).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Bombelli, A. et al. An outlook on the Sub-Saharan Africa carbon balance. Biogeosciences 6, 2193‚Äì2205 (2009).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Valentini, R. et al. A full greenhouse gases budget of Africa: Synthesis, uncertainties, and vulnerabilities. Biogeosciences 11, 381‚Äì407 (2014).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Saatchi, S. S. et al. Benchmark map of forest carbon stocks in tropical regions across three continents. Proc. Natl. Acad. Sci. 108, 9899‚Äì9904. https://doi.org/10.1073/pnas.1019576108 (2011).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Baccini, A. et al. Estimated carbon dioxide emissions from tropical deforestation improved by carbon-density maps. Nat. Clim. Change 2, 182‚Äì185. https://doi.org/10.1038/nclimate1354 (2012).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Avitabile, V. et al. An integrated pan-tropical biomass map using multiple reference datasets. Glob. Change Biol. 22, 1406‚Äì1420 (2016).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;FAO. Global forest resources assessment 2010. (Food and Agriculture Organization of the United Nations, 2010).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Liu, Y. Y. et al. Recent reversal in loss of global terrestrial biomass. Nat. Clim. Change 5(5), 470‚Äì474. https://doi.org/10.1038/nclimate2581 (2015).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Lewis, S. L. et al. Above-ground biomass and structure of 260 African tropical forests. Philos. Trans. R. Soc. B Biol. Sci. 368, 20120295 (2013).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Ryan, C. M., Williams, M. &amp;amp; Grace, J. Above- and belowground carbon stocks in a miombo woodland landscape of mozambique. Biotropica 43, 423‚Äì432. https://doi.org/10.1111/j.1744-7429.2010.00713.x (2011).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Runfola, D. et al. GeoBoundaries: A global database of political administrative boundaries. PLoS ONE 15(4), e0231866. https://doi.org/10.1371/journal.pone.0231866 (2020).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Environmental Systems Research Institute (ESRI). ArcGIS Desktop, version 10.8.2. https://desktop.arcgis.com/ (2023)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Dinerstein, E. et al. An Ecoregion-based approach to protecting half the terrestrial realm. Bioscience 67, 534‚Äì545. https://doi.org/10.1093/biosci/bix014 (2017).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Araza, A. et al. A comprehensive framework for assessing the accuracy and uncertainty of global above-ground biomass maps. Remote Sens. Environ. 272, 112917 (2022).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Hubau, W. et al. Asynchronous carbon sink saturation in African and Amazonian tropical forests. Nature 579, 80‚Äì87 (2020).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Baccini, A. et al. Tropical forests are a net carbon source based on aboveground measurements of gain and loss. Science 358(6360), 230‚Äì234. https://doi.org/10.1126/science.aam5962 (2017).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Palmer, P. I. et al. Net carbon emissions from African biosphere dominate pan-tropical atmospheric CO2 signal. Nat. Commun. 10, 3344. https://doi.org/10.1038/s41467-019-11097-w (2019).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Ernst, Y. et al. The African Regional Greenhouse Gases Budget (2010‚Äì2019). Global Biogeochem. Cycles 38, e2023GB008016. https://doi.org/10.1029/2023GB008016 (2024).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Neeff, T. et al. Slowly getting there: A review of country experience on estimating emissions and removals from forest degradation. Carbon Balance Manage. 19, 38. https://doi.org/10.1186/s13021-024-00281-1 (2024).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Abdalati, W. et al. The ICESat-2 laser altimetry mission. Proc. IEEE 98, 735‚Äì751 (2010).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Dubayah, R. et al. The global ecosystem dynamics investigation: High-resolution laser ranging of the Earth‚Äôs forests and topography. Sci. Remote Sens. 1, 100002 (2020).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Fan, L. et al. Satellite-observed pantropical carbon dynamics. Nat. Plants 5, 944‚Äì951 (2019).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Xu, L. et al. Changes in global terrestrial live biomass over the 21st century. Sci. Adv. 7, eabe9829. https://doi.org/10.1126/sciadv.abe9829 (2021).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Harris, N. L. et al. Global maps of twenty-first century forest carbon fluxes. Nat. Clim. Chang. 11, 234‚Äì240 (2021).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;McNicol, I. M., Ryan, C. M. &amp;amp; Mitchard, E. T. A. Carbon losses from deforestation and widespread degradation offset by extensive growth in African woodlands. Nat. Commun. 9, 3045. https://doi.org/10.1038/s41467-018-05386-z (2018).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Rodr√≠guez-Veiga, P. et al. Forest biomass retrieval approaches from earth observation in different biomes. Int. J. Appl. Earth Obs. Geoinf. 77, 53‚Äì68 (2019).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Bond, W. J. &amp;amp; Midgley, G. F. Carbon dioxide and the uneasy interactions of trees and savannah grasses. Philos. Trans. R. Soc. B Biol. Sci. 367, 601‚Äì612 (2012).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Hansen, M. C. et al. High-resolution global maps of 21st-century forest cover change. Science 342, 850‚Äì853. https://doi.org/10.1126/science.1244693 (2013).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Bouvet, A. et al. An above-ground biomass map of African savannahs and woodlands at 25 m resolution derived from ALOS PALSAR. Remote Sens. Environ. 206, 156‚Äì173. https://doi.org/10.1016/j.rse.2017.12.030 (2018).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Santoro, M. et al. The global forest above-ground biomass pool for 2010 estimated from high-resolution satellite observations. Earth Syst. Sci. Data 13, 3927‚Äì3950. https://doi.org/10.5194/essd-13-3927-2021 (2010).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Santoro, M. &amp;amp; Cartus, O. ESA Biomass climate change initiative (Biomass_cci): Global datasets of forest above-ground biomass for the years 2010, 2017, 2018, 2019 and 2020, v4. V4 ed. (NERC EDS Centre for Environmental Data Analysis; 2023).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Lund, H. G. Rev* Definitions of Forest, Deforestation, Afforestation, and Reforestation [Online]. Forest Information Services 2012 [cited 2012, 15‚Äì05‚Äì2012] Misc. pagination. Note, this paper has been continuously updated since 1998. Available from: http://home.comcast.net/~gyde/DEFpaper.htm&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Lund, H. G. When is a forest not a forest?. J. Forest. 100, 21‚Äì28 (2002).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Knoke, T. et al. Trends in tropical forest loss and the social value of emission reductions. Nat. Sustain. 6, 1373‚Äì1384. https://doi.org/10.1038/s41893-023-01175-9 (2023).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;FAO. Global forest resources assessment 2020‚Äìkey findings, Policy research paper. (FAO, Rome, 2020).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Tyukavina, A. et al. Congo Basin forest loss dominated by increasing smallholder clearing. Sci. Adv. 4(11), eaat2993. https://doi.org/10.1126/sciadv.aat2993 (2018).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;CIFOR-ICRAF, Eba‚Äôa Atyi, R., Hiol Hiol, F., Lescuyer, G., Mayaux, P., Defourny, P., Bayol, N., Saracco, F., Pokem, D., Sufo Kankeu, R. &amp;amp; Nasi, R. (2022). The forests of the Congo basin: State of the forests 2021. Bogor, Indonesia: CIFOR-ICRAF. https://doi.org/10.17528/cifor/008700&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Gatti, L. V. et al. Amazonia as a carbon source linked to deforestation and climate change. Nature 595, 388‚Äì393. https://doi.org/10.1038/s41586-021-03629-6 (2021).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Feng, Y. et al. Doubling of annual forest carbon loss over the tropics during the early twenty-first century. Nat. Sustain. 5, 444‚Äì451. https://doi.org/10.1038/s41893-022-00854-3 (2022).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Gasser, T., Ciais, P. &amp;amp; Lewis, S. L. How the Glasgow Declaration on Forests can help keep alive the 1.5 C target. Proc. Nat. Acad. Sci. 119, e2200519119 (2022).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Bamwesigye, D., Doli, A. &amp;amp; Hlavackova, P. REDD+: An analysis of initiatives in East Africa amidst increasing deforestation. Eur. J. Sustain. Dev. 9, 224‚Äì224 (2020).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Fischer, R., Giessen, L. &amp;amp; G√ºnter, S. Governance effects on deforestation in the tropics: A review of the evidence. Environ. Sci. Policy 105, 84‚Äì101 (2020).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Henry, M., Maniatis, D., Gitz, V., Huberman, D. &amp;amp; Valentini, R. Implementation of REDD+ in sub-Saharan Africa: State of knowledge, challenges and opportunities. Environ. Dev. Econ. 16, 381‚Äì404 (2011).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Roberts, J. et al. Pyeo: A python package for near-real-time forest cover change detection from Earth observation using machine learning. Comput. Geosci. 167, 105192 (2022).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;AUDA-NEPAD. AFR100‚ÄîThe African Forest Landscape Restoration Initiative. African Union Development Agency, https://afr100.org/ (2023).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Restor. Restor global network movement. https://restor.eco/ (2024).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Schutz, B. E., Zwally, H. J., Shuman, C. A., Hancock, D. &amp;amp; DiMarzio, J. P. Overview of the ICESat mission. Geophys. Res. Lett. 32, L21S01. https://doi.org/10.1029/2005gl024009 (2005).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Lefsky, M. A. et al. Estimates of forest canopy height and aboveground biomass using ICESat. Geophys. Res. Lett. 32, L22S02 (2005).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Dubayah, R., Tang, H., Armston, J., Luthcke, S., Hofton, M. &amp;amp; Blair, J. GEDI L2B canopy cover and vertical profile metrics data global footprint level V001. NASA EOSDIS land processes DAAC. Accessed 2020-04-11 from https://lpdaac.usgs.gov/products/gedi02_bv001/. (2020).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Labriere, N. et al. In situ reference datasets from the TropiSAR and AfriSAR campaigns in support of upcoming spaceborne biomass missions. IEEE J. Select. Top. Appl. Earth Observ. Remote Sens. 11, 3617‚Äì3627 (2018).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Saatchi, S. S., Chave, J., Labriere, N., Barbier, N., R√âJou-M√âChain, M., Ferraz, A. &amp;amp; Tao, S. AfriSAR: Aboveground Biomass for Lope, Mabounie, Mondah, and Rabi Sites, Gabon. ORNL DAAC. (ORNL Distributed Active Archive Center, 2019).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Adhikari, H. et al. Determinants of aboveground biomass across an Afromontane landscape mosaic in Kenya. Remote Sens. 9, 827 (2017).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Amara, E. et al. Aboveground biomass distribution in a multi-use savannah landscape in Southeastern Kenya: Impact of land use and fences. Land 9, 381 (2020).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Rodriguez-Veiga, P. et al. Forest biomass retrieval approaches from earth observation in different biomes. Int. J. Appl. Earth Obs. Geoinf. 77, 53‚Äì68 (2019).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Shimada, M. &amp;amp; Ohtaki, T. Generating large-scale high-quality SAR mosaic datasets: Application to PALSAR data for global monitoring. IEEE J. Select. Top. Appl. Earth Observ. Remote Sens. 3, 637‚Äì656 (2010).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Shimada, M. et al. New global forest/non-forest maps from ALOS PALSAR data (2007‚Äì2010). Remote Sens. Environ. 155, 13‚Äì31. https://doi.org/10.1016/j.rse.2014.04.014 (2014).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Marshak, C., Simard, M. &amp;amp; Denbina, M. Monitoring forest loss in ALOS/PALSAR time-series with superpixels. Remote Sens. 11, 556 (2019).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Mitchard, E. T. A. et al. Mapping tropical forest biomass with radar and spaceborne LiDAR in Lop√© National Park, Gabon: Overcoming problems of high biomass and persistent cloud. Biogeosciences 9, 179‚Äì191. https://doi.org/10.5194/bg-9-179-2012 (2012).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Buchhorn, M., Bertels, L., Smets, B., De Roo, B., Lesiv, M., Tsendbazar, N., Masiliunas, D. &amp;amp; Li, L. Copernicus global land service: Land cover 100 m: Version 3 globe 2015-2019: Product user manual. (Zenodo, Geneve, Switzerland, 2020).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Breiman, L. Random forests. Mach. Learn. 45, 5‚Äì32 (2001).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Rodr√≠guez-Veiga, P. et al. Carbon stocks and fluxes in Kenyan forests and wooded grasslands derived from earth observation and model-data fusion. Remote Sens. 12, 2380 (2020).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Bispo, P. D. C. et al. Woody aboveground biomass mapping of the Brazilian Savanna with a multi-sensor and machine learning approach. Remote Sens. 12, 2685 (2020).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Ploton, P. et al. Spatial validation reveals poor predictive performance of large-scale ecological mapping models. Nat. Commun. 11, 1‚Äì11 (2020).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Nilsson, M. et al. A nationwide forest attribute map of Sweden predicted using airborne laser scanning data and field data from the National Forest Inventory. Remote Sens. Environ. 194, 447‚Äì454. https://doi.org/10.1016/j.rse.2016.10.022 (2017).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Baskerville, G. Use of logarithmic regression in the estimation of plant biomass. Can. J. For. Res. 2, 49‚Äì53 (1972).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Snowdon, P. A ratio estimator for bias correction in logarithmic regressions. Can. J. For. Res. 21, 720‚Äì724. https://doi.org/10.1139/x91-101 (1991).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Sen, P. K. Estimates of the regression coefficient based on Kendall‚Äôs tau. J. Am. Stat. Assoc. 63, 1379‚Äì1389 (1968).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;IPCC. 2019 Refinement to the 2006 IPCC Guidelines for National Greenhouse Gas Inventories. (IPCC, Switzerland, 2019).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Olson, D. M. et al. Terrestrial ecoregions of the world: A New Map of Life on Earth A new global map of terrestrial ecoregions provides an innovative tool for conserving biodiversity. Bioscience 51(11), 933‚Äì938. https://doi.org/10.1641/0006-3568(2001)051[0933:TEOTWA]2.0.CO;2 (2001).&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Acknowledgements&lt;/head&gt;
    &lt;p&gt;This research was supported by the Natural Environment Research Council (NERC) through the National Centre for Earth Observation and the European Space Agency‚Äôs Climate Change Initiative Plus on Biomass (Biomass CCI+ contract ESA-EOP-SC-AMT-2021-26). The research was also supported by the Short-term International Fellowship awarded by the Japan Society for Promotion of Science (JSPS) to P.R.V and N.T (PE18044). H.B and N.A. were financially supported by the Natural Environment Research Council (NERC, NE/Y006216/1). The authors would like to thank the following organizations for making the data freely available: JAXA, NASA, USGS, GEDI mission, ORNL DAAC, Google, and University of Maryland. P.P., J.H. and H.A. acknowledge Ministry for Foreign AÔ¨Äairs of Finland, Academy of Finland (Decision number 318645) and Faculty of Science, University of Helsinki for funding of Lidar data acquisitions in Kenya. C.M.R, J.C, S.Q, and M.W were supported by the NERC-funded SECO project (NE/T01279X/1); plot data collection by T.B, C.M.R and data analysis by C.J.N was supported by SEOSAW, the Socio-ecological observatory for studying African woodlands (NE/P008755/1; NE/T004258/1).&lt;/p&gt;
    &lt;head rend="h2"&gt;Author information&lt;/head&gt;
    &lt;head rend="h3"&gt;Authors and Affiliations&lt;/head&gt;
    &lt;head rend="h3"&gt;Contributions&lt;/head&gt;
    &lt;p&gt;P.R.V. and H.B. conceived the experiments, P.R.V. and A.A. conducted the experiments and data analyses, P.R.V., J.C., S.Q., A.A. and N.T. contributed to the methodological design. J.C., J.H., P.P., H.A., C.N., and A.A contributed with datasets used in the analysis. All authors contributed to the interpretation of the analysis results and reviewed the manuscript.&lt;/p&gt;
    &lt;head rend="h3"&gt;Corresponding authors&lt;/head&gt;
    &lt;head rend="h2"&gt;Ethics declarations&lt;/head&gt;
    &lt;head rend="h3"&gt;Competing interests&lt;/head&gt;
    &lt;p&gt;The authors declare no competing interests.&lt;/p&gt;
    &lt;head rend="h2"&gt;Additional information&lt;/head&gt;
    &lt;head rend="h3"&gt;Publisher‚Äôs note&lt;/head&gt;
    &lt;p&gt;Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.&lt;/p&gt;
    &lt;head rend="h2"&gt;Supplementary Information&lt;/head&gt;
    &lt;p&gt;Below is the link to the electronic supplementary material.&lt;/p&gt;
    &lt;head rend="h2"&gt;Rights and permissions&lt;/head&gt;
    &lt;p&gt;Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article‚Äôs Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article‚Äôs Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.&lt;/p&gt;
    &lt;head rend="h2"&gt;About this article&lt;/head&gt;
    &lt;head rend="h3"&gt;Cite this article&lt;/head&gt;
    &lt;p&gt;Rodr√≠guez-Veiga, P., Carreiras, J.M.B., Quegan, S. et al. Loss of tropical moist broadleaf forest has turned Africa‚Äôs forests from a carbon sink into a source. Sci Rep 15, 41744 (2025). https://doi.org/10.1038/s41598-025-27462-3&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Received:&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Accepted:&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Published:&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Version of record:&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;DOI: https://doi.org/10.1038/s41598-025-27462-3&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46414443</guid><pubDate>Sun, 28 Dec 2025 20:59:21 +0000</pubDate></item><item><title>MongoBleed Explained Simply</title><link>https://bigdata.2minutestreaming.com/p/mongobleed-explained-simply</link><description>&lt;doc fingerprint="b899ddd9ce0a239a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;MongoBleed explained simply&lt;/head&gt;
    &lt;head rend="h3"&gt;CVE-2025-14847 allows attackers to read any arbitrary data from the database's heap memory. It affects all MongoDB versions since 2017, here's how it works:&lt;/head&gt;
    &lt;p&gt;MongoBleed, officially CVE-2025-14847, is a recently-uncovered extremely sensitive vulnerability affecting basically all versions of MongoDB since ~2017.&lt;/p&gt;
    &lt;p&gt;It is a bug in the zlib1 message compression path in MongoDB.&lt;/p&gt;
    &lt;p&gt;It allows an attacker to read off any uninitialized heap memory, meaning anything that was allocated to memory from a previous database operation could be read.&lt;/p&gt;
    &lt;p&gt;The bug was introduced in 20172. It is dead-easy to exploit - it only requires connectivity to the database (no auth needed). It is fixed as of writing, but some EOL versions (3.6, 4.0, 4.2) will not get it.&lt;/p&gt;
    &lt;head rend="h1"&gt;MongoDB Basics&lt;/head&gt;
    &lt;p&gt;Let‚Äôs get a few basics out of the way before we explain the bug:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;MongoDB uses its own TCP wire protocol instead of e.g HTTP. This is standard for databases, especially ones chasing high performance.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Mongo uses the BSON format for messages3. It‚Äôs basically binary json but with some key optimizations. We will talk about one later because it is essential to the exploit.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Mongo doesn‚Äôt have endpoints or RPCs. It only uses a single op code called OP_MSG.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The OP_MSG command contains a BSON message. The contents of the message denote what type of request it is. Concretely, it‚Äôs the first field of the message that marks the request type. 4&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The request can be compressed. In that case, an OP_COMPRESSED message is sent which wraps the now-compressed OP_MSG BSON.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The request then looks like this:&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;     OP_COMPRESSED message
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ standard header (16 bytes) ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ originalOpcode (int32)     ‚îÇ
‚îÇ uncompressedSize (int32)   ‚îÇ
‚îÇ compressorId (int8)        ‚îÇ
‚îÇ compressed OP payload      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Critically, the&lt;/p&gt;&lt;code&gt;uncompressedSize&lt;/code&gt;field denotes how large the payload is once it‚Äôs uncompressed.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;Exploit Part 1&lt;/head&gt;
    &lt;p&gt;The first part of the exploit is to get the server to wrongfully think that an overly-large OP_MSG is coming.&lt;/p&gt;
    &lt;p&gt;An attacker can send a falsefully large &lt;code&gt;`uncompressedSize`&lt;/code&gt; field, say 1MB5, when in reality the underlying message is 1KB uncompressed. &lt;/p&gt;
    &lt;p&gt;This will make the server allocate a 1MB buffer in memory to decompress the message into. This is fine.&lt;/p&gt;
    &lt;p&gt;The critical bug here is that, once finished decompressing, the server does NOT check the actual resulting size of the newly-uncompressed payload.&lt;/p&gt;
    &lt;p&gt;Instead, it trusts the user‚Äôs input and uses that as the canonical size of the payload, even if it got a different number.6&lt;/p&gt;
    &lt;p&gt;The result is an in-memory representation of the BSON message which looks something like this:&lt;/p&gt;
    &lt;code&gt;[ 1KB of REAL DATA |      999KB of UNREFERENCED HEAP GARBAGE       ]
                   ‚Üë                                               ‚Üë
        actual length (1KB)                     user input length (1MB)&lt;/code&gt;
    &lt;head rend="h3"&gt;Unreferenced Heap Garbage&lt;/head&gt;
    &lt;p&gt;Like in every programming language, when a variables in the code goes out of scope, the runtime marks the memory it previously took up as available.&lt;/p&gt;
    &lt;p&gt;In most modern languages, the memory gets zeroed out. In other words, the old bytes that used to take up the space get deleted.&lt;/p&gt;
    &lt;p&gt;In C/C++, this doesn‚Äôt happen. When you allocate memory via &lt;code&gt;`malloc()`&lt;/code&gt;, you get whatever was previously there.&lt;/p&gt;
    &lt;p&gt;Since Mongo is writen in C++, that unreferenced heap garbage part can represent anything that was in memory from previous operations, including:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Cleartext passwords and credentials&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Session tokens / API keys&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Customer data and PII&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Database configs and system info&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Docker paths and client IP addresses&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;[ REAL BSON DATA | password: 123 | apiKey: jA2sa | ip: 219.117.127.202 ]&lt;/code&gt;
    &lt;head rend="h1"&gt;Exploit Part 2&lt;/head&gt;
    &lt;p&gt;Now that the server has wrongfully allocated some potentially-sensitive data to the input message, the only thing left for the attacker is to somehow get the server return the data.&lt;/p&gt;
    &lt;head rend="h3"&gt;BSON&lt;/head&gt;
    &lt;p&gt;As mentioned, BSON is Mongo‚Äôs way of serializing JSON. As mentioned on its site, it was designed with efficiency in mind:&lt;/p&gt;
    &lt;quote&gt;
      &lt;head&gt;3. Efficient&lt;/head&gt;
      &lt;p&gt;Encoding data to BSON and decoding from BSON can be performed very quickly in most languages due to the use of C data types.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;C Strings&lt;/head&gt;
    &lt;p&gt;C famously uses null-terminated strings7. A null-terminated string means that a null byte is used to mark the end of the string:&lt;/p&gt;
    &lt;code&gt;char* s = "hello"
// in memory, this is represented as an array of characters with the last element being the null terminator: h e l l o \0&lt;/code&gt;
    &lt;p&gt;The way such strings get parsed is very simple - the deserializer reads every character until it finds a null terminator.&lt;/p&gt;
    &lt;head rend="h3"&gt;Malicious BSON Input&lt;/head&gt;
    &lt;p&gt;If you recall, I said earlier that the first field of the BSON message denotes what type of ‚ÄúRPC‚Äù the command is.&lt;/p&gt;
    &lt;p&gt;As such, the first thing a server does when handling an incoming message over the wire is‚Ä¶ parse the first field!&lt;/p&gt;
    &lt;p&gt;Because fields are strings, and strings are null-terminated CStrings, the deserializing logic in the MongoDB server parses the field until the first null terminator found.&lt;/p&gt;
    &lt;p&gt;An attacker can send a compressed, invalid BSON object that does NOT contain a null terminator. This forces the server to continue scanning through foreign data in the wrongly-allocated memory buffer until it finds the first null terminator (\0)&lt;/p&gt;
    &lt;code&gt;# Conceptual
[ REAL DATA |             UNREFERENCED HEAP GARBAGE                 ]
# Practical Example
[ { "a      | password: 123\0 | apiKey: jA2sa | ip: 219.117.127.202 ]&lt;/code&gt;
    &lt;p&gt;As the first null terminator is right after the password, the server would now think that the first field of the BSON is:&lt;/p&gt;
    &lt;code&gt;"a      | password: 123"&lt;/code&gt;
    &lt;p&gt;Obviously that is an invalid BSON field, so the server responds with an error to the client. In order to be helpful, the response contains an error message that shows which field was invalid:&lt;/p&gt;
    &lt;code&gt;{
  "ok": 0,
  "errmsg": "invalid BSON field name 'a      | password: 123'",
  "code": 2,
  "codeName": "BadValue"
}&lt;/code&gt;
    &lt;p&gt;Boom. The attacker successfully got the server to leak data to it.&lt;/p&gt;
    &lt;p&gt;Any serious attacker would then run this over and over again, thousands of time a second, until they believe they‚Äôve scanned the majority of the database‚Äôs heap. They can then repeat this ad infinitum.&lt;/p&gt;
    &lt;head rend="h1"&gt;üí• Impact&lt;/head&gt;
    &lt;head rend="h3"&gt;1. Ease of Exploitation - ‚ÄúPre-Auth‚Äù&lt;/head&gt;
    &lt;p&gt;The impact of this is particularly nasty, because the request-response parsing cycle happens before any authentication can be made. This makes sense, since you cannot begin to authenticate a request you still haven‚Äôt deserialized.&lt;/p&gt;
    &lt;p&gt;This allows any attacker to gain access to any piece of potentially-sensitive data. The only thing they need is internet access to the database.&lt;/p&gt;
    &lt;p&gt;Exposing your database to the internet is a practice that‚Äôs heavily frowned upon8. At the same time, Shodan shows that there are over 213,000 publicly-accessible Mongo databases.&lt;/p&gt;
    &lt;head rend="h3"&gt;2. Eight Years of Vulnerability (handled badly)&lt;/head&gt;
    &lt;p&gt;The PR that introduced the bug was from May 2017. This means that, roughly from version 3.6.0, any publicly-accessible MongoDB instance has been vulnerable to this.&lt;/p&gt;
    &lt;p&gt;It is unknown whether the exploit was known and exploited by actors prior to its disclosure. Given the simplicity of it, I bet it was.&lt;/p&gt;
    &lt;p&gt;As of the exploit‚Äôs disclosure, which happened on 19th of December, it has been a race to patch the database.&lt;/p&gt;
    &lt;p&gt;Sifting through Git history, it seems like the fix was initially committed on the 17th of December. Interestingly enough, it was only merged a full 5 days after - on the 22nd of December (1-line fix btw).&lt;/p&gt;
    &lt;p&gt;That beig said, MongoDB 8.0.17 containing the fix was released on Dec 19, consistent with the CVE publish data. But JIRA activity shows that patches went out on the 22nd of December.&lt;/p&gt;
    &lt;p&gt;Because there‚Äôs no official timeline posted, members of the community like me have to guess. As of writing, 10 days later in Dec 28, 2025, Mongo have still NOT properly addressed the issue publicly.&lt;/p&gt;
    &lt;p&gt;They only issued a community disclosure of the CVE a full five days after the publication of it. It is then, on the 24th of December, that they announced that all of their database instances in their cloud service Atlas were fully patched.&lt;/p&gt;
    &lt;p&gt;I believe this implies that all Atlas databases exposed to the internet were vulnerable to this issue for almost a week. By default, Atlas databases use an IP allowlist for connectivity. But users could configure it to allow connections from anywhere.&lt;/p&gt;
    &lt;p&gt;Mongo says that they haven‚Äôt verified exploitation so far:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;‚Äúat this time, we have no evidence that this issue has been exploited or that any customer data has been compromised‚Äù&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;3. Ease of Mitigation&lt;/head&gt;
    &lt;p&gt;Mitigation is admittedly very easy, you have one of two choices:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Update to the newest patch&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Disable zlib network compression&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I found the latter wasn‚Äôt circulated a lot in online talk, but I understand is just as good as a short-term mitigation.&lt;/p&gt;
    &lt;head rend="h1"&gt;A bit of Drama?&lt;/head&gt;
    &lt;p&gt;The tech lead for Security at Elastic coined the name MongoBleed by posting a Python script that acts as a proof of concept to exploiting the vulnerability: https://github.com/joe-desimone/mongobleed&lt;/p&gt;
    &lt;p&gt;This is particularly interesting, because despite being different systems, Mongo competes with Elastic on Vector Search, Text Search and Analytical use cases.&lt;/p&gt;
    &lt;head rend="h1"&gt;Summary&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;The exploit allows attackers to read arbitrary heap data, including user data, plaintext passwords, api keys/secrets, and more.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;It is performed by leveraging a simple, malformed zlib-compressed request.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;MongoDB versions from 2017-2025 are vulnerable to this exploit.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Rough timeline:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;June 1, 2017: Commit introducing the bug gets merged.&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Dec 17, 2025: Code for the fix is written (original commit date).&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Dec 19, 2025: CVE officially published.&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Dec 22, 2025: Code with the fix is merged.&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Dec 24, 2025: MongoDB announce the patch, say all Atlas databases are patched.&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;On Dec 24th, MongoDB reported they have no evidence of anybody exploiting the CVE. Given the fact this exploit lived on for ~8 years, and their honey-pot cloud service Atlas took a full 5 days to patch since the official CVE publish date‚Ä¶ I find that hard to believe.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;MongoDB have not apologized yet.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;There are over 213k+ potentially vulnerable internet-exposed MongoDB instances, ensuring that this exploit is web scale:&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;Interesting Links&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Official CVE: https://nvd.nist.gov/vuln/detail/CVE-2025-14847 (Dec 19, 2025)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;PR introducing the bug: https://github.com/mongodb/mongo/pull/1152 (May 2017)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Commit fixing the issue: https://github.com/mongodb/mongo/commit/505b660a14698bd2b5233bd94da3917b585c5728#diff-e5f6e2daef81ce1c3c4e9f7d992bd6ff9946b3b4d98a601e4d9573e5ef0cb07dR77&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Security Report on the incident, including fix versions: https://www.ox.security/blog/attackers-could-exploit-zlib-to-exfiltrate-data-cve-2025-14847/&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Write-up on how to detect exploitation attempts via log analysis: https://blog.ecapuano.com/p/hunting-mongobleed-cve-2025-14847&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Somebody also vibe-coded a detector: https://github.com/Neo23x0/mongobleed-detector&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Other Reads You May Like:&lt;/head&gt;
    &lt;p&gt;zlib is a library for compression. It uses the DEFLATE algorithm under the hood, but produces results in a specific wire format to ease sending such data over the wire. (e.g includes metadata like flags, checksums, etc)&lt;/p&gt;
    &lt;p&gt;Here is the PR that introduced it. I‚Äôm not aware of Mongo‚Äôs public review practices, but it appears as if nobody explicitly reviewed the change.&lt;/p&gt;
    &lt;p&gt;They actually created it. There‚Äôs a very good site for it - https://bsonspec.org/&lt;/p&gt;
    &lt;p&gt;Weird, I know. Here are examples of different commands, just so you get a sense:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Insert a document into the users table&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;{
  "insert": "users",
  "documents": [{ "name": "alice", "age": 30 }]
}&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Delete users with&lt;/p&gt;
        &lt;code&gt;inactive=true&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;{
  "delete": "users",
  "deletes": [ { "q": { "inactive": true }, "limit": 0 }]
}&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Check the server‚Äôs status&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;{ "serverStatus": 1 }&lt;/code&gt;
    &lt;p&gt;I‚Äôm making this number up. There is probably some limit on the server side as to how large a request can be - perhaps 1MB is too large.&lt;/p&gt;
    &lt;p&gt;Here is the line (pre-fix): https://github.com/mongodb/mongo/blame/b2f3ca9c996ba409e7d48601fca16c28fd58b774/src/mongo/transport/message_compressor_zlib.cpp#L83&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;`output` &lt;/code&gt;is the large memory buffer that was allocated earlier&lt;/p&gt;
    &lt;p&gt;The code, instead, ought to return the referenced &lt;code&gt;`length`&lt;/code&gt; field, as that gets updated with the actual length that was seen post-compression.&lt;/p&gt;
    &lt;p&gt;This has been the cause of many security issues in the past.&lt;/p&gt;
    &lt;p&gt;The most common comment I saw online is that you ‚Äúdeserved it‚Äù if you exposed your DB to the wild. üòÅ&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46414475</guid><pubDate>Sun, 28 Dec 2025 21:03:03 +0000</pubDate></item><item><title>Software engineers should be a little bit cynical</title><link>https://www.seangoedecke.com/a-little-bit-cynical/</link><description>&lt;doc fingerprint="2c3f995c3e160190"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Software engineers should be a little bit cynical&lt;/head&gt;
    &lt;p&gt;A lot of my readers call me a cynic when I say things like ‚Äúyou should do things that make your manager happy‚Äù or ‚Äúbig tech companies get to decide what projects you work on‚Äù. Alex Wennerberg put the ‚ÄúSean Goedecke is a cynic‚Äù case well in his post Software Engineers Are Not Politicians. Here are some excerpts:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;I have no doubt that [Sean‚Äôs] advice is quite effective for navigating the upper levels of an organization dedicated to producing a large, mature software product. But what is lost is any sort of conception of value. Is it too naive to say that engineers are more than ‚Äútools in a political game‚Äù, they are specialized professionals whose role is to apply their expertise towards solving meaningful problems?&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;The irony is that this kind of thinking destroys a company‚Äôs ability to actually make money ‚Ä¶ the idea that engineers should begin with a self-conception of doing what their manager tells them to is, to me, very bleak. It may be a good way to operate smoothly within a bureaucratic organization, and of course, one must often make compromises and take direction, but it is a bad way to do good work.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I can see why people would think this way. But I love working in big tech companies! I do see myself as a professional solving meaningful problems. And I think navigating the organization to put real features or improvements in the hands of users is an excellent way - maybe the best way - to do good work.&lt;/p&gt;
    &lt;p&gt;Why do I write such cynical posts, then? Well, I think that a small amount of cynicism is necessary in order to think clearly about how organizations work, and to avoid falling into the trap of being overly cynical. In general, I think good engineers ought to be a little bit cynical.&lt;/p&gt;
    &lt;head rend="h3"&gt;The idealist view is more cynical than idealists think&lt;/head&gt;
    &lt;p&gt;One doctrinaire ‚Äúidealist‚Äù view of software engineering goes something like this. I‚Äôm obviously expressing it in its most lurid form, but I do think many people believe this more or less literally:1&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;We live in a late-stage-capitalist hellscape, where large companies are run by aspiring robber barons who have no serious convictions beyond desiring power. All those companies want is for obedient engineering drones to churn out bad code fast, so they can goose the (largely fictional) stock price. Meanwhile, end-users are left holding the bag: paying more for worse software, being hassled by advertisements, and dealing with bugs that are unprofitable to fix. The only thing an ethical software engineer can do is to try and find some temporary niche where they can defy their bosses and do real, good engineering work, or to retire to a hobby farm and write elegant open-source software in their free time.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;When you write it all out, I think it‚Äôs clear to see that this is incredibly cynical. At the very least, it‚Äôs a cynical way to view your coworkers and bosses, who are largely people like you: doing a job, balancing a desire to do good work with the need to please their own bosses. It‚Äôs a cynical way to view the C-staff of a company. I think it‚Äôs also inaccurate: from my limited experience, the people who run large tech companies really do want to deliver good software to users.&lt;/p&gt;
    &lt;p&gt;It‚Äôs idealistic only in the sense that it does not accept the need for individual software engineers to compromise. According to this view, you never need to write bad software. No matter how hard the company tells you to compromise and just get something out, you‚Äôre morally required to plant your feet and tell them to go to hell. In fact, by doing so, you‚Äôre taking a stand against the general degeneration of the modern software world. You‚Äôre protecting - unsung, like Batman - the needs of the end-user who will never know you exist.&lt;/p&gt;
    &lt;p&gt;I can certainly see the appeal of this view! But I don‚Äôt think it‚Äôs an idealistic appeal. It comes from seeing the world as fundamentally corrupted and selfish, and believing that real positive change is impossible. In other words, I think it‚Äôs a cynical appeal.&lt;/p&gt;
    &lt;head rend="h3"&gt;The cynical view is more idealistic than idealists think&lt;/head&gt;
    &lt;p&gt;I don‚Äôt see a hard distinction between engineers being ‚Äútools in a political game‚Äù and professionals who solve meaningful problems. In fact, I think that in practice almost all meaningful problems are solved by playing political games.&lt;/p&gt;
    &lt;p&gt;There are very few problems that you can solve entirely on your own. Software engineers encounter more of these problems than average, because the nature of software means that a single engineer can have huge leverage by sitting down and making a single code change. But in order to make changes to large products - for instance, to make it possible for GitHub‚Äôs 150M users to use LaTeX in markdown - you need to coordinate with many other people at the company, which means you need to be involved in politics.&lt;/p&gt;
    &lt;p&gt;It is just a plain fact that software engineers are not the movers and shakers in large tech organizations. They do not set the direction of the company. To the extent that they have political influence, it‚Äôs in how they translate the direction of the company into specific technical changes. But that is actually quite a lot of influence!&lt;/p&gt;
    &lt;p&gt;Large tech companies serve hundreds of millions (or billions) of users. Small changes to these products can have a massive positive or negative effect in the aggregate. As I see it, choosing to engage in the messy, political process of making these changes - instead of washing your hands of it as somehow impure - is an act of idealism.&lt;/p&gt;
    &lt;p&gt;I think the position of a software engineer in a large tech company is similar to people who go into public service: idealistically hoping that they can do some good, despite knowing that they themselves will never set the broad strokes of government policy.&lt;/p&gt;
    &lt;p&gt;Of course, big-tech software engineers are paid far better, so many people who go into this kind of work in fact are purely financially-motivated cynics. But I‚Äôm not one of them! I think it‚Äôs possible, by doing good work, to help steer the giant edifice of a large tech company for the better.&lt;/p&gt;
    &lt;head rend="h3"&gt;Cynicism as inoculation&lt;/head&gt;
    &lt;p&gt;Cynical writing is like most medicines: the dose makes the poison. A healthy amount of cynicism can serve as an inoculation from being overly cynical.&lt;/p&gt;
    &lt;p&gt;If you don‚Äôt have an slightly cynical explanation for why engineers write bad code in large tech companies - such as the one I write about here - you risk adopting an overly cynical one. For instance, you might think that big tech engineers are being deliberately demoralized as part of an anti-labor strategy to prevent them from unionizing, which is nuts. Tech companies are simply not set up to engage in these kind of conspiracies.&lt;/p&gt;
    &lt;p&gt;If you don‚Äôt have a slightly cynical explanation for why large tech companies sometimes make inefficient decisions - such as this one - you risk adopting an overly cynical one. For instance, you might think that tech companies are full of incompetent losers, which is simply not true. Tech companies have a normal mix of strong and weak engineers.&lt;/p&gt;
    &lt;head rend="h3"&gt;Final thoughts&lt;/head&gt;
    &lt;p&gt;Idealist writing is massively over-represented in writing about software engineering. There is no shortage of books or blog posts (correctly) explaining that we ought to value good code, that we ought to be kind to our colleagues, that we ought to work on projects with positive real-world impact, and so on. There is a shortage of writing that accurately describes how big tech companies operate.&lt;/p&gt;
    &lt;p&gt;Of course, cynical writing can harm people: by making them sad, or turning them into bitter cynics. But idealist writing can harm people too. There‚Äôs a whole generation of software engineers who came out of the 2010s with a factually incorrect model of how big tech companies work, and who are effectively being fed into the woodchipper in the 2020s. They would be better off if they internalized a correct model of how these companies work: not just less likely to get into trouble, but better at achieving their own idealist goals2.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;I don‚Äôt think I‚Äôm strawmanning here - I‚Äôve seen many people make all of these points in the past, and I suspect at least some readers will be genuinely nodding along to the following paragraph. If you‚Äôre one of those readers (or if you only agree with about 50%), consider doing me a favor and emailing me to let me know! If I don‚Äôt get any emails I will probably rewrite this.&lt;/p&gt;‚Ü©&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;For some concrete details on this, see my post How I influence tech company politics as a staff software engineer. Also, if you‚Äôre interested, I wrote a much less well-developed version of this post right at the start of 2024, called Is it cynical to do what your manager wants?.&lt;/p&gt;‚Ü©&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you liked this post, consider subscribing to email updates about my new posts, or sharing it on Hacker News. Here's a preview of a related post that shares tags with this one.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;You can't design software you don't work on&lt;/p&gt;&lt;p&gt;Only the engineers who work on a large software system can meaningfully participate in the design process. That‚Äôs because you cannot do good software design without an intimate understanding of the concrete details of the system. In other words, generic software design advice is typically useless for most practical software design problems.&lt;/p&gt;&lt;p&gt;What is generic software design? It‚Äôs ‚Äúdesigning to the problem‚Äù: the kind of advice you give when you have a reasonable understanding of the domain, but very little knowledge of the existing codebase. Unfortunately, this is the only kind of advice you‚Äôll read in software books and blog posts. Engineers love giving generic software design advice for the same reason that all technical professionals love ‚Äútalking shop‚Äù. However, you should be very careful about applying generic advice to your concrete day-to-day work problems.&lt;/p&gt;&lt;lb/&gt;Continue reading...&lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46414723</guid><pubDate>Sun, 28 Dec 2025 21:29:32 +0000</pubDate></item><item><title>Unity's Mono problem: Why your C# code runs slower than it should</title><link>https://marekfiser.com/blog/mono-vs-dot-net-in-unity/</link><description>&lt;doc fingerprint="f319d3cb995267ca"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Unity's Mono problem: Why your C# code runs slower than it should&lt;/head&gt;
    &lt;p&gt;Execution of C# code in Unity√¢s Mono runtime is slow by today√¢s standards, much slower than you might expect! Our game runs 2-3x faster on modern .NET compared to Unity√¢s Mono, and in a few small benchmarks I measured speedups of up to 15x. I√¢ve spent some time investigating what√¢s going on and in this article I will present my findings and why everyone should want Unity√¢s .NET modernization to become production-ready as soon as possible.&lt;/p&gt;
    &lt;head rend="h2"&gt;How did we get here&lt;/head&gt;
    &lt;p&gt;Unity uses the Mono framework to run C# programs and back in 2006 it was one of the only viable multi-platform implementations of .NET. Mono is also open-source, allowing Unity to do some tweaks to better suit game development.&lt;/p&gt;
    &lt;p&gt;An interesting twist happened nearly 10 years later. In 2014, Microsoft began open-sourcing .NET (notably .NET Core later that year) and in June 2016, .NET Core 1.0 shipped with official cross-platform support. Since then, the .NET ecosystem gained momentum and lots of improvements have been made, including the Roslyn compiler platform, a new JIT (just-in-time compiler), performance improvements, more features, etc.&lt;/p&gt;
    &lt;p&gt;In 2018, Unity engineers discussed that they are working on porting the engine to .NET CoreCLR, the multi-platform version of Common Language Runtime (CLR), a component that runs .NET programs. Their main motivations behind this project were performance and convergence. In their post they said:&lt;/p&gt;
    &lt;quote&gt;...CoreCLR could be great for Unity game developers, as it will provide a significant boost in performance, by an order of 2x to 5x compare to the Mono runtime sometimes up to x10 on some workload!&lt;/quote&gt;
    &lt;p&gt;Unfortunately, now it√¢s the end of 2025 and we still can√¢t run games on CoreCLR.&lt;/p&gt;
    &lt;head rend="h2"&gt;The performance gap&lt;/head&gt;
    &lt;p&gt;We don√¢t hear about the performance gap between Mono and .NET much, likely because it is not possible to run games written for Unity under modern .NET. But we can still do a direct comparison with code that does not depend on Unity directly.&lt;/p&gt;
    &lt;p&gt;Our game has a unique architecture √¢ we strictly separate the game simulation code (business logic) from rendering. So much so that the simulation code does not depend on Unity√¢s libraries and can be compiled and run under any .NET version.&lt;/p&gt;
    &lt;p&gt;One day I was debugging an issue in map generation and it was time-consuming because it was taking over 2 minutes to start a game. To make debugging faster, I√¢ve written a unit test, hoping to cut down on the turn-around time since Unity takes 15+ seconds just to crunch new DLLs and reload the domain before the game can be launched and it also initializes rendering stuff that I did not care about. When I ran the test, it finished in 40 seconds. I was quite surprised that it was more than 3x faster, so I started digging deeper.&lt;/p&gt;
    &lt;p&gt;Long story short, Figure 1 shows traces from a profiler showing the difference between the game launching in Unity running under Mono vs. a unit test running under .NET.&lt;/p&gt;
    &lt;p&gt;Note that all shown benchmarks are using either Unity 6.0 or .NET 10.&lt;/p&gt;
    &lt;p&gt;So our benchmark shows that loading a save file, generating a map, and initializing the simulation takes 100 seconds in Unity/Mono but only 38 seconds in .NET. This result alone is already something that may raise eyebrows and has real consequences of how you may want to approach debugging and testing.&lt;/p&gt;
    &lt;p&gt;I also know from experience with Unity that Release mode running as a standalone executable (without the Unity editor) is much faster, so I decided to test that next.&lt;/p&gt;
    &lt;head rend="h2"&gt;.NET vs. Mono in standalone Release mode&lt;/head&gt;
    &lt;p&gt;Debug mode slowness is not great, but even non-optimized C++ code can be slow. To compare the real performance gap between Mono and .NET, let√¢s run the same benchmark as above but in release mode, standalone executable.&lt;/p&gt;
    &lt;p&gt;First up: Unity. I√¢ve run our deploy script to get an optimized executable and run it directly. Unsurprisingly, optimized standalone executable is beating Unity editor by a big margin, more than 3x faster. Next, the same code running under .NET in Release mode. Figure 2 shows the results.&lt;/p&gt;
    &lt;p&gt;Yep. 12 seconds. It√¢s actually mind-boggling how much work is being done in these 12 seconds and when I saw this for the first time, I was not only shocked, but also impressed. Just so you know, a 4k √É 4k map is being generated using all available threads out of hundreds of combined noise functions in like 3 seconds. Figure 3 shows the trace expanded.&lt;/p&gt;
    &lt;p&gt;If you are interested in seeing the actual x86 assembly generated by Mono and .NET JITs, see the Extras section at the end of this article.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;As you can see from the presented benchmarks, Mono is massively behind .NET in terms of performance. This is primarily due to differences in runtime optimizations and JIT that generates unoptimized assembly. The actual speedup surely depends on the code itself, but from my research, 1.5-3x speedup of C# execution is very likely for most projects.&lt;/p&gt;
    &lt;p&gt;If you are a game developer using Unity, or even a player, you can now understand that CoreCLR would be a massive boost to performance of games and even the Unity editor. Unfortunately, for the past 8 years, Unity leadership was more interested in √¢other things√¢ and did not give .NET modernization the attention it deserves.&lt;/p&gt;
    &lt;p&gt;Some view .NET modernization as support for new language features in C#, but that is just a cherry on top. New C# adds some handy features, but the new JIT can deliver multi-x speedups.&lt;/p&gt;
    &lt;p&gt;At this year's Unite conference, Unity announced that CoreCLR is still ongoing but it won√¢t be production ready in 2026. The good news is that it now seems to be on the Unity 6.x roadmap, and not left for later versions as suggested by 2024√¢s Unite presentation.&lt;/p&gt;
    &lt;p&gt;Moreover, CoreCLR is not just new JIT and C#, it unlocks broader and better-optimized support for things like Span&amp;lt;T&amp;gt;-style APIs, hardware intrinsics, and newer SIMD paths that devs cannot use these days. These features could add another multiplier to the performance gains for some classes of code. For example, our map generator heavily uses 2D and 3D simplex noise. I bet that having access to new runtime features in CoreCLR could speed up the map generation by another 2x.&lt;/p&gt;
    &lt;p&gt;Unity has a technology called Burst that automatically converts marked C# methods to optimized native assembly via the LLVM compiler. This sounds neat as it can avoid the poor JIT performance, but the downside is that Burst has strict limitations on what can be converted and supports only subset of C#. I believe that CoreCLR with modern JIT will have very similar performance characteristics to Burst. I am curious what would happen in a universe where Unity invested all the time and effort in CoreCLR support and high-performance C#, instead of developing and maintaining Burst.&lt;/p&gt;
    &lt;p&gt;Another interesting consequence of CoreCLR support is the ability to pre-compile the .NET intermediate assembly to machine code using ahead-of-time compilation (AOT). AOT can further improve startup time and is essential on platforms where JIT is restricted (notably iOS). Nowadays, Unity solves this with IL2CPP that takes the intermediate code and compiles it to C++ which is then optimized and compiled to native assembly. However, according to RichardFine (Unity staff), using CoreCLR AOT is not planned and IL2CPP is here to stay:&lt;/p&gt;
    &lt;quote&gt;AOT for IL2CPP is completely independent of AOT for CoreCLR (which we have no plans to adopt anyway). GC behaviour on IL2CPP improves when we upgrade the GC there, it√¢s not really affected by CoreCLR at all.&lt;/quote&gt;
    &lt;p&gt;In conclusion, CoreCLR won√¢t magically fix every bottleneck in a Unity game, but it does fix many of the code generation inefficiencies and allows writing higher-performance code. The benchmark presented in this article is meant to illustrate that modern .NET has spent years squeezing more work into fewer CPU cycles, and Unity users are largely locked out of those gains today.&lt;/p&gt;
    &lt;p&gt;If Unity can deliver production-ready CoreCLR support, it won√¢t just mean √¢newer C#√¢. It will mean faster runtime performance, faster iteration times, more performance headroom, no domain reload, better GC behavior, and maybe even more managed code and less native code. Until then, the gap will remain an invisible tax on every Unity project that leans on managed code.&lt;/p&gt;
    &lt;p&gt;I√¢m cheering for you, Unity devs, CoreCLR for the win!&lt;/p&gt;
    &lt;head rend="h2"&gt;Extras: Comparison of x86 assembly&lt;/head&gt;
    &lt;p&gt;I have actually dug much deeper into the performance aspects of Mono vs .NET but for the sake of this article not being too long, here is a brief summary.&lt;/p&gt;
    &lt;p&gt;Code listing 1 shows the testing code. It does some basic summing of custom structs that are wrappers around ints. This is an interesting example because Mono is very bad at inlining and simplifying expressions, even obvious ones, and we have plenty of structs like these in our code base (e.g. Quantity, MechPower, Tile2i, etc).&lt;/p&gt;
    &lt;code&gt;
      &lt;table&gt;
        &lt;tr&gt;
          &lt;td&gt;
            &lt;quote&gt;1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 &lt;/quote&gt;
          &lt;/td&gt;
          &lt;td&gt;
            &lt;quote&gt;static class Program { static void Main() { Console.WriteLine(RunTest(int.MaxValue)); } public static TestStruct RunTest(int iterations) { TestStruct value1 = new TestStruct(iterations % 2); TestStruct value2 = new TestStruct(iterations % 7); TestStruct value3 = new TestStruct(iterations % 13); TestStruct result = default; for (int i = 0; i &amp;lt; iterations; ++i) { result += value1 + value2; result += value1 + value3; } return result; } } readonly struct TestStruct { public readonly int Value; public TestStruct(int value) { Value = value; } public static TestStruct operator +(TestStruct lhs, TestStruct rhs) { return new TestStruct(lhs.Value + rhs.Value); } public override string ToString() =&amp;gt; Value.ToString(); }&lt;/quote&gt;
          &lt;/td&gt;
        &lt;/tr&gt;
      &lt;/table&gt;
    &lt;/code&gt;
    &lt;p&gt;To obtain assembly code, I√¢ve compiled the code in Release mode and ran it as a standalone executable. Then, I attached a debugger to the running process. An easy way to find this loop was to make it long/infinite and just break the program at any time, it would end up in that loop.&lt;/p&gt;
    &lt;p&gt;First, let√¢s take a look at .NET. Here is the x64 assembly of the for-loop section of the code.&lt;/p&gt;
    &lt;code&gt;
      &lt;table&gt;
        &lt;tr&gt;
          &lt;td&gt;
            &lt;quote&gt;1 2 3 4 5 6 7 8 9 10 &lt;/quote&gt;
          &lt;/td&gt;
          &lt;td&gt;
            &lt;quote&gt;add r8d,edx add edx,r10d 00007FFDEC338E88: mov r10d,r8d add r9d,r10d mov r10d,edx add r9d,r10d inc ecx cmp ecx,eax jl 00007FFDEC338E88&lt;/quote&gt;
          &lt;/td&gt;
        &lt;/tr&gt;
      &lt;/table&gt;
    &lt;/code&gt;
    &lt;p&gt; In both cases, the full loop of &lt;code&gt;int.MaxValue&lt;/code&gt; iterations took around 750 ms on my machine.
&lt;/p&gt;
    &lt;p&gt; This looks neat. Even if you don√¢t read assembly, you can see that there are two add instructions, one decrement, and one jump. It seems that the JIT hoisted the invariant sums &lt;code&gt;a = value1 + value2&lt;/code&gt; and &lt;code&gt;b = value1 + value3&lt;/code&gt; out of the loop and then just accumulates them.
&lt;/p&gt;
    &lt;p&gt;I also tested x86 assembly, and it looks very similar:&lt;/p&gt;
    &lt;code&gt;
      &lt;table&gt;
        &lt;tr&gt;
          &lt;td&gt;
            &lt;quote&gt;1 2 3 4 5 6 7 &lt;/quote&gt;
          &lt;/td&gt;
          &lt;td&gt;
            &lt;quote&gt;082E18D0: lea ebx,[esi+edi] add eax,ebx lea ebx,[esi+edx] add eax,ebx dec ecx jne 082E18D0&lt;/quote&gt;
          &lt;/td&gt;
        &lt;/tr&gt;
      &lt;/table&gt;
    &lt;/code&gt;
    &lt;p&gt;Interestingly, the loop direction was reversed, counting down. This saves one instruction as comparison to zero and conditional jump can be done as one instruction.&lt;/p&gt;
    &lt;p&gt;Now let√¢s look at Mono√¢s x64 assembly.&lt;/p&gt;
    &lt;code&gt;
      &lt;table&gt;
        &lt;tr&gt;
          &lt;td&gt;
            &lt;quote&gt;1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 &lt;/quote&gt;
          &lt;/td&gt;
          &lt;td&gt;
            &lt;quote&gt;1E87D2F3E20: movsxd rax,dword ptr [rsp+0C0h] mov dword ptr [rsp+40h],eax movsxd rax,dword ptr [rsp+0B8h] mov dword ptr [rsp+38h],eax movsxd rax,dword ptr [rsp+40h] mov dword ptr [rsp+0A0h],eax movsxd rax,dword ptr [rsp+38h] mov dword ptr [rsp+98h],eax movsxd rax,dword ptr [rsp+0A0h] movsxd rcx,dword ptr [rsp+98h] add eax,ecx mov dword ptr [rsp+90h],0 mov dword ptr [rsp+90h],eax mov dword ptr [rsp+30h],eax movsxd rax,dword ptr [rsp+0A8h] mov dword ptr [rsp+88h],eax movsxd rax,dword ptr [rsp+30h] mov dword ptr [rsp+80h],eax movsxd rax,dword ptr [rsp+88h] movsxd rcx,dword ptr [rsp+80h] add eax,ecx mov dword ptr [rsp+78h],0 mov dword ptr [rsp+78h],eax mov dword ptr [rsp+0A8h],eax mov dword ptr [rsp+28h],eax movsxd rax,dword ptr [rsp+0C0h] mov dword ptr [rsp+20h],eax movsxd rax,dword ptr [rsp+0B0h] mov dword ptr [rsp+18h],eax movsxd rax,dword ptr [rsp+20h] mov dword ptr [rsp+70h],eax movsxd rax,dword ptr [rsp+18h] mov dword ptr [rsp+68h],eax movsxd rax,dword ptr [rsp+70h] movsxd rcx,dword ptr [rsp+68h] add eax,ecx mov dword ptr [rsp+60h],0 mov dword ptr [rsp+60h],eax mov dword ptr [rsp+10h],eax movsxd rax,dword ptr [rsp+28h] mov dword ptr [rsp+58h],eax movsxd rax,dword ptr [rsp+10h] mov dword ptr [rsp+50h],eax movsxd rax,dword ptr [rsp+58h] movsxd rcx,dword ptr [rsp+50h] add eax,ecx mov dword ptr [rsp+48h],0 mov dword ptr [rsp+48h],eax mov dword ptr [rsp+0A8h],eax inc esi cmp esi,7FFFFFFFh jl 1E87D2F3E20&lt;/quote&gt;
          &lt;/td&gt;
        &lt;/tr&gt;
      &lt;/table&gt;
    &lt;/code&gt;
    &lt;p&gt; As you can see just from the number of instructions, this code will run way slower. The full loop of &lt;code&gt;int.MaxValue&lt;/code&gt; iterations took around 11500 ms, that√¢s ~15x slower.
&lt;/p&gt;
    &lt;p&gt;In the assembly you can see the four add instructions in the loop, the √¢inefficient√¢ increment + comparison + jump (instead of decrement + conditional jump), and most importantly a sea of mov instructions, which are just memory copies from inefficient inlining of the struct fields. Basically Mono is just tossing values around memory.&lt;/p&gt;
    &lt;p&gt;I have also tested assembly compiled in Debug mode running in the Unity editor and it√¢s even worse. The full loop takes 67 seconds (67000 ms)! In Unity Editor, the JIT likely switches to far less optimized codegen and includes additional checks/sequence-point overhead, which balloons runtime.&lt;/p&gt;
    &lt;p&gt;Takeaway: modern .NET√¢s JIT can scalarize tiny value types and hoist invariant work so the hot loop becomes a handful of register ops, while Mono often fails to do so and ends up shuffling values through memory, exactly the kind of gap that shows up as slowdowns in real simulation-heavy code.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46414819</guid><pubDate>Sun, 28 Dec 2025 21:41:42 +0000</pubDate></item><item><title>Dolphin Progress Report: Release 2512</title><link>https://dolphin-emu.org/blog/2025/12/22/dolphin-progress-report-release-2512/</link><description>&lt;doc fingerprint="b630491f28054ad1"&gt;
  &lt;main&gt;
    &lt;p&gt;With the holiday season reaching its apex, we have a few surprises for those of you that have been patiently waiting. The latest release of Dolphin is stuffed with treats. Our first present is presentation - frame presentation, that is. Two new options have arrived and will help users both reduce latency and smooth out games that struggle with frame pacing.&lt;/p&gt;
    &lt;p&gt;Some games do outright naughty things that make emulation difficult. A slew of them are being coerced onto the nice list this year thanks to a sack full of patches that bypass their troublesome behaviors. Fans of the Broadband Adapter (BBA) have a great present tailored just to them: a new local mode BBA! Designed for allowing multiple instances of Dolphin on the same computer to connect together, it's perfect for use with Parsec or other similar services. And perhaps another gift will have you singing your favorite Wii hits?&lt;/p&gt;
    &lt;p&gt;But alas, what fun would the holiday season be if we spoiled all the gifts? Read on to unwrap the latest edition of the Dolphin Progress Report.&lt;/p&gt;
    &lt;p&gt;...&lt;/p&gt;
    &lt;p&gt;...&lt;/p&gt;
    &lt;p&gt;Huh? We've received word that apparently the Android users have made the nice list? Really? That can't be right... but this gift is addressed to them.&lt;/p&gt;
    &lt;p&gt;After more than a couple bumps in the road, RetroAchievements support has finally arrived on the Android version of Dolphin! In Release 2512, the core achievement experience is now available in your pocket. This initial version hasn't quite reached parity yet with the desktop experience, but we didn't want to hold things up any longer. The important thing for Android RetroAchievements users is that you can log in and unlock achievements in supported GameCube games. Because some menus are incomplete, it may be best to have the RetroAchievements website open in the background for achievement lists and other things while we finish up the in-app UI.&lt;/p&gt;
    &lt;head rend="h3"&gt;Notable Changes¬∂&lt;/head&gt;
    &lt;head rend="h4"&gt;2509-493 - Add Rush Frame Presentation and Smooth Frame Presentation Options by Billiard¬∂&lt;/head&gt;
    &lt;p&gt;Latency was once a huge challenge for emulators, and it is still a major concern. At one point not too long ago, it was pretty much infeasible for most emulators to match the latency of their console counterparts. Compared to a dedicated game console racing the beam on a CRT television, emulators had to deal with sluggish OS window managers with on-by-default triple-buffer V-Sync holding three frames back, first-gen wireless controllers that added latency right at the source of input, slower displays that added several additional frames of latency (much more if that display was a TV without game mode), and on top of everything the emulator still needed to do its job and take time to actually emulate everything.&lt;/p&gt;
    &lt;p&gt;Dolphin just missed out on the worst of this. By the time Dolphin's performance and compatibility were good enough for users to worry about things like latency, the overall situation had improved dramatically. Low latency and high refresh rate monitors paired with features like Exclusive Fullscreen removed most of the major bottlenecks that emulators had to fight against.&lt;/p&gt;
    &lt;p&gt;The designs of the GameCube and Wii also afford Dolphin some opportunities that other emulators don't have. The GameCube/Wii are double buffer V-Sync'd by default, resulting in a final input latency of roughly 60ms on a CRT in an optimized 60fps title. However, because of how the XFB Output Pipeline works, Dolphin has the opportunity to bypass the buffers and grab those XFB copies early, and immediately present them directly to the screen. We call this feature Immediately Present XFB, and it cuts out quite a bit of the latency present on the console.&lt;/p&gt;
    &lt;p&gt;Tricks like these let Dolphin match console latency as long as the host device is capable enough. On extremely optimal setups with low latency VRR monitors combined with Immediately Present XFB, Dolphin could even dip a frame below real console latency!&lt;/p&gt;
    &lt;p&gt;There are some caveats, though. While Immediately Present XFB is a powerful tool for reducing latency, it is also a hack that relies on games behaving in a specific manner. If a game messes with the XFB pipeline, such as if it applies post processing using the CPU, or stitches together multiple XFB Copies, the hack will cause Dolphin to output nonsensical garbage.&lt;/p&gt;
    &lt;p&gt;Even when Immediate isn't outright breaking the game, the XFB pipeline is a big part of how some games handle frame pacing, so bypassing it can make the game feel less smooth. For the best experience in many games, a user probably would prefer superior latency and good frame pacing.&lt;/p&gt;
    &lt;p&gt;And that was Billiard's goal. He saw an opportunity to improve the situation and started work on two new options. One feature was a way to reduce latency without disrupting how the game rendered, and the other would allow smoother frame pacing even in games that struggled on real console. How would he accomplish these feats? Well, by making the emulator throttle smarter.&lt;/p&gt;
    &lt;p&gt;Modern computers are powerful enough to emulate most GameCube and Wii games faster than the original hardware could run them. Disable the framelimiter and try it for yourself! To keep emulation on pace, Dolphin‚Äôs Throttle function stalls emulation to produce a mostly properly paced simulation of the original hardware. Throttling is necessary for playable emulation on modern systems, but making the host CPU wait adds time. If input from the user and stalls are aligned poorly, throttling can add a small amount of latency.&lt;/p&gt;
    &lt;p&gt;To address this, Billiard has added a new throttling mode called Rush Frame Presentation, where throttling becomes centered around presenting the frame as soon as it can after the input is read. In theory, this reduces the time between click and photon and can have a very noticeable effect, especially in lower frame rate titles. To the end user, all of this is completely invisible. All of this is happening sub-frame, so Dolphin still will throttle the appropriate amount of time to maintain the correct frame rate.&lt;/p&gt;
    &lt;p&gt;The faster your computer, the more of an effect this will have because it will be able to emulate the part of the frame faster. We can easily catch the difference in The Legend of Zelda: Wind Waker and Super Mario Sunshine by using a high speed camera, for example.&lt;/p&gt;
    &lt;p&gt;Some games will also see a further benefit by combining Immediately Present XFB with Rush Frame Presentation. Unfortunately, this combination can lead certain games to looking gnarly as they will just be spitting out the image at whatever point they're finished rendering during a frame. That's why the second new option is important.&lt;/p&gt;
    &lt;head rend="h5"&gt;Smoothing Things Out¬∂&lt;/head&gt;
    &lt;p&gt;Immediately Present XFB could already cause poor frame pacing, and now Rush Frame Presentation could make it even worse. In some games, it can get so poor that VRR monitors will fall out of their operating range!&lt;/p&gt;
    &lt;p&gt;Smooth Frame Presentation allows Dolphin to delay presentation by roughly 1-2ms so that it can more consistently output frames, using previous frame times as a heuristic. This option can be used in any game that has poor frame pacing in order to try to improve the situation.&lt;/p&gt;
    &lt;p&gt;The results of Smooth Frame Presentation are good enough that a lot of games that needed the XFB Output Pipeline enabled by default because of frame pacing issues can now take advantage of the lower input latency provided by Immediately Present XFB and Rush Frame Presentation without any noticeable side-effects.&lt;/p&gt;
    &lt;p&gt;Even if you're using neither of the latency features, some games just have bad frame pacing even on real console. Smooth Frame Presentation can help them, too. There are rare cases, especially when using Rush Frame Presentation alongside Immediately Present XFB, where a game's output will be so inconsistent that smoothing won't help. We're looking at you, Dragon Ball Z: Budokai.&lt;/p&gt;
    &lt;head rend="h5"&gt;Outside Verification¬∂&lt;/head&gt;
    &lt;p&gt;All of these results sound great, but outside of a few camera tests on lower frame rate games, we were mostly trusting latency offset numbers provided by Dolphin. Testers did also report better latency, but given that the placebo effect exists and these are such small differences, we wanted more concrete data. But other than just game feel, how do you test latency?&lt;/p&gt;
    &lt;p&gt;In the past, we've used the light sensor present on Rock Band 3 guitars alongside an in-game synchronization test to get some very rough offset values. The problem with that is that it will only ever test one game, and the guitar controller isn't particularly viable in most games outside of unusual controller runs.&lt;/p&gt;
    &lt;p&gt;Before making any claims about our latency, we wanted to do our due dilligence in respect to both Dolphin and real hardware. To accomplish this, we contacted some professionals that have been fighting against latency for quite some time. Fizzi from Slippi.gg and adapter expert Arte graciously donated their time and helped us measure latency in the latest version of Dolphin versus console. Arte specifically developed a GameCube controller adapter with a photon sensor designed to determine controller latency, which is rather convenient because that's exactly what we want to measure.&lt;/p&gt;
    &lt;p&gt;Their GameCube controller adapter polls the controller at 1000Hz, and a light sensor on it can be programmed to look for certain changes in output from the game. By having access to both the source of the input and the change on the screen, the adapter can provide a real world measurement of how exactly how long it takes for a user input to result in a change on the display - what is commonly referred to as "click to photon". As an added bonus, the adapter can also be hooked up to real console with no conversion or added latency, letting us compare directly with games running on real hardware and a CRT.&lt;/p&gt;
    &lt;p/&gt;
    &lt;p&gt;The exciting thing about these numbers is that they confirm our experience. Dolphin's latency compares favorably to console. To be fair to the GameCube, Arte's emulation setup included a modern low-latency 144Hz monitor and the lowest latency controller adapter. Dolphin couldn't quite compete with Slippi, but most of that can be attributed to deep modifications to how Super Smash Bros. Melee outputs.&lt;/p&gt;
    &lt;p&gt;For all the samples above, an input bug present in the original game has been patched out. This is to make getting consistent results a little bit easier. That fix did mean that combining Rush Frame Presentation and Immediately Present XFB no longer benefited that title when testing. However, Arte modified the test to work with other games, and some games respond incredibly well when combining Rush and Immediate.&lt;/p&gt;
    &lt;p/&gt;
    &lt;p&gt;Due to time constraints with holiday vacations, adjusting the photon sensor for different games, we weren't able to gather all of the numbers we wanted from other games. However, with Wind Waker the numbers were interesting enough that we managed to get enough samples right under the wire, at least in Dolphin. The graph above shows latency with default settings, Immediately Present XFB and the combined efforts of Immediately Present XFB and Rush Frame Presentation. Note that the default settings were measured last, and as such the error range is estimated.&lt;/p&gt;
    &lt;p&gt;The reason why we wanted to squeeze in this particular case is because it demonstrates that combining Rush Frame Presentation and Immediately Present XFB can result in lower latency than what was possible before. This is not always true, and this 10ms reduction is from an ideal example. Unmodified Melee, for instance, showed the combination reducing latency by less than 4ms. Some games saw no benefit from combining both features together.&lt;/p&gt;
    &lt;p&gt;In the end, how much these two new options will help varies greatly depending on the game and setup. Currently, we've left them both disabled by default in the Configuration -&amp;gt; Advanced Tab, but that may change as the settings get more testing and we gauge what users want the most.&lt;/p&gt;
    &lt;head rend="h4"&gt;2509-74 - GameCube - Add SDL Stock Profile by Samb¬∂&lt;/head&gt;
    &lt;p&gt;Unlike the wacky Wii Remote and its mess of attachments, the GameCube Controller mostly mirrors modern controllers. Sure, the sticks can't be pushed in, it's missing a shoulder button, the two stage analog+digital triggers can be tricky, and the face buttons are weird. But at the end of the day, it's a four face button, twin stick controller with a D-pad. Close enough?&lt;/p&gt;
    &lt;p&gt;So we have added an SDL Profile that players can use to speed up their GameCube controller mapping.&lt;/p&gt;
    &lt;p&gt;Using the profile is simple:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Go the GameCube "Standard Controller" mapping window.&lt;/item&gt;
      &lt;item&gt;Select your controller from the Device dropdown. Pick the variant that starts with "SDL".&lt;/item&gt;
      &lt;item&gt;Select the &lt;code&gt;SDL Gamepad (Stock)&lt;/code&gt;Profile and click Load.&lt;/item&gt;
      &lt;item&gt;Optional: Calibrate your joysticks (please don't skip this Hall effect users!) and set up deadzone.&lt;/item&gt;
      &lt;item&gt;Enjoy.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Since the GameCube controller doesn't map 1:1 to modern controllers, this is a best guess stock profile that will reasonably work for most people, most controllers, and most games. If a game demands a button combination that doesn't work with your hands on your controller, or if the face buttons or any other button simply aren't to your liking, you can build from the stock profile and adjust everything until it's just right.&lt;/p&gt;
    &lt;head rend="h4"&gt;2509-237 and 2509-339 - Add Option to Reset Settings Back to Default by JoshuaVandaele and Simonx22¬∂&lt;/head&gt;
    &lt;p&gt;Dolphin is a complicated emulator with a lot of options. Some of it is definitely our fault, and some of it is just the reality of trying to emulate something as complicated as the GameCube and Wii. To the average user, a lot of these settings aren't immediately obvious, even with descriptions.&lt;/p&gt;
    &lt;p&gt;"Emulated Memory this? EFB, XFB that, VBI what?"&lt;/p&gt;
    &lt;p&gt;The most experienced users (and even developers) can sometimes get frustrated enough with an issue that percussive maintenance is necessary, leading to one changing lots of settings around until something happens. Sometimes this works, leading to a temporary moment of joy, before it all comes crashing down when none of your other games will boot. Some people might be able to backtrack and figure out what went wrong, but a lot of users are left lost.&lt;/p&gt;
    &lt;p&gt;Until recently, users had to delete the Dolphin settings files from their computer to restore everything to default. No one liked that, but a button to reset settings was non-trivial thanks to Dolphin's multiple layers of settings that are a handful to manage.&lt;/p&gt;
    &lt;p&gt;Thankfully, JoshuaVandaele was finally able to sort out all of the implementation details and give us the long awaited "Reset All Settings" button.&lt;/p&gt;
    &lt;p&gt;Thanks to Simonx22, Android users also get access to this feature. It can be found in the advanced settings menu in the Android GUI.&lt;/p&gt;
    &lt;head rend="h4"&gt;2509-217 - GamePatch: Modify Certain Games to Behave Better in Dolphin by SuperSamus with additional contributors¬∂&lt;/head&gt;
    &lt;p&gt;Some games are more strenuous to emulate than others. Sometimes it's because the game pushes the console, and other times it's because the game does something annoying to emulate - maliciously or not.&lt;/p&gt;
    &lt;p&gt;The latter situation can be especially frustrating, as sometimes 99% of what the game does is fine, with just one little behavior, design decision, or sometimes even an underlying bug causing problems for Dolphin.&lt;/p&gt;
    &lt;p&gt;SuperSamus identified a few common game behaviors that causes Dolphin a lot of headaches. Properly fixing these issues would difficult, with some requiring large rewrites to the emulator that probably will never come. Fortunately, there is a simpler solution: patch out the difficult to emulate game behaviors. These small patches increase emulation performance, and sometimes even work around other unwanted behaviors.&lt;/p&gt;
    &lt;head rend="h5"&gt;Complex Idle Loops¬∂&lt;/head&gt;
    &lt;p&gt;Idle loops are a sequence of instructions that makes the CPU run laps doing nothing while it waits for something to happen. Obviously, emulating a CPU burning cycles doing nothing isn't efficient for an emulator, so one of Dolphin's earliest optimizations was the ability to detect and skip these idle loops. This feature is fairly standard among emulators and is called Idle Skipping.&lt;/p&gt;
    &lt;p&gt;Idle loops are more common than you might think. It's very rare for a game to absolutely max out the console's CPU, so for the majority of frames the CPU will idle a bit. And as long as Dolphin can detect the idle loops, they can be skipped to increase overall performance. Idle Skipping is one of the key things that make some areas of a game more demanding than others, especially on the CPU side of things. Essentially, Idle Skipping is less effective the more that the CPU has to do.&lt;/p&gt;
    &lt;p&gt;Dolphin can't detect all idle loops, and the scope of what it can find is rather limited. We have to find as many idle loops as accurately as possible while also ensuring that we don't invest too much time and resources into finding them. The more complex the heuristic is, the more of an impact it will have on the JIT. As such, some games have annoying idle loops that we know about but are not worth detecting.&lt;/p&gt;
    &lt;p&gt;Need for Speed: Nitro and Rayman Raving Rabbids are two such games with these complex idle loops. After examining their code in detail, SuperSamus realized that their idling behaviors could be modified with a patch that would allow Dolphin to more easily detect and skip them.&lt;/p&gt;
    &lt;p&gt;This results in massive performance boosts, especially in lighter areas, with some menus running at four times as fast. Of course, heavier areas see less of a benefit. We'll have a chart showing some of the raw numbers at the end.&lt;/p&gt;
    &lt;head rend="h5"&gt;Running Uncapped¬∂&lt;/head&gt;
    &lt;p&gt;Dolphin is not a cycle accurate emulator. In fact, Dolphin is fundamentally not designed to be cycle accurate, especially with GPU operations. Dolphin's emulated GPU was designed to be infinitely fast, only being limited by other factors like CPU emulation or the maximum performance of the host device. Nowadays, it has been tamed a bit with synchronization points that provide a rough approximation of the timings of the original command processor, but it is still by no means accurate, let alone cycle accurate. Also, rendering is still infinitely fast, as the only thing that limits it is the speed of your host GPU.&lt;/p&gt;
    &lt;p&gt;And yet, most games run at the correct frame rate, because they limit themselves.&lt;/p&gt;
    &lt;p&gt;The GameCube and Wii were designed for analog TVs, so they used the analog television's sign to start a new frame as a synchronization point called the Vertical Blanking Interupt (VBI). All a game had to do was start a new frame every time it saw a VBI and finish the frame before the next VBI, and it would be perfectly synchronized to the frame rate of the display. That's it. It was simple and efficient, so the vast majority of GameCube and Wii library tie their frame rates to the VBI. Thanks to that, Dolphin's early developers didn't even need to care about how fast the emulated GPU runs; as long Dolphin emits VBIs at the correct frequency, most games will just run at the correct frame rate regardless of what's going on under the hood. And this gives Dolphin bonuses like not emulating GPU slowdown, allowing games that struggled on console to perform much better in Dolphin.&lt;/p&gt;
    &lt;p&gt;But the GameCube and Wii don't have operating systems. Games run on the bare metal and have full control of the machine, so developers could do whatever they wanted. And some games eschew the VBI and run uncapped.&lt;/p&gt;
    &lt;p&gt;Uncapped games break Dolphin's assumptions. They can render way, way too fast. For example, when running Hulk (2003) in Dolphin, it only displays at 60 FPS, but the physics engine could be doing hundreds of steps per second behind the scenes. This is cool because technically a game like this can easily be hacked to run at higher frame rates, but terrible because it hammers people's devices with all kinds of unnecessary work!&lt;/p&gt;
    &lt;p&gt;It gets even worse from here. The physics engine's independence from the output frame rate isn't perfect. If the number of steps per second gets too high, small rounding and math errors start to accumulate, and parts of the game not properly tuned to run at these higher frame rates start to break. This mostly results in dialogue timing issues, but in one stage halfway through the game, it causes a physics calculation issue where a required ledge can't be climbed, essentially softlocking the player.&lt;/p&gt;
    &lt;p&gt;Some other games choose to synchronize to the VBI, but don't bother in incredibly simple scenes where the frame rate doesn't matter. This is most commonly seen with splash screens and loading screens. One such game with this behavior is Bully: Scholarship Edition. Most of the time it is perfectly stable, but because the loading screens are uncapped, it can actually cause the game to randomly hang on transitions. The Simpsons Hit &amp;amp; Run also has this issue, but only during the initial load. While most desktop computers are able to handle the initial load relatively well, Android users have reported tremendous slowdowns where the loading time would take more than 45 seconds.&lt;/p&gt;
    &lt;p&gt;SuperSamus identified these problems and either created patches for these behaviors themselves, or helped others with those titles create patches. Most of these patches are only one or two lines and simply limit the game's frame rate to the VBI frequency. The patches prevent the game from slamming Dolphin's overpowered emulated GPU, greatly increasing performance while fixing issues caused by the games running internally at too high of a frame rate.&lt;/p&gt;
    &lt;p&gt;This comes with a little bonus - since they are now bound to the VBI frequency, you can now use VBI Frequency Override to adjust their frame rate up or down as desired, allowing Dolphin to take advantage of the fact that these games "support" running at higher frame rates.&lt;/p&gt;
    &lt;p&gt;In addition to the games above that had emulation bugs caused by framelimiting, SuperSamus and Billiard also created limiter patches for the following games purely for performance reasons:&lt;/p&gt;
    &lt;p/&gt;
    &lt;p&gt;Somewhat ironically, by slowing down uncapped games we improve their performance in Dolphin, so when turning off Dolphin's framelimiter they go much faster. ...ignore that and use this as a measure of their performance improvement. If a game's frame rate doubled in this chart, then the performance required to run that game at fullspeed has been roughly halved.&lt;/p&gt;
    &lt;p&gt;These numbers don't tell the whole story on some of these games. Hulk (2003) and other games which ran uncapped would get progressively worse performance in Dolphin depending on how light the scene was to render.&lt;/p&gt;
    &lt;p&gt;As a final reminder, these patches should not be considered proper solutions to fixing uncapped games. They are purely to increase playability of these titles.&lt;/p&gt;
    &lt;head rend="h5"&gt;Eggmania: Eggstream Madness¬∂&lt;/head&gt;
    &lt;p&gt;The Force Progressive Output patch we included for the Japanese version of this game was causing it to crash on boot. Since we were adding new patches and extrems had already pointed us to a correct version of the patch, we decided to update it alongside all of the other patches.&lt;/p&gt;
    &lt;p&gt;Despite being the Japanese version of a rather obscure game, this crash was somehow reported multiple times by different users. For the two of you out there waiting, the fix is finally here.&lt;/p&gt;
    &lt;head rend="h4"&gt;2509-242 - BBA: IPC for BBA Between Multiple Instances of Dolphin on the Same Machine by cristian64¬∂&lt;/head&gt;
    &lt;p&gt;The Broadband Adapter (BBA) for the GameCube was all about connecting games to a network. Whether across the hall with a Local Area Network (LAN) or across the continent with the information superhighway, the BBA brought Nintendo consoles together like nothing before it! Well, except the Modem Adapter, but we're not talking about that today.&lt;/p&gt;
    &lt;p&gt;Dolphin has supported the BBA emulation for many years. An early LLE implementation was fairly accurate, but required the user to setup TAP servers and suffered from performance bottlenecks with said TAP servers depending on the operating system. It wasn't until BBA-HLE in 2022 that BBA emulation became readily accessible to the average user.&lt;/p&gt;
    &lt;p&gt;BBA-HLE is great if you want to connect Dolphin to another computer or real hardware. But if you want to run multiple instances on the same computer, things get more complicated. While it was technically possible through networking trickery, it was far beyond what we'd expect of the average user.&lt;/p&gt;
    &lt;p&gt;cristian64 realized that this problem could be pretty cleanly solved by using a library called &lt;code&gt;cpp-ipc&lt;/code&gt;.  This library would allow separate instances of Dolphin on the same machine to share data easily and efficiently through Inter-Process Communication (IPC).  Instead of using a network stack, we can just simulate the instances of Dolphin being in their very own network and let them communicate without the need for any outside support. &lt;/p&gt;
    &lt;p&gt;With BBA-IPC added, here's a quick rundown of the many options you have for BBA.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Broadband Adapter (TAP): Dolphin's LLE solution for the Broadband Adapter that requires a TAP interface.&lt;/item&gt;
      &lt;item&gt;Broadband Adapter (XLink-Kai): LLE solution that can connect to the Xlink-Kai service to allow players on separate networks to connect together. Success highly depends on each game's latency tolerance and the latency between the players.&lt;/item&gt;
      &lt;item&gt;Broadband Adapter (HLE): HLE solution that hooks the Broadband Adapter up to the host's network interface to connect with other devices on the network or to a server for certain online games.&lt;/item&gt;
      &lt;item&gt;Broadband Adapter (IPC): HLE solution that allows Dolphin instances on the same machine to share memory and communicate directly without the need for a host network.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;BBA-IPC allows for easy testing of BBA features on a single PC, and can also be used in conjunction with game streaming services like Parsec to play BBA titles over the internet without needing to meet the strict latency requirements of emulating the BBA over the internet. This is admittedly a rather niche usecase for BBA, but the feature is relatively compact and easy to maintain.&lt;/p&gt;
    &lt;p&gt;Currently, only Windows and Linux are supported by BBA-IPC, but if there is enough interest, cristian64 has already found another library that could let us support this in other operating systems.&lt;/p&gt;
    &lt;head rend="h4"&gt;2509-250 - IP/Top: Make InetAToN async by sepalani¬∂&lt;/head&gt;
    &lt;p&gt;Playing Mario Kart Wii online in Dolphin back when WFC support was first added was a rather rough experience. It was functional, but slow and stuttery, just enough to be playable and help players with preserving traffic to the official WFC in the final months before the servers were finally shut down.&lt;/p&gt;
    &lt;p&gt;But that wasn't the end. Thanks to revival efforts, many Wii games still have online communities, with Mario Kart Wii being quite possibly the biggest. In the years since, Dolphin's Wi-Fi emulation has gotten to the point that users can play alongside real Wiis in most games without any issues.&lt;/p&gt;
    &lt;p&gt;Well, most users. Unfortunately, even in modern builds of Dolphin there were a couple of users having severe stuttering and freezing problems, and all of them were using Android devices. These problems were handwaved away as typical Android performance issues at first, but after closer examination, it was obvious that there was something else going wrong. One user recorded us tons of examples and helped narrow down what was happening.&lt;/p&gt;
    &lt;p&gt;They (accurately!) surmized that it had something to do with a player joining or leaving the online lobby. In Mario Kart Wii, players can join or leave a race mid-match, with new players spectating until the next race began. The only mystery left was figuring out why this was only happening to a couple of users while everyone else was fine.&lt;/p&gt;
    &lt;p&gt;Using their wealth of knowledge from working on the Monster Hunter Tri replacement Wi-Fi servers, sepalani jumped in and investigated the issue. Same as us, they couldn't find any issues at first. They were able to play without any kind of freezing, even on their phone. However, after unlocking the frame rate on a desktop computer, sepalani noticed a small hitch when the function &lt;code&gt;InetAToN&lt;/code&gt; was called.  This hitch was impossible to see when running at normal speed, but when running at 1000%+ speed, the dip was just barely noticeable.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;InetAToN&lt;/code&gt; is a standard networking function that takes an IP address string and converts it into its equivalent binary form. For example, the string &lt;code&gt;192.51.100.50&lt;/code&gt; would be transformed into the hexadecimal number &lt;code&gt;0xC0336432&lt;/code&gt;. On the Wii, &lt;code&gt;InetAToN&lt;/code&gt; has some additional functionality that is unusual: in addition to text-based IPs, it also accepts hostnames as inputs. This means that a developer can choose to pass a hostname like &lt;code&gt;google.com&lt;/code&gt; into &lt;code&gt;InetAToN&lt;/code&gt;, and the function will perform a Domain Name System (DNS) lookup to resolve the hostname into an IP address. &lt;/p&gt;
    &lt;p&gt;sepalani figured out that if a hostname was provided to &lt;code&gt;InetAToN&lt;/code&gt; and the resulting DNS lookup was slow enough, the function would cause a stutter because Dolphin waits for the result of the lookup before continuing. The time it takes for a DNS lookup to finish varies depending on various factors, such as the user's internet connection. On a desktop PC connected to home internet, the lookup would complete before the user even saw a frame drop. But many of our Android users are connected to the internet via cellular data, which can have particularly poor latency depending on signal strength and network congestion.&lt;/p&gt;
    &lt;p&gt;Therefore, sepalani changed &lt;code&gt;InetAToN&lt;/code&gt; to be non-blocking so that DNS resolution can take as long as it wants without locking up the emulator.&lt;/p&gt;
    &lt;head rend="h4"&gt;2509-481 - SDIO: Fix CSD/CID emulation by Naim2000¬∂&lt;/head&gt;
    &lt;p&gt;There's no way to sugarcoat this one. This was a pretty bad %$#&amp;amp; up. Dolphin's SD card emulation was broken, and has been broken for a very long time. Yet somehow, it worked. This is one of the dangers of emulation - sometimes you can do something very wrong yet the software just trudges on.&lt;/p&gt;
    &lt;p&gt;The modern Wii homebrew scene lives off of SD Cards. It's a convenient tool available on most Wiis, and can be used in conjunction with the Homebrew Channel, BootMii, and other essential homebrew. It's not uncommon to see a 32GB SD card in a Wii loaded with homebrew emulators, software, and game mods.&lt;/p&gt;
    &lt;p&gt;One oddity about the Wii is that it was released without support for the then new SDHC standard (2GB to 32GB SD cards). This would be rectified in an update three years later, but the damage was done. Thanks to the way Nintendo designed the Wii's software stack, games released prior to SDHC support being added would be forced to use older IOSes that lacked SDHC support. Even if your System Menu, homebrew, and newer games worked with your big SD card, a game like Super Smash Bros. Brawl wouldn't. At least without some help.&lt;/p&gt;
    &lt;p&gt;The Brawl community refused to be restricted by Nintendo's paltry limitations. They used homebrew to load the game with newer IOS versions and even patched the game to remove the nonsensical 3 minute time limit on replays. Small patches like these were just the beginning, as the community started making bigger changes to the game, like balance patches Brawl+ and Brawl-, and eventually full game overhauls like Project M and its offspring.&lt;/p&gt;
    &lt;p&gt;The Smash Bros. community is still pushing the limits of homebrew on both the Wii and Dolphin. One such effort is Super Smash REX. REX has an astounding amount of stages, music, and characters to the point that just the base mod is over 8GB in size. And that's where we come into all of this.&lt;/p&gt;
    &lt;p&gt;A developer from REX reached out to us about how they were reaching some kind of limit with the virtual SD card in Dolphin. Once the amount of data on the SD card exceeded 10.7GiB, Dolphin's SD card support would completely fall apart.&lt;/p&gt;
    &lt;p&gt;This actually wasn't too surprising. There have been scattered reports from Rock Band 3 and Just Dance failing when too much DLC was installed to the SD card. However, reproducing those issues required owning a mountain of DLC, which none of us did. With REX, everything was readily available and the actual launcher was just a homebrew application. This made reproducing the issue much easier, and they did most of the work for us.&lt;/p&gt;
    &lt;p&gt;The key detail was that the files on the SD card didn't even have to relate to the mod. We could just fill it with cat pictures, load up the Homebrew Channel or any other homebrew that relied on the SD card, and watch the fireworks. What could be going wrong?&lt;/p&gt;
    &lt;p&gt;In a community as old as the GameCube/Wii scene, there are some voices that must be heeded. Before the REX developers reached out to us, one of the Supreme GameCube/Wii Sages, extrems, hadst been prognosticating doom since the new year if a bug with the card ID (CID) and card-specific data (CSD) registers in our SD card emulation was not mended. Verily, it came to pass.&lt;/p&gt;
    &lt;p&gt;The problem was extremely straightforward. When the emulated Wii queried the virtual SD card for its capabilities, such as size and speed, Dolphin would return complete garbage due to the reports being in the wrong byte order. The strange part of this was that SD card emulation even worked at all.&lt;/p&gt;
    &lt;p&gt;When REX reported there were SD card issues, we quickly thought of the flaws that extrems pointed out to us. However, knowing the problem and fixing it were two very different things. Our attempts to make things right only made things worse. Two different developers took a stab at it, and both left SD card emulation completely non-functional. We were left defeated and efforts on the problem slowed.&lt;/p&gt;
    &lt;p&gt;In mid-November, Billiard was perusing &lt;del&gt;Pull Requests&lt;/del&gt; ancient tomes of knowledge that fell through the cracks. One such tome was named "SDIO: report write lock status". Emulating a SD card's write lock switch was a rather unimportant implementation detail that would not affect emulation. As such, there was no rush to review it. But when Billiard did finally get to reviewing it, he saw that it also fixed Dolphin's CID and CSD byte ordering.&lt;/p&gt;
    &lt;p&gt;A few quick reviews and a rebase later, and the bug was finally quelled. Now Dolphin properly works with virtual SD cards up to 32GB in size.&lt;/p&gt;
    &lt;head rend="h4"&gt;2509-542 - USB: Emulated Support for Logitech Microphone for Wii by Biendeo¬∂&lt;/head&gt;
    &lt;p&gt;The Logitech Microphone is an iconic accessory for the Nintendo Wii. Roughly 100 games, including popular titles in the Guitar Hero and Rock Band series, support the peripheral.&lt;/p&gt;
    &lt;p&gt;Dolphin has had support for the physical Logitech Microphone via USB Passthrough for years, but even as other USB peripherals received their emulated counterparts, those wanting to sing into an emulated Wii using generic microphones had to wait. Even the maligned Wii Speak received support before the Logitech Microphone, despite the fact that it supported far fewer games and was much more complicated to implement. This is just how emulation works sometimes. The more complicated, less useful accessory was just far more interesting than the popular, yet generic accessory.&lt;/p&gt;
    &lt;p&gt;But the foundation created for emulating the Wii Speak did lend itself well toward implementing a second microphone accessory in Dolphin. After all, the Wii Speak was a microphone, so a few people took shots at adapting the Wii Speak code to work with the Logitech Microphone.&lt;/p&gt;
    &lt;p&gt;First, supermilkdude67 posted a WIP fork with very basic support that fizzled out. A few months later, a certain announcement brought renewed interest toward making the many singing games more accessible to users. Biendeo took over the mantle using the initial fork as a base. With some help from veteran Dolphin developers, a few fixes, and some upgrades to the GUI, it was ready to go.&lt;/p&gt;
    &lt;p&gt;You can now emulate the Logitech Microphone with any standard PC microphone! The exact volume levels might need to be adjusted depending on the input source, so make sure you properly calibrate before your first jam session. Given that this is a new feature, compatiblity isn't perfect, and some games may take more fiddling than others.&lt;/p&gt;
    &lt;p&gt;For our Android users, things aren't ready yet. While the core feature should be mostly compatible between the two environments, it will need a completely different GUI. As such, it might be a while before everything gets ported over to our Android builds.&lt;/p&gt;
    &lt;head rend="h4"&gt;2509-406 - On-Screen Display: Add New Default Font by TryTwo¬∂&lt;/head&gt;
    &lt;p&gt;Dolphin's On-Screen Display is an important part of communicating with users, whether it's statistics, performance metrics, or notifications from features like RetroAchievements. Unfortunately, the pixel font we were using could be hard to read, especially for some users with HiDPI screens. TryTwo improved the situation by adding the ability to change the font size of on-screen display messages, but this didn't solve the problem - making a pixel font larger can look quite bad, especially at non-integer scales. Dolphin needed a new font that could scale to arbitrary resolutions while still looking clear.&lt;/p&gt;
    &lt;p&gt;With this change, Dolphin's OSD now uses a proper vector font!&lt;/p&gt;
    &lt;p&gt;Vera Sans Mono is the new default font. We think that in pretty much every scenario, it's easier to read than our old font. However, TryTwo figured that since we were adding a font anyway, that Dolphin could just provide the ability for users to override the font. So if you're unsatisfied with our typography tastes, add the font of your choice to the "Load" folder of your User directory and name it "OSD_Font.ttf". Any TrueType font will work!&lt;/p&gt;
    &lt;p&gt;Now that the On-Screen Display has even more options, we've decided to consolidate them within their own config section. So if you're looking to adjust what is displayed, font size, or if you want them disabled altogether, the settings can now be found in one place within the On-Screen Display section of the Configuration window.&lt;/p&gt;
    &lt;head rend="h4"&gt;2509-554 - AX-HLE: Fix Low Pass Filter Edge Case by flacs¬∂&lt;/head&gt;
    &lt;p&gt;If you were an arcade junkie in the late 90s and early 2000s, then you're probably familiar with the Midway classic NFL Blitz. Authentic teams thrust into a deliberately inaccurate simulation with colorful gameplay and flashy graphics made the series a bombastic hit in arcades. And of course, the blisteringly difficult AI that would absolutely cheat drained quarters from avid players, making it enticing for arcade owners as well.&lt;/p&gt;
    &lt;p&gt;In this, we're not talking about beloved or hated arcade games, but instead the home console exclusive NFL Blitz Pro. This attempt to adapt to the console landscape was a commercial failure that lacked the charm and personality of the earlier titles. And we can confirm the popularity thing - this title had broken audio by default in Dolphin for over four years with no one making a formal bug report and only two forgotten comments throughout our community.&lt;/p&gt;
    &lt;p&gt;It wasn't until Billiard was testing games to see if they worked with Immediately Present XFB that we became aware that the game's sound was broken.&lt;/p&gt;
    &lt;p&gt;After doing some quick testing, Billiard realized that DSP-LLE resolved the issue, and with no further leads he made a setting adjustment and disabled HLE audio for this game by default in 2509-551.&lt;/p&gt;
    &lt;p&gt;The very next day, flacs saw the audio regression, found what build broke it, figured out why it was broken, and fixed the issue. NFL Blitz Pro is another game that uses the low-pass filter. In Dolphin, the low-pass filter was notoriously broken and left completely disabled for many, many years. During the process in which the feature was finally fixed and re-enabled, no one thought to test this game. If we had, we would have noticed that it hit an edge-case in the HLE implementation of the low-pass filter.&lt;/p&gt;
    &lt;p&gt;When the game reserves a region of memory, its memory allocator sets all bytes within the region to &lt;code&gt;0xAB&lt;/code&gt;.  This behavior isn't necessarily abnormal, as the initial state of the memory shouldn't matter. When using freshly allocated memory, it is best practice to first initialize it with sane default values before attempting to use it. Unfortunately, the developers of NFL Blitz Pro forgot to perfom this initialization when reserving memory for the game's audio engine. The MusyX library determines if the low-pass filter is enabled by checking if the appropriate bytes within its assigned memory region are not zero.  Because &lt;code&gt;0xAB&lt;/code&gt; is not zero, the library assumes that the low-pass filter should be enabled.&lt;/p&gt;
    &lt;p&gt;Because most audio related memory is still in the default state of &lt;code&gt;0xAB&lt;/code&gt;, the two low pass filter coefficients are also set to &lt;code&gt;0xABAB&lt;/code&gt;.  When DSP-HLE goes to calculate how much the low-pass filter should quiet or amplify things, it adds these values together.  For our purposes, the values are interpreted by the game as 16-bit fixed point values, so the coefficients would be roughly &lt;code&gt;1.341156&lt;/code&gt;.  These two values added together are supposed to add up to about &lt;code&gt;1.0&lt;/code&gt;, but in this case the result is roughly &lt;code&gt;2.682312&lt;/code&gt;.  DSP-HLE sees this number and boosts the volume accordingly, making things sound rather unpleasant.&lt;/p&gt;
    &lt;p/&gt;
    &lt;p&gt;The programmers were using uninitialized memory by accident, which is a game bug. Regardless of their intent, these were the values that the game used. So why was Dolphin broken? flacs figured out that the second filter value is actually a signed value. For a typical 16-bit fixed point value, this means positive values are &lt;code&gt;0x0000&lt;/code&gt; to &lt;code&gt;0x7FFF&lt;/code&gt;, and negative values are &lt;code&gt;0x8000&lt;/code&gt; to &lt;code&gt;0xFFFF&lt;/code&gt;.  &lt;code&gt;0xABAB&lt;/code&gt; is supposed to be interpreted as a negative value.&lt;/p&gt;
    &lt;p&gt;With the bug fixed, the equation changes to &lt;code&gt;1.34 + (-0.66)&lt;/code&gt;, giving us a coefficient sum of roughly &lt;code&gt;0.68&lt;/code&gt;.  The filter is still active and now lowering the volume of the game, but this is accurate to real hardware. It seems that the developers worked around the filter's unintentional activation by just making the game louder to compensate.  By handling these uninitialized values correctly, DSP-HLE now produces proper audio in this title.&lt;/p&gt;
    &lt;p/&gt;
    &lt;head rend="h3"&gt;This Release's Contributors...¬∂&lt;/head&gt;
    &lt;p&gt;Special thanks to all of the contributors that incremented Dolphin by 585 commits after Release 2509!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46414916</guid><pubDate>Sun, 28 Dec 2025 21:57:04 +0000</pubDate></item><item><title>Researchers Discover Molecular Difference in Autistic Brains</title><link>https://medicine.yale.edu/news-article/molecular-difference-in-autistic-brains/</link><description>&lt;doc fingerprint="f7fe1347089b1bdd"&gt;
  &lt;main&gt;
    &lt;p&gt;Yale School of Medicine (YSM) scientists have discovered a molecular difference in the brains of autistic people compared to their neurotypical counterparts.&lt;/p&gt;
    &lt;p&gt;Autism is a neurodevelopmental condition associated with behavioral differences including difficulties with social interaction, restrictive or intense interests, and repetitive movements or speech. But it‚Äôs not clear what makes autistic brains different.&lt;/p&gt;
    &lt;p&gt;Now, a new study in The American Journal of Psychiatry has found that brains of autistic people have fewer of a specific kind of receptor for glutamate, the most common excitatory neurotransmitter in the brain. The reduced availability of these receptors may be associated with various characteristics linked to autism.&lt;/p&gt;
    &lt;p&gt;‚ÄúWe have found this really important, never-before-understood difference in autism that is meaningful, has implications for intervention, and can help us understand autism in a more concrete way than we ever have before,‚Äù says James McPartland, PhD, Harris Professor of Child Psychiatry and Psychology in the Child Study Center at YSM and the study‚Äôs co-principal investigator.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46415129</guid><pubDate>Sun, 28 Dec 2025 22:23:33 +0000</pubDate></item><item><title>What an unprocessed photo looks like</title><link>https://maurycyz.com/misc/raw_photo/</link><description>&lt;doc fingerprint="e049e655b7503a7"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;What an unprocessed photo looks like:&lt;/head&gt;(Photography)&lt;p&gt;Here‚Äôs a photo of a Christmas tree, as my camera‚Äôs sensor sees it:&lt;/p&gt;&lt;p&gt;It‚Äôs not even black-and-white, it‚Äôs gray-and-gray. This is becuase while the ADC‚Äôs output can theoretically go from 0 to 16382, the actual data doesn‚Äôt cover that whole range:&lt;/p&gt;&lt;p&gt;The real range of ADC values is ~2110 to ~136000. Let‚Äôs set those values as the white and black in the image:&lt;/p&gt;&lt;p&gt;Vnew = (Vold - Black)/(White - Black)&lt;/p&gt;&lt;p&gt;Much better, but it‚Äôs still more monochromatic then I remember the tree being. Camera sensors aren‚Äôt actually able to see color: They only measure how much light hit each pixel.&lt;/p&gt;&lt;p&gt;In a color camera, the sensor is covered by a grid of alternating color filters:&lt;/p&gt;&lt;p&gt;Let‚Äôs color each pixel the same as the filter it‚Äôs looking through:&lt;/p&gt;&lt;p&gt;This version is more colorful, but each pixel only has one third of it‚Äôs RGB color. To fix this, I just averaged the values each pixel with it‚Äôs neighbors:&lt;/p&gt;&lt;p&gt;Applying this process to the whole photo gives the lights some color:&lt;/p&gt;&lt;p&gt;However, the image is still very dark. This is because monitors don‚Äôt have as much dynamic range as the human eye, or a camera sensor: Even if you are using an OLED, the screen still has some ambient light reflecting off of it and limiting how black it can get.&lt;/p&gt;&lt;p&gt;There‚Äôs also another, sneaker factor causing this:&lt;/p&gt;&lt;p&gt;Our perception of brightness is non-linear.&lt;/p&gt;&lt;p&gt;If brightness values are quantized, most of the ADC bins will be wasted on nearly identical shades of white while every other tone is crammed into the bottom. Because this is an inefficient use of memory, most color spaces assign extra bins to darker colors:&lt;/p&gt;&lt;p&gt;As a result of this, if the linear data is displayed directly, it will appear much darker then it should be.&lt;/p&gt;&lt;p&gt;Both problems can be solved by applying a non-linear curve to each color channel to brighten up the dark areas‚Ä¶ but this doesn‚Äôt quite work out:&lt;/p&gt;&lt;p&gt;Some of this green cast is caused by the camera sensor being intrinsically more sensitive to green light, but some of it is my fault: There are twice as many green pixels in the filter matrix. When combined with my rather naive demosaicing, this resulted in the green channel being boosted even higher.&lt;/p&gt;&lt;p&gt;In either case, it can fixed with proper white-balance: Equalize the channels by multiply each one with a constant.&lt;/p&gt;&lt;p&gt;However, because the image is now non-linear, I have to go back a step to do this. Here‚Äôs the dark image from before with all the values temporarily scaled up so I can see the problem:&lt;/p&gt;&lt;p&gt;‚Ä¶ here‚Äôs that image with the green taken down to mach the other channels:&lt;/p&gt;&lt;p&gt;‚Ä¶ and after re-applying the curve:&lt;/p&gt;&lt;p&gt;This is really just the bare minimum: I haven‚Äôt done any color calibration, the white balance isn‚Äôt perfect, there‚Äôs lots of noise that needs to be cleaned up‚Ä¶&lt;/p&gt;&lt;p&gt;Additionally, applying the curve to each color channel accidentally desaturated the highlights. This effect looks rather good ‚Äî and is what we‚Äôve come to expect from film ‚Äî but it‚Äôs has de-yellowed the star. It‚Äôs possible to separate the luminance and curve it while preserving color. On it‚Äôs own, this would make the LED Christmas lights into an overstaturated mess, but combining both methods can produce nice results.&lt;/p&gt;&lt;p&gt;For comparison, here‚Äôs the image my camera produced from the same data:&lt;/p&gt;&lt;p&gt;Far from being an ‚Äúunedited‚Äù photo: there‚Äôs a huge amount of math that‚Äôs gone into making an image that nicely represents what the subject looks like in person.&lt;/p&gt;&lt;p&gt;There‚Äôs nothing that happens when you adjust the contrast or white balance in editing software that the camera hasn‚Äôt done under the hood. The edited image isn‚Äôt ‚Äúfaker‚Äù then the original: they are different renditions of the same data.&lt;/p&gt;&lt;p&gt;In the end, replicating human perception is hard, and it‚Äôs made harder when constrained to the limitations of display technology or printed images. There‚Äôs nothing wrong with tweaking the image when the automated algorithms make the wrong call.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46415225</guid><pubDate>Sun, 28 Dec 2025 22:35:02 +0000</pubDate></item><item><title>As AI gobbles up chips, prices for devices may rise</title><link>https://www.npr.org/2025/12/28/nx-s1-5656190/ai-chips-memory-prices-ram</link><description>&lt;doc fingerprint="8a32d49868b221ec"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Memory loss: As AI gobbles up chips, prices for devices may rise&lt;/head&gt;
    &lt;head rend="h4"&gt;Memory loss: As AI gobbles up chips, prices for devices may rise&lt;/head&gt;
    &lt;p&gt;The world has a memory problem, thanks to artificial intelligence.&lt;/p&gt;
    &lt;p&gt;The explosion in AI-related cloud computing and data centers has led to so much demand for certain types of memory chips that now there's a shortage. The imbalance is expected to start affecting prices of all sorts of products powered by technology.&lt;/p&gt;
    &lt;p&gt;"I keep telling everybody that if you want a device, you buy it now," said Avril Wu, a senior research vice president at TrendForce, a Taiwan-based consultancy that tracks markets for computer components. "I myself bought an iPhone 17 already,"&lt;/p&gt;
    &lt;p&gt;The chips are known as RAM, or random access memory, and are crucial to making sure that things like smartphones, computers and game consoles run smoothly. Chips allow you to keep multiple tabs open in browsers, for instance, or watch videos without them being choppy.&lt;/p&gt;
    &lt;p&gt;Wu said TrendForce's data indicates that demand for RAM chips exceeds supply by 10% ‚Äì and it's growing so fast that manufacturers are having to shell out a lot more to buy them each month.&lt;/p&gt;
    &lt;p&gt;Wu said this quarter alone, they're paying 50% more than the previous quarter for the most common type of RAM, known as DRAM ‚Äì dynamic random access memory. And if producers want the chips sooner, they're paying two to three times more.&lt;/p&gt;
    &lt;p&gt;Wu expects DRAM prices to rise another 40% in the coming quarter, and she doesn't expect the prices to go down in 2026.&lt;/p&gt;
    &lt;head rend="h3"&gt;How AI is gobbling up memory&lt;/head&gt;
    &lt;p&gt;AI data centers require huge amounts of memory to accompany their cutting-edge graphics processing unit (GPU) microprocessors that train and operate AI models.&lt;/p&gt;
    &lt;p&gt;"AI workloads are built around memory," said Sanchit Vir Gogia, CEO of the tech advisory firm Greyhound Research.&lt;/p&gt;
    &lt;p&gt;What's more, AI companies are spending billions of dollars constructing data centers at warp speed around the world. It's the reason why Gogia says the demand for these chips isn't just a cyclical blip.&lt;/p&gt;
    &lt;p&gt;"AI has changed the nature of demand itself," he said. "Training and inference systems require large, persistent memory footprints, extreme bandwidth, and tight proximity to compute. You cannot dial this down without breaking performance."&lt;/p&gt;
    &lt;head rend="h3"&gt;More chips for AI means fewer chips for other products&lt;/head&gt;
    &lt;p&gt;Idaho-based Micron Technology is one of the world's top makers of RAM and it's benefited from this increase in demand. It reported better-than-expected quarterly earnings last week on the back of higher memory chip prices.&lt;/p&gt;
    &lt;p&gt;CEO Sanjay Mehrotra said the company expected the market to remain strong, as the AI boom continues apace. "We believe that the aggregate industry supply will remain substantially short of the demand for the foreseeable future," he said on a webcast after the earnings report.&lt;/p&gt;
    &lt;p&gt;Chipmakers like Micron have shifted production to meet as much of the lucrative AI-related demand for high-end memory as they can, according to analysts. That translates into fewer chips for other segments of the market ‚Äì personal computers, mobile phones, games and consumer products like TVs.&lt;/p&gt;
    &lt;p&gt;And that means higher costs. Dell Technologies Chief Operating Officer Jeff Clarke noted the higher costs on an earnings call on Nov. 25. For PC's, he said "I don't see how this will certainly not make its way into the customer base."&lt;/p&gt;
    &lt;p&gt;Analysts say there is no short-term fix.&lt;/p&gt;
    &lt;p&gt;Tech consultant Wu said the memory chip industry faces a significant bottleneck. By the end of 2026, she said, chip makers will have maxed out how much they can expand production in their current facilities.&lt;/p&gt;
    &lt;p&gt;She said the next new factory expected to come online is being built by Micron in Idaho. The company says it will be operational in 2027.&lt;/p&gt;
    &lt;p&gt;Expect suppliers to keep raising prices for the foreseeable future, Wu said.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46415338</guid><pubDate>Sun, 28 Dec 2025 22:52:21 +0000</pubDate></item><item><title>62 years in the making: NYC's newest water tunnel nears the finish line</title><link>https://ny1.com/nyc/all-boroughs/news/2025/11/09/water--dep--tunnels-</link><description>&lt;doc fingerprint="aac0a95e8b65c6b6"&gt;
  &lt;main&gt;
    &lt;p&gt;Turn on the tap, and water flows without a second thought. But deep beneath New York City, hundreds of feet below street level, workers are finishing a project that‚Äôs been under construction for more than half a century ‚Äî a massive water tunnel that will help keep that simple act possible for generations to come.&lt;/p&gt;
    &lt;p&gt;Tunnel No. 3, as it‚Äôs known, is one of the most ambitious infrastructure projects in the city‚Äôs history.&lt;/p&gt;
    &lt;p&gt;When complete, it will ensure New Yorkers continue to receive clean water from upstate reservoirs ‚Äî some more than 125 miles away ‚Äî while allowing long-overdue maintenance on the city‚Äôs two older tunnels, built in 1917 and 1936.&lt;/p&gt;
    &lt;p&gt;City Department of Environmental Protection Commissioner Rohit Aggarwala and DEP Portfolio Manager Lauren D‚ÄôAttile recently took an elevator nearly 800 feet down to see the progress for themselves.&lt;/p&gt;
    &lt;p&gt;‚ÄúIt‚Äôs not quite as far down as the Empire State Building is tall, but it‚Äôs getting there,‚Äù Aggarwala said during the 10-minute descent.&lt;/p&gt;
    &lt;p&gt;Down below, flashlights cut through the darkness as water dripped from the rock walls. Workers stood in waterproof boots along the cool, damp concrete ‚Äî the result of decades of digging, drilling and sealing off bare rock to create a watertight tunnel system.&lt;/p&gt;
    &lt;p&gt;‚ÄúWhen this tunnel was originally constructed, it was built by a tunnel boring machine, which is a very large piece of equipment with cutter heads on the front,‚Äù said D‚ÄôAttile. ‚ÄúWe drill the tunnel and after that we line that bare rock with a couple of feet of concrete ‚Äî so that‚Äôs what you‚Äôre seeing now, because this tunnel is complete.‚Äù&lt;/p&gt;
    &lt;p&gt;Construction on Tunnel No. 3 began in 1970.&lt;/p&gt;
    &lt;p&gt;The Bronx and Manhattan already receive water from it, and the final phase ‚Äî extending service to Brooklyn and Queens ‚Äî is expected to be completed by 2032.&lt;/p&gt;
    &lt;p&gt;‚ÄúThe project started in 1970, it will be finished in 2032 ‚Äî that‚Äôs 62 years to build this thing,‚Äù Aggarwala said. ‚ÄúBut a project like this is going to serve New York for two, three hundred years, who knows how much longer than that. Seems worth it. Totally worth it. It‚Äôs what makes the city work because we are constantly investing in our future.‚Äù&lt;/p&gt;
    &lt;p&gt;When it‚Äôs complete, the DEP will finally be able to take the older tunnels offline for repairs ‚Äî a step city engineers have waited decades to take.&lt;/p&gt;
    &lt;p&gt;Above ground, New Yorkers will keep turning on their faucets, washing dishes, and filling glasses ‚Äî rarely thinking about the billion gallons of water flowing through the underground arteries that make city life possible.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46415426</guid><pubDate>Sun, 28 Dec 2025 23:05:53 +0000</pubDate></item><item><title>CEOs are hugely expensive. Why not automate them?</title><link>https://www.newstatesman.com/business/companies/2023/05/ceos-salaries-expensive-automate-robots</link><description>&lt;doc fingerprint="6f04d93f3c6713e8"&gt;
  &lt;main&gt;
    &lt;p&gt;On Wednesday 31 May, it was reported that Alex Mahon, CEO of Channel 4, could receive record annual pay of ¬£1.4m. This article was originally published on 26 April 2021 and asks, as Executive pay continues to rise, does a company need a CEO at all?&lt;/p&gt;
    &lt;p&gt;Over the next two weeks, the boards of BAE Systems, AstraZeneca, Glencore, Flutter Entertainment and the London Stock Exchange all face the possibility of shareholder revolts over executive pay at their forthcoming annual general meetings (AGMs). As the AGM season begins, there is a particular focus on pay.&lt;/p&gt;
    &lt;p&gt;Executive pay is often the most contentious item at an AGM, but this year is clearly exceptional. The people running companies that have been severely impacted by Covid-19 can‚Äôt be blamed for the devastation of their revenues by the pandemic, but they also can‚Äôt take credit for the government stimulus that has kept them afloat. Last week, for example, nearly 40 per cent of shareholders in the estate agents Foxtons voted against its chief executive officer, Nicholas Budden, receiving a bonus of just under ¬£1m; Foxtons has received about ¬£7m in direct government assistance and is benefiting from the government‚Äôs continued inflation of the housing market. The person who has done most to ensure Foxtons‚Äô ongoing good fortune is not Nicholas Budden but Rishi Sunak.&lt;/p&gt;
    &lt;p&gt;Under the Enterprise and Regulatory Reform Act, executive pay is voted on at least every three years, and this process forces shareholders and the public to confront how much the people at the top take home. Tim Steiner, the highest-paid CEO in the FTSE 100, was paid ¬£58.7m in 2019 for running Ocado, which is 2,605 times the median income of his employees for that year, while the average FTSE100 CEO makes more than ¬£15,000 a day.&lt;/p&gt;
    &lt;p&gt;As the High Pay Centre‚Äôs annual assessment of CEO pay points out, a top-heavy wage bill extends beyond the CEO, and could be unsustainable for any company this year. ‚ÄúWhen one considers high earners beyond the CEO‚Äù, says the report, ‚Äùthere is actually quite significant potential for companies to safeguard jobs and incomes by asking higher-paid staff to make sacrifices‚Äù.&lt;/p&gt;
    &lt;p&gt;In the longer term, as companies commit to greater automation of many roles, it‚Äôs pertinent to ask whether a company needs a CEO at all.&lt;/p&gt;
    &lt;p&gt;A few weeks ago Christine Carrillo, an American tech CEO, raised this question herself when she tweeted a spectacularly tone-deaf appreciation of her executive assistant, whose work allows Carrillo to ‚Äúwrite [and] surf every day‚Äù as well as ‚Äúcook dinner and read every night‚Äù. In Carrillo‚Äôs unusually frank description of the work her EA does ‚Äì most of her emails, most of the work on fundraising, playbooks, operations, recruitment, research, updating investors, invoicing ‚Äúand so much more‚Äù ‚Äì she guessed that this unnamed worker ‚Äúsaves me 60% of time‚Äù.&lt;/p&gt;
    &lt;p&gt;Predictably, a horde arrived to point out that if someone else is doing 60 per cent of Carrillo‚Äôs job, they should be paid 50 per cent more than her. But as Carrillo ‚Äì with a frankly breathtaking lack of self-awareness ‚Äì informed another commenter, her EA is based in the Philippines. The main (and often the only) reason to outsource a role is to pay less for it.&lt;/p&gt;
    &lt;p&gt;[See also: The scourge of greedflation]&lt;/p&gt;
    &lt;p&gt;If most of a CEO‚Äôs job can be outsourced, this suggests it could also be automated. But while companies are racing to automate entry- and mid-level roles, senior executives and decision makers show much less interest in automating themselves.&lt;/p&gt;
    &lt;p&gt;There‚Äôs a good argument for automating from the top rather than from the bottom. As we know from the annotated copy of Thinking, Fast and Slow that sits (I assume) on every CEO‚Äôs Isamu Noguchi nightstand, human decision-making is the product of irrational biases and assumptions. This is one of the reasons strategy is so difficult, and roles that involve strategic decision-making are so well paid. But the difficulty of making genuinely rational strategic decisions, and the cost of the people who do so, are also good reasons to hand this work over to software.&lt;/p&gt;
    &lt;p&gt;Automating jobs can be risky, especially in public-facing roles. After Microsoft sacked a large team of journalists in 2020 in order to replace them with AI, it almost immediately had to contend with the PR disaster of the software‚Äôs failure to distinguish between two women of colour. Amazon had to abandon its AI recruitment tool after it learned to discriminate against women. And when GPT-3, one of the most advanced AI language models, was used as a medical chatbot in 2020, it responded to a (simulated) patient presenting with suicidal ideation by telling them to kill themselves.&lt;/p&gt;
    &lt;p&gt;What links these examples is that they were all attempts to automate the kind of work that happens without being scrutinised by lots of other people in a company. Top-level strategic decisions are different. They are usually debated before they‚Äôre put into practice ‚Äì unless, and this is just another reason to automate them, employees feel they can‚Äôt speak up for fear of incurring the CEO‚Äôs displeasure.&lt;/p&gt;
    &lt;p&gt;Where automated management ‚Äì or ‚Äúdecision intelligence‚Äù, as Google and IBM call it ‚Äì has been deployed, it‚Äôs produced impressive results. Hong Kong‚Äôs mass transit system put software in charge of scheduling its maintenance in 2004, and enjoys a reputation as one of the world‚Äôs most punctual and best-run metros.&lt;/p&gt;
    &lt;p&gt;Clearly, chief execs didn‚Äôt get where they are today by volunteering to clear out their corner offices and hand over their caviar spittoons to robots. But management is a very large variable cost that only seems to increase ‚Äì Persimmon‚Äôs bonus scheme paid out half a billion pounds to 150 execs in a single year ‚Äì while technology moves in the other direction, becoming cheaper and more reliable over time.&lt;/p&gt;
    &lt;p&gt;It is often asked whether CEO pay is fair or ethical. But company owners and investors should be asking if their top management could be done well by a machine ‚Äì and if so, why is it so expensive?&lt;/p&gt;
    &lt;p&gt;[See also: The milkman on a mission]&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46415488</guid><pubDate>Sun, 28 Dec 2025 23:17:07 +0000</pubDate></item></channel></rss>