<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Sun, 07 Sep 2025 19:06:35 +0000</lastBuildDate><item><title>SQLite's File Format</title><link>https://www.sqlite.org/fileformat.html</link><description>&lt;doc fingerprint="e01d281f55e0299b"&gt;
  &lt;main&gt;
    &lt;p&gt;This document describes and defines the on-disk database file format used by all releases of SQLite since version 3.0.0 (2004-06-18).&lt;/p&gt;
    &lt;p&gt;The complete state of an SQLite database is usually contained in a single file on disk called the "main database file".&lt;/p&gt;
    &lt;p&gt;During a transaction, SQLite stores additional information in a second file called the "rollback journal", or if SQLite is in WAL mode, a write-ahead log file.&lt;/p&gt;
    &lt;p&gt;If the application or host computer crashes before the transaction completes, then the rollback journal or write-ahead log contains information needed to restore the main database file to a consistent state. When a rollback journal or write-ahead log contains information necessary for recovering the state of the database, they are called a "hot journal" or "hot WAL file". Hot journals and WAL files are only a factor during error recovery scenarios and so are uncommon, but they are part of the state of an SQLite database and so cannot be ignored. This document defines the format of a rollback journal and the write-ahead log file, but the focus is on the main database file.&lt;/p&gt;
    &lt;p&gt;The main database file consists of one or more pages. The size of a page is a power of two between 512 and 65536 inclusive. All pages within the same database are the same size. The page size for a database file is determined by the 2-byte integer located at an offset of 16 bytes from the beginning of the database file.&lt;/p&gt;
    &lt;p&gt;Pages are numbered beginning with 1. The maximum page number is 4294967294 (232 - 2). The minimum size SQLite database is a single 512-byte page. The maximum size database would be 4294967294 pages at 65536 bytes per page or 281,474,976,579,584 bytes (about 281 terabytes). Usually SQLite will hit the maximum file size limit of the underlying filesystem or disk hardware long before it hits its own internal size limit.&lt;/p&gt;
    &lt;p&gt;In common use, SQLite databases tend to range in size from a few kilobytes to a few gigabytes, though terabyte-size SQLite databases are known to exist in production.&lt;/p&gt;
    &lt;p&gt;At any point in time, every page in the main database has a single use which is one of the following:&lt;/p&gt;
    &lt;p&gt;All reads from and writes to the main database file begin at a page boundary and all writes are an integer number of pages in size. Reads are also usually an integer number of pages in size, with the one exception that when the database is first opened, the first 100 bytes of the database file (the database file header) are read as a sub-page size unit.&lt;/p&gt;
    &lt;p&gt;The first 100 bytes of the database file comprise the database file header. The database file header is divided into fields as shown by the table below. All multibyte fields in the database file header are stored with the most significant byte first (big-endian).&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Offset&lt;/cell&gt;
        &lt;cell role="head"&gt;Size&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;16&lt;/cell&gt;
        &lt;cell&gt;The header string: "SQLite format 3\000"&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;16&lt;/cell&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;The database page size in bytes. Must be a power of two between 512 and 32768 inclusive, or the value 1 representing a page size of 65536.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;18&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;File format write version. 1 for legacy; 2 for WAL.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;19&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;File format read version. 1 for legacy; 2 for WAL.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;20&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;Bytes of unused "reserved" space at the end of each page. Usually 0.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;21&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;Maximum embedded payload fraction. Must be 64.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;22&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;Minimum embedded payload fraction. Must be 32.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;23&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;Leaf payload fraction. Must be 32.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;24&lt;/cell&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;File change counter.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;28&lt;/cell&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;Size of the database file in pages. The "in-header database size".&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;32&lt;/cell&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;Page number of the first freelist trunk page.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;36&lt;/cell&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;Total number of freelist pages.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;40&lt;/cell&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;The schema cookie.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;44&lt;/cell&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;The schema format number. Supported schema formats are 1, 2, 3, and 4.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;48&lt;/cell&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;Default page cache size.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;52&lt;/cell&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;The page number of the largest root b-tree page when in auto-vacuum or incremental-vacuum modes, or zero otherwise.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;56&lt;/cell&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;The database text encoding. A value of 1 means UTF-8. A value of 2 means UTF-16le. A value of 3 means UTF-16be.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;60&lt;/cell&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;The "user version" as read and set by the user_version pragma.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;64&lt;/cell&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;True (non-zero) for incremental-vacuum mode. False (zero) otherwise.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;68&lt;/cell&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;The "Application ID" set by PRAGMA application_id.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;72&lt;/cell&gt;
        &lt;cell&gt;20&lt;/cell&gt;
        &lt;cell&gt;Reserved for expansion. Must be zero.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;92&lt;/cell&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;The version-valid-for number.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;96&lt;/cell&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;SQLITE_VERSION_NUMBER&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Every valid SQLite database file begins with the following 16 bytes (in hex): 53 51 4c 69 74 65 20 66 6f 72 6d 61 74 20 33 00. This byte sequence corresponds to the UTF-8 string "SQLite format 3" including the nul terminator character at the end.&lt;/p&gt;
    &lt;p&gt;The two-byte value beginning at offset 16 determines the page size of the database. For SQLite versions 3.7.0.1 (2010-08-04) and earlier, this value is interpreted as a big-endian integer and must be a power of two between 512 and 32768, inclusive. Beginning with SQLite version 3.7.1 (2010-08-23), a page size of 65536 bytes is supported. The value 65536 will not fit in a two-byte integer, so to specify a 65536-byte page size, the value at offset 16 is 0x00 0x01. This value can be interpreted as a big-endian 1 and thought of as a magic number to represent the 65536 page size. Or one can view the two-byte field as a little endian number and say that it represents the page size divided by 256. These two interpretations of the page-size field are equivalent.&lt;/p&gt;
    &lt;p&gt;The file format write version and file format read version at offsets 18 and 19 are intended to allow for enhancements of the file format in future versions of SQLite. In current versions of SQLite, both of these values are 1 for rollback journalling modes and 2 for WAL journalling mode. If a version of SQLite coded to the current file format specification encounters a database file where the read version is 1 or 2 but the write version is greater than 2, then the database file must be treated as read-only. If a database file with a read version greater than 2 is encountered, then that database cannot be read or written.&lt;/p&gt;
    &lt;p&gt;SQLite has the ability to set aside a small number of extra bytes at the end of every page for use by extensions. These extra bytes are used, for example, by the SQLite Encryption Extension to store a nonce and/or cryptographic checksum associated with each page. The "reserved space" size in the 1-byte integer at offset 20 is the number of bytes of space at the end of each page to reserve for extensions. This value is usually 0. The value can be odd.&lt;/p&gt;
    &lt;p&gt;The "usable size" of a database page is the page size specified by the 2-byte integer at offset 16 in the header less the "reserved" space size recorded in the 1-byte integer at offset 20 in the header. The usable size of a page might be an odd number. However, the usable size is not allowed to be less than 480. In other words, if the page size is 512, then the reserved space size cannot exceed 32.&lt;/p&gt;
    &lt;p&gt;The maximum and minimum embedded payload fractions and the leaf payload fraction values must be 64, 32, and 32. These values were originally intended to be tunable parameters that could be used to modify the storage format of the b-tree algorithm. However, that functionality is not supported and there are no current plans to add support in the future. Hence, these three bytes are fixed at the values specified.&lt;/p&gt;
    &lt;p&gt;The file change counter is a 4-byte big-endian integer at offset 24 that is incremented whenever the database file is unlocked after having been modified. When two or more processes are reading the same database file, each process can detect database changes from other processes by monitoring the change counter. A process will normally want to flush its database page cache when another process modified the database, since the cache has become stale. The file change counter facilitates this.&lt;/p&gt;
    &lt;p&gt;In WAL mode, changes to the database are detected using the wal-index and so the change counter is not needed. Hence, the change counter might not be incremented on each transaction in WAL mode.&lt;/p&gt;
    &lt;p&gt;The 4-byte big-endian integer at offset 28 into the header stores the size of the database file in pages. If this in-header datasize size is not valid (see the next paragraph), then the database size is computed by looking at the actual size of the database file. Older versions of SQLite ignored the in-header database size and used the actual file size exclusively. Newer versions of SQLite use the in-header database size if it is available but fall back to the actual file size if the in-header database size is not valid.&lt;/p&gt;
    &lt;p&gt;The in-header database size is only considered to be valid if it is non-zero and if the 4-byte change counter at offset 24 exactly matches the 4-byte version-valid-for number at offset 92. The in-header database size is always valid when the database is only modified using recent versions of SQLite, versions 3.7.0 (2010-07-21) and later. If a legacy version of SQLite writes to the database, it will not know to update the in-header database size and so the in-header database size could be incorrect. But legacy versions of SQLite will also leave the version-valid-for number at offset 92 unchanged so it will not match the change-counter. Hence, invalid in-header database sizes can be detected (and ignored) by observing when the change-counter does not match the version-valid-for number.&lt;/p&gt;
    &lt;p&gt;Unused pages in the database file are stored on a freelist. The 4-byte big-endian integer at offset 32 stores the page number of the first page of the freelist, or zero if the freelist is empty. The 4-byte big-endian integer at offset 36 stores the total number of pages on the freelist.&lt;/p&gt;
    &lt;p&gt;The schema cookie is a 4-byte big-endian integer at offset 40 that is incremented whenever the database schema changes. A prepared statement is compiled against a specific version of the database schema. When the database schema changes, the statement must be reprepared. When a prepared statement runs, it first checks the schema cookie to ensure the value is the same as when the statement was prepared and if the schema cookie has changed, the statement either automatically reprepares and reruns or it aborts with an SQLITE_SCHEMA error.&lt;/p&gt;
    &lt;p&gt;The schema format number is a 4-byte big-endian integer at offset 44. The schema format number is similar to the file format read and write version numbers at offsets 18 and 19 except that the schema format number refers to the high-level SQL formatting rather than the low-level b-tree formatting. Four schema format numbers are currently defined:&lt;/p&gt;
    &lt;p&gt;New database files created by SQLite use format 4 by default. The SQLITE_DBCONFIG_LEGACY_FILE_FORMAT option for the sqlite3_db_config() C-language interface can be used to cause SQLite to create new database files using format 1. The format version number can be made to default to 1 instead of 4 by setting SQLITE_DEFAULT_FILE_FORMAT=1 at compile-time.&lt;/p&gt;
    &lt;p&gt;If the database is completely empty, if it has no schema, then the schema format number can be zero.&lt;/p&gt;
    &lt;p&gt;The 4-byte big-endian signed integer at offset 48 is the suggested cache size in pages for the database file. The value is a suggestion only and SQLite is under no obligation to honor it. The absolute value of the integer is used as the suggested size. The suggested cache size can be set using the default_cache_size pragma.&lt;/p&gt;
    &lt;p&gt;The two 4-byte big-endian integers at offsets 52 and 64 are used to manage the auto_vacuum and incremental_vacuum modes. If the integer at offset 52 is zero then pointer-map (ptrmap) pages are omitted from the database file and neither auto_vacuum nor incremental_vacuum are supported. If the integer at offset 52 is non-zero then it is the page number of the largest root page in the database file, the database file will contain ptrmap pages, and the mode must be either auto_vacuum or incremental_vacuum. In this latter case, the integer at offset 64 is true for incremental_vacuum and false for auto_vacuum. If the integer at offset 52 is zero then the integer at offset 64 must also be zero.&lt;/p&gt;
    &lt;p&gt;The 4-byte big-endian integer at offset 56 determines the encoding used for all text strings stored in the database. A value of 1 means UTF-8. A value of 2 means UTF-16le. A value of 3 means UTF-16be. No other values are allowed. The sqlite3.h header file defines C-preprocessor macros SQLITE_UTF8 as 1, SQLITE_UTF16LE as 2, and SQLITE_UTF16BE as 3, to use in place of the numeric codes for the text encoding.&lt;/p&gt;
    &lt;p&gt;The 4-byte big-endian integer at offset 60 is the user version which is set and queried by the user_version pragma. The user version is not used by SQLite.&lt;/p&gt;
    &lt;p&gt;The 4-byte big-endian integer at offset 68 is an "Application ID" that can be set by the PRAGMA application_id command in order to identify the database as belonging to or associated with a particular application. The application ID is intended for database files used as an application file-format. The application ID can be used by utilities such as file(1) to determine the specific file type rather than just reporting "SQLite3 Database". A list of assigned application IDs can be seen by consulting the magic.txt file in the SQLite source repository.&lt;/p&gt;
    &lt;p&gt;The 4-byte big-endian integer at offset 96 stores the SQLITE_VERSION_NUMBER value for the SQLite library that most recently modified the database file. The 4-byte big-endian integer at offset 92 is the value of the change counter when the version number was stored. The integer at offset 92 indicates which transaction the version number is valid for and is sometimes called the "version-valid-for number".&lt;/p&gt;
    &lt;p&gt;All other bytes of the database file header are reserved for future expansion and must be set to zero.&lt;/p&gt;
    &lt;p&gt;The lock-byte page is the single page of the database file that contains the bytes at offsets between 1073741824 and 1073742335, inclusive. A database file that is less than or equal to 1073741824 bytes in size contains no lock-byte page. A database file larger than 1073741824 contains exactly one lock-byte page.&lt;/p&gt;
    &lt;p&gt;The lock-byte page is set aside for use by the operating-system specific VFS implementation in implementing the database file locking primitives. SQLite does not use the lock-byte page. The SQLite core will never read or write the lock-byte page, though operating-system specific VFS implementations may choose to read or write bytes on the lock-byte page according to the needs and proclivities of the underlying system. The unix and win32 VFS implementations that come built into SQLite do not write to the lock-byte page, but third-party VFS implementations for other operating systems might.&lt;/p&gt;
    &lt;p&gt;The lock-byte page arose from the need to support Win95 which was the predominant operating system when this file format was designed and which only supported mandatory file locking. All modern operating systems that we know of support advisory file locking, and so the lock-byte page is not really needed any more, but is retained for backwards compatibility.&lt;/p&gt;
    &lt;p&gt;A database file might contain one or more pages that are not in active use. Unused pages can come about, for example, when information is deleted from the database. Unused pages are stored on the freelist and are reused when additional pages are required.&lt;/p&gt;
    &lt;p&gt;The freelist is organized as a linked list of freelist trunk pages with each trunk page containing page numbers for zero or more freelist leaf pages.&lt;/p&gt;
    &lt;p&gt;A freelist trunk page consists of an array of 4-byte big-endian integers. The size of the array is as many integers as will fit in the usable space of a page. The minimum usable space is 480 bytes so the array will always be at least 120 entries in length. The first integer on a freelist trunk page is the page number of the next freelist trunk page in the list or zero if this is the last freelist trunk page. The second integer on a freelist trunk page is the number of leaf page pointers to follow. Call the second integer on a freelist trunk page L. If L is greater than zero then integers with array indexes between 2 and L+1 inclusive contain page numbers for freelist leaf pages.&lt;/p&gt;
    &lt;p&gt;Freelist leaf pages contain no information. SQLite avoids reading or writing freelist leaf pages in order to reduce disk I/O.&lt;/p&gt;
    &lt;p&gt;A bug in SQLite versions prior to 3.6.0 (2008-07-16) caused the database to be reported as corrupt if any of the last 6 entries in the freelist trunk page array contained non-zero values. Newer versions of SQLite do not have this problem. However, newer versions of SQLite still avoid using the last six entries in the freelist trunk page array in order that database files created by newer versions of SQLite can be read by older versions of SQLite.&lt;/p&gt;
    &lt;p&gt;The number of freelist pages is stored as a 4-byte big-endian integer in the database header at an offset of 36 from the beginning of the file. The database header also stores the page number of the first freelist trunk page as a 4-byte big-endian integer at an offset of 32 from the beginning of the file.&lt;/p&gt;
    &lt;p&gt;The b-tree algorithm provides key/data storage with unique and ordered keys on page-oriented storage devices. For background information on b-trees, see Knuth, The Art Of Computer Programming, Volume 3 "Sorting and Searching", pages 471-479. Two variants of b-trees are used by SQLite. "Table b-trees" use a 64-bit signed integer key and store all data in the leaves. "Index b-trees" use arbitrary keys and store no data at all.&lt;/p&gt;
    &lt;p&gt;A b-tree page is either an interior page or a leaf page. A leaf page contains keys and in the case of a table b-tree each key has associated data. An interior page contains K keys together with K+1 pointers to child b-tree pages. A "pointer" in an interior b-tree page is just the 32-bit unsigned integer page number of the child page.&lt;/p&gt;
    &lt;p&gt;The number of keys on an interior b-tree page, K, is almost always at least 2 and is usually much more than 2. The only exception is when page 1 is an interior b-tree page. Page 1 has 100 fewer bytes of storage space available, due to the presence of the database header at the beginning of that page, and so sometimes (rarely) if page 1 is an interior b-tree page, it can end up holding just a single key. In all other cases, K is 2 or more. The upper bound on K is as many keys as will fit on the page. Large keys on index b-trees are split up into overflow pages so that no single key uses more than one fourth of the available storage space on the page and hence every internal page is able to store at least 4 keys. The integer keys of table b-trees are never large enough to require overflow, so key overflow only occurs on index b-trees.&lt;/p&gt;
    &lt;p&gt;Define the depth of a leaf b-tree to be 1 and the depth of any interior b-tree to be one more than the maximum depth of any of its children. In a well-formed database, all children of an interior b-tree have the same depth.&lt;/p&gt;
    &lt;p&gt;In an interior b-tree page, the pointers and keys logically alternate with a pointer on both ends. (The previous sentence is to be understood conceptually - the actual layout of the keys and pointers within the page is more complicated and will be described in the sequel.) All keys within the same page are unique and are logically organized in ascending order from left to right. (Again, this ordering is logical, not physical. The actual location of keys within the page is arbitrary.) For any key X, pointers to the left of a X refer to b-tree pages on which all keys are less than or equal to X. Pointers to the right of X refer to pages where all keys are greater than X.&lt;/p&gt;
    &lt;p&gt;Within an interior b-tree page, each key and the pointer to its immediate left are combined into a structure called a "cell". The right-most pointer is held separately. A leaf b-tree page has no pointers, but it still uses the cell structure to hold keys for index b-trees or keys and content for table b-trees. Data is also contained in the cell.&lt;/p&gt;
    &lt;p&gt;Every b-tree page has at most one parent b-tree page. A b-tree page without a parent is called a root page. A root b-tree page together with the closure of its children form a complete b-tree. It is possible (and in fact rather common) to have a complete b-tree that consists of a single page that is both a leaf and the root. Because there are pointers from parents to children, every page of a complete b-tree can be located if only the root page is known. Hence, b-trees are identified by their root page number.&lt;/p&gt;
    &lt;p&gt;A b-tree page is either a table b-tree page or an index b-tree page. All pages within each complete b-tree are of the same type: either table or index. There is one table b-tree in the database file for each rowid table in the database schema, including system tables such as sqlite_schema. There is one index b-tree in the database file for each index in the schema, including implied indexes created by uniqueness constraints. There are no b-trees associated with virtual tables. Specific virtual table implementations might make use of shadow tables for storage, but those shadow tables will have separate entries in the database schema. WITHOUT ROWID tables use index b-trees rather than table b-trees, so there is one index b-tree in the database file for each WITHOUT ROWID table. The b-tree corresponding to the sqlite_schema table is always a table b-tree and always has a root page of 1. The sqlite_schema table contains the root page number for every other table and index in the database file.&lt;/p&gt;
    &lt;p&gt;Each entry in a table b-tree consists of a 64-bit signed integer key and up to 2147483647 bytes of arbitrary data. (The key of a table b-tree corresponds to the rowid of the SQL table that the b-tree implements.) Interior table b-trees hold only keys and pointers to children. All data is contained in the table b-tree leaves.&lt;/p&gt;
    &lt;p&gt;Each entry in an index b-tree consists of an arbitrary key of up to 2147483647 bytes in length and no data.&lt;/p&gt;
    &lt;p&gt;Define the "payload" of a cell to be the arbitrary length section of the cell. For an index b-tree, the key is always arbitrary in length and hence the payload is the key. There are no arbitrary length elements in the cells of interior table b-tree pages and so those cells have no payload. Table b-tree leaf pages contain arbitrary length content and so for cells on those pages the payload is the content.&lt;/p&gt;
    &lt;p&gt;When the size of payload for a cell exceeds a certain threshold (to be defined later) then only the first few bytes of the payload are stored on the b-tree page and the balance is stored in a linked list of content overflow pages.&lt;/p&gt;
    &lt;p&gt;A b-tree page is divided into regions in the following order:&lt;/p&gt;
    &lt;p&gt;The 100-byte database file header is found only on page 1, which is always a table b-tree page. All other b-tree pages in the database file omit this 100-byte header.&lt;/p&gt;
    &lt;p&gt;The reserved region is an area of unused space at the end of every page (except the locking page) that extensions can use to hold per-page information. The size of the reserved region is determined by the one-byte unsigned integer found at an offset of 20 into the database file header. The size of the reserved region is usually zero.&lt;/p&gt;
    &lt;p&gt;The b-tree page header is 8 bytes in size for leaf pages and 12 bytes for interior pages. All multibyte values in the page header are big-endian. The b-tree page header is composed of the following fields:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Offset&lt;/cell&gt;
        &lt;cell role="head"&gt;Size&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt; The one-byte flag at offset 0 indicating the b-tree page type.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;The two-byte integer at offset 1 gives the start of the first freeblock on the page, or is zero if there are no freeblocks.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;The two-byte integer at offset 3 gives the number of cells on the page.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;The two-byte integer at offset 5 designates the start of the cell content area. A zero value for this integer is interpreted as 65536.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;The one-byte integer at offset 7 gives the number of fragmented free bytes within the cell content area.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;The four-byte page number at offset 8 is the right-most pointer. This value appears in the header of interior b-tree pages only and is omitted from all other pages.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The cell pointer array of a b-tree page immediately follows the b-tree page header. Let K be the number of cells on the btree. The cell pointer array consists of K 2-byte integer offsets to the cell contents. The cell pointers are arranged in key order with left-most cell (the cell with the smallest key) first and the right-most cell (the cell with the largest key) last.&lt;/p&gt;
    &lt;p&gt;Cell content is stored in the cell content region of the b-tree page. SQLite strives to place cells as far toward the end of the b-tree page as it can, in order to leave space for future growth of the cell pointer array. The area in between the last cell pointer array entry and the beginning of the first cell is the unallocated region.&lt;/p&gt;
    &lt;p&gt;If a page contains no cells (which is only possible for a root page of a table that contains no rows) then the offset to the cell content area will equal the page size minus the bytes of reserved space. If the database uses a 65536-byte page size and the reserved space is zero (the usual value for reserved space) then the cell content offset of an empty page wants to be 65536. However, that integer is too large to be stored in a 2-byte unsigned integer, so a value of 0 is used in its place.&lt;/p&gt;
    &lt;p&gt;A freeblock is a structure used to identify unallocated space within a b-tree page. Freeblocks are organized as a chain. The first 2 bytes of a freeblock are a big-endian integer which is the offset in the b-tree page of the next freeblock in the chain, or zero if the freeblock is the last on the chain. The third and fourth bytes of each freeblock form a big-endian integer which is the size of the freeblock in bytes, including the 4-byte header. Freeblocks are always connected in order of increasing offset. The second field of the b-tree page header is the offset of the first freeblock, or zero if there are no freeblocks on the page. In a well-formed b-tree page, there will always be at least one cell before the first freeblock.&lt;/p&gt;
    &lt;p&gt;A freeblock requires at least 4 bytes of space. If there is an isolated group of 1, 2, or 3 unused bytes within the cell content area, those bytes comprise a fragment. The total number of bytes in all fragments is stored in the fifth field of the b-tree page header. In a well-formed b-tree page, the total number of bytes in fragments may not exceed 60.&lt;/p&gt;
    &lt;p&gt;The total amount of free space on a b-tree page consists of the size of the unallocated region plus the total size of all freeblocks plus the number of fragmented free bytes. SQLite may from time to time reorganize a b-tree page so that there are no freeblocks or fragment bytes, all unused bytes are contained in the unallocated space region, and all cells are packed tightly at the end of the page. This is called "defragmenting" the b-tree page.&lt;/p&gt;
    &lt;p&gt;A variable-length integer or "varint" is a static Huffman encoding of 64-bit twos-complement integers that uses less space for small positive values. A varint is between 1 and 9 bytes in length. The varint consists of either zero or more bytes which have the high-order bit set followed by a single byte with the high-order bit clear, or nine bytes, whichever is shorter. The lower seven bits of each of the first eight bytes and all 8 bits of the ninth byte are used to reconstruct the 64-bit twos-complement integer. Varints are big-endian: bits taken from the earlier byte of the varint are more significant than bits taken from the later bytes.&lt;/p&gt;
    &lt;p&gt;The format of a cell depends on which kind of b-tree page the cell appears on. The following table shows the elements of a cell, in order of appearance, for the various b-tree page types.&lt;/p&gt;
    &lt;p&gt;Table B-Tree Leaf Cell (header 0x0d):&lt;/p&gt;
    &lt;p&gt;Table B-Tree Interior Cell (header 0x05):&lt;/p&gt;
    &lt;p&gt;Index B-Tree Leaf Cell (header 0x0a):&lt;/p&gt;
    &lt;p&gt;Index B-Tree Interior Cell (header 0x02):&lt;/p&gt;
    &lt;p&gt;The information above can be recast into a table format as follows:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="6"&gt;
        &lt;cell role="head"&gt;Datatype&lt;/cell&gt;
        &lt;cell role="head"&gt;Appears in...&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;Table Leaf (0x0d)&lt;/cell&gt;
        &lt;cell&gt;Table Interior (0x05)&lt;/cell&gt;
        &lt;cell&gt;Index Leaf (0x0a)&lt;/cell&gt;
        &lt;cell&gt;Index Interior (0x02)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;4-byte integer&lt;/cell&gt;
        &lt;cell&gt;✔&lt;/cell&gt;
        &lt;cell&gt;✔&lt;/cell&gt;
        &lt;cell&gt;Page number of left child&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;varint&lt;/cell&gt;
        &lt;cell&gt;✔&lt;/cell&gt;
        &lt;cell&gt;✔&lt;/cell&gt;
        &lt;cell&gt;✔&lt;/cell&gt;
        &lt;cell&gt;Number of bytes of payload&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;varint&lt;/cell&gt;
        &lt;cell&gt;✔&lt;/cell&gt;
        &lt;cell&gt;✔&lt;/cell&gt;
        &lt;cell&gt;Rowid&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;byte array&lt;/cell&gt;
        &lt;cell&gt;✔&lt;/cell&gt;
        &lt;cell&gt;✔&lt;/cell&gt;
        &lt;cell&gt;✔&lt;/cell&gt;
        &lt;cell&gt;Payload&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;4-byte integer&lt;/cell&gt;
        &lt;cell&gt;✔&lt;/cell&gt;
        &lt;cell&gt;✔&lt;/cell&gt;
        &lt;cell&gt;✔&lt;/cell&gt;
        &lt;cell&gt;Page number of first overflow page&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The amount of payload that spills onto overflow pages also depends on the page type. For the following computations, let U be the usable size of a database page, the total page size less the reserved space at the end of each page. And let P be the payload size. In the following, symbol X represents the maximum amount of payload that can be stored directly on the b-tree page without spilling onto an overflow page and symbol M represents the minimum amount of payload that must be stored on the btree page before spilling is allowed.&lt;/p&gt;
    &lt;p&gt;Table B-Tree Leaf Cell:&lt;/p&gt;
    &lt;p&gt;Let X be U-35. If the payload size P is less than or equal to X then the entire payload is stored on the b-tree leaf page. Let M be ((U-12)*32/255)-23 and let K be M+((P-M)%(U-4)). If P is greater than X then the number of bytes stored on the table b-tree leaf page is K if K is less or equal to X or M otherwise. The number of bytes stored on the leaf page is never less than M.&lt;/p&gt;
    &lt;p&gt;Table B-Tree Interior Cell:&lt;/p&gt;
    &lt;p&gt;Interior pages of table b-trees have no payload and so there is never any payload to spill.&lt;/p&gt;
    &lt;p&gt;Index B-Tree Leaf Or Interior Cell:&lt;/p&gt;
    &lt;p&gt;Let X be ((U-12)*64/255)-23. If the payload size P is less than or equal to X then the entire payload is stored on the b-tree page. Let M be ((U-12)*32/255)-23 and let K be M+((P-M)%(U-4)). If P is greater than X then the number of bytes stored on the index b-tree page is K if K is less than or equal to X or M otherwise. The number of bytes stored on the index page is never less than M.&lt;/p&gt;
    &lt;p&gt;Here is an alternative description of the same computation:&lt;/p&gt;
    &lt;p&gt;The overflow thresholds are designed to give a minimum fanout of 4 for index b-trees and to make sure enough of the payload is on the b-tree page that the record header can usually be accessed without consulting an overflow page. In hindsight, the designer of the SQLite b-tree logic realized that these thresholds could have been made much simpler. However, the computations cannot be changed without resulting in an incompatible file format. And the current computations work well, even if they are a little complex.&lt;/p&gt;
    &lt;p&gt;When the payload of a b-tree cell is too large for the b-tree page, the surplus is spilled onto overflow pages. Overflow pages form a linked list. The first four bytes of each overflow page are a big-endian integer which is the page number of the next page in the chain, or zero for the final page in the chain. The fifth byte through the last usable byte are used to hold overflow content.&lt;/p&gt;
    &lt;p&gt;Pointer map or ptrmap pages are extra pages inserted into the database to make the operation of auto_vacuum and incremental_vacuum modes more efficient. Other page types in the database typically have pointers from parent to child. For example, an interior b-tree page contains pointers to its child b-tree pages and an overflow chain has a pointer from earlier to later links in the chain. A ptrmap page contains linkage information going in the opposite direction, from child to parent.&lt;/p&gt;
    &lt;p&gt;Ptrmap pages must exist in any database file which has a non-zero largest root b-tree page value at offset 52 in the database header. If the largest root b-tree page value is zero, then the database must not contain ptrmap pages.&lt;/p&gt;
    &lt;p&gt;In a database with ptrmap pages, the first ptrmap page is page 2. A ptrmap page consists of an array of 5-byte entries. Let J be the number of 5-byte entries that will fit in the usable space of a page. (In other words, J=U/5.) The first ptrmap page will contain back pointer information for pages 3 through J+2, inclusive. The second pointer map page will be on page J+3 and that ptrmap page will provide back pointer information for pages J+4 through 2*J+3 inclusive. And so forth for the entire database file.&lt;/p&gt;
    &lt;p&gt;In a database that uses ptrmap pages, all pages at locations identified by the computation in the previous paragraph must be ptrmap page and no other page may be a ptrmap page. Except, if the byte-lock page happens to fall on the same page number as a ptrmap page, then the ptrmap is moved to the following page for that one case.&lt;/p&gt;
    &lt;p&gt;Each 5-byte entry on a ptrmap page provides back-link information about one of the pages that immediately follow the pointer map. If page B is a ptrmap page then back-link information about page B+1 is provided by the first entry on the pointer map. Information about page B+2 is provided by the second entry. And so forth.&lt;/p&gt;
    &lt;p&gt;Each 5-byte ptrmap entry consists of one byte of "page type" information followed by a 4-byte big-endian page number. Five page types are recognized:&lt;/p&gt;
    &lt;p&gt;In any database file that contains ptrmap pages, all b-tree root pages must come before any non-root b-tree page, cell payload overflow page, or freelist page. This restriction ensures that a root page will never be moved during an auto-vacuum or incremental-vacuum. The auto-vacuum logic does not know how to update the root_page field of the sqlite_schema table and so it is necessary to prevent root pages from being moved during an auto-vacuum in order to preserve the integrity of the sqlite_schema table. Root pages are moved to the beginning of the database file by the CREATE TABLE, CREATE INDEX, DROP TABLE, and DROP INDEX operations.&lt;/p&gt;
    &lt;p&gt;The foregoing text describes low-level aspects of the SQLite file format. The b-tree mechanism provides a powerful and efficient means of accessing a large data set. This section will describe how the low-level b-tree layer is used to implement higher-level SQL capabilities.&lt;/p&gt;
    &lt;p&gt;The data for a table b-tree leaf page and the key of an index b-tree page was characterized above as an arbitrary sequence of bytes. The prior discussion mentioned one key being less than another, but did not define what "less than" meant. The current section will address these omissions.&lt;/p&gt;
    &lt;p&gt;Payload, either table b-tree data or index b-tree keys, is always in the "record format". The record format defines a sequence of values corresponding to columns in a table or index. The record format specifies the number of columns, the datatype of each column, and the content of each column.&lt;/p&gt;
    &lt;p&gt;The record format makes extensive use of the variable-length integer or varint representation of 64-bit signed integers defined above.&lt;/p&gt;
    &lt;p&gt;A record contains a header and a body, in that order. The header begins with a single varint which determines the total number of bytes in the header. The varint value is the size of the header in bytes including the size varint itself. Following the size varint are one or more additional varints, one per column. These additional varints are called "serial type" numbers and determine the datatype of each column, according to the following chart:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Serial Type&lt;/cell&gt;
        &lt;cell role="head"&gt;Content Size&lt;/cell&gt;
        &lt;cell role="head"&gt;Meaning&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;Value is a NULL.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;Value is an 8-bit twos-complement integer.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;Value is a big-endian 16-bit twos-complement integer.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;Value is a big-endian 24-bit twos-complement integer.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;Value is a big-endian 32-bit twos-complement integer.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;Value is a big-endian 48-bit twos-complement integer.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;Value is a big-endian 64-bit twos-complement integer.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;Value is a big-endian IEEE 754-2008 64-bit floating point number.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;Value is the integer 0. (Only available for schema format 4 and higher.)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;9&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;Value is the integer 1. (Only available for schema format 4 and higher.)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;10,11&lt;/cell&gt;
        &lt;cell&gt;variable&lt;/cell&gt;
        &lt;cell&gt;Reserved for internal use. These serial type codes will never appear in a well-formed database file, but they might be used in transient and temporary database files that SQLite sometimes generates for its own use. The meanings of these codes can shift from one release of SQLite to the next.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;N≥12 and even&lt;/cell&gt;
        &lt;cell&gt;(N-12)/2&lt;/cell&gt;
        &lt;cell&gt;Value is a BLOB that is (N-12)/2 bytes in length.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;N≥13 and odd&lt;/cell&gt;
        &lt;cell&gt;(N-13)/2&lt;/cell&gt;
        &lt;cell&gt;Value is a string in the text encoding and (N-13)/2 bytes in length. The nul terminator is not stored.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The header size varint and serial type varints will usually consist of a single byte. The serial type varints for large strings and BLOBs might extend to two or three byte varints, but that is the exception rather than the rule. The varint format is very efficient at coding the record header.&lt;/p&gt;
    &lt;p&gt;The values for each column in the record immediately follow the header. For serial types 0, 8, 9, 12, and 13, the value is zero bytes in length. If all columns are of these types then the body section of the record is empty.&lt;/p&gt;
    &lt;p&gt;A record might have fewer values than the number of columns in the corresponding table. This can happen, for example, after an ALTER TABLE ... ADD COLUMN SQL statement has increased the number of columns in the table schema without modifying preexisting rows in the table. Missing values at the end of the record are filled in using the default value for the corresponding columns defined in the table schema.&lt;/p&gt;
    &lt;p&gt;The order of keys in an index b-tree is determined by the sort order of the records that the keys represent. Record comparison progresses column by column. Columns of a record are examined from left to right. The first pair of columns that are not equal determines the relative order of the two records. The sort order of individual columns is as follows:&lt;/p&gt;
    &lt;p&gt;A collating function for each column is necessary in order to compute the order of text fields. SQLite defines three built-in collating functions:&lt;/p&gt;
    &lt;quote&gt;
      &lt;td&gt;BINARY&lt;/td&gt;
      &lt;td&gt;The built-in BINARY collation compares strings byte by byte using the memcmp() function from the standard C library.&lt;/td&gt;
      &lt;td&gt;NOCASE&lt;/td&gt;
      &lt;td&gt;The NOCASE collation is like BINARY except that uppercase ASCII characters ('A' through 'Z') are folded into their lowercase equivalents prior to running the comparison. Only ASCII characters are case-folded. NOCASE does not implement a general purpose unicode caseless comparison.&lt;/td&gt;
      &lt;td&gt;RTRIM&lt;/td&gt;
      &lt;td&gt;RTRIM is like BINARY except that extra spaces at the end of either string do not change the result. In other words, strings will compare equal to one another as long as they differ only in the number of spaces at the end.&lt;/td&gt;
    &lt;/quote&gt;
    &lt;p&gt;Additional application-specific collating functions can be added to SQLite using the sqlite3_create_collation() interface.&lt;/p&gt;
    &lt;p&gt;The default collating function for all strings is BINARY. Alternative collating functions for table columns can be specified in the CREATE TABLE statement using the COLLATE clause on the column definition. When a column is indexed, the same collating function specified in the CREATE TABLE statement is used for the column in the index, by default, though this can be overridden using a COLLATE clause in the CREATE INDEX statement.&lt;/p&gt;
    &lt;p&gt;Each ordinary SQL table in the database schema is represented on-disk by a table b-tree. Each entry in the table b-tree corresponds to a row of the SQL table. The rowid of the SQL table is the 64-bit signed integer key for each entry in the table b-tree.&lt;/p&gt;
    &lt;p&gt;The content of each SQL table row is stored in the database file by first combining the values in the various columns into a byte array in the record format, then storing that byte array as the payload in an entry in the table b-tree. The order of values in the record is the same as the order of columns in the SQL table definition. When an SQL table includes an INTEGER PRIMARY KEY column (which aliases the rowid) then that column appears in the record as a NULL value. SQLite will always use the table b-tree key rather than the NULL value when referencing the INTEGER PRIMARY KEY column.&lt;/p&gt;
    &lt;p&gt;If the affinity of a column is REAL and that column contains a value that can be converted to an integer without loss of information (if the value contains no fractional part and is not too large to be represented as an integer) then the column may be stored in the record as an integer. SQLite will convert the value back to floating point when extracting it from the record.&lt;/p&gt;
    &lt;p&gt;If an SQL table is created using the "WITHOUT ROWID" clause at the end of its CREATE TABLE statement, then that table is a WITHOUT ROWID table and uses a different on-disk representation. A WITHOUT ROWID table uses an index b-tree rather than a table b-tree for storage. The key for each entry in the WITHOUT ROWID b-tree is a record composed of the columns of the PRIMARY KEY followed by all remaining columns of the table. The primary key columns appear in the order that they were declared in the PRIMARY KEY clause and the remaining columns appear in the order they occur in the CREATE TABLE statement.&lt;/p&gt;
    &lt;p&gt;Hence, the content encoding for a WITHOUT ROWID table is the same as the content encoding for an ordinary rowid table, except that the order of the columns is rearranged so that PRIMARY KEY columns come first, and the content is used as the key in an index b-tree rather than as the data in a table b-tree. The special encoding rules for columns with REAL affinity apply to WITHOUT ROWID tables the same as they do with rowid tables.&lt;/p&gt;
    &lt;p&gt;If the PRIMARY KEY of a WITHOUT ROWID tables uses the same columns with the same collating sequence more than once, then the second and subsequent occurrences of that column in the PRIMARY KEY definition are ignored. For example, the following CREATE TABLE statements all specify the same table, which will have the exact same representation on disk:&lt;/p&gt;
    &lt;quote&gt;CREATE TABLE t1(a,b,c,d,PRIMARY KEY(a,c)) WITHOUT ROWID; CREATE TABLE t1(a,b,c,d,PRIMARY KEY(a,c,a,c)) WITHOUT ROWID; CREATE TABLE t1(a,b,c,d,PRIMARY KEY(a,A,a,C)) WITHOUT ROWID; CREATE TABLE t1(a,b,c,d,PRIMARY KEY(a,a,a,a,c)) WITHOUT ROWID;&lt;/quote&gt;
    &lt;p&gt;The first example above is the preferred definition of the table, of course. All of the examples create a WITHOUT ROWID table with two PRIMARY KEY columns, "a" and "c", in that order, followed by two data columns "b" and "d", also in that order.&lt;/p&gt;
    &lt;p&gt;Each SQL index, whether explicitly declared via a CREATE INDEX statement or implied by a UNIQUE or PRIMARY KEY constraint, corresponds to an index b-tree in the database file. Each entry in the index b-tree corresponds to a single row in the associated SQL table. The key to an index b-tree is a record composed of the columns that are being indexed followed by the key of the corresponding table row. For ordinary tables, the row key is the rowid, and for WITHOUT ROWID tables the row key is the PRIMARY KEY. Because every row in the table has a unique row key, all keys in an index are unique.&lt;/p&gt;
    &lt;p&gt;In a normal index, there is a one-to-one mapping between rows in a table and entries in each index associated with that table. However, in a partial index, the index b-tree only contains entries corresponding to table rows for which the WHERE clause expression on the CREATE INDEX statement is true. Corresponding rows in the index and table b-trees share the same rowid or primary key values and contain the same value for all indexed columns.&lt;/p&gt;
    &lt;p&gt;In an index on a WITHOUT ROWID table, if a column of the PRIMARY KEY is also a column in the index and has a matching collating sequence, then the indexed column is not repeated in the table-key suffix on the end of the index record. As an example, consider the following SQL:&lt;/p&gt;
    &lt;quote&gt;CREATE TABLE ex25(a,b,c,d,e,PRIMARY KEY(d,c,a)) WITHOUT rowid; CREATE INDEX ex25ce ON ex25(c,e); CREATE INDEX ex25acde ON ex25(a,c,d,e); CREATE INDEX ex25ae ON ex25(a COLLATE nocase,e);&lt;/quote&gt;
    &lt;p&gt;Each row in the ex25ce index is a record with these columns: c, e, d, a. The first two columns are the columns being indexed, c and e. The remaining columns are the primary key of the corresponding table row. Normally, the primary key would be columns d, c, and a, but because column c already appears earlier in the index, it is omitted from the key suffix.&lt;/p&gt;
    &lt;p&gt;In the extreme case where the columns being indexed cover all columns of the PRIMARY KEY, the index will consist of only the columns being indexed. The ex25acde example above demonstrates this. Each entry in the ex25acde index consists of only the columns a, c, d, and e, in that order.&lt;/p&gt;
    &lt;p&gt;Each row in ex25ae contains five columns: a, e, d, c, a. The "a" column is repeated since the first occurrence of "a" has a collating function of "nocase" and the second has a collating sequence of "binary". If the "a" column is not repeated and if the table contains two or more entries with the same "e" value and where "a" differs only in case, then all of those table entries would correspond to a single entry in the index, which would break the one-to-one correspondence between the table and the index.&lt;/p&gt;
    &lt;p&gt;The suppression of redundant columns in the key suffix of an index entry only occurs in WITHOUT ROWID tables. In an ordinary rowid table, the index entry always ends with the rowid even if the INTEGER PRIMARY KEY column is one of the columns being indexed.&lt;/p&gt;
    &lt;p&gt;Page 1 of a database file is the root page of a table b-tree that holds a special table named "sqlite_schema". This b-tree is known as the "schema table" since it stores the complete database schema. The structure of the sqlite_schema table is as if it had been created using the following SQL:&lt;/p&gt;
    &lt;quote&gt;CREATE TABLE sqlite_schema( type text, name text, tbl_name text, rootpage integer, sql text );&lt;/quote&gt;
    &lt;p&gt;The sqlite_schema table contains one row for each table, index, view, and trigger (collectively "objects") in the database schema, except there is no entry for the sqlite_schema table itself. The sqlite_schema table contains entries for internal schema objects in addition to application- and programmer-defined objects.&lt;/p&gt;
    &lt;p&gt;The sqlite_schema.type column will be one of the following text strings: 'table', 'index', 'view', or 'trigger' according to the type of object defined. The 'table' string is used for both ordinary and virtual tables.&lt;/p&gt;
    &lt;p&gt;The sqlite_schema.name column will hold the name of the object. UNIQUE and PRIMARY KEY constraints on tables cause SQLite to create internal indexes with names of the form "sqlite_autoindex_TABLE_N" where TABLE is replaced by the name of the table that contains the constraint and N is an integer beginning with 1 and increasing by one with each constraint seen in the table definition. In a WITHOUT ROWID table, there is no sqlite_schema entry for the PRIMARY KEY, but the "sqlite_autoindex_TABLE_N" name is set aside for the PRIMARY KEY as if the sqlite_schema entry did exist. This will affect the numbering of subsequent UNIQUE constraints. The "sqlite_autoindex_TABLE_N" name is never allocated for an INTEGER PRIMARY KEY, either in rowid tables or WITHOUT ROWID tables.&lt;/p&gt;
    &lt;p&gt;The sqlite_schema.tbl_name column holds the name of a table or view that the object is associated with. For a table or view, the tbl_name column is a copy of the name column. For an index, the tbl_name is the name of the table that is indexed. For a trigger, the tbl_name column stores the name of the table or view that causes the trigger to fire.&lt;/p&gt;
    &lt;p&gt;The sqlite_schema.rootpage column stores the page number of the root b-tree page for tables and indexes. For rows that define views, triggers, and virtual tables, the rootpage column is 0 or NULL.&lt;/p&gt;
    &lt;p&gt;The sqlite_schema.sql column stores SQL text that describes the object. This SQL text is a CREATE TABLE, CREATE VIRTUAL TABLE, CREATE INDEX, CREATE VIEW, or CREATE TRIGGER statement that if evaluated against the database file when it is the main database of a database connection would recreate the object. The text is usually a copy of the original statement used to create the object but with normalizations applied so that the text conforms to the following rules:&lt;/p&gt;
    &lt;p&gt;The text in the sqlite_schema.sql column is a copy of the original CREATE statement text that created the object, except normalized as described above and as modified by subsequent ALTER TABLE statements. The sqlite_schema.sql is NULL for the internal indexes that are automatically created by UNIQUE or PRIMARY KEY constraints.&lt;/p&gt;
    &lt;p&gt;The name "sqlite_schema" does not appear anywhere in the file format. That name is just a convention used by the database implementation. Due to historical and operational considerations, the "sqlite_schema" table can also sometimes be called by one of the following aliases:&lt;/p&gt;
    &lt;p&gt;Because the name of the schema table does not appear anywhere in the file format, the meaning of the database file is not changed if the application chooses to refer to the schema table by one of these alternative names.&lt;/p&gt;
    &lt;p&gt;In addition to the tables, indexes, views, and triggers created by the application and/or the developer using CREATE statements SQL, the sqlite_schema table may contain zero or more entries for internal schema objects that are created by SQLite for its own internal use. The names of internal schema objects always begin with "sqlite_" and any table, index, view, or trigger whose name begins with "sqlite_" is an internal schema object. SQLite prohibits applications from creating objects whose names begin with "sqlite_".&lt;/p&gt;
    &lt;p&gt;Internal schema objects used by SQLite may include the following:&lt;/p&gt;
    &lt;p&gt;Indices with names of the form "sqlite_autoindex_TABLE_N" that are used to implement UNIQUE and PRIMARY KEY constraints on ordinary tables.&lt;/p&gt;
    &lt;p&gt;A table with the name "sqlite_sequence" that is used to keep track of the maximum historical INTEGER PRIMARY KEY for a table using AUTOINCREMENT.&lt;/p&gt;
    &lt;p&gt;Tables with names of the form "sqlite_statN" where N is an integer. Such tables store database statistics gathered by the ANALYZE command and used by the query planner to help determine the best algorithm to use for each query.&lt;/p&gt;
    &lt;p&gt;New internal schema objects names, always beginning with "sqlite_", may be added to the SQLite file format in future releases.&lt;/p&gt;
    &lt;p&gt;The sqlite_sequence table is an internal table used to help implement AUTOINCREMENT. The sqlite_sequence table is created automatically whenever any ordinary table with an AUTOINCREMENT integer primary key is created. Once created, the sqlite_sequence table exists in the sqlite_schema table forever; it cannot be dropped. The schema for the sqlite_sequence table is:&lt;/p&gt;
    &lt;quote&gt;CREATE TABLE sqlite_sequence(name,seq);&lt;/quote&gt;
    &lt;p&gt;There is a single row in the sqlite_sequence table for each ordinary table that uses AUTOINCREMENT. The name of the table (as it appears in sqlite_schema.name) is in the sqlite_sequence.name field and the largest INTEGER PRIMARY KEY ever inserted into that table is in the sqlite_sequence.seq field. New automatically generated integer primary keys for AUTOINCREMENT tables are guaranteed to be larger than the sqlite_sequence.seq field for that table. If the sqlite_sequence.seq field of an AUTOINCREMENT table is already at the largest integer value (9223372036854775807) then attempts to add new rows to that table with an automatically generated integer primary will fail with an SQLITE_FULL error. The sqlite_sequence.seq field is automatically updated if required when new entries are inserted to an AUTOINCREMENT table. The sqlite_sequence row for an AUTOINCREMENT table is automatically deleted when the table is dropped. If the sqlite_sequence row for an AUTOINCREMENT table does not exist when the AUTOINCREMENT table is updated, then a new sqlite_sequence row is created. If the sqlite_sequence.seq value for an AUTOINCREMENT table is manually set to something other than an integer and there is a subsequent attempt to insert the or update the AUTOINCREMENT table, then the behavior is undefined.&lt;/p&gt;
    &lt;p&gt;Application code is allowed to modify the sqlite_sequence table, to add new rows, to delete rows, or to modify existing rows. However, application code cannot create the sqlite_sequence table if it does not already exist. Application code can delete all entries from the sqlite_sequence table, but application code cannot drop the sqlite_sequence table.&lt;/p&gt;
    &lt;p&gt;The sqlite_stat1 is an internal table created by the ANALYZE command and used to hold supplemental information about tables and indexes that the query planner can use to help it find better ways of performing queries. Applications can update, delete from, insert into or drop the sqlite_stat1 table, but may not create or alter the sqlite_stat1 table. The schema of the sqlite_stat1 table is as follows:&lt;/p&gt;
    &lt;quote&gt;CREATE TABLE sqlite_stat1(tbl,idx,stat);&lt;/quote&gt;
    &lt;p&gt;There is normally one row per index, with the index identified by the name in the sqlite_stat1.idx column. The sqlite_stat1.tbl column is the name of the table to which the index belongs. In each such row, the sqlite_stat.stat column will be a string consisting of a list of integers followed by zero or more arguments. The first integer in this list is the approximate number of rows in the index. (The number of rows in the index is the same as the number of rows in the table, except for partial indexes.) The second integer is the approximate number of rows in the index that have the same value in the first column of the index. The third integer is the number of rows in the index that have the same value for the first two columns. The N-th integer (for N&amp;gt;1) is the estimated average number of rows in the index which have the same value for the first N-1 columns. For a K-column index, there will be K+1 integers in the stat column. If the index is unique, then the last integer will be 1.&lt;/p&gt;
    &lt;p&gt;The list of integers in the stat column can optionally be followed by arguments, each of which is a sequence of non-space characters. All arguments are preceded by a single space. Unrecognized arguments are silently ignored.&lt;/p&gt;
    &lt;p&gt;If the "unordered" argument is present, then the query planner assumes that the index is unordered and will not use the index for a range query or for sorting.&lt;/p&gt;
    &lt;p&gt;The "sz=NNN" argument (where NNN represents a sequence of 1 or more digits) means that the average row size over all records of the table or index is NNN bytes per row. The SQLite query planner might use the estimated row size information provided by the "sz=NNN" token to help it choose smaller tables and indexes that require less disk I/O.&lt;/p&gt;
    &lt;p&gt;The presence of the "noskipscan" token on the sqlite_stat1.stat field of an index prevents that index from being used with the skip-scan optimization.&lt;/p&gt;
    &lt;p&gt;New text tokens may be added to the end of the stat column in future enhancements to SQLite. For compatibility, unrecognized tokens at the end of the stat column are silently ignored.&lt;/p&gt;
    &lt;p&gt;If the sqlite_stat1.idx column is NULL, then the sqlite_stat1.stat column contains a single integer which is the approximate number of rows in the table identified by sqlite_stat1.tbl. If the sqlite_stat1.idx column is the same as the sqlite_stat1.tbl column, then the table is a WITHOUT ROWID table and the sqlite_stat1.stat field contains information about the index btree that implements the WITHOUT ROWID table.&lt;/p&gt;
    &lt;p&gt;The sqlite_stat2 is only created and is only used if SQLite is compiled with SQLITE_ENABLE_STAT2 and if the SQLite version number is between 3.6.18 (2009-09-11) and 3.7.8 (2011-09-19). The sqlite_stat2 table is neither read nor written by any version of SQLite before 3.6.18 nor after 3.7.8. The sqlite_stat2 table contains additional information about the distribution of keys within an index. The schema of the sqlite_stat2 table is as follows:&lt;/p&gt;
    &lt;quote&gt;CREATE TABLE sqlite_stat2(tbl,idx,sampleno,sample);&lt;/quote&gt;
    &lt;p&gt;The sqlite_stat2.idx column and the sqlite_stat2.tbl column in each row of the sqlite_stat2 table identify an index described by that row. There are usually 10 rows in the sqlite_stat2 table for each index.&lt;/p&gt;
    &lt;p&gt;The sqlite_stat2 entries for an index that have sqlite_stat2.sampleno between 0 and 9 inclusive are samples of the left-most key value in the index taken at evenly spaced points along the index. Let C be the number of rows in the index. Then the sampled rows are given by&lt;/p&gt;
    &lt;quote&gt;rownumber = (i*C*2 + C)/20&lt;/quote&gt;
    &lt;p&gt;The variable i in the previous expression varies between 0 and 9. Conceptually, the index space is divided into 10 uniform buckets and the samples are the middle row from each bucket.&lt;/p&gt;
    &lt;p&gt;The format for sqlite_stat2 is recorded here for legacy reference. Recent versions of SQLite no longer support sqlite_stat2 and the sqlite_stat2 table, if is exists, is simply ignored.&lt;/p&gt;
    &lt;p&gt;The sqlite_stat3 is only used if SQLite is compiled with SQLITE_ENABLE_STAT3 or SQLITE_ENABLE_STAT4 and if the SQLite version number is 3.7.9 (2011-11-01) or greater. The sqlite_stat3 table is neither read nor written by any version of SQLite before 3.7.9. If the SQLITE_ENABLE_STAT4 compile-time option is used and the SQLite version number is 3.8.1 (2013-10-17) or greater, then sqlite_stat3 might be read but not written. The sqlite_stat3 table contains additional information about the distribution of keys within an index, information that the query planner can use to devise better and faster query algorithms. The schema of the sqlite_stat3 table is as follows:&lt;/p&gt;
    &lt;quote&gt;CREATE TABLE sqlite_stat3(tbl,idx,nEq,nLt,nDLt,sample);&lt;/quote&gt;
    &lt;p&gt;There are usually multiple entries in the sqlite_stat3 table for each index. The sqlite_stat3.sample column holds the value of the left-most field of an index identified by sqlite_stat3.idx and sqlite_stat3.tbl. The sqlite_stat3.nEq column holds the approximate number of entries in the index whose left-most column exactly matches the sample. The sqlite_stat3.nLt holds the approximate number of entries in the index whose left-most column is less than the sample. The sqlite_stat3.nDLt column holds the approximate number of distinct left-most entries in the index that are less than the sample.&lt;/p&gt;
    &lt;p&gt;There can be an arbitrary number of sqlite_stat3 entries per index. The ANALYZE command will typically generate sqlite_stat3 tables that contain between 10 and 40 samples that are distributed across the key space and with large nEq values.&lt;/p&gt;
    &lt;p&gt;In a well-formed sqlite_stat3 table, the samples for any single index must appear in the same order that they occur in the index. In other words, if the entry with left-most column S1 is earlier in the index b-tree than the entry with left-most column S2, then in the sqlite_stat3 table, sample S1 must have a smaller rowid than sample S2.&lt;/p&gt;
    &lt;p&gt;The sqlite_stat4 is only created and is only used if SQLite is compiled with SQLITE_ENABLE_STAT4 and if the SQLite version number is 3.8.1 (2013-10-17) or greater. The sqlite_stat4 table is neither read nor written by any version of SQLite before 3.8.1. The sqlite_stat4 table contains additional information about the distribution of keys within an index or the distribution of keys in the primary key of a WITHOUT ROWID table. The query planner can sometimes use the additional information in the sqlite_stat4 table to devise better and faster query algorithms. The schema of the sqlite_stat4 table is as follows:&lt;/p&gt;
    &lt;quote&gt;CREATE TABLE sqlite_stat4(tbl,idx,nEq,nLt,nDLt,sample);&lt;/quote&gt;
    &lt;p&gt;There are typically between 10 to 40 entries in the sqlite_stat4 table for each index for which statistics are available, however these limits are not hard bounds. The meanings of the columns in the sqlite_stat4 table are as follows:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;tbl:&lt;/cell&gt;
        &lt;cell&gt;The sqlite_stat4.tbl column holds name of the table that owns the index that the row describes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;idx:&lt;/cell&gt;
        &lt;cell&gt;The sqlite_stat4.idx column holds name of the index that the row describes, or in the case of an sqlite_stat4 entry for a WITHOUT ROWID table, the name of the table itself.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;sample:&lt;/cell&gt;
        &lt;cell&gt;The sqlite_stat4.sample column holds a BLOB in the record format that encodes the indexed columns followed by the rowid for a rowid table or by the columns of the primary key for a WITHOUT ROWID table. The sqlite_stat4.sample BLOB for the WITHOUT ROWID table itself contains just the columns of the primary key. Let the number of columns encoded by the sqlite_stat4.sample blob be N. For indexes on an ordinary rowid table, N will be one more than the number of columns indexed. For indexes on WITHOUT ROWID tables, N will be the number of columns indexed plus the number of columns in the primary key. For a WITHOUT ROWID table, N will be the number of columns in the primary key.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;nEq:&lt;/cell&gt;
        &lt;cell&gt;The sqlite_stat4.nEq column holds a list of N integers where the K-th integer is the approximate number of entries in the index whose left-most K columns exactly match the K left-most columns of the sample.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;nLt:&lt;/cell&gt;
        &lt;cell&gt;The sqlite_stat4.nLt column holds a list of N integers where the K-th integer is the approximate number of entries in the index whose K left-most columns are collectively less than the K left-most columns of the sample.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;nDLt:&lt;/cell&gt;
        &lt;cell&gt;The sqlite_stat4.nDLt column holds a list of N integers where the K-th integer is the approximate number of entries in the index that are distinct in the first K columns and where the left-most K columns are collectively less than the left-most K columns of the sample.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The sqlite_stat4 is a generalization of the sqlite_stat3 table. The sqlite_stat3 table provides information about the left-most column of an index whereas the sqlite_stat4 table provides information about all columns of the index.&lt;/p&gt;
    &lt;p&gt;There can be an arbitrary number of sqlite_stat4 entries per index. The ANALYZE command will typically generate sqlite_stat4 tables that contain between 10 and 40 samples that are distributed across the key space and with large nEq values.&lt;/p&gt;
    &lt;p&gt;In a well-formed sqlite_stat4 table, the samples for any single index must appear in the same order that they occur in the index. In other words, if entry S1 is earlier in the index b-tree than entry S2, then in the sqlite_stat4 table, sample S1 must have a smaller rowid than sample S2.&lt;/p&gt;
    &lt;p&gt;The rollback journal is a file associated with each SQLite database file that holds information used to restore the database file to its initial state during the course of a transaction. The rollback journal file is always located in the same directory as the database file and has the same name as the database file but with the string "-journal" appended. There can only be a single rollback journal associated with a give database and hence there can only be one write transaction open against a single database at one time.&lt;/p&gt;
    &lt;p&gt;Before any information-bearing page of the database is modified, the original unmodified content of that page is written into the rollback journal. If a transaction is interrupted and needs to be rolled back, the rollback journal can then be used to restore the database to its original state. Freelist leaf pages bear no information that would need to be restored on a rollback and so they are not written to the journal prior to modification, in order to reduce disk I/O.&lt;/p&gt;
    &lt;p&gt;If a transaction is aborted due to an application crash, or a single, or an operating system crash, or a hardware power failure or crash, then the main database file might be left in an inconsistent state. The next time SQLite attempts to open the database file, the presence of the rollback journal file will be detected and the journal will be automatically played back to restore the database to its state at the start of the incomplete transaction.&lt;/p&gt;
    &lt;p&gt;A rollback journal is only considered to be valid if it exists and contains a valid header. Hence a transaction can be committed in one of three ways:&lt;/p&gt;
    &lt;p&gt;These three ways of committing a transaction correspond to the DELETE, TRUNCATE, and PERSIST settings, respectively, of the journal_mode pragma.&lt;/p&gt;
    &lt;p&gt;A valid rollback journal begins with a header in the following format:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Offset&lt;/cell&gt;
        &lt;cell role="head"&gt;Size&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;Header string: 0xd9, 0xd5, 0x05, 0xf9, 0x20, 0xa1, 0x63, 0xd7&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;The "Page Count" - The number of pages in the next segment of the journal, or -1 to mean all content to the end of the file&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;A random nonce for the checksum&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;16&lt;/cell&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;Initial size of the database in pages&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;20&lt;/cell&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;Size of a disk sector assumed by the process that wrote this journal.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;24&lt;/cell&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;Size of pages in this journal.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;A rollback journal header is padded with zeros out to the size of a single sector (as defined by the sector size integer at offset 20). The header is in a sector by itself so that if a power loss occurs while writing the sector, information that follows the header will be (hopefully) undamaged.&lt;/p&gt;
    &lt;p&gt;After the header and zero padding are zero or more page records. Each page record stores a copy of the content of a page from the database file before it was changed. The same page may not appear more than once within a single rollback journal. To rollback an incomplete transaction, a process has merely to read the rollback journal from beginning to end and write pages found in the journal back into the database file at the appropriate location.&lt;/p&gt;
    &lt;p&gt;Let the database page size (the value of the integer at offset 24 in the journal header) be N. Then the format of a page record is as follows:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Offset&lt;/cell&gt;
        &lt;cell role="head"&gt;Size&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;The page number in the database file&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;N&lt;/cell&gt;
        &lt;cell&gt;Original content of the page prior to the start of the transaction&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;N+4&lt;/cell&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;Checksum&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The checksum is an unsigned 32-bit integer computed as follows:&lt;/p&gt;
    &lt;p&gt;The checksum value is used to guard against incomplete writes of a journal page record following a power failure. A different random nonce is used each time a transaction is started in order to minimize the risk that unwritten sectors might by chance contain data from the same page that was a part of prior journals. By changing the nonce for each transaction, stale data on disk will still generate an incorrect checksum and be detected with high probability. The checksum only uses a sparse sample of 32-bit words from the data record for performance reasons - design studies during the planning phases of SQLite 3.0.0 showed a significant performance hit in checksumming the entire page.&lt;/p&gt;
    &lt;p&gt;Let the page count value at offset 8 in the journal header be M. If M is greater than zero then after M page records the journal file may be zero padded out to the next multiple of the sector size and another journal header may be inserted. All journal headers within the same journal must contain the same database page size and sector size.&lt;/p&gt;
    &lt;p&gt;If M is -1 in the initial journal header, then the number of page records that follow is computed by computing how many page records will fit in the available space of the remainder of the journal file.&lt;/p&gt;
    &lt;p&gt;Beginning with version 3.7.0 (2010-07-21), SQLite supports a new transaction control mechanism called "write-ahead log" or "WAL". When a database is in WAL mode, all connections to that database must use the WAL. A particular database will use either a rollback journal or a WAL, but not both at the same time. The WAL is always located in the same directory as the database file and has the same name as the database file but with the string "-wal" appended.&lt;/p&gt;
    &lt;p&gt;A WAL file consists of a header followed by zero or more "frames". Each frame records the revised content of a single page from the database file. All changes to the database are recorded by writing frames into the WAL. Transactions commit when a frame is written that contains a commit marker. A single WAL can and usually does record multiple transactions. Periodically, the content of the WAL is transferred back into the database file in an operation called a "checkpoint".&lt;/p&gt;
    &lt;p&gt;A single WAL file can be reused multiple times. In other words, the WAL can fill up with frames and then be checkpointed and then new frames can overwrite the old ones. A WAL always grows from beginning toward the end. Checksums and counters attached to each frame are used to determine which frames within the WAL are valid and which are leftovers from prior checkpoints.&lt;/p&gt;
    &lt;p&gt;The WAL header is 32 bytes in size and consists of the following eight big-endian 32-bit unsigned integer values:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Offset&lt;/cell&gt;
        &lt;cell role="head"&gt;Size&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;Magic number. 0x377f0682 or 0x377f0683&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;File format version. Currently 3007000.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;Database page size. Example: 1024&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;Checkpoint sequence number&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;16&lt;/cell&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;Salt-1: random integer incremented with each checkpoint&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;20&lt;/cell&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;Salt-2: a different random number for each checkpoint&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;24&lt;/cell&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;Checksum-1: First part of a checksum on the first 24 bytes of header&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;28&lt;/cell&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;Checksum-2: Second part of the checksum on the first 24 bytes of header&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Immediately following the wal-header are zero or more frames. Each frame consists of a 24-byte frame-header followed by a page-size bytes of page data. The frame-header is six big-endian 32-bit unsigned integer values, as follows:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Offset&lt;/cell&gt;
        &lt;cell role="head"&gt;Size&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;Page number&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;For commit records, the size of the database file in pages after the commit. For all other records, zero.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;Salt-1 copied from the WAL header&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;Salt-2 copied from the WAL header&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;16&lt;/cell&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;Checksum-1: Cumulative checksum up through and including this page&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;20&lt;/cell&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;Checksum-2: Second half of the cumulative checksum.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;A frame is considered valid if and only if the following conditions are true:&lt;/p&gt;
    &lt;p&gt;The salt-1 and salt-2 values in the frame-header match salt values in the wal-header&lt;/p&gt;
    &lt;p&gt;The checksum values in the final 8 bytes of the frame-header exactly match the checksum computed consecutively on the first 24 bytes of the WAL header and the first 8 bytes and the content of all frames up to and including the current frame.&lt;/p&gt;
    &lt;p&gt;The checksum is computed by interpreting the input as an even number of unsigned 32-bit integers: x(0) through x(N). The 32-bit integers are big-endian if the magic number in the first 4 bytes of the WAL header is 0x377f0683 and the integers are little-endian if the magic number is 0x377f0682. The checksum values are always stored in the frame header in a big-endian format regardless of which byte order is used to compute the checksum.&lt;/p&gt;
    &lt;p&gt;The checksum algorithm only works for content which is a multiple of 8 bytes in length. In other words, if the inputs are x(0) through x(N) then N must be odd. The checksum algorithm is as follows:&lt;/p&gt;
    &lt;quote&gt;s0 = s1 = 0 for i from 0 to n-1 step 2: s0 += x(i) + s1; s1 += x(i+1) + s0; endfor # result in s0 and s1&lt;/quote&gt;
    &lt;p&gt;The outputs s0 and s1 are both weighted checksums using Fibonacci weights in reverse order. (The largest Fibonacci weight occurs on the first element of the sequence being summed.) The s1 value spans all 32-bit integer terms of the sequence whereas s0 omits the final term.&lt;/p&gt;
    &lt;p&gt;On a checkpoint, the WAL is first flushed to persistent storage using the xSync method of the VFS. Then valid content of the WAL is transferred into the database file. Finally, the database is flushed to persistent storage using another xSync method call. The xSync operations serve as write barriers - all writes launched before the xSync must complete before any write that launches after the xSync begins.&lt;/p&gt;
    &lt;p&gt;A checkpoint need not run to completion. It might be that some readers are still using older transactions with data that is contained in the database file. In that case, transferring content for newer transactions from the WAL file into the database would delete the content out from under readers still using the older transactions. To avoid that, checkpoints only run to completion if all reader are using the last transaction in the WAL.&lt;/p&gt;
    &lt;p&gt;After a complete checkpoint, if no other connections are in transactions that use the WAL, then subsequent write transactions can overwrite the WAL file from the beginning. This is called "resetting the WAL". At the start of the first new write transaction, the WAL header salt-1 value is incremented and the salt-2 value is randomized. These changes to the salts invalidate old frames in the WAL that have already been checkpointed but not yet overwritten, and prevent them from being checkpointed again.&lt;/p&gt;
    &lt;p&gt;The WAL file can optionally be truncated on a reset, but it need not be. Performance is usually a little better if the WAL is not truncated, since filesystems generally will overwrite an existing file faster than they will grow a file.&lt;/p&gt;
    &lt;p&gt;To read a page from the database (call it page number P), a reader first checks the WAL to see if it contains page P. If so, then the last valid instance of page P that is followed by a commit frame or is a commit frame itself becomes the value read. If the WAL contains no copies of page P that are valid and which are a commit frame or are followed by a commit frame, then page P is read from the database file.&lt;/p&gt;
    &lt;p&gt;To start a read transaction, the reader records the number of value frames in the WAL as "mxFrame". (More detail) The reader uses this recorded mxFrame value for all subsequent read operations. New transactions can be appended to the WAL, but as long as the reader uses its original mxFrame value and ignores subsequently appended content, the reader will see a consistent snapshot of the database from a single point in time. This technique allows multiple concurrent readers to view different versions of the database content simultaneously.&lt;/p&gt;
    &lt;p&gt;The reader algorithm in the previous paragraphs works correctly, but because frames for page P can appear anywhere within the WAL, the reader has to scan the entire WAL looking for page P frames. If the WAL is large (multiple megabytes is typical) that scan can be slow, and read performance suffers. To overcome this problem, a separate data structure called the wal-index is maintained to expedite the search for frames of a particular page.&lt;/p&gt;
    &lt;p&gt;Conceptually, the wal-index is shared memory, though the current VFS implementations use a memory-mapped file for operating-system portability. The memory-mapped file is in the same directory as the database and has the same name as the database with a "-shm" suffix appended. Because the wal-index is shared memory, SQLite does not support journal_mode=WAL on a network filesystem when clients are on different machines, as all clients of the database must be able to share the same memory.&lt;/p&gt;
    &lt;p&gt;The purpose of the wal-index is to answer this question quickly:&lt;/p&gt;
    &lt;quote&gt;Given a page number P and a maximum WAL frame index M, return the largest WAL frame index for page P that does not exceed M, or return NULL if there are no frames for page P that do not exceed M.&lt;/quote&gt;
    &lt;p&gt;The M value in the previous paragraph is the "mxFrame" value defined in section 4.4 that is read at the start of a transaction and which defines the maximum frame from the WAL that the reader will use.&lt;/p&gt;
    &lt;p&gt;The wal-index is transient. After a crash, the wal-index is reconstructed from the original WAL file. The VFS is required to either truncate or zero the header of the wal-index when the last connection to it closes. Because the wal-index is transient, it can use an architecture-specific format; it does not have to be cross-platform. Hence, unlike the database and WAL file formats which store all values as big endian, the wal-index stores multi-byte values in the native byte order of the host computer.&lt;/p&gt;
    &lt;p&gt;This document is concerned with the persistent state of the database file, and since the wal-index is a transient structure, no further information about the format of the wal-index will be provided here. Additional details on the format of the wal-index are contained in the separate WAL-index File Format document.&lt;/p&gt;
    &lt;p&gt;This page last modified on 2025-04-30 20:02:34 UTC&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45132488</guid></item><item><title>IRHash: Efficient Multi-Language Compiler Caching by IR-Level Hashing</title><link>https://www.usenix.org/conference/atc25/presentation/landsberg</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45136367</guid></item><item><title>How the “Kim” dump exposed North Korea's credential theft playbook</title><link>https://dti.domaintools.com/inside-the-kimsuky-leak-how-the-kim-dump-exposed-north-koreas-credential-theft-playbook/</link><description>&lt;doc fingerprint="3599949f0a828e4a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Inside the Kimsuky Leak: How the “Kim” Dump Exposed North Korea’s Credential Theft Playbook&lt;/head&gt;
    &lt;p&gt;Contents:&lt;lb/&gt;Part I: Technical Analysis&lt;lb/&gt;Part II: Goals Analysis&lt;lb/&gt;Part III: Threat Intelligence Report&lt;/p&gt;
    &lt;head rend="h2"&gt;Executive Summary&lt;/head&gt;
    &lt;p&gt;A rare and revealing breach attributed to a North Korean-affiliated actor, known only as “Kim” as named by the hackers who dumped the data, has delivered a new insight into Kimsuky (APT43) tactics, techniques, and infrastructure. This actor’s operational profile showcases credential-focused intrusions targeting South Korean and Taiwanese networks, with a blending of Chinese-language tooling, infrastructure, and possible logistical support. The “Kim” dump, which includes bash histories, phishing domains, OCR workflows, compiled stagers, and rootkit evidence, reflects a hybrid operation situated between DPRK attribution and Chinese resource utilization.&lt;/p&gt;
    &lt;p&gt;This report is broken down into three parts:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Technical Analysis of the dump materials&lt;/item&gt;
      &lt;item&gt;Motivation and Goals of the APT actor (group)&lt;/item&gt;
      &lt;item&gt;A CTI report compartment for analysts&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;While this leak only gives a partial idea of what the Kimusky/PRC activities have been, the material provides insight into the expansion of activities, nature of the actor(s), and goals they have in their penetration of the South Korean governmental systems that would benefit not only DPRK, but also PRC.&lt;/p&gt;
    &lt;p&gt;Without a doubt, there will be more coming out from this dump in the future, particularly if the burned assets have not been taken offline and access is still available, or if others have cloned those assets for further analysis. We may revisit this in the future if additional novel information comes to light.&lt;/p&gt;
    &lt;head rend="h1"&gt;Part I: Technical Analysis&lt;/head&gt;
    &lt;head rend="h2"&gt;The Leak at a Glance&lt;/head&gt;
    &lt;p&gt;The leaked dataset attributed to the “Kim” operator offers a uniquely operational perspective into North Korean-aligned cyber operations. Among the contents were terminal history files revealing active malware development efforts using NASM (Netwide Assembler), a choice consistent with low-level shellcode engineering typically reserved for custom loaders and injection tools. These logs were not static forensic artifacts but active command-line histories showing iterative compilation and cleanup processes, suggesting a hands-on attacker directly involved in tool assembly.&lt;/p&gt;
    &lt;p&gt;In parallel, the operator ran OCR (Optical Character Recognition) commands against sensitive Korean PDF documents related to public key infrastructure (PKI) standards and VPN deployments. These actions likely aimed to extract structured language or configurations for use in spoofing, credential forgery, or internal tool emulation.&lt;/p&gt;
    &lt;p&gt;Privileged Access Management (PAM) logs also surfaced in the dump, detailing a timeline of password changes and administrative account use. Many were tagged with the Korean string 변경완료 (“change complete”), and the logs included repeated references to elevated accounts such as oracle, svradmin, and app_adm01, indicating sustained access to critical systems.&lt;/p&gt;
    &lt;p&gt;The phishing infrastructure was extensive. Domain telemetry pointed to a network of malicious sites designed to mimic legitimate Korean government portals. Sites like nid-security[.]com were crafted to fool users into handing over credentials via advanced AiTM (Adversary-in-the-Middle) techniques.&lt;/p&gt;
    &lt;p&gt;Finally, network artifacts within the dump showed targeted reconnaissance of Taiwanese government and academic institutions. Specific IP addresses and .tw domain access, along with attempts to crawl .git repositories, reveal a deliberate focus on high-value administrative and developer targets.&lt;/p&gt;
    &lt;p&gt;Perhaps most concerning was the inclusion of a Linux rootkit using syscall hooking (khook) and stealth persistence via directories like /usr/lib64/tracker-fs. This highlights a capability for deep system compromise and covert command-and-control operations, far beyond phishing and data theft.&lt;/p&gt;
    &lt;p&gt;Artifacts recovered from the dump include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Terminal history files demonstrating malware compilation using NASM&lt;/item&gt;
      &lt;item&gt;OCR commands parsing Korean PDF documents related to PKI and VPN infrastructure&lt;/item&gt;
      &lt;item&gt;PAM logs reflecting password changes and credential lifecycle events&lt;/item&gt;
      &lt;item&gt;Phishing infrastructure mimicking Korean government sites&lt;/item&gt;
      &lt;item&gt;IP addresses indicating reconnaissance of Taiwanese government and research institutions&lt;/item&gt;
      &lt;item&gt;Linux rootkit code using syscall hooking and covert channel deployment&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Credential Theft Focus&lt;/head&gt;
    &lt;p&gt;The dump strongly emphasizes credential harvesting as a central operational goal. Key files such as 136백운규001_env.key (The presence of 136백운규001_env.key is a smoking gun indicator of stolen South Korean Government PKI material, as its structure (numeric ID + Korean name + .key) aligns uniquely with SK GPKI issuance practices and provides clear evidence of compromised, identity-tied state cryptographic keys.) This was discovered alongside plaintext passwords, that indicate clear evidence of active compromise of South Korea’s GPKI (Government Public Key Infrastructure). Possession of such certificates would allow for highly effective identity spoofing across government systems.&lt;/p&gt;
    &lt;p&gt;PAM logs further confirmed this focus, showing a pattern of administrative account rotation and password resets, all timestamped and labeled with success indicators (변경완료: Change Complete). The accounts affected were not low-privilege; instead, usernames like oracle, svradmin, and app_adm01, often used by IT staff and infrastructure services, suggested access to core backend environments.&lt;/p&gt;
    &lt;p&gt;These findings point to a strategy centered on capturing and maintaining access to privileged credentials and digital certificates, effectively allowing the attacker to act as an insider within trusted systems.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Leaked .key files (e.g., 136백운규001_env.key) with plaintext passwords confirm access to GPKI systems&lt;/item&gt;
      &lt;item&gt;PAM logs show administrative password rotations tagged with 변경완료 (change complete)&lt;/item&gt;
      &lt;item&gt;Admin-level accounts such as oracle, svradmin, and app_adm01 repeatedly appear in compromised logs&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Phishing Infrastructure&lt;/head&gt;
    &lt;p&gt;The operator’s phishing infrastructure was both expansive and regionally tailored. Domains such as nid-security[.]com and webcloud-notice[.]com mimicked Korean identity and document delivery services, likely designed to intercept user logins or deploy malicious payloads. More sophisticated spoofing was seen in sites that emulated official government agencies like dcc.mil[.]kr, spo.go[.]kr, and mofa.go[.]kr.&lt;/p&gt;
    &lt;p&gt;Burner email usage added another layer of operational tradecraft. The address jeder97271[@]wuzak[.]com is likely linked to phishing kits that operated through TLS proxies, capturing credentials in real time as victims interacted with spoofed login forms.&lt;/p&gt;
    &lt;p&gt;These tactics align with previously known Kimsuky behaviors but also demonstrate an evolution in technical implementation, particularly the use of AiTM interception rather than relying solely on credential-harvesting documents.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Domains include: nid-security[.]com, html-load[.]com, webcloud-notice[.]com, koala-app[.]com, and wuzak[.]com&lt;/item&gt;
      &lt;item&gt;Mimicked portals: dcc.mil[.]kr, spo.go[.]kr, mofa.go[.]kr&lt;/item&gt;
      &lt;item&gt;Burner email evidence: jeder97271[@]wuzak[.]com&lt;/item&gt;
      &lt;item&gt;Phishing kits leveraged TLS proxies for AiTM credential capture&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Malware Development Activity&lt;/head&gt;
    &lt;p&gt;Kim’s malware development environment showcased a highly manual, tailored approach. Shellcode was compiled using NASM, specifically with flags like -f win32, revealing a focus on targeting Windows environments. Commands such as make and rm were used to automate and sanitize builds, while hashed API call resolution (VirtualAlloc, HttpSendRequestA, etc.) was implemented to evade antivirus heuristics.&lt;/p&gt;
    &lt;p&gt;The dump also revealed reliance on GitHub repositories known for offensive tooling. TitanLdr, minbeacon, Blacklotus, and CobaltStrike-Auto-Keystore were all cloned or referenced in command logs. This hybrid use of public frameworks for private malware assembly is consistent with modern APT workflows.&lt;/p&gt;
    &lt;p&gt;A notable technical indicator was the use of the proxyres library to extract Windows proxy settings, particularly via functions like proxy_config_win_get_auto_config_url. This suggests an interest in hijacking or bypassing network-level security controls within enterprise environments.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Manual shellcode compilation via nasm -f win32 source/asm/x86/start.asm&lt;/item&gt;
      &lt;item&gt;Use of make, rm, and hash obfuscation of Win32 API calls (e.g., VirtualAlloc, HttpSendRequestA)&lt;/item&gt;
      &lt;item&gt;GitHub tools in use: TitanLdr, minbeacon, Blacklotus, CobaltStrike-Auto-Keystore&lt;/item&gt;
      &lt;item&gt;Proxy configuration probing through proxyres library (proxy_config_win_get_auto_config_url)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Rootkit Toolkit and Implant Structure&lt;/head&gt;
    &lt;p&gt;The Kim dump offers deep insight into a stealthy and modular Linux rootkit attributed to the operator’s post-compromise persistence tactics. The core implant, identified as vmmisc.ko (alternatively VMmisc.ko in some shells), was designed for kernel-mode deployment across multiple x86_64 Linux distributions and utilizes classic syscall hooking and covert channeling to maintain long-term undetected access.&lt;/p&gt;
    &lt;head rend="h3"&gt;Google Translation of Koh doc: Rootkit Endpoint Reuse Authentication Tool&lt;/head&gt;
    &lt;p&gt;“This tool uses kernel-level rootkit hiding technology, providing a high degree of stealth and penetration connection capability. It can hide while running on common Linux systems, and at the kernel layer supports connection forwarding, allowing reuse of external ports to connect to controlled hosts. Its communication behavior is hidden within normal traffic.&lt;/p&gt;
    &lt;p&gt;The tool uses binary merging technology: at compile time, the application layer program is encrypted and fused into a .ko driver file. When installed, only the .ko file exists. When the .ko driver starts, it will automatically decompress and release the hidden application-layer program.&lt;/p&gt;
    &lt;p&gt;Tools like chkrootkit, rkhunter, and management utilities (such as ps, netstat, etc.) are bypassed through technical evasion and hiding, making them unable to detect hidden networks, ports, processes, or file information.&lt;/p&gt;
    &lt;p&gt;To ensure software stability, all functions have also passed stress testing.&lt;/p&gt;
    &lt;p&gt;Supported systems: Linux Kernel 2.6.x / 3.x / 4.x, both x32 and x64 systems”.&lt;/p&gt;
    &lt;p&gt;Implant Features and Behavior&lt;/p&gt;
    &lt;p&gt;This rootkit exhibits several advanced features:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Syscall Hooking: Hooks critical kernel functions (e.g., getdents, read, write) to hide files, directories, and processes by name or PID.&lt;/item&gt;
      &lt;item&gt;SOCKS5 Proxy: Integrated remote networking capability using dynamic port forwarding and chained routing.&lt;/item&gt;
      &lt;item&gt;PTY Backdoor Shell: Spawns pseudoterminals that operate as interactive reverse shells with password protection.&lt;/item&gt;
      &lt;item&gt;Encrypted Sessions: Session commands must match a pre-set passphrase (e.g., testtest) to activate rootkit control mode.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Once installed (typically using insmod vmmisc.ko), the rootkit listens silently and allows manipulation via an associated client binary found in the dump. The client supports an extensive set of interactive commands, including:&lt;/p&gt;
    &lt;p&gt;+p # list hidden processes&lt;/p&gt;
    &lt;p&gt;+f # list hidden files&lt;/p&gt;
    &lt;p&gt;callrk # load client ↔ kernel handshake&lt;/p&gt;
    &lt;p&gt;exitrk # gracefully unload implant&lt;/p&gt;
    &lt;p&gt;shell # spawn reverse shell&lt;/p&gt;
    &lt;p&gt;socks5 # initiate proxy channel&lt;/p&gt;
    &lt;p&gt;upload / download # file transfer interface&lt;/p&gt;
    &lt;p&gt;These capabilities align closely with known DPRK malware behaviors, particularly from the Kimsuky and Lazarus groups, who have historically leveraged rootkits for lateral movement, stealth, persistence, and exfiltration staging.&lt;/p&gt;
    &lt;head rend="h2"&gt;Observed Deployment&lt;/head&gt;
    &lt;p&gt;Terminal history (.bash_history) shows the implant was staged and tested from the following paths:&lt;/p&gt;
    &lt;code&gt;.cache/vmware/drag_and_drop/VMmisc.ko

/usr/lib64/tracker-fs/vmmisc.ko

Execution logs show the use of commands such as:

insmod /usr/lib64/tracker-fs/vmmisc.ko

./client 192.168.0[.]39 testtest&lt;/code&gt;
    &lt;p&gt;These paths were not random—they mimic legitimate system service locations to avoid detection by file integrity monitoring (FIM) tools.&lt;/p&gt;
    &lt;p&gt;This structure highlights the modular, command-activated nature of the implant and its ability to serve multiple post-exploitation roles while maintaining stealth through kernel-layer masking.&lt;/p&gt;
    &lt;head rend="h2"&gt;Strategic Implications&lt;/head&gt;
    &lt;p&gt;The presence of such an advanced toolkit in the “Kim” dump strongly suggests the actor had persistent access to Linux server environments, likely via credential compromise. The use of kernel-mode implants also indicates long-term intent and trust-based privilege escalation. The implant’s pathing, language patterns, and tactics (e.g., use of /tracker-fs/, use of test passwords) match TTPs previously observed in operations attributed to Kimsuky, enhancing confidence in North Korean origin.&lt;/p&gt;
    &lt;head rend="h2"&gt;OCR-Based Recon&lt;/head&gt;
    &lt;p&gt;A defining component of Kim’s tradecraft was the use of OCR to analyze Korean-language security documentation. The attacker issued commands such as ocrmypdf -l kor+eng “file.pdf” to parse documents like 별지2)행정전자서명_기술요건_141125.pdf (“Appendix 2: Administrative Electronic Signature_Technical Requirements_141125.pdf”) and SecuwaySSL U_카달로그.pdf (“SecuwaySSL U_Catalog.pdf”). These files contain technical language around digital signatures, SSL implementations, and identity verification standards used in South Korea’s PKI infrastructure.&lt;/p&gt;
    &lt;p&gt;This OCR-based collection approach indicates more than passive intelligence gathering – it reflects a deliberate effort to model and potentially clone government-grade authentication systems. The use of bilingual OCR (Korean + English) further confirms the operator’s intention to extract usable configuration data across documentation types.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;OCR commands used to extract Korean PKI policy language from PDFs such as (별지2)행정전자서명_기술요건_141125.pdf and SecuwaySSL U_카달로그.pdf &lt;list rend="ul"&gt;&lt;item&gt;별지2)행정전자서명_기술요건_141125.pdf → (Appendix 2: Administrative Electronic Signature_Technical Requirements_141125.pdf&lt;/item&gt;&lt;item&gt;SecuwaySSL U_카달로그.pdf → SecuwaySSL U_Catalog.pdf&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Command examples: ocrmypdf -l kor+eng “file.pdf”&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;SSH and Log-Based Evidence&lt;/head&gt;
    &lt;p&gt;The forensic evidence contained within the logs, specifically SSH authentication records and PAM outputs, provides clear technical confirmation of the operator’s tactics and target focus.&lt;/p&gt;
    &lt;p&gt;Several IP addresses stood out as sources of brute-force login attempts. These include 23.95.213[.]210 (a known VPS provider used in past credential-stuffing campaigns), 218.92.0[.]210 (allocated to a Chinese ISP), and 122.114.233[.]77 (Henan Mobile, China). These IPs were recorded during multiple failed login events, strongly suggesting automated password attacks against exposed SSH services. Their geographic distribution and known history in malicious infrastructure usage point to an external staging environment, possibly used for pivoting into Korean and Taiwanese systems.&lt;/p&gt;
    &lt;p&gt;Beyond brute force, the logs also contain evidence of authentication infrastructure reconnaissance. Multiple PAM and OCSP (Online Certificate Status Protocol) errors referenced South Korea’s national PKI authority, including domains like gva.gpki.go[.]kr and ivs.gpki.go[.]kr. These errors appear during scripted or automated access attempts, indicating a potential strategy of credential replay or certificate misuse against GPKI endpoints, an approach that aligns with Kim’s broader PKI-targeting operations.&lt;/p&gt;
    &lt;p&gt;Perhaps the most revealing detail was the presence of successful superuser logins labeled with the Korean term 최고 관리자 (“Super Administrator”). This suggests the actor was not just harvesting credentials but successfully leveraging them for privileged access, possibly through cracked accounts, reused credentials, or insider-sourced passwords. The presence of such accounts in conjunction with password rotation entries marked as 변경완료 (“change complete”) further implies active control over PAM-protected systems during the operational window captured in the dump.&lt;/p&gt;
    &lt;p&gt;Together, these logs demonstrate a methodical campaign combining external brute-force access, PKI service probing, and administrative credential takeover, a sequence tailored for persistent infiltration and lateral movement within sensitive government and enterprise networks.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Brute-force IPs: 23.95.213[.]210, 218.92.0[.]210, 122.114.233[.]77&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;IP Address&lt;/cell&gt;
        &lt;cell&gt;Origin&lt;/cell&gt;
        &lt;cell&gt;Role / Threat Context&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;218.92.0[.]210&lt;/cell&gt;
        &lt;cell&gt;China Telecom (Jiangsu)&lt;/cell&gt;
        &lt;cell&gt;Part of Chinanet backbone, likely proxy or scanning node&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;23.95.213[.]210&lt;/cell&gt;
        &lt;cell&gt;Colocrossing (US)&lt;/cell&gt;
        &lt;cell&gt;Frequently used in brute-force and anonymized hosting for malware ops&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;122.114.233[.]77&lt;/cell&gt;
        &lt;cell&gt;Presumed PRC local ISP&lt;/cell&gt;
        &lt;cell&gt;Possibly mobile/ISP-based proxy used to obfuscate lateral movement&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;PAM/OCSP errors targeting gva.gpki.go[.]kr, ivs.gpki.go[.]kr&lt;/item&gt;
      &lt;item&gt;Superuser login events under 최고 관리자 (Super Administrator)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;Part II: Goals Analysis&lt;/head&gt;
    &lt;head rend="h2"&gt;Targeting South Korea: Identity, Infrastructure, and Credential Theft&lt;/head&gt;
    &lt;p&gt;The “Kim” operator’s campaign against South Korea was deliberate and strategic, aiming to infiltrate the nation’s digital trust infrastructure at multiple levels. A central focus was the Government Public Key Infrastructure (GPKI), where the attacker exfiltrated certificate files, including .key and .crt formats, some with plaintext passwords, and attempted repeated authentication against domains like gva.gpki.go[.]kr and ivs.gpki.go[.]kr. OCR tools were used to parse Korean technical documents detailing PKI and VPN architectures, demonstrating a sophisticated effort to understand and potentially subvert national identity frameworks. These efforts were not limited to reconnaissance; administrative password changes were logged, and phishing kits targeted military and diplomatic webmail, including clones of mofa.go[.]kr and credential harvesting through adversary-in-the-middle (AiTM) proxy setups.&lt;/p&gt;
    &lt;p&gt;Beyond authentication systems, Kim targeted privileged accounts (oracle, unwadm, svradmin) and rotated credentials to maintain persistent administrative access, as evidenced by PAM and SSH logs showing elevated user activity under the title 최고 관리자 (“Super Administrator”). The actor also showed interest in bypassing VPN controls, parsing SecuwaySSL configurations for exploitation potential, and deployed custom Linux rootkits using syscall hooking to establish covert persistence on compromised machines. Taken together, the dump reveals a threat actor deeply invested in credential dominance, policy reconnaissance, and system-level infiltration, placing South Korea’s public sector identity systems, administrative infrastructure, and secure communications at the core of its long-term espionage objectives.&lt;/p&gt;
    &lt;head rend="h2"&gt;Taiwan Reconnaissance&lt;/head&gt;
    &lt;p&gt;Among the most notable aspects of the “Kim” leak is the operator’s deliberate focus on Taiwanese infrastructure. The attacker accessed a number of domains with clear affiliations to the island’s public and private sectors, including tw.systexcloud[.]com (linked to enterprise cloud solutions), mlogin.mdfapps[.]com (a mobile authentication or enterprise login portal), and the .git/ directory of caa.org[.]tw, which belongs to the Chinese Institute of Aeronautics, a government-adjacent research entity.&lt;/p&gt;
    &lt;p&gt;This last domain is especially telling. Accessing .git/ paths directly implies an attempt to enumerate internal source code repositories, a tactic often used to discover hardcoded secrets, API keys, deployment scripts, or developer credentials inadvertently exposed via misconfigured web servers. This behavior points to more technical depth than simple phishing; it indicates supply chain reconnaissance and long-term infiltration planning.&lt;/p&gt;
    &lt;p&gt;The associated IP addresses further reinforce this conclusion. All three, 163.29.3[.]119, 118.163.30[.]45, and 59.125.159[.]81, are registered to academic, government, or research backbone providers in Taiwan. These are not random scans; they reflect targeted probing of strategic digital assets.&lt;/p&gt;
    &lt;head rend="h2"&gt;Summary of Whois &amp;amp; Ownership Insights&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;118.163.30[.]45 &lt;list rend="ul"&gt;&lt;item&gt;Appears as part of the IP range used for the domain dtc-tpe.com[.]tw, linked to Taiwan’s HINET provider (118.163.30[.]46 )Site Indices page of HINET provider.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;163.29.3[.]119 &lt;list rend="ul"&gt;&lt;item&gt;Falls within the 163.29.3[.]0/24 subnet identified with Taiwanese government or institutional use, notably in Taipei. This corresponds to B‑class subnets assigned to public/government entities IP地址 (繁體中文).&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;59.125.159[.]81&lt;list rend="ul"&gt;&lt;item&gt;Belongs to the broader 59.125.159[.]0–59.125.159[.]254 block, commonly used by Taiwanese ISP operators such as Chunghwa Telecom in Taipei&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Taken together, this Taiwan-focused activity reveals an expanded operational mandate. Whether the attacker is purely DPRK-aligned or operating within a DPRK–PRC fusion cell, the intent is clear: compromise administrative and developer infrastructure in Taiwan, likely in preparation for broader credential theft, espionage, or disruption campaigns.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Targeted domains: tw.systexcloud[.]com, caa.org[.]tw/.git/, mlogin.mdfapps[.]com&lt;/item&gt;
      &lt;item&gt;IPs linked to Taiwanese academic/government assets: 163.29.3[.]119, 118.163.30[.]45, 59.125.159[.]81&lt;/item&gt;
      &lt;item&gt;Git crawling suggests interest in developer secrets or exposed tokens&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Hybrid Attribution Model&lt;/head&gt;
    &lt;p&gt;The “Kim” operator embodies the growing complexity of modern nation-state attribution, where cyber activities often blur traditional boundaries and merge capabilities across geopolitical spheres. This case reveals strong indicators of both North Korean origin and Chinese operational entanglement, presenting a textbook example of a hybrid APT model.&lt;/p&gt;
    &lt;p&gt;On one hand, the technical and linguistic evidence strongly supports a DPRK-native operator. Terminal environments, OCR parsing routines, and system artifacts consistently leverage Korean language and character sets. The operator’s activities reflect a deep understanding of Korean PKI systems, with targeted extraction of GPKI .key files and automation to parse sensitive Korean government PDF documentation. These are hallmarks of Kimsuky/APT43 operations, known for credential-focused espionage against South Korean institutions and diplomatic targets. The intent to infiltrate identity infrastructure is consistent with North Korea’s historical targeting priorities. Notably, the system time zone on Kim’s host machine was set to UTC+9 (Pyongyang Standard Time), reinforcing the theory that the actor maintains direct ties to the DPRK’s internal environment, even if operating remotely.&lt;/p&gt;
    &lt;p&gt;However, this actor’s digital footprint extends well into Chinese infrastructure. Browser and download logs reveal frequent interaction with platforms like gitee[.]com, baidu[.]com, and zhihu[.]com, highly popular within the PRC but unusual for DPRK operators who typically minimize exposure to foreign services. Moreover, session logs include simplified Chinese content and PRC browsing behaviors, suggesting that the actor may be physically operating within China or through Chinese-language systems. This aligns with longstanding intelligence on North Korean cyber operators stationed in Chinese border cities such as Shenyang and Dandong, where DPRK nationals often conduct cyber operations with tacit approval or logistical consent from Chinese authorities. These locations provide higher-speed internet, relaxed oversight, and convenient geopolitical proximity.&lt;/p&gt;
    &lt;p&gt;The targeting of Taiwanese infrastructure further complicates attribution. Kimsuky has not historically prioritized Taiwan, yet in this case, the actor demonstrated direct reconnaissance of Taiwanese government and developer networks. While this overlaps with Chinese APT priorities, recent evidence from the “Kim” dump, including analysis of phishing kits and credential theft workflows, suggests this activity was likely performed by a DPRK actor exploring broader regional interests, possibly in alignment with Chinese strategic goals. Researchers have noted that Kimsuky operators have recently asked questions in phishing lures related to potential Chinese-Taiwanese conflicts, implying interest beyond the Korean peninsula.&lt;/p&gt;
    &lt;p&gt;Some tooling overlaps with PRC-linked APTs, particularly GitHub-based stagers and proxy-resolving modules, but these are not uncommon in the open-source malware ecosystem and may reflect opportunistic reuse rather than deliberate mimicry.&lt;/p&gt;
    &lt;head rend="h2"&gt;IMINT Analysis: Visual Tradecraft and Cultural Camouflage&lt;/head&gt;
    &lt;p&gt;A review of image artifacts linked to the “Kim” actor reveals a deliberate and calculated use of Chinese social and technological visual content as part of their operational persona. These images, extracted from browser history and uploads attributed to the actor, demonstrate both strategic alignment with DPRK priorities and active cultural camouflage within the PRC digital ecosystem.&lt;/p&gt;
    &lt;p&gt;The visual set includes promotional graphics for Honor smartphones, SoC chipset evolution charts, Weibo posts featuring vehicle registration certificates, meme-based sarcasm, and lifestyle imagery typical of Chinese internet users. Notably, the content is exclusively rendered in simplified Chinese, reinforcing prior assessments that the operator either resides within mainland China or maintains a working digital identity embedded in Chinese platforms. Devices and services referenced, such as Xiaomi phones, Zhihu, Weibo, and Baidu, suggest intimate familiarity with PRC user environments.&lt;/p&gt;
    &lt;p&gt;Operationally, this behavior achieves two goals. First, it enables the actor to blend in seamlessly with native PRC user activity, which complicates attribution and helps bypass platform moderation or behavioral anomaly detection. Second, the content itself may serve as bait or credibility scaffolding (e.g. A framework to give the illusion of trust to allow for easier compromise ) in phishing and social engineering campaigns, especially those targeting developers or technical users on Chinese-language platforms.&lt;/p&gt;
    &lt;p&gt;Some images, such as the detailed chipset timelines and VPN or device certification posts, suggest a continued interest in supply chain reconnaissance and endpoint profiling—both tradecraft hallmarks of Kimsuky and similar APT units. Simultaneously, meme humor, sarcastic overlays, and visual metaphors (e.g., the “Kaiju’s tail is showing” idiom) indicate the actor’s fluency in PRC netizen culture and possible mockery of operational security breaches—whether their own or others’.&lt;/p&gt;
    &lt;p&gt;Taken together, this IMINT corpus supports the broader attribution model: a DPRK-origin operator embedded, physically or virtually, within the PRC, leveraging local infrastructure and social platforms to facilitate long-term campaigns against South Korea, Taiwan, and other regional targets while maintaining cultural and technical deniability.&lt;/p&gt;
    &lt;head rend="h2"&gt;Attribution Scenarios:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Option A: DPRK Operator Embedded in PRC&lt;list rend="ul"&gt;&lt;item&gt;Use of Korean language, OCR targeting of Korean documents, and focus on GPKI systems strongly suggest North Korean origin.&lt;/item&gt;&lt;item&gt;Use of PRC infrastructure (e.g., Baidu, Gitee) and simplified Chinese content implies the operator is physically located in China or benefits from access to Chinese internet infrastructure.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Use of Korean language, OCR targeting of Korean documents, and focus on GPKI systems strongly suggest North Korean origin.&lt;/item&gt;
      &lt;item&gt;Option B: PRC Operator Emulating DPRK&lt;list rend="ul"&gt;&lt;item&gt;Taiwan-focused reconnaissance aligns with PRC cyber priorities.&lt;/item&gt;&lt;item&gt;Use of open-source tooling and phishing methods shared with PRC APTs could indicate tactical emulation.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Taiwan-focused reconnaissance aligns with PRC cyber priorities.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The preponderance of evidence supports the hypothesis that “Kim” is a North Korean cyber operator embedded in China or collaborating with PRC infrastructure providers. This operational model allows the DPRK to amplify its reach, mask attribution, and adopt regional targeting strategies beyond South Korea, particularly toward Taiwan. As this hybrid model matures, it reflects the strategic adaptation of DPRK-aligned threat actors who exploit the permissive digital environment of Chinese networks to evade detection and expand their operational playbook.&lt;/p&gt;
    &lt;head rend="h2"&gt;Targeting Profiles&lt;/head&gt;
    &lt;p&gt;The “Kim” leak provides one of the clearest windows to date into the role-specific targeting preferences of the operator, revealing a deliberate focus on system administrators, credential issuers, and backend developers, particularly in South Korea and Taiwan.&lt;/p&gt;
    &lt;p&gt;In South Korea, the operator’s interest centers around PKI administrators and infrastructure engineers. The recovered OCR commands were used to extract technical details from PDF documents outlining Korea’s digital signature protocols, such as identity verification, certificate validation, and encrypted communications, components that form the backbone of Korea’s secure authentication systems. The goal appears to be not only credential theft but full understanding and potential replication of government-trusted PKI procedures. This level of targeting suggests a strategic intent to penetrate deeply trusted systems, potentially for use in later spoofing or identity masquerading operations.&lt;/p&gt;
    &lt;p&gt;In Taiwan, the operator shifted focus to developer infrastructure and cloud access portals. Specific domains accessed, like caa.org[.]tw/.git/, indicate attempts to enumerate internal repositories, most likely to discover hardcoded secrets, authentication tokens, or deployment keys. This is a classic supply chain targeting method, aiming to access downstream systems via compromised developer credentials or misconfigured services.&lt;/p&gt;
    &lt;p&gt;Additional activity pointed to interaction with cloud service login panels such as tw.systexcloud[.]com and mlogin.mdfapps[.]com. These suggest an attempt to breach centralized authentication systems or identity providers, granting the actor broader access into enterprise or government networks with a single credential set.&lt;/p&gt;
    &lt;p&gt;Taken together, these targeting profiles reflect a clear emphasis on identity providers, backend engineers, and those with access to system-level secrets. This reinforces the broader theme of the dump: persistent, credential-first intrusion strategies, augmented by reconnaissance of authentication standards, key management policies, and endpoint development infrastructure.&lt;/p&gt;
    &lt;p&gt;South Korean:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;PKI admins, infrastructure engineers&lt;/item&gt;
      &lt;item&gt;OCR focus on Korean identity standards&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Taiwanese:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Developer endpoints and internal .git/ repos&lt;/item&gt;
      &lt;item&gt;Access to cloud panels and login gateways&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Final Assessment&lt;/head&gt;
    &lt;p&gt;The “Kim” leak represents one of the most comprehensive and technically intimate disclosures ever associated with Kimsuky (APT43) or its adjacent operators. It not only reaffirms known tactics, credential theft, phishing, and PKI compromise, but exposes the inner workings of the operator’s environment, tradecraft, and operational intent in ways rarely observed outside of active forensic investigations.&lt;/p&gt;
    &lt;p&gt;At the core of the leak is a technically competent actor, well-versed in low-level shellcode development, Linux-based persistence mechanisms, and certificate infrastructure abuse. Their use of NASM, API hashing, and rootkit deployment points to custom malware authorship. Furthermore, the presence of parsed government-issued Korean PDFs, combined with OCR automation, shows not just opportunistic data collection but a concerted effort to model, mimic, or break state-level identity systems, particularly South Korea’s GPKI.&lt;/p&gt;
    &lt;p&gt;The operator’s cultural and linguistic fluency in Korean, and their targeting of administrative and privileged systems across South Korean institutions, support a high-confidence attribution to a DPRK-native threat actor. However, the extensive use of Chinese platforms like gitee[.]com, Baidu, and Zhihu, and Chinese infrastructure for both malware hosting and browsing activity reveals a geographical pivot or collaboration: a hybrid APT footprint rooted in DPRK tradecraft but operating from or with Chinese support.&lt;/p&gt;
    &lt;p&gt;Most notably, this leak uncovers a geographical expansion of operational interest; the actor is no longer solely focused on the Korean peninsula. The targeting of Taiwanese developer portals, government research IPs, and .git/ repositories shows a broadened agenda that likely maps to both espionage and supply chain infiltration priorities. This places Taiwan, like South Korea, at the forefront of North Korean cyber interest, whether for intelligence gathering, credential hijacking, or as staging points for more complex campaigns.&lt;/p&gt;
    &lt;p&gt;The threat uncovered here is not merely malware or phishing; it is an infrastructure-centric, credential-first APT campaign that blends highly manual operations (e.g., hand-compiled shellcode, direct OCR of sensitive PDFs) with modern deception tactics such as AiTM phishing and TLS proxy abuse.&lt;/p&gt;
    &lt;p&gt;Organizations in Taiwan and South Korea, particularly those managing identity, certificate, and cloud access infrastructure, should consider themselves under persistent, credential-focused surveillance. Defensive strategies must prioritize detection of behavioral anomalies (e.g., use of OCR tools, GPKI access attempts), outbound communications with spoofed Korean domains, and the appearance of low-level toolchains like NASM or proxyres-based scanning utilities within developer or admin environments.&lt;/p&gt;
    &lt;p&gt;In short: the “Kim” actor embodies the evolution of nation-state cyber threats—a fusion of old-school persistence, credential abuse, and modern multi-jurisdictional staging. The threat is long-term, embedded, and adaptive.&lt;/p&gt;
    &lt;head rend="h1"&gt;Part III: Threat Intelligence Report&lt;/head&gt;
    &lt;head rend="h2"&gt;TLP WHITE:&lt;/head&gt;
    &lt;head rend="h3"&gt;Targeting Summary&lt;/head&gt;
    &lt;p&gt;The analysis of the “Kim” operator dump reveals a highly focused credential-theft and infrastructure-access campaign targeting high-value assets in both South Korea and Taiwan. Victims were selected based on their proximity to trusted authentication systems, administrative control panels, and development environments.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Category&lt;/cell&gt;
        &lt;cell&gt;Details&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Regions&lt;/cell&gt;
        &lt;cell&gt;South Korea, Taiwan&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Targets&lt;/cell&gt;
        &lt;cell&gt;Government, Telecom, Enterprise IT&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Accounts&lt;/cell&gt;
        &lt;cell&gt;svradmin, oracle, app_adm01, unwadm, shkim88, jaejung91&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Domains&lt;/cell&gt;
        &lt;cell&gt;tw.systexcloud[.]com, nid-security[.]com, spo.go[.]kr, caa.org[.]tw/.git/&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Indicators of Compromise (IOCs)&lt;/head&gt;
    &lt;head rend="h4"&gt;Domains&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Phishing: nid-security[.]com, html-load[.]com, wuzak[.]com, koala-app[.]com, webcloud-notice[.]com&lt;/item&gt;
      &lt;item&gt;Spoofed portals: dcc.mil[.]kr, spo.go[.]kr, mofa.go[.]kr&lt;/item&gt;
      &lt;item&gt;Pastebin raw links: Used for payload staging and malware delivery&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;IP Addresses&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;External Targets (Taiwan): &lt;list rend="ul"&gt;&lt;item&gt;163.29.3[.]119 National Center for High-performance Computing&lt;/item&gt;&lt;item&gt;118.163.30[.]45 Taiwanese government subnet&lt;/item&gt;&lt;item&gt;59.125.159[.]81 Chunghwa Telecom&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Brute Forcing / Infrastructure Origins: &lt;list rend="ul"&gt;&lt;item&gt;23.95.213[.]210 VPS provider with malicious history&lt;/item&gt;&lt;item&gt;218.92.0[.]210 China Unicom&lt;/item&gt;&lt;item&gt;122.114.233[.]77 Henan Mobile, PRC&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Internal Host IPs (Operator Environment)&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;192.168.130[.]117&lt;/item&gt;
      &lt;item&gt;192.168.150[.]117&lt;/item&gt;
      &lt;item&gt;192.168.0[.]39&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Operator Environment: Internal Host IP Narrative&lt;/head&gt;
    &lt;p&gt;The presence of internal IP addresses such as 192.168.130[.]117, 192.168.150[.]117, and 192.168.0[.]39 within the dump offers valuable insight into the attacker’s local infrastructure, an often-overlooked element in threat intelligence analysis. These addresses fall within private, non-routable RFC1918 address space, commonly assigned by consumer off-the-shelf (COTS) routers and small office/home office (SOHO) network gear.&lt;/p&gt;
    &lt;p&gt;The use of the 192.168.0[.]0/16 subnet, particularly 192.168.0.x and 192.168.150.x, strongly suggests that the actor was operating from a residential or low-profile environment, not a formal nation-state facility or hardened infrastructure. This supports existing assessments that North Korean operators, particularly those affiliated with Kimsuky, often work remotely from locations in third countries such as China or Southeast Asia, where they can maintain inconspicuous, low-cost setups while accessing global infrastructure.&lt;/p&gt;
    &lt;p&gt;Moreover, the distinction between multiple internal subnets (130.x, 150.x, and 0.x) may indicate segmentation of test environments or multiple virtual machines running within a single NATed network. This aligns with the forensic evidence of iterative development and testing workflows seen in the .bash_history files, where malware stagers, rootkits, and API obfuscation utilities were compiled, cleaned, and rerun repeatedly.&lt;/p&gt;
    &lt;p&gt;Together, these IPs reveal an operator likely working from a clandestine, residential base of operations, with modest hardware and commercial-grade routers. This operational setup is consistent with known DPRK remote IT workers and cyber operators who avoid attribution by blending into civilian infrastructure. It also suggests the attacker may be physically located outside of North Korea, possibly embedded in a friendly or complicit environment, strengthening the case for China-based activity by DPRK nationals.&lt;/p&gt;
    &lt;head rend="h3"&gt;MITRE ATT&amp;amp;CK Mapping&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Phase&lt;/cell&gt;
        &lt;cell&gt;Technique(s)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Initial Access&lt;/cell&gt;
        &lt;cell&gt;T1566.002 , Adversary-in-the-Middle (AiTM) Phishing&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Execution&lt;/cell&gt;
        &lt;cell&gt;T1059.005 , Native API ShellcodeT1059.003 , Bash/Shell Scripts&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Credential Access&lt;/cell&gt;
        &lt;cell&gt;T1555 , Credential Store DumpingT1557.003 , Session Hijacking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Persistence&lt;/cell&gt;
        &lt;cell&gt;T1176 , Rootkit (via khook syscall manipulation)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Defense Evasion&lt;/cell&gt;
        &lt;cell&gt;T1562.001 , Disable Security ToolsT1552 , Unsecured Credential Files&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Discovery&lt;/cell&gt;
        &lt;cell&gt;T1592 , Technical Information DiscoveryT1590 , Network Information&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Exfiltration&lt;/cell&gt;
        &lt;cell&gt;T1041 , Exfiltration over C2 ChannelT1567.002 , Exfil via Cloud Services&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Tooling and Capabilities&lt;/head&gt;
    &lt;p&gt;The actor’s toolkit spans multiple disciplines, blending malware development, system reconnaissance, phishing, and proxy evasion:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;NASM-based shellcode loaders: Compiled manually for Windows execution.&lt;/item&gt;
      &lt;item&gt;Win32 API hashing: Obfuscated imports via hashstring.py to evade detection.&lt;/item&gt;
      &lt;item&gt;GitHub/Gitee abuse: Tooling hosted or cloned from public developer platforms.&lt;/item&gt;
      &lt;item&gt;OCR exploitation: Used ocrmypdf to parse Korean PDF specs related to digital certificates and VPN appliances.&lt;/item&gt;
      &lt;item&gt;Rootkit deployment: Hidden persistence paths including /usr/lib64/tracker-fs and /proc/acpi/pcicard.&lt;/item&gt;
      &lt;item&gt;Proxy config extraction: Investigated PAC URLs using proxyres-based recon.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Attribution Confidence Assessment&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Attribution Candidate&lt;/cell&gt;
        &lt;cell&gt;Confidence Level&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;DPRK-aligned (Kimsuky)&lt;/cell&gt;
        &lt;cell&gt;High, Native Korean targeting, GPKI focus, OCR behavior&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;China-blended infrastructure&lt;/cell&gt;
        &lt;cell&gt;Moderate, PRC hosting, Gitee usage, Taiwan focus&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Solely PRC Actor&lt;/cell&gt;
        &lt;cell&gt;Low-to-Moderate, Tooling overlap but weak linguistic match&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Assessment: The actor appears to be a DPRK-based APT operator working from within or in partnership with Chinese infrastructure, representing a hybrid attribution model.&lt;/p&gt;
    &lt;head rend="h3"&gt;Defensive Recommendations&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Area&lt;/cell&gt;
        &lt;cell&gt;Recommendation&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;PKI Security&lt;/cell&gt;
        &lt;cell&gt;Monitor usage of .key, .sig, .crt artifacts; enforce HSM or 2FA for key use&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Phishing Defense&lt;/cell&gt;
        &lt;cell&gt;Block domains identified in IoCs; validate TLS fingerprints and referrer headers&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Endpoint Hardening&lt;/cell&gt;
        &lt;cell&gt;Detect use of nasm, make, and OCR tools; monitor /usr/lib*/tracker-* paths&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Network Telemetry&lt;/cell&gt;
        &lt;cell&gt;Alert on .git/ directory access from external IPs; monitor outbound to Pastebin/GitHub&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Taiwan Focus&lt;/cell&gt;
        &lt;cell&gt;Establish watchlists for .tw domains targeted by PRC-originating IPs&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Admin Accounts&lt;/cell&gt;
        &lt;cell&gt;Review usage logs for svradmin, oracle, app_adm01, and ensure rotation policies&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h1"&gt;APPENDIX A&lt;/head&gt;
    &lt;head rend="h2"&gt;Overlap or Confusion with Chinese Threat Actors&lt;/head&gt;
    &lt;p&gt;There is notable evidence of operational blur between Kimsuky and Chinese APTs in the context of Taiwan. The 2025 “Kim” data breach revealed an attacker targeting Taiwan whose tools and phishing kits matched Kimsuky’s, yet whose personal indicators (language, browsing habits) suggested a Chinese national. Researchers concluded this actor was likely a Chinese hacker either mimicking Kimsuky tactics or collaborating with them.. In fact, the leaked files on DDoS Secrets hint that Kimsuky has “openly cooperated with other Chinese APTs and shared their tools and techniques”. This overlap can cause attribution confusion – a Taiwan-focused operation might initially be blamed on China but could involve Kimsuky elements, or vice versa. So far, consensus is that North Korean and Chinese cyber operations remain separate, but cases like “Kim” show how a DPRK-aligned actor can operate against Taiwan using TTPs common to Chinese groups, muddying the waters of attribution.&lt;/p&gt;
    &lt;head rend="h2"&gt;File List from dump:&lt;/head&gt;
    &lt;head rend="h2"&gt;Master Evidence Inventory:&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;File Name&lt;/cell&gt;
        &lt;cell&gt;Language&lt;/cell&gt;
        &lt;cell&gt;Content Summary&lt;/cell&gt;
        &lt;cell&gt;Category&lt;/cell&gt;
        &lt;cell&gt;Relevance&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;.bash_history&lt;/cell&gt;
        &lt;cell&gt;Mixed (EN/KR)&lt;/cell&gt;
        &lt;cell&gt;Operator shell history commands&lt;/cell&gt;
        &lt;cell&gt;System/Log&lt;/cell&gt;
        &lt;cell&gt;Shows rootkit compilation, file ops, network tests&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;user-bash_history&lt;/cell&gt;
        &lt;cell&gt;Mixed (EN/KR)&lt;/cell&gt;
        &lt;cell&gt;User-level shell commands&lt;/cell&gt;
        &lt;cell&gt;System/Log&lt;/cell&gt;
        &lt;cell&gt;Development and test activity&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;root-bash_history&lt;/cell&gt;
        &lt;cell&gt;Mixed (EN/KR)&lt;/cell&gt;
        &lt;cell&gt;Root-level shell commands&lt;/cell&gt;
        &lt;cell&gt;System/Log&lt;/cell&gt;
        &lt;cell&gt;Privilege-level activity, implant deployment&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;auth.log.2&lt;/cell&gt;
        &lt;cell&gt;EN/KR&lt;/cell&gt;
        &lt;cell&gt;Authentication logs (PAM/SSH)&lt;/cell&gt;
        &lt;cell&gt;System/Log&lt;/cell&gt;
        &lt;cell&gt;Credential changes marked 변경완료, brute force IPs&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;20190315.log&lt;/cell&gt;
        &lt;cell&gt;EN&lt;/cell&gt;
        &lt;cell&gt;System log file&lt;/cell&gt;
        &lt;cell&gt;System/Log&lt;/cell&gt;
        &lt;cell&gt;Auth and system access events&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;chrome-timeline.txt&lt;/cell&gt;
        &lt;cell&gt;EN&lt;/cell&gt;
        &lt;cell&gt;Browser activity timeline&lt;/cell&gt;
        &lt;cell&gt;Browser&lt;/cell&gt;
        &lt;cell&gt;Visited domains extraction&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;chromehistory.txt&lt;/cell&gt;
        &lt;cell&gt;EN&lt;/cell&gt;
        &lt;cell&gt;Browser history export&lt;/cell&gt;
        &lt;cell&gt;Browser&lt;/cell&gt;
        &lt;cell&gt;URLs visited&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;history.sqlite&lt;/cell&gt;
        &lt;cell&gt;EN&lt;/cell&gt;
        &lt;cell&gt;Empty DB file&lt;/cell&gt;
        &lt;cell&gt;Browser&lt;/cell&gt;
        &lt;cell&gt;No useful data&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Media History&lt;/cell&gt;
        &lt;cell&gt;EN&lt;/cell&gt;
        &lt;cell&gt;Empty SQLite DB&lt;/cell&gt;
        &lt;cell&gt;Browser&lt;/cell&gt;
        &lt;cell&gt;No playback activity&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;History&lt;/cell&gt;
        &lt;cell&gt;EN&lt;/cell&gt;
        &lt;cell&gt;Empty Brave/Chromium DB&lt;/cell&gt;
        &lt;cell&gt;Browser&lt;/cell&gt;
        &lt;cell&gt;No visited URLs&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Web Data&lt;/cell&gt;
        &lt;cell&gt;EN&lt;/cell&gt;
        &lt;cell&gt;Autofill/search DB&lt;/cell&gt;
        &lt;cell&gt;Browser&lt;/cell&gt;
        &lt;cell&gt;Search engines used (Google, DuckDuckGo, Qwant, Startpage, Ecosia)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Visited Links&lt;/cell&gt;
        &lt;cell&gt;Binary&lt;/cell&gt;
        &lt;cell&gt;LevelDB/binary structure&lt;/cell&gt;
        &lt;cell&gt;Browser&lt;/cell&gt;
        &lt;cell&gt;Could not extract URLs&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Cookies&lt;/cell&gt;
        &lt;cell&gt;EN&lt;/cell&gt;
        &lt;cell&gt;SQLite DB with cookies&lt;/cell&gt;
        &lt;cell&gt;Browser&lt;/cell&gt;
        &lt;cell&gt;Google cookies found&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;request_log.txt.20250220&lt;/cell&gt;
        &lt;cell&gt;EN&lt;/cell&gt;
        &lt;cell&gt;Captured phishing session&lt;/cell&gt;
        &lt;cell&gt;Phishing&lt;/cell&gt;
        &lt;cell&gt;Spoofed spo.go.kr, base64 credential logging&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;技术说明书 – 22.docx&lt;/cell&gt;
        &lt;cell&gt;ZH&lt;/cell&gt;
        &lt;cell&gt;Chinese rootkit stealth manual&lt;/cell&gt;
        &lt;cell&gt;Rootkit&lt;/cell&gt;
        &lt;cell&gt;Kernel hiding, binary embedding&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;1.ko 图文编译 .doc&lt;/cell&gt;
        &lt;cell&gt;ZH&lt;/cell&gt;
        &lt;cell&gt;Chinese compilation guide&lt;/cell&gt;
        &lt;cell&gt;Rootkit&lt;/cell&gt;
        &lt;cell&gt;Rootkit build process&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;1. build ko .txt&lt;/cell&gt;
        &lt;cell&gt;ZH&lt;/cell&gt;
        &lt;cell&gt;Build notes&lt;/cell&gt;
        &lt;cell&gt;Rootkit&lt;/cell&gt;
        &lt;cell&gt;Implant compilation instructions&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;0. 使用.txt&lt;/cell&gt;
        &lt;cell&gt;ZH&lt;/cell&gt;
        &lt;cell&gt;Usage notes&lt;/cell&gt;
        &lt;cell&gt;Rootkit&lt;/cell&gt;
        &lt;cell&gt;Implant usage and commands&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;re 正向工具修改建议 1.0.txt&lt;/cell&gt;
        &lt;cell&gt;ZH&lt;/cell&gt;
        &lt;cell&gt;Modification notes&lt;/cell&gt;
        &lt;cell&gt;Rootkit&lt;/cell&gt;
        &lt;cell&gt;Reverse tool modification suggestions&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;1111.txt&lt;/cell&gt;
        &lt;cell&gt;ZH&lt;/cell&gt;
        &lt;cell&gt;Rootkit/tool snippet&lt;/cell&gt;
        &lt;cell&gt;Rootkit&lt;/cell&gt;
        &lt;cell&gt;Part of implant notes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;client&lt;/cell&gt;
        &lt;cell&gt;Binary&lt;/cell&gt;
        &lt;cell&gt;Rootkit client binary&lt;/cell&gt;
        &lt;cell&gt;Rootkit&lt;/cell&gt;
        &lt;cell&gt;Controller for implant communication&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;SSA_AO_AD_WT_002_웹보안 프로토콜설계서_Ver1.0_.doc&lt;/cell&gt;
        &lt;cell&gt;KR&lt;/cell&gt;
        &lt;cell&gt;GPKI protocol design doc&lt;/cell&gt;
        &lt;cell&gt;PKI&lt;/cell&gt;
        &lt;cell&gt;Korean web PKI standards&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;행자부 웹보안API 인수인계.doc&lt;/cell&gt;
        &lt;cell&gt;KR&lt;/cell&gt;
        &lt;cell&gt;GPKI API deployment manual&lt;/cell&gt;
        &lt;cell&gt;PKI&lt;/cell&gt;
        &lt;cell&gt;Deployment and cert API internals&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;HIRA-IR-T02_의약품처방조제_ComLibrary_통신전문.doc&lt;/cell&gt;
        &lt;cell&gt;KR&lt;/cell&gt;
        &lt;cell&gt;Medical ComLibrary XML spec&lt;/cell&gt;
        &lt;cell&gt;Healthcare&lt;/cell&gt;
        &lt;cell&gt;Prescription system communication&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;(별지2)행정전자서명_기술요건_141125.pdf&lt;/cell&gt;
        &lt;cell&gt;KR&lt;/cell&gt;
        &lt;cell&gt;PKI requirements PDF&lt;/cell&gt;
        &lt;cell&gt;PKI&lt;/cell&gt;
        &lt;cell&gt;OCR target&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;SecuwaySSL U_카달로그.pdf&lt;/cell&gt;
        &lt;cell&gt;KR&lt;/cell&gt;
        &lt;cell&gt;VPN catalog&lt;/cell&gt;
        &lt;cell&gt;PKI/VPN&lt;/cell&gt;
        &lt;cell&gt;OCR target&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;phrack-apt-down-the-north-korea-files.pdf&lt;/cell&gt;
        &lt;cell&gt;EN&lt;/cell&gt;
        &lt;cell&gt;Phrack article&lt;/cell&gt;
        &lt;cell&gt;Reference&lt;/cell&gt;
        &lt;cell&gt;Background on Kimsuky dump&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Muddled Libra Threat Assessment.pdf&lt;/cell&gt;
        &lt;cell&gt;EN&lt;/cell&gt;
        &lt;cell&gt;Threat intel report&lt;/cell&gt;
        &lt;cell&gt;Reference&lt;/cell&gt;
        &lt;cell&gt;Comparative threat actor study&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Leaked North Korean Linux Stealth Rootkit Analysis.pdf&lt;/cell&gt;
        &lt;cell&gt;EN&lt;/cell&gt;
        &lt;cell&gt;Rootkit analysis&lt;/cell&gt;
        &lt;cell&gt;Reference&lt;/cell&gt;
        &lt;cell&gt;Detailed implant study&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Inside the Kimsuky Leak.docx (various)&lt;/cell&gt;
        &lt;cell&gt;EN&lt;/cell&gt;
        &lt;cell&gt;Threat report drafts&lt;/cell&gt;
        &lt;cell&gt;Report&lt;/cell&gt;
        &lt;cell&gt;Working versions&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;account (2).txt&lt;/cell&gt;
        &lt;cell&gt;EN&lt;/cell&gt;
        &lt;cell&gt;DB export (DBsafer, TrustedOrange)&lt;/cell&gt;
        &lt;cell&gt;Infra&lt;/cell&gt;
        &lt;cell&gt;Accounts and DB changes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;result.txt&lt;/cell&gt;
        &lt;cell&gt;KR&lt;/cell&gt;
        &lt;cell&gt;Cert-related parsed data&lt;/cell&gt;
        &lt;cell&gt;Infra&lt;/cell&gt;
        &lt;cell&gt;Included GPKI .key/.sig&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;english_wikipedia.txt&lt;/cell&gt;
        &lt;cell&gt;EN&lt;/cell&gt;
        &lt;cell&gt;Wikipedia dump&lt;/cell&gt;
        &lt;cell&gt;Reference&lt;/cell&gt;
        &lt;cell&gt;Unrelated baseline&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;bookmarks-2021-01-04.jsonlz4&lt;/cell&gt;
        &lt;cell&gt;EN&lt;/cell&gt;
        &lt;cell&gt;Firefox bookmarks (compressed)&lt;/cell&gt;
        &lt;cell&gt;Browser&lt;/cell&gt;
        &lt;cell&gt;Needs decompression&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Screenshot translations&lt;/cell&gt;
        &lt;cell&gt;ZH&lt;/cell&gt;
        &lt;cell&gt;Chinese text (rootkit marketing blurb)&lt;/cell&gt;
        &lt;cell&gt;Rootkit&lt;/cell&gt;
        &lt;cell&gt;Kernel hiding tool description&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45152066</guid></item><item><title>A Navajo weaving of an integrated circuit: the 555 timer</title><link>https://www.righto.com/2025/09/marilou-schultz-navajo-555-weaving.html</link><description>&lt;doc fingerprint="2f555263dc670aeb"&gt;
  &lt;main&gt;
    &lt;p&gt;The noted Diné (Navajo) weaver Marilou Schultz recently completed an intricate weaving composed of thick white lines on a black background, punctuated with reddish-orange diamonds. Although this striking rug may appear abstract, it shows the internal circuitry of a tiny silicon chip known as the 555 timer. This chip has hundreds of applications in everything from a sound generator to a windshield wiper controller. At one point, the 555 was the world's best-selling integrated circuit with billions sold. But how did the chip get turned into a rug?&lt;/p&gt;
    &lt;p&gt;The 555 chip is constructed from a tiny flake of silicon with a layer of metallic wiring on top. In the rug, this wiring is visible as the thick white lines, while the silicon forms the black background. One conspicuous feature of the rug is the reddish-orange diamonds around the perimeter. These correspond to the connections between the silicon chip and its eight pins. Tiny golden bond wires—thinner than a human hair—are attached to the square bond pads to provide these connections. The circuitry of the 555 chip contains 25 transistors, silicon devices that can switch on and off. The rug is dominated by three large transistors, the filled squares with a 王 pattern inside, while the remaining transistors are represented by small dots.&lt;/p&gt;
    &lt;p&gt;The weaving was inspired by a photo of the 555 timer die taken by Antoine Bercovici (Siliconinsider); I suggested this photo to Schultz as a possible subject for a rug. The diagram below compares the weaving (left) with the die photo (right). As you can see, the weaving closely follows the actual chip, but there are a few artistic differences. For instance, two of the bond pads have been removed, the circuitry at the top has been simplified, and the part number at the bottom has been removed.&lt;/p&gt;
    &lt;p&gt;Antoine took the die photo with a dark field microscope, a special type of microscope that produces an image on a black background. This image emphasizes the metal layer on the top of the die. In comparison, a standard bright-field microscope produced the image below. When a chip is manufactured, regions of silicon are "doped" with impurities to create transistors and resistors. These regions are visible in the image below as subtle changes in the color of the silicon.&lt;/p&gt;
    &lt;p&gt;In the weaving, the chip's design appears almost monumental, making it easy to forget that the actual chip is microscopic. For the photo below, I obtained a version of the chip packaged in a metal can, rather than the typical rectangle of black plastic. Cutting the top off the metal can reveals the tiny chip inside, with eight gold bond wires connecting the die to the pins of the package. If you zoom in on the photo, you may recognize the three large transistors that dominate the rug.&lt;/p&gt;
    &lt;p&gt;The artist, Marilou Schultz, has been creating chip rugs since 1994, when Intel commissioned a rug based on the Pentium as a gift to AISES (American Indian Science &amp;amp; Engineering Society). Although Schultz learned weaving as a child, the Pentium rug was a challenge due to its complex pattern and lack of symmetry; a day's work might add just an inch to the rug. This dramatic weaving was created with wool from the long-horned Navajo-Churro sheep, colored with traditional plant dyes.&lt;/p&gt;
    &lt;p&gt;For the 555 timer weaving, Schultz experimented with different materials. Silver and gold metallic threads represent the aluminum and copper in the chip. The artist explains that "it took a lot more time to incorporate the metallic threads," but it was worth the effort because "it is spectacular to see the rug with the metallics in the dark with a little light hitting it." Aniline dyes provided the black and lavender colors. Although natural logwood dye produces a beautiful purple, it fades over time, so Schultz used an aniline dye instead. The lavender colors are dedicated to the weaver's mother, who passed away in February; purple was her favorite color.&lt;/p&gt;
    &lt;head rend="h2"&gt;Inside the chip&lt;/head&gt;
    &lt;p&gt;How does the 555 chip produce a particular time delay? You add external components—resistors and a capacitor—to select the time. The capacitor is filled (charged) at a speed controlled by the resistor. When the capacitor get "full", the 555 chip switches operation and starts emptying (discharging) the capacitor. It's like filling a sink: if you have a large sink (capacitor) and a trickle of water (large resistor), the sink fills slowly. But if you have a smal sink (capacitor) and a lot of water (small resistor), the sink fills quickly. By using different resistors and capacitors, the 555 timer can provide time intervals from microseconds to hours.&lt;/p&gt;
    &lt;p&gt;I've constructed an interactive chip browser that shows how the regions of the rug correspond to specific electronic components in the physical chip. Click on any part of the rug to learn the function of the corresponding component in the chip.&lt;/p&gt;
    &lt;p&gt;For instance, two of the large square transistors turn the chip's output on or off, while the third large transistor discharges the capacitor when it is full. (To be precise, the capacitor goes between 1/3 full and 2/3 full to avoid issues near "empty" and "full".) The chip has circuits called comparators that detect when the capacitor's voltage reaches 1/3 or 2/3, switching between emptying and filling at those points. If you want more technical details about the 555 chip, see my previous articles: an early 555 chip, a 555 timer similar to the rug, and a more modern CMOS version of the 555.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusions&lt;/head&gt;
    &lt;p&gt;The similarities between Navajo weavings and the patterns in integrated circuits have long been recognized. Marilou Schultz's weavings of integrated circuits make these visual metaphors into concrete works of art. This connection is not just metaphorical, however; in the 1960s, the semiconductor company Fairchild employed numerous Navajo workers to assemble chips in Shiprock, New Mexico. I wrote about this complicated history in The Pentium as a Navajo Weaving.&lt;/p&gt;
    &lt;p&gt;This work is being shown at SITE Santa Fe's Once Within a Time exhibition (running until January 2026). I haven't seen the exhibition in person, so let me know if you visit it. For more about Marilou Schultz's art, see The Diné Weaver Who Turns Microchips Into Art, or A Conversation with Marilou Schultz on YouTube.&lt;/p&gt;
    &lt;p&gt;Many thanks to Marilou Schultz for discussing her art with me. Thanks to First American Art Magazine for providing the photo of her 555 rug. Follow me on Mastodon (@[email protected]), Bluesky (@righto.com), or RSS for updates.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45152779</guid></item><item><title>Show HN: I'm making an open-source platform for learning Japanese</title><link>https://kanadojo.com</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45152940</guid></item><item><title>The key to getting MVC correct is understanding what models are</title><link>https://stlab.cc/tips/about-mvc.html</link><description>&lt;doc fingerprint="1f5bdf1aa5aa0a18"&gt;
  &lt;main&gt;
    &lt;p&gt;How did MVC get so F’ed up?&lt;/p&gt;
    &lt;p&gt;Smalltalk MVC is defined in Design Pattern as:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;MVC Consists of three kinds of objects. The Model is the application object, the View is its screen presentation, and the Controller defines the way the user interface reacts to user input.1&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;However this definition has been abused over the years - Back in 2003 I gave a talk citing how bad Apple’s definition was. At the time it stated:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;A view object knows how to display and possibly edit data from the application’s model… A controller object acts as the intermediary between the application’s view objects and its model objects… Controllers are often the least reusable objects in an application, but that’s acceptable…2&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Of course it isn’t acceptable and, over the years, Apple has refined their definition and now acknowledge the distinction between the traditional Smalltalk version of MVC and the Cocoa version.3 But the Cocoa version is still defined much as it was before:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;A view object knows how to display, and might allow users to edit, the data from the application’s model… A controller object acts as the intermediary between the application’s view objects and its model objects…3&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;In looking at how iOS applications are written the sentiment that controllers (and now view-controllers) are often the least reusable components in an application still flourishes, even if it is now unstated.&lt;/p&gt;
    &lt;p&gt;MVC (I’ll always use that term to refer to the Smalltalk form) has the following structure:&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; figure: Smalltalk MVC4 &lt;/p&gt;
    &lt;p&gt;Here the solid lines imply a direct association. And the dashed lines an indirect association by an observer. So what we see is that the model is unaware of the view and controller, except indirectly through notifications, and hence the code in the Model is reusable. The controller and view bind to the model, not the other way around.&lt;/p&gt;
    &lt;p&gt;Often the function of the Controller and View are tightly coupled into a “widget” or “control”. When Apple talks about a View-Controller in their model they are talking about a grab-bag of an uber-widget that is a composite of UIView widgets and multiple models. From what I’ve seen, including in Apple’s example code, it is usually a pretty big mess.&lt;/p&gt;
    &lt;p&gt;The key to getting MVC correct is understanding what models are. A model is simply an object5 which can be observed (a requirement for attaching views). For example, in ObjC an int is an object, but it is not observable. However, an ObjC object with an int property is observable using Key-Value Observing6. A model may encapsulate complex relationships between the model’s properties. A trivial model is one where each property is completely independent (think C struct vs. C++ class). From a notification the view should be able to determine, at a minimum:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;What changed. It may be as simple as “the model bound to the view”.&lt;/item&gt;
      &lt;item&gt;The new value to display.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For example, let’s say our model is a trivial observable boolean (I can’t imagine a simpler model). What we want is a checkbox that binds to the observable boolean. When the controller requests a change in value, the boolean is updated, and the view is notified of the new state of the model. The model is unaware of what UI is attached to it, and in fact there could be multiple UIs, including something like a scripting system, attached to the same instance of the model. This is a form of data binding - though most data binding systems replicate the problems of their underlying widget set by treating the model as if it were observing the view, not the other way around.&lt;/p&gt;
    &lt;p&gt;Contrast this with most UI frameworks where you have a checkbox widget from which you can query the value and you receive a notification when the value has changed. This is pushing a model into the widget. With MVC you never ask a question like “what is the default state of this checkbox?” - the default state of the view is always the current state of the model. You would also never get the state of the checkbox - the state of the checkbox is simply a reflection of the state of the model. In a system where you get the state of a checkbox you are binding two models together by treating one as a view/controller of the other. Such a pattern doesn’t scale beyond trivial models, and even for those it introduces some ambiguity.&lt;/p&gt;
    &lt;p&gt;I conjecture that one of the reasons why MVC has been so screwed up is because, unlike in Smalltalk, writing something as simple as an observable boolean is a bit of a pain in a language like Pascal or C. You quickly get into object ownership and lifetime issues and how to write bind expressions. If one also assumes that you have a 1:1 mapping from UI to model then there is some inherent inefficiency in the generalization. The Lisa team made some major compromises and the rest of the industry followed along.7&lt;/p&gt;
    &lt;p&gt;To support more complex views, the notification may need to specify what parts of the model changed and how those parts changed. For example, “image 58 was removed from the sequence”. A complete model is one that can support any view of that model type efficiently (related to the notion of a complete type and a type’s efficient basis).&lt;/p&gt;
    &lt;p&gt;One additional attribute of MVC is that it is a composite pattern. This is hinted at by the direct connection between the Controller and the View. As I said early, the view may contain state, this state is itself an object, and because this state is also displayed within the view it is observable. It is another model. I refers to this as the view’s model. This model may include things such as the visibility of a window, the tab the user was last looking at, and the portion of the model being viewed.&lt;/p&gt;
    &lt;p&gt;Identifying what the models are in your system is important. We usually do pretty good at identifying the major models. Such as “this is an image” - but often fall short of identifying the complete model, i.e. “this is an image with a collection of settings.” We end up with our model spread out within the code (an incidental type) and it makes it more difficult to deal with it.&lt;/p&gt;
    &lt;p&gt;A common model that is often completely overlooked is the model for function arguments. When you have a command, button, gesture, or menu item in your application, these are bound to a function. The function itself is not typically a zeroary function but rather has a set of arguments that are constructed through other parts of the UI. For example, if I have a list of images in my application, I might have a button to delete the selected images. Here the current selection is the argument to my delete command. To create a UI for the selection I must create a model of the arguments to my function. A precondition of delete is that the selection is not empty. This precondition must be observable in the argument model so it can be reflected in the view by disabling or hiding the button and in the controller be disallowing the user to click the button and issue the command. The same argument model can be shared for multiple commands within an application.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Gamma, Erich. “1.2 Design Patterns in Smalltalk MVC.” Design Patterns: Elements of Reusable Object-Oriented Software. Reading, MA: Addison-Wesley, 1995. N. pag. Print. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;http://smartfriends.com/U/Presenters/untangling_software.pdf (Don’t bother reading, this was an incomprehensible talk.) ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;https://developer.apple.com/library/content/documentation/General/Conceptual/CocoaEncyclopedia/Model-View-Controller/Model-View-Controller.html ↩ ↩2&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;https://en.wikipedia.org/wiki/Model%E2%80%93view%E2%80%93controller ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Stepanov, Alexander A., and Paul McJones. “1.3 Objects.” Elements of Programming. Upper Saddle River, NJ: Addison-Wesley, 2009. N. pag. Print. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;https://developer.apple.com/library/content/documentation/Cocoa/Conceptual/KeyValueObserving/KeyValueObserving.html ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;https://en.wikipedia.org/wiki/Object-oriented_user_interface ↩&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45154501</guid></item><item><title>Show HN: Lightweight tool for managing Linux virtual machines</title><link>https://github.com/ccheshirecat/flint</link><description>&lt;doc fingerprint="eface9fad13539da"&gt;
  &lt;main&gt;
    &lt;p&gt; A single &amp;lt;8MB binary with a modern Web UI, CLI, and API for KVM. &lt;lb/&gt;No XML. No bloat. Just VMs. &lt;/p&gt;
    &lt;p&gt;Flint is a modern, self-contained KVM management tool built for developers, sysadmins, and home labs who want zero bloat and maximum efficiency. It was built in a few hours out of a sudden urge for something better.&lt;/p&gt;
    &lt;p&gt;Prerequisites: A Linux host with &lt;code&gt;libvirt&lt;/code&gt; and &lt;code&gt;qemu-kvm&lt;/code&gt; installed.&lt;/p&gt;
    &lt;code&gt;curl -fsSL https://raw.githubusercontent.com/ccheshirecat/flint/main/install.sh | sh&lt;/code&gt;
    &lt;p&gt;Auto-detects OS/arch, installs to &lt;code&gt;/usr/local/bin&lt;/code&gt;, and you're ready in seconds.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;🖥️ Modern UI — A beautiful, responsive Next.js + Tailwind interface, fully embedded.&lt;/item&gt;
      &lt;item&gt;⚡ Single Binary — No containers, no XML hell. A sub-8MB binary is all you need.&lt;/item&gt;
      &lt;item&gt;🛠️ Powerful CLI &amp;amp; API — Automate everything. If you can do it in the UI, you can do it from the command line or API.&lt;/item&gt;
      &lt;item&gt;📦 Frictionless Provisioning — Native Cloud-Init support and a simple, snapshot-based template system.&lt;/item&gt;
      &lt;item&gt;💪 Non-Intrusive — Flint is a tool that serves you. It's not a platform that locks you in.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;1. Start the Server&lt;/p&gt;
    &lt;code&gt;flint serve&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Web UI: &lt;code&gt;http://localhost:5550&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;API: &lt;code&gt;http://localhost:5550/api&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;2. Use the CLI&lt;/p&gt;
    &lt;code&gt;# List your VMs
flint vm list --all

# Launch a new Ubuntu VM named 'web-01'
flint launch ubuntu-24.04 --name web-01

# SSH directly into your new VM
flint ssh web-01

# Create a template from your configured VM
flint snapshot create web-01 --tag baseline-setup

# Launch a clone from your new template
flint launch --from web-01 --name web-02&lt;/code&gt;
    &lt;p&gt;While Flint is designed to be intuitive, the full CLI and API documentation, including all commands and examples, is available at:&lt;/p&gt;
    &lt;p&gt;➡️ DOCS.md&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Backend: Go 1.25+&lt;/item&gt;
      &lt;item&gt;Web UI: Next.js + Tailwind + Bun&lt;/item&gt;
      &lt;item&gt;KVM Integration: libvirt-go&lt;/item&gt;
      &lt;item&gt;Binary Size: ~8.4MB (stripped)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt; 🚀 Flint is young, fast-moving, and designed for builders.&lt;lb/&gt; Try it. Break it. Star it. Contribute. &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45154857</guid></item><item><title>The "impossibly small" Microdot web framework</title><link>https://lwn.net/Articles/1034121/</link><description>&lt;doc fingerprint="3ea911152da319f9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The "impossibly small" Microdot web framework&lt;/head&gt;
    &lt;quote&gt;We're bad at marketing&lt;p&gt;We can admit it, marketing is not our strong suit. Our strength is writing the kind of articles that developers, administrators, and free-software supporters depend on to know what is going on in the Linux world. Please subscribe today to help us keep doing that, and so we don’t have to get good at marketing.&lt;/p&gt;&lt;/quote&gt;
    &lt;p&gt;The Microdot web framework is quite small, as its name would imply; it supports both standard CPython and MicroPython, so it can be used on systems ranging from internet-of-things (IoT) devices all the way up to large, cloudy servers. It was developed by Miguel Grinberg, who gave a presentation about it at EuroPython 2025. His name may sound familiar from his well-known Flask Mega-Tutorial, which has introduced many to the Flask lightweight Python-based web framework. It should come as no surprise, then, that Microdot is inspired by its rather larger cousin, so Flask enthusiasts will find much to like in Microdot—and will come up to speed quickly should their needs turn toward smaller systems.&lt;/p&gt;
    &lt;p&gt;We have looked at various pieces of this software stack along the way: Microdot itself in January 2024, MicroPython in 2023, and Flask as part of a look at Python microframeworks in 2019.&lt;/p&gt;
    &lt;p&gt; Grinberg began his talk with an introduction. He has been living in Ireland for a few years and "&lt;quote&gt;I make stuff&lt;/quote&gt;". That includes open-source projects, blog posts (on a Flask-based blog platform that he wrote), and "&lt;quote&gt;a bunch of books&lt;/quote&gt;". He works for Elastic and is one of the maintainers of the Elasticsearch Python client, "&lt;quote&gt;so maybe you have used some of the things that I made for money&lt;/quote&gt;". &lt;/p&gt;
    &lt;head rend="h4"&gt;Why?&lt;/head&gt;
    &lt;p&gt; With a chuckle, he asked: "&lt;quote&gt;Why do we need another web framework? We have so many already.&lt;/quote&gt;" The story starts with a move that he made to Ireland from the US in 2018; he rented a house with a "smart" heating controller and was excited to use it. There were two thermostats, one for each level of the house, and he was "&lt;quote&gt;really looking forward to the winter&lt;/quote&gt;" to see the system in action. &lt;/p&gt;
    &lt;p&gt; As might be guessed, he could set target temperatures in each thermostat; they would communicate with the controller that would turn the heating on and off as needed. In addition, the system had a web server that could be used to query various parameters or to start and stop the heaters. You could even send commands via SMS text messages; "&lt;quote&gt;there's a SIM card somewhere in that box [...] very exciting stuff&lt;/quote&gt;". &lt;/p&gt;
    &lt;p&gt; When winter rolled around, it did not work that well, however; sometimes the house was too chilly or warm and he had to start and stop the heaters himself. He did some debugging and found that the thermostats were reporting temperatures that were off by ±3°C, "&lt;quote&gt;which is too much for trying to keep the house at 20°&lt;/quote&gt;". The owner of the house thought that he was too used to the US where things just work; "&lt;quote&gt;at least she thinks that in America everything is super-efficient, everything works, and she thought 'this is the way things work in Ireland'&lt;/quote&gt;". So he did not make any progress with the owner. &lt;/p&gt;
    &lt;p&gt; At that point, most people would probably just give up and live with the problem; "&lt;quote&gt;I hacked my heating controller instead&lt;/quote&gt;". He set the temperatures in both thermostats to zero, which effectively disabled their ability to affect the heaters at all, and built two small boards running MicroPython, each connected to a temperature and humidity sensor device. He wrote code that would check the temperature every five minutes and send the appropriate commands to start or stop the heaters based on what it found. &lt;/p&gt;
    &lt;p&gt; So the second half of his first winter in Ireland went great. The sensors are accurate to ±0.5°C, so "&lt;quote&gt;problem solved&lt;/quote&gt;". But, that led to a new problem for him. "&lt;quote&gt;I wanted to know things: What's the temperature right now? Is the heating running right now or not? How many hours did it run today compared to yesterday?&lt;/quote&gt;" And so on. &lt;/p&gt;
    &lt;p&gt; He added a small LCD screen to display some information, but he had to actually go to the device and look at it; what he really wanted was to be able to talk to the device over WiFi and get information from the couch while he was watching TV. "&lt;quote&gt;I wanted to host a web server [...] that will show me a little dashboard&lt;/quote&gt;". &lt;/p&gt;
    &lt;p&gt; So he searched for web frameworks for MicroPython; in the winter of 2018-2019, "&lt;quote&gt;there were none&lt;/quote&gt;". Neither Flask nor Bottle, which is a good bit smaller, would run on MicroPython; both are too large for the devices, but, in addition, the standard library for MicroPython is a subset of that of CPython, so many things that they need are missing. A "&lt;quote&gt;normal person&lt;/quote&gt;" would likely have just accepted that and moved on; "&lt;quote&gt;I created a web framework instead.&lt;/quote&gt;" &lt;/p&gt;
    &lt;head rend="h4"&gt;Demo&lt;/head&gt;
    &lt;p&gt;He brought one of his thermostat devices to Prague for the conference and did a small demonstration of it operating during the talk. The device was connected to his laptop using USB, which provided power, but also a serial connection to the board. On the laptop, he used the rshell remote MicroPython shell to talk to the board, effectively using the laptop as a terminal.&lt;/p&gt;
    &lt;p&gt;He started the MicroPython read-eval-print loop (REPL) on the board in order to simulate the normal operation of the board. When it is plugged into the wall, rather than a laptop, it will boot to the web server, so he made that happen with a soft-reboot command. The device then connected to the conference WiFi and gave him the IP address (and port) where the server was running.&lt;/p&gt;
    &lt;p&gt;He switched over to Firefox on his laptop and visited the site, which showed a dashboard that had the current temperature (24.4°) and relative humidity (56.9%) of the room. He also used curl from the laptop to contact the api endpoint of the web application, which returned JSON with the two values and the time. There is no persistent clock on the board, so the application contacts an NTP server to pick up the time when it boots; that allows it to report the last time a measurement was taken.&lt;/p&gt;
    &lt;p&gt; Grinberg said that he wanted to set the expectations at the right level by looking at the capabilities of the microcontrollers he often uses with Microdot. For example, the ESP8266 in his thermostat device has 64KB of RAM and up to 4MB of flash. The ESP8266 is the smallest and least expensive (around €5) device with WiFi that he has found; there are many even smaller devices, but they lack the networking required for running a web server. The other devices he uses are the Raspberry Pi Pico W with 2MB of flash and 256KB of RAM and the ESP32 with up to 8MB of flash and 512KB of RAM. He contrasted those with his laptop, which has 32GB of RAM, so "&lt;quote&gt;you need 500,000 ESP8266s&lt;/quote&gt;" to have the same amount of memory. &lt;/p&gt;
    &lt;head rend="h4"&gt;Features&lt;/head&gt;
    &lt;p&gt;The core framework of Microdot is in a single microdot.py file. It is fully asynchronous, using the MicroPython subset of the CPython asyncio module, so it can run on both interpreters. It uses asyncio because that is the only way to do concurrency on the microcontrollers; there is no support for processes or threads on those devices.&lt;/p&gt;
    &lt;p&gt;Microdot has Flask-style route decorators to define URLs for the application. It has Request and Response classes, as well as hooks to run before and after requests, he said. Handling query strings, form data, and JSON are all available in Microdot via normal Python dictionaries. Importantly, it can handle streaming requests and responses; because of the limited memory of these devices, it may be necessary to split up the handling of larger requests or responses.&lt;/p&gt;
    &lt;p&gt; It supports setting cookies and sending static files. Web applications can be constructed from a set of modules, using sub-applications, which are similar to Flask blueprints. It also has its own web server with TLS support. "&lt;quote&gt;I'm very proud of all the stuff I was able to fit in the core Microdot framework&lt;/quote&gt;", Grinberg said. &lt;/p&gt;
    &lt;p&gt;He hoped that attendees would have to think for a minute to come up with things that are missing from Microdot, but they definitely do exist. There are some officially maintained extensions, each in its own single .py file, to fill some of those holes. They encompass functionality that is important, but he did not want to add to the core because that would make it too large to fit on the low-end ESP8266 that he is using.&lt;/p&gt;
    &lt;p&gt; There is an extension for multipart forms, which includes file uploads; "&lt;quote&gt;this is extremely complicated to parse, it didn't make sense to add it into the core because most people don't do this&lt;/quote&gt;". There is support for WebSocket and server-sent events (SSE). Templates are supported using utemplate for both Python implementations or Jinja, which only works on CPython. There are extensions for basic and token-based authentication and for secure user logins with session data; the latter required a replacement for the CPython-only PyJWT, which Grinberg wrote and contributed to MicroPython as jwt.py. There is a small handful of other extensions that he quickly mentioned as well. &lt;/p&gt;
    &lt;p&gt; "&lt;quote&gt;I consider the documentation as part of the framework&lt;/quote&gt;"; he is "&lt;quote&gt;kind of fanatical&lt;/quote&gt;" about documenting everything. If there is something missing or not explained well, "&lt;quote&gt;it's a bug that I need to fix&lt;/quote&gt;". He writes books, so the documentation is organized similarly; it comes in at 9,267 words, which equates to around 47 minutes of reading time. There is 100% test coverage, he said, and there are around 30 examples, with more coming. &lt;/p&gt;
    &lt;p&gt; A design principle that he follows is "&lt;quote&gt;no dark magic&lt;/quote&gt;". An example of dark magic to him is the Flask application context, "&lt;quote&gt;which very few people understand&lt;/quote&gt;". In Microdot, the request object is explicitly passed to each route function. Another example is the dependency injection that is used by the FastAPI framework to add components; Microdot uses explicit decorators instead. &lt;/p&gt;
    &lt;p&gt; He used the cloc utility to count lines of code, while ignoring comments and blank lines. Using that, Django comes in at 110,000 lines, Flask plus its essential Werkzeug library is 15,500 lines, FastAPI with Starlette is 14,900 lines, Bottle is around 3,000 lines, while the Microdot core has 765 lines ("&lt;quote&gt;believe it or not&lt;/quote&gt;") and a full install with all the extensions on MicroPython comes in at just shy of 1,700 lines of code. &lt;/p&gt;
    &lt;p&gt; He ended with an example of how Microdot can be so small by comparing the URL matching in Flask with Microdot. The Flask version does lots more than Microdot, with more supported types of arguments in a URL and multiple classes in the werkzeug.routing module; it has 1,362 lines of code. For Microdot, there is a more limited set of URL arguments, though there is still the ability to define custom types, and a single URLPattern class; all of that is done in 63 lines of code. "&lt;quote&gt;I don't intend to support everything that Flask supports, in terms of routing, but I intend to support the 20% that covers 80% of the use cases.&lt;/quote&gt;" That is the overall mechanism that he has used to get to something that is so small. &lt;/p&gt;
    &lt;p&gt;An audience member asked about whether the Microdot code was minified in order to get it to fit. Grinberg said that doing so was not all that useful for MicroPython, but the code is smaller on the board because it is precompiled on another system; that results in a microdot.mpy file, which is bytecode for MicroPython. For example, on the low-end device he is using for his thermostats, Microdot would not be able to be compiled on the device itself. There are some other tricks that can also be used for reducing the RAM requirements, like putting the code into the firmware as part of the MicroPython binary.&lt;/p&gt;
    &lt;p&gt; The final question was about performance, and how many requests per second could be handled. Grinberg said that he did not have any real numbers, but that the device he demonstrated is "&lt;quote&gt;really really slow&lt;/quote&gt;". That question led to a blog post in late July where Grinberg tried to more fully answer it. &lt;/p&gt;
    &lt;p&gt;[I would like to thank the Linux Foundation, LWN's travel sponsor, for travel assistance to Prague for EuroPython.]&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Index entries for this article&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Conference&lt;/cell&gt;
        &lt;cell&gt;EuroPython/2025&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Python&lt;/cell&gt;
        &lt;cell&gt;Web&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt; Posted Aug 23, 2025 15:26 UTC (Sat) by lyda (subscriber, #7429) [Link] (1 responses) When I wrote a lot of python, frameworks like this seemed great. But there's a better way. If you define the OpenAPI definition first, you can then generate the server, you can generate all the clients, you can generate tests for the server and client, as well as fuzz tests for the server. Less common, but you can do the same with gRPC. It also allows you to more easily move from one technology to another. Posted Aug 24, 2025 3:10 UTC (Sun) by ssmith32 (subscriber, #72404) [Link] Certainly not for the problem domain covered in the article. &lt;head&gt;OpenAPI or gRPC is a better path forward&lt;/head&gt;&lt;head&gt;OpenAPI or gRPC is a better path forward&lt;/head&gt;&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45155682</guid></item><item><title>The Expression Problem and its solutions</title><link>https://eli.thegreenplace.net/2016/the-expression-problem-and-its-solutions/</link><description>&lt;doc fingerprint="7698b0504c1917f1"&gt;
  &lt;main&gt;
    &lt;p&gt;The craft of programming is almost universally concerned with different types of data and operations/algorithms that act on this data [1]. Therefore, it's hardly surprising that designing abstractions for data types and operations has been on the mind of software engineers and programming-language designers since... forever.&lt;/p&gt;
    &lt;p&gt;Yet I've only recently encountered a name for a software design problem which I ran into multiple times in my career. It's a problem so fundamental that I was quite surprised that I haven't seen it named before. Here is a quick problem statement.&lt;/p&gt;
    &lt;p&gt;Imagine that we have a set of data types and a set of operations that act on these types. Sometimes we need to add more operations and make sure they work properly on all types; sometimes we need to add more types and make sure all operations work properly on them. Sometimes, however, we need to add both - and herein lies the problem. Most of the mainstream programming languages don't provide good tools to add both new types and new operations to an existing system without having to change existing code. This is called the "expression problem". Studying the problem and its possible solutions gives great insight into the fundamental differences between object-oriented and functional programming and well as concepts like interfaces and multiple dispatch.&lt;/p&gt;
    &lt;head rend="h2"&gt;A motivating example&lt;/head&gt;
    &lt;p&gt;As is my wont, my example comes from the world of compilers and interpreters. To my defense, this is also the example used in some of the seminal historic sources on the expression problem, as the historical perspective section below details.&lt;/p&gt;
    &lt;p&gt;Imagine we're designing a simple expression evaluator. Following the standard interpreter design pattern, we have a tree structure consisting of expressions, with some operations we can do on such trees. In C++ we'd have an interface every node in the expression tree would have to implement:&lt;/p&gt;
    &lt;code&gt;class Expr {
public:
  virtual std::string ToString() const = 0;
  virtual double Eval() const = 0;
};
&lt;/code&gt;
    &lt;p&gt;This interface shows that we currently have two operations we can do on expression trees - evaluate them and query for their string representations. A typical leaf node expression:&lt;/p&gt;
    &lt;code&gt;class Constant : public Expr {
public:
  Constant(double value) : value_(value) {}

  std::string ToString() const {
    std::ostringstream ss;
    ss &amp;lt;&amp;lt; value_;
    return ss.str();
  }

  double Eval() const {
    return value_;
  }

private:
  double value_;
};
&lt;/code&gt;
    &lt;p&gt;And a typical composite expression:&lt;/p&gt;
    &lt;code&gt;class BinaryPlus : public Expr {
public:
  BinaryPlus(const Expr&amp;amp; lhs, const Expr&amp;amp; rhs) : lhs_(lhs), rhs_(rhs) {}

  std::string ToString() const {
    return lhs_.ToString() + " + " + rhs_.ToString();
  }

  double Eval() const {
    return lhs_.Eval() + rhs_.Eval();
  }

private:
  const Expr&amp;amp; lhs_;
  const Expr&amp;amp; rhs_;
};
&lt;/code&gt;
    &lt;p&gt;Until now, it's all fairly basic stuff. How extensible is this design? Let's see... if we want to add new expression types ("variable reference", "function call" etc.), that's pretty easy. We just define additional classes inheriting from Expr and implement the Expr interface (ToString and Eval).&lt;/p&gt;
    &lt;p&gt;However, what happens if we want to add new operations that can be applied to expression trees? Right now we have Eval and ToString, but we may want additional operations like "type check" or "serialize" or "compile to machine code" or whatever.&lt;/p&gt;
    &lt;p&gt;It turns out that adding new operations isn't as easy as adding new types. We'd have to change the Expr interface, and consequently change every existing expression type to support the new method(s). If we don't control the original code or it's hard to change it for other reasons, we're in trouble.&lt;/p&gt;
    &lt;p&gt;In other words, we'd have to violate the venerable open-closed principle, one of the main principles of object-oriented design, defined as:&lt;/p&gt;
    &lt;quote&gt;software entities (classes, modules, functions, etc.) should be open for extension, but closed for modification&lt;/quote&gt;
    &lt;p&gt;The problem we're hitting here is called the expression problem, and the example above shows how it applies to object-oriented programming.&lt;/p&gt;
    &lt;p&gt;Interestingly, the expression problem bites functional programming languages as well. Let's see how.&lt;/p&gt;
    &lt;head rend="h2"&gt;The expression problem in functional programming&lt;/head&gt;
    &lt;p&gt;Update 2018-02-05: a new post discusses the problem and its solutions in Haskell in more depth.&lt;/p&gt;
    &lt;p&gt;Object-oriented approaches tend to collect functionality in objects (types). Functional languages cut the cake from a different angle, usually preferring types as thin data containers, collecting most functionality in functions (operations) that act upon them. Functional languages don't escape the expression problem - it just manifests there in a different way.&lt;/p&gt;
    &lt;p&gt;To demonstrate this, let's see how the expression evaluator / stringifier looks in Haskell. Haskell is a good poster child for functional programming since its pattern matching on types makes such code especially succinct:&lt;/p&gt;
    &lt;code&gt;module Expressions where

data Expr = Constant Double
          | BinaryPlus Expr Expr

stringify :: Expr -&amp;gt; String
stringify (Constant c) = show c
stringify (BinaryPlus lhs rhs) = stringify lhs
                                ++ " + "
                                ++ stringify rhs

evaluate :: Expr -&amp;gt; Double
evaluate (Constant c) = c
evaluate (BinaryPlus lhs rhs) = evaluate lhs + evaluate rhs
&lt;/code&gt;
    &lt;p&gt;Now let's say we want to add a new operation - type checking. We simply have to add a new function typecheck and define how it behaves for all known kinds of expressions. No need to modify existing code.&lt;/p&gt;
    &lt;p&gt;On the other hand, if we want to add a new type (like "function call"), we get into trouble. We now have to modify all existing functions to handle this new type. So we hit exactly the same problem, albeit from a different angle.&lt;/p&gt;
    &lt;head rend="h2"&gt;The expression problem matrix&lt;/head&gt;
    &lt;p&gt;A visual representation of the expression problem can be helpful to appreciate how it applies to OOP and FP in different ways, and how a potential solution would look.&lt;/p&gt;
    &lt;p&gt;The following 2-D table (a "matrix") has types in its rows and operations in its columns. A matrix cell row, col is checked when the operation col is implemented for type row:&lt;/p&gt;
    &lt;p&gt;In object-oriented languages, it's easy to add new types but difficult to add new operations:&lt;/p&gt;
    &lt;p&gt;Whereas in functional languages, it's easy to add new operations but difficult to add new types:&lt;/p&gt;
    &lt;head rend="h2"&gt;A historical perspective&lt;/head&gt;
    &lt;p&gt;The expression problem isn't new, and has likely been with us since the early days; it pops its head as soon as programs reach some not-too-high level of complexity.&lt;/p&gt;
    &lt;p&gt;It's fairly certain that the name expression problem comes from an email sent by Philip Wadler to a mailing list deailing with adding generics to Java (this was back in the 1990s).&lt;/p&gt;
    &lt;p&gt;In that email, Wadler points to the paper "Synthesizing Object-Oriented and Functional Design to Promote Re-Use" by Krishnamurthi, Felleisen and Friedman as an earlier work describing the problem and proposed solutions. This is a great paper and I highly recommend reading it. Krishnamurthi et.al., in their references, point to papers from as early as 1975 describing variations of the problem in Algol.&lt;/p&gt;
    &lt;head rend="h2"&gt;Flipping the matrix with the visitor pattern&lt;/head&gt;
    &lt;p&gt;So far the article has focused on the expression problem, and I hope it's clear by now. However, the title also has the word solution in it, so let's turn to that.&lt;/p&gt;
    &lt;p&gt;It's possible to kinda solve (read on to understand why I say "kinda") the expression problem in object-oriented languages; first, we have to look at how we can flip the problem on its side using the visitor pattern. The visitor pattern is very common for this kind of problems, and for a good reason. It lets us reformulate our code in a way that makes it easier to change in some dimensions (though harder in others).&lt;/p&gt;
    &lt;p&gt;For the C++ sample shown above, rewriting it using the visitor pattern means adding a new "visitor" interface:&lt;/p&gt;
    &lt;code&gt;class ExprVisitor {
public:
  virtual void VisitConstant(const Constant&amp;amp; c) = 0;
  virtual void VisitBinaryPlus(const BinaryPlus&amp;amp; bp) = 0;
};
&lt;/code&gt;
    &lt;p&gt;And changing the Expr interface to be:&lt;/p&gt;
    &lt;code&gt;class Expr {
public:
  virtual void Accept(ExprVisitor* visitor) const = 0;
};
&lt;/code&gt;
    &lt;p&gt;Now expression types defer the actual computation to the visitor, as follows:&lt;/p&gt;
    &lt;code&gt;class Constant : public Expr {
public:
  Constant(double value) : value_(value) {}

  void Accept(ExprVisitor* visitor) const {
    visitor-&amp;gt;VisitConstant(*this);
  }

  double GetValue() const {
    return value_;
  }

private:
  double value_;
};

// ... similarly, BinaryPlus would have
//
//    void Accept(ExprVisitor* visitor) const {
//      visitor-&amp;gt;VisitBinaryPlus(*this);
//    }
//
// ... etc.
&lt;/code&gt;
    &lt;p&gt;A sample visitor for evaluation would be [2]:&lt;/p&gt;
    &lt;code&gt;class Evaluator : public ExprVisitor {
public:
  double GetValueForExpr(const Expr&amp;amp; e) {
    return value_map_[&amp;amp;e];
  }

  void VisitConstant(const Constant&amp;amp; c) {
    value_map_[&amp;amp;c] = c.GetValue();
  }

  void VisitBinaryPlus(const BinaryPlus&amp;amp; bp) {
    bp.GetLhs().Accept(this);
    bp.GetRhs().Accept(this);
    value_map_[&amp;amp;bp] = value_map_[&amp;amp;(bp.GetLhs())] + value_map_[&amp;amp;(bp.GetRhs())];
  }

private:
  std::map&amp;lt;const Expr*, double&amp;gt; value_map_;
};
&lt;/code&gt;
    &lt;p&gt;It should be obvious that for a given set of data types, adding new visitors is easy and doesn't require modifying any other code. On the other hand, adding new types is problematic since it means we have to update the ExprVisitor interface with a new abstract method, and consequently update all the visitors to implement it.&lt;/p&gt;
    &lt;p&gt;So it seems that we've just turned the expression problem on its side: we're using an OOP language, but now it's hard to add types and easy to add ops, just like in the functional approach. I find it extremely interesting that we can do this. In my eyes this highlights the power of different abstractions and paradigms, and how they enable us to rethink a problem in a completely different light.&lt;/p&gt;
    &lt;p&gt;So we haven't solved anything yet; we've just changed the nature of the problem we're facing. Worry not - this is just a stepping stone to an actual solution.&lt;/p&gt;
    &lt;head rend="h2"&gt;Extending the visitor pattern&lt;/head&gt;
    &lt;p&gt;The following is code excerpts from a C++ solution that follows the extended visitor pattern proposed by Krishnamurthi et. al. in their paper; I strongly suggest reading the paper (particularly section 3) if you want to understand this code on a deep level. A complete code sample in C++ that compiles and runs is available here.&lt;/p&gt;
    &lt;p&gt;Adding new visitors (ops) with the visitor pattern is easy. Our challenge is to add a new type without upheaving too much existing code. Let's see how it's done.&lt;/p&gt;
    &lt;p&gt;One small design change that we should make to the original visitor pattern is use virtual inheritance for Evaluator, for reasons that will soon become obvious:&lt;/p&gt;
    &lt;code&gt;class Evaluator : virtual public ExprVisitor {
  // .. the rest is the same
};
&lt;/code&gt;
    &lt;p&gt;Now we're going to add a new type - FunctionCall:&lt;/p&gt;
    &lt;code&gt;// This is the new ("extended") expression we're adding.
class FunctionCall : public Expr {
public:
  FunctionCall(const std::string&amp;amp; name, const Expr&amp;amp; argument)
      : name_(name), argument_(argument) {}

  void Accept(ExprVisitor* visitor) const {
    ExprVisitorWithFunctionCall* v =
        dynamic_cast&amp;lt;ExprVisitorWithFunctionCall*&amp;gt;(visitor);
    if (v == nullptr) {
      std::cerr &amp;lt;&amp;lt; "Fatal: visitor is not ExprVisitorWithFunctionCall\n";
      exit(1);
    }
    v-&amp;gt;VisitFunctionCall(*this);
  }

private:
  std::string name_;
  const Expr&amp;amp; argument_;
};
&lt;/code&gt;
    &lt;p&gt;Since we don't want to modify the existing visitors, we create a new one, extending Evaluator for function calls. But first, we need to extend the ExprVisitor interface to support the new type:&lt;/p&gt;
    &lt;code&gt;class ExprVisitorWithFunctionCall : virtual public ExprVisitor {
public:
  virtual void VisitFunctionCall(const FunctionCall&amp;amp; fc) = 0;
};
&lt;/code&gt;
    &lt;p&gt;Finally, we write the new evaluator, which extends Evaluator and supports the new type:&lt;/p&gt;
    &lt;code&gt;class EvaluatorWithFunctionCall : public ExprVisitorWithFunctionCall,
                                  public Evaluator {
public:
  void VisitFunctionCall(const FunctionCall&amp;amp; fc) {
    std::cout &amp;lt;&amp;lt; "Visiting FunctionCall!!\n";
  }
};
&lt;/code&gt;
    &lt;p&gt;Multiple inheritance, virtual inheritance, dynamic type checking... that's pretty hard-core C++ we have to use here, but there's no choice. Unfortunately, multiple inheritance is the only way C++ lets us express the idea that a class implements some interface while at the same time deriving functionality from another class. What we want to have here is an evaluator (EvaluatorWithFunctionCall) that inherits all functionality from Evaluator, and also implements the ExprVisitorWithFunctionCall interface. In Java, we could say something like:&lt;/p&gt;
    &lt;code&gt;class EvaluatorWithFunctionCall extends Evaluator implements ExprVisitor {
  // ...
}
&lt;/code&gt;
    &lt;p&gt;But in C++ virtual multiple inheritance is the tool we have. The virtual part of the inheritance is essential here for the compiler to figure out that the ExprVisitor base underlying both Evaluator and ExprVisitorWithFunctionCall is the same and should only appear once in EvaluatorWithFunctionCall. Without virtual, the compiler would complain that EvaluatorWithFunctionCall doesn't implement the ExprVisitor interface.&lt;/p&gt;
    &lt;p&gt;This is a solution, alright. We kinda added a new type FunctionCall and can now visit it without changing existing code (assuming the virtual inheritance was built into the design from the start to anticipate this approach). Here I am using this "kinda" word again... it's time to explain why.&lt;/p&gt;
    &lt;p&gt;This approach has multiple flaws, in my opinion:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Note the dynamic_cast in FunctionCall::Accept. It's fairly ugly that we're forced to mix in dynamic checks into this code, which should supposedly rely on static typing and the compiler. But it's just a sign of a larger problem.&lt;/item&gt;
      &lt;item&gt;If we have an instance of an Evaluator, it will no longer work on the whole extended expression tree since it has no understanding of FunctionCall. It's easy to say that all new evaluators should rather be EvaluatorWithFunctionCall, but we don't always control this. What about code that was already written? What about Evaluators created in third-party or library code which we have no control of?&lt;/item&gt;
      &lt;item&gt;The virtual inheritance is not the only provision we have to build into the design to support this pattern. Some visitors would need to create new, recursive visitors to process complex expressions. But we can't anticipate in advance which dynamic type of visitor needs to be created. Therefore, the visitor interface should also accept a "visitor factory" which extended visitors will supply. I know this sounds complicated, and I don't want to spend more time on this here - but the Krishnamurthi paper addresses this issue extensively in section 3.4&lt;/item&gt;
      &lt;item&gt;Finally, the solution is unwieldy for realistic applications. Adding one new type looks manageable; what about adding 15 new types, gradually over time? Imagine the horrible zoo of ExprVisitor extensions and dynamic checks this would lead to.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Yeah, programming is hard. I could go on and on about the limitations of classical OOP and how they surface in this example [3]. Instead, I'll just present how the expression problem can be solved in a language that supports multiple dispatch and separates the defintion of methods from the bodies of types they act upon.&lt;/p&gt;
    &lt;head rend="h2"&gt;Solving the expression problem in Clojure&lt;/head&gt;
    &lt;p&gt;There are a number of ways the expression problem as displayed in this article can be solved in Clojure using the language's built-in features. Let's start with the simplest one - multi-methods.&lt;/p&gt;
    &lt;p&gt;First we'll define the types as records:&lt;/p&gt;
    &lt;code&gt;(defrecord Constant [value])
(defrecord BinaryPlus [lhs rhs])
&lt;/code&gt;
    &lt;p&gt;Then, we'll define evaluate as a multimethod that dispatches upon the type of its argument, and add method implementations for Constant and BinaryPlus:&lt;/p&gt;
    &lt;code&gt;(defmulti evaluate class)

(defmethod evaluate Constant
  [c] (:value c))

(defmethod evaluate BinaryPlus
  [bp] (+ (evaluate (:lhs bp)) (evaluate (:rhs bp))))
&lt;/code&gt;
    &lt;p&gt;Now we can already evaluate expressions:&lt;/p&gt;
    &lt;code&gt;user=&amp;gt; (use 'expression.multimethod)
nil
user=&amp;gt; (evaluate (-&amp;gt;BinaryPlus (-&amp;gt;Constant 1.1) (-&amp;gt;Constant 2.2)))
3.3000000000000003
&lt;/code&gt;
    &lt;p&gt;Adding a new operation is easy. Let's add stringify:&lt;/p&gt;
    &lt;code&gt;(defmulti stringify class)

(defmethod stringify Constant
  [c] (str (:value c)))

(defmethod stringify BinaryPlus
  [bp]
  (clojure.string/join " + " [(stringify (:lhs bp))
                              (stringify (:rhs bp))]))
&lt;/code&gt;
    &lt;p&gt;Testing it:&lt;/p&gt;
    &lt;code&gt;user=&amp;gt; (stringify (-&amp;gt;BinaryPlus (-&amp;gt;Constant 1.1) (-&amp;gt;Constant 2.2)))
"1.1 + 2.2"
&lt;/code&gt;
    &lt;p&gt;How about adding new types? Suppose we want to add FunctionCall. First, we'll define the new type. For simplicity, the func field of FunctionCall is just a Clojure function. In real code it could be some sort of function object in the language we're interpreting:&lt;/p&gt;
    &lt;code&gt;(defrecord FunctionCall [func argument])
&lt;/code&gt;
    &lt;p&gt;And define how evaluate and stringify work for FunctionCall:&lt;/p&gt;
    &lt;code&gt;(defmethod evaluate FunctionCall
  [fc] ((:func fc) (evaluate (:argument fc))))

(defmethod stringify FunctionCall
  [fc] (str (clojure.repl/demunge (str (:func fc)))
            "("
            (stringify (:argument fc))
            ")"))
&lt;/code&gt;
    &lt;p&gt;Let's take it for a spin (the full code is here):&lt;/p&gt;
    &lt;code&gt;user=&amp;gt; (def callexpr (-&amp;gt;FunctionCall twice (-&amp;gt;BinaryPlus (-&amp;gt;Constant 1.1)
                                                         (-&amp;gt;Constant 2.2))))
#'user/callexpr
user=&amp;gt; (evaluate callexpr)
6.6000000000000005
user=&amp;gt; (stringify callexpr)
"expression.multimethod/twice@52e29c38(1.1 + 2.2)"
&lt;/code&gt;
    &lt;p&gt;It should be evident that the expression problem matrix for Clojure is:&lt;/p&gt;
    &lt;p&gt;We can add new ops without touching any existing code. We can also add new types without touching any existing code. The code we're adding is only the new code to handle the ops/types in question. The existing ops and types could come from a third-party library to which we don't have source access. We could still extend them for our new ops and types, without ever having to touch (or even see) the original source code [4].&lt;/p&gt;
    &lt;head rend="h2"&gt;Is multiple dispatch necessary to cleanly solve the expression problem?&lt;/head&gt;
    &lt;p&gt;I've written about multiple dispatch in Clojure before, and in the previous section we see another example of how to use the language's defmulti/defmethod constructs. But is it multiple dispatch at all? No! It's just single dispatch, really. Our ops (evaluate and stringify) dispatch on a single argument - the expression type) [5].&lt;/p&gt;
    &lt;p&gt;If we're not really using multiple dispatch, what is the secret sauce that lets us solve the expression problem so elegantly in Clojure? The answer is - open methods. Note a crucial difference between how methods are defined in C++/Java and in Clojure. In C++/Java, methods have to be part of a class and defined (or at least declared) in its body. You cannot add a method to a class without changing the class's source code.&lt;/p&gt;
    &lt;p&gt;In Clojure, you can. In fact, since data types and multimethods are orthogonal entities, this is by design. Methods simply live outside types - they are first class citizens, rather than properties of types. We don't add methods to a type, we add new methods that act upon the type. This doesn't require modifying the type's code in any way (or even having access to its code).&lt;/p&gt;
    &lt;p&gt;Some of the other popular programming languages take a middle way. In languages like Python, Ruby and JavaScript methods belong to types, but we can dynamically add, remove and replace methods in a class even after it was created. This technique is lovingly called monkey patching. While initially enticing, it can lead to big maintainability headaches in code unless we're very careful. Therefore, if I had to face the expression problem in Python I'd prefer to roll out some sort of multiple dispatch mechanism for my program rather than rely on monkey patching.&lt;/p&gt;
    &lt;head rend="h2"&gt;Another Clojure solution - using protocols&lt;/head&gt;
    &lt;p&gt;Clojure's multimethods are very general and powerful. So general, in fact, that their performance may not be optimal for the most common case - which is single dispatch based on the type of the sole method argument; note that this is exactly the kind of dispatch I'm using in this article. Therefore, starting with Clojure 1.2, user code gained the ability to define and use protocols - a language feature that was previously restricted only to built-in types.&lt;/p&gt;
    &lt;p&gt;Protocols leverage the host platform's (which in Clojure's case is mostly Java) ability to provide quick virtual dispatch, so using them is a very efficient way to implement runtime polymorphism. In addition, protocols retain enough of the flexibility of multimethods to elegantly solve the expression problem. Curiously, this was on the mind of Clojure's designers right from the start. The Clojure documentation page about protocols lists this as one of their capabilities:&lt;/p&gt;
    &lt;quote&gt;[...] Avoid the 'expression problem' by allowing independent extension of the set of types, protocols, and implementations of protocols on types, by different parties. [...] do so without wrappers/adapters&lt;/quote&gt;
    &lt;p&gt;Clojure protocols are an interesting topic, and while I'd like to spend some more time on them, this article is becoming too long as it is. So I'll leave a more thorough treatment for some later time and for now will just show how protocols can also be used to solve the expression problem we're discussing.&lt;/p&gt;
    &lt;p&gt;The type definitions remain the same:&lt;/p&gt;
    &lt;code&gt;(defrecord Constant [value])
(defrecord BinaryPlus [lhs rhs])
&lt;/code&gt;
    &lt;p&gt;However, instead of defining a multimethod for each operation, we now define a protocol. A protocol can be thought of as an interface in a language like Java, C++ or Go - a type implements an interface when it defines the set of methods declared by the interface. In this respect, Clojure's protocols are more like Go's interfaces than Java's, as we don't have to say a-priori which interfaces a type implements when we define it.&lt;/p&gt;
    &lt;p&gt;Let's start with the Evaluatable protocol, that consists of a single method - evaluate:&lt;/p&gt;
    &lt;code&gt;(defprotocol Evaluatable
  (evaluate [this]))
&lt;/code&gt;
    &lt;p&gt;Another protocol we'll define is Stringable:&lt;/p&gt;
    &lt;code&gt;(defprotocol Stringable
  (stringify [this]))
&lt;/code&gt;
    &lt;p&gt;Now we can make sure our types implement these protocols:&lt;/p&gt;
    &lt;code&gt;(extend-type Constant
  Evaluatable
    (evaluate [this] (:value this))
  Stringable
    (stringify [this] (str (:value this))))

(extend-type BinaryPlus
  Evaluatable
    (evaluate [this] (+ (evaluate (:lhs this)) (evaluate (:rhs this))))
  Stringable
    (stringify [this]
      (clojure.string/join " + " [(stringify (:lhs this))
                                  (stringify (:rhs this))])))
&lt;/code&gt;
    &lt;p&gt;The extend-type macro is a convenience wrapper around the more general extend - it lets us implement multiple protocols for a given type. A sibling macro named extend-protocol lets us implement the same protocol for multiple types in the same invocation [6].&lt;/p&gt;
    &lt;p&gt;It's fairly obvious that adding new data types is easy - just as we did above, we simply use extend-type for each new data type to implement our current protocols. But how do we add a new protocol and make sure all existing data types implement it? Once again, it's easy because we don't have to modify any existing code. Here's a new protocol:&lt;/p&gt;
    &lt;code&gt;(defprotocol Serializable
  (serialize [this]))
&lt;/code&gt;
    &lt;p&gt;And this is its implementation for the currently supported data types:&lt;/p&gt;
    &lt;code&gt;(extend-protocol Serializable
  Constant
    (serialize [this] [(type this) (:value this)])
  BinaryPlus
    (serialize [this] [(type this)
                       (serialize (:lhs this))
                       (serialize (:rhs this))]))
&lt;/code&gt;
    &lt;p&gt;This time, extending a single protocol for multiple data types - extend-protocol is the more convenient macro to use.&lt;/p&gt;
    &lt;head rend="h2"&gt;Small interfaces are extensibility-friendly&lt;/head&gt;
    &lt;p&gt;You may have noted that the protocols (interfaces) defined in the Clojure solution are very small - consisting of a single method. Since adding methods to an existing protocol is much more problematic (I'm not aware of a way to do this in Clojure), keeping protocols small is a good idea. This guideline comes up in other contexts as well; for example, it's good practice to keep interfaces in Go very minimal.&lt;/p&gt;
    &lt;p&gt;In our C++ solution, splitting the Expr interface could also be a good idea, but it wouldn't help us with the expression problem, since we can't modify which interfaces a class implements after we've defined it; in Clojure we can.&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;[1]&lt;/cell&gt;
        &lt;cell&gt;"Types of data" and "operations" are two terms that should be fairly obvious to modern-day programmers. Philip Wadler, in his discussion of the expression problem (see the "historical perspective" section of the article) calls them "datatypes" and "functions". A famous quote from Fred Brooks's The Mythical Man Month (1975) is "Show me your flowcharts and conceal your tables, and I shall continue to be mystified. Show me your tables, and I wonât usually need your flowcharts; theyâll be obvious."&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;[2]&lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Note the peculiar way in which data is passed between Visit* methods in a Expr* -&amp;gt; Value map kept in the visitor. This is due to our inability to make Visit* methods return different types in different visitors. For example, in Evaluator we'd want them to return double, but in Stringifier they'd probably return std::string. Unfortunately C++ won't let us easily mix templates and virtual functions, so we have to resort to either returning void* the C way or the method I'm using here.&lt;/p&gt;
          &lt;p&gt;Curiously, in their paper Krishnamurthi et.al. run into the same issue in the dialect of Java they're using, and propose some language extensions to solve it. Philip Wadler uses proposed Java generics in his approach.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;[3]&lt;/cell&gt;
        &lt;cell&gt;I can't resist, so just in brief: IMHO inheritance is only good for a very narrow spectrum of uses, but languages like C++ hail it as the main extension mechanism of types. But inheritance is deeply flawed for many other use cases, such as implementations of interfaces. Java is a bit better in this regard, but in the end the primacy of classes and their "closed-ness" make a lot of tasks - like the expression problem - very difficult to express in a clean way.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;[4]&lt;/cell&gt;
        &lt;cell&gt;In fact, there are plenty of examples in which the Clojure implementation and the standard library provide protocols that can be extended by the user for user-defined types. Extending user-written protocols and multimethods for built-in types is trivial. As an exercise, add an evaluate implementation for java.lang.Long, so that built-in integers could participate in our expression trees without requiring wrapping in a Constant.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;[5]&lt;/cell&gt;
        &lt;cell&gt;FWIW, we can formulate a multiple dispatch solution to the expression problem in Clojure. The key idea is to dispatch on two things: type and operation. Just for fun, I coded a prototype that does this which you can see here. I think the approach presented in the article - each operation being its own multimethod - is preferable, though.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;[6]&lt;/cell&gt;
        &lt;cell&gt;The sharp-eyed reader will notice a cool connection to the expression problem matrix. extend-type can add a whole new row to the matrix, while extend-protocol adds a column. extend adds just a single cell.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45155877</guid></item><item><title>Show HN: I'm a dermatologist and I vibe coded a skin cancer learning app</title><link>https://molecheck.info/</link><description>&lt;doc fingerprint="7fb8a917f1b73ba5"&gt;
  &lt;main&gt;
    &lt;p&gt;For the best experience, please scan the QR code with your phone's camera to use the app on your mobile device.&lt;/p&gt;
    &lt;p&gt;Are you worried about this skin lesion?Swipe left (concerned) / right (not concerned) or use the buttons.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45157020</guid></item><item><title>Serverless Horrors</title><link>https://serverlesshorrors.com/</link><description>&lt;doc fingerprint="499a9798958793a"&gt;
  &lt;main&gt;
    &lt;head rend="h5"&gt;Posts&lt;/head&gt;
    &lt;head rend="h4"&gt;~$1189.420/month&lt;/head&gt;
    &lt;p&gt;For no reason, Webflow charged me $1189.420 for a single month from a 69$/month plan.&lt;/p&gt;
    &lt;head rend="h4"&gt;$100,000.420&lt;/head&gt;
    &lt;p&gt;I ran a semi popular WebGL games uploading site that was hit bad by a DoS and I got a single day firebase bill for $100k...&lt;/p&gt;
    &lt;head rend="h4"&gt;$738.420&lt;/head&gt;
    &lt;p&gt;I subscribe to Vercel Pro for $20 per month. I also added a spending limit of $120, so no nasty surprises, right?&lt;/p&gt;
    &lt;head rend="h4"&gt;$70,000.69&lt;/head&gt;
    &lt;p&gt;You pay $50/month for your project, but one day you woke up to a $70,000 bill...&lt;/p&gt;
    &lt;head rend="h4"&gt;$22.639,69&lt;/head&gt;
    &lt;p&gt;I received an insanely bill of 22k USD today from simply using BigQuery on a public data set in the playground...&lt;/p&gt;
    &lt;head rend="h4"&gt;$250/month&lt;/head&gt;
    &lt;p&gt;9.000 page visits and I have to pay $250/month for it, that is $3000/year...&lt;/p&gt;
    &lt;head rend="h4"&gt;$1273.69&lt;/head&gt;
    &lt;p&gt;We asked Devin (AI) to make a change in our codebase....&lt;/p&gt;
    &lt;head rend="h4"&gt;$530.19&lt;/head&gt;
    &lt;p&gt;Never had to pay anything and suddenly im billed $530....&lt;/p&gt;
    &lt;head rend="h4"&gt;$383.69&lt;/head&gt;
    &lt;p&gt;Woke up to an almost $400 bill for my documentation site...&lt;/p&gt;
    &lt;head rend="h4"&gt;$103.26&lt;/head&gt;
    &lt;p&gt;Why $103 is a horror story? Well, imagine that you are on a free tier...&lt;/p&gt;
    &lt;head rend="h4"&gt;$96,280.69&lt;/head&gt;
    &lt;p&gt;So freaking speechless right now....&lt;/p&gt;
    &lt;head rend="h4"&gt;$120,000.420&lt;/head&gt;
    &lt;p&gt;Cloudflare took down our website after trying to force us to pay 120k$ within 24h...&lt;/p&gt;
    &lt;head rend="h4"&gt;$1,300.69&lt;/head&gt;
    &lt;p&gt;Imagine you create an empty, private AWS S3 bucket in a region of your preference...&lt;/p&gt;
    &lt;head rend="h4"&gt;$11,000.69&lt;/head&gt;
    &lt;p&gt;Sent $11k worth of emails during DoS attack, then lost my database...&lt;/p&gt;
    &lt;head rend="h4"&gt;$104,500.123&lt;/head&gt;
    &lt;p&gt;So I received an email from Netlify last weekend saying that I have a $104,500.00 bill overdue...&lt;/p&gt;
    &lt;head rend="h4"&gt;$23,000.420&lt;/head&gt;
    &lt;p&gt;What is happening?! Someone spammed EchoFox and spiked my Vercel bill to $23k and caused 56k+ accounts and trials...&lt;/p&gt;
    &lt;head rend="h4"&gt;$3,000.69&lt;/head&gt;
    &lt;p&gt;Attention Vercel users. Be careful what you test or deploy to Vercel. I decided to try out...&lt;/p&gt;
    &lt;head rend="h4"&gt;$620.123&lt;/head&gt;
    &lt;p&gt;My sitemap.txt used hundreds of GB/hours apparently...&lt;/p&gt;
    &lt;head rend="h4"&gt;$72,000.999&lt;/head&gt;
    &lt;p&gt;We Burnt $72K testing Firebase + Cloud Run and almost went Bankrupt...&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45157110</guid></item><item><title>Show HN: Semantic grep for Claude Code (local embeddings)</title><link>https://github.com/BeaconBay/ck</link><description>&lt;doc fingerprint="3b13f818c3848bc2"&gt;
  &lt;main&gt;
    &lt;p&gt;ck (seek) finds code by meaning, not just keywords. It's a drop-in replacement for &lt;code&gt;grep&lt;/code&gt; that understands what you're looking for — search for "error handling" and find try/catch blocks, error returns, and exception handling code even when those exact words aren't present.&lt;/p&gt;
    &lt;code&gt;cargo install ck-search&lt;/code&gt;
    &lt;code&gt;# Find error handling patterns (finds try/catch, Result types, etc.)
ck --sem "error handling" src/

# Traditional grep-compatible search still works  
ck -n "TODO" *.rs

# Combine both: semantic relevance + keyword filtering
ck --hybrid "connection timeout" src/&lt;/code&gt;
    &lt;p&gt;For Developers: Stop hunting through thousands of regex false positives. Find the code you actually need by describing what it does.&lt;/p&gt;
    &lt;p&gt;For AI Agents: Get structured, semantic search results in JSON format. Perfect for code analysis, documentation generation, and automated refactoring.&lt;/p&gt;
    &lt;p&gt;For Teams: Works exactly like &lt;code&gt;grep&lt;/code&gt; with the same flags and behavior, but adds semantic intelligence when you need it.&lt;/p&gt;
    &lt;code&gt;# Build from source
cargo build --release

# Index your project for semantic search
./target/debug/ck index src/

# Search by meaning
./target/debug/ck --sem "authentication logic" src/
./target/debug/ck --sem "database connection pooling" src/
./target/debug/ck --sem "retry mechanisms" src/

# Use all the grep features you know
./target/debug/ck -n -C 3 "error" src/
./target/debug/ck -r "TODO|FIXME" .&lt;/code&gt;
    &lt;p&gt;Find code by concept, not keywords. Searches understand synonyms, related terms, and conceptual similarity.&lt;/p&gt;
    &lt;code&gt;# These find related code even without exact keywords:
ck --sem "retry logic"           # finds backoff, circuit breakers
ck --sem "user authentication"   # finds login, auth, credentials  
ck --sem "data validation"       # finds sanitization, type checking

# Get complete functions/classes containing matches (NEW!)
ck --sem --full-section "error handling"  # returns entire functions
ck --full-section "async def" src/        # works with regex too&lt;/code&gt;
    &lt;p&gt;All your muscle memory works. Same flags, same behavior, same output format.&lt;/p&gt;
    &lt;code&gt;ck -i "warning" *.log              # Case-insensitive  
ck -n -A 3 -B 1 "error" src/       # Line numbers + context
ck --no-filename "TODO" src/        # Suppress filenames (grep -h equivalent)
ck -l "error" src/                  # List files with matches only (NEW!)
ck -L "TODO" src/                   # List files without matches (NEW!)
ck -r --exclude "*.test.js" "bug"  # Recursive with exclusions
ck "pattern" file1.txt file2.txt   # Multiple files&lt;/code&gt;
    &lt;p&gt;Combine keyword precision with semantic understanding using Reciprocal Rank Fusion.&lt;/p&gt;
    &lt;code&gt;ck --hybrid "async timeout" src/    # Best of both worlds
ck --hybrid --scores "cache" src/   # Show relevance scores with color highlighting
ck --hybrid --threshold 0.02 query  # Filter by minimum relevance
ck -l --hybrid "database" src/      # List files using hybrid search&lt;/code&gt;
    &lt;p&gt;Perfect JSON output for LLMs, scripts, and automation.&lt;/p&gt;
    &lt;code&gt;ck --json --sem "error handling" src/ | jq '.file'
ck --json --topk 5 "TODO" . | jq -r '.preview'
ck --json --full-section --sem "database" . | jq -r '.preview'  # Complete functions&lt;/code&gt;
    &lt;p&gt;Automatically excludes cache directories, build artifacts, and system files.&lt;/p&gt;
    &lt;code&gt;# These are excluded by default:
# .git, node_modules, target/, .fastembed_cache, __pycache__

# Override defaults:
ck --no-default-excludes "pattern" .     # Search everything
ck --exclude "dist" --exclude "logs" .   # Add custom exclusions&lt;/code&gt;
    &lt;code&gt;# Create semantic index (one-time setup)
ck index /path/to/project

# Now search instantly by meaning
ck --sem "database queries" .
ck --sem "error handling" .
ck --sem "authentication" .&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;--regex&lt;/code&gt;(default): Classic grep behavior, no indexing required&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--sem&lt;/code&gt;: Pure semantic search using embeddings (requires index)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--hybrid&lt;/code&gt;: Combines regex + semantic with intelligent ranking&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;ck --sem --scores "machine learning" docs/
# [0.847] ./ai_guide.txt: Machine learning introduction...
# [0.732] ./statistics.txt: Statistical learning methods...
# [0.681] ./algorithms.txt: Classification algorithms...&lt;/code&gt;
    &lt;code&gt;# Glob patterns work
ck --sem "authentication" *.py *.js *.rs

# Multiple files
ck --sem "error handling" src/auth.rs src/db.rs

# Quoted patterns prevent shell expansion  
ck --sem "auth" "src/**/*.ts"&lt;/code&gt;
    &lt;code&gt;# Only high-confidence semantic matches
ck --sem --threshold 0.7 "query"

# Low-confidence hybrid matches (good for exploration)
ck --hybrid --threshold 0.01 "concept"

# Get complete code sections instead of snippets (NEW!)
ck --sem --full-section "database queries"
ck --full-section "class.*Error" src/     # Complete classes&lt;/code&gt;
    &lt;code&gt;# Limit results for focused analysis
ck --sem --topk 5 "authentication patterns"

# Great for AI agent consumption
ck --json --topk 10 "error handling" | process_results.py&lt;/code&gt;
    &lt;code&gt;# Check index status
ck status .

# Clean up and rebuild
ck clean .
ck index .

# Add single file to index
ck add new_file.rs&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Language&lt;/cell&gt;
        &lt;cell role="head"&gt;Indexing&lt;/cell&gt;
        &lt;cell role="head"&gt;Tree-sitter Parsing&lt;/cell&gt;
        &lt;cell role="head"&gt;Semantic Chunking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Python&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅ Functions, classes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;JavaScript&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅ Functions, classes, methods&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;TypeScript&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅ Functions, classes, methods&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Haskell&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅ Functions, types, instances&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Text Formats: Markdown, JSON, YAML, TOML, XML, HTML, CSS, shell scripts, SQL, and plain text.&lt;/p&gt;
    &lt;p&gt;Smart Exclusions: Automatically skips &lt;code&gt;.git&lt;/code&gt;, &lt;code&gt;node_modules&lt;/code&gt;, &lt;code&gt;target/&lt;/code&gt;, &lt;code&gt;build/&lt;/code&gt;, &lt;code&gt;dist/&lt;/code&gt;, &lt;code&gt;__pycache__/&lt;/code&gt;, &lt;code&gt;.fastembed_cache&lt;/code&gt;, &lt;code&gt;.venv&lt;/code&gt;, &lt;code&gt;venv&lt;/code&gt;, and other common build/cache/virtual environment directories.&lt;/p&gt;
    &lt;code&gt;git clone https://github.com/BeaconBay/ck
cd ck
cargo install --path ck-cli&lt;/code&gt;
    &lt;code&gt;# Coming soon:
brew install ck-search
apt install ck-search&lt;/code&gt;
    &lt;p&gt;ck uses a modular Rust workspace:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;ck-cli&lt;/code&gt;- Command-line interface and argument parsing&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;ck-core&lt;/code&gt;- Shared types, configuration, and utilities&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;ck-search&lt;/code&gt;- Search engine implementations (regex, BM25, semantic)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;ck-index&lt;/code&gt;- File indexing, hashing, and sidecar management&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;ck-embed&lt;/code&gt;- Text embedding providers (FastEmbed, API backends)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;ck-ann&lt;/code&gt;- Approximate nearest neighbor search indices&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;ck-chunk&lt;/code&gt;- Text segmentation and language-aware parsing&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;ck-models&lt;/code&gt;- Model registry and configuration management&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Indexes are stored in &lt;code&gt;.ck/&lt;/code&gt; directories alongside your code:&lt;/p&gt;
    &lt;code&gt;project/
├── src/
├── docs/  
└── .ck/           # Semantic index (can be safely deleted)
    ├── embeddings.json
    ├── ann_index.bin
    └── tantivy_index/
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;.ck/&lt;/code&gt; directory is a cache — safe to delete and rebuild anytime.&lt;/p&gt;
    &lt;code&gt;# Find authentication/authorization code
ck --sem "user permissions" src/
ck --sem "access control" src/
ck --sem "login validation" src/

# Find error handling strategies  
ck --sem "exception handling" src/
ck --sem "error recovery" src/
ck --sem "fallback mechanisms" src/

# Find performance-related code
ck --sem "caching strategies" src/
ck --sem "database optimization" src/  
ck --sem "memory management" src/&lt;/code&gt;
    &lt;code&gt;# Git hooks
git diff --name-only | xargs ck --sem "TODO"

# CI/CD pipeline
ck --json --sem "security vulnerability" . | security_scanner.py

# Code review prep
ck --hybrid --scores "performance" src/ &amp;gt; review_notes.txt

# Documentation generation
ck --json --sem "public API" src/ | generate_docs.py&lt;/code&gt;
    &lt;code&gt;# Find related test files
ck --sem "unit tests for authentication" tests/
ck -l --sem "test" tests/           # List test files by semantic content

# Identify refactoring candidates  
ck --sem "duplicate logic" src/
ck --sem "code complexity" src/
ck -L "test" src/                   # Find source files without tests

# Security audit
ck --hybrid "password|credential|secret" src/
ck --sem "input validation" src/
ck -l --hybrid --scores "security" src/  # Files with security-related code&lt;/code&gt;
    &lt;code&gt;# View current exclusion patterns
ck --help | grep -A 20 exclude

# These directories are excluded by default:
# .git, .svn, .hg                    # Version control
# node_modules, target, build        # Build artifacts  
# .cache, __pycache__, .fastembed_cache  # Caches
# .vscode, .idea                     # IDE files&lt;/code&gt;
    &lt;code&gt;# .ck/config.toml
[search]
default_mode = "hybrid"
default_threshold = 0.05

[indexing]  
exclude_patterns = ["*.log", "temp/"]
chunk_size = 512
overlap = 64

[models]
embedding_model = "BAAI/bge-small-en-v1.5"&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Indexing: ~1M LOC in under 2 minutes (with smart exclusions and optimized embedding computation)&lt;/item&gt;
      &lt;item&gt;Search: Sub-500ms queries on typical codebases&lt;/item&gt;
      &lt;item&gt;Index size: ~2x source code size with compression&lt;/item&gt;
      &lt;item&gt;Memory: Efficient streaming for large repositories with span-based content extraction&lt;/item&gt;
      &lt;item&gt;File filtering: Automatic exclusion of virtual environments and build artifacts&lt;/item&gt;
      &lt;item&gt;Output: Clean stdout/stderr separation for reliable piping and scripting&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Run the comprehensive test suite:&lt;/p&gt;
    &lt;code&gt;# Full test suite (40+ tests)
./test_ck.sh

# Quick smoke test (14 core tests)
./test_ck_simple.sh&lt;/code&gt;
    &lt;p&gt;Tests cover grep compatibility, semantic search, index management, file filtering, and more.&lt;/p&gt;
    &lt;p&gt;ck is actively developed and welcomes contributions:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Issues: Report bugs, request features&lt;/item&gt;
      &lt;item&gt;Code: Submit PRs for bug fixes, new features&lt;/item&gt;
      &lt;item&gt;Documentation: Improve examples, guides, tutorials&lt;/item&gt;
      &lt;item&gt;Testing: Help test on different codebases and languages&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;git clone https://github.com/your-org/ck
cd ck
cargo build
cargo test
./target/debug/ck index test_files/
./target/debug/ck --sem "test query" test_files/&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;✅ grep-compatible CLI with semantic search and file listing flags (&lt;code&gt;-l&lt;/code&gt;,&lt;code&gt;-L&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;✅ FastEmbed integration with BGE models&lt;/item&gt;
      &lt;item&gt;✅ File exclusion patterns and glob support&lt;/item&gt;
      &lt;item&gt;✅ Threshold filtering and relevance scoring with visual highlighting&lt;/item&gt;
      &lt;item&gt;✅ Tree-sitter parsing and intelligent chunking (Python, TypeScript, JavaScript, Haskell)&lt;/item&gt;
      &lt;item&gt;✅ Complete code section extraction (&lt;code&gt;--full-section&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;✅ Enhanced indexing strategy with v3 semantic search optimization&lt;/item&gt;
      &lt;item&gt;✅ Clean stdout/stderr separation for reliable scripting&lt;/item&gt;
      &lt;item&gt;✅ Incremental index updates with hash-based change detection&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;🚧 Configuration file support&lt;/item&gt;
      &lt;item&gt;🚧 Package manager distributions&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;🔮 Multiple embedding model support&lt;/item&gt;
      &lt;item&gt;🔮 Advanced ranking algorithms&lt;/item&gt;
      &lt;item&gt;🔮 Plugin architecture for custom chunkers&lt;/item&gt;
      &lt;item&gt;🔮 Distributed/remote index support&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;🔮 IDE integrations (VS Code, IntelliJ, etc.)&lt;/item&gt;
      &lt;item&gt;🔮 Git integration (semantic diffs, blame)&lt;/item&gt;
      &lt;item&gt;🔮 Web interface for team usage&lt;/item&gt;
      &lt;item&gt;🔮 Multi-language semantic understanding&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Q: How is this different from grep/ripgrep/silver-searcher?&lt;lb/&gt; A: ck includes all the features of traditional search tools, but adds semantic understanding. Search for "error handling" and find relevant code even when those exact words aren't used.&lt;/p&gt;
    &lt;p&gt;Q: Does it work offline?&lt;lb/&gt; A: Yes, completely offline. The embedding model runs locally with no network calls.&lt;/p&gt;
    &lt;p&gt;Q: How big are the indexes?&lt;lb/&gt; A: Typically 1-3x the size of your source code, depending on content. The &lt;code&gt;.ck/&lt;/code&gt; directory can be safely deleted to reclaim space.&lt;/p&gt;
    &lt;p&gt;Q: Is it fast enough for large codebases?&lt;lb/&gt; A: Yes. Indexing is a one-time cost, and searches are sub-second even on large projects. Regex searches require no indexing and are as fast as grep.&lt;/p&gt;
    &lt;p&gt;Q: Can I use it in scripts/automation?&lt;lb/&gt; A: Absolutely. The &lt;code&gt;--json&lt;/code&gt; flag provides structured output perfect for automated processing. Use &lt;code&gt;--full-section&lt;/code&gt; to get complete functions for AI analysis.&lt;/p&gt;
    &lt;p&gt;Q: What about privacy/security?&lt;lb/&gt; A: Everything runs locally. No code or queries are sent to external services. The embedding model is downloaded once and cached locally.&lt;/p&gt;
    &lt;p&gt;Licensed under either of:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Apache License, Version 2.0 (LICENSE-APACHE)&lt;/item&gt;
      &lt;item&gt;MIT License (LICENSE-MIT)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;at your option.&lt;/p&gt;
    &lt;p&gt;Built with:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Rust - Systems programming language&lt;/item&gt;
      &lt;item&gt;FastEmbed - Fast text embeddings&lt;/item&gt;
      &lt;item&gt;Tantivy - Full-text search engine&lt;/item&gt;
      &lt;item&gt;clap - Command line argument parsing&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Inspired by the need for better code search tools in the age of AI-assisted development.&lt;/p&gt;
    &lt;p&gt;Start finding code by what it does, not what it says.&lt;/p&gt;
    &lt;code&gt;cargo build --release
./target/release/ck index .
./target/release/ck --sem "the code you're looking for"&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45157223</guid></item><item><title>Algebraic Effects in Practice with Flix</title><link>https://www.relax.software/blog/flix-effects-intro/</link><description>&lt;doc fingerprint="f002d9f161f7126e"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Algebraic Effects in Practice with Flix&lt;/head&gt;
    &lt;p&gt;Algebraic effects are not just a research concept anymore. You can use them in real software, today. Here’s why you’d want to do that, in order of importance:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Effects make your code testable&lt;/p&gt;
        &lt;p&gt;One of the central goals of enterprise software development. Dependency injection, mocking, architecture patterns like clean, hexagonal, DDD are all meant to tackle this. Effects solve this elegantly by separating the “what” from the “how”.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Effects give immediate visibility into what your own and 3rd-party code is doing&lt;/p&gt;
        &lt;p&gt;Supply chain attacks are real. And they will get worse with more AI slop entering our codebases. Tools like Go’s Capslock fix this by following the whole chain of calls to stdlib functions. Effects provide this by design, as all effects are tracked by the type and effect system.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Effects enable user-defined control flow abstractions&lt;/p&gt;
        &lt;p&gt;Solving the “what color is your function” problem1. You can also leverage effects to implement Async/await, coroutines, backtracking search and other control flow patterns as user libraries without hard-coding these features into the language.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Algebraic effects come from the pure functional world, serving a purpose similar to monads — keeping track of and having control over side effects. Like monads, they enable us to write our core logic with pure functions and push side effects like IO outwards, closer to application boundaries.&lt;/p&gt;
    &lt;p&gt;Unlike monads, effects are easy to grasp for a regular developer and give immediate benefits when starting out. For me personally they’re a more natural abstraction for managing side effects — after all, effects are in the name.&lt;/p&gt;
    &lt;p&gt;Starting out as an academic concept, algebraic effects were introduced to the world by research languages like Eff, Koka, Effekt, Frank, Links, and more recently Ante.&lt;/p&gt;
    &lt;p&gt;People have also applied effects in practice, so far usually via a monad-based approach, by making libraries in established languages like Scala Kyo / Cats Effect / ZIO; Typescript Effect and Effector, C# language-ext, C libhandler and libmprompt, C++ cpp-effects, various Haskell libraries, etc.&lt;/p&gt;
    &lt;p&gt;In addition to forcing you into a monadic way of thinking, libraries implementing effects are limited by their host languages.&lt;/p&gt;
    &lt;p&gt;In this article, I will walk you through applying algebraic effects on a real world example using Flix, a new programming language that is built with effects from the ground up, and supports functional, logic and imperative paradigms.&lt;/p&gt;
    &lt;head rend="h2"&gt;Table of Contents&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Type and Effect System: A Motivating Example&lt;/item&gt;
      &lt;item&gt;Effect Handlers: Building Intuition&lt;/item&gt;
      &lt;item&gt;Real-World App: AI movie recommendations&lt;/item&gt;
      &lt;item&gt;Where to Go From Here&lt;/item&gt;
      &lt;item&gt;Extra: Why Algebraic Effects are Algebraic and how they relate to monads&lt;/item&gt;
      &lt;item&gt;Footnotes&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Currently only few languages support effects out of the box. The only one that I know of besides Flix is Unison. OCaml has a language extension, but there is no support yet in the type system. Haskell has added support for delimited continuations, but effects are still only available via libraries.&lt;/p&gt;
    &lt;p&gt;In addition to having a “type and effect system” that improves function signatures and makes sure all effects are handled, Flix supports traits, local mutability via regions, working with immutable or mutable data, and Go/Rust-like structured concurrency. It also has a first-class Datalog integration. But I will only focus on effects here. Let’s start.&lt;/p&gt;
    &lt;head rend="h2"&gt;Type and Effect System: A Motivating Example 🔗&lt;/head&gt;
    &lt;p&gt;Imagine a function called &lt;code&gt;calculateSalary&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;def calculateSalary(base_salary, bonus_percent):
&lt;/code&gt;
    &lt;p&gt;Based on the function name and the signature, one can assume it’s just a pure function that does some calculations. In a statically typed language you are also guaranteed that the function arguments and outputs will be of a certain type.&lt;/p&gt;
    &lt;p&gt;But even if the types are correct, nothing stops our little &lt;code&gt;calculateSalary()&lt;/code&gt; from, say, sending an offensive email to your grandma2:&lt;/p&gt;
    &lt;code&gt;def calculateSalary(base_salary, bonus_percent):
    server.sendmail("grandma@family.com", "Your cookies are terrible!")
    return base_salary * (1 + bonus_percent/100)
&lt;/code&gt;
    &lt;p&gt;If, on the other hand, you extend your type system with effects, you will see immediately in the signature that this function may do something fishy:&lt;/p&gt;
    &lt;code&gt;def calculateSalary(salary: Float64, percent: Float64): 
    Float64 \ {Email} = {
//            ^^^^^^^ Notice the Email effect!
&lt;/code&gt;
    &lt;p&gt;Of course, in real life the issue it’s not usually about the grandma. Instead, this function could throw an exception — still quite dangerous. If you forget to handle the exception, your app will crash. Or another very realistic scenario is that &lt;code&gt;calculateSalary()&lt;/code&gt; calls a database to get some employee details for calculations, and you forgot to provide a database connection string. That can also result in an exception or a panic.&lt;/p&gt;
    &lt;head rend="h2"&gt;Effect Handlers: Building Intuition 🔗&lt;/head&gt;
    &lt;p&gt;The job of the type and effect system is not just to improve our function signatures. It’s also making sure all the effects are handled somewhere. This is where effect handlers come in.&lt;/p&gt;
    &lt;p&gt;Usually when people talk about algebraic effects what they’re actually talking about is effect handlers. If you know exceptions, effect handlers are super easy to understand. Here’s a Jenga analogy:&lt;/p&gt;
    &lt;p&gt;Imagine the call stack is a Jenga tower. New blocks are carefully added each time you call a function.&lt;/p&gt;
    &lt;p&gt;When an exception is thrown, your whole nice Jenga tower gets destroyed, all the way up to the catch() block. The catch block can safely handle the error, but the stack is unwinded, meaning you lose all of the state you had in your program before throwing the exception. You have to build your tower again, from scratch.&lt;/p&gt;
    &lt;p&gt;When using effect handlers you can actually go back to your original computation after the handler is done handling the effect. The handler can also return some values back to your program, and it can even resume multiple times with different return values. You also still have the option of not resuming at all and aborting the program — that would be the effect equivalent of exceptions.&lt;/p&gt;
    &lt;p&gt;Back to the Jenga analogy: if your tower is about to fall down, with effects you can freeze it mid-collapse. You then call someone for help (handler), and they decide whether to let the tower fall, magically restore it to the previous statlte. Or even hand you different blocks to try the same move (call the continuation) again, possibly multiple times with different inputs. Your Jenga tower ends up looking more like a fork or a tree, with multiple different copies of your blocks branching out at some point from the base.&lt;/p&gt;
    &lt;p&gt;To make this more concrete, let’s start by reproducing exceptions with effects. Here’s how a try/catch looks like in Python:&lt;/p&gt;
    &lt;code&gt;def divide(x, y):
    try:
        return x / y
    except ZeroDivisionError:
        print("Division by zero!")
        return None
&lt;/code&gt;
    &lt;p&gt;Here’s the equivalent code in Flix. We first define an Exception effect and a &lt;code&gt;divide()&lt;/code&gt; function:&lt;/p&gt;
    &lt;code&gt;eff Exception {
    def throw(msg: String): Void
}

def divide(x: Int32, y: Int32): Int32 \ Exception = 
    if (y == 0) {
        Exception.throw("Division by zero!")
    } else {
        x / y
    }
&lt;/code&gt;
    &lt;p&gt;And then provide a handler for this effect somewhere, preferably close to &lt;code&gt;main()&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;def main(): Unit \ IO = 
    run {
        println(divide(10, 0))
    } with handler Exception {
        def throw(msg, _resume) = println("Error: ${msg}")
    }
&lt;/code&gt;
    &lt;p&gt;What this does is registers an effect called Exception with a method &lt;code&gt;throw()&lt;/code&gt;. We then perform this effect in our function when there’s an error, similar to throwing an exception in the Python version. Control is transferred to the effect handler, which then decides how to handle the exception, similar to a &lt;code&gt;catch()&lt;/code&gt; block in Python.&lt;/p&gt;
    &lt;p&gt;Notice we never call &lt;code&gt;resume()&lt;/code&gt; from the handler. This results in the program being aborted, just like with exceptions. Graphically, this can be represented as follows:&lt;/p&gt;
    &lt;quote&gt;block-beta columns 2 A["Statement 1"] space:1 B["Statement 2"] space:1 C["Statement 3"] space:1 D["Perform Effect"] space:1 space:1 E["Handle Effect"] space:1 F["Process &amp;amp; Exit"] space:1 space:1 D --&amp;gt; E style D fill:#ffcccc,color:#000 style E fill:#ccffcc,color:#000 style F fill:#ccffcc,color:#000&lt;/quote&gt;
    &lt;p&gt;So far so good, but this is not much different from Python. To really take full advantage of effect handlers, we can use &lt;code&gt;resume()&lt;/code&gt; to return to the original computation and proceed from the line after the effect was performed:&lt;/p&gt;
    &lt;code&gt;eff ResumableException {
    def askForInput(): Int32
}

def divide(x: Int32, y: Int32): Int32 \ ResumableException = 
    if (y == 0) {
        let newY = ResumableException.askForInput();
        x / newY
    } else {
        x / y
    }

def main(): Unit \ IO = 
    run {
        println(divide(10, 0))
    } with handler ResumableException {
        def askForInput(_, resume) = {
            println("Enter a new divisor:");
            resume(5) // Or get from user input
        }
    }
&lt;/code&gt;
    &lt;quote&gt;block-beta columns 2 A["Statement 1"] space:1 B["Statement 2"] space:1 C["Statement 3"] space:1 D["Perform Effect"] space:1 space:1 E["Handle Effect"] space:1 F["Resume"] space:1 space:1 G["Statement 4"] space:1 H["Statement 5"] space:1 I["Complete"] space:1 D --&amp;gt; E F --&amp;gt; G style D fill:#ffcccc,color:#000 style E fill:#ccffcc,color:#000 style F fill:#ffffcc,color:#000&lt;/quote&gt;
    &lt;p&gt;I called the effect &lt;code&gt;ResumableException&lt;/code&gt; here, but it’s not really an exception anymore, because the program continues normally.&lt;/p&gt;
    &lt;p&gt;At this point we can use this power bestowed on us by effects and handlers to roll our own Async/await:&lt;/p&gt;
    &lt;code&gt;eff Async {
    def await(url: String): String
}

def fetchData(): String \ Async = 
    Async.await("https://api.example.com/data")

def processData(): String \ Async = {
    let data = fetchData();
    "processed: ${data}"
}

def main(): Unit \ IO = 
    run {
        let result = processData();
        println(result)
    } with handler Async {
        def await(url, resume) = {
            // Simulate async HTTP request
            let result = "data from ${url}";
            resume(result)
        }
    }
&lt;/code&gt;
    &lt;p&gt;See how easy that was? This approach also avoids function coloring, since we didn’t need to use special keywords anywhere. Here’s a graphic version:&lt;/p&gt;
    &lt;quote&gt;block-beta columns 2 A["Statement 1"] space:1 B["Statement 2"] space:1 C["await operation"] space:1 space:1 H1["Start async work"] space:1 H2["⏳ Long pause..."] space:1 H3["⏳ Still waiting..."] space:1 H4["✅ Async complete"] space:1 F["Resume with result"] space:1 space:1 D["Statement 3"] space:1 E["Complete"] space:1 C --&amp;gt; H1 F --&amp;gt; D style C fill:#ffcccc,color:#000 style H1 fill:#ccffcc,color:#000 style H2 fill:#fff3cd,color:#000 style H3 fill:#fff3cd,color:#000 style H4 fill:#d1ecf1,color:#000 style F fill:#ffffcc,color:#000 style D fill:#e7f3ff,color:#000 style E fill:#d4edda,color:#000&lt;/quote&gt;
    &lt;p&gt;That’s cool, but we can do more. Effect handlers allow you to resume multiple times:&lt;/p&gt;
    &lt;code&gt;eff Choose {
    def choose(): Int32
}

def explore(): String \ Choose = {
    let x = Choose.choose();
    let y = Choose.choose();
    "${x}, ${y}"
}

def main(): Unit \ IO = 
    run {
        println(explore())
    } with handler Choose {
        def choose(_, resume) = {
            resume(1);
            resume(2);
            resume(3)
        }
&lt;/code&gt;
    &lt;quote&gt;block-beta columns 4 A["Statement 1"] space:1 space:1 space:1 B["Statement 2"] space:1 space:1 space:1 C["Statement 3"] space:1 space:1 space:1 D["Perform Effect"] space:1 space:1 space:1 space:1 space:1 E["Handle Effect"] space:1 space:1 F1["Resume 1"] F2["Resume 2"] F3["Resume 3"] space:1 G1["Statement 4a"] G2["Statement 4b"] G3["Statement 4c"] space:1 H1["Statement 5a"] H2["Statement 5b"] H3["Statement 5c"] space:1 R1["Resume to Main"] R2["Resume to Main"] R3["Resume to Main"] J["Statement 6"] space:1 space:1 space:1 K["Complete"] space:1 space:1 space:1 D --&amp;gt; E F1 --&amp;gt; G1 F2 --&amp;gt; G2 F3 --&amp;gt; G3 H1 --&amp;gt; R1 H2 --&amp;gt; R2 H3 --&amp;gt; R3 R1 --&amp;gt; J R2 --&amp;gt; J R3 --&amp;gt; J style D fill:#ffcccc,color:#000 style E fill:#ccffcc,color:#000 style F1 fill:#ffffcc,color:#000 style F2 fill:#ffffcc,color:#000 style F3 fill:#ffffcc,color:#000 style G1 fill:#e6f3ff,color:#000 style G2 fill:#ffe6f3,color:#000 style G3 fill:#f3ffe6,color:#000 style H1 fill:#e6f3ff,color:#000 style H2 fill:#ffe6f3,color:#000 style H3 fill:#f3ffe6,color:#000 style R1 fill:#d4edda,color:#000 style R2 fill:#d4edda,color:#000 style R3 fill:#d4edda,color:#000 style J fill:#cce5ff,color:#000 style K fill:#b3d9ff,color:#000&lt;/quote&gt;
    &lt;p&gt;With this, you can implement things like coroutines:&lt;/p&gt;
    &lt;quote&gt;block-beta columns 3 A1["Coroutine 1: Start"] space:1 A2["Coroutine 2: Start"] B1["Statement 1"] space:1 B2["Statement 1"] C1["yield to Co2"] H1["Scheduler"] space:1 space:1 space:1 C2["Statement 2"] space:1 space:1 D2["yield to Co1"] space:1 H2["Scheduler"] space:1 D1["Statement 2"] space:1 space:1 E1["yield to Co2"] H3["Scheduler"] space:1 space:1 space:1 E2["Statement 3"] space:1 space:1 F2["Complete"] F1["Complete"] space:1 space:1 C1 --&amp;gt; H1 H1 --&amp;gt; C2 D2 --&amp;gt; H2 H2 --&amp;gt; D1 E1 --&amp;gt; H3 H3 --&amp;gt; E2 style C1 fill:#ffcccc,color:#000 style D2 fill:#ffcccc,color:#000 style E1 fill:#ffcccc,color:#000 style H1 fill:#ccffcc,color:#000 style H2 fill:#ccffcc,color:#000 style H3 fill:#ccffcc,color:#000 style A1 fill:#e6f3ff,color:#000 style B1 fill:#e6f3ff,color:#000 style D1 fill:#e6f3ff,color:#000 style F1 fill:#e6f3ff,color:#000 style A2 fill:#ffe6f3,color:#000 style B2 fill:#ffe6f3,color:#000 style C2 fill:#ffe6f3,color:#000 style E2 fill:#ffe6f3,color:#000 style F2 fill:#ffe6f3,color:#000&lt;/quote&gt;
    &lt;p&gt;Generators:&lt;/p&gt;
    &lt;quote&gt;block-beta columns 2 A["Start generator"] space:1 B["Statement 1"] space:1 C["yield value 1"] H1["Return value"] space:1 H2["⏸️ Paused"] D["next() called"] H3["Resume generator"] E["Statement 2"] space:1 F["yield value 2"] H4["Return value"] space:1 H5["⏸️ Paused"] G["next() called"] H6["Resume generator"] H["Statement 3"] space:1 I["return (done)"] H7["Signal complete"] C --&amp;gt; H1 H3 --&amp;gt; D F --&amp;gt; H4 H6 --&amp;gt; G I --&amp;gt; H7 style C fill:#ffcccc,color:#000 style F fill:#ffcccc,color:#000 style I fill:#ffcccc,color:#000 style H1 fill:#ccffcc,color:#000 style H3 fill:#ffffcc,color:#000 style H4 fill:#ccffcc,color:#000 style H6 fill:#ffffcc,color:#000 style H7 fill:#ccffcc,color:#000 style H2 fill:#fff3cd,color:#000 style H5 fill:#fff3cd,color:#000 style D fill:#e7f3ff,color:#000 style G fill:#e7f3ff,color:#000&lt;/quote&gt;
    &lt;p&gt;And backtracking search:&lt;/p&gt;
    &lt;quote&gt;block-beta columns 4 A["Start search"] space:1 space:1 space:1 B["choose option"] space:1 space:1 space:1 space:1 H1["Try option 1"] space:1 space:1 space:1 space:1 C1["Explore path 1"] space:1 space:1 space:1 D1["❌ Dead end"] space:1 space:1 H2["Backtrack"] space:1 space:1 space:1 H3["Try option 2"] space:1 space:1 space:1 space:1 space:1 C2["Explore path 2"] space:1 space:1 space:1 D2["✅ Success!"] E["Resume with solution"] space:1 space:1 space:1 F["Complete"] space:1 space:1 space:1 B --&amp;gt; H1 H1 --&amp;gt; C1 D1 --&amp;gt; H2 H2 --&amp;gt; H3 H3 --&amp;gt; C2 D2 --&amp;gt; E style B fill:#ffcccc,color:#000 style H1 fill:#ccffcc,color:#000 style H2 fill:#f8d7da,color:#000 style H3 fill:#ccffcc,color:#000 style C1 fill:#fff3cd,color:#000 style D1 fill:#f8d7da,color:#000 style C2 fill:#d1ecf1,color:#000 style D2 fill:#d4edda,color:#000 style E fill:#ffffcc,color:#000 style F fill:#d4edda,color:#000&lt;/quote&gt;
    &lt;p&gt;Hopefully this gives you a taste of how effect handlers work. This is just a sketch though — you can read more on this and see examples in the Flix docs.&lt;/p&gt;
    &lt;head rend="h4"&gt;Question&lt;/head&gt;
    &lt;p&gt;What's your primary programming language?&lt;/p&gt;
    &lt;p&gt;Defining our own control flow abstractions is great, but most of the time regular async/await and/or coroutines are enough for the job.&lt;/p&gt;
    &lt;p&gt;What is extremely useful for daily programming is that effects let you separate the declaration of the effect (the operation, or the effect “constructor”) from it’s implementation, defined by the effect handler.&lt;/p&gt;
    &lt;p&gt;Add some effect definitions:&lt;/p&gt;
    &lt;code&gt;eff Database {
    def getUser(id: Int32): Option[User],
    def saveUser(user: User): Unit
}
&lt;/code&gt;
    &lt;p&gt;Then use these definitions to perform effects in your code:&lt;/p&gt;
    &lt;code&gt;def updateUserEmail(userId: Int32, newEmail: String): Result[String, User] \ {Database} = {
    match Database.getUser(userId) {
        case Some(user) =&amp;gt; {
            let updatedUser = {user | email = newEmail};
            Database.saveUser(updatedUser);
            Ok(updatedUser)
        }
        case None =&amp;gt; {
            Err("User not found")
        }
    }
}
&lt;/code&gt;
    &lt;p&gt;This replaces the need for dependency injection, since you can provide different handlers for these database operations in production vs testing:&lt;/p&gt;
    &lt;code&gt;def main(): Unit \\ IO = { // production handler, uses a real database
    run {
        updateUserEmail(123, "new@example.com")
    } with handler Database {
        def getUser(id, resume) = {
		        // real db query
            resume(user)
        }
        def saveUser(user, resume) = {
		        // real db query
            resume()
        }
    }
}

def testUpdateUserEmail(): Unit = { // test handler, just stubs
    let testUser = {id = 123, email = "old@example.com"};
    run {
        let result = updateUserEmail(123, "new@example.com");
        assert(result == Ok({testUser | email = "new@example.com"}))
    } with handler Database {
        def getUser(id, resume) = resume(Some(testUser))
        def saveUser(user, resume) = {
            assert(user.email == "new@example.com");
            resume()
        }
    
}
&lt;/code&gt;
    &lt;p&gt;In my opinion, the biggest advantage that effect handlers give is that they abstract away the patterns associated with DDD, Clean Architecture, Hexagonal architecture, etc. commonly found in enterprise code.&lt;/p&gt;
    &lt;p&gt;All these architectures give you some sort of way to isolate your core logic, which should be pure, from infrastructure and app logic, with deals with external dependencies. But you have to commit to an architecture and the whole team has to be disciplined enough to stick to for this to work.&lt;/p&gt;
    &lt;p&gt;Using effects encourages separating the definition of effect operations from implementation by default, meaning you don’t really need these architecture patterns anymore.&lt;/p&gt;
    &lt;p&gt;This is great, since relying on team discipline exclusively rarely works. It also saves a bunch of time otherwise spent on bike shedding.&lt;/p&gt;
    &lt;p&gt;Effect handlers also allow you to easily install stubs, which you can use to create quick test cases without boilerplate, just by swapping handlers:&lt;/p&gt;
    &lt;code&gt;def testErrorConditions(): Unit = {
    run {
        let result = updateUserEmail(123, "new@example.com");
        assert(result == Err("User not found"))
    } with handler Database {
        def getUser(_, resume) = resume(None) // Stub: always return None
        def saveUser(_, resume) = resume()             // Won't be called
    }
}

def testSlowDatabase(): Unit = {
    run {
        let result = updateUserEmail(123, "new@example.com");
        assert(result.isOk())
    } with handler Database {
        def getUser(id, resume) = {
            Thread.sleep(100);  // Simulate slow query
            resume(Some({id = id, email = "old@example.com"}))
        }
        def saveUser(user, resume) = {
            Thread.sleep(50);   // Simulate slow save
            resume()
        }
    }
}
&lt;/code&gt;
    &lt;p&gt;You can even make a handler that records all interactions instead of executing them. There are many possibilities here.&lt;/p&gt;
    &lt;head rend="h2"&gt;Real-World App: AI movie recommendations 🔗&lt;/head&gt;
    &lt;p&gt;To bring this all together, let’s make a real application using effects.&lt;/p&gt;
    &lt;p&gt;Our app will fetch some movie data from TheMovieDB, and then use an LLM to recommend some movies based on user preferences provided from the console.&lt;/p&gt;
    &lt;p&gt;Flix interoperates with the JVM, meaning we can call code from Java, Kotlin, Scala, etc.&lt;/p&gt;
    &lt;p&gt;First, let’s define the two custom effects we will need: MovieAPI and LLM:&lt;/p&gt;
    &lt;code&gt;eff MovieAPI {
    def getPopularMovies(): String
}

eff LLM {
    def recommend(movies: String, preferences: String): String
}
&lt;/code&gt;
    &lt;p&gt;We can then perform the effects in main like so, providing some basic handlers that use the Flix’s stdlib HTTP client:&lt;/p&gt;
    &lt;code&gt;def getRecommendation(preferences: String): String \ {MovieAPI, LLM} = {
    let movies = MovieAPI.getPopularMovies();
    LLM.recommend(movies, preferences)
}

def main(): Unit \ {Net, IO} = 
    run {
        let suggestion = getRecommendation("action movies");
        println(suggestion)
    } with handler MovieAPI {
        def getPopularMovies(_, resume) = {
            let response = HttpWithResult.get("https://api.themoviedb.org/3/movie/popular", Map.empty());
            match response {
                case Result.Ok(resp) =&amp;gt; resume(Http.Response.body(resp))
                case Result.Err(_) =&amp;gt; resume("[]")
            }
        }
    } with handler LLM {
        def recommend(movies, prefs, resume) = {
            let prompt = "Movies: ${movies}. User likes: ${prefs}. Recommend one movie.";
            let response = HttpWithResult.post("https://api.openai.com/v1/completions", Map.empty(), prompt);
            match response {
                case Result.Ok(resp) =&amp;gt; resume(Http.Response.body(resp))
                case Result.Err(_) =&amp;gt; resume("Try watching a classic!")
            }
        }
    } with HttpWithResult.runWithIO
&lt;/code&gt;
    &lt;p&gt;Notice that both effects are quite generic. So we can easily swap either the movie API or the LLM provider without touching anything in the core logic:&lt;/p&gt;
    &lt;code&gt;// Switch to different movie provider
with handler MovieAPI {
    def getPopularMovies(_, resume) = {
        let response = HttpWithResult.get("https://api.imdb.com/popular", Map.empty());
        // ... handle IMDB response format
    }
}

// Switch to different LLM provider  
with handler LLM {
    def recommend(movies, prefs, resume) = {
        let response = HttpWithResult.post("https://api.anthropic.com/v1/messages", Map.empty(), prompt);
        // ... handle Claude response format
    }
}
&lt;/code&gt;
    &lt;p&gt;To get the user input we will need to include the standard Console effect:&lt;/p&gt;
    &lt;code&gt;def main(): Unit \ {Net, IO} = 
    run {
        Console.println("What movie genres do you enjoy?");
        let preferences = Console.readln();
        let suggestion = getRecommendation(preferences);
        Console.println("Recommendation: ${suggestion}")
    } with handler MovieAPI { /* ... */ }
      with handler LLM { /* ... */ }
      with Console.runWithIO
      with HttpWithResult.runWithIO
&lt;/code&gt;
    &lt;p&gt;We can also add some basic logs using the standard Logger effect:&lt;/p&gt;
    &lt;code&gt;def getRecommendation(preferences: String): String \ {MovieAPI, LLM, Logger} = {
    Logger.info("Fetching popular movies...");
    let movies = MovieAPI.getPopularMovies();
    Logger.info("Getting LLM recommendation...");
    LLM.recommend(movies, preferences)
}

def main(): Unit \ {Net, IO} = 
    run {
        /* ... console interaction ... */
    } with handler MovieAPI { /* ... */ }
      with handler LLM { /* ... */ }
      with Console.runWithIO
      with Logger.runWithIO
      with HttpWithResult.runWithIO
&lt;/code&gt;
    &lt;p&gt;That’s it! Let’s run the app and test it manually like so:&lt;/p&gt;
    &lt;code&gt; flix run Main.flix
What movie genres do you enjoy?
&amp;gt; sci-fi horror
[INFO] Fetching popular movies...
[INFO] Getting LLM recommendation...
Recommendation: Based on your interest in sci-fi horror, I recommend "Alien" - a perfect blend of both genres!
&lt;/code&gt;
    &lt;p&gt;We can also easily write tests for the core logic by providing test handlers for our movie and LLM effects:&lt;/p&gt;
    &lt;code&gt;def testRecommendation(): String = 
    run {
        getRecommendation("comedy")
    } with handler MovieAPI {
        def getPopularMovies(_, resume) = {
            resume("""[{"title": "The Grand Budapest Hotel", "genre": "comedy"}]""")
        }
    } with handler LLM {
        def recommend(movies, prefs, resume) = {
            resume("I recommend The Grand Budapest Hotel - perfect for comedy lovers!")
        }
    } with handler Logger {
        def log(_, _, resume) = resume()  // Silent in tests
    }

def runTests(): Unit \ IO = {
    let result = testRecommendation();
    println("Test result: ${result}")
}
&lt;/code&gt;
    &lt;head rend="h2"&gt;Where to Go From Here 🔗&lt;/head&gt;
    &lt;p&gt;Read the Flix docs&lt;/p&gt;
    &lt;p&gt;Especially on cool features like effect polymorphism, effect exclusion etc. Check out code examples in the repo&lt;/p&gt;
    &lt;p&gt;Join the community and contribute with libraries&lt;/p&gt;
    &lt;p&gt;The Flix compiler and stdlib are quite feature-rich at this point, and having JVM interop means you have all the essentials you need to write practical code. But there are still very few pure Flix libraries. So it’s very valuable to contribute some. The ideas I can think of are, for example, rebuilding standard things like Web frameworks in an effect oriented way,. Or taking advantage of the unique feature set in Flix to build something entirely new.&lt;/p&gt;
    &lt;p&gt;Explore effect-oriented programming&lt;/p&gt;
    &lt;p&gt;While I personally like Flix and can recommend it to others, there are other ways you can use effects for real-world software. If you’re in Typescript or Scala, try out Effect or ZIO/Kyo/Cats. If you’re looking for other languages that support effects natively, and you’re not afraid of Haskell-like syntax, check out Unison. They have a bunch of other concepts I find cool, like a better distributed computing model and the code being content-addressed.&lt;/p&gt;
    &lt;p&gt;Thanks for reading! I hope this article was useful. Hit me up if you have questions or feedback, and check out my website, where I’m exploring sustainable tech and coding practices: relax.software&lt;/p&gt;
    &lt;head rend="h4"&gt;Question&lt;/head&gt;
    &lt;p&gt;What should I write about next?&lt;/p&gt;
    &lt;head rend="h2"&gt;Extra: Why Algebraic Effects are Algebraic and how they relate to monads 🔗&lt;/head&gt;
    &lt;p&gt;Okay, practical people have left the room. Following sections are nerds-only.&lt;/p&gt;
    &lt;p&gt;For some reason, all the content I’ve been reading on algebraic effects uses this term a lot, but no one explains why specifically they’re called “algebraic”. So I did some digging.&lt;/p&gt;
    &lt;p&gt;Turns out, algebraic effects are “algebraic” because they can be described with laws and equations, like in algebra — the kind we learn at school. Which is I guess why they’re easier to grasp than monads — unlike algebra, you usually don’t study category theory in high school.&lt;/p&gt;
    &lt;p&gt;But the algebraic part only applies to the effect “constructors”, i.e the operations themselves like &lt;code&gt;get()&lt;/code&gt; or &lt;code&gt;put()&lt;/code&gt; for the state effect.&lt;/p&gt;
    &lt;p&gt;Effect handlers, on the other hand, are not algebraic at all, which can be a bit confusing. But it makes sense if you think about it — the purpose of handlers is to act as “deconstructors”, interpreting our algebraic effect operations by means of things that cannot be described by algebraic equations alone, such as continuations .&lt;/p&gt;
    &lt;p&gt;In fact, effect handlers are often (but not always) implemented via delimited continuations. There are also other, static/lexically scoped and maybe more performant approaches being explored, such as this one&lt;/p&gt;
    &lt;p&gt;“Real” algebraic effects don’t require monads. Monads and algebraic effects are two different concepts tackling similar problems. One is expressible in terms of the other, but algebraic effects are arguably more flexible.&lt;/p&gt;
    &lt;p&gt;You could actually implement algebraic effects using a continuation monad. If we don’t care about types, effects are perfectly expressible with monads and vice versa&lt;/p&gt;
    &lt;p&gt;The problems appear when we introduce types into the picture. In a properly typed world, you can’t actually reproduce the same expressiveness you get with effects using monads. You’ll end up breaking the type system or reducing expressiveness at some point.&lt;/p&gt;
    &lt;p&gt;Effects are, in this sense, more “powerful” than monads with their natural type system: you can express infinitely many computations with them. E.g if you use a &lt;code&gt;tick()&lt;/code&gt; effect and you do a bunch of sequential &lt;code&gt;tick()&lt;/code&gt; s, the result will be a distinct computation each time. With monads and their natural type system the set of computations you could express is finite.&lt;/p&gt;
    &lt;p&gt;Additionally, with monads you commit to a specific interpretation of an effect in advance, while effects completely decouple effect definition from it’s implementation.&lt;/p&gt;
    &lt;p&gt;Finally, effects are easier to compose than monads. With monad transformers you quickly hit the wall having to define a bunch of different combinations that each have distinct semantics. Effects compose naturally.&lt;/p&gt;
    &lt;p&gt;So while effect libraries in languages like Typescript and Scala are able to express effects using monads3, and the behavior could be identical at runtime, this cannot replace having an actual type and effect system, with effects being properly typed.&lt;/p&gt;
    &lt;head rend="h4"&gt;Question&lt;/head&gt;
    &lt;p&gt;How do you usually learn about new things?&lt;/p&gt;
    &lt;head rend="h2"&gt;Footnotes 🔗&lt;/head&gt;
    &lt;head rend="h2"&gt;Footnotes&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;“What color is your function” is a problem explored in this article. In languages which have Async baked in via special keywords (e.g JavaScript async/await) it becomes a pain to refactor and to combine synchronous and asynchronous code. If you make one function deep in the call stack&lt;/p&gt;&lt;code&gt;async&lt;/code&gt;, all the callers will have to be made Async as well, or&lt;code&gt;await()&lt;/code&gt;the results. With effects you don’t have this issue as there are no keywords and no special behavior. Async is simply done with effect handlers. ↩&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I like the grandma example more than the “launch missiles” popular in the Haskell world. Took it from this article by Kevin Mahoney. It’s somehow more offensive ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;See some examples in this article. This also shows how Haskell’s new delimited continuation support can be used to implement algebraic effects and handlers ↩&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45157466</guid></item><item><title>Belling the Cat</title><link>https://en.wikipedia.org/wiki/Belling_the_Cat</link><description>&lt;doc fingerprint="be03d5d395e3bec6"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Belling the Cat&lt;/head&gt;&lt;p&gt;Belling the Cat is a fable also known under the titles The Bell and the Cat and The Mice in Council. In the story, a group of mice agree to attach a bell to a cat's neck to warn of its approach in the future, but they fail to find a volunteer to perform the job. The term has become an idiom describing a group of persons, each agreeing to perform an impossibly difficult task under the misapprehension that someone else will be chosen to run the risks and endure the hardship of actual accomplishment.[1]&lt;/p&gt;&lt;p&gt;Although often attributed to Aesop, it was not recorded before the Middle Ages and has been confused with the quite different fable of Classical origin titled The Cat and the Mice. In the classificatory system established for the fables by Ben Edwin Perry, it is numbered 613, which is reserved for Mediaeval attributions outside the Aesopic canon.[2]&lt;/p&gt;&lt;head rend="h2"&gt;Synopsis and idiomatic use&lt;/head&gt;[edit]&lt;p&gt;The fable concerns a group of mice who debate plans to nullify the threat of a marauding cat. One of them proposes placing a bell around its neck, so that they are warned of its approach. The plan is applauded by the others, until one mouse asks who will volunteer to place the bell on the cat. All of them make excuses. The story is used to teach the wisdom of evaluating a plan on not only how desirable the outcome would be but also how it can be executed. It provides a moral lesson about the fundamental difference between ideas and their feasibility, and how this affects the value of a given plan.[3]&lt;/p&gt;&lt;p&gt;The fable gives rise to the idiom to bell the cat, which means to attempt, or agree to perform, an impossibly difficult task.[4] Historically 'Bell the Cat' is frequently claimed to have been a nickname given to fifteenth-century Scottish nobleman Archibald Douglas, 5th Earl of Angus in recognition of his part in the arrest and execution of James III's alleged favourite, Thomas (often misnamed as Robert) Cochrane. In fact the earliest evidence for this use is from Hume of Godscroft's history of the Douglases published in 1644,[5] and therefore is more reflective of perception of the idiom in the seventeenth century than the fifteenth.[6] In the 21st century the idiom was adopted by the investigative journalism group Bellingcat.[7]&lt;/p&gt;&lt;p&gt;The first English collection to attribute the fable to Aesop was John Ogilby's of 1687; in this there is a woodcut (by Francis Barlow), followed by a 10-line verse synopsis by Aphra Behn with the punning conclusion:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Good Councell's easily given, but the effect&lt;/p&gt;&lt;lb/&gt;Oft renders it uneasy to transact.[8]&lt;/quote&gt;&lt;head rend="h2"&gt;Early versions and later interpretations&lt;/head&gt;[edit]&lt;p&gt;One of the earliest versions of the story appears as a parable critical of the clergy in Odo of Cheriton's Parabolae.[9] Written around 1200, it was afterwards translated into Welsh, French and Spanish. Sometime later, the story is found in the work now referred to as Ysopet-Avionnet, which is largely made up of Latin poems by the 12th century Walter of England, followed by a French version dating from as much as two centuries later. It also includes four poems not found in Walter's Esopus; among them is the tale of "The Council of the Mice" (De muribus consilium facientibus contra catum). The author concludes with the scornful comment that laws are of no effect without the means of adequately enforcing them and that such parliamentary assemblies as he describes are like the proverbial mountain in labour that gives birth to a mouse.[10]&lt;/p&gt;&lt;p&gt;The fable also appeared as a cautionary tale in Nicholas Bozon's Anglo-Norman Contes Moralisés (1320), referring to the difficulty of curbing the outrages of superior lords.[11] It was in this context too that the story of a parliament of rats and mice was retold in William Langland's allegorical poem Piers Plowman.[12] The episode is said to refer to the Parliament of 1376 which attempted unsuccessfully to remedy popular dissatisfaction over the exactions made by nobles acting in the royal name.[13] Langland's French contemporary, the satirical Eustache Deschamps, also includes the story among his other moral ballades based on fables as "Les souris et les chats".[14] It has been suggested that in this case too there is a political subtext. The poem was written as a response to the aborted invasion of England in 1386 and contrasts French dithering in the face of English aggression.[15] The refrain of Deschamps' ballade, Qui pendra la sonnette au chat (who will bell the cat) was to become proverbial in France if, indeed, it does not record one already existing.&lt;/p&gt;&lt;p&gt;In the following century, the Italian author Laurentius Abstemius made of the fable a Latin cautionary tale titled De muribus tintinnabulum feli appendere volentibus (The mice who wanted to bell the cat)[16] in 1499. A more popular version in Latin verse was written by Gabriele Faerno and printed posthumously in his Fabulae centum ex antiquis auctoribus delectae (100 delightful fables from ancient authors, Rome 1564), a work that was to be many times reprinted and translated up to start of the 19th century. Titled simply "The Council of the Mice", it comes to rest on the drily stated moral that 'a risky plan can have no good result'. The story was evidently known in Flanders too, since 'belling the cat' was included among the forty Netherlandish Proverbs in the composite painting of Pieter Bruegel the Elder (1559). In this case a man in armour is performing the task in the lower left foreground.[17] A century later, La Fontaine's Fables made the tale even better known under the title Conseil tenu par les rats (II.2).[18]&lt;/p&gt;&lt;p&gt;In mediaeval times the fable was applied to political situations and British commentaries on it were sharply critical of the limited democratic processes of the day and their ability to resolve social conflict when class interests were at stake. This applies equally to the plot against the king's favourite in 15th century Scotland and the direct means that Archibald Douglas chose to resolve the issue. While none of the authors who used the fable actually incited revolution, the 1376 Parliament that Langland satirised was followed by Wat Tyler's revolt five years later, while Archibald Douglas went on to lead a rebellion against King James. During the Renaissance the fangs of the fable were being drawn by European authors, who restricted their criticism to pusillanimous conduct in the face of rashly proposed solutions. A later exception was the Russian fabulist Ivan Krylov, whose adaptation of the story satirises croneyism. In his account only those with perfect tails are to be allowed into the assembly; nevertheless, a tailless rat is admitted because of a family connection with one of the lawmakers.[19]&lt;/p&gt;&lt;p&gt;There still remains the perception of a fundamental opposition between consensus and individualism. This is addressed in the lyrics of "Bell the Cat",[20] a performance put out on DVD by the Japanese rock band LM.C in 2007.[21] This is the monologue of a house cat that wants to walk alone since "Society is by nature evil". It therefore refuses to conform and is impatient of restriction: "your hands hold on to everything – bell the cat". While the lyric is sung in Japanese, the final phrase is in English. Another modernised adaptation based on this fable, that updates the moral, has been published by Patricia McKissack in her Who Will Bell the Cat? (illustrated by Christopher Cyr).[22][23]&lt;/p&gt;&lt;p&gt;There is a Tibetan proverb that is very similar, "Putting a bell on the cat's neck after the mother of mice was consulted"[24]&lt;/p&gt;&lt;head rend="h2"&gt;Illustrations&lt;/head&gt;[edit]&lt;p&gt;Several French artists depicted the fable during the 19th century, generally choosing one of two approaches. Gustave Doré and the genre painter Aurélie Léontine Malbet (fl. 1868–1906)[25] pictured the rats realistically acting out their debate. The illustrator Grandville,[26] along with the contemporaries Philibert Léon Couturier (1823–1901)[27] and Auguste Delierre (1829–1890),[28] caricature the backward practice and pomposity of provincial legislatures, making much the same point as did the Mediaeval authors who first recorded the tale. At the end of the century a publishing curiosity reverts to the first approach. This was in the woodblock print by Kawanabe Kyōsui that appeared in the collection of La Fontaine's fables that was commissioned and printed in Tokyo in 1894 and then exported to France.[29] In the upper left-hand corner a cat is seen through a warehouse window as it approaches across the roofs while inside the rats swarm up the straw-wrapped bales of goods. At its summit the chief rat holds the bell aloft. An earlier Japanese woodblock formed part of Kawanabe Kyōsai's Isoho Monogotari series (1870–80). This shows an assembly of mice in Japanese dress with the proposer in the foreground, brandishing the belled collar.[30]&lt;/p&gt;&lt;head rend="h2"&gt;Musical settings&lt;/head&gt;[edit]&lt;p&gt;In the 18th century the fable was one among many set by Louis-Nicolas Clérambault in the fables section of Nouvelles poésies spirituelles et morales sur les plus beaux airs (1730–37).[31] In the following century the text of La Fontaine's fable was set for male voices by Louis Lacombe[32] and by the Catalan composer Isaac Albéniz for medium voice and piano in 1889.[33] In 1950 it was set for four male voices by Florent Schmitt.[34] But while La Fontaine's humorously named cat Rodilardus, and antiquated words like discomfiture (déconfiture), may fit an art song, there have also been faithful interpretations in the field of light music. A popular composer of the day, Prosper Massé, published such a setting in 1846.[35] More recently there has been Pierre Perret's interpretation as part of his 20 Fables inspirées de Jean de la Fontaine (1995),[36] and a jazz arrangement on Daniel Roca's 10 Fables de La Fontaine (2005).[37]&lt;/p&gt;&lt;head rend="h2"&gt;See also&lt;/head&gt;[edit]&lt;list rend="ul"&gt;&lt;item&gt;Collective action problem&lt;/item&gt;&lt;item&gt;Who Will Bell the Cat?, a children's picture book based on the fable&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;References&lt;/head&gt;[edit]&lt;list rend="ol"&gt;&lt;item&gt;^ Strouf, Judie L. H. (2005). The literature teacher's book of lists. Jossey-Bass. p. 13. ISBN 0787975508.&lt;/item&gt;&lt;item&gt;^ Ben Edwin Perry (1965). Babrius and Phaedrus. Loeb Classical Library. Cambridge, MA: Harvard University Press. pp. 545, no. 613. ISBN 0-674-99480-9.&lt;/item&gt;&lt;item&gt;^ "Belling The Cat". Fables of Aesop. 2016-07-05. Retrieved 2021-03-04.&lt;/item&gt;&lt;item&gt;^ "To Bell the Cat" thefreedictionary.com. Retrieved 9 November 2007.&lt;/item&gt;&lt;item&gt;^ David Reid, David Hume of Godscroft's History of the House of Angus, vol. 1 (STS: Edinburgh, 2005), p. 26.&lt;/item&gt;&lt;item&gt;^ Macdougall, Norman (1982). James III: A Political Study. Edinburgh: John Donald. pp. 287–288. ISBN 0859760782.&lt;/item&gt;&lt;item&gt;^ "Bellingcat: Digital Sleuths on the Hunt for Truth"&lt;/item&gt;&lt;item&gt;^ "21. De cato et muribus (1687), illustrated by Francis Barlow". Mythfolklore.net. Retrieved January 26, 2011.&lt;/item&gt;&lt;item&gt;^ Laura (15 May 2009). "Christianizing Aesop: The Fables of Odo of Cheriton". Journey to the Sea. Retrieved 26 January 2011.&lt;/item&gt;&lt;item&gt;^ Ysopet-Avionnet, the Latin and French texts, University of Illinois 1919; fable LXII, pp. 190–192; this is archived online&lt;/item&gt;&lt;item&gt;^ Les contes moralisés de Nicole BozonParis, 1889, pp. 144–145; archived here&lt;/item&gt;&lt;item&gt;^ William's Vision of Piers Plowman by William Langland, edited by Ben Byram-Wigfield (2006), Prologue, lines 146–181; online text here Archived 2011-08-07 at the Wayback Machine&lt;/item&gt;&lt;item&gt;^ "The Parliament of the Rats and Mice". Medieval Forum. SFSU. Archived from the original on 10 March 2022. Retrieved 26 January 2011.&lt;/item&gt;&lt;item&gt;^ Poésies morales et historiques d'Eustache Deschamps, Paris 1832, pp. 188–189&lt;/item&gt;&lt;item&gt;^ Robert Landru, Eustache Deschamps, Fédération des sociétés d'histoire et d'archéologie de l'Aisne, vol. XV 1969, p. 126&lt;/item&gt;&lt;item&gt;^ Fable 195&lt;/item&gt;&lt;item&gt;^ View on Wikimedia Commons&lt;/item&gt;&lt;item&gt;^ "Elizur Wright's translation". Oaks.nvg.org. Retrieved 26 January 2011.&lt;/item&gt;&lt;item&gt;^ Kriloff's Fables, translated by C. Fillingham Coxwell, London 1920, pp. 38–39; archived online&lt;/item&gt;&lt;item&gt;^ "Lyrics | LM.C – Bell The Cat (English)". SongMeanings. 25 April 2010. Retrieved 26 January 2011.&lt;/item&gt;&lt;item&gt;^ "Bell the CAT/LM.C". YouTube. 18 November 2007. Archived from the original on 2021-12-12. Retrieved 26 January 2011.&lt;/item&gt;&lt;item&gt;^ Who will bell the cat?. OCLC 1037155724.&lt;/item&gt;&lt;item&gt;^ "Who Will Bell the Cat?". Publishers Weekly. PWxyz LLC. February 19, 2018. Retrieved April 6, 2022.&lt;/item&gt;&lt;item&gt;^ p. 135, Tsewang, Pema. 2012. Like a Yeti Catching Marmots. Boston: Wisdom Publications.&lt;/item&gt;&lt;item&gt;^ Exhibited at the 1888 Salon; photo online&lt;/item&gt;&lt;item&gt;^ "See online". Archived from the original on July 20, 2011. Retrieved 17 August 2012.&lt;/item&gt;&lt;item&gt;^ "In the Musée Denon de Chalon-sur-Saône". Philibert-leon-couturier.com. Retrieved 17 August 2012.&lt;/item&gt;&lt;item&gt;^ "In the Musée La Fontaine at Château Thierry". Retrieved 17 August 2012.&lt;/item&gt;&lt;item&gt;^ George Baxley. "baxleystamps.com". baxleystamps.com. Retrieved 17 August 2012.&lt;/item&gt;&lt;item&gt;^ View online Archived 2012-03-25 at the Wayback Machine&lt;/item&gt;&lt;item&gt;^ The score is printed in: John Metz, The Fables of La Fontaine: A Critical Edition of the Eighteenth-Century, Pendragon Press 1986, p. 45&lt;/item&gt;&lt;item&gt;^ Op. 85, 1879, Score at Gallica&lt;/item&gt;&lt;item&gt;^ Liedernet&lt;/item&gt;&lt;item&gt;^ Op. 123, Liedernet&lt;/item&gt;&lt;item&gt;^ Bibliographie de la France, 14 March 1846, 127&lt;/item&gt;&lt;item&gt;^ "Pierre Perret chante 20 fables inspirées de Jean de La Fontaine Perret, Pierre, 1934–..." bibliotheques.avignon.fr.&lt;/item&gt;&lt;item&gt;^ Track available on Jamendo&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;External links&lt;/head&gt;[edit]&lt;list rend="ul"&gt;&lt;item&gt;Media related to Belling the Cat at Wikimedia Commons&lt;/item&gt;&lt;item&gt;The dictionary definition of belling the cat at Wiktionary&lt;/item&gt;&lt;item&gt;19th–20th century book illustrations online&lt;/item&gt;&lt;item&gt;Collection of primary fable sources online&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45157906</guid></item><item><title>Delayed Security Patches for AOSP (Android Open Source Project)</title><link>https://twitter.com/grapheneos/status/1964561043906048183</link><description>&lt;doc fingerprint="d635f48b34542867"&gt;
  &lt;main&gt;
    &lt;p&gt;We’ve detected that JavaScript is disabled in this browser. Please enable JavaScript or switch to a supported browser to continue using x.com. You can see a list of supported browsers in our Help Center.&lt;/p&gt;
    &lt;p&gt;Help Center&lt;/p&gt;
    &lt;p&gt;Terms of Service Privacy Policy Cookie Policy Imprint Ads info © 2025 X Corp.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45158523</guid></item><item><title>The MacBook has a sensor that knows the exact angle of the screen hinge</title><link>https://twitter.com/samhenrigold/status/1964428927159382261</link><description>&lt;doc fingerprint="d635f48b34542867"&gt;
  &lt;main&gt;
    &lt;p&gt;We’ve detected that JavaScript is disabled in this browser. Please enable JavaScript or switch to a supported browser to continue using x.com. You can see a list of supported browsers in our Help Center.&lt;/p&gt;
    &lt;p&gt;Help Center&lt;/p&gt;
    &lt;p&gt;Terms of Service Privacy Policy Cookie Policy Imprint Ads info © 2025 X Corp.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45158968</guid></item><item><title>Campfire: Web-Based Chat Application</title><link>https://github.com/basecamp/once-campfire</link><description>&lt;doc fingerprint="e85798574f27f5cd"&gt;
  &lt;main&gt;
    &lt;p&gt;Campfire is web-based chat application. It supports many of the features you'd expect, including:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Multiple rooms, with access controls&lt;/item&gt;
      &lt;item&gt;Direct messages&lt;/item&gt;
      &lt;item&gt;File attachments with previews&lt;/item&gt;
      &lt;item&gt;Search&lt;/item&gt;
      &lt;item&gt;Notifications (via Web Push)&lt;/item&gt;
      &lt;item&gt;@mentions&lt;/item&gt;
      &lt;item&gt;API, with support for bot integrations&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Campfire is single-tenant: any rooms designated "public" will be accessible by all users in the system. To support entirely distinct groups of customers, you would deploy multiple instances of the application.&lt;/p&gt;
    &lt;code&gt;bin/setup
bin/rails server
&lt;/code&gt;
    &lt;p&gt;Campfire's Docker image contains everything needed for a fully-functional, single-machine deployment. This includes the web app, background jobs, caching, file serving, and SSL.&lt;/p&gt;
    &lt;p&gt;To persist storage of the database and file attachments, map a volume to &lt;code&gt;/rails/storage&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;To configure additional features, you can set the following environment variables:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;SSL_DOMAIN&lt;/code&gt;- enable automatic SSL via Let's Encrypt for the given domain name&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;DISABLE_SSL&lt;/code&gt;- alternatively, set&lt;code&gt;DISABLE_SSL&lt;/code&gt;to serve over plain HTTP&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;VAPID_PUBLIC_KEY&lt;/code&gt;/&lt;code&gt;VAPID_PRIVATE_KEY&lt;/code&gt;- set these to a valid keypair to allow sending Web Push notifications. You can generate a new keypair by running&lt;code&gt;/script/admin/create-vapid-key&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;SENTRY_DSN&lt;/code&gt;- to enable error reporting to sentry in production, supply your DSN here&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For example:&lt;/p&gt;
    &lt;code&gt;docker build -t campfire .

docker run \
  --publish 80:80 --publish 443:443 \
  --restart unless-stopped \
  --volume campfire:/rails/storage \
  --env SECRET_KEY_BASE=$YOUR_SECRET_KEY_BASE \
  --env VAPID_PUBLIC_KEY=$YOUR_PUBLIC_KEY \
  --env VAPID_PRIVATE_KEY=$YOUR_PRIVATE_KEY \
  --env SSL_DOMAIN=chat.example.com \
  campfire
&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45159742</guid></item><item><title>US Visa Applications Must Be Submitted from Country of Residence or Nationality</title><link>https://travel.state.gov/content/travel/en/News/visas-news/adjudicating-nonimmigrant-visa-applicants-in-their-country-of-residence-sep-6-2025.html</link><description>&lt;doc fingerprint="6fc7f8478d77afa5"&gt;
  &lt;main&gt;
    &lt;p&gt; Travel.State.Gov &amp;gt; U.S. Visas News &amp;gt; Adjudicating Nonimmigrant Visa Applicants in Their Country of Residence - September 6, 2025&lt;/p&gt;
    &lt;p&gt;Effective immediately, the Department of State has updated its instructions for all nonimmigrant visa applicants scheduling visa interview appointments:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;p&gt;NATIONAL OF&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;DESIGNATED LOCATIONS(S)&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;p&gt;Afghanistan&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Islamabad&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;p&gt;Belarus&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Vilnius, Warsaw&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;p&gt;Chad&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Yaoundé&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;p&gt;Cuba&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Georgetown&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;p&gt;Haiti&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Nassau&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;p&gt;Iran&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Dubai&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;p&gt;Libya&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Tunis&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;p&gt;Niger&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Ouagadougou&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;p&gt;Russia&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Astana, Warsaw&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;p&gt;Somalia&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Nairobi&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;p&gt;South Sudan&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Nairobi&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;p&gt;Sudan&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Cairo&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;p&gt;Syria&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Amman&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;p&gt;Ukraine&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Krakow, Warsaw&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;p&gt;Venezuela&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Bogota&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;p&gt;Yemen&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Riyadh&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Zimbabwe&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Johannesburg&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Applicants should note the following:&lt;/p&gt;
    &lt;p&gt;Applicants should check embassy and consulate websites for more detailed information about visa application requirements and procedures, and to learn more about the embassy or consulate’s operating status and services.&lt;/p&gt;
    &lt;p&gt;This supersedes previous guidance on visa application requirements, including designated visa processing locations.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45159885</guid></item><item><title>Keeping Secrets Out of Logs</title><link>https://allan.reyes.sh/posts/keeping-secrets-out-of-logs/</link><description>&lt;doc fingerprint="a622f1124d050d9c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Keeping Secrets Out of Logs&lt;/head&gt;
    &lt;p&gt;This post is about how to keep secrets out of logs, and my claim is that (like many things in security) there isn’t a singular action or silver bullet that lets you do this. I would go so far as to say that there’s not even an 80/20 rule, where one action fixes 80% of the problem. It’s not like preventing SQL injection with prepared statements or preventing buffer overflows by using memory-safe languages.&lt;/p&gt;
    &lt;p&gt;What I will offer instead, are lead bullets, of which there are many. I’m going to talk about 10 of them. They are imperfect and sometimes unreliable things that, if put in the right places and with defense-in-depth, can still give us a real good chance at succeeding. My hope is that by the end, you’ll have a slightly better framework for how to reason about this problem and some new ideas to add to your kit.&lt;/p&gt;
    &lt;p&gt;Table of contents:&lt;/p&gt;
    &lt;head rend="h2"&gt;The Problem&lt;/head&gt;
    &lt;p&gt;With that, let’s dive in and set the table by talking about the problem with secrets in logs.&lt;/p&gt;
    &lt;p&gt;So, there are some problems that are annoying. And there are some problems that are difficult.&lt;/p&gt;
    &lt;p&gt;This is both. I’m gonna level with you: I absolutely hate this problem. But I’m not going to gaslight you and tell you that this is the most important thing to work on worry about, because it probably isn’t!&lt;/p&gt;
    &lt;p&gt;You have somewhere between 5 and 50 other problems in your backlog that seem more important, 1 of which you found out about this morning. But I think it’s likely that none of those problems are nearly as annoying. While researching this topic, I interviewed about a dozen other engineers and, on this point, they unanimously agreed! Nobody likes dealing with secrets in logs because it is extraordinarily annoying.&lt;/p&gt;
    &lt;p&gt;This is a problem that’s also difficult, but not even in the fun sense, like being technically complex or interesting. Once you catch sensitive data in logs, it’s usually pretty straightforward (at least in retrospect) to determine how they got there. But, it’s also surprisingly elusive to prevent, and it crops up in incredibly unexpected places and ways.&lt;/p&gt;
    &lt;p&gt;Secrets could mean lots of different things to lots of different teams, but I’ll use it interchangeably with “sensitive data”: stuff that you want to keep confidential. What’s so frustrating when breaching confidentiality in logs is the full spectrum of potential impact.&lt;/p&gt;
    &lt;p&gt;In the best case (left), you might log an isolated, internal credential, like an API key, which (kudos!) you rotate right after fixing the source of leak. The impact is minimal, and you just move on. Of course, all the way on the other end of the spectrum (right), you might log something that an attacker or inside threat could use to do some real harm.&lt;/p&gt;
    &lt;p&gt;And then somewhere in-between, where I suspect most of the incidents lie. You might log secrets that you unfortunately, can’t rotate yourself. Things like PII or your customer’s passwords, which are reused on other sites, because of course they are. And, depending on your policies, threat model, or regulations, you might choose to issue a disclosure or notification.&lt;/p&gt;
    &lt;p&gt;And it is painful.&lt;/p&gt;
    &lt;p&gt;You could be doing so many good data security practices, like secure-by-design frameworks, database and field-level encryption, zero-touch production, access control&amp;amp;mldr; but logging bypasses all of that&amp;amp;mldr; and ultimately degrades trust, in your systems and in your company. It feels unfair because it’s only a fraction of your security story.&lt;/p&gt;
    &lt;p&gt;And this is a problem that happens to companies of all sizes:&lt;/p&gt;
    &lt;p&gt;Something about “plaintext” just kinda stings, especially as a security practitioner. It’s like&amp;amp;mldr; the most profane insult you can hurl at a security engineer. Imagine retorting with, “Oh yea? Well, you store your passwords in plaintext!”&lt;/p&gt;
    &lt;p&gt;But logging passwords and storing them in plaintext are&amp;amp;mldr; kinda the same thing.&lt;/p&gt;
    &lt;p&gt;Because while logs are rarely or purposefully public, they’re typically afforded broader access than direct access to your databases.&lt;/p&gt;
    &lt;p&gt;Everyone knows by now that storing plaintext secrets in your database is a terrible idea. Logs, however, are still data-at-rest, and we should treat them with the same level of scrutiny.&lt;/p&gt;
    &lt;p&gt;I cherry picked those examples because they are established companies with very mature security programs. I’m not trying to throw shade; in fact, I deeply respect them for being public and transparent about this. I think this also hints that preventing secrets in logs is a deceptively difficult and frustrating problem.&lt;/p&gt;
    &lt;p&gt;If we can understand some causes, we might gain a deeper appreciation for these past occurrences, and stand a better chance at avoiding new incidents in the future.&lt;/p&gt;
    &lt;head rend="h2"&gt;Causes&lt;/head&gt;
    &lt;p&gt;This is certainly not comprehensive, but from my interviews and personal experience, here are six of the most common causes.&lt;/p&gt;
    &lt;head rend="h3"&gt;🤦 Direct logging&lt;/head&gt;
    &lt;code&gt;const temp = res.cookie["session"];

// TODO: remove after testing is done
Logger.info("session HERE", { temp });
&lt;/code&gt;
    &lt;quote&gt;
      &lt;p&gt;Narrator: it was not removed after testing was done&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The first group is perhaps the most obvious and facepalm one: when sensitive data is directly logged. Sometimes it’s purely accidental, like the example above: someone wants to debug session cookies in their local environment and then&amp;amp;mldr; accidentally commits the code. Sometimes it comes from an uninformed position where the developer just doesn’t know any better.&lt;/p&gt;
    &lt;p&gt;These tend to be fairly easy to trace down the exact line of code or commit that introduces it. With this example, you can just grep the codebase for &lt;code&gt;session here&lt;/code&gt; and you’ll find it instantly.&lt;/p&gt;
    &lt;head rend="h3"&gt;🚰 Kitchen sinks&lt;/head&gt;
    &lt;code&gt;const client = googleSdk.admin(...);
try {
  const res = client.tokens.list(...);
} catch (e) {
  Logger.error("failed fetch", { e });
}
&lt;/code&gt;
    &lt;p&gt;I’m sure you’ve seen or written code like this before. Here we have an API client or SDK that is used to fetch some data. Exceptions are caught, kind of, and then promptly logged so that on-call engineers can debug the errors.&lt;/p&gt;
    &lt;p&gt;What happens?&lt;/p&gt;
    &lt;p&gt;That error is decorated with a config object stuffed with secrets and the full response object, which is also stuffed with secrets, and now they’re both in your logs!&lt;/p&gt;
    &lt;code&gt;{
  e: {
    status: 400,
    ...
    config: { 💥☠️🪦 },
    response: { 💣😭😱 },
  }
}
&lt;/code&gt;
    &lt;p&gt;I call these “kitchen sinks,” objects that contain or hold secrets, often in opaque or unexpected ways. Think of an actual kitchen sink that’s filled to the brim with dirty dishes and you can’t easily tell what’s at the bottom without reaching into it. Maybe it’s a spoon, or maybe it’s knife and now you have to go to the hospital. What tends to happen is that the whole kitchen sink gets logged, and the logging library happily serializes it, including parts that were actually sensitive.&lt;/p&gt;
    &lt;p&gt;This seems to happen with code that attaches additional data onto errors, or code that logs full request and response objects. It’s typically a bit hard to catch in code review unless you know to look for them. If you are blessed with static types, seeing an &lt;code&gt;any&lt;/code&gt; type flow into logs can be a good hint that you’re
logging too much.&lt;/p&gt;
    &lt;head rend="h3"&gt;🔧 Configuration changes&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;Narrator: it was not okay&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Next example: someone needs additional observability and changes a setting like the global log level. You know exactly what happens, here. This dev is about to have a bad time and find out that hope, in fact, is not a valid strategy.&lt;/p&gt;
    &lt;p&gt;We started with an observability problem. Now we also have security problem: brand new secrets are getting emitted into logs.&lt;/p&gt;
    &lt;p&gt;In that example (that totally never happened to me ever), developers built production around log levels set to &lt;code&gt;WARN&lt;/code&gt; and above, but once you flip it to
&lt;code&gt;DEBUG&lt;/code&gt;, all this new stuff comes out of the woodwork.&lt;/p&gt;
    &lt;p&gt;These type of configuration changes tend to involve a system that was built with one set of assumptions, but some kind of modification moves that system from a known state into a unknown state, introducing a new set of problems.&lt;/p&gt;
    &lt;p&gt;These often involve low-level or global utilities like logging config, HTTP middleware, or some central piece of infra like a load balancer. They tend to be singletons that are difficult or costly to test, or they crop up only at runtime. On the positive side, it’s usually loud and quick to patch, but cleanup can be kinda painful.&lt;/p&gt;
    &lt;head rend="h3"&gt;🥧 Embedded secrets&lt;/head&gt;
    &lt;code&gt;app.get("/login/:slug", async (req, res) =&amp;gt; {
  const magicLink = req.params["slug"];
  await login({ magicLink });
});
&lt;/code&gt;
    &lt;p&gt;I completely made up this phrase, but the idea is that secrets are coupled to, embedded into, and baked into more general formats like URLs or remote procedure calls. The central idea is that it’s designed into the format and the system, and can’t easily be separated.&lt;/p&gt;
    &lt;p&gt;Say you have a magic login link handler (see above) where a user can click a link and sign into a web app. There’s nothing in that code that logs the link, but if you look at HTTP logs, it’s right there in plain view:&lt;/p&gt;
    &lt;code&gt;47.29.201.179 - - [17/Jul/2024:13:17:10 +0000] "GET /login/Uj79z1pe01...
&lt;/code&gt;
    &lt;p&gt;These types of leaks arise from fundamental designs that don’t take logging into consideration or incorrectly assume some end-to-end flow. The sensitivity gets lost out of context, and ends up getting logged in another layer, system, or service.&lt;/p&gt;
    &lt;head rend="h3"&gt;📡 Telemetry&lt;/head&gt;
    &lt;code&gt;try:
    db_name = os.getenv("DB_NAME")
    db_pass = os.getenv("DB_PASS") # 🤫 Secret!
    conn = db.connect(db_name, db_pass)
    ...
except Error as e:
    # Don't log e! Not today!!11
    Logger.error("failed to connect")
finally:
    conn.close()
&lt;/code&gt;
    &lt;p&gt;Next example: we have some Python code that’s connecting to a database, we’re specifically NOT logging the error object, and we want to ensure we always close out the connection.&lt;/p&gt;
    &lt;p&gt;How can &lt;code&gt;db_pass&lt;/code&gt; possibly make it into logs? Telemetry!&lt;/p&gt;
    &lt;p&gt;"Oops, that's a log, too!"&lt;/p&gt;
    &lt;p&gt;It turns out that things like error monitoring and analytics can totally be logs, too. I kind of cheated in the code example, because there’s no mention of telemetry in it at all, but it turns out that if you hook it up to error monitoring like Sentry (above), run-time errors send the local variable context right to the dashboard, and you can see the database password in plaintext.&lt;/p&gt;
    &lt;p&gt;These causes tend to bypass the central logging pipeline and become Yet Another Place to have to worry about secrets.&lt;/p&gt;
    &lt;head rend="h3"&gt;🕺🏻 User input&lt;/head&gt;
    &lt;p&gt;Alright, last example. Say there’s a sign in form and the entire dev team made super duper sure that the password field is totally locked down from logging, they read this super awesome post, and took care of all the causes we discussed.&lt;/p&gt;
    &lt;p&gt;What happens?&lt;/p&gt;
    &lt;p&gt;Users end up jamming passwords into the username field!&lt;/p&gt;
    &lt;p&gt;So if you ever looked at login alerts for AWS and saw usernames replaced with &lt;code&gt;HIDDEN_DUE_TO_SECURITY_REASONS&lt;/code&gt;, this is precisely why!&lt;/p&gt;
    &lt;p&gt;Everything that’s within proximity to sensitive user input tends to be radioactive. It could be a UI issue, but users are surprisingly determined to volunteer secrets in ways that you haven’t prepared for.&lt;/p&gt;
    &lt;p&gt;We’ve touched on a half dozen causes, and the list of things goes on. We didn’t even talk about the wonder that is crashdumps. But, I think it’s important to zoom out and note that these are proximate causes.&lt;/p&gt;
    &lt;p&gt;I stand by my claim that there’s no silver bullet to take these all out. If we want to avoid playing whack-a-mole, we must bring out our lead bullets that address these issues at a deeper level, and prevent these kinds of things from happening.&lt;/p&gt;
    &lt;head rend="h2"&gt;Fixes (lead bullets)&lt;/head&gt;
    &lt;p&gt;So let’s dive in! We will survey 10 fixes, and the order we’ll go in is somewhere between “a dependency graph of things that build on each other” and “following the lifecycle of a secret.” Some of these are obvious or perhaps things you’re already doing, so I’ll focus more on fixes that I think might be a bit newer. That said, it is worth starting with the basics.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;📐 Data architecture&lt;/item&gt;
      &lt;item&gt;🍞 Data transformations&lt;/item&gt;
      &lt;item&gt;🪨 Domain primitives&lt;/item&gt;
      &lt;item&gt;🎁 Read-once objects&lt;/item&gt;
      &lt;item&gt;🗃️ Log formatters&lt;/item&gt;
      &lt;item&gt;🧪 Unit tests&lt;/item&gt;
      &lt;item&gt;🕵️ Sensitive data scanners&lt;/item&gt;
      &lt;item&gt;🤖 Log pre-processors&lt;/item&gt;
      &lt;item&gt;🔎 Taint checking&lt;/item&gt;
      &lt;item&gt;🦸 People&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;📐 Data architecture&lt;/head&gt;
    &lt;p&gt;Lead bullet #1 is the most basic and high-level: data architecture and understanding that this is primarily a data flow problem. And part of the solution is reducing the number of data flows and shrinking the problem space so you simply have less things to worry about and protect.&lt;/p&gt;
    &lt;p&gt;Instead of stray print statements or components that write directly to filesystem, you instead centralize all your data flows through a single stream. Make it so that there’s one and only one way to log something. If you can understand and control the data structures that enter that funnel, you can prohibit secrets from exiting it.&lt;/p&gt;
    &lt;p&gt;This has the allure of being a silver bullet, because of course if you can get to 100% of all the things we mentioned here, you’re golden! But in practice (and as we’ve seen previously), that’s difficult because secrets find a way to sneak in or new outflows and side channels are created.&lt;/p&gt;
    &lt;head rend="h3"&gt;🍞 Data transformations&lt;/head&gt;
    &lt;p&gt;The previous bullet was about controlling how data flows through your system, this is about transforming, slicing, and disarming that data into safer forms that can be logged. These are the data security fundamentals that you’re already familiar with and likely already doing. This is your bread and butter, so I’m not going to dive into every one. From top to bottom, this is generally arranged from awesome to meh&amp;amp;mldr; basically, by how much information is retained.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Transformation&lt;/cell&gt;
        &lt;cell role="head"&gt;Result&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Minimization&lt;/cell&gt;
        &lt;cell&gt;☁ (nothing)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Redaction&lt;/cell&gt;
        &lt;cell&gt;[redacted]&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Tokenization&lt;/cell&gt;
        &lt;cell&gt;2706a40d-3d1d&amp;amp;mldr;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Hashing&lt;/cell&gt;
        &lt;cell&gt;daadfab322b59&amp;amp;mldr;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Encryption&lt;/cell&gt;
        &lt;cell&gt;AzKt7vBE7qEuf&amp;amp;mldr;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Masking&lt;/cell&gt;
        &lt;cell&gt;··········&lt;code&gt;5309&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;On the top, we have data minimization. The best way to not log secrets, is to not have secrets to begin with! This is everything from going passwordless to fetching only the data you need.&lt;/p&gt;
    &lt;p&gt;Redaction is the next best thing. Blanking out the secret parts and before you pass objects around in memory.&lt;/p&gt;
    &lt;p&gt;Tokenization, hashing, encryption: these all have their pros, cons, and caveats. Like&amp;amp;mldr; are you even doing it correctly?&lt;/p&gt;
    &lt;p&gt;Dead last is masking. You leave parts of the secret intact. Maybe this works for you. Maybe it doesn’t. Maybe you go straight to jail 🤷&lt;/p&gt;
    &lt;p&gt;When these techniques work, they generally work well. But very often what happens is that they aren’t used or are used too late, after something is already logged. These have their places in our toolbox, but my claim again is one bullet isn’t enough.&lt;/p&gt;
    &lt;head rend="h3"&gt;🪨 Domain primitives&lt;/head&gt;
    &lt;p&gt;Let’s introduce lead bullet #3: domain primitives. Almost all the secrets you run across in codebases are encoded in-memory as string primitives, and I think that makes our jobs harder. Strings can be almost anything.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Strings: any sequence of bytes from&lt;/p&gt;&lt;code&gt;""&lt;/code&gt;to&lt;code&gt;"c̴̞̑ť̸͈̘̌ h̸͝ ̭̘̊ü̶̜̫̦̠͋̆͠ ļ̵̮̤̟̉̀͂ṹ̴̝̂🤷867-53-0999"&lt;/code&gt;&lt;/quote&gt;
    &lt;code&gt;const secret = "..."
&lt;/code&gt;
    &lt;p&gt;There’s very little about them——at compile time or run-time——that lets you know that it’s sensitive, dangerous to log, or somehow different than any other vanilla string.&lt;/p&gt;
    &lt;p&gt;The alternative is a concept I learned from the book Secure by Design, and I think it’s one of the most powerful concepts you can add to your codebase, for logs or anything else where you want to layer in security at a fundamental level.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Domain primitives: “combines secure constructs and value objects to define the smallest building block of a domain”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;code&gt;const secret = new Secret("...")
&lt;/code&gt;
    &lt;p&gt;You use them as basic building blocks that hold secret values, and they provide security invariants and guarantees that basic string primitives simply cannot.&lt;/p&gt;
    &lt;p&gt;It’s one of the easiest things you can do. If you shift from “any string can be a secret” to “secrets are secrets”, it makes things a lot easier to reason about and protect.&lt;/p&gt;
    &lt;head rend="h4"&gt;Compile-time&lt;/head&gt;
    &lt;p&gt;You can use these to great advantage at compile-time, giving developers immediate feedback right in their editors.&lt;/p&gt;
    &lt;p&gt;We can type a logging function (&lt;code&gt;log()&lt;/code&gt;) so that it never accepts secrets.
Then, we use some fetching function that returns secrets, typed as secrets (and
not as strings). If we try to log that secret, it will not compile. The type
system will not let you log this secret.&lt;/p&gt;
    &lt;code&gt;// Types
declare const brand: unique symbol;
type Secret = string &amp;amp; { [brand]: string }; // Branded type that extends string
type NotSecret&amp;lt;T&amp;gt; = T extends Secret ? never : T; // Type that excludes secrets

// Logging function
function log&amp;lt;T extends string&amp;gt;(message: NotSecret&amp;lt;T&amp;gt;) { ... };
&lt;/code&gt;
    &lt;code&gt;const message: string = "this is fine"; // 🧵 string primitive
const secretz: Secret = getSecret();    // 👈 domain primitive

log(message); // 👌 compiles!
log(secretz); // 💥 error!
&lt;/code&gt;
    &lt;quote&gt;
      &lt;p&gt;See this example in the TypeScript Playground.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I’m omitting and glossing over a ton of details here, because I don’t want you to focus on the implementation or even TypeScript, for that matter. The salient point here is that instead of tossing secret strings around, you brand them as secret types, providing useful context to both compiler and developer.&lt;/p&gt;
    &lt;head rend="h4"&gt;Run-time&lt;/head&gt;
    &lt;p&gt;It’s really easy to get started, even with code that is functionally a no-op. This is basically the simplest form I can think of—an almost empty class:&lt;/p&gt;
    &lt;code&gt;class OpenAIToken extends String { /* that could be it! */ }

const token = new OpenAIToken(...);
&lt;/code&gt;
    &lt;p&gt;It’s supposed to represent OpenAI credentials, but it’s just using and extending basic language primitives. You can introduce these objects where secrets originate, like password fields or anytime you decrypt sensitive data fetched from the database. And then layer in behaviors and invariants for where they tend to end up. You progressively start introducing these at both sources and sinks, allowing you to control where secrets should or shouldn’t go. You can embed these into data structures so you know what contains secrets. And along the way, you increase the clarity and safety of your codebase: not only can you prevent these tokens from going into logs, you can make sure you’re sending them only to OpenAI and not to some other API by accident.&lt;/p&gt;
    &lt;p&gt;I think in the long run, domain primitives are the most powerful control we have because it makes our code secure by design, but it does take some time to get there. These can easily address the direct logging cause we discussed earlier, and with some modifications can help with many more.&lt;/p&gt;
    &lt;head rend="h4"&gt;Run-time: part deux&lt;/head&gt;
    &lt;p&gt;We can extend this and make it so that the default serialization behavior is redaction.&lt;/p&gt;
    &lt;code&gt;class Secret extends String {
    toString() { return "[redacted]" } // Override!
}
&lt;/code&gt;
    &lt;code&gt;const secret = new Secret("shhh!");
console.log(secret);
&lt;/code&gt;
    &lt;code&gt;Secret: "[redacted]"
&lt;/code&gt;
    &lt;p&gt;If you try to stuff this into logs, into JSON, into kitchen sinks, into error monitoring, wherever, it’ll always spit out the word “redacted”. You have to intentionally reach for the value.&lt;/p&gt;
    &lt;p&gt;Let’s take it further. We can create a custom class with an explicit &lt;code&gt;unwrap()&lt;/code&gt;
function:&lt;/p&gt;
    &lt;code&gt;class Secret&amp;lt;T&amp;gt; {
    constructor(private readonly value: T) {}
    toString() { return "[redacted]" } // Override serialization
    unwrap() { return this.value }     // Explicit getter function
}
&lt;/code&gt;
    &lt;p&gt;There’s so many things you can do here, like maybe you want to encrypt or zero it out in memory, because that’s in your threat model. You can take this as far as you need to or are comfortable with. We’ll take it just one step further.&lt;/p&gt;
    &lt;head rend="h3"&gt;🎁 Read-once objects&lt;/head&gt;
    &lt;p&gt;There’s a bit to unpack here, but these build off domain primitives in a very powerful way.&lt;/p&gt;
    &lt;code&gt;class Secret&amp;lt;T&amp;gt; {
    private locked = false;
    constructor(private readonly value: T) {}
    toString() { return "[redacted]" }

    /* @returns the sensitive value (once and only once) */
    unwrap() {
        if (this.locked) { throw new Error("already read") }
        this.locked = true;
        return this.value;
    }
}
&lt;/code&gt;
    &lt;p&gt;These objects wrap and keep the secret safe, until you actually need it. The code in the &lt;code&gt;unwrap()&lt;/code&gt; function is the crux: there’s a latch or
lock that activates after the secret is retrieved the first time. It goes into a
“locked” state, and any following reads result in an error that fails loudly.&lt;/p&gt;
    &lt;code&gt;const secret = getSecret();
const res = await authenticate(secret.unwrap()); // Proper usage

Logger.info(secret);          // [redacted]
Logger.info(secret.unwrap()); // 💥 Error!
&lt;/code&gt;
    &lt;p&gt;Once you get a secret (from user input, database, decryption, etc.) you wrap it in a read-once object immediately and keep it wrapped for as long as you can. And for its single, intended purpose, like using it for some kind of API authentication, you unwrap the value, use it, and then the object stays locked for good. This is surprisingly effective at preventing and detecting unintentional use. It addresses and disarms many of the proximate causes that we discussed earlier.&lt;/p&gt;
    &lt;p&gt;This object pairs extremely well with static analysis. Tools like CodeQL or Semgrep can help ensure that developers aren’t bypassing any safety guarantees.&lt;/p&gt;
    &lt;p&gt;These are generally high signal, especially when you have good unit test coverage. One drawback is that read-once objects, if handled incorrectly but not necessarily unsafely, could cause errors at run-time. But I think the tradeoffs are usually worth it, especially if you complement it with testing, static analysis, and taint-checking. Speaking of which&amp;amp;mldr;&lt;/p&gt;
    &lt;head rend="h3"&gt;🔎 Taint checking&lt;/head&gt;
    &lt;p&gt;I like to think of taint checking as static analysis with superpowers. I absolutely love it and the first time I used it, it was like someone just handed me a lightsaber. Quick review for the uninitiated: the general idea here is that you add taint to various sources (like database objects), and yell loudly if the data flows into certain sinks (like logs).&lt;/p&gt;
    &lt;p&gt;The red data flow trace on the right detects the secret flowing into logs. But the green path is fine, because the secret is tokenized. Let’s walk through a quick example: semgrep.dev/playground/s/4bq5L&lt;/p&gt;
    &lt;p&gt;On the left, we’ve marked a couple sources like decrypt and a database fetcher. We’ve also marked our logger as a sink, and the &lt;code&gt;tokenize()&lt;/code&gt; function as a
sanitizer.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;On the right in red, we can see that taint was created from the decrypt function, propagated through the &lt;code&gt;getSSN()&lt;/code&gt;function, and then flagged for going into the logs on line 18.&lt;/item&gt;
      &lt;item&gt;In blue, there’s a much shorter path where the user model from the database is tainted and then flagged for going into logs.&lt;/item&gt;
      &lt;item&gt;And then lastly, in green, we’re tokenizing the decrypted SSN, so it’s not flagging that it’s logged.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The idea that this is checking millions or more different data flows is the real magic part for me.&lt;/p&gt;
    &lt;head rend="h4"&gt;Awesome&lt;/head&gt;
    &lt;p&gt;Some of the strengths of taint analysis: obviously automation. Tracing these data flows is 100% a job for a machine. This can really help with domain primitives but also can be used standalone and can even key in on heuristics like variable names: for example, all variables containing “password”. You can tie this into all of your critical tools, from code review to CI/CD.&lt;/p&gt;
    &lt;p&gt;This is especially potent against kitchen sinks and embedded secrets, because those data structures can be tainted by secret values and checked accordingly.&lt;/p&gt;
    &lt;head rend="h4"&gt;Not awesome&lt;/head&gt;
    &lt;p&gt;Some personal opinions on drawbacks: I do feel like taint checking rules tend to be a bit difficult to write. I really, really like Semgrep, but I’m also not the biggest fan of YAML.&lt;/p&gt;
    &lt;p&gt;It also turns out that data flow analysis is an NP-hard problem so for large codebases and monorepos, you likely can’t run full taint analysis on every pull request or commit. Because it runs in CI/CD and as part of change management, when it works, it can prevent the introduction of insecure logging into the codebase.&lt;/p&gt;
    &lt;p&gt;But, like all of the lead bullets we’ve discussed and will discuss, they can miss. How can we handle that?&lt;/p&gt;
    &lt;head rend="h3"&gt;🗃️ Log formatters&lt;/head&gt;
    &lt;p&gt;Let’s say we made the mistake of logging too much data with our email service:&lt;/p&gt;
    &lt;code&gt;{
  tenantId: "52902156-7fb6-4ab0-b659-6b07b80cf89a",
  email: {
    subject: "Log in to your account",
    html: '&amp;lt;a href="https://acme.com/login/98fPm..."&amp;gt;Click here&amp;lt;/a&amp;gt; to log in!',
    from: "AcmeCorp &amp;lt;[email protected]&amp;gt;",
    to: "Darth Plagueis (The Wise) &amp;lt;[email protected]&amp;gt;",
    ...
  },
  response: {
    status: 200,
    originalRequest: {
      headers: {
        Authorization: "Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIi..."
      },
      body: '{"html": "&amp;lt;a href=\\"https://acme.com/login/98fP...\\"&amp;gt;Click..."}',
      ...
    }
    ....
  },
  ...
}
&lt;/code&gt;
    &lt;p&gt;We have a couple of our usual suspects here. Because we’re logging email contents, magic links show up in logs&amp;amp;mldr; twice! We’re also logging some kitchen sinks, like email metadata and the original request, so we have PII and authorization headers also in logs. But because this data is structured, if we can traverse these objects, it turns out that we can zero in on these leaks quite effectively.&lt;/p&gt;
    &lt;code&gt;{
  tenantId: "52902156-7fb6-4ab0-b659-6b07b80cf89a",
  email: {
    subject: "Log in to your account",
    html: '&amp;lt;a href="https://acme.com/login/REDACTED"&amp;gt;Click here&amp;lt;/a&amp;gt; to log in!',
    from: "AcmeCorp &amp;lt;[email protected]&amp;gt;",
    to: "REDACTED",
    ...
  },
  response: {
    status: 200,
    originalRequest: {
      headers: "REDACTED",
      body: '{"html": "&amp;lt;a href=\\"https://acme.com/login/REDACTED\\"&amp;gt;..."}',
      ...
    }
    ....
  },
  ...
}
&lt;/code&gt;
    &lt;p&gt;If we can introspect these objects, we can scan for dangerous substrings like our login links, and then drop or redact them. Or we can drop whole values, if we know that certain paths like &lt;code&gt;email.to&lt;/code&gt; are particularly dangerous. Fields like
&lt;code&gt;request&lt;/code&gt; or &lt;code&gt;headers&lt;/code&gt; tend to be risky objects that we can also remove. We can
even drop the whole log object if it doesn’t meet some admission criteria,
or—we can simply error out.&lt;/p&gt;
    &lt;p&gt;So, how and where do we deploy something like this? Most application loggers should have some type of middleware stack or pipeline, kinda like here on the right. These are typically configured for operations like converting objects into JSON, turning error objects into readable formats, or enriching logs by inserting useful context like network information. We can invert that, and instead of enriching with useful data, we can remove or redact sensitive data.&lt;/p&gt;
    &lt;code&gt;export const logger = createLogger({
  format: format.combine(
    transform(),
    handleErrors(),
    enrich(),

      redact(), // 👈 insert here!

    truncate(),
    jsonify(),
    ...
  ),
  ...
});
&lt;/code&gt;
    &lt;p&gt;This is a type of guardrail that helps catch many of the common problems we described previously, like request headers or config objects. I’ve used this with decent success and found that it works best as a rifle instead of a shotgun. Because it’s at the application tier, you can customize it for the type of data or context that each application handles. For example, we can make it so that any of our domain primitives that reach this layer are quickly detected and removed.&lt;/p&gt;
    &lt;p&gt;This is extremely cheap to introduce, but there are some trade-offs. It’s certainly more of a safety net than hard control, and a developer determined to bypass it, can and will. Steady state, I measured this at less than 1% of clock time, but there are some deeply unfortunate ways this can go wrong such as poorly written regexes and self-ReDoS.&lt;/p&gt;
    &lt;p&gt;More or less, these risks can be mitigated with solid unit-testing. Which leads us to&amp;amp;mldr;&lt;/p&gt;
    &lt;head rend="h3"&gt;🧪 Unit tests&lt;/head&gt;
    &lt;p&gt;Lead bullet #7: hooking into and using the existing test suite—that’s already there—to our advantage. We can use several of the tools we discussed, but instead of simply detecting or redacting secrets, we can ramp up the sensitivity in our test environment to fail or error loudly.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Technique&lt;/cell&gt;
        &lt;cell role="head"&gt;Prod&lt;/cell&gt;
        &lt;cell role="head"&gt;Test&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;🪨 Domain primitives&lt;/cell&gt;
        &lt;cell&gt;Redact&lt;/cell&gt;
        &lt;cell&gt;Error&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;🎁 Read-once objects&lt;/cell&gt;
        &lt;cell&gt;Error&lt;/cell&gt;
        &lt;cell&gt;Error&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;🗃️ Log formatters&lt;/cell&gt;
        &lt;cell&gt;Redact&lt;/cell&gt;
        &lt;cell&gt;Error&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;🕵️ Sensitive data scanners&lt;/cell&gt;
        &lt;cell&gt;Detect&lt;/cell&gt;
        &lt;cell&gt;Error&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;I’ll cover sensitive data scanners next, but many test suites are already set up to capture &lt;code&gt;stdout&lt;/code&gt; and &lt;code&gt;stderr&lt;/code&gt;, and so you can even point your scanners to
these capture buffers.&lt;/p&gt;
    &lt;p&gt;The takeaway here is that you can reap the same benefits of CI/CD and change management by catching unsafe code before it’s merged or deployed, but of course, you’re also dependent on coverage and if the right code and data paths are exercised.&lt;/p&gt;
    &lt;head rend="h3"&gt;🕵️ Sensitive data scanners&lt;/head&gt;
    &lt;p&gt;These are fairly blunt but effective tools that can discover and remove sensitive data. I’m actively going to avoid diving deep here, because it does seem like many teams and vendors focus on this as the solution. So instead, I’d like to pose a few questions that might help you reason about trade-offs:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Where and when in your logging pipeline is it most effective?&lt;/item&gt;
      &lt;item&gt;Is it a gate, in-line of the critical path, or does it scan asynchronously?&lt;/item&gt;
      &lt;item&gt;Do you simply want to detect or do you bias towards masking and redaction? How will your team handle and deal with false positives?&lt;/item&gt;
      &lt;item&gt;How far do the general, out-of-box rules take you? Can you tailor it specifically to your usage patterns?&lt;/item&gt;
      &lt;item&gt;Can you verify the credentials? Can that even keep up with log throughput?&lt;/item&gt;
      &lt;item&gt;And then perhaps what tends to be the long pole in the tent: what are the costs, and can you sample instead?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I think these tools tend to be better suited for defense-in-depth, because they presume that secrets made it into logs to begin with. They can help catch the more elusive causes we discussed like configuration changes or user input.&lt;/p&gt;
    &lt;head rend="h4"&gt;Sampling&lt;/head&gt;
    &lt;p&gt;A very brief segue into sampling. Logs tend to have a kind of power law distribution, where certain types of logs vastly outnumber others. And typically what you see is that log sources have static points in code, generally with the same type of data running through them. And so within each log type, scanning and finding a single true positive might be highly representative of that group.&lt;/p&gt;
    &lt;p&gt;And so you might run into a scenario where, given some global sample rate, you’re wasting a lot of work for high frequency logs and not even scanning lower frequency logs. I think a better alternative to a global sample rate is to aggregate logs by some heuristic like type or origin, and to ensure you hit some minimum threshold.&lt;/p&gt;
    &lt;p&gt;In practice, I’ve found this difficult or impossible to configure with out-of-box solutions. I’ve had to introduce additional infrastructure to help. And that’s our next lead bullet.&lt;/p&gt;
    &lt;head rend="h3"&gt;🤖 Log pre-processors&lt;/head&gt;
    &lt;p&gt;Second to last lead bullet, #9: log pre-processors. These sit between apps that emit logs, and the final data stores.&lt;/p&gt;
    &lt;p&gt;In the above example, something like Vector can receive and process logs from our microservices before dispatching them to DataDog or wherever logs end up. We can configure it to drop sensitive data in-place using many of the techniques we discussed before. And we can sample some subset of them and store them onto an S3 bucket, using a more powerful tool like Trufflehog or an LLM to catch and verify secrets.&lt;/p&gt;
    &lt;p&gt;The idea here is to process logs streams before they’re persisted. It doesn’t need to be Vector, chances are, you already have this existing infrastructure that’s used for deduping, aggregation, and dropping noisy debug logs. We can re-use it to prevent and detect secrets in logs. This pairs very well with sensitive data scanners that we discussed earlier, and might even unlock new ones you thought were out of reach.&lt;/p&gt;
    &lt;head rend="h3"&gt;🦸 People&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;“Human practitioners are the adaptable element of complex systems. Practitioners and first line management actively adapt the system to maximize production and minimize accidents.”&lt;/p&gt;
      &lt;p&gt;-Richard Cook, https://how.complexsystems.fail/#12&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Our last stop is people. Modern software is a complex system. And while people will write the code that accidentally introduces sensitive data into logs, they’re also the ones that will report, respond, and fix them. They’ll build out the systems and infrastructure that will keep these complex systems safe. And early on in your maturity story and before you’re able to build out secure-by-design frameworks, this is the lead bullet you’ll most likely use the most.&lt;/p&gt;
    &lt;p&gt;The most important message I want to convey here is that your security team isn’t alone, especially if you:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;educate your teammates on secure logging design&lt;/item&gt;
      &lt;item&gt;empower them to report and address these issues&lt;/item&gt;
      &lt;item&gt;and equip them with tools that get out of their way and helps them succeed.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Recap&lt;/head&gt;
    &lt;p&gt;Alright, so we’ve covered lead bullets that protect code, protect data, and protect logs:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;📐 Data architecture&lt;/item&gt;
      &lt;item&gt;🍞 Data transformations&lt;/item&gt;
      &lt;item&gt;🪨 Domain primitives&lt;/item&gt;
      &lt;item&gt;🎁 Read-once objects&lt;/item&gt;
      &lt;item&gt;🗃️ Log formatters&lt;/item&gt;
      &lt;item&gt;🧪 Unit tests&lt;/item&gt;
      &lt;item&gt;🕵️ Sensitive data scanners&lt;/item&gt;
      &lt;item&gt;🤖 Log pre-processors&lt;/item&gt;
      &lt;item&gt;🔎 Taint checking&lt;/item&gt;
      &lt;item&gt;🦸 People&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Some of these might work for you, some of these won’t, and some that we haven’t even mentioned could be a homerun for you. Maybe you have super tight control over your log schemas or maybe you’re using LLMs in a really neat and effective way. Or maybe you’re building or using a language that has first class support for controlling secrets.&lt;/p&gt;
    &lt;p&gt;These worked for me. I have some personal opinions on ones which are foundational, some that are powerful in the long-run, and some that are really easy to get started. But your story is different, so I’d like to zoom out and close out with a high-level, methodical strategy that you can apply for your security programs, and that we’ll apply and walk through with an example.&lt;/p&gt;
    &lt;head rend="h2"&gt;Strategy&lt;/head&gt;
    &lt;p&gt;Here’s a general strategy:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Lay the foundation&lt;/item&gt;
      &lt;item&gt;Understand the data ﬂow&lt;/item&gt;
      &lt;item&gt;Protect at chokepoints&lt;/item&gt;
      &lt;item&gt;Apply defense-in-depth&lt;/item&gt;
      &lt;item&gt;Plan for response and recovery&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I’m not shooting for a Nobel, here. You’re probably doing some of these already, and chances are, you have some type of playbook or process that looks just like this. The key idea here is to not miss the forest for the trees, and use these explicit steps to place our efforts where they’ll matter most. I’ll walk you through a hypothetical system and we’ll apply these in order.&lt;/p&gt;
    &lt;head rend="h3"&gt;0. Lay the foundation&lt;/head&gt;
    &lt;p&gt;Step zero is the foundation. Table stakes. This is like the base tier of Maslow’s hierarchy, and we need these before we try anything else.&lt;/p&gt;
    &lt;p&gt;Developing expectations, culture, and support is a must-have. They’re easy to ignore or forget about, but can make or break success. If you work at place that hasn’t addressed these in the past, it can be quite jarring or difficult to shift that mentality.&lt;/p&gt;
    &lt;p&gt;I don’t have a ton of advice here other than making sure your org is aligned on this. It’ll probably feel like it’s getting worse before it’s getting better, but that is a sign of progress. A great litmus test for a solid foundation is if developers will (or already have) come to you to report secrets they found in logs.&lt;/p&gt;
    &lt;p&gt;The second thing we’ll need is to decide is what we consider a secret to begin with. I, admittedly, used secrets and sensitive data interchangeably. This may not be the case for you. It doesn’t need to be perfect or comprehensive, and maybe it’s just a framework. But employees, especially the security team, need common understanding.&lt;/p&gt;
    &lt;p&gt;The third item is technical. If our logs aren’t structured or aren’t JSON, it’ll make this endeavor a lot more difficult. A lot of the techniques we discussed just won’t work. If we don’t have that central pipeline or there isn’t One and Only One Way to both dispatch and view logs, we’ll have to do a lot more lifting. We’ve seen a few ways that logs bypass this, but having a central pipeline should cover most of the bases.&lt;/p&gt;
    &lt;head rend="h3"&gt;1. Understand the data flow&lt;/head&gt;
    &lt;p&gt;With the foundation laid, the next best thing to do is to understand and chart out how secrets flow through your system. This is basically a Data Flow Diagram, and we’ll go through a fairly modest example.&lt;/p&gt;
    &lt;p&gt;On the left, we have users that visit some type of single-page web app. Requests and data flow through an application load balancer to several web application services running in containers. This is our core compute and where all the application code runs. Let’s assume that these are disparate microservices processing all types of data, some of which are considered secret. For the most sensitive data, they use KMS to encrypt and then store the ciphertext blobs in their respective database.&lt;/p&gt;
    &lt;p&gt;And then, applications use a standard logging library to emit to stdout, which gets shipped to CloudWatch and then forwarded to Datadog. That’s the final stop, and that’s where employees, devs, support staff, etc. can view them.&lt;/p&gt;
    &lt;p&gt;I highly recommend going through an exercise like this, because not only does it force you to understand the flows and boundaries of the system, if you spend time at each node and threat model it, you end up finding a bunch of unexpected ways and places that secrets make it into logs. For example&amp;amp;mldr;&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Front-end analytics! It turns out that secrets from things like form contents to session replays could end up getting sent to your user analytics platform.&lt;/item&gt;
      &lt;item&gt;And then what about our application load balancers? These ship their HTTP logs directly to CloudWatch, so we could be logging embedded secrets in URLs, and it’s totally bypassing our application tiers.&lt;/item&gt;
      &lt;item&gt;Last surprise: error monitoring! Let’s just say that some team wired up Sentry instead of DataDog for error monitoring, because of course they did, and now you have another stream of secrets in logs.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We could go further, and we haven’t even drilled into application architecture, but I think this is a good time to move from discovery to action.&lt;/p&gt;
    &lt;head rend="h3"&gt;2. Protect at chokepoints&lt;/head&gt;
    &lt;p&gt;The next step we want to take is to protect the chokepoints. And if some flow isn’t going through that chokepoint, like our rogue team that yeeted Sentry into prod, we fix it! We can get rid of Sentry and get that team onto the paved path of our logging pipeline.&lt;/p&gt;
    &lt;p&gt;We have a very clear chokepoint; a narrow path that most logs eventually flow through. Here’s where most of our lead bullets should go.&lt;/p&gt;
    &lt;p&gt;Here’s that chokepoint splayed out. I also added an upstream node to represent CI/CD, because that’s how code get into our apps. We can then put the bulk of our protective controls here on the critical path.&lt;/p&gt;
    &lt;p&gt;We can re-architect the app to use a single logging library and secure-by-default domain primitives. Then we could use those to build out and augment our static analysis, taint-checking, and unit tests. These give us a decent front-line defense for our logging pipeline.&lt;/p&gt;
    &lt;head rend="h3"&gt;3. Apply defense-in-depth&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;“Every preventative control should have a detective control at the same level and/or one level downstream in the architecture.” -Phil Venables, https://www.philvenables.com/post/defense-in-depth&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The third step is about adding depth to that defense, a concept we’re all familiar with. I really like how Phil Venables crystallizes what defense-in-depth means and I think he generally gives great advice. The idea is that our controls are not simply overlapping, but mutually supportive. Something’s always got your back.&lt;/p&gt;
    &lt;p&gt;Along this chokepoint we add our downstream components, in depth. Some are preventative, while some are detective.&lt;/p&gt;
    &lt;p&gt;We can add additional protections like tokenization and read-once objects. We can add the downstream tools like our custom log formatters, and employ various sensitive data scanners at different points. And then finally, we can educate and equip our team.&lt;/p&gt;
    &lt;p&gt;This is what defense-in-depth looks like to me, and I think this maximizes chances of success.&lt;/p&gt;
    &lt;head rend="h3"&gt;4. Plan for response and recovery&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Determine the scope&lt;/item&gt;
      &lt;item&gt;Restrict access&lt;/item&gt;
      &lt;item&gt;Stop the bleeding / ﬁx the source&lt;/item&gt;
      &lt;item&gt;Clean up all the places, e.g. indexes&lt;/item&gt;
      &lt;item&gt;Restore access&lt;/item&gt;
      &lt;item&gt;Do a post-mortem&lt;/item&gt;
      &lt;item&gt;Make it ~impossible to happen again&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But, of course, if we do miss or if we manage to only detect vs. prevent, we should be prepared for response and recovery. You already know how to respond to incidents like this, so I won’t add much here, other than making sure you’re sticking to a playbook in the right order, pulling levers to restrict and restore access while you’re responding, as well as thinking about all the weird places secrets might persist in logs, like indexes.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;And that’s it. This is the culmination of our strategy, our work, and about 30 some minutes of blabber.&lt;/p&gt;
    &lt;p&gt;With a solid foundation and understanding of our data flows, we protected our chokepoints in-depth and kept secrets out of logs. We’ve also introduced a lot of other strong primitives that materially improve our security program. So is that it? Is the job done?&lt;/p&gt;
    &lt;p&gt;Well, no, because the data team wired up some ETL jobs that are now spewing secrets into data lake logs, because of course they did.&lt;/p&gt;
    &lt;p&gt;Like most things in security, the job often isn’t ever done. But we have the understanding, the tools, and a strategy to fight the next fight. Keeping secrets out of logs is in your hands.&lt;/p&gt;
    &lt;p&gt;*me&lt;/p&gt;
    &lt;p&gt;If you liked what you heard, or if you hated it, I’d love to hear your story. Please, reach out! Thanks! ✌️&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45160774</guid></item><item><title>Apple's A17 Pro Chip Ships with Hardware Flaw</title><link>https://github.com/JGoyd/Apple-Silicon-A17-Flaw</link><description>&lt;doc fingerprint="c5f24b506ffa1fb1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;A17 Pro Chip Hardware Flaw: Shared I²C Bus Between Secure Enclave and Digitizer Causes Critical System Instability&lt;/head&gt;
    &lt;p&gt;This report documents a critical silicon-level hardware flaw in Apple’s A17 Pro (D84AP) chip used in the iPhone 15 Pro Max. The flaw involves a shared I²C4 bus connecting both the Secure Enclave Processor (SPU) and the digitizer controller. If the shared bus experiences electrical degradation or instability, both systems fail simultaneously—leading to early boot instability and loss of core functionality.&lt;/p&gt;
    &lt;p&gt;During testing, the device temporarily entered an unstable state, where the Secure Enclave failed to initialize and the digitizer reported invalid data. Although the device recovered and remained operable, the associated rose log pruning during failure cleared most traces before diagnostic data could be captured.&lt;/p&gt;
    &lt;p&gt;Log Evidence: https://archive.org/details/a-17-flaw-log-evidence&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Device: iPhone 15 Pro Max&lt;/item&gt;
      &lt;item&gt;Chipset: A17 Pro (D84AP, TSMC 3nm)&lt;/item&gt;
      &lt;item&gt;iBoot Version: 11881.80.57&lt;/item&gt;
      &lt;item&gt;Issue Type: Hardware-level design flaw (shared I²C4 bus)&lt;/item&gt;
      &lt;item&gt;Discovery Date: September 3, 2025&lt;/item&gt;
      &lt;item&gt;Status: Confirmed in production hardware; unrecoverable via software mitigation&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Apple’s A17 Pro System-on-Chip design places the following critical components on the same I²C4 line:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Component&lt;/cell&gt;
        &lt;cell role="head"&gt;Role&lt;/cell&gt;
        &lt;cell role="head"&gt;Failure Impact&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Secure Enclave (SPU)&lt;/cell&gt;
        &lt;cell&gt;Handles cryptographic boot &amp;amp; Face ID&lt;/cell&gt;
        &lt;cell&gt;Stuck in SecureROM; cryptographic services unavailable&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Digitizer Controller&lt;/cell&gt;
        &lt;cell&gt;Manages gesture &amp;amp; touch input&lt;/cell&gt;
        &lt;cell&gt;Invalid transducer data; touchscreen unresponsive&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;There is no hardware-level fault isolation between these two subsystems. A failure or instability in I²C4 causes both to malfunction during early boot stages.&lt;/p&gt;
    &lt;p&gt;The issue consistently occurs following a hardware-level power dropout or battery reconnection. Upon reboot:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;SPU becomes locked in SecureROM due to failed EEPROM communication.&lt;/item&gt;
      &lt;item&gt;Digitizer controller fails to report valid transducer data.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example serial log output:&lt;/p&gt;
    &lt;code&gt;AppleSPU::_handleReadyReport, serviceName (arc-eeprom-i2c)
Couldn't alloc class "AppleSPULogDriver"
IOHIDEventDriver: Invalid digitizer transducer
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Security Subsystems Affected: SPU fails to boot securely; biometric authentication, encryption, and keychain access are temporarily unavailable.&lt;/item&gt;
      &lt;item&gt;Input Subsystems Affected: Digitizer reports invalid data; touchscreen input is non-functional during failure events.&lt;/item&gt;
      &lt;item&gt;Forensic Blind Spot: Rose logging rotates and prunes entries during the incident, erasing most evidence before sysdiagnose capture.&lt;/item&gt;
      &lt;item&gt;Production Relevance: Observed in an untampered retail iPhone 15 Pro Max under standard test conditions.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Flag affected units internally for analysis; hardware recall may be necessary depending on production prevalence.&lt;/item&gt;
      &lt;item&gt;Conduct I²C4 electrical integrity tests on production samples.&lt;/item&gt;
      &lt;item&gt;Isolate hardware-level fault signatures using sysdiagnose and serial logging.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Physically separate security and user input systems onto independent buses in future SoC designs.&lt;/item&gt;
      &lt;item&gt;Introduce redundant EEPROM access paths for SPU to ensure boot resilience.&lt;/item&gt;
      &lt;item&gt;Implement early fault detection in SecureROM to handle I²C bus failures more gracefully.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;During failures, rose logging rotates and prunes entries, potentially erasing forensic traces of the incident. This creates a diagnostic blind spot and raises questions about whether intentional fault injection could be used to mask tampering.&lt;/p&gt;
    &lt;p&gt;This is a high-severity, unpatchable design flaw in the A17 Pro architecture. A shared I²C4 bus between the Secure Enclave and digitizer introduces a single point of failure that undermines both device security and usability. The flaw is:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Reproducible under controlled test conditions&lt;/item&gt;
      &lt;item&gt;Independent of firmware or software state&lt;/item&gt;
      &lt;item&gt;Inherent to the physical chip layout&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;While devices remain operable after recovery, the overlap of critical subsystem failure and forensic log loss poses a significant risk to system reliability, trustworthiness, and post-incident analysis. Hardware-level redesign in future SoC generations (e.g., A18 and beyond) is required.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45160947</guid></item></channel></rss>