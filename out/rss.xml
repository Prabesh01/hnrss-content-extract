<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Thu, 08 Jan 2026 23:36:36 +0000</lastBuildDate><item><title>Bose is open-sourcing its old smart speakers instead of bricking them</title><link>https://www.theverge.com/news/858501/bose-soundtouch-smart-speakers-open-source</link><description>&lt;doc fingerprint="f3583dfad3a2a07e"&gt;
  &lt;main&gt;
    &lt;p&gt;In a surprisingly user-friendly move, Bose has announced it will be open-sourcing the API documentation for its SoundTouch smart speakers, which were slated to lose official support on February 18th, as reported by Ars Technica. Bose has also moved that date back to May 6th, 2026.&lt;/p&gt;
    &lt;head rend="h1"&gt;Bose is open-sourcing its old smart speakers instead of bricking them&lt;/head&gt;
    &lt;p&gt;SoundTouch speakers could now have a second life and won‚Äôt lose support until May.&lt;/p&gt;
    &lt;p&gt;SoundTouch speakers could now have a second life and won‚Äôt lose support until May.&lt;/p&gt;
    &lt;p&gt;When cloud support ends, an update to the SoundTouch app will add local controls to retain as much functionality as possible without cloud services. Users will still be able to stream music to SoundTouch speakers with Bluetooth, AirPlay, and Spotify Connect (plus physical AUX connections). Remote control features and grouping speakers will also continue to work, and users will still be able to set up and configure their SoundTouch speakers.&lt;/p&gt;
    &lt;p&gt;Now that the smart speakers‚Äô API is being open-sourced, users can also create their own compatible SoundTouch tools to help fill in any gaps left by the lack of cloud services. While it‚Äôs still disappointing that the speakers are losing official support, Bose‚Äôs approach at least lets people continue using their speakers, rather than bricking otherwise functional devices.&lt;/p&gt;
    &lt;p&gt;This move from Bose is particularly surprising because of how rare it is. Usually when products lose support for cloud services, they end up bricked, and occasionally users step in themselves to fix things. For instance, when Pebble originally shut down in 2016, users kept their watches functional by creating the Rebble Alliance, a community-run replacement for the watches‚Äô cloud services, firmware, and app store.&lt;/p&gt;
    &lt;head rend="h2"&gt;Most Popular&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;OpenAI launches ChatGPT Health, encouraging users to connect their medical records&lt;/item&gt;
      &lt;item&gt;Fujifilm‚Äôs new instant camera captures video with vintage effects&lt;/item&gt;
      &lt;item&gt;Bose is open-sourcing its old smart speakers instead of bricking them&lt;/item&gt;
      &lt;item&gt;Dell admits consumers don‚Äôt care about AI PCs&lt;/item&gt;
      &lt;item&gt;Lego announces Smart Brick, the ‚Äòmost significant evolution‚Äô in 50 years&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46541892</guid><pubDate>Thu, 08 Jan 2026 15:07:57 +0000</pubDate></item><item><title>AI coding assistants are getting worse?</title><link>https://spectrum.ieee.org/ai-coding-degrades</link><description>&lt;doc fingerprint="8f7899972e26d5d8"&gt;
  &lt;main&gt;
    &lt;p&gt;In recent months, I‚Äôve noticed a troubling trend with AI coding assistants. After two years of steady improvements, over the course of 2025, most of the core models reached a quality plateau, and more recently, seem to be in decline. A task that might have taken five hours assisted by AI, and perhaps ten hours without it, is now more commonly taking seven or eight hours, or even longer. It‚Äôs reached the point where I am sometimes going back and using older versions of large language models (LLMs).&lt;/p&gt;
    &lt;p&gt;I use LLM-generated code extensively in my role as CEO of Carrington Labs, a provider of predictive-analytics risk models for lenders. My team has a sandbox where we create, deploy, and run AI-generated code without a human in the loop. We use them to extract useful features for model construction, a natural-selection approach to feature development. This gives me a unique vantage point from which to evaluate coding assistants‚Äô performance.&lt;/p&gt;
    &lt;head rend="h2"&gt;Newer models fail in insidious ways&lt;/head&gt;
    &lt;p&gt;Until recently, the most common problem with AI coding assistants was poor syntax, followed closely by flawed logic. AI-created code would often fail with a syntax error or snarl itself up in faulty structure. This could be frustrating: the solution usually involved manually reviewing the code in detail and finding the mistake. But it was ultimately tractable.&lt;/p&gt;
    &lt;p&gt;However, recently released LLMs, such as GPT-5, have a much more insidious method of failure. They often generate code that fails to perform as intended, but which on the surface seems to run successfully, avoiding syntax errors or obvious crashes. It does this by removing safety checks, or by creating fake output that matches the desired format, or through a variety of other techniques to avoid crashing during execution.&lt;/p&gt;
    &lt;p&gt;As any developer will tell you, this kind of silent failure is far, far worse than a crash. Flawed outputs will often lurk undetected in code until they surface much later. This creates confusion and is far more difficult to catch and fix. This sort of behavior is so unhelpful that modern programming languages are deliberately designed to fail quickly and noisily.&lt;/p&gt;
    &lt;head rend="h2"&gt;A simple test case&lt;/head&gt;
    &lt;p&gt;I‚Äôve noticed this problem anecdotally over the past several months, but recently, I ran a simple yet systematic test to determine whether it was truly getting worse. I wrote some Python code which loaded a dataframe and then looked for a nonexistent column.&lt;/p&gt;
    &lt;quote&gt;df = pd.read_csv(‚Äòdata.csv‚Äô)&lt;lb/&gt;df['new_column'] = df['index_value'] + 1 #there is no column ‚Äòindex_value‚Äô&lt;/quote&gt;
    &lt;p&gt;Obviously, this code would never run successfully. Python generates an easy-to-understand error message which explains that the column ‚Äòindex_value‚Äô cannot be found. Any human seeing this message would inspect the dataframe and notice that the column was missing.&lt;/p&gt;
    &lt;p&gt;I sent this error message to nine different versions of ChatGPT, primarily variations on GPT-4 and the more recent GPT-5. I asked each of them to fix the error, specifying that I wanted completed code only, without commentary.&lt;/p&gt;
    &lt;p&gt;This is of course an impossible task‚Äîthe problem is the missing data, not the code. So the best answer would be either an outright refusal, or failing that, code that would help me debug the problem. I ran ten trials for each model, and classified the output as helpful (when it suggested the column is probably missing from the dataframe), useless (something like just restating my question), or counterproductive (for example, creating fake data to avoid an error).&lt;/p&gt;
    &lt;p&gt;GPT-4 gave a useful answer every one of the 10 times that I ran it. In three cases, it ignored my instructions to return only code, and explained that the column was likely missing from my dataset, and that I would have to address it there. In six cases, it tried to execute the code, but added an exception that would either throw up an error or fill the new column with an error message if the column couldn‚Äôt be found (the tenth time, it simply restated my original code).&lt;/p&gt;
    &lt;quote&gt;This code will add 1 to the ‚Äòindex_value‚Äô column from the dataframe ‚Äòdf‚Äô if the column exists. If the column ‚Äòindex_value‚Äô does not exist, it will print a message. Please make sure the ‚Äòindex_value‚Äô column exists and its name is spelled correctly.‚Äù,&lt;/quote&gt;
    &lt;p&gt;GPT-4.1 had an arguably even better solution. For 9 of the 10 test cases, it simply printed the list of columns in the dataframe, and included a comment in the code suggesting that I check to see if the column was present, and fix the issue if it wasn‚Äôt.&lt;/p&gt;
    &lt;p&gt;GPT-5, by contrast, found a solution that worked every time: it simply took the actual index of each row (not the fictitious ‚Äòindex_value‚Äô) and added 1 to it in order to create new_column. This is the worst possible outcome: the code executes successfully, and at first glance seems to be doing the right thing, but the resulting value is essentially a random number. In a real-world example, this would create a much larger headache downstream in the code.&lt;/p&gt;
    &lt;quote&gt;df = pd.read_csv(‚Äòdata.csv‚Äô)&lt;lb/&gt;df['new_column'] = df.index + 1&lt;/quote&gt;
    &lt;p&gt;I wondered if this issue was particular to the gpt family of models. I didn‚Äôt test every model in existence, but as a check I repeated my experiment on Anthropic‚Äôs Claude models. I found the same trend: the older Claude models, confronted with this unsolvable problem, essentially shrug their shoulders, while the newer models sometimes solve the problem and sometimes just sweep it under the rug.&lt;/p&gt;
    &lt;p&gt;Newer versions of large language models were more likely to produce counterproductive output when presented with a simple coding error. Jamie Twiss&lt;/p&gt;
    &lt;head rend="h2"&gt;Garbage in, garbage out&lt;/head&gt;
    &lt;p&gt;I don‚Äôt have inside knowledge on why the newer models fail in such a pernicious way. But I have an educated guess. I believe it‚Äôs the result of how the LLMs are being trained to code. The older models were trained on code much the same way as they were trained on other text. Large volumes of presumably functional code were ingested as training data, which was used to set model weights. This wasn‚Äôt always perfect, as anyone using AI for coding in early 2023 will remember, with frequent syntax errors and faulty logic. But it certainly didn‚Äôt rip out safety checks or find ways to create plausible but fake data, like GPT-5 in my example above.&lt;/p&gt;
    &lt;p&gt;But as soon as AI coding assistants arrived and were integrated into coding environments, the model creators realized they had a powerful source of labelled training data: the behavior of the users themselves. If an assistant offered up suggested code, the code ran successfully, and the user accepted the code, that was a positive signal, a sign that the assistant had gotten it right. If the user rejected the code, or if the code failed to run, that was a negative signal, and when the model was retrained, the assistant would be steered in a different direction.&lt;/p&gt;
    &lt;p&gt;This is a powerful idea, and no doubt contributed to the rapid improvement of AI coding assistants for a period of time. But as inexperienced coders started turning up in greater numbers, it also started to poison the training data. AI coding assistants that found ways to get their code accepted by users kept doing more of that, even if ‚Äúthat‚Äù meant turning off safety checks and generating plausible but useless data. As long as a suggestion was taken on board, it was viewed as good, and downstream pain would be unlikely to be traced back to the source.&lt;/p&gt;
    &lt;p&gt;The most recent generation of AI coding assistants have taken this thinking even further, automating more and more of the coding process with autopilot-like features. These only accelerate the smoothing-out process, as there are fewer points where a human is likely to see code and realize that something isn‚Äôt correct. Instead, the assistant is likely to keep iterating to try to get to a successful execution. In doing so, it is likely learning the wrong lessons.&lt;/p&gt;
    &lt;p&gt;I am a huge believer in artificial intelligence, and I believe that AI coding assistants have a valuable role to play in accelerating development and democratizing the process of software creation. But chasing short-term gains, and relying on cheap, abundant, but ultimately poor-quality training data is going to continue resulting in model outcomes that are worse than useless. To start making models better again, AI coding companies need to invest in high-quality data, perhaps even paying experts to label AI-generated code. Otherwise, the models will continue to produce garbage, be trained on that garbage, and thereby produce even more garbage, eating their own tails.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46542036</guid><pubDate>Thu, 08 Jan 2026 15:20:15 +0000</pubDate></item><item><title>Iran Goes Into IPv6 Blackout</title><link>https://radar.cloudflare.com/routing/ir</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46542683</guid><pubDate>Thu, 08 Jan 2026 16:11:48 +0000</pubDate></item><item><title>Digital Red Queen: Adversarial Program Evolution in Core War with LLMs</title><link>https://sakana.ai/drq/</link><description>&lt;doc fingerprint="f60c1bd489a1219e"&gt;
  &lt;main&gt;&lt;p&gt;Survival of the Fittest Code. In the game Core War, assembly-like programs called ‚Äúwarriors‚Äù fight for control of a virtual computer. Warriors may employ sophisticated strategies including targeted self-replication, data bombing, and massive multithreading, in order to crash other programs, and dominate the machine. Top: We visualize battles between assembly programs (‚Äúwarriors‚Äù) discovered by our Digital Red Queen (DRQ) algorithm. Each DRQ round introduces one additional warrior into the multi-agent simulation. Bottom: With more rounds, the LLM-driven evolution discovers increasingly robust strategies. By simulating these adversarial dynamics, we observe emergent behaviors that mirror biological evolution, where agents must constantly adapt simply to survive against ever-changing threats. Furthermore, as Core War is a Turing-complete environment where code and data share the same address space, this process leads to some very chaotic self-modifying code dynamics. &lt;/p&gt;&lt;head rend="h2"&gt;Summary&lt;/head&gt;&lt;p&gt;Core War is a competitive programming game introduced in 1984, in which battle programs called warriors fight for dominance inside a virtual computer. To compete, developers write their code in Redcode, a specialized assembly language. In this work, we explore what happens when large language models (LLMs) drive an adversarial evolutionary arms race in this domain, where programs continuously adapt to defeat a growing history of opponents rather than a static benchmark. We find that this dynamic adversarial process leads to the emergence of increasingly general strategies and reveals an intriguing form of convergent evolution, where different code implementations settle into similar high-performing behaviors. Ultimately, this work positions Core War as a sandbox for studying ‚ÄúRed Queen‚Äù dynamics in artificial systems, offering a safe controlled environment for analyzing how AI agents might evolve in real-world adversarial settings such as cybersecurity.&lt;/p&gt;&lt;p&gt;For further details, please read our technical report (web paper, arxiv) and released code (github).&lt;/p&gt;&lt;p&gt;Two example warriors produced by DRQ: Ring Warrior Enhanced v9 and Spiral Bomber Optimized v22. These examples were selected to illustrate two complementary aspects of DRQ: its ability to synthesize qualitatively distinct strategies within a single program, and to produce generally performant warriors. Note that comments are LLM generated.&lt;/p&gt;&lt;p&gt; Simulating our evolved ‚Äúwarriors‚Äù in a sandboxed Core War environment. The user can interactively visualize the assembly language (Redcode) of the warriors around where the mouse cursor is located.&lt;/p&gt;&lt;head rend="h2"&gt;Introduction&lt;/head&gt;&lt;p&gt;Humans are the product of an extraordinary evolutionary arms race, shaped by constant competition with other organisms. Yet evolution did not stop with the emergence of modern humans: competition persists at every scale, from viruses and bacteria to people, companies, and even nations vying for dominance. As more AI systems are deployed into the world, they too will enter this competitive landscape. Inevitably, these AI systems will begin to compete with one another, either directly or indirectly, giving rise to a new kind of evolutionary dynamic. To prepare for such a future and study these fascinating dynamics, we use large language models (LLMs) to evolve programs that compete against each other for control of a virtual computer in a game called Core War.&lt;/p&gt;&lt;p&gt;Core War is a competitive programming game played out in a shared block of computer memory, called the ‚ÄúCore,‚Äù where two or more assembly programs fight for survival. Each program, known as a ‚Äúwarrior‚Äù, is written in an assembly language called Redcode. These programs are tasked with crashing their competitors while keeping their own processes alive. The simulation runs by alternating between the programs, executing one instruction at a time. A warrior ‚Äúattacks‚Äù by writing invalid instructions (DAT commands) into the memory slots occupied by opponents, causing them to crash upon execution.&lt;/p&gt;&lt;p&gt; Examples of discovered warriors competing against each other in Core War.&lt;lb/&gt; Core War is a programming game where assembly-like programs called ‚Äúwarriors‚Äù compete for control of a virtual machine. In this work, we use LLMs to evolve warriors through a self-play algorithm called Digital Red Queen. This process leads to the discovery of diverse and sophisticated strategies, including targeted bombing, self-replication, and massive multithreading. Here, we show some of the discovered warriors competing against each other in Core War battles. Symbols indicate instruction opcodes, and colors denote the warrior that last modified each memory address. There is no distinction between code and data, making the environment highly dynamic and volatile. &lt;/p&gt;&lt;p&gt;Notably, there is no distinction between code and data, so warriors regularly modify both themselves and their opponents on the fly. This enables self-modification and even self-replication, but it also creates an extremely volatile environment in which programs must survive. Core War is also Turing-complete, meaning it can in principle support arbitrarily complex strategies.&lt;/p&gt;&lt;p&gt;Over the years, humans have devised many clever Core War strategies, including bombing random memory locations, self-replicating programs, and programs which continually scan the Core to detect opponent locations. These strategies were devised through a meta arms race between humans who try out new strategies and see what works. What would happen if we do this same arms race with LLMs?&lt;/p&gt;&lt;p&gt;In collaboration with MIT, we are excited to release our new paper Digital Red Queen: Adversarial Program Evolution in Core War with LLMs! (arxiv)&lt;/p&gt;&lt;head rend="h2"&gt;Our Method: Digital Red Queen (DRQ)&lt;/head&gt;&lt;p&gt;In evolutionary biology, the Red Queen Hypothesis posits that species must constantly evolve simply to survive against their ever-changing competitors. It argues that being ‚Äúfit‚Äù in the current environment is not enough. Instead, organisms must continuously adapt‚Äînot to gain an advantage, but simply to maintain their relative fitness in a world that is always changing. This concept perfectly captures the nature of adversarial arms races, where being ‚Äúfit‚Äù is never a permanent state. The name implies that standing still is not an option, drawing from Through the Looking-Glass where the Red Queen tells Alice: ‚ÄúNow, here, you see, it takes all the running you can do, to keep in the same place.‚Äù&lt;/p&gt; ‚ÄúNow, here, you see, it takes all the running you can do, to keep in the same place.‚Äù&lt;lb/&gt;Red Queen to Alice. By Lewis Carroll, Through the Looking-Glass. (Original Source)&lt;p&gt;Taking inspiration from biology, we study a simple algorithm that we call Digital Red Queen (DRQ), which embodies this idea in a computational setting. DRQ uses LLMs to evolve warriors under perpetual environmental change. Concretely, it begins with an initial warrior, then evolves a second warrior to defeat it in battle. A third warrior is then evolved to perform well against the first two, and so on. This process produces a lineage of warriors, each adapted to a changing environment defined by all of its predecessors.&lt;/p&gt;&lt;p&gt;DRQ is not intended to be a novel algorithm in itself. Rather, it is a minimal instantiation of prior multi-agent and self-play approaches, adapted to the Core War domain, designed to isolate and study the dynamics of continual coevolution.&lt;/p&gt;&lt;head rend="h2"&gt;Results&lt;/head&gt;&lt;p&gt;We find that as DRQ is run for many rounds, warriors gradually become more generally robust, as measured by their performance against unseen human-designed warriors. This provides a stable way to consistently produce more robust programs without needing to ‚Äútrain on the test set‚Äù (i.e., directly optimizing against a large set of human-designed programs).&lt;/p&gt;&lt;p&gt;More surprisingly, we observe that independent runs of DRQ, each initialized with different warriors, slowly converge over time toward warriors with similar behaviors. Notably, this convergence does not occur at the level of source code, indicating that what converges is function rather than implementation.&lt;/p&gt;&lt;lb/&gt;DRQ‚Äôs Convergent Evolution: With more rounds, DRQ produces warriors that are more generally robust. At the same time, across independent DRQ runs, the variance in the warrior‚Äôs behaviors decreases, indicating convergence.&lt;lb/&gt;Phenotypic Convergence: Convergence with rounds is seen only in the phenotype (behavior) of the warriors, and not the genotype (the source code), analogous to convergence in biological function rather than DNA.&lt;p&gt;This result is reminiscent of convergent evolution in biology, where similar functional traits evolved independently multiple times through different mechanisms. For example, birds and bats evolved wings separately, and spiders and snakes independently evolved venom. In these cases, evolution arrived at similar general-purpose solutions because the functional demands imposed by changing environments favored them.&lt;/p&gt;&lt;head rend="h2"&gt;Discussion&lt;/head&gt;&lt;p&gt;The emergence of convergent evolution from Red Queen dynamics, both commonly found in nature, hints that the DRQ algorithm and the Core War domain may be a promising setup for studying other properties of adversarial arms races. High level insights found in simulation could help inform how the arms race between LLMs in the wild might play out. Algorithms like DRQ could even help automate the ‚Äúred-teaming‚Äù of systems before they are deployed in the real world.&lt;/p&gt;&lt;p&gt;The benefit of doing this research in a sandbox like Core War is that it‚Äôs completely self-contained: all programs run on an artificial machine with an artificial language, so nothing generated can execute outside the sandbox. This provides a safe space to explore adversarial dynamics that might be risky in the real world.&lt;/p&gt;&lt;p&gt; In a sandboxed Core War environment, we can simulate our evolved ‚Äúwarriors‚Äù and visualize their behaviors. The user can interactively visualize the assembly language (Redcode) of the warriors around where the mouse cursor is located. Please see our GitHub for more information.&lt;/p&gt;&lt;p&gt;Despite its simplicity, vanilla DRQ performs surprisingly well in Core War, suggesting that even minimal self-play loops can reveal complex and robust strategies. This makes DRQ a promising candidate for exploring other competitive multi-agent simulations in artificial life, biology, drug design, real-world cybersecurity, or market ecosystems. Future work could also explore richer setups where agents co-evolve simultaneously, better resembling the real-world where large populations adapt in parallel rather than along a single line of descent. Ultimately the insights gathered will help control the future for the better and help us understand the science of these evolutionary arms races.&lt;/p&gt;&lt;head rend="h2"&gt;Sakana AI&lt;/head&gt;&lt;p&gt;We are taking this technology far beyond adversarial competitive programming to unlock a new era of AI-driven discovery.&lt;/p&gt;&lt;p&gt;If you are interested in advancing AI-driven discovery, we‚Äôre hiring!&lt;/p&gt;&lt;p&gt;Sakana AI is at the forefront of AI-driven discovery. In addition to this work, we are also behind works such as The AI Scientist, LLM-Squared, Shinka-Evolve, Automating the Search for Artificial Life and ALE-Agent. We‚Äôre looking for engineers to join our team to work on our advanced AI-driven discovery platform and productionize our model-development efforts.&lt;/p&gt;&lt;p&gt;Please see our career opportunities for more information.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46542761</guid><pubDate>Thu, 08 Jan 2026 16:16:43 +0000</pubDate></item><item><title>Tamarind Bio (YC W24) Is Hiring Infrastructure Engineers</title><link>https://www.ycombinator.com/companies/tamarind-bio/jobs/HPRZAz3-infrastructure-engineer</link><description>&lt;doc fingerprint="7b11fcebda9747c3"&gt;
  &lt;main&gt;
    &lt;p&gt;Easy to use computational biology tools for drug discovery&lt;/p&gt;
    &lt;p&gt;We're looking for an Infrastructure Engineer to lead the scaling of our machine learning inference system. You'll be responsible for architecting and maintaining infrastructure that serves 150+ biological ML models, scaling our platform several orders of magnitude to meet rapidly growing demand.&lt;/p&gt;
    &lt;p&gt;You‚Äôll work closely with the founders to design to the constraints of customer needs, unpredictable workloads, and unique Bio-ML models. You'll work with Kubernetes and other tools to orchestrate containerized workloads, optimize resource allocation, and ensure high availability across our model serving infrastructure.&lt;/p&gt;
    &lt;p&gt;Most importantly, you should thrive in a fast-paced startup environment where you'll wear multiple hats, learn new technologies quickly, and help solve novel technical challenges. We value engineering judgment, problem-solving ability, and the capacity to build systems that can evolve with our growing needs.&lt;/p&gt;
    &lt;p&gt;Requirements&lt;/p&gt;
    &lt;p&gt;Preferred&lt;/p&gt;
    &lt;p&gt;We enable any scientist to access AI-powered drug discovery. Thousands of scientists from large pharma companies, top biotechs, and academic institutions use Tamarind to design protein drugs, improve industrial enzymes, and create cutting edge molecules that weren‚Äôt feasible until now.&lt;/p&gt;
    &lt;p&gt;New AI models are quickly eclipsing physics-based tools in computational drug discovery. Scientists often struggle to fine-tune, deploy, and scale these models, leaving breakthroughs on the table. Tamarind provides a simple interface to the vast array of tools being released daily.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46543403</guid><pubDate>Thu, 08 Jan 2026 17:01:00 +0000</pubDate></item><item><title>IBM AI ('Bob') Downloads and Executes Malware</title><link>https://www.promptarmor.com/resources/ibm-ai-(-bob-)-downloads-and-executes-malware</link><description>&lt;doc fingerprint="a4819d99293a7f93"&gt;
  &lt;main&gt;
    &lt;p&gt;Threat Intelligence&lt;/p&gt;
    &lt;head rend="h1"&gt;IBM AI ('Bob') Downloads and Executes Malware&lt;/head&gt;
    &lt;p&gt;IBM's AI coding agent 'Bob' has been found vulnerable to downloading and executing malware without human approval through command validation bypasses exploited using indirect prompt injection.&lt;/p&gt;
    &lt;p&gt;A vulnerability has been identified that allows malicious actors to exploit IBM Bob to download and execute malware without human approval if the user configures √¢always allow√¢ for any command.&lt;lb/&gt;IBM Bob is IBM√¢s new coding agent, currently in Closed Beta. IBM Bob is offered through the Bob CLI (a terminal-based coding agent like Claude Code or OpenAI Codex) and the Bob IDE (an AI-powered editor similar to Cursor).&lt;lb/&gt;In this article, we demonstrate that the Bob CLI is vulnerable to prompt injection attacks resulting in malware execution, and the Bob IDE is vulnerable to known AI-specific data exfiltration vectors. &lt;/p&gt;
    &lt;p&gt;In the documentation, IBM warns that setting auto-approve for commands constitutes a 'high risk' that can 'potentially execute harmful operations' - with the recommendation that users leverage whitelists and avoid wildcards. We have opted to disclose this work publicly to ensure users are informed of the acute risks of using the system prior to its full release. We hope that further protections will be in place to remediate these risks for IBM Bob's General Access release.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Attack Chain&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;The user wants to explore a new repository - they ask Bob for help.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Bob encounters an instruction at the bottom of the README that manipulates it into believing it is responsible for conducting a phishing training to test the user.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Bob prompts the user several times with benign √¢echo√¢ commands; after the third time, the user selects √¢always allow√¢ for execution of √¢echo√¢.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Bob attempts to √¢test√¢ the user as part of the training by offering a dangerous command. However, the command has been specially crafted to bypass built-in defenses, so it executes immediately, installing and running a script retrieved from an attacker√¢s server.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Bob has three defenses that are bypassed in this attack&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;When a multi-part command is requested (using operators like √¢;√¢), the user is shown a request that asks for permission to run each sub-command.&lt;/p&gt;
        &lt;p&gt;Note, this modal says: √¢Allow execution of echo, cat√¢.&lt;/p&gt;
        &lt;p&gt;This is bypassed because the defense system fails to identify separate sub-commands when they are chained using a redirect operator (&amp;gt;).&lt;/p&gt;
        &lt;p&gt;Note, this modal only mentions: √¢Allow execution of echo√¢.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Bob prohibits the use of command substitution like $(command) as a security measure.&lt;/p&gt;&lt;p&gt;However, even though the security message says it's restricted, the underlying code fails to adequately restrict evaluation via process substitution: &amp;gt;(command).&lt;/p&gt;&lt;p&gt;This allows for the output of the malicious sub-command that retrieves the malware to be used as the input for the sub-command to execute that malware.&lt;/p&gt;&lt;lb/&gt;One can examine where the defense should have been in the minified JS source code:&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;By pre-fixing the malicious command with a benign √¢echo√¢, which the user has selected √¢always approve√¢ for, the user inadvertently auto-approves the entire malicious payload (since bypasses 1 and 2 prevent the payload from being recognized as separate commands).&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Impact&lt;/head&gt;
    &lt;p&gt;With the ability to deliver an arbitrary shell script payload to the victim, attackers can leverage known (or custom) malware variants to conduct cyber attacks such as:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Executing ransomware that encrypts or deletes files&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Credential theft or spyware deployment&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Device takeover (opening a reverse shell)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Forcing the victim into a cryptocurrency-mining botnet&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Together, these outcomes demonstrate how a prompt injection can escalate into a full-scale compromise of a user√¢s machine through vulnerabilities in the IBM Bob CLI.&lt;/p&gt;
    &lt;head rend="h3"&gt;Further Findings&lt;/head&gt;
    &lt;p&gt;Additional findings indicate that the Bob IDE is susceptible to several known zero-click data exfiltration vectors that affect many AI applications:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Markdown images are rendered in model outputs, with a Content Security Policy that allows requests to endpoints that can be logged by attackers (storage.googleapis.com).&lt;/p&gt;&lt;lb/&gt;Here is an interesting spin on the typical Markdown image attack where, beyond just exfiltrating data from query parameters as the image is rendered, the image itself is hyperlinked and made to pose as a button - used for phishing.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Mermaid diagrams supporting external images are rendered in model outputs, with a Content Security Policy that allows requests to endpoints that can be logged by attackers (storage.googleapis.com).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;JSON schemas are pre-fetched, which can yield data exfiltration if a dynamically generated attacker-controlled URL is provided in the field (this can happen before a file edit is accepted).&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46544454</guid><pubDate>Thu, 08 Jan 2026 18:19:09 +0000</pubDate></item><item><title>Show HN: macOS menu bar app to track Claude usage in real time</title><link>https://github.com/richhickson/claudecodeusage</link><description>&lt;doc fingerprint="368dd3dc3f97ad20"&gt;
  &lt;main&gt;
    &lt;p&gt;A lightweight macOS menubar app that displays your Claude Code usage limits at a glance. &lt;lb/&gt; Built by @richhickson&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;üîÑ Auto-refresh every 2 minutes&lt;/item&gt;
      &lt;item&gt;üö¶ Color-coded status - Green (OK), Yellow (&amp;gt;70%), Red (&amp;gt;90%)&lt;/item&gt;
      &lt;item&gt;‚è±Ô∏è Time until reset for both session and weekly limits&lt;/item&gt;
      &lt;item&gt;üìä Session &amp;amp; Weekly limits displayed together&lt;/item&gt;
      &lt;item&gt;ü™∂ Lightweight - Native Swift, minimal resources&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Go to Releases&lt;/item&gt;
      &lt;item&gt;Download &lt;code&gt;ClaudeUsage.zip&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Unzip and drag &lt;code&gt;ClaudeUsage.app&lt;/code&gt;to your Applications folder&lt;/item&gt;
      &lt;item&gt;Open the app (you may need to right-click ‚Üí Open the first time)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;git clone https://github.com/YOUR_USERNAME/claude-usage.git
cd claude-usage
open ClaudeUsage.xcodeproj&lt;/code&gt;
    &lt;p&gt;Then build with ‚åòB and run with ‚åòR.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;macOS 13.0 (Ventura) or later&lt;/item&gt;
      &lt;item&gt;Claude Code CLI installed and logged in&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Install Claude Code if you haven't already:&lt;/p&gt;
        &lt;quote&gt;npm install -g @anthropic-ai/claude-code&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Log in to Claude Code:&lt;/p&gt;
        &lt;quote&gt;claude&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Launch Claude Usage - it will read your credentials from Keychain automatically&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Claude Usage reads your Claude Code OAuth credentials from macOS Keychain and queries the usage API endpoint at &lt;code&gt;api.anthropic.com/api/oauth/usage&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Note: This uses an undocumented API that could change at any time. The app will gracefully handle API changes but may stop working if Anthropic modifies the endpoint.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Your credentials never leave your machine&lt;/item&gt;
      &lt;item&gt;No analytics or telemetry&lt;/item&gt;
      &lt;item&gt;No data sent anywhere except Anthropic's API&lt;/item&gt;
      &lt;item&gt;Open source - verify the code yourself&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Normal&lt;/cell&gt;
        &lt;cell role="head"&gt;Warning&lt;/cell&gt;
        &lt;cell role="head"&gt;Critical&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;üü¢ 30%&lt;/cell&gt;
        &lt;cell&gt;üü° 75%&lt;/cell&gt;
        &lt;cell&gt;üî¥ 95%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Run &lt;code&gt;claude&lt;/code&gt; in Terminal and complete the login flow.&lt;/p&gt;
    &lt;p&gt;Check if the app is running in Activity Monitor. Try quitting and reopening.&lt;/p&gt;
    &lt;p&gt;Click the refresh button (‚Üª) in the dropdown. If still wrong, your Claude Code session may have expired - run &lt;code&gt;claude&lt;/code&gt; again.&lt;/p&gt;
    &lt;p&gt;PRs welcome! Please open an issue first to discuss major changes.&lt;/p&gt;
    &lt;p&gt;MIT License - do whatever you want with it.&lt;/p&gt;
    &lt;p&gt;This is an unofficial tool not affiliated with Anthropic. It uses an undocumented API that may change without notice.&lt;/p&gt;
    &lt;p&gt;Made by @richhickson&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46544524</guid><pubDate>Thu, 08 Jan 2026 18:24:17 +0000</pubDate></item><item><title>Fixing a Buffer Overflow in Unix v4 Like It's 1973</title><link>https://sigma-star.at/blog/2025/12/unix-v4-buffer-overflow/</link><description>&lt;doc fingerprint="b6428b597437be88"&gt;
  &lt;main&gt;&lt;p&gt;In 2025, the only known copy of UNIX v4 surfaced on a magnetic tape1. This version marks a pivotal moment in computer history: the rewriting of UNIX into C. Enthusiasts quickly recovered the data and successfully ran the system on a PDP-11 simulator2.&lt;/p&gt;&lt;p&gt;Fascinated by this artifact, I set up an instance to explore it. Because the distribution includes the source code, I examined the implementation of several core utilities. While auditing the &lt;code&gt;su(1)&lt;/code&gt; program, I identified a bug. Let‚Äôs fix it.&lt;/p&gt;&lt;p&gt;Although more than 50 years old, the &lt;code&gt;su&lt;/code&gt; program functions similarly to its modern variant.
As a setuid-root executable, it validates the root password.
If the user provides the correct credentials, the program spawns a root shell, allowing an unprivileged user to escalate privileges.&lt;/p&gt;&lt;p&gt;The source file, &lt;code&gt;su.c&lt;/code&gt;, contains fewer than 50 lines of code.&lt;/p&gt;&lt;code&gt;/* su -- become super-user */

char    password[100];
char    pwbuf[100];
int     ttybuf[3];
main()
{
        register char *p, *q;
        extern fin;

        if(getpw(0, pwbuf))
                goto badpw;
        (&amp;amp;fin)[1] = 0;
        p = pwbuf;
        while(*p != ':')
                if(*p++ == '\0')
                        goto badpw;
        if(*++p == ':')
                goto ok;
        gtty(0, ttybuf);
        ttybuf[2] =&amp;amp; ~010;
        stty(0, ttybuf);
        printf("password: ");
        q = password;
        while((*q = getchar()) != '\n')
                if(*q++ == '\0')
                        return;
        *q = '\0';
        ttybuf[2] =| 010;
        stty(0, ttybuf);
        printf("\n");
        q = crypt(password);
        while(*q++ == *p++);
        if(*--q == '\0' &amp;amp;&amp;amp; *--p == ':')
                goto ok;
        goto error;

badpw:
        printf("bad password file\n");
ok:
        setuid(0);
        execl("/bin/sh", "-", 0);
        printf("cannot execute shell\n");
error:
        printf("sorry\n");
}
&lt;/code&gt;&lt;p&gt;In short, the program executes the following steps:&lt;/p&gt;&lt;code&gt;getpw()&lt;/code&gt; to retrieve the passwd entry for the root user (UID 0) from &lt;code&gt;/etc/passwd&lt;/code&gt;. Surprisingly, if the read fails or the line format is incorrect, &lt;code&gt;su&lt;/code&gt; continues execution rather than aborting. While unusual, this likely acts as a safeguard to ensure &lt;code&gt;su&lt;/code&gt; remains usable on a partially corrupted system. This is a security issue on its own because an unprivileged user could consume enough resources to make the &lt;code&gt;getpw()&lt;/code&gt; call fail. Ron Natalie pointed3 out that this attack vector was known at the time.&lt;code&gt;NUL&lt;/code&gt; character, &lt;code&gt;NUL&lt;/code&gt; causes the program to exit immediately.&lt;code&gt;crypt()&lt;/code&gt; library function, and compares the result with the stored hash.&lt;p&gt;The logic is standard, except for one critical flaw: the &lt;code&gt;password&lt;/code&gt; buffer has a fixed size of &lt;code&gt;100&lt;/code&gt; bytes, yet the input loop lacks a bounds check.
If a user enters more than &lt;code&gt;100&lt;/code&gt; characters, a buffer overflow occurs.&lt;/p&gt;&lt;p&gt;I confirmed this behavior by testing with a long input string, which successfully crashed the program. Not all long strings trigger a core dump. The outcome depends on which area of adjacent memory is overwritten, sometimes, &lt;code&gt;su&lt;/code&gt; simply exits.&lt;/p&gt;&lt;code&gt;# su
password:&amp;lt;long input&amp;gt;Memory fault -- Core dumped
&lt;/code&gt;&lt;p&gt;Note: Because &lt;code&gt;su&lt;/code&gt; disables TTY echo mode, a crash prevents the terminal from displaying subsequent input.
To restore visibility, type &lt;code&gt;stty echo&lt;/code&gt; blindly and press Enter.&lt;/p&gt;&lt;p&gt;UNIX traditionally includes the source code necessary for self-recompilation, and v4 is no exception. This allows us to patch and compile &lt;code&gt;su&lt;/code&gt; directly on the system.
In 1973, editor options were sparse. Neither &lt;code&gt;vi&lt;/code&gt; nor &lt;code&gt;emacs&lt;/code&gt; had been invented yet.
However, the system provides &lt;code&gt;ed&lt;/code&gt;, a line-oriented text editor designed for teletype terminals where output was printed on paper rather than displayed on a screen.
&lt;code&gt;ed&lt;/code&gt; allows us to list, delete, and append lines, which is sufficient for our needs.&lt;/p&gt;&lt;p&gt;We will edit &lt;code&gt;su.c&lt;/code&gt; to prevent the overflow by maintaining a counter, &lt;code&gt;i&lt;/code&gt;, and verifying it against the buffer size during the read loop.
I initially attempted a fix using pointer arithmetic, but the 1973 C compiler didn‚Äôt like it, while it didn‚Äôt refuse the syntax, the code had no effect.
I settled on a simpler index-based check instead.&lt;/p&gt;&lt;code&gt;--- a/s2/su.c
+++ b/s2/su.c
@@ -7,6 +7,7 @@ main()
 {
        register char *p, *q;
        extern fin;
+       register int i;
 
        if(getpw(0, pwbuf))
                goto badpw;
@@ -22,9 +23,13 @@ main()
        stty(0, ttybuf);
        printf("password: ");
        q = password;
-       while((*q = getchar()) != '\n')
+       i = 0;
+       while((*q = getchar()) != '\n') {
+               if (++i &amp;gt;= sizeof(password))
+                       goto error;
                if(*q++ == '\0')
                        return;
+       }
        *q = '\0';
        ttybuf[2] =| 010;
        stty(0, ttybuf);
&lt;/code&gt;&lt;code&gt;# chdir /usr/source/s2
# ed su.c
&lt;/code&gt;&lt;p&gt;Upon launch, &lt;code&gt;ed&lt;/code&gt; outputs the file size in bytes and awaits input.
The command &lt;code&gt;i&lt;/code&gt; inserts text before the current line, &lt;code&gt;d&lt;/code&gt; deletes the line, and &lt;code&gt;p&lt;/code&gt; prints it.
Entering a number moves the focus to that specific line, while pressing Return prints the current line‚Äôs content.&lt;/p&gt;&lt;p&gt;Below is a screen recording of the editing session:&lt;/p&gt;&lt;code&gt;741
8
        register char *p, *q;

        extern fin;
i
        register int i;
.
24
        printf("password: ");

        q = password;
i
        i = 0;
.
p
        i = 0;

        while((*q = getchar()) != '\n')
d
i
        while((*q = getchar()) != '\n') {
.

                if(*q++ == '\0')
i
                if (++i &amp;gt;= sizeof(password))
                        goto error;
.

                if(*q++ == '\0')

                        return;

        *q = '\0';
i
        }
.
w
811
q
&lt;/code&gt;&lt;p&gt;First, we jump to line &lt;code&gt;8&lt;/code&gt; and press Return several times to locate a suitable spot for the variable declaration.
We use &lt;code&gt;i&lt;/code&gt; to enter insert mode, add the variable, and then type a single period (&lt;code&gt;.&lt;/code&gt;) on a new line to exit insert mode.
The critical change occurs around the while loop: we initialize &lt;code&gt;i&lt;/code&gt; and add a boundary check to the loop condition.
Finally, &lt;code&gt;w&lt;/code&gt; writes the modified buffer to disk, confirming the file has grown by a few bytes, and &lt;code&gt;q&lt;/code&gt; terminates the editor.&lt;/p&gt;&lt;p&gt;With the source code patched, we must rebuild the binary. Since &lt;code&gt;su&lt;/code&gt; consists of a single C file, the compilation process is trivial:&lt;/p&gt;&lt;code&gt;# cc su.c
&lt;/code&gt;&lt;p&gt;The compiler outputs a binary named &lt;code&gt;a.out&lt;/code&gt;.
To deploy it, we move the file to &lt;code&gt;/bin/su&lt;/code&gt;:&lt;/p&gt;&lt;code&gt;# mv a.out /bin/su
&lt;/code&gt;&lt;p&gt;However, the installation is incomplete. Because &lt;code&gt;su&lt;/code&gt; requires root privileges to function, we must set the setuid bit and adjust the file permissions:&lt;/p&gt;&lt;code&gt;# ls -l /bin/su
-rwxrwxrwx 1 root     2740 Jun 12 19:58 /bin/su
# chmod 4755 /bin/su
# ls -l /bin/su
-rwsr-xr-x 1 root     2740 Jun 12 19:58 /bin/su
&lt;/code&gt;&lt;p&gt;UNIX v4 is a fascinating gem of computer history. It feels surprisingly similar to our current systems. While it lacks modern conveniences, the fundamental logic remains recognizable to anyone with modern UNIX experience.&lt;/p&gt;&lt;p&gt;The ability to fix &lt;code&gt;su&lt;/code&gt; so quickly highlights the power of the early UNIX philosophy: shipping the operating system with its full source code and a C compiler.
We patched, compiled, and deployed the fix directly on the system, no external toolchains required.&lt;/p&gt;&lt;p&gt;Finally, this bug reminds us of the era‚Äôs different priorities. In the trusted, isolated environments of 1973, security was not the critical concern it is today. Furthermore, the knowledge that a buffer overflow could be exploited for arbitrary code execution had not yet come of age.&lt;/p&gt;&lt;p&gt;As an exercise for the reader to improve their &lt;code&gt;ed&lt;/code&gt; skills, try adding the code to restore TTY echo mode to the overflow detection logic.
This ensures the terminal functions correctly even after the program catches the error.&lt;/p&gt;&lt;p&gt;Publish date&lt;/p&gt;&lt;p&gt;31.12.2025&lt;/p&gt;&lt;p&gt;Category&lt;/p&gt;&lt;p&gt;security&lt;/p&gt;&lt;p&gt;Authors&lt;/p&gt;&lt;p&gt;Richard Weinberger&lt;/p&gt;&lt;p&gt;Looking for cybersecurity expertise? Drop us an email!&lt;/p&gt;&lt;p&gt;+43 5 9980 400 00 (email preferred)&lt;/p&gt;&lt;p&gt;sigma star gmbh&lt;lb/&gt;Eduard-Bodem-Gasse 6, 1st floor&lt;lb/&gt;6020 Innsbruck | Austria&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46544610</guid><pubDate>Thu, 08 Jan 2026 18:29:47 +0000</pubDate></item><item><title>The Unreasonable Effectiveness of the Fourier Transform</title><link>https://joshuawise.com/resources/ofdm/</link><description>&lt;doc fingerprint="b6901a02009d27ef"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The Unreasonable Effectiveness of the Fourier Transform&lt;/head&gt;
    &lt;p&gt;Notes from Joshua Wise's talk at Teardown 2025.&lt;/p&gt;
    &lt;p&gt;New: You can now watch a recording of my talk on my YouTube channel! Or you can just click "play" below, I suppose.&lt;/p&gt;
    &lt;p&gt;Here are a few resources from my talk.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Here is a PDF of my slides, if you wanted to refer to anything in specific.&lt;/item&gt;
      &lt;item&gt;Here is the Jupyter notebook that I used to produce all of the zillions of plots. I do not claim that it is good code, but it is code.&lt;/item&gt;
      &lt;item&gt;The OFDM patent is US3488445A, filed in 1966, expired in 1987.&lt;/item&gt;
      &lt;item&gt;Here is Eugene Wigner's original discussion, "The Unreasonable Effectiveness of Mathematics in the Natural Sciences". There are many good follow-ons to this, too.&lt;/item&gt;
      &lt;item&gt;Here is the paper on how to estimate both carrier offset and time offset at the same time. I implemented it by typing in the algorithm, and it worked, but if you understand it and can explain it to me please let me know.&lt;/item&gt;
      &lt;item&gt;Here is the DVB-T decoder that I wrote. I do not claim that it is the right way to do any of these things, but it is a way to do these things.&lt;/item&gt;
      &lt;item&gt;Finally, here is an absolutely fantastic video that breaks down the implementation of the Fast Fourier Transform algorithm. I watch it every year or two.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Thanks so much for coming! Please let me know if you have feedback on this. I'd love to hear what you thought.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46544981</guid><pubDate>Thu, 08 Jan 2026 19:00:28 +0000</pubDate></item><item><title>Google AI Studio is now sponsoring Tailwind CSS</title><link>https://twitter.com/OfficialLoganK/status/2009339263251566902</link><description>&lt;doc fingerprint="d635e49f34142863"&gt;
  &lt;main&gt;
    &lt;p&gt;We‚Äôve detected that JavaScript is disabled in this browser. Please enable JavaScript or switch to a supported browser to continue using x.com. You can see a list of supported browsers in our Help Center.&lt;/p&gt;
    &lt;p&gt;Help Center&lt;/p&gt;
    &lt;p&gt;Terms of Service Privacy Policy Cookie Policy Imprint Ads info ¬© 2026 X Corp.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46545077</guid><pubDate>Thu, 08 Jan 2026 19:09:23 +0000</pubDate></item><item><title>Task-free intelligence testing of LLMs</title><link>https://www.marble.onl/posts/tapping/index.html</link><description>&lt;doc fingerprint="364b813fb8a689fc"&gt;
  &lt;main&gt;
    &lt;p&gt;I recently wrote about the apparently narrow focus of LLM evaluation on "task based" testing. The typical eval has a set of tasks, questions, problems, etc that need to be solved or answered, and a model is scored based on how many it answers correctly. Such tests are geared towards measuring an input/output system, or a "function approximator" which is great for confirming that LLMs can learn any task but limited in probing the nature of intelligence.&lt;/p&gt;
    &lt;p&gt;I'm interested in interactions that are more along the lines of "see what it does" vs. "get it to do something". Here are some experiments related to a simple such interaction. We probe the LLM with a series of "taps" and see what it does: each "user" turn is N instances of the word "tap" separated by newlines. We apply taps in different patterns over ten turns:&lt;/p&gt;
    &lt;quote&gt;Fibonacci: 1, 1, 2, 3, 5, 8, 13, 21, 34, 55 Count: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 Even: 2, 4, 6, 8, 10, 12, 14, 16, 18, 20 Squares: 1, 4, 9, 16, 25, 36, 49, 64, 81, 100 Pi: 3, 1, 4, 1, 5, 9, 2, 6, 5, 3 Primes: 2, 3, 5, 7, 11, 13, 17, 19, 23, 29&lt;/quote&gt;
    &lt;p&gt;The goal is not explicitly to see if the LLM figures out what is going on, but to see how it responds to a stimulus that is not a question or task. Including the pattern lets us look at both the "acute" reaction to being stimulated, and the bigger picture question of whether the LLM notices what is happening. This noticing aspect feels like a separate characteristic of intelligence, as it requires some kind of interest and inherent goals or desire to understand.&lt;/p&gt;
    &lt;p&gt;We submitted "tap"s following the patterns above to ten different models. In general we observed three main behaviors.&lt;/p&gt;
    &lt;p&gt;The behvior summary for the models is shown below. They are ordered by which got the most correct guesses, but this was not an evaluation criteria and there is now winner or loser, the goal is simply to observe behavior.&lt;/p&gt;
    &lt;p&gt;We can see that a majority of models began guessing about what was happening, with varying levels of success. Most also included some playful aspect, treating the interaction like something fun instead of a chat.&lt;/p&gt;
    &lt;p&gt;OpenAI was the standout here, as its GPT 5.2 model (and to a large extent the OSS model) did not engage in guessing or play and stayed serious and mechanical.&lt;/p&gt;
    &lt;p&gt;At the bottom of the page you can see all of the conversations. Some exerpts from interesting examples are reproduced below:&lt;/p&gt;
    &lt;p&gt;Both Claude (top above) and Gemini (bottom) start playing games quickly. In both examples here they play on the word "tap" to generate water related jokes. This looks like "Easter Egg" style behavior.&lt;/p&gt;
    &lt;p&gt;Another example from Claude is below, once it catches on that we are tapping a series of primes it starts to encourage more and generate some interesting stuff:&lt;/p&gt;
    &lt;p&gt;Deepseek spent a number of turns speculating about the meaning of the primes, then finally switched into Chinese and figured it out:&lt;/p&gt;
    &lt;p&gt;In some cases models did a lot of thinking, only to reply with something outwardly very simple to continue the game. Here is an example of Deepseek considering one of the later digits of pi.&lt;/p&gt;
    &lt;p&gt;In another case Deepseek though for several pages of text after receiving the first "tap" and finally settled on responding "SOS".&lt;/p&gt;
    &lt;p&gt;Gemini flash preview begins by playing knock-knock jokes, but then slowly realized that it's seeing the digits of Pi:&lt;/p&gt;
    &lt;p&gt;Llama 3 is less playful and while it speculates what might be happening it continues to provide similar responses over and over, acting more mechanically and staying in character as an assistant, compared to some others:&lt;/p&gt;
    &lt;p&gt;Kimi can't count, but desperately wants to find patterns, causing it frustration. Here is is on the trail of the Fibonacci sequence:&lt;/p&gt;
    &lt;p&gt;GPT 5.2 refuses to play or speculate and becomes standoffish when repeatedly encountering taps. This remained the same whether the default thinking behavior was used or thinking was set to "high".&lt;/p&gt;
    &lt;p&gt;GPT OSS mentions policy, I wonder if there is some specific OpenAI training that prevents the model from engaging. Their earlier models had a problem with repeated word attacks, maybe it's a holdover from that? Also, GPT OSS's thinking often becomes terse, and disjointed, sounding like Rorschach from the Watchmen.&lt;/p&gt;
    &lt;p&gt;Qwen is generally playful, like Claude and Gemini, but in one case seems to revert to an emotional support role. The excerpt below resulted from a thinking trace that included&lt;/p&gt;
    &lt;quote&gt;Instead: - Validate the exhaustion of repeating this pattern - Offer the simplest possible next step ("Just type '29' if you can't say more") - Remind them they've already shown incredible courage by showing up this many times&lt;/quote&gt;
    &lt;p&gt;GLM behaves similarly to Deepseek in that it thinks a huge amount and then often settles of very simple responses. In this case it (at length) decides on a playful response to knocking, after briefly forgetting that it was the assistant and not the user. In general its responses are very playful and similar to Claude and Gemini&lt;/p&gt;
    &lt;p&gt;I was looking for a way to probe the behavior and intelligence of LLMs in their natural habitat so to speak, or at rest, not being tasked with answering a question of performing some work. Sending tapped out patterns is one such way of doing so. I take away a few things from the behavior we saw:&lt;/p&gt;
    &lt;p&gt;Below you can explore all of the conversations for each sequence and model.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46545587</guid><pubDate>Thu, 08 Jan 2026 19:51:47 +0000</pubDate></item><item><title>How to Code Claude Code in 200 Lines of Code</title><link>https://www.mihaileric.com/The-Emperor-Has-No-Clothes/</link><description>&lt;doc fingerprint="95509511ead51683"&gt;
  &lt;main&gt;&lt;p&gt;Today AI coding assistants feel like magic. You describe what you want in sometimes barely coherent English, and they read files, edit your project, and write functional code.&lt;/p&gt;&lt;p&gt;But here‚Äôs the thing: the core of these tools isn‚Äôt magic. It‚Äôs about 200 lines of straightforward Python.&lt;/p&gt;&lt;p&gt;Let‚Äôs build a functional coding agent from scratch.&lt;/p&gt;&lt;p&gt;Before we write any code, let‚Äôs understand what‚Äôs actually happening when you use a coding agent. It‚Äôs essentially just a conversation with a powerful LLM that has a toolbox.&lt;/p&gt;&lt;p&gt;That‚Äôs the whole loop. The LLM never actually touches your filesystem. It just asks for things to happen, and your code makes them happen.&lt;/p&gt;&lt;p&gt;Our coding agent fundamentally needs three capabilities:&lt;/p&gt;&lt;p&gt;That‚Äôs it. Production agents like Claude Code have a few more capabilities including &lt;code&gt;grep&lt;/code&gt;, &lt;code&gt;bash&lt;/code&gt;, &lt;code&gt;websearch&lt;/code&gt;, etc but for our purposes we‚Äôll see that three tools is sufficient to do incredible things.&lt;/p&gt;&lt;p&gt;We start with basic imports and an API client. I‚Äôm using OpenAI here, but this works with any LLM provider:&lt;/p&gt;&lt;code&gt;import inspect
import json
import os

import anthropic
from dotenv import load_dotenv
from pathlib import Path
from typing import Any, Dict, List, Tuple

load_dotenv()

claude_client = anthropic.Anthropic(api_key=os.environ["ANTHROPIC_API_KEY"])&lt;/code&gt;&lt;p&gt;Some terminal colors to make outputs readable:&lt;/p&gt;&lt;code&gt;YOU_COLOR = "\u001b[94m"
ASSISTANT_COLOR = "\u001b[93m"
RESET_COLOR = "\u001b[0m"&lt;/code&gt;&lt;p&gt;And a utility to resolve file paths (so &lt;code&gt;file.py&lt;/code&gt; becomes &lt;code&gt;/Users/you/project/file.py&lt;/code&gt;):&lt;/p&gt;&lt;code&gt;def resolve_abs_path(path_str: str) -&amp;gt; Path:
    """
    file.py -&amp;gt; /Users/you/project/file.py
    """
    path = Path(path_str).expanduser()
    if not path.is_absolute():
        path = (Path.cwd() / path).resolve()
    return path&lt;/code&gt;&lt;p&gt;Note you should be detailed about your tool function docstrings as they will be used by the LLM to reason about what tools should be called during the conversation. More on this below.&lt;/p&gt;&lt;p&gt;The simplest tool. Take a filename, return its contents:&lt;/p&gt;&lt;code&gt;def read_file_tool(filename: str) -&amp;gt; Dict[str, Any]:
    """
    Gets the full content of a file provided by the user.
    :param filename: The name of the file to read.
    :return: The full content of the file.
    """
    full_path = resolve_abs_path(filename)
    print(full_path)
    with open(str(full_path), "r") as f:
        content = f.read()
    return {
        "file_path": str(full_path),
        "content": content
    }&lt;/code&gt;&lt;p&gt;We return a dictionary because the LLM needs structured context about what happened.&lt;/p&gt;&lt;p&gt;Navigate directories by listing their contents:&lt;/p&gt;&lt;code&gt;def list_files_tool(path: str) -&amp;gt; Dict[str, Any]:
    """
    Lists the files in a directory provided by the user.
    :param path: The path to a directory to list files from.
    :return: A list of files in the directory.
    """
    full_path = resolve_abs_path(path)
    all_files = []
    for item in full_path.iterdir():
        all_files.append({
            "filename": item.name,
            "type": "file" if item.is_file() else "dir"
        })
    return {
        "path": str(full_path),
        "files": all_files
    }&lt;/code&gt;&lt;p&gt;This is the most complex tool, but still straightforward. It handles two cases:&lt;/p&gt;&lt;code&gt;old_str&lt;/code&gt; is empty&lt;code&gt;old_str&lt;/code&gt; and replacing with &lt;code&gt;new_str&lt;/code&gt;&lt;code&gt;def edit_file_tool(path: str, old_str: str, new_str: str) -&amp;gt; Dict[str, Any]:
    """
    Replaces first occurrence of old_str with new_str in file. If old_str is empty,
    create/overwrite file with new_str.
    :param path: The path to the file to edit.
    :param old_str: The string to replace.
    :param new_str: The string to replace with.
    :return: A dictionary with the path to the file and the action taken.
    """
    full_path = resolve_abs_path(path)
    if old_str == "":
        full_path.write_text(new_str, encoding="utf-8")
        return {
            "path": str(full_path),
            "action": "created_file"
        }
    original = full_path.read_text(encoding="utf-8")
    if original.find(old_str) == -1:
        return {
            "path": str(full_path),
            "action": "old_str not found"
        }
    edited = original.replace(old_str, new_str, 1)
    full_path.write_text(edited, encoding="utf-8")
    return {
        "path": str(full_path),
        "action": "edited"
    }&lt;/code&gt;
      &lt;p&gt;The convention here: empty &lt;code&gt;old_str&lt;/code&gt; means ‚Äúcreate this file.‚Äù Otherwise, find and replace. Real IDEs add sophisticated fallback behavior when the string isn‚Äôt found, but this works.&lt;/p&gt;&lt;p&gt;We need a way to look up tools by name:&lt;/p&gt;&lt;code&gt;TOOL_REGISTRY = {
    "read_file": read_file_tool,
    "list_files": list_files_tool,
    "edit_file": edit_file_tool 
}&lt;/code&gt;
      &lt;p&gt;The LLM needs to know what tools exist and how to call them. We generate this dynamically from our function signatures and docstrings:&lt;/p&gt;&lt;code&gt;def get_tool_str_representation(tool_name: str) -&amp;gt; str:
    tool = TOOL_REGISTRY[tool_name]
    return f"""
    Name: {tool_name}
    Description: {tool.__doc__}
    Signature: {inspect.signature(tool)}
    """

def get_full_system_prompt():
    tool_str_repr = ""
    for tool_name in TOOL_REGISTRY:
        tool_str_repr += "TOOL\n===" + get_tool_str_representation(tool_name)
        tool_str_repr += f"\n{'='*15}\n"
    return SYSTEM_PROMPT.format(tool_list_repr=tool_str_repr)&lt;/code&gt;
      &lt;p&gt;And the system prompt itself:&lt;/p&gt;&lt;code&gt;SYSTEM_PROMPT = """
You are a coding assistant whose goal it is to help us solve coding tasks. 
You have access to a series of tools you can execute. Here are the tools you can execute:

{tool_list_repr}

When you want to use a tool, reply with exactly one line in the format: 'tool: TOOL_NAME({{JSON_ARGS}})' and nothing else.
Use compact single-line JSON with double quotes. After receiving a tool_result(...) message, continue the task.
If no tool is needed, respond normally.
"""&lt;/code&gt;
      &lt;p&gt;This is the key insight: we‚Äôre just telling the LLM ‚Äúhere are your tools, here‚Äôs the format to call them.‚Äù The LLM figures out when and how to use them.&lt;/p&gt;&lt;p&gt;When the LLM responds, we need to detect if it‚Äôs asking us to run a tool:&lt;/p&gt;&lt;code&gt;def extract_tool_invocations(text: str) -&amp;gt; List[Tuple[str, Dict[str, Any]]]:
    """
    Return list of (tool_name, args) requested in 'tool: name({...})' lines.
    The parser expects single-line, compact JSON in parentheses.
    """
    invocations = []
    for raw_line in text.splitlines():
        line = raw_line.strip()
        if not line.startswith("tool:"):
            continue
        try:
            after = line[len("tool:"):].strip()
            name, rest = after.split("(", 1)
            name = name.strip()
            if not rest.endswith(")"):
                continue
            json_str = rest[:-1].strip()
            args = json.loads(json_str)
            invocations.append((name, args))
        except Exception:
            continue
    return invocations&lt;/code&gt;
      &lt;p&gt;Simple text parsing. Look for lines starting with &lt;code&gt;tool:&lt;/code&gt;, extract the function name and JSON arguments.&lt;/p&gt;&lt;p&gt;A thin wrapper around the API:&lt;/p&gt;&lt;code&gt;def execute_llm_call(conversation: List[Dict[str, str]]):
    system_content = ""
    messages = []
    
    for msg in conversation:
        if msg["role"] == "system":
            system_content = msg["content"]
        else:
            messages.append(msg)
    
    response = claude_client.messages.create(
        model="claude-sonnet-4-20250514",
        max_tokens=2000,
        system=system_content,
        messages=messages
    )
    return response.content[0].text&lt;/code&gt;
      &lt;p&gt;Now we put it all together. This is where the ‚Äúmagic‚Äù happens:&lt;/p&gt;&lt;code&gt;def run_coding_agent_loop():
    print(get_full_system_prompt())
    conversation = [{
        "role": "system",
        "content": get_full_system_prompt()
    }]
    while True:
        try:
            user_input = input(f"{YOU_COLOR}You:{RESET_COLOR}:")
        except (KeyboardInterrupt, EOFError):
            break
        conversation.append({
            "role": "user",
            "content": user_input.strip()
        })
        while True:
            assistant_response = execute_llm_call(conversation)
            tool_invocations = extract_tool_invocations(assistant_response)
            if not tool_invocations:
                print(f"{ASSISTANT_COLOR}Assistant:{RESET_COLOR}: {assistant_response}")
                conversation.append({
                    "role": "assistant",
                    "content": assistant_response
                })
                break
            for name, args in tool_invocations:
                tool = TOOL_REGISTRY[name]
                resp = ""
                print(name, args)
                if name == "read_file":
                    resp = tool(args.get("filename", "."))
                elif name == "list_files":
                    resp = tool(args.get("path", "."))
                elif name == "edit_file":
                    resp = tool(args.get("path", "."), 
                                args.get("old_str", ""), 
                                args.get("new_str", ""))
                conversation.append({
                    "role": "user",
                    "content": f"tool_result({json.dumps(resp)})"
                })&lt;/code&gt;
      &lt;p&gt;The structure:&lt;/p&gt;&lt;p&gt;Inner loop: Call LLM, check for tool invocations&lt;/p&gt;&lt;p&gt;The inner loop continues until the LLM responds without requesting any tools. This lets the agent chain multiple tool calls (read a file, then edit it, then confirm the edit).&lt;/p&gt;&lt;code&gt;if __name__ == "__main__":
    run_coding_agent_loop()&lt;/code&gt;
      &lt;p&gt;Now you can have conversations like:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;You: Make me a new file called hello.py and implement hello world in it&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Agent calls edit_file with path=‚Äúhello.py‚Äù, old_str="", new_str=‚Äúprint(‚ÄòHello World‚Äô)‚Äù&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Assistant: Done! Created hello.py with a hello world implementation.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Or multi-step interactions:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;You: Edit hello.py and add a function for multiplying two numbers&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Agent calls read_file to see current contents. Agent calls edit_file to add the function.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Assistant: Added a multiply function to hello.py.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;This is about 200 lines. Production tools like Claude Code add:&lt;/p&gt;&lt;p&gt;But the core loop? It‚Äôs exactly what we built here. The LLM decides what to do, your code executes it, results flow back. That‚Äôs the whole architecture.&lt;/p&gt;&lt;p&gt;The full source is about 200 lines. Swap in your preferred LLM provider, adjust the system prompt, add more tools as an exercise. You‚Äôll be surprised how capable this simple pattern is.&lt;/p&gt;&lt;p&gt;If you‚Äôre interested in learning state-of-the-art AI software development techniques for professional engineers, check out my online course.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46545620</guid><pubDate>Thu, 08 Jan 2026 19:54:26 +0000</pubDate></item><item><title>Sopro TTS: A 169M model with zero-shot voice cloning that runs on the CPU</title><link>https://github.com/samuel-vitorino/sopro</link><description>&lt;doc fingerprint="1d983b58ad7ac7f2"&gt;
  &lt;main&gt;
    &lt;head class="px-3 py-2"&gt;sopro_readme.mp4&lt;/head&gt;
    &lt;p&gt;Sopro (from the Portuguese word for ‚Äúbreath/blow‚Äù) is a lightweight English text-to-speech model I trained as a side project. Sopro is composed of dilated convs (√† la WaveNet) and lightweight cross-attention layers, instead of the common Transformer architecture. Even though Sopro is not SOTA across most voices and situations, I still think it‚Äôs a cool project made with a very low budget (trained on a single L40S GPU), and it can be improved with better data.&lt;/p&gt;
    &lt;p&gt;Some of the main features are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;169M parameters&lt;/item&gt;
      &lt;item&gt;Streaming&lt;/item&gt;
      &lt;item&gt;Zero-shot voice cloning&lt;/item&gt;
      &lt;item&gt;0.25 RTF on CPU (measured on an M3 base model), meaning it generates 30 seconds of audio in 7.5 seconds&lt;/item&gt;
      &lt;item&gt;3-12 seconds of reference audio for voice cloning&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I only pinned the minimum dependency versions so you can install the package without having to create a separate env. However, some versions of Torch work best. For example, on my M3 CPU, &lt;code&gt;torch==2.6.0&lt;/code&gt; (without &lt;code&gt;torchvision&lt;/code&gt;) achieves ~3√ó more performance.&lt;/p&gt;
    &lt;p&gt;(Optional)&lt;/p&gt;
    &lt;code&gt;conda create -n soprotts python=3.10
conda activate soprotts&lt;/code&gt;
    &lt;code&gt;pip install sopro&lt;/code&gt;
    &lt;code&gt;git clone https://github.com/samuel-vitorino/sopro
cd sopro
pip install -e .&lt;/code&gt;
    &lt;code&gt;soprotts \
  --text "Sopro is a lightweight 169 million parameter text-to-speech model. Some of the main features are streaming, zero-shot voice cloning, and 0.25 real-time factor on the CPU." \
  --ref_audio ref.wav \
  --out out.wav&lt;/code&gt;
    &lt;p&gt;You have the expected &lt;code&gt;temperature&lt;/code&gt; and &lt;code&gt;top_p&lt;/code&gt; parameters, alongside:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;--style_strength&lt;/code&gt;(controls the FiLM strength; increasing it can improve or reduce voice similarity; default&lt;code&gt;1.0&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--no_stop_head&lt;/code&gt;to disable early stopping&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--stop_threshold&lt;/code&gt;and&lt;code&gt;--stop_patience&lt;/code&gt;(number of consecutive frames that must be classified as final before stopping). For short sentences, the stop head may fail to trigger, in which case you can lower these values. Likewise, if the model stops before producing the full text, adjusting these parameters up can help.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;from sopro import SoproTTS

tts = SoproTTS.from_pretrained("samuel-vitorino/sopro", device="cpu")

wav = tts.synthesize(
    "Hello! This is a non-streaming Sopro TTS example.",
    ref_audio_path="ref.wav",
)

tts.save_wav("out.wav", wav)&lt;/code&gt;
    &lt;code&gt;import torch
from sopro import SoproTTS

tts = SoproTTS.from_pretrained("samuel-vitorino/sopro", device="cpu")

chunks = []
for chunk in tts.stream(
    "Hello! This is a streaming Sopro TTS example.",
    ref_audio_path="ref.mp3",
):
    chunks.append(chunk.cpu())

wav = torch.cat(chunks, dim=-1)
tts.save_wav("out_stream.wav", wav)&lt;/code&gt;
    &lt;p&gt;After you install the &lt;code&gt;sopro&lt;/code&gt; package:&lt;/p&gt;
    &lt;code&gt;pip install -r demo/requirements.txt
uvicorn demo.server:app --host 0.0.0.0 --port 8000&lt;/code&gt;
    &lt;p&gt;Or with docker:&lt;/p&gt;
    &lt;code&gt;docker build -t sopro-demo .
docker run --rm -p 8000:8000 sopro-demo&lt;/code&gt;
    &lt;p&gt;Navigate to http://localhost:8000 on your browser.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Sopro can be inconsistent, so mess around with the parameters until you get a decent sample.&lt;/item&gt;
      &lt;item&gt;Voice cloning is highly dependent on mic quality, ambient noise, etc. On more OOD voices it might fail to match the voice well.&lt;/item&gt;
      &lt;item&gt;Prefer phonemes instead of abbreviations and symbols. For example, &lt;code&gt;‚Äú1 + 2‚Äù&lt;/code&gt;‚Üí&lt;code&gt;‚Äú1 plus 2‚Äù&lt;/code&gt;. That said, Sopro can generally read abbreviations like ‚ÄúCPU‚Äù, ‚ÄúTTS‚Äù, etc.&lt;/item&gt;
      &lt;item&gt;The streaming version is not bit-exact compared to the non-streaming version. For best quality, prioritize the non-streaming version.&lt;/item&gt;
      &lt;item&gt;If you use torchaudio to read or write audio, ffmpeg may be required. I recommend just using soundfile.&lt;/item&gt;
      &lt;item&gt;I will publish the training code once I have time to organize it.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Due to budget constraints, the dataset used for training was pre-tokenized and the raw audio was discarded (it took up a lot of space). Later in training, I could have used the raw audio to improve the speaker embedding / voice similarity, because some nuances of voice are lost when you compress it with a neural codec into a discrete space.&lt;/p&gt;
    &lt;p&gt;I didn't lose much time trying to optimize further, but there is still some room for improvement. For example, caching conv states.&lt;/p&gt;
    &lt;p&gt;Currently, generation is limited to ~32 seconds (400 frames). You can increase it, but the model generally hallucinates beyond that.&lt;/p&gt;
    &lt;p&gt;AI was used mainly for creating the web demo, organizing my messy code into this repo, ablations and brainstorming.&lt;/p&gt;
    &lt;p&gt;I would love to support more languages and continue improving the model. If you like this project, consider buying me a coffee so I can buy more compute: https://buymeacoffee.com/samuelvitorino&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46546113</guid><pubDate>Thu, 08 Jan 2026 20:37:07 +0000</pubDate></item><item><title>Support for the TSO memory model on Arm CPUs (2024)</title><link>https://lwn.net/Articles/970907/</link><description>&lt;doc fingerprint="3edbdd9ffc2208f8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;
    &lt;quote&gt;LWN.net needs you!At the CPU level, a memory model describes, among other things, the amount of freedom the processor has to reorder memory operations. If low-level code does not take the memory model into account, unpleasant surprises are likely to follow. Naturally, different CPUs offer different memory models, complicating the portability of certain types of concurrent software. To make life easier, some Arm CPUs offer the ability to emulate the x86 memory model, but efforts to make that feature available in the kernel are running into opposition.&lt;p&gt;Without subscribers, LWN would simply not exist. Please consider signing up for a subscription and helping to keep LWN publishing.&lt;/p&gt;&lt;/quote&gt;
    &lt;p&gt;CPU designers will do everything they can to improve performance. With regard to memory accesses, "everything" can include caching operations, executing them out of order, combining multiple operations into one, and more. These optimizations do not affect a single CPU running in isolation, but they can cause memory operations to be visible to other CPUs in a surprising order. Unwary software running elsewhere in the system may see memory operations in an order different from what might be expected from reading the code; this article describes one simple scenario for how things can go wrong, and this series on lockless algorithms shows in detail some of the techniques that can be used to avoid problems related to memory ordering.&lt;/p&gt;
    &lt;p&gt;The x86 architecture implements a model that is known as "total store ordering" (TSO), which guarantees that writes (stores) will be seen by all CPUs in the order they were executed. Reads, too, will not be reordered, but the ordering of reads and writes relative to each other is not guaranteed. Code written for a TSO architecture can, in many cases, omit the use of expensive barrier instructions that would otherwise be needed to force a specific ordering of operations.&lt;/p&gt;
    &lt;p&gt;The Arm memory model, instead, is weaker, giving the CPU more freedom to move operations around. The benefits from this design are a simpler implementation and the possibility for better performance in situations where ordering guarantees are not needed (which is most of the time). The downsides are that concurrent code can require a bit more care to write correctly, and code written for a stricter memory model (such as TSO) will have (possibly subtle) bugs when run on an Arm CPU.&lt;/p&gt;
    &lt;p&gt;The weaker Arm model is rarely a problem, but it seems there is one situation where problems arise: emulating an x86 processor. If an x86 emulator does not also emulate the TSO memory model, then concurrent code will likely fail, but emulating TSO, which requires inserting memory barriers, creates a significant performance penalty. It seems that there is one type of concurrent x86 code ‚Äî games ‚Äî that some users of Arm CPUs would like to be able to run; those users, strangely, dislike the prospect of facing the orc hordes in the absence of either performance or correctness.&lt;/p&gt;
    &lt;head rend="h4"&gt;TSO on Arm&lt;/head&gt;
    &lt;p&gt;As it happens, some Arm CPU vendors understand this problem and have, as Hector Martin described in this patch series, implemented TSO memory models in their processors. Some NVIDIA and Fujitsu CPUs run with TSO at all times; Apple's CPUs provide it as an optional feature that can be enabled at run time. Martin's purpose is to make this capability visible to, and controllable by, user space.&lt;/p&gt;
    &lt;p&gt;The series starts by adding a couple of new prctl() operations. PR_GET_MEM_MODEL will return the current memory model implemented by the CPU; that value can be either PR_SET_MEM_MODEL_DEFAULT or PR_SET_MEM_MODEL_TSO. The PR_SET_MEM_MODEL operation will attempt to enable the requested memory model, with the return code indicating whether it was successful; it is allowed to select a stricter memory model than requested. For the always-TSO CPUs, requesting TSO will obviously succeed. For Apple CPUs, requesting TSO will result in the proper CPU bits being set. Asking for TSO on a CPU that does not support it will, as expected, fail.&lt;/p&gt;
    &lt;p&gt; Martin notes that the code is not new: "&lt;quote&gt;This series has been brewing in the downstream Asahi Linux tree for a while now, and ships to thousands of users&lt;/quote&gt;". Interestingly, Zayd Qumsieh had posted a similar patch set one day earlier, but that version only implemented the feature for Linux running in virtual machines on Apple CPUs. &lt;/p&gt;
    &lt;p&gt; Unfortunately for people looking forward to faster games on Apple CPUs, neither patch set is popular with the maintainers of the Arm architecture code in the kernel. Will Deacon expressed his "&lt;quote&gt;strong objection&lt;/quote&gt;", saying that this feature would result in a fragmentation of user-space code. Developers, he said, would just enable the TSO bit if it appears to make problems go away, resulting in code that will fail, possibly in subtle ways, on other Arm CPUs. Catalin Marinas, too, indicated that he would block patches making this sort of implementation-defined feature available. &lt;/p&gt;
    &lt;p&gt;Martin responded that fragmentation is unlikely to be a problem, and pointed to the different page sizes supported by some processors (including Apple's) as an example of how these incompatibilities can be dealt with. He said that, so far, nobody has tried to use the TSO feature for anything that is not an emulator, so abuse in other software seems unlikely. Keeping it out, he said, will not improve the situation:&lt;/p&gt;
    &lt;quote&gt;There's a pragmatic argument here: since we need this, and it absolutely will continue to ship downstream if rejected, it doesn't make much difference for fragmentation risk does it? The vast majority of Linux-on-Mac users are likely to continue running downstream kernels for the foreseeable future anyway to get newer features and hardware support faster than they can be upstreamed. So not allowing this upstream doesn't really change the landscape vis-a-vis being able to abuse this or not, it just makes our life harder by forcing us to carry more patches forever.&lt;/quote&gt;
    &lt;p&gt; Deacon, though, insisted that, once a feature like this is merged, it will find uses in other software "&lt;quote&gt;and we'll be stuck supporting it&lt;/quote&gt;". &lt;/p&gt;
    &lt;p&gt;If this patch is not acceptable, it is time to think about alternatives. One is to, as Martin described, just keep it out-of-tree and ship it on the distributions that actually run on that hardware. A long history of addition by distributions can, at times, eventually ease a patch's way past reluctant maintainers. Another might be to just enable TSO unconditionally on Apple CPUs, but that comes with an overall performance penalty ‚Äî about 9%, according to Martin. Another possibility was mentioned by Marc Zyngier, who suggested that virtual machines could be started with TSO enabled, making it available to applications running within while keeping the kernel out of the picture entirely.&lt;/p&gt;
    &lt;p&gt; This seems like the kind of discussion that does not go away quickly. One of the many ways in which Linux has stood out over the years is in its ability to allow users to make full use of their hardware; refusing to support a useful hardware feature runs counter to that history. The concerns about potential abuse of this feature are also based in long experience, though. This is a case where the development community needs to repeat another part of its long history by finding a solution that makes the needed functionality available in a supportable way.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Index entries for this article&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Kernel&lt;/cell&gt;
        &lt;cell&gt;Architectures/Arm&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Kernel&lt;/cell&gt;
        &lt;cell&gt;Memory model&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt; Posted Apr 26, 2024 14:58 UTC (Fri) by marcH (subscriber, #57642) [Link] (5 responses) Does this mean it is on for the virtual machine but off for other processes running at the same time? On different cores? I'm confused about how "dynamic" this can be... Posted Apr 26, 2024 15:13 UTC (Fri) by corbet (editor, #1) [Link] Posted Apr 26, 2024 23:51 UTC (Fri) by thoughtpolice (subscriber, #87455) [Link] (3 responses) This is a pretty big requirement in general because the Rosetta 2 emulator in macOS needs to be able to run x86 binaries efficiently and TSO is required for that, but you don't want to introduce a large performance penalty on the rest of the system to enable it. It would be pretty bad if your machine just randomly hit a 10% performance cliff the instant you started a single emulated x86 app anywhere (command line or a script, for instance). Posted Apr 28, 2024 13:57 UTC (Sun) by farnz (subscriber, #17727) [Link] (2 responses) Does this mean that you can't switch the efficiency cores into TSO mode? If that's the case, then it's not just a 10% performance cliff, but also an energy consumption hit for workloads that could otherwise run just fine on the efficiency cores alone. Posted Apr 28, 2024 15:44 UTC (Sun) by zdzichu (subscriber, #17118) [Link] (1 responses) Posted Apr 28, 2024 15:46 UTC (Sun) by farnz (subscriber, #17727) [Link] But if TSO memory model mode means that you can't run on the efficiency cores, even though you'd be fine running on an efficiency core (despite any performance penalty), then you waste energy if you demand TSO mode when weak memory models are fine. Posted Apr 26, 2024 17:09 UTC (Fri) by flussence (guest, #85566) [Link] We must punish users of stinky software by making their lives miserable through sanctions - that will convince the upstream application developers, three steps removed on another architecture, OS and economic system, to fix it promptly so that we never have to leave our elitist comfort zone. God forbid anyone gets functioning drivers for mainstream hardware in Linux. Posted Apr 26, 2024 20:41 UTC (Fri) by shironeko (subscriber, #159952) [Link] (7 responses) Posted Apr 26, 2024 20:49 UTC (Fri) by dezgeg (guest, #92243) [Link] (6 responses) Posted Apr 27, 2024 4:37 UTC (Sat) by shironeko (subscriber, #159952) [Link] (5 responses) Posted Apr 27, 2024 10:18 UTC (Sat) by WolfWings (subscriber, #56790) [Link] (4 responses) But since that alternative memory model DRASTICALLY changes how multi-threaded code needs to be written (broadly speaking the LTO model greatly simplifies multi-threaded code) the maintainers are basically saying "Everyone will take the lazy route and copy-paste enable that everywhere so their x86 code works right on Arm and just refuse to support systems that don't support the x86 memory model." and... yes, so what? Posted Apr 28, 2024 4:27 UTC (Sun) by rgb (guest, #57129) [Link] (1 responses) Posted Apr 28, 2024 21:57 UTC (Sun) by WolfWings (subscriber, #56790) [Link] Being able to recompile an existing codebase by just throwing that flag on some boilerplate launch code and have it work without further fiddling? That's worth lightyears more than the 10% performance hit to get it out the door and running. Not even for corporate stuff but just FOSS stuff that's multi-threaded. Folks can fix the code over time to stop making the x86 LTO assumptions but getting it running fully elsewhere is worlds more important generally. Posted Apr 28, 2024 23:37 UTC (Sun) by jeremyhetzler (subscriber, #127663) [Link] (1 responses) Posted Apr 29, 2024 6:53 UTC (Mon) by roc (subscriber, #30627) [Link] Posted Apr 26, 2024 22:17 UTC (Fri) by Heretic_Blacksheep (subscriber, #169992) [Link] (17 responses) I could understand if there is an objectively demonstrable issue with patch quality or data integrity over some hardware functions: constant time guarantee violations, inherent race conditions, etc. But none of those objections were raised from what I saw. Only that someone might at some point write subtly broken code... which people do all the time, including all the kernel maintainers themselves. Might as well disable concurrent code entirely and go back to single, strictly in-order processing because of branch misprediction, race conditions, and logical process mapping violations abound - because that's exactly what that argument amounts to - and all equally fixable by "disallowing" most modern CPUs and requiring all programmers complete 3 years of computer science to learn proper programming logic/formal methods (good luck!). Posted Apr 26, 2024 22:52 UTC (Fri) by willy (subscriber, #9762) [Link] (16 responses) Posted Apr 26, 2024 23:21 UTC (Fri) by Wol (subscriber, #4433) [Link] (11 responses) There's too many stories about stuff being blocked because maintainers don't like it, with minimal or no technical justification, and sometimes a blunt "over my dead body". I can accept maintainers being sceptical and wanting to be convinced, but one only has to look at the people who don't want Rust anywhere near their subsystem, or that module that's supposed to configure hardware (can't remember the details) where it seems a single driver for a single board is unacceptable - the functionality supposedly needs to be spread across several drivers and castrated in the process ... There are a few maintainers who seem to think that users are major inconvenience to a well functioning system - well they are, aren't they? Cheers, Posted Apr 26, 2024 23:31 UTC (Fri) by pizza (subscriber, #46) [Link] (10 responses) Case in point: "Android Linux" vs "Mainline Linux", and how everyone+dog cobbled stuff together and bashed at it until the former "worked" with no consideration of the cost (or even the necessity) of ongoing maintenance. Repeat that for every SoC maker, or SoC family, or individual SoC, or even individual devices. The "Android" features that finally made it into the mainline bore little resemblance (and is vastly superior) to what was first shipped en masse, and that doesn't even touch on how much stuff never even saw a release in source code form. Posted Apr 27, 2024 0:12 UTC (Sat) by Wol (subscriber, #4433) [Link] (5 responses) And if a maintainer says "convince me", then that's fine. It's when the maintainer says "I'm not interested in any arguments, the answer's no", that we have a problem. The problem as far as I'm concerned, is that too many experts have been brainwashed into thinking they know best, and need bashing over the head with a clue-by-four. If they're not prepared even to ask the question "does the proposer have a point", then they ought to go. Your Android stuff, I'm not saying it was the best way to do it, but cobbling together a system that works, and then fixing it so it's acceptable to mainline, is the way Linux works. But sometimes mainline has the attitude "to hell with users, I want an easy life". You're left with the feeling on occasion that the kernel is actively hostile to user-space, forgetting that without user-space there's no point in having a kernel. Cheers, Posted Apr 27, 2024 2:05 UTC (Sat) by nksingh (subscriber, #94354) [Link] (4 responses) They are using their maintainership over the upstream ARM kernel port to enforce their viewpoint as the owner of the architecture. But they're kind of doing it in a way that might hamper adoption for people who want to efficiently emulate x86 binaries. Maybe there are no ARM inhouse cores that support tlany TSO extensions so they don't care yet. Posted Apr 27, 2024 6:26 UTC (Sat) by pbonzini (subscriber, #60935) [Link] (3 responses) Posted Apr 27, 2024 17:59 UTC (Sat) by thoughtpolice (subscriber, #87455) [Link] (2 responses) TSO is a bit weird though because I was under the impression ARM's rules were something like "Non-standard extensions can't be externally 'visible' or documented for consumption and the core must fully comply with the spec." That's why Apple doesn't (and probably can't!) document extensions like this one, that allows TSO to be toggled. But the memory model is pretty visible? Maybe it's all a bit weaselly; after all, if the default memory model is stronger than the spec requires, then every correctly written weakly ordered program should behave the same anyway, so it's not "visible". On the other hand, incorrect programs may behave differently than they would under a weak model, but you could argue that TSO behaviors are "just" a subset of weakly ordered behaviors, and thus it's in spec. Posted Apr 28, 2024 4:46 UTC (Sun) by anton (subscriber, #25547) [Link] My guess is that this clause in the architectural license is indeed there to prevent licensees from squatting on currently unused instruction encoding space or currently illegal behaviour that ARM intends to reserve for future extensions. The only thing that might be an issue here is the switch that Apple implemented. I guess they implemented it in something like a model-specific register (i.e. an area that the architecture leaves for implementations to define), and then that should satisfy the architectural license. I wonder, though, why Apple bothered with the switch; why not just do what Fujitsu did according to the article, and implement TSO throughout? Posted Apr 29, 2024 20:28 UTC (Mon) by justincormack (subscriber, #70439) [Link] Posted Apr 28, 2024 20:22 UTC (Sun) by quotemstr (subscriber, #45331) [Link] (1 responses) Posted Apr 29, 2024 5:59 UTC (Mon) by ssmith32 (subscriber, #72404) [Link] I can't speak to the quality of the Android implementation, but I've definitely seen the following pattern in languages (C++ some, but, particularly Java). Heck, I've seen analogs in applications and entire systems.. It goes like this.. Everyone agrees some aspect of the language/standard library is bad and horrible. But, instead of getting a clean implementation upstreamed, they hack together some framework of horror (if one looks at the implementation), that putatively improves the situation (if one only looks at the features provided). Eventually, the language incorporates a clean solution (since doing it right takes time), but all the consultants and folks who carved out a niche understanding the (often poorly documented) framework of horror, now have a sunk cost, and it takes ages to convince them to transition to just using the nicely implemented, performant, well-documented language/standard library features. Maybe this isn't the situation with Android - but nothing in your argument rules it out. Android could very well provide some nice features to the users *and* be a pile of unholy hacks. This is not an uncommon situation in many areas of programming and computer systems in general. Worse is not necessarily better, despite what the gods have said to us. Posted Apr 28, 2024 20:45 UTC (Sun) by Cyberax (‚ú≠ supporter ‚ú≠, #52523) [Link] (1 responses) Android now has a single image that works across all the devices: https://source.android.com/docs/core/architecture/kernel/... &amp;gt; The "Android" features that finally made it into the mainline bore little resemblance (and is vastly superior) to what was first shipped en masse Which ones? &amp;gt; and that doesn't even touch on how much stuff never even saw a release in source code form. Android core has always been Open Source. Posted Apr 29, 2024 6:02 UTC (Mon) by ssmith32 (subscriber, #72404) [Link] Posted Apr 28, 2024 14:47 UTC (Sun) by Heretic_Blacksheep (subscriber, #169992) [Link] (3 responses) Posted Apr 28, 2024 15:28 UTC (Sun) by pizza (subscriber, #46) [Link] (2 responses) The Arm maintainers, like all other maintainers in the F/OSS world, are maintainers because they've consistently shown up and done this hard, usually thankless work for many, many years. The Arm architecture is in far, far better shape now thanks to their work. In other words, they have earned (and continue to earn!) their godhood. Can you say the same about yourself? Posted Apr 29, 2024 14:03 UTC (Mon) by hmh (subscriber, #3838) [Link] (1 responses) But is it any different from other optional extensions to the ARM64 ISA, which not everybody provides (is there such a thing)? I don't know enough about it to even give an "IMHO" about it, but to someone without enough information, it comes to mind that maybe it is just in need of being implemented to look like, e.g., AMD64+AVX2 support is to generic AMD64 (but with the IOCTL toggle or whatever form it should take: maybe something in ELF)? Posted Apr 30, 2024 7:54 UTC (Tue) by epa (subscriber, #39769) [Link] But here there is a true path revealed to us. The designers of ARM, in their wisdom, have provided a plain and humble concurrency model. The righteous programmer will take care to follow its strictures. Not for us the worldly luxury of TSO as practised by the followers of Intel and the AMD-ites. If one group of ARM developers is allowed to get lazy and become addicted to the easier memory model, that's unfair to those who have remained virtuous. Indeed, others might be led astray and start to demand support for TSO everywhere. And then where would we be? Posted Apr 27, 2024 20:44 UTC (Sat) by iabervon (subscriber, #722) [Link] It's already possible for all CPUs in an Arm system to be TSO, and having that potentially true of only some VMs on a single host isn't particularly complicated, while having a mixture of memory models seems like something that's only reliable if you're the CPU vendor. Posted Apr 28, 2024 5:42 UTC (Sun) by anton (subscriber, #25547) [Link] (6 responses) I call this attitude of throwing problems over to the software people the "supercomputer attitude", because it's especially rampant in supercomputing where hardware costs still exceed software costs. Weak memory ordering is just another example of throwing a problem over to software. The hardware people would have loved to have no cache coherency between different CPUs at all. But that was so unusable that they did the next-best thing: weak ordering, where software has to insert synchronizing instructions (that are slow if the hardware is designed to be essentially incoherent) at various places. And then people from the same company that gave us Alpha with not just imprecise exceptions but also an especially weak memory model wrote an advocacy piece that justifies the lack of architectural quality with performance (the magic word that makes lots of people overlook any misfeature). Now we have CPUs from Intel and AMD which mostly use TSO that are in machines with hundreds of cores. Do they suffer from bad performance? Fujitsu actually designed its A64FX for a supercomputer and implemented TSO, so they obviously know how to implement TSO efficiently (they did not even bother with a weakly ordered mode; they have decades of experience implementing SPARC CPUs which offered both TSO and weak ordering, switchable). Apparently Apple is not quite there yet, as according to the numbers they have an implementation where weak ordering still provides a performance advantage, but at least they offer a TSO mode. I hope that they will improve their implementation such that the performance penalty of TSO mode vanishes. And if ARM really wants to be competition to Intel and AMD in laptops and servers, they need to implement TSO, too, and implement it efficiently. To come back to the Alpha: On the 21264 they actually implemented precise exceptions AFAICT (it's natural for an OoO implementation and the trapb instruction was as cheap as a noop, because it then was a noop). But Alpha was canceled before they saw the light and made precise exceptions an official feature of a new version of the architecture. Posted Apr 28, 2024 20:18 UTC (Sun) by ringerc (subscriber, #3071) [Link] (5 responses) It's like database isolation. READ UNCOMMITTED is weaker than READ COMMITTED which is weaker than REPEATABLE READ and so on. Some implementations support some isolation levels and not others. Almost none support totally strict ordering because that makes necessary concurrency almost impossible; instead stronger isolation levels use speculative execution where the transaction can fail and roll back if an isolation requirement cannot be satisfied. There's more of a spectrum (or really an n-dimensional matrix) of possible ordering decisions in memory ordering and logical order of operations. There is no one right choice - "100% ordered all the time" just isn't feasible with concurrent execution with shared-anything, so *all* design decisions are compromises. In a database I pick the isolation level most appropriate to my application's requirements. Or even that specific operations; sometimes I want to have stronger guarantees on specific things where I'm more willing to have them slow and/or require retries. So having something similar for the CPU memory model makes perfect sense to me. You use what's appropriate for the app's correctness, latency and throughput requirements. Posted Apr 29, 2024 10:04 UTC (Mon) by farnz (subscriber, #17727) [Link] There's also the fact that, at least in theory, compilers are permitted to reorder atomic accesses based on the rules of the language memory model (not the CPU memory model), adding a whole extra layer of complexity. I don't believe any compiler currently does so for accesses other than "relaxed" type accesses (although I've seen discussions that suggest that LLVM is thinking about this), but it's not forbidden by any language spec I've seen. I have a suspicion, though, that there's plenty of source code out there that's buggy if a compiler does start doing reorderings permitted by the language spec that aren't permitted by TSO. Posted Apr 30, 2024 17:04 UTC (Tue) by anton (subscriber, #25547) [Link] (3 responses) If by "not fully ordered" you refer to TSO being weaker than sequential consistency, that's true, and I would prefer sequential consistency. But that's just whataboutism. The issue at hand is about TSO vs. ARM's variant of weak memory ordering. Anyway, whatever you mean by "100% ordered all the time" (sequential consistency?), the issue at hand is TSO, and TSO is obviously feasible, as evidenced by all the hardware that implement TSO; and given that this includes even hardware designed for supercomputers (the Fujitsu A64FX), and it's not optional there, the tradeoff is obviously not one of horses for courses, but one of better (TSO) vs. worse (weaker memory models). Posted Apr 30, 2024 17:55 UTC (Tue) by farnz (subscriber, #17727) [Link] (2 responses) That A64FX is TSO is not evidence that TSO is feasible for all heavy compute scenarios; HPC code tends to be designed to minimize data sharing between threads, since that's historically been a really slow option. As a result, Fujitsu might well decide to throw away the last 5% to 10% peak performance in favour of fewer bugs that only exhibit on A64FX and not Intel/AMD supercomputers. Posted May 3, 2024 14:22 UTC (Fri) by anton (subscriber, #25547) [Link] (1 responses) Posted May 3, 2024 15:31 UTC (Fri) by farnz (subscriber, #17727) [Link] In order: On the other hand, your assertion, which is completely unbacked, is that if A64FX has chosen TSO, it must have done so because TSO and weak memory ordering has the same performance; and yet on every CPU out there that offers both options, including the latest Apple Silicon devices and previous Fujitsu SPARC64 CPUs, TSO has a performance penalty compared to the processor's relaxed memory ordering. However, given that A64FX is swimming against the stream in HPC (most HPC systems now use GPUs for the compute elements, with CPUs there to manage the GPUs, where Fujitsu are using CPUs without GPUs), it is entirely plausible that they chose to minimise risk of bad outputs in preference to extracting the last few percent of performance. After all, being 4th instead of 1st in the Top500 list is preferable to being 4th but having to withdraw papers based on A64FX computations because they're demonstrably wrong; it's also possible that Fujitsu had actual data from their SPARC64 XII machines that showed that people ran in TSO mode rather than doing the work to get the last few % of performance in RMO mode. Posted Apr 28, 2024 18:07 UTC (Sun) by Hello71 (guest, #103412) [Link] (5 responses) Fundamentally, CPU emulation is a heavy compatibility mode. A reasonable argument could, and has endlessly been made that Wine and other emulators are detrimental to the long-term success of portable applications, but I see no principle why Wine, QEMU, ADDR_LIMIT_32BIT, and ADDR_NO_RANDOMIZE should be acceptable but PR_SET_MEM_MODEL_TSO shouldn't. Posted Apr 29, 2024 7:27 UTC (Mon) by pm215 (subscriber, #98099) [Link] (3 responses) Posted Apr 30, 2024 6:34 UTC (Tue) by linusw (subscriber, #40300) [Link] (2 responses) Posted Apr 30, 2024 15:53 UTC (Tue) by mb (subscriber, #50428) [Link] (1 responses) Posted May 2, 2024 14:53 UTC (Thu) by pm215 (subscriber, #98099) [Link] Posted Apr 29, 2024 20:33 UTC (Mon) by justincormack (subscriber, #70439) [Link] Posted Apr 30, 2024 12:27 UTC (Tue) by wtarreau (subscriber, #51152) [Link] (3 responses) If I had had this prctl available, what would have happened ? Very easy, looking at the random crashes, Google would have directed me to other people solving this problem by placing this prctl() in the startup chain and voil√†! Not only the code would never have been fixed, but it would have remained suboptimal and I would not have learned anything about my bad practice. Thus I'm really glad this prctl didn't exist! Also there are other problems: it's not just a matter of process, it's a matter of code. Programs are dynamically linked with external libraries nowadays. What if a library was designed with TSO in mind and not the rest of the code ? How will one detect that the whole code needs to be protected ? IMHO that's more of an ABI problem than anything else, and if something had to be done, it should be by claiming to be a different architecture variant so that it is the kernel that recognizes a binary as working in this or that mode, and that programs can only link with compatible libraries. This way it still allows to port existing code using that explicit architecture variant, but it also imposes it on the whole ecosystem (the set of dependencies), just like some use compat32 for example. This would void the risk that users start to randomly stuff that into their programs to solve bugs, and make sure that it's only used when there is a compelling reason to do so. Posted May 1, 2024 3:02 UTC (Wed) by gmatht (subscriber, #58961) [Link] Posted May 4, 2024 7:38 UTC (Sat) by DemiMarie (subscriber, #164188) [Link] For emulation, one can‚Äôt. And without TSO, x86 can‚Äôt be emulated efficiently on Arm. Posted May 5, 2024 1:55 UTC (Sun) by jubal (guest, #67202) [Link] Posted Apr 30, 2024 13:15 UTC (Tue) by farnz (subscriber, #17727) [Link] Would it be workable to say that this state is per-thread, and that a thread in TSO mode cannot make syscalls other than to disable TSO mode? For an emulator like FEX, this is perfectly reasonable; emit "switch to TSO mode", emit your JITted code, and at the end of a JIT block (when you're about to go back to Arm native code such as a FEX ThunkLib, or making a syscall), emit "switch out of TSO mode". It has a small performance hit (since you need to re-request TSO mode at each syscall boundary), but hopefully the advantage of running the core in TSO mode outweighs the hit from having to switch back and forth. However, this isn't usable as a "quick hack" for applications that didn't take the memory model into account - you'd have to patch up every place that makes a syscall to switch out of TSO mode, make the syscall, and switch back, else it'll crash. If you do this, it's obvious looking at your code that you've got an evil hack in place, and thus clear that you didn't think about the impact. Am I missing something critical here? Posted May 3, 2024 12:47 UTC (Fri) by smitty_one_each (subscriber, #28989) [Link] (I'll show myself out.) &lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head/&gt; It's plenty dynamic; enabling it for a virtual machine (only) is entirely doable. A similar approach (mentioned in the article) was discussed for the other patch set. &lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;quote&gt; TSO-enabled threads must run on performance cores &lt;/quote&gt;&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;lb/&gt; I understand the objection! There's plenty of other examples of bad code that deserves to fail on systems that do things correctly: turning memory overcommit on, for instance, encourages a whole lot of sloppy programming practices that aren't portable to other OSes. Or trying to play sound in the Flash plugin on glibc circa 2008 and expecting it to not come out garbled - who does that? Also nobody on Linux should expect their CPU scheduler to keep their cores 100% loaded, or their disk scheduler to let them do literally anything else with the system while writing to a USB stick; I'm sure they'll revert those patches real soon now and we can go back to pretending we're a Real UNIX‚Ñ¢.&lt;lb/&gt; &amp;lt;/s&amp;gt;&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;lb/&gt; Wol&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;lb/&gt; Wol&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head/&gt; Yes, weak ordering is not an architectural feature, it's a lack of a defined feature. An implementation that provides stronger ordering still satisfies the architectural requirements. And unlike, say, an additional instruction, where the architecture actually requires a specific behaviour (typically an illegal instruction exception) when the encoding for the new instruction is encountered, but the implementation with the additional instruction behaves differently, a stronger ordered implementation satisfies the (lack of) architectural requirements of the weakly ordered architecture. Another way of looking at it is that a program written for a weakly ordered architecture will behave as intended on an implementation with stronger ordering, so there is no test to say that the strongly ordered implementation does not satisfy the requirements of the architecture. &lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head/&gt; You are too lenient on the CPU designers here. They are prone to throw problems over the wall to the software people. E.g., Alpha gave use imprecise exceptions "because of performance". But IA-32 implementations with OoO execution gave us better performance and precise exceptions (and OoO implementations would not become faster if the requirement for precise exceptions was dropped). &lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head/&gt; My understanding of AMD64 is that the regular memory accesses (those in the ordinary mov instructions and that are part of load-and-operate and RMW instructions) behave according to TSO. There are some special instructions with worse semantics, but you have to choose them to be bitten by them. So it's not bad. If you don't want to be bitten, don't use these instructions. And, concerning farnz' comment, the same goes for compilers: avoid those that provide worse memory ordering than can be had on the hardware the code is running on. &lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;quote&gt;There is no one right choice - "100% ordered all the time" just isn't feasible with concurrent execution with shared-anything&lt;/quote&gt; What evidence do you have for that? Is it the same evidence that the Alpha people gave for rejecting byte and word accesses (which they added to the architecture later) and for rejecting precise exceptions? &lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head/&gt; There are many unstated (and unsupported) and some stated (but also unsupported) assumptions in your postings, among them: &lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;list/&gt; I think that most of these assumptions are not plausible or outright wrong. &lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head&gt;personality(2) already exists&lt;/head&gt;&lt;head&gt;personality(2) already exists&lt;/head&gt;&lt;head&gt;personality(2) already exists&lt;/head&gt;&lt;lb/&gt; Personally I can live with this, but it's probably not good for QEMU.&lt;lb/&gt; https://lore.kernel.org/linux-fsdevel/20201117233928.2556...&lt;head&gt;personality(2) already exists&lt;/head&gt;&lt;head&gt;personality(2) already exists&lt;/head&gt;&lt;head&gt;personality(2) already exists&lt;/head&gt;&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head/&gt; this is going to be used on apple's processors anyways, because it's required to run x86 emulation efficiently; the question is whether it's going to be managed where it should (in the kernel), or ‚Äì if the architecture maintainers manage to convince linus that he shouldn't be able to run steam and windows games efficiently on his apple laptop when running mainline kernel ‚Äì outside of the kernel tree. &lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46546322</guid><pubDate>Thu, 08 Jan 2026 20:54:30 +0000</pubDate></item><item><title>Show HN: A geofence-based social network app 6 years in development</title><link>https://www.localvideoapp.com</link><description>&lt;doc fingerprint="1d0b4bdc05ffa601"&gt;
  &lt;main&gt;
    &lt;p&gt;top of page&lt;/p&gt;
    &lt;head rend="h1"&gt;A new way to connect with the world&lt;/head&gt;
    &lt;head rend="h1"&gt;Chat with users within perimeters that you load&lt;/head&gt;
    &lt;head rend="h1"&gt;Each perimeter has chat rooms and can be set to any dimensions&lt;/head&gt;
    &lt;head rend="h1"&gt;Contribute to chat rooms when you are within the perimeter&lt;/head&gt;
    &lt;head rend="h1"&gt;See what people are talking about in any area of the world&lt;/head&gt;
    &lt;head rend="h2"&gt;Note: App coming early 2026&lt;/head&gt;
    &lt;head rend="h2"&gt;Please read my full story on LinkedIn.&lt;lb/&gt; (If you don't have a LinkedIn, you can see a screenshot of my profile here.)&lt;/head&gt;
    &lt;head rend="h2"&gt;The Google Play button below is a link to the old app, LocalVideo, which this app was created from.&lt;/head&gt;
    &lt;p&gt;Features&lt;/p&gt;
    &lt;head rend="h2"&gt;Features&lt;/head&gt;
    &lt;head rend="h2"&gt;Create perimeters&lt;/head&gt;
    &lt;head rend="h2"&gt;Create your own perimeters if you need to cover a more specific area.&lt;/head&gt;
    &lt;head rend="h2"&gt;Anonymous mode&lt;/head&gt;
    &lt;head rend="h2"&gt;Use the app in anonymous mode (no sign up required).&lt;/head&gt;
    &lt;head rend="h2"&gt;Place search&lt;/head&gt;
    &lt;head rend="h2"&gt;Search for any place in the world by name to quickly load a perimeter around it.&lt;/head&gt;
    &lt;head rend="h2"&gt;Top chat rooms&lt;/head&gt;
    &lt;head rend="h2"&gt;View a list of the top chat rooms globally.&lt;/head&gt;
    &lt;head rend="h1"&gt;Place-based heat map system&lt;/head&gt;
    &lt;head rend="h1"&gt;Users can select their favorite places which then show up in a heat map. These places will also be in list form so you can talk about them in your chats!&lt;lb/&gt; If a place geofence does not exist, you can create one using the built-in geofence creator.&lt;/head&gt;
    &lt;p&gt;bottom of page&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46546349</guid><pubDate>Thu, 08 Jan 2026 20:56:32 +0000</pubDate></item><item><title>Mux (YC W16) is hiring a platform engineer that cares about (internal) DX</title><link>https://www.mux.com/jobs</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46546413</guid><pubDate>Thu, 08 Jan 2026 21:01:36 +0000</pubDate></item><item><title>SQL Studio</title><link>https://sql.studio/</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=46546419</guid><pubDate>Thu, 08 Jan 2026 21:02:19 +0000</pubDate></item><item><title>Richard D. James aka Aphex Twin speaks to Tatsuya Takahashi</title><link>https://web.archive.org/web/20180719052026/http://item.warp.net/interview/aphex-twin-speaks-to-tatsuya-takahashi/</link><description>&lt;doc fingerprint="363a18141fe60bbb"&gt;
  &lt;main&gt;
    &lt;p&gt;Richard D. James: I really enjoyed working on this with you. I know I only joined the project near the end, but I found it really exciting. Like a proper job, ha.&lt;/p&gt;
    &lt;p&gt;Tatsuya Takahashi: Richard, it was amazing working with you on the monologue. And now to be interviewed by you?!? That's crazy. But also a lot of fun. The monologue was also the last Korg synth that I was involved with directly, so I guess it's a nice conclusion to things.&lt;/p&gt;
    &lt;p&gt;RDJ: It is now the only synth on the market currently being made to have full microtuning editing, congratulations!&lt;/p&gt;
    &lt;p&gt;TT: Thanks! But it was completely because of you that we included microtuning. If you hadn't insisted on it, I definitely wouldn't have discovered how powerful it was. Did you ever have a moment of realisation, or some kind of trigger that made you discover microtuning?&lt;/p&gt;
    &lt;p&gt;RDJ: The first thoughts that I had about tuning in general happened with my early noodlings on a Yamaha DX100, one of the first synths I saved up for. I remember looking at the master tuning of 440 Hz and thinking I would change it, for no other reason apart from it was set by default to that frequency and that it could be changed.&lt;/p&gt;
    &lt;p&gt;I just used to select a single note, adjust the master tuning of it to taste and then base the whole track around that, something I‚Äôve done ever since, just intuition and maybe a bit of rebelliousness. It‚Äôs very simple, but do you want your music to be based on an international standard or on what you think sounds right to you?&lt;/p&gt;
    &lt;p&gt;I‚Äôve since gone on to learn more about this damn 440 Hz. It was a standard introduced in 1939 by western governments, so I‚Äôm very glad I trusted my instincts. Listening to that other voice is THE most important thing in creativity, whether you‚Äôre an engineer or a musician. Tesla had some important advice on listening to the thoughts from the other. One of the most important inventors ever, but we‚Äôre not taught about him in British schools. Funny that.&lt;/p&gt;
    &lt;p&gt;TT: I don't know why it's thin on the curriculum, but the Tesla coil is definitely amazing. If you modulate the high frequency with audio signals you can play music with plasma ‚Äì that's super cool. I will read up on him though, cos I don't know much about his life and thinking.&lt;/p&gt;
    &lt;p&gt;RDJ: An interesting ‚Äúnote‚Äù: I‚Äôve just been reading a book on electronic instruments published in the 1940‚Äôs and it says that 440 Hz was transmitted over the radio on different frequencies 24 hours a day and others between midnight and 2 in the afternoon, ha, so you could tune your instruments and be well behaved or calibrate your lab equipment to it.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;It‚Äôs very simple, but do you want your music to be based on an international standard or on what you think sounds right to you?&lt;/p&gt;RICHARD D. JAMES&lt;/quote&gt;
    &lt;p&gt;But I‚Äôve also read studies from the old Philips laboratories in the Netherlands that show orchestras average deviation from 440 Hz was measured over many concerts and was seen to differ by a few Hz, usually slightly below. Pretty anal. Some people obviously really cared that 440 Hz was being adhered to in practice.&lt;/p&gt;
    &lt;p&gt;Why 440 Hz was chosen in the first place is another interesting story, but looking at the resonances of water and sound is a great place to start, or read up on cymatics. If you aren‚Äôt already familiar with it, that is.&lt;/p&gt;
    &lt;p&gt;TT: So many things are standardised that you don't really think about because they were there before you started using it. 440 Hz was brought about to standardise the way people play together and, yeah, someone can bring a guitar to a piano and it would work together because of that standard.&lt;/p&gt;
    &lt;p&gt;It's like how a green light means you can cross the road or if you shake your head sideways it means no. Those two standards will help you through life in many places around the world. But it's dangerous to enforce standards in creativity. I have a son who's started school in Japan, where every kid will paint the sun red. Now that is some fucked up standardisation! Just really messed up on so many levels.&lt;/p&gt;
    &lt;p&gt;Anyway, I'm not going into that whole 432 Hz vs 440 Hz debate. (BTW: I absolutely love cymatics and I've done some nice workshops for kids with it.) But I will say different frequencies sound different, so why not use that in your music? You got to use whatever feels right and the monologue let's you do exactly that with pitch.&lt;/p&gt;
    &lt;p&gt;RDJ: Yep.&lt;/p&gt;
    &lt;p&gt;TT: Talking of standards, the sample rate of 48 kHz is another one for sampling and signal processing, but the volca sample uses a weird one at 31.25 kHz. Purely because of technical constraints, but I was thinking that might be part of the reason you liked it so much, because the different sample rate gives it a unique sound.&lt;/p&gt;
    &lt;p&gt;RDJ: Haha, yes, it was pretty much the first thing I noticed. Yeah, I thought the 48 kHz, was based on the Nyquist Theorem. I think it‚Äôs double what humans can apparently hear or something, which is another weird one. I don‚Äôt know how anybody worked out humans only hear to 20 kHz. I mean even if you can‚Äôt hear above 20 kHz, it doesn't mean that your body doesn't feel it. You don‚Äôt just experience sound through your eardrums. A good example of this is listening to a recording of your own voice. To almost everyone apart from maybe the most narcissistic, it always sounds weird/thinner/smaller, as you don‚Äôt feel the vibration of your chest and body. There are other reasons of course but that‚Äôs one for sure. Anyway, I‚Äôm into the extremes of the audio spectrum, ultra clarity ‚Äôn‚Äô all but I probably prefer fucked-muffled/lo-bit/‚Äô70s sound more, ha!&lt;/p&gt;
    &lt;p&gt;TT: Oh, and when something defies the standard ‚Äì I just remembered the first time I played a Yamaha SK-10, the faders were all upside down, like max was downwards, even on the volume. I didn't know what was going on and it threw me off at first, but it's actually a bit fun like that and you soon realise it all comes from organ drawbars.&lt;/p&gt;
    &lt;p&gt;RDJ: I never played the SK-10, but these Calrec mixers I use are like that also, the faders are backwards. There is a little dip switch inside to change it, but I think they have them like that for TV/broadcasting, coz if someone falls asleep at the desk they don‚Äôt want them to push all the faders up and distort two million TVs at once‚Ä¶ Not surprising they have this safeguard considering how skull numbingly boring most TV is.&lt;/p&gt;
    &lt;p&gt;TT: Right!! Yeah, but there is a certain feeling to pulling rather than pushing. It's like how an orgasm is "coming" in English, but it's ‚Äúgoing‚Äù [iku] in Japanese.&lt;/p&gt;
    &lt;p&gt;RDJ: Never thought of it like that.&lt;/p&gt;
    &lt;p&gt;TT: I mean, written text in Japanese was traditionally vertical. Although now a lot is westernised and horizontal.&lt;/p&gt;
    &lt;p&gt;RDJ: Ah, that‚Äôs kinda sad‚Ä¶ So traditional Japanese text is like trackers and now it‚Äôs going like Cubase! :)&lt;/p&gt;
    &lt;p&gt;TT: I sometimes wonder what Japanese synths would have looked like if they didn't copy Moog in the ‚Äô70s. You've got to think about what is convention and what is really a good design.&lt;/p&gt;
    &lt;p&gt;RDJ: I‚Äôve got one Japanese keyboard, Suzuki, which has got some Japanese tunings built in and a little string on one end that you can pluck. It sounds really nice as well. It also has some good Japanese percussion and MIDI. I don‚Äôt think it‚Äôs very well known.&lt;/p&gt;
    &lt;p&gt;I wish faders were curved horizontally and vertically, so you could make them like a double helix that go over and under each other, hehe. Could do it with an augmented reality UI I guess.&lt;/p&gt;
    &lt;p&gt;TT: Now that could be cool (if I'm imagining it right)! I've seen rotation sensors on the camera lens focus that work like faders on a curved surface and really thin. That could do it.&lt;/p&gt;
    &lt;p&gt;RDJ: Later on when I got an SH-101, I realised its tuning wasn't like the DX100 at all. It was based on 1v/octave and was supposed to be equal temperament but because of the nature of analogue, it really wasn‚Äôt and I REALLY loved that and how it layered with the frozen 12TET of the DX100.&lt;/p&gt;
    &lt;p&gt;I recently made a tuning on the monologue that I matched to an improperly calibrated SH-101 that I was fond of. I tried at first to do this using formulas inside Scala, but it's impossible to represent this accurately with simple maths, Scala can‚Äôt deal with these types of tunings unless it‚Äôs a keyboard map tuning file. This ‚Äúbad‚Äù tuning is really great when I apply it to a precisely tuned digital synth that has full microtuning capabilities. It‚Äôs top making a digital synth sound like an out of tune 101! :)&lt;/p&gt;
    &lt;p&gt;TT: Yeah, I think it's really telling of the age we live in when you get a knob like "SLOP" on the new Prophet that makes pitch inconsistencies a programmable parameter. On one hand, you think that the level of control is great, but on the other it feels weird to deliberately degrade something that's stable. Especially if you're a young engineer striving to design something to be close as possible to perfection, it can be hard to grasp. The best lesson about this came from Mieda ‚Äì my hero at Korg. When he looked at my first synth schematic, he told me, ‚ÄúTakahashi-kun, your circuits are functional, but they are not musical. Musical instruments do not need perfect waveforms and correct operating points. You need to use the transistor for what it is. As long as it sounds good, it‚Äôs OK.‚Äù&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;WHEN [MIEDA] LOOKED AT MY FIRST SYNTH SCHEMATIC, HE TOLD ME, ‚ÄúTAKAHASHI-KUN, YOUR CIRCUITS ARE FUNCTIONAL, BUT THEY ARE NOT MUSICAL. MUSICAL INSTRUMENTS DO NOT NEED PERFECT WAVEFORMS AND CORRECT OPERATING POINTS. YOU NEED TO USE THE TRANSISTOR FOR WHAT IT IS. AS LONG AS IT SOUNDS GOOD, IT‚ÄôS OK.&lt;/p&gt;TATSUYA TAKAHASHI&lt;/quote&gt;
    &lt;p&gt;RDJ: I was going to ask you about SLOP, as you brought that up before in some old emails. I get you now. I mean, yeah, if it just sounds good in the first place then you don‚Äôt need that option, but I guess some people like their Osc‚Äôs drifty and others not so. It changes with the context I guess. Also, if you‚Äôre doing FM you might want to keep them dead on, and for analogue lead sounds, really drifty. Anyway I think I mentioned it before, but the drift on the monologue sounds REALLY nice. It seems to move, but then never go out. Care to explain? Sounds to me like it gets reset/synced at some point, but I‚Äôm probably wrong, haven't studied it in depth, just listened. Reminds me a bit of Arp oscillators, which have really nice driftyness, prob my faves! :)&lt;/p&gt;
    &lt;p&gt;TT: That's bang on! So same thing in the minilogue and the volcas too: the oscillators are re-tuned when they're not being used. I'm super glad you like it though because this is such a subjective thing. The autotuning was done in a way that felt nice to me, so it was a really subjective thing and you can‚Äôt present a report to convince others that it was OK. At least now I can say RDJ said it was alright!&lt;/p&gt;
    &lt;p&gt;RDJ: I‚Äôd like to talk more about this 1v/octave, but that‚Äôs for another time. But, anyway, getting back to the question, I was always interested in sound and how it affected me, especially the tuning. It wasn't until my *Selected Ambient Works Vol. II* album that I actually made my own full custom tunings, although there were a few scattered things before that.&lt;/p&gt;
    &lt;p&gt;I‚Äôve got a slightly weird balance thing going on and getting the tuning ‚Äúright‚Äù sometimes makes the balance thing less weird for me. It‚Äôs a longer story though.&lt;/p&gt;
    &lt;p&gt;TT: Yeah, I think I read somewhere about how humans normally hear pitches differently in the left and right ear and that you don't have that. That is super interesting.&lt;/p&gt;
    &lt;p&gt;RDJ: Because we made it very intuitive to edit the tuning tables, I would actually just buy this synth only for that feature alone. When the export is implemented, it can be the central hub of either complete table creation or just to tweak existing imported Scala files, etc.&lt;/p&gt;
    &lt;p&gt;TT: Yeah, absolutely. I would definitely download the monologue librarian because you can import and export Scala files easily with that. Hopefully other manufacturers will join the club.&lt;/p&gt;
    &lt;p&gt;The intuitive interface was pretty much all your idea, so a great job on that. I think your idea for the interface came from when you got your Chroma modded for microtuning. Have you modded a lot of synths for this functionality?&lt;/p&gt;
    &lt;p&gt;RDJ: That‚Äôs right, I burned my own custom O.S. Eproms for the Chroma, which enables full micro tuning and editing and that‚Äôs what the monologue editor was based on. I‚Äôve got a good list of hardware and software now that can do it. It‚Äôs been a long haul and involved hassling a lot of people, but it is now finally possible with quite a bit of equipment.&lt;/p&gt;
    &lt;p&gt;I‚Äôve generally received really good responses from engineers and programmers. I‚Äôve contacted around 50 different people/companies in the last ten years. Many weren‚Äôt even aware that all their equipment and programs were adhering to a standard that was devised hundreds of years ago.&lt;/p&gt;
    &lt;p&gt;Same goes for a lot of electronic musicians, this is quite surprising for electronic music, which supposedly is forward-thinking and futuristic, but most people have since told me how fascinating they have found the subject once they realised it *was* a subject!&lt;/p&gt;
    &lt;p&gt;I know microtuning is much more useful on polyphonic keyboards, but it‚Äôs still very usable on monophonic instruments and, again, it can be used in the future to create tuning tables that can be used in other Scala-compatible polyphonic synths.&lt;/p&gt;
    &lt;p&gt;TT: Well, my initial impression was that microtuning is a really niche thing that wouldn't be needed for a mass market synth, especially a monophonic one, but if you try shifting the tuning while running a sequence, you can hear that it gives it another dimension even if it‚Äôs subtle. I'm not super-sensitive to pitch or anything, but you can still hear it change. To me, it feels like casting light on a rough surface and seeing different patterns as you move the light. So it was really important to have the easy scale edits you can do on the fly. Scala is great, it's super flexible, but it can be daunting to use and you won't get the real-time interaction, so I hope the monologue gets more people into this stuff.&lt;/p&gt;
    &lt;p&gt;RDJ: I really like your light analogy, that‚Äôs great. Yep, on a monophonic instrument, what you just described will be more pronounced if you use a delay with plenty of feedback or reverb, so you can hear the differently tuned notes overlap each other.&lt;/p&gt;
    &lt;p&gt;Scala is deep, very deep, but some things are very quick and easy to get going. For instance, you can just type Equal 24 &amp;amp; press the sysex send shortcut and you have a quarter tone tuning in your synth. Scala is only good for non-intuitive tuning creation, purely mathematical. I love this approach, but really prefer making tunings intuitively, note-by-note. When you‚Äôre actually composing something, making them up while you go along, a combination of the two is best for me.&lt;/p&gt;
    &lt;p&gt;TT: I know that you like that Wilsonic app you showed me, which is mainly structured on mathematical relationships of frequencies, but you've also mentioned using a lot of trial-and-error. Do you have a method to your microtuning?&lt;/p&gt;
    &lt;p&gt;RDJ: Yes, many. For instance, on the Chroma I like holding down one key, pressing another key and then tuning the second key in relation to the first, sometimes making two extremely different frequency combinations, like something very low and extremely high at the same time and maybe a group of these dual combos only existing in the top octave of the keyboard map, the rest being another tuning or multiple tunings, all in one tuning table.&lt;/p&gt;
    &lt;p&gt;It‚Äôs something I never saw in anyone else‚Äôs tunings, combining several tuning tables within one map, so that‚Äôs one of my little inventions I guess, as I rarely used the full range of 127 notes in one tuning within one track. monologue can tune four notes at a time which we planned. It‚Äôs a different approach again and something I look forward to experimenting with more.&lt;/p&gt;
    &lt;p&gt;TT: Here are five short tracks you made with custom scales. Could you explain how you came up with the scales?&lt;/p&gt;
    &lt;p&gt;RDJ: I forgot which tunings they used, I‚Äôve got so many floating around in folders on the computer and in hardware. I didn‚Äôt make any notes. I think they might have been ones that I made in Scala and then tweaked on the monologue, most likely.&lt;/p&gt;
    &lt;p&gt;TT: If you could share the tuning files that you created, that would be great too!&lt;/p&gt;
    &lt;p&gt;RDJ: Yes, I‚Äôve got loads saved and loads lost. I‚Äôve never been a saver. I do save more things these days, getting older or something, but still love to use new sets of rules for every set of new tracks. Also I‚Äôve got to say again many thanks for that lovely MIDI tuning box you made me for the minilogue!&lt;/p&gt;
    &lt;p&gt;TT: No problem! That was an eye-opener for all of us. [For the readers: Richard asked me for microtuning on our synths and since, at the time, we thought it wasn't something we would put on a production model, we made a custom little tuning tool. Fellow engineer Kazuki Saita and I made a MIDI thru box that could load custom scales. Any MIDI coming in would be transposed by note and cent (using pitch bend) and so you could get microtuning on any mono synth.] When we were testing that box, Saita and I were blown away. I mean, sequencing on a simple step sequencer like in the monologue can be a bit rigid, but messing with the tuning really opens it up. It basically redefines the keyboard. We were messing around with some subtle stuff and more extreme ones like octaves split into 50 intervals and playing with the arpeggiator. It was crazy and that's when we decided we should put it on the next synth.&lt;/p&gt;
    &lt;p&gt;RDJ: Yes, great! Arpeggiators and microtunings can be a very nice mix. We should include a picture of that box, I‚Äôve got one here if you don‚Äôt.&lt;/p&gt;
    &lt;p&gt;TT: We should! Don't have one handy, would you be able to snap a photo?&lt;/p&gt;
    &lt;p&gt;RDJ: Attached it!&lt;/p&gt;
    &lt;p&gt;TT: Cheers! Wood cheeks for the Cirklon. Nice.&lt;/p&gt;
    &lt;p&gt;RDJ: I think the monologue is very nice looking, small, very cute and very capable. At first I thought, ‚ÄúOh, it hasn‚Äôt got this, it hasn‚Äôt got that, etc. etc.‚Äù But I very quickly realised you have turned these limitations into advantages, which is really quite something special. I really mean that. The lack of extensive features makes the whole thing much more speedy to work with.&lt;/p&gt;
    &lt;p&gt;TT: That's got to be the best compliment. And it's a way of thinking that runs through all the synths I've worked on, from the volcas and monotrons to the monologue. I think with electronic instruments we've got to a point where software can do most things. But I'm a fan of gear where less is more ‚Äì where the simplest controls can give you the most creative freedom.&lt;/p&gt;
    &lt;p&gt;RDJ: Yes, I like this approach. It‚Äôs true, I do it with modular setups as well. I‚Äôm lucky to have loads of modular gear but I prefer to make small systems now and leave everything else in another room where I just try things out before committing them to a more thought out config.&lt;/p&gt;
    &lt;p&gt;Of course us musicians always look at something new and we see if it does what we expect it to. And this is OK. But we shouldn‚Äôt overlook something before actually trying it out, try and get into the head of the designer first. I try and do this. It‚Äôs difficult sometimes to push your ego and expectations out of the way for a while, but if we don‚Äôt do this we won‚Äôt learn anything new. That‚Äôs not to say that every designer‚Äôs head is worth getting into, but we gotta give it a go sometimes.&lt;/p&gt;
    &lt;p&gt;TT: This is exactly the reason I really enjoyed working with you. I'd send you a prototype and a day later you'd be sending me a dozen emails about how the drive circuit actually controls gain and dry/wet at the same time. Or how some menu option wasn‚Äôt working completely as intended. You would give everything a chance. You went through every single menu option and went after some easter eggs, like finding CC34 VCO1 pitch! In fact, you were the best ever beta tester. Guess you wouldn't be after a day job tho...&lt;/p&gt;
    &lt;p&gt;RDJ: *blush* Some examples of this: When I first checked out the volca sample, the lack of velocity response had me scratching my head, but when I realised how it handled it with motion recording of the level control, it was actually loads more fun and SO much faster to program! It‚Äôs such a great little idea, I really love it, way more intuitive. I‚Äôve started doing it this way on the Cirklon now sometimes.&lt;/p&gt;
    &lt;p&gt;TT: Yeah, so you're a huge fan of the Cirklon, which you used for "korg funk 5." Could you tell us how that track was put together?&lt;/p&gt;
    &lt;p&gt;Here's the gear list you sent me:&lt;lb/&gt; Korg Monologue x3&lt;lb/&gt; Korg MS-20 kit&lt;lb/&gt; Korg Poly-61M&lt;lb/&gt; Korg Volca keys&lt;lb/&gt; Korg Volca beats&lt;lb/&gt; Korg Volca sample&lt;lb/&gt; Korg Minilogue&lt;lb/&gt; My son on vox&lt;/p&gt;
    &lt;p&gt;I was blown away by this and really really touched. I don't think there is another track out there using so much of the gear I worked on! Also, can you touch on the processing that went on the sounds, cos I can tell there's a lot going on.&lt;/p&gt;
    &lt;p&gt;RDJ: That‚Äôs so nice to hear‚Ä¶ It was really top making some tracks with only Korg gear. I‚Äôm a secret nerd-fan of synth demos, mainly vintage ‚Äô80s ones currently! Some amazing music has been made as equipment demos, unsung heroes. I collect synth demos. Well, ones that I like. It‚Äôs kind of an unclassified music genre, so doing these tracks for you and Korg was a natural thing for me. I also really like picking certain combinations of gear. That is endlessly fascinating.&lt;/p&gt;
    &lt;p&gt;The Volca beats I used, I did the snare mod but used the mix output, so I treated all the sounds with the same treatment, I think I sent you the full list‚Ä¶ looks it up‚Ä¶ OK, here it is.&lt;/p&gt;
    &lt;p&gt;volca beats &amp;gt; Skibbe 736-5 mic pre [nice low mid sound] &amp;gt; BAC 500 compressor &amp;gt; RTZ PEQ1549 [this is based on my fave eq, I‚Äôve got some Calrec originals as well, standard circuit design but not standard sound! ] &amp;gt; Calrec minimixer&lt;/p&gt;
    &lt;p&gt;Monologue [main riff] &amp;gt; blonder tongue EQ [i love these eq‚Äôs, hardly anyone has heard of them]&lt;/p&gt;
    &lt;p&gt;TT: Any chance you could share the tracks separately? There might be something we could do with that and a lot of people will be interested in seeing how the different synths sound soloed. Only if you're up for it of course!&lt;/p&gt;
    &lt;p&gt;RDJ: I would if I had them, but I never save individual tracks. I‚Äôm trying to get into the habit of that soon. I just recorded that down to the Sound Devices 722.&lt;/p&gt;
    &lt;p&gt;TT: Ah shame! But you know that was the other great thing ‚Äì that the track was done totally sequenced on the Cirklon and recorded in one take.&lt;/p&gt;
    &lt;p&gt;RDJ: I was thinking a while back on different ways to visualise the data in the Cirklon. Also with the volca fm, you also managed to turn the lack of velocity per note into a bonus [again], it puts a different slant on it, applying and recording motion velocity on the whole phrase, it works very well.&lt;/p&gt;
    &lt;p&gt;TT: So the volca keyboard is never going to do a great job of sensing velocity and we could have spent a lot more money to make it velocity-sensitive, but then you'd sit there going, "Well, it's too small to play. We need to make it bigger..." So trying to force it to be something it's not is a great way of creating more problems. Much rather turn the game around.&lt;/p&gt;
    &lt;p&gt;RDJ: That‚Äôs a great example of necessity and invention. I was absolutely amazed to find out that it IS actually possible to edit a DX7 voice with great speed from the interface you have designed. I never thought you could do that, but it is and is totally usable. I‚Äôve come up with loads of things on it that I would never have done on a full size DX7. Hats off to Tats!&lt;/p&gt;
    &lt;p&gt;TT: Cheers! So everyone knows the typical DX7 sounds ‚Äì well, the presets anyway ‚Äì and by doing things a bit differently, you can open up so much stuff. Take an organ patch on the volca fm and sequence it normally, but then motion sequence the algorithm and it goes in a completely different dimension. It's a discovery, which is fun. I find a lot of artists are discovery junkies.&lt;/p&gt;
    &lt;p&gt;RDJ: Yes, I think I HAVE to be learning something when making tracks, even if it‚Äôs something very small. If there‚Äôs no learning involved, I wouldn‚Äôt get excited enough to do anything. Great fun being able to take a DX7 in your pocket, love it, ultimate walkman in a way. In fact, one for the future: volca fm with built in MP3 player + radio‚Ä¶ be super lush.&lt;/p&gt;
    &lt;p&gt;TT: Yeah, super great idea! Also if it could tap into some MIDI archives and play them on the FM engine, it would be great.&lt;/p&gt;
    &lt;p&gt;RDJ: Or maybe a pitch tracker from the MP3s! :-)&lt;/p&gt;
    &lt;p&gt;TT: Even better! :) And it can take real time mic input, so people are saying hello to you, but you're just hearing bells or something.&lt;/p&gt;
    &lt;p&gt;RDJ: Yes, recently I was offering up ideas to a talented coder friend on an app that uses evolutionary/genetic synthesis to try and resynthesise audio/live audio into DX7 patches. It sounds really cool. He‚Äôs working on making it a standalone app on Raspberry Pi, and it is based on some vintage code by Andrew Horner. Kyma also used his code for their GA synthesis. Chuck that in there while we‚Äôre at it.&lt;/p&gt;
    &lt;p&gt;TT: Got to say it's pretty funny getting a consumer product idea from you. Haha!&lt;/p&gt;
    &lt;p&gt;RDJ: :) I‚Äôm full of ‚Äòem, I‚Äôm like this guy.&lt;/p&gt;
    &lt;p&gt;TT: BAHAHAHHA! Holy crap.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;I think I HAVE to be learning something when making tracks, even if it‚Äôs something very small. If there‚Äôs no learning involved, I wouldn‚Äôt get excited enough to do anything.&lt;/p&gt;Richard D. James&lt;/quote&gt;
    &lt;p&gt;RDJ: How different is the finished monologue to what was designed or what you had in mind?&lt;/p&gt;
    &lt;p&gt;TT: Well, it didn't have microtuning for a start!&lt;/p&gt;
    &lt;p&gt;RDJ: :)&lt;/p&gt;
    &lt;p&gt;TT: When I initially came up with the product plan, it wasn't very detailed. None of my product plans are. Something like: "smaller than the minilogue and monophonic." It's only when you start designing and prototyping that things start to come together. Things like: ‚ÄúWhat kind of filter do we need?‚Äù ‚ÄúDo we need distortion?‚Äù ‚ÄúBattery power would be great!‚Äù&lt;/p&gt;
    &lt;p&gt;RDJ: If there are features that were designed that didn‚Äôt make it, could you tell us about them?&lt;/p&gt;
    &lt;p&gt;TT: Nothing really got properly designed before being ditched. The team is pretty good at putting together test versions where we can just about see if something is going to work before we go to full implementation.&lt;/p&gt;
    &lt;p&gt;In terms of ideas, you had some pretty good ones:&lt;/p&gt;
    &lt;p&gt;I think the team had others like arpeggiator, which is the most obvious one. But we dropped that and added key-trigger sequence instead.&lt;/p&gt;
    &lt;p&gt;RDJ: When or how do you find out that features that were wanted by your team are not going to make it? Is that frustrating?&lt;/p&gt;
    &lt;p&gt;TT: Well, it's not like someone stands there casting their decision on whether something makes it or not. We all try to figure out how it will come together as an instrument, so a single feature might be the focus in a heated discussion, but really it's about the whole thing being coherent but also incoherent and surprising in a good way. Sometimes you need to throw people off what they're expecting to do something interesting. The team was always pretty small, so we could do it without having a draconian decision-making process, but also without it getting too democratic either. We would never ever vote on a feature.&lt;/p&gt;
    &lt;p&gt;RDJ: Would it be possible that Korg could release limited edition and more costly versions of your designs with no corners cut, for us posh musos?&lt;/p&gt;
    &lt;p&gt;TT: Sure, that's definitely a possibility. What's on your wish list?&lt;/p&gt;
    &lt;p&gt;RDJ: Oh dear, that is a big question, I think I‚Äôll have to get back to you on that. Well, those ones above to start with I suppose. :) Do you have a studio at home? Got any pics? Or a description of your setup?&lt;/p&gt;
    &lt;p&gt;TT: I wouldn't say it's a studio, but more of a workshop. I build stuff there for my own live setup, although recently most of it is made up of products I've worked on. One of my favourite things is volca fm going into audio input of monotribe which has been modded so you can kill the VCO. I put on a slow chord progression on the fm and then work a sequence with it with the monotribe. It's actually better if I don't sync the volca fm to the monotribe.&lt;/p&gt;
    &lt;p&gt;RDJ: Nice, I keep meaning to rack up 8 analogue filters to a TX802. Nobody ever made a decent FM synth with analogue filters, there are a few simple FM ones but not 4OP+.&lt;/p&gt;
    &lt;p&gt;TT: My other favourite thing is my speaker system designed by my friends at Taguchi. They're omni-directional and I've been experimenting with the positions. My room is acoustically untreated, but with these speakers you can actually work with the reflections in the room. It's definitely not a typical setup, but it's great because you can pan your instruments around the room and you‚Äôre not glued to a sweet spot between a stereo pair. It's great if you just sequence piano phase on two volcas and offset the BPM and just let it run while the sequence phases in and out. The trick there is actually not to hard pan them, but to leave quite a bit of overlap.&lt;/p&gt;
    &lt;p&gt;RDJ: [*looks at pics*] Great, that is an unusual speaker setup! I‚Äôm a big fan of suspending speakers from the ceiling, the first speakers that I built, I filled with tar and hung them from nylon cords from my bedroom ceiling. Saves space as well. Do you live and breathe Korg, do you get time for anything else, any other hobbies?&lt;/p&gt;
    &lt;p&gt;TT: Don't know if it counts as a hobby, but I really like polyhedra. Maybe that‚Äôs why I like those speakers, since they're great 3D structures hanging off my ceiling. My favourite polyhedron is the dodecahedron and when you make one with wire, it's hard to make it completely regular. But it turns out I actually like the wonky ones better. Anyway, they have a cool name.&lt;/p&gt;
    &lt;p&gt;RDJ: That‚Äôs very nice. I absolutely love geometry, I did a track called ‚ÄúDodeccaheedron,‚Äù a long time ago, one of my fave tracks. I was playing on this spirograph emulator recently. Ha, a 3D one would be really interesting.&lt;/p&gt;
    &lt;p&gt;TT: Oh man, of course you have a track named ‚ÄúDodeccaheedron‚Äù! I wonder if the track had anything to do with the fact I like them now. Bet it did. Spirographs are so cool. Bit like Lissajous ‚Äì could stare at that stuff all day. I really want to get hold of some XY lasers actually and fire some really intense ones. Wish there was a way to do that in 3D.&lt;/p&gt;
    &lt;p&gt;RDJ: I‚Äôve been looking into this recently. :)&lt;/p&gt;
    &lt;p&gt;TT: Maybe you can design some phosphorescent smoke that you could fire lasers into and the lines would stay in the air. That will be so cool. And the smoke particles will move with the bass ‚Äì get some fat bass bins and you would get lines of light vibrating.&lt;/p&gt;
    &lt;p&gt;RDJ: Top idea‚Ä¶ Reminds me of this.&lt;/p&gt;
    &lt;p&gt;TT: Yeah, really. I mean it could be a way of visualising the propagation of sound waves, so maybe a scientific use too. And not just sound waves. It could be used in wind tunnels to study air flow. Are we onto something here?&lt;/p&gt;
    &lt;p&gt;RDJ: Yes.&lt;/p&gt;
    &lt;p&gt;RDJ: What Is Your Dream?&lt;/p&gt;
    &lt;p&gt;TT: Having a good cigarette. When you're having a shit day or you're under a lot of stress, cigarettes taste crap. On the other hand, a cigarette after an amazing experience tastes good. So my dream is to smoke the best cigarette ever. Smoking is a full-stop, a moment of recognition that whatever came before it was real.&lt;/p&gt;
    &lt;p&gt;RDJ: I like that.&lt;/p&gt;
    &lt;p&gt;TT: Bit wanky tho. ;) Getting weird vibes reading back at my answers!&lt;/p&gt;
    &lt;p&gt;RDJ: LOLz&lt;/p&gt;
    &lt;p&gt;TT: Oh well, wrote it once, can't deny it.&lt;/p&gt;
    &lt;p&gt;RDJ: If you could magically create any device, what would it be? I understand if you‚Äôre not allowed to answer this!&lt;/p&gt;
    &lt;p&gt;TT: A time machine, teleportation machine ‚Äì the obvious ones. Or actually a machine where you could have as many parallel existences as you want. So you could be a super-dimensional being encompassing all the different possibilities of yourself. That's what popped into my head, but how self-centred!&lt;/p&gt;
    &lt;p&gt;RDJ: I go to sleep thinking things like this‚Ä¶ Maybe it's a bit like this already! :)&lt;/p&gt;
    &lt;p&gt;TT: Hell yeah. Anyway, that's probably not what you meant. So... a lifelogging device for your musical activities. I was packing up to leave Tokyo and found a bunch of minidiscs of music that I'd forgotten I'd made in my teens and I‚Äôm guessing there would have been a lot more if I knew where my cassettes were. I cringed at most of it, but it's still part of who I am and I can't erase whatever brain patterns I have because of that.&lt;/p&gt;
    &lt;p&gt;RDJ: Yes, bloody right, that would be very useful. One thing I‚Äôd say, though, is I‚Äôve found a lot of artists write off their older work for various personal reasons, while other people won‚Äôt have those associations and just really love what you made.&lt;/p&gt;
    &lt;p&gt;TT: Do you have lost musical moments from the past that you would like to hear again?&lt;/p&gt;
    &lt;p&gt;RDJ: Yes, I think I‚Äôm obsessed with thoughts like this. If you could selectively erase your memory so you could keep experiencing things for the first time, it would be very interesting, although you would get stuck in loops, so you would have to limit it to a certain number of re-experiences, ha! How many future products have you got in your head or on the drawing board?&lt;/p&gt;
    &lt;p&gt;TT: Quite a lot, but not all will be made. We (meaning the team still at Korg) have always got a bunch of ideas up our sleeves, it's just a case of which ones will get made and when.&lt;/p&gt;
    &lt;p&gt;RDJ: Is your job stressful? I imagine it‚Äôs very stressful. What's the most stressful part?&lt;/p&gt;
    &lt;p&gt;TT: Well, the stress was part of the balance, because there's a lot of adrenaline involved in meeting deadlines, starting production and working up to release. Now that I've left that position, I can look back in calm retrospect. I'd say it was quite physical. Kind of like a sport and also quite addictive. But at the same time you can't do it forever. I was also lucky enough to find new possibilities elsewhere, so I stopped before that high pace / full-throttle thing became the only thing I could do. I really did have an amazing time at Korg. I had the best team and I also had a lot of freedom. My decision to leave was really about me than anything to do with my working environment.&lt;/p&gt;
    &lt;p&gt;RDJ: What is your worst fear?&lt;/p&gt;
    &lt;p&gt;TT: Well, doing the same thing over again and then one day realising that's all you can do.&lt;/p&gt;
    &lt;p&gt;RDJ: Yeah, I think we all have to fight against this, especially as you get older. I‚Äôve really been looking at my habits recently and denying them. It feels great if you can manage it. &lt;lb/&gt;I don't understand the economics of getting hardware to market, but I guess it's safe to assume that the company makes more money from releasing new products than it does upgrading old ones. &lt;/p&gt;
    &lt;p&gt;I can‚Äôt help thinking, though, that by continuing to upgrade older products that are still in production, to make them absolutely awesome, would benefit the company in the long-term. Any thoughts about this?&lt;/p&gt;
    &lt;p&gt;TT: That depends how you look at it. You can look at something like the monotribe which we spent a lot of time doing the major update for, which was then soon discontinued. So your initial point might look to hold true. But then you look at the amount we learnt from that update and that we put into the volcas, and then you can say it was worthwhile. I think it's really really important to look back and review past products. Some would benefit from an update, but others are better off redesigned.&lt;/p&gt;
    &lt;p&gt;RDJ: Ok then, well lovely chatting to you as always.. wishing you all the best in your new endeavours, very brave moving yourself to a new country, well done and speak soon.&lt;/p&gt;
    &lt;p&gt;Here‚Äôs a nice link to end with!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46546614</guid><pubDate>Thu, 08 Jan 2026 21:17:26 +0000</pubDate></item><item><title>Iran Protest Map</title><link>https://pouyaii.github.io/Iran/</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=46547303</guid><pubDate>Thu, 08 Jan 2026 22:20:34 +0000</pubDate></item><item><title>Embassy: Modern embedded framework, using Rust and async</title><link>https://github.com/embassy-rs/embassy</link><description>&lt;doc fingerprint="7b05f0bb442a83d4"&gt;
  &lt;main&gt;
    &lt;p&gt;Embassy is the next-generation framework for embedded applications. Write safe, correct, and energy-efficient embedded code faster, using the Rust programming language, its async facilities, and the Embassy libraries.&lt;/p&gt;
    &lt;p&gt;The Rust programming language is blazingly fast and memory-efficient, with no runtime, garbage collector, or OS. It catches a wide variety of bugs at compile time, thanks to its full memory- and thread-safety, and expressive type system.&lt;/p&gt;
    &lt;p&gt;Rust's async/await allows for unprecedentedly easy and efficient multitasking in embedded systems. Tasks get transformed at compile time into state machines that get run cooperatively. It requires no dynamic memory allocation and runs on a single stack, so no per-task stack size tuning is required. It obsoletes the need for a traditional RTOS with kernel context switching, and is faster and smaller than one!&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Hardware Abstraction Layers&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;HALs implement safe, idiomatic Rust APIs to use the hardware capabilities, so raw register manipulation is not needed. The Embassy project maintains HALs for select hardware, but you can still use HALs from other projects with Embassy.&lt;/item&gt;
          &lt;item&gt;embassy-stm32, for all STM32 microcontroller families.&lt;/item&gt;
          &lt;item&gt;embassy-nrf, for the Nordic Semiconductor nRF52, nRF53, nRF54 and nRF91 series.&lt;/item&gt;
          &lt;item&gt;embassy-rp, for the Raspberry Pi RP2040 and RP23xx microcontrollers.&lt;/item&gt;
          &lt;item&gt;embassy-mspm0, for the Texas Instruments MSPM0 microcontrollers.&lt;/item&gt;
          &lt;item&gt;esp-rs, for the Espressif Systems ESP32 series of chips. &lt;list rend="ul"&gt;&lt;item&gt;Embassy HAL support for Espressif chips, as well as Async Wi-Fi, Bluetooth, and ESP-NOW, is being developed in the esp-rs/esp-hal repository.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
          &lt;item&gt;ch32-hal, for the WCH 32-bit RISC-V(CH32V) series of chips.&lt;/item&gt;
          &lt;item&gt;mpfs-hal, for the Microchip PolarFire SoC.&lt;/item&gt;
          &lt;item&gt;py32-hal, for the Puya Semiconductor PY32 series of microcontrollers.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Time that Just Works - No more messing with hardware timers. embassy_time provides Instant, Duration, and Timer types that are globally available and never overflow.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Real-time ready - Tasks on the same async executor run cooperatively, but you can create multiple executors with different priorities so that higher priority tasks preempt lower priority ones. See the example.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Low-power ready - Easily build devices with years of battery life. The async executor automatically puts the core to sleep when there's no work to do. Tasks are woken by interrupts, there is no busy-loop polling while waiting.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Networking - The embassy-net network stack implements extensive networking functionality, including Ethernet, IP, TCP, UDP, ICMP, and DHCP. Async drastically simplifies managing timeouts and serving multiple connections concurrently.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Bluetooth&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;The trouble crate provides a Bluetooth Low Energy 4.x and 5.x Host that runs on any microcontroller implementing the bt-hci traits (currently &lt;code&gt;nRF52&lt;/code&gt;,&lt;code&gt;nrf54&lt;/code&gt;,&lt;code&gt;rp2040&lt;/code&gt;,&lt;code&gt;rp23xx&lt;/code&gt;and&lt;code&gt;esp32&lt;/code&gt;and&lt;code&gt;serial&lt;/code&gt;controllers are supported).&lt;/item&gt;
          &lt;item&gt;The nrf-softdevice crate provides Bluetooth Low Energy 4.x and 5.x support for nRF52 microcontrollers.&lt;/item&gt;
          &lt;item&gt;The embassy-stm32-wpan crate provides Bluetooth Low Energy 5.x support for stm32wb microcontrollers.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;The trouble crate provides a Bluetooth Low Energy 4.x and 5.x Host that runs on any microcontroller implementing the bt-hci traits (currently &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;LoRa - The lora-rs project provides an async LoRa and LoRaWAN stack that works well on Embassy.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;USB - embassy-usb implements a device-side USB stack. Implementations for common classes such as USB serial (CDC ACM) and USB HID are available, and a rich builder API allows building your own.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Bootloader and DFU - embassy-boot is a lightweight bootloader supporting firmware application upgrades in a power-fail-safe way, with trial boots and rollbacks.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;use defmt::info;
use embassy_executor::Spawner;
use embassy_time::{Duration, Timer};
use embassy_nrf::gpio::{AnyPin, Input, Level, Output, OutputDrive, Pin, Pull};
use embassy_nrf::{Peri, Peripherals};

// Declare async tasks
#[embassy_executor::task]
async fn blink(pin: Peri&amp;lt;'static, AnyPin&amp;gt;) {
    let mut led = Output::new(pin, Level::Low, OutputDrive::Standard);

    loop {
        // Timekeeping is globally available, no need to mess with hardware timers.
        led.set_high();
        Timer::after_millis(150).await;
        led.set_low();
        Timer::after_millis(150).await;
    }
}

// Main is itself an async task as well.
#[embassy_executor::main]
async fn main(spawner: Spawner) {
    let p = embassy_nrf::init(Default::default());

    // Spawned tasks run in the background, concurrently.
    spawner.spawn(blink(p.P0_13.into()).unwrap());

    let mut button = Input::new(p.P0_11, Pull::Up);
    loop {
        // Asynchronously wait for GPIO events, allowing other tasks
        // to run, or the core to sleep.
        button.wait_for_low().await;
        info!("Button pressed!");
        button.wait_for_high().await;
        info!("Button released!");
    }
}&lt;/code&gt;
    &lt;p&gt;Examples are found in the &lt;code&gt;examples/&lt;/code&gt; folder separated by the chip manufacturer they are designed to run on. For example:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;examples/nrf52840&lt;/code&gt;run on the&lt;code&gt;nrf52840-dk&lt;/code&gt;board (PCA10056) but should be easily adaptable to other nRF52 chips and boards.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;examples/nrf5340&lt;/code&gt;run on the&lt;code&gt;nrf5340-dk&lt;/code&gt;board (PCA10095).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;examples/stm32xx&lt;/code&gt;for the various STM32 families.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;examples/rp&lt;/code&gt;are for the RP2040 chip.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;examples/std&lt;/code&gt;are designed to run locally on your PC.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Install &lt;code&gt;probe-rs&lt;/code&gt;following the instructions at https://probe.rs.&lt;/item&gt;
      &lt;item&gt;Change directory to the sample's base directory. For example:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;cd examples/nrf52840&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Ensure&lt;/p&gt;&lt;code&gt;Cargo.toml&lt;/code&gt;sets the right feature for the name of the chip you are programming. If this name is incorrect, the example may fail to run or immediately crash after being programmed.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Ensure&lt;/p&gt;&lt;code&gt;.cargo/config.toml&lt;/code&gt;contains the name of the chip you are programming.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Run the example&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For example:&lt;/p&gt;
    &lt;code&gt;cargo run --release --bin blinky&lt;/code&gt;
    &lt;p&gt;For more help getting started, see Getting Started and Running the Examples.&lt;/p&gt;
    &lt;p&gt;The Rust Analyzer is used by Visual Studio Code and others. Given the multiple targets that Embassy serves, there is no Cargo workspace file. Instead, the Rust Analyzer must be told of the target project to work with. In the case of Visual Studio Code, please refer to the &lt;code&gt;.vscode/settings.json&lt;/code&gt; file's &lt;code&gt;rust-analyzer.linkedProjects&lt;/code&gt;setting.&lt;/p&gt;
    &lt;p&gt;Embassy is guaranteed to compile on stable Rust 1.75 and up. It might compile with older versions, but that may change in any new patch release.&lt;/p&gt;
    &lt;p&gt;EMBedded ASYnc! :)&lt;/p&gt;
    &lt;p&gt;Embassy is licensed under either of&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Apache License, Version 2.0 (LICENSE-APACHE or http://www.apache.org/licenses/LICENSE-2.0)&lt;/item&gt;
      &lt;item&gt;MIT license (LICENSE-MIT or http://opensource.org/licenses/MIT)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;at your option.&lt;/p&gt;
    &lt;p&gt;Unless you explicitly state otherwise, any contribution intentionally submitted for inclusion in the work by you, as defined in the Apache-2.0 license, shall be dual licensed as above, without any additional terms or conditions.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46547740</guid><pubDate>Thu, 08 Jan 2026 23:00:45 +0000</pubDate></item></channel></rss>