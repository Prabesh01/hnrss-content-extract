<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Sun, 12 Oct 2025 16:10:22 +0000</lastBuildDate><item><title>Microsoft only lets you opt out of AI photo scanning 3x a year</title><link>https://hardware.slashdot.org/story/25/10/11/0238213/microsofts-onedrive-begins-testing-face-recognizing-ai-for-photos-for-some-preview-users</link><description>&lt;doc fingerprint="27d138be7ae4dabe"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Microsoft's OneDrive Begins Testing Face-Recognizing AI for Photos (for Some Preview Users) (microsoft.com) 58&lt;/head&gt;
    &lt;p&gt;And...&lt;/p&gt;
    &lt;p&gt;"You can only turn off this setting 3 times a year."&lt;/p&gt;
    &lt;p&gt;If I moved the slidebar for that setting to the left (for "No"), it moved back to the right, and said "Something went wrong while updating this setting." (Apparently it's not one of those three times of the year.)&lt;/p&gt;
    &lt;p&gt;The feature is already rolling out to a limited number of users in a preview, a Microsoft publicist confirmed to Slashdot. (For the record, I don't remember signing up for this face-recognizing "preview".) But there's a link at the bottom of the screen for a "Microsoft Privacy Statement" that leads to a Microsoft support page, which says instead that "This feature is coming soon and is yet to be released." And in the next sentence it's been saying "Stay tuned for more updates" for almost two years...&lt;/p&gt;
    &lt;p&gt;A Microsoft publicist agreed to answer Slashdot's questions...&lt;/p&gt;
    &lt;p&gt;Slashdot: What's the reason OneDrive tells users this setting can only be turned off 3 times a year? (And are those any three times — or does that mean three specific days, like Christmas, New Year's Day, etc.)&lt;/p&gt;
    &lt;p&gt;[Microsoft's publicist chose not to answer this question.]&lt;/p&gt;
    &lt;p&gt;Slashdot: If I move the slidebar to the left (for "No"), it moves back to the right, and says "Something went wrong while updating this setting." So is it correct to say that there's no way for users to select "No" now?&lt;/p&gt;
    &lt;p&gt;Microsoft: We haven't heard about the experience you are having with toggling, but our Microsoft contacts would like to investigate why this is happening for you. Can you share what type of device you are using, so we can put you in touch with the right team?&lt;/p&gt;
    &lt;p&gt;Slashdot: Is this feature really still "coming soon"? Can you give me more specific details on when "soon" will be?&lt;/p&gt;
    &lt;p&gt;Microsoft: This feature is currently rolling out to limited users in a preview so we can learn and improve. We have nothing more to share at this time.&lt;/p&gt;
    &lt;p&gt;Slashdot: I want to confirm something about how this feature is "yet to be released." Does this mean that currently OneDrive is not (and has never) used AI to "recognize" faces in photos?&lt;/p&gt;
    &lt;p&gt;Microsoft: Privacy is built into all Microsoft OneDrive experiences. Microsoft OneDrive services adhere to the Microsoft Privacy Statement and follow Microsoft's compliance with General Data Protection Regulation and the Microsoft EU Data Boundary.&lt;/p&gt;
    &lt;p&gt;Slashdot: Some privacy advocates prefer "opt-in" features, but it looks like here OneDrive is planning a (limited) opt-out feature. What is the reasoning for going with opt-out rather than opt-in?&lt;/p&gt;
    &lt;p&gt;Microsoft: Microsoft OneDrive inherits privacy features and settings from Microsoft 365 and SharePoint, where applicable.&lt;/p&gt;
    &lt;p&gt;Slashdot also spoke to EFF security/privacy activist Thorin Klosowski, who expressed concerns. "Any feature related to privacy really should be opt-in and companies should provide clear documentation so its users can understand the risks and benefits to make that choice for themselves."&lt;/p&gt;
    &lt;p&gt;Microsoft's "three times a year" policy also seemed limiting to Klosowski. "People should also be able to change those settings at-will whenever possible because we all encounter circumstances were we need to re-evaluate and possibly change our privacy settings."&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45551504</guid><pubDate>Sat, 11 Oct 2025 18:36:51 +0000</pubDate></item><item><title>A Guide for WireGuard VPN Setup with Pi-Hole Adblock and Unbound DNS</title><link>https://psyonik.tech/posts/a-guide-for-wireguard-vpn-setup-with-pi-hole-adblock-and-unbound-dns/</link><description>&lt;doc fingerprint="b678e959f90921c1"&gt;
  &lt;main&gt;&lt;p&gt;I’ve used Mullvad as my VPN provider for a few years. Their service is good, they provide keys for 5 devices, rely on the Wireguard protocol, and offer alternative configurations as well. Despite that, my needs have changed and I wanted to be able to have granular control over DNS queries and the ability to connect my various devices on my network through simple addresses, such as http://emby.home.server. Enter Wireguard, Pi-Hole and Unbound.&lt;/p&gt;&lt;p&gt;Of course, there are many tools that might achieve the same results with a lot less work, such as Tailscale. I would reach devices on the network easily, one of the nodes could be set to act as an exit node and you could also apply network-wide ad-block with Next DNS (I believe that is, at the time of writing, the solution they offer). Outside of Tailscale there are other fully open source alternatives, such as Headscale, Nebula and others. However, I chose to set up my own Wireguard network, as it gave me the opportunity to learn a lot and helped me better understand everything involved in the process.&lt;/p&gt;&lt;p&gt;For devices, I have the following configuration, and the below IPs will be used throughout the article. They represent a starting point, but should be sufficient for anyone to build their network according to their needs.&lt;/p&gt;&lt;code&gt;10.10.10.1&lt;/code&gt;) - running on a VPS (Virtual Private Server) from a provider that meets my requirements (close physical proximity, high bandwidth and at least 1 Gbps transfer speeds)&lt;code&gt;10.10.10.10&lt;/code&gt;) - running on my home network and providing various service (media server, personal cloud, torrent client, etc.)&lt;code&gt;10.10.10.11&lt;/code&gt;) - running on my home network&lt;code&gt;10.10.10.12&lt;/code&gt;) - running on my home network, but might connect through outside networks (i.e.: coffee shops, libraries, airports, etc.)&lt;code&gt;10.10.10.100&lt;/code&gt;) - running on my home network, mobile data, mobile data roaming or public networks (coffee shops, libraries, airports, etc.)&lt;code&gt;10.10.10.101&lt;/code&gt;) - same as above&lt;p&gt;I will use a hub and spoke network topology where the VPS will act as the in-between for all inter-device communications on the network as well as the exit node for all Internet-facing communications. For example, if my Desktop connects to the Internet it will do so by navigating to the VPS, which will then resolve the query and block any domains that are on the block list. If it wants to connect to the Home Server it will do so without the VPS, since the Home Server and Desktop are on the same local network, connected via a router. If Smartphone 1 wants to connect to the Internet, the query is resolved by the VPS, which will block any domains on the block list. If it wants to connect to the Home Server, the VPS will receive the request and route it to the IP of the Home Server. It’s important to keep in mind that the contents of the request become visible as they leave the Wireguard tunnel between Smartphone 1 and the VPS. On the VPS contents will be encrypted again and then forwarded to the Home Server.&lt;/p&gt;&lt;p&gt;I chose a VPS provider that is physically near my location, has minimum 1 Gbps speed (up/down), unlimited bandwidth and good performance while gaming (i.e.: specifically no packet loss). There are many options; you can check resources like LowEndBox and VPSBenchmarks to get an idea of price ranges, availability and regions. For your own needs, start by defining a set of requirements and see which of these match your needs.&lt;/p&gt;&lt;p&gt;My only recommendation is to initially opt for a cheaper provider, that offers hourly billing and understand the end-to-end-process before committing. Keep in mind that various providers have different configurations and settings, so setups can vary.&lt;/p&gt;&lt;p&gt;If you are completely new to setting up servers using a command line interface (CLI) only, I recommend checking out the Digital Ocean cloud computing series. Most providers have their own guides, so for specific steps check their respective documentation. Same goes for any chosen OS. For this guide I will be using Ubuntu 24.04. For the remainder of the article I will assume you are running an OS with access to a terminal that can send commands to a CLI that can generate SSH keys, establish SSH connections and can run a text editor that you can use to create, edit, update and delete text files. I will be using BASH/Fish with &lt;code&gt;nano&lt;/code&gt; as my text editor.&lt;/p&gt;&lt;p&gt;To start, create an SSH key on your local machine and upload it to your chosen cloud provider. This first step already greatly improves security of the VPS and simplifies connection to it as you don’t need to remember a password.&lt;/p&gt;&lt;code&gt;# Run inside your terminal on your client machine 
# https://www.man7.org/linux/man-pages/man1/ssh-keygen.1.html
$ ssh-keygen -t ed25519
&lt;/code&gt;&lt;p&gt;Follow the on-screen prompts to provide a name, set a passphrase (recommended) and have the private and public files created at the given path in the prompt.&lt;/p&gt;&lt;p&gt;Next, copy the contents of the .pub file, or the file itself to your chosen VPS provider’s server setup page. Complete the initial server setup as per your provider’s steps. Once ready, you will have a public IPv4 address to which you can connect (this is a requirement in this guide, and some providers might only offer IPv6 addresses on their cheapest VPS). A connection can now be established with:&lt;/p&gt;&lt;code&gt;# replace root with the username your provider created by default; could be ubuntu as well
# replace 123.123.123.123 with the IP address of your VPS
$ ssh -i .ssh/article root@123.123.123.123
&lt;/code&gt;&lt;p&gt;You are now connected to the server, as the &lt;code&gt;root&lt;/code&gt; user. If you connected via another user that has &lt;code&gt;sudo&lt;/code&gt; access, then prepend all commands with &lt;code&gt;sudo&lt;/code&gt;. If I write &lt;code&gt;ufw status numbered&lt;/code&gt; use &lt;code&gt;sudo ufw status numbered&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;First, make sure you have &lt;code&gt;unattended-upgrades&lt;/code&gt; installed on your VPS.&lt;/p&gt;&lt;code&gt;$ apt install unattended-upgrades
Reading package lists... Done
Building dependency tree... Done
Reading state information... Done
unattended-upgrades is already the newest version (2.9.1+nmu4ubuntu1).
0 upgraded, 0 newly installed, 0 to remove and 29 not upgraded.
&lt;/code&gt;&lt;p&gt;Next, prepare the settings you want to apply. Because the application reads through different configuration files in order, I made a copy of /etc/apt/apt.conf.d/50unattended-upgrades and called it /etc/apt/apt.conf.d/51unattended-upgrades. This means that even after an update, while the original file could be overwritten, my settings will persist. The main settings I changed in my copy are found below. Read through the different options and adjust to your preferences.&lt;/p&gt;&lt;code&gt;# Make a copy and give a new name
$ cp /etc/apt/apt.conf.d/50unattended-upgrades /etc/apt/apt.conf.d/51unattended-upgrades
# Edit the file 
$ nano /etc/apt/apt.conf.d/51unattended-upgrades
# File contents below are edited for brevity
Unattended-Upgrade::Allowed-Origins {
        "${distro_id}:${distro_codename}";
        "${distro_id}:${distro_codename}-security";
        // Extended Security Maintenance; doesn't necessarily exist for
        // every release and this system may not have it installed, but if
        // available, the policy for updates is such that unattended-upgrades
        // should also install from here by default.
        "${distro_id}ESMApps:${distro_codename}-apps-security";
        "${distro_id}ESM:${distro_codename}-infra-security";
        "${distro_id}:${distro_codename}-updates";
};

// Remove unused automatically installed kernel-related packages
// (kernel images, kernel headers and kernel version locked tools).
Unattended-Upgrade::Remove-Unused-Kernel-Packages "true";

// Do automatic removal of newly unused dependencies after the upgrade
Unattended-Upgrade::Remove-New-Unused-Dependencies "true";

// Do automatic removal of unused packages after the upgrade
// (equivalent to apt-get autoremove)
Unattended-Upgrade::Remove-Unused-Dependencies "true";

// Automatically reboot *WITHOUT CONFIRMATION* if
// the file /var/run/reboot-required is found after the upgrade
Unattended-Upgrade::Automatic-Reboot "true";

// If automatic reboot is enabled and needed, reboot at the specific
// time instead of immediately
// Default: "now" - change this to whatever works for you!
Unattended-Upgrade::Automatic-Reboot-Time "00:00";
&lt;/code&gt;&lt;p&gt;Next, update the periodic file to provide &lt;code&gt;unattended-upgrades&lt;/code&gt; with the intervals at which it checks for updates and runs the clean command. Create a new file at /etc/apt/apt.conf.d/21periodic (you can name it anything else, as these are parsed in order, so 22periodic, 23…). The below are the values for the purposes of this server:&lt;/p&gt;&lt;code&gt;APT::Periodic::Update-Package-Lists "1";
APT::Periodic::Download-Upgradeable-Packages "1";
APT::Periodic::AutocleanInterval "7";
APT::Periodic::Unattended-Upgrade "1";
&lt;/code&gt;&lt;p&gt;Number 1 means the option is enabled and runs daily. Zero means the option is turned off. Any other number shows the frequency with which the step is completed (e.g.: 7 - the action is performed once a week).&lt;/p&gt;&lt;p&gt;Run &lt;code&gt;unattended-upgrade -d&lt;/code&gt; to quickly debug your current app configuration and confirm the settings you have setup so far.&lt;/p&gt;&lt;p&gt;Next off, I configure the system hostname. The hostname can then be used as part of a fully qualified domain name for the system (e.g.: hostname = pihole, fqdn = pihole.psyonik.com)&lt;/p&gt;&lt;code&gt;# you can check current hostname with
hostname
# set hostname to something else
hostnamectl set-hostname pihole
# edit hosts file to create static associations between IP addresses and hostnames/domains
# which the system prioritizes before DNS for name resolution
nano /etc/hosts
# The contents of the file might be similar, adjust them to match what I have below 
# replacing &amp;lt;pihole&amp;gt; and &amp;lt;psyonik&amp;gt; with your settings
127.0.0.1 localhost
127.0.0.1 pihole
127.0.1.1 pihole.psyonik.com pihole
# replace the below address with your VPS actual IP address
123.123.123.113 pihole.psyonik.com pihole
&lt;/code&gt;&lt;p&gt;I check if the hostname is setup correctly by calling &lt;code&gt;ping pihole.psyonik.com&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;As mentioned previously, I connect to the server by using &lt;code&gt;ssh -i .ssh/article root@123.123.123.123&lt;/code&gt;. The command passes in the path to the SSH key I use, sets the username I want to connect as, and sets the IP address of the server. By default, the SSH service runs on port 22. This is not explicitly stated in the connection command, it is just assumed as being set as such. One of the first things I like to change on a new VPS is the default port to something else. On Ubuntu 24.04 you will have this option in the sshd_config file, but the actual port used will be picked up from the &lt;code&gt;ssh.socket&lt;/code&gt; service. This can be confusing, as the setting is still present in the sshd_config file.&lt;/p&gt;&lt;p&gt;The first step is to make a copy of the configuration file and then edit the following lines in the original file.&lt;/p&gt;&lt;code&gt;$ cp /etc/ssh/sshd_config /etc/ssh/sshd_config.1
# Edit the SSH configuration - disable root login, password auth, enable pubkey login
$ nano /etc/ssh/sshd_config

# Find and edit the below values in the file
AddressFamily inet # SSH Service will only listen to IPv4 addresses
PermitRootLogin no # disable root login
PubkeyAuthentication yes # only allow SSH key-based authentication  
AuthorizedKeysFile .ssh/authorized_keys # file that contains allowed public keys  
PasswordAuthentication no # do not allow password auth  
PermitEmptyPasswords no # do not allow empty passwords 
ChallengeResponseAuthentication no # Specifies if challenge-response auth is allowed
UsePAM no # disable authentication through PAM (Pluggable Authentication Module)
&lt;/code&gt;&lt;p&gt;Next, I create a folder and a configuration file to change the SSH port for the &lt;code&gt;ssh.socket&lt;/code&gt;.&lt;/p&gt;&lt;code&gt;$ mkdir /etc/systemd/system/ssh.socket.d
# Create and add the values into the listen.conf file
$ nano /etc/systemd/system/ssh.socket.d/listen.conf
[Socket]
ListenStream=
ListenStream=12345
# Save and exit file
$ systemctl daemon-reload
# Restart the SSH Socket
$ systemctl restart ssh.socket
# Restart the SSH Service
$ systemctl restart ssh.service
# Check the status of the service after restart; you should see the new port number displayed
$ systemctl status ssh.service
# Output for my configuration
● ssh.service - OpenBSD Secure Shell server
     Loaded: loaded (/usr/lib/systemd/system/ssh.service; disabled; preset: enabled)
     Active: active (running) since Sun 1999-01-01 12:01:04 UTC; 5s ago
TriggeredBy: ● ssh.socket
       Docs: man:sshd(8)
             man:sshd_config(5)
    Process: 12928 ExecStartPre=/usr/sbin/sshd -t (code=exited, status=0/SUCCESS)
   Main PID: 12929 (sshd)
      Tasks: 1 (limit: 4543)
     Memory: 1.2M (peak: 1.3M)
        CPU: 36ms
     CGroup: /system.slice/ssh.service
             └─12929 "sshd: /usr/sbin/sshd -D [listener] 0 of 10-100 startups"

Jan 01 12:01:04 pihole systemd[1]: Starting ssh.service - OpenBSD Secure Shell server...
Jan 01 12:01:04 pihole sshd[12929]: Server listening on :: port 12345.
Jan 01 12:01:04 pihole systemd[1]: Started ssh.service - OpenBSD Secure Shell server.
&lt;/code&gt;&lt;p&gt;Now, in a new tab, if I try to connect with the previous command, this should not work. I would now also need to specify the port, but if I try to connect as &lt;code&gt;root&lt;/code&gt; this should get denied.&lt;/p&gt;&lt;code&gt;$ ssh -i .ssh/article -p 12345 root@123.123.123.123 # this should return an error
&lt;/code&gt;&lt;p&gt;While keeping my session open, I will create a new user, add the user to &lt;code&gt;sudo&lt;/code&gt;, copy the .ssh/authorized_keys file over to the new users home directory and try to connect with that user. I then complete the remaining configuration using &lt;code&gt;root&lt;/code&gt;.&lt;/p&gt;&lt;code&gt;# Create a new user and follow on-screen prompts
$ adduser psyonik
# Add user to sudo
$ usermod -aG sudo psyonik
# Copy authorizedkeys file to the new user folder and change ownership 
$ cp -r .ssh/ /home/psyonik/
$ chown psyonik:psyonik -R /home/psyonik/.ssh
# Ensure ownership is set correctly
$ ls -alh /home/psyonik/.ssh/
total 12K
drwx------ 2 psyonik psyonik 4.0K Jan  1 12:09 .
drwxr-x--- 3 psyonik psyonik 4.0K Jan  1 12:09 ..
-rw------- 1 psyonik psyonik  162 Jan  1 12:09 authorized_keys
&lt;/code&gt;&lt;p&gt;When I connect from my local machine to the VPS using this new username, the connection should be established correctly.&lt;/p&gt;&lt;code&gt;$ ssh -i .ssh/article -p 12345 psyonik@123.123.123.123
&lt;/code&gt;&lt;p&gt;This completes the setup of SSH. Next up, we setup the firewall. If you encounter issues, you can use &lt;code&gt;ssh -vvv&lt;/code&gt; to have verbose logging enabled during the SSH connection. This should highlight any issues such as access denied, incorrect keys or anything else of the sort.&lt;/p&gt;&lt;p&gt;Although the server can now do automatic updates, doesn’t rely on the default port to allow incoming connections and has &lt;code&gt;root&lt;/code&gt; login disabled, we still need to configure the firewall. There are various ways to do this, but I like to use &lt;code&gt;ufw&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;Uncomplicated firewall is a great way to setup some basic rules and security on the server. I allow only 2 ports open on IPv4 on the server at this stage, enable logging, and deny all other incoming requests while allowing all outgoing requests.&lt;/p&gt;&lt;code&gt;# the SSH port should be replaced with whatever you wish to use instead
$ ufw allow from 0.0.0.0/0 to any port 12345 comment "SSH"
$ ufw allow from 0.0.0.0/0 to any port 51820 comment "Wireguard"
# block all incoming connections except those going to the SSH/Wireguard ports
$ ufw default deny incoming
# allow 
$ ufw default allow outgoing
# enable logging
$ ufw logging on
# enable ufw and display the rules in a numbered list
$ ufw enable
$ ufw status numbered
Status: active

     To                         Action      From
     --                         ------      ----
[ 1] 12345                      ALLOW IN    Anywhere                   # SSH
[ 2] 51820                      ALLOW IN    Anywhere                   # Wireguard
&lt;/code&gt;&lt;p&gt;Each time you edit a rule you can call &lt;code&gt;ufw reload&lt;/code&gt; to refresh the new rules. You can then check the status of the rules with &lt;code&gt;ufw status numbered&lt;/code&gt;. This also gives you an easy way to delete any unused rules.&lt;/p&gt;&lt;p&gt;Lastly, there is one more change that can be done so that the server will not respond to ping requests.&lt;/p&gt;&lt;code&gt;sudo nano /etc/ufw/before.rules
# Look for the following row and change from ACCEPT to DROP
-A ufw-before-input -p icmp --icmp-type echo-request -j DROP
&lt;/code&gt;&lt;p&gt;This completes a basic server configuration for this machine. Other settings can be changed and if I were to do this often, I would opt for a script file as a minimum, or ideally a cloud-init script. Feel free to explore that once you feel comfortable with this setup.&lt;/p&gt;&lt;p&gt;Next, I’ll setup Wireguard, Pi-Hole and Unbound.&lt;/p&gt;&lt;p&gt;Unlike traditional VPN services, where a central server acts as a point of control and has clients, Wireguard uses the concept of peers. Peers can connect directly to each other, thus allowing for lower latency for connections and removing the single point of failure of a server. However, for my purposes, I configured one of the peers, in Wireguard parlance, to act as a server due to the hub-and-spoke network topology I employ in this setup. As such, the VPS will be referred to as the ‘server’ and all other devices as ‘clients’.&lt;/p&gt;&lt;p&gt;I will expand this article once I have a configuration in which clients that need to be able to connect to each other will do so directly, without a VPS in-between.&lt;/p&gt;&lt;p&gt;Next, to setup each client and the server, I need to create keys for all devices and create configuration files using these keys. The below steps will create public, private and pre-shared keys for all devices.&lt;/p&gt;&lt;code&gt;# Install or make sure Wireguard is installed
$ apt install wireguard

# Create a folder to keep client keys and one for client configurations
# This is not strictly needed, as once clients are added, the keys can be removed from the server
# Except for the preshared keys 
$ mkdir /etc/wireguard/clients
$ mkdir /etc/wireguard/clientconfs

# Create VPS keys - key, pub, psk
$ wg genkey &amp;gt; vps.key # I use .key to show we're dealing with a private key
$ wg pubkey &amp;lt; vps.key &amp;gt; vps.pub # I use .pub to show this is a public key

$ wg genkey &amp;gt; /etc/wireguard/clients/homeserver.key
$ wg pubkey &amp;lt; /etc/wireguard/clients/homeserver.key &amp;gt; /etc/wireguard/clients/homeserver.pub
$ wg genpsk &amp;gt; /etc/wireguard/clients/homeserver.psk # I use .psk to show this is a preshared key

$ wg genkey &amp;gt; /etc/wireguard/clients/desktop.key
$ wg pubkey &amp;lt; /etc/wireguard/clients/desktop.key &amp;gt; /etc/wireguard/clients/desktop.pub
$ wg genpsk &amp;gt; /etc/wireguard/clients/desktop.psk

$ wg genkey &amp;gt; /etc/wireguard/clients/laptop.key
$ wg pubkey &amp;lt; /etc/wireguard/clients/laptop.key &amp;gt; /etc/wireguard/clients/laptop.pub
$ wg genpsk &amp;gt; /etc/wireguard/clients/laptop.psk

$ wg genkey &amp;gt; /etc/wireguard/clients/mobile1.key
$ wg pubkey &amp;lt; /etc/wireguard/clients/mobile1.key &amp;gt; /etc/wireguard/clients/mobile1.pub
$ wg genpsk &amp;gt; /etc/wireguard/clients/mobile1.psk

$ wg genkey &amp;gt; /etc/wireguard/clients/mobile2.key
$ wg pubkey &amp;lt; /etc/wireguard/clients/mobile2.key &amp;gt; /etc/wireguard/clients/mobile2.pub
$ wg genpsk &amp;gt; /etc/wireguard/clients/mobile2.psk
&lt;/code&gt;&lt;p&gt;With the keys created, I prepare the VPS configuration file. Each peer that wants to join the VPN needs its own configuration file. In Wireguard, the configuration file is split into two main sections: &lt;code&gt;[Interface]&lt;/code&gt; and &lt;code&gt;[Peer]&lt;/code&gt;. Each configuration file can contain zero, one or multiple peers that I can connect to. If I have one peer in my config, I’m connecting to that one device, if I have more, it means I can directly connect to all those peers.&lt;/p&gt;&lt;p&gt;In this case, the server will have multiple peers while the peers will each have a single peer: the server. The term interface is used because Wireguard creates a new network interface which is named according to the name of the configuration file.&lt;/p&gt;&lt;p&gt;Following this, I create a new configuration file named wg0, which means that once I start Wireguard with this configuration, I will have a new network interface called &lt;code&gt;wg0&lt;/code&gt;. This can be renamed to anything else.&lt;/p&gt;&lt;code&gt;$ nano /etc/wireguard/wg0.conf
# Next I will populate the file with the below information
[Interface]
# Replace the contents with the value you got in vps.key
PrivateKey = QELYVAdCPCaVQfoyE3KrADMZBBdoNotUseAsKeyThisIsMine+eWUA=
Address = 10.10.10.1/24 # the IP address the server will have inside the wg0 network 
ListenPort = 51820 # the port on which the server will listen for incoming connections
SaveConfig = true # https://manpages.debian.org/unstable/wireguard-tools/wg-quick.8.en.html
PreUp = sysctl -w net.ipv4.ip_forward=1 # enables ipv4 forwarding on the VPS
# PreUp = sysctl -w net.ipv6.conf.all.forward=1 if you use ipv6 instead
&lt;/code&gt;&lt;p&gt;At this stage I have the base for my Wireguard network on the server. But because I want to hide traffic from peers behind the server’s IP address I need to apply masquerading to the incoming requests from the clients, a function similar to NAT. To achieve this, we need to get the data coming in on the network interface Wireguard creates out to the network interface of the server and allow the server to alter the source IP.&lt;/p&gt;&lt;p&gt;The first step is finding the default network interface on the server, so I run the below command:&lt;/p&gt;&lt;code&gt;$ ip route list default
# Output could be something like: 
# default via 123.123.123.123 dev eth0 proto dhcp src 123.123.123.123 metric 100
&lt;/code&gt;&lt;p&gt;The important bit here is to see the device (in my sample above it is &lt;code&gt;dev eth0&lt;/code&gt;). This means I want to route requests that I receive out &lt;code&gt;eth0&lt;/code&gt; and apply masquerading, which makes it look like a request received from a smartphone originated from the server.&lt;/p&gt;&lt;p&gt;I will go back to the configuration and continue applying a few more settings to enable masquerading out the &lt;code&gt;eth0&lt;/code&gt; interface (again, this can be different on your machine) and allow traffic that comes in on the &lt;code&gt;wg0&lt;/code&gt; interface (named after the configuration file) to go out on &lt;code&gt;eth0&lt;/code&gt;.&lt;/p&gt;&lt;code&gt;$ nano /etc/wireguard/wg0.conf
# Next I will populate the file with the below information
[Interface]
PrivateKey = QELYVAdCPCaVQfoyE3KrADMZBBdoNotUseAsKeyThisIsMine+eWUA=
Address = 10.10.10.1/24 # the IP address the server will have inside the wg0 network 
ListenPort = 51820 # the port on which the server will listen for incoming connections
SaveConfig = true # https://manpages.debian.org/unstable/wireguard-tools/wg-quick.8.en.html
PreUp = sysctl -w net.ipv4.ip_forward=1 # enables ipv4 forwarding on the VPS
# PreUp = sysctl -w net.ipv6.conf.all.forward=1 if you use ipv6 instead
PostUp= ufw route allow in on wg0 out on eth0
PostUp= iptables -t nat -I POSTROUTING -o eth0 -j MASQUERADE
PreDown= ufw route delete allow in on wg0 out on eth0
PreDown= iptables -t nat -D POSTROUTING -o eth0 -j MASQUERADE
&lt;/code&gt;&lt;p&gt;This completes the configuration of the Wireguard interface. After exiting and saving the file, I enable a &lt;code&gt;systemd&lt;/code&gt; service that follows the name of the config file/interface and check to make sure it runs.&lt;/p&gt;&lt;code&gt;# Enable Wireguard service at system startup 
$ systemctl enable wg-quick@wg0.service
# Start Wireguard service
$ systemctl start wg-quick@wg0.service
# Check service status 
$ systemctl status wg-quick@wg0.service
● wg-quick@wg0.service - WireGuard via wg-quick(8) for wg0
     Loaded: loaded (/usr/lib/systemd/system/wg-quick@.service; enabled; preset: enabled)
     Active: active (exited) since Sun 2024-01-01 12:12:42 UTC; 1min 29s ago
       Docs: man:wg-quick(8)
             man:wg(8)
             https://www.wireguard.com/
             https://www.wireguard.com/quickstart/
             https://git.zx2c4.com/wireguard-tools/about/src/man/wg-quick.8
             https://git.zx2c4.com/wireguard-tools/about/src/man/wg.8
    Process: 13987 ExecStart=/usr/bin/wg-quick up wg0 (code=exited, status=0/SUCCESS)
   Main PID: 13987 (code=exited, status=0/SUCCESS)
        CPU: 325ms

Jan 01 12:12:41 pihole wg-quick[13997]: net.ipv4.ip_forward = 1
Jan 01 12:12:41 pihole wg-quick[13987]: [#] ip link add wg0 type wireguard
Jan 01 12:12:41 pihole wg-quick[13987]: [#] wg setconf wg0 /dev/fd/63
Jan 01 12:12:41 pihole wg-quick[13987]: [#] ip -4 address add 10.10.10.1/24 dev wg0
Jan 01 12:12:41 pihole wg-quick[13987]: [#] ip link set mtu 1420 up dev wg0
Jan 01 12:12:41 pihole wg-quick[13987]: [#] ufw route allow in on wg0 out on eth0
Jan 01 12:12:41 pihole wg-quick[14017]: Rule added
Jan 01 12:12:41 pihole wg-quick[14017]: Rule added (v6)
Jan 01 12:12:42 pihole wg-quick[13987]: [#] iptables -t nat -I POSTROUTING -o eth0 -j MASQUERADE
Jan 01 12:12:42 pihole systemd[1]: Finished wg-quick@wg0.service - WireGuard via wg-quick(8) for wg0.
# Check wireguard status directly and verify the key
$ wg
interface: wg0
  public key: qUo/OLUZadoNotUseAsKeyThisIsMineokE6T3pYl0c=
  private key: (hidden)
  listening port: 51820
# Show network interfaces
$ ip a
1: lo: &amp;lt;LOOPBACK,UP,LOWER_UP&amp;gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever  
2: eth0: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000
    link/ether 1f:1f:1f:1f:1f:1f brd ff:ff:ff:ff:ff:ff
    inet 123.123.123.123/32 metric 100 scope global dynamic eth0
       valid_lft 60719sec preferred_lft 60719sec
4: wg0: &amp;lt;POINTOPOINT,NOARP,UP,LOWER_UP&amp;gt; mtu 1420 qdisc noqueue state UNKNOWN group default qlen 1000
    link/none 
    inet 10.10.10.1/24 scope global wg0
       valid_lft forever preferred_lft forever
&lt;/code&gt;&lt;p&gt;After this, I setup the first peer - remember, I need a configuration file for this. The private key field holds the private key of the peer, the pre-shared key is the value of the pre-shared key I created for the peer, and the public key section in the file holds the public key of the VPS.&lt;/p&gt;&lt;code&gt;# Just naming it dwg0 as for me it will be easy to see that this is the desktop config
# To bring this interface up on my desktop I would use `sudo wg-quick up dwg0`
$ nano dwg0.conf
[Interface]
# Replace with the private key file contents for THIS particular device, in my case desktop
PrivateKey = uIwALjBCXdoNotUseAsKeyThisIsMine1Vb/kI3XGXk=
# The address I had mentioned initially in the topology overview
Address = 10.10.10.11/32
# The VPS will act as a DNS server for this device
DNS = 10.10.10.1

[Peer]
# Replace with the public key file contents for your VPS, in my case vps.pub
PublicKey = qUo/OLUdoNotUseAsKeyThisIsMineWgokE6T3pYl0c=
# Replace with the value you got in your .psk file for THIS particular device, in my case desktop
PresharedKey = pMTDdoNotUseAsKeyThisIsMineeaGilNZRO9OGy3Q=
# https://www.procustodibus.com/blog/2021/03/wireguard-allowedips-calculator/#background
AllowedIPs = 0.0.0.0/0
# Replace 123.123.123.123:51820 with the IP of your VPS and the port you used if different
Endpoint = 123.123.123.123:51820
PersistentKeepalive = 25 # https://wiki.archlinux.org/title/WireGuard#Unable_to_establish_a_persistent_connection_behind_NAT_/_firewall
&lt;/code&gt;&lt;p&gt;At this stage, if I enable the configuration on my desktop, it will not connect to the server. First, I need to add the desktop as a peer of the VPS. I can do that by either bringing down the &lt;code&gt;wg0&lt;/code&gt; interface or by using the &lt;code&gt;wg set&lt;/code&gt; command. I use the latter, which edits the /etc/wireguard/wg0.conf file and adds a new entry under the &lt;code&gt;[Peer]&lt;/code&gt; section.&lt;/p&gt;&lt;code&gt;# The key that I provide to the command is the public key of the desktop 
$ wg set wg0 peer KKdoNotUseAsKeyThisIsMineRFCyZVorUW8m7E/3S0= preshared-key /etc/wireguard/clients/desktop.psk allowed-ips 10.10.10.11
# Check the peer has been added to the interface
$ wg
interface: wg0
  public key: qUo/OLUdoNotUseAsKeyThisIsMineWgokE6T3pYl0c=
  private key: (hidden)
  listening port: 51820

peer: KKdoNotUseAsKeyThisIsMineRFCyZVorUW8m7E/3S0=
  preshared key: (hidden)
  allowed ips: 10.10.10.11/32
&lt;/code&gt;&lt;p&gt;This completes this part of the setup. To use the server to resolve DNS queries and block ads, I setup Pi-Hole and Unbound next.&lt;/p&gt;&lt;p&gt;I installed Pi-Hole using the automated script from their website. I won’t go into the details of piping a script to bash directly, but for those of you who wish to dissect the script or use alternative methods, you can clone the repo and run the script that way, or you can download the installer file and run it. You can also use Docker to deploy it, but I won’t cover those in this guide.&lt;/p&gt;&lt;code&gt;$ curl -sSL https://install.pi-hole.net | bash
&lt;/code&gt;&lt;p&gt;You’ll be greeted by a step by step graphical install. Assuming you’re using a VPS with a dedicated IP, you can confirm that you do have a Static IP and select Continue. You’ll next be asked for the interface, at this stage select &lt;code&gt;wg0&lt;/code&gt; as we want Pi-Hole to run for those connected on the Wireguard interface.&lt;/p&gt;&lt;p&gt;For upstream DNS provider I selected Quad9 for now. Next you’ll be asked to confirm using the suggested block list, select Yes and continue the install. Select Yes for the admin interface and select Yes to install &lt;code&gt;lighttpd&lt;/code&gt; and the required PHP modules (adjust if you don’t wish to use these). It is up to yourself if you wish to enable query logging or not. You can turn this off later on, if you enable it now. Select the privacy level for the query log.&lt;/p&gt;&lt;p&gt;Once the install finishes you should see the IP on which Pi-Hole is running, the interface on which it is running and the password. Make sure to save the password.&lt;/p&gt;&lt;p&gt;With both the server and the client configured and Pi-Hole installed, we’re close to turning on the client. Before that, there is one thing to consider. The client configuration sets the server as the DNS resolver. However, the server currently only allows access to port 12345 (SSH) and port 51820 for Wireguard. Ubuntu has &lt;code&gt;systemd-resolved&lt;/code&gt; using port 53 for DNS resolution, but access to this port is currently restricted, so I’ll add one more rule to UFW on the server, to allow requests to port 53 coming from the Wireguard IP range I defined earlier.&lt;/p&gt;&lt;p&gt;I also want to be able to configure Pi-Hole once I bring the Wireguard interface up, so for that I need to tell UFW to allow connections to port 80 (HTTP) on the Wireguard IP range.&lt;/p&gt;&lt;code&gt;# Allow requests to port 53 from any ip in the 10.10.10.1 - 10.10.10.255 range
$ ufw allow from 10.10.10.0/24 to any port 53
# Allow requests to port 80 from any ip in the 10.10.10.1 - 10.10.10.255 range
$ ufw allow from 10.10.10.0/24 to any port 80
# Reload firewall to apply settings
$ ufw reload
# Check rules are showing up
$ ufw status numbered
&lt;/code&gt;&lt;p&gt;At this point, I can copy the contents of the client config to my desktop, bring up the interface on the desktop (using &lt;code&gt;wg-quick&lt;/code&gt;) and navigate to http://10.10.10.1/admin and change any settings on the Pi-Hole instance running on the VPS server. But before doing that, I want to have Unbound setup on the server too, and then bring up the interface on my desktop and change the DNS resolver on Pi-Hole from Quad9 to Unbound.&lt;/p&gt;&lt;p&gt;Unbound is a recursive, caching DNS resolver. During the Pi-Hole setup, I had picked an upstream DNS resolver, but the issue is, from a privacy standpoint, that the upstream server (in this case Quad9) knows my queries and the queries of everyone using VPS as their DNS, because it’s not VPS resolving the query, it just forwards it to the upstream provider. More information on this, along with the Unbound configuration and setup can be found on the Pi-Hole website.&lt;/p&gt;&lt;p&gt;In this guide I’ll just walk you through the setup of Unbound and the configuration I use. To start off, I’ll install unbound and configure it.&lt;/p&gt;&lt;code&gt;$ apt install unbound
&lt;/code&gt;&lt;p&gt;Chances are that at this point when you check the status of the Unbound service, it shows as failed. The reason is that you have already a resolver running on port 53. To get around this, I setup a new configuration for Unbound in line with the one provided in the Pi-Hole documentation, with a few tweaks to match my VPS and IP ranges.&lt;/p&gt;&lt;p&gt;I create the new configuration file at /etc/unbound/unbound.conf.d/pi-hole.conf, and more details on options and how to change them can be found at the Unbound configuration documentation.&lt;/p&gt;&lt;code&gt;$ nano /etc/unbound/unbound.conf.d/pi-hole.conf
# Add the below to the configuration file; adjust according to your needs and server capabilities  
server:
  num-threads: 4

  # Enable operation information logging; up to 5
  verbosity: 1

  # Listen to queries on all interfaces
  interface: 127.0.0.1
  port: 5353

  # Disable ipv6
  do-ip6: no

  # IP range authorized to send queries to DNS
  access-control: 0.0.0.0/0 refuse
  access-control: 127.0.0.1/32 allow
  access-control: 10.10.10.0/24 allow

  # Hide id.server and hostname.bind
  hide-identity: yes

  # Hide version.server and version.bind
  hide-version: yes

  # Hide addresses on the private network
  private-address: 10.0.0.0/8

  # A total number of unwanted replies is kept track of; when reached cache is cleared to prevent DNS Poisoning
  unwanted-reply-threshold: 10000000

  # Because my server has low traffic/usage I enable prefetch; this adds load but cache elements are prefetched before expiry
  prefetch: yes
  prefetch-key: yes

  # Add minimum cache lifetime in seconds
  cache-min-ttl: 1800
  cache-max-ttl: 14400

  # Secure DNS and use DNSSEC
  harden-glue: yes
  harden-dnssec-stripped: yes
&lt;/code&gt;&lt;p&gt;Next, I restart the service and run a test query to make sure it’s resolving it.&lt;/p&gt;&lt;code&gt;$ systemctl restart unbound.service
$ systemctl status unbound.service
$ dig news.ycombinator.com/ @127.0.0.1 -p 5353
; &amp;lt;&amp;lt;&amp;gt;&amp;gt; DiG 9.18.28-0ubuntu0.24.04.1-Ubuntu &amp;lt;&amp;lt;&amp;gt;&amp;gt; news.ycombinator.com/ @127.0.0.1 -p 5353
;; global options: +cmd
;; Got answer:
;; -&amp;gt;&amp;gt;HEADER&amp;lt;&amp;lt;- opcode: QUERY, status: NXDOMAIN, id: 31572
;; flags: qr rd ra ad; QUERY: 1, ANSWER: 0, AUTHORITY: 1, ADDITIONAL: 1

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 1232
;; QUESTION SECTION:
;news.ycombinator.com/.		IN	A

;; AUTHORITY SECTION:
.			3563	IN	SOA	a.root-servers.net. nstld.verisign-grs.com. 2024120801 1800 900 604800 86400

;; Query time: 18 msec
;; SERVER: 127.0.0.1#5353(127.0.0.1) (UDP)
;; WHEN: Sun Jan 01 12:14:17 UTC 2024
;; MSG SIZE  rcvd: 125
&lt;/code&gt;&lt;p&gt;The query should now be resolved by Unbound as the &lt;code&gt;SERVER&lt;/code&gt; in the response gives &lt;code&gt;127.0.0.1#5353&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;With this, we’re almost done. I now turn on the Wireguard interface on the desktop as I have the configuration file prepared previously, and I added the client to the VPS’s list of peers.&lt;/p&gt;&lt;p&gt;This will disconnect the current SSH connection, but assuming all previous steps went OK, I am able to reconnect to the VPS server and also access http://10.10.10.1/admin to reach the admin interface for the current Pi-Hole installation.&lt;/p&gt;&lt;code&gt;# On my desktop I will enable the dwg0 service
$ sudo systemctl enable wg-quick@dwg0.service
# Start the dwg0 service which will create the dwg0 interface on my desktop 
# (I could also name this wg0, I just opted to rename it 
# to make it easier when working with multiple configs)
$ sudo systemctl start wg-quick@dwg0.service
# Check the status of the service and make sure it is active and running
$ sudo systemctl status wg-quick@dwg0.service
&lt;/code&gt;&lt;p&gt;Now I can go to the admin interface of the Pi-Hole installation I had setup earlier and change the DNS provider from Quad9 to the instance of Unbound that runs on port 5353 on the VPS.&lt;/p&gt;&lt;p&gt;Once I log into the interface using the password displayed at the end of the Pi-Hole installation, I next go to Settings, select the DNS tab and here uncheck the Quad9 section in the Upstream DNS Servers part of the page. Then, I check the Custom 1 (IPv4) checkbox and enter &lt;code&gt;127.0.0.1#5353&lt;/code&gt;, make sure that Bind only to interface wg0 is selected and then Save the settings.&lt;/p&gt;&lt;p&gt;Now I can visit DNS Leak Test and run an extended test. If everything runs as expected, the only result here should be the IP address of the VPS. If you are getting different servers showing up it means you are leaking DNS queries. There are a multitude of reasons for this, on Ubuntu derived distributions it might be down to Network Manager changing your DNS settings or you having different DNS servers configured which override the Wireguard DNS server.&lt;/p&gt;&lt;p&gt;This can be difficult to debug and it could take some time for you to chase it down, but some tips to get started would be to run &lt;code&gt;resolvectl dns&lt;/code&gt; which will show you the global DNS (if this was configured in /etc/systemd/resolved.conf) or if a global DNS is picked up from somewhere else like /run/systemd/resolve/resolved.conf or Network Manager. For Network Manager, check /etc/NetworkManager/system-connections and look for the name of your connection. The file in there will have a setting in the &lt;code&gt;[ipv4]&lt;/code&gt; section called &lt;code&gt;dns&lt;/code&gt;. This supports multiple DNS settings separated by semicolons.&lt;/p&gt;&lt;p&gt;There are further steps you could take and I recommend the following article by Andrea Corbellini. You can also check to see if it was indeed Pi-Hole that listened and resolved your query by running &lt;code&gt;lsof -i -P -n | grep LISTEN&lt;/code&gt; on the VPS and check to see if pihole is actually listening on port 53 or if there is another resolver that is using the port and responding to your queries.&lt;/p&gt;&lt;p&gt;While getting a configuration file from the VPS to a device with a keyboard is simple, Android and iOS devices aren’t that straightforward. To get a configuration over to a mobile device, I use &lt;code&gt;qrencode&lt;/code&gt;. The application creates a QR Code from a given configuration file.&lt;/p&gt;&lt;code&gt;# Install qrencode
$ apt install qrencode
# Create a QR Code from the previously created dwg0.conf file  
$ qrencode -t ansiutf8 &amp;lt; dwg0.conf
# If the file was created by root and you are now signed in with a non-root account then use 
$ sudo sh -c 'qrencode -t ansiutf8 &amp;lt; dwg0.conf'
&lt;/code&gt;&lt;p&gt;The above command outputs a QR code to the terminal directly, which you can then scan with the Wireguard application on iOS or Android. Don’t forget that you need different configuration files for each new device you wish to add, so don’t try to reuse the same configuration file across multiple devices.&lt;/p&gt;&lt;p&gt;One more goal that I wanted to achieve was that of being able to access my home server resources while I’m not on my home network. The final setup is a bit clunky, but it works for now. I’d be curious about any improvements any of you out there can think of. Feel free to e-mail me with any alternatives or ideas on the vpn email address for this domain.&lt;/p&gt;&lt;p&gt;First, go to the Pi-Hole administrator website, select Local DNS and then DNS Records. Then, add a number of domain/IP addresses to cover your use cases. While the VPN IP address range covers &lt;code&gt;10.10.10.0/24&lt;/code&gt; the local home network IP address range covers &lt;code&gt;192.168.1.0/24&lt;/code&gt;. Assuming these IP ranges, I ended up with the following list, assuming the home server runs on &lt;code&gt;10.10.10.10&lt;/code&gt; within the Wireguard network and &lt;code&gt;192.168.1.10&lt;/code&gt; on the local network:&lt;/p&gt;&lt;code&gt;192.168.1.10&lt;/code&gt;&lt;code&gt;10.10.10.10&lt;/code&gt;&lt;code&gt;192.168.1.10&lt;/code&gt;&lt;code&gt;10.10.10.10&lt;/code&gt;&lt;p&gt;The home server is running a reverse proxy in front of these services, so each call gets resolved by the reverse proxy to their respective services. The VPS is already allowing traffic to port &lt;code&gt;80&lt;/code&gt;, but if you are using HTTPS on any of these services, you would also need a rule to allow traffic to port &lt;code&gt;443&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;Lastly, any request coming in on the VPS &lt;code&gt;wg0&lt;/code&gt; interface needs to be forwarded out on the same interface, so a couple more &lt;code&gt;ufw&lt;/code&gt; rules need added to allow any device to easily reach these services whether they sit on the same LAN, or connect via Wireguard.&lt;/p&gt;&lt;code&gt;# Set the following ufw rules on the VPS - this allows forwarding of requests received on wg0 on wg0
$ ufw route allow in on wg0 out on wg0
# Set the rule to allow HTTPS traffic if your services are running with HTTPS
$ ufw allow from 10.10.10.0/24 to any port 443
$ ufw reload
&lt;/code&gt;&lt;p&gt;An easier way to persist the rules is to add them to the &lt;code&gt;PostUp&lt;/code&gt; and &lt;code&gt;PreDown&lt;/code&gt; sections of the Wireguard configuration file. The above settings also mean that if I’m on my home network, I can use emby.home.server and access my home server with its local IP address and when I’m outside my home network, then I can access it via emby.travel.server. It’s not ideal, but this seemed to work most consistently across various OSes, devices and apps.&lt;/p&gt;&lt;p&gt;With this change, the travel or external domain should work, but the local one might not, depending on the device you try to access it from. That is because the &lt;code&gt;AllowedIPs&lt;/code&gt; setting tells Wireguard to route all traffic through the tunel, but the IP 192.168.1.10 is not reachable through the Wireguard network, as it is an internal network IP address. On most Linux distributions, there are default rules in place to route traffic to these internal or default IP ranges. On Linux you can check the default routes with &lt;code&gt;ip route show table main&lt;/code&gt;. On other devices, you need to adjust the &lt;code&gt;AllowedIPs&lt;/code&gt; setting, so that all traffic, except for that particular IP range, is routed through the Wireguard tunnel.&lt;/p&gt;&lt;p&gt;I used this Wireguard AllowedIPs Calculator to update the IP range for my mobile devices. By setting Allowed IPs on the page to &lt;code&gt;0.0.0.0/0&lt;/code&gt; and the Disallowed IPs to &lt;code&gt;192.168.1.0/24&lt;/code&gt;, I would get a list of &lt;code&gt;AllowedIPs&lt;/code&gt; like such:&lt;/p&gt;&lt;code&gt;AllowedIPs = 0.0.0.0/1, 128.0.0.0/2, 192.0.0.0/9, 192.128.0.0/11, 192.160.0.0/13, 192.168.0.0/24, 192.168.2.0/23, 192.168.4.0/22, 192.168.8.0/21, 192.168.16.0/20, 192.168.32.0/19, 192.168.64.0/18, 192.168.128.0/17, 192.169.0.0/16, 192.170.0.0/15, 192.172.0.0/14, 192.176.0.0/12, 192.192.0.0/10, 193.0.0.0/8, 194.0.0.0/7, 196.0.0.0/6, 200.0.0.0/5, 208.0.0.0/4, 224.0.0.0/3
&lt;/code&gt;&lt;p&gt;On mobile clients you can also go directly into the Wireguard app, click on edit and select the checkbox Exclude private IPs then save the changes to the configuration file. With these changes you should now be able to reach applications on your local network or through the tunnel using the above domain names.&lt;/p&gt;&lt;p&gt;If after the setup, you notice that your connection isn’t that great or that you see a significant drop in connection performance, you can test the connection speed between the VPS and a client that is running Linux using &lt;code&gt;iperf3&lt;/code&gt;. This is a good way to check if your speed is slow because of the VPS or some other factors. I would recommend using the Cloudflare Speedtest on a client to get an idea of your current speed without the Wireguard tunnel enabled.&lt;/p&gt;&lt;p&gt;You can then use the &lt;code&gt;speedtest-cli&lt;/code&gt; application on the VPS to test its upload and download connection and then use &lt;code&gt;iperf3&lt;/code&gt; to test the speed of the connection between client and VPS to find any bottlenecks.&lt;/p&gt;&lt;p&gt;On the VPS I install &lt;code&gt;iperf3&lt;/code&gt; and allow connections on port &lt;code&gt;5201&lt;/code&gt; to run the speed test. I then start &lt;code&gt;iperf3&lt;/code&gt; in server mode, which sets it up to listen for incoming requests.&lt;/p&gt;&lt;code&gt;# Install iperf3 if not already installed
$ apt install iperf3
# Allow connections on port 5201 from the Wireguard IP range
$ ufw allow from 10.10.10.0/24 to any port 5201
$ ufw reload
# Start iperf3 in server mode
$ iperf3 --server
# Install iperf3 on client too and then start a test by defining the ip of the VPS on the Wireguard network
# The below command will test the upload speed from client to VPS
$ iperf3 --client 10.10.10.1
Connecting to host 10.10.10.1, port 5201
[  5] local 10.10.10.11 port 56590 connected to 10.10.10.1 port 5201
[ ID] Interval           Transfer     Bitrate         Retr  Cwnd
[  5]   0.00-1.00   sec  1.21 MBytes  10.1 Mbits/sec    0   81.5 KBytes       
[  5]   1.00-2.00   sec  2.39 MBytes  20.1 Mbits/sec    0    195 KBytes       
[  5]   2.00-3.00   sec  2.94 MBytes  24.7 Mbits/sec    0    331 KBytes       
[  5]   3.00-4.00   sec  3.37 MBytes  28.3 Mbits/sec    0    484 KBytes       
[  5]   4.00-5.00   sec  3.86 MBytes  32.4 Mbits/sec    0    640 KBytes       
[  5]   5.00-6.00   sec  2.45 MBytes  20.5 Mbits/sec    0    786 KBytes       
[  5]   6.00-7.00   sec  3.86 MBytes  32.4 Mbits/sec    0    945 KBytes       
[  5]   7.00-8.00   sec  3.50 MBytes  29.3 Mbits/sec    0   1.08 MBytes       
[  5]   8.00-9.00   sec  3.75 MBytes  31.5 Mbits/sec    0   1.23 MBytes       
[  5]   9.00-10.00  sec  2.50 MBytes  21.0 Mbits/sec    0   1.35 MBytes       
- - - - - - - - - - - - - - - - - - - - - - - - -
[ ID] Interval           Transfer     Bitrate         Retr
[  5]   0.00-10.00  sec  29.8 MBytes  25.0 Mbits/sec    0             sender
[  5]   0.00-10.61  sec  27.6 MBytes  21.8 Mbits/sec                  receiver

iperf Done.
# The below command will test the download speed from VPS to the client
$ iperf3 --client 10.10.10.1 --reverse
Connecting to host 10.10.10.1, port 5201
Reverse mode, remote host 10.10.10.1 is sending
[  5] local 10.10.10.11 port 34346 connected to 10.10.10.1 port 5201
[ ID] Interval           Transfer     Bitrate
[  5]   0.00-1.00   sec  3.50 MBytes  29.4 Mbits/sec                  
[  5]   1.00-2.00   sec  7.35 MBytes  61.7 Mbits/sec                  
[  5]   2.00-3.00   sec  11.7 MBytes  98.0 Mbits/sec                  
[  5]   3.00-4.00   sec  23.6 MBytes   198 Mbits/sec                  
[  5]   4.00-5.00   sec  39.6 MBytes   332 Mbits/sec                  
[  5]   5.00-6.00   sec  44.4 MBytes   372 Mbits/sec                  
[  5]   6.00-7.00   sec  40.6 MBytes   340 Mbits/sec                  
[  5]   7.00-8.00   sec  40.5 MBytes   340 Mbits/sec                  
[  5]   8.00-9.00   sec  38.4 MBytes   322 Mbits/sec                  
[  5]   9.00-10.00  sec  40.8 MBytes   342 Mbits/sec                  
- - - - - - - - - - - - - - - - - - - - - - - - -
[ ID] Interval           Transfer     Bitrate         Retr
[  5]   0.00-10.05  sec   294 MBytes   245 Mbits/sec    0             sender
[  5]   0.00-10.00  sec   290 MBytes   244 Mbits/sec                  receiver

iperf Done.
&lt;/code&gt;&lt;p&gt;In the above tests I can see that my upload speed to this test VPS is between 10 and 30 Mbps and my download speed is between 30 and 340 Mbps. After completing the test, I can stop the server on the VPS and even close down the port. For more information on tuning the performance of Wireguard, I recommend the detailed article on the Pro Custodibus website.&lt;/p&gt;&lt;p&gt;At this stage, you should have your VPS setup, your first two devices connected and you should also be able to access any remote resources.&lt;/p&gt;&lt;p&gt;Most of this could not be done without the articles below. These were great resources and I would greatly recommend them for further reading of a particular topic:&lt;/p&gt;&lt;p&gt;There are plenty more websites that I read through for quick fixes and I apologise for not recording those consistently, as the information there helped me fix some local issues on my Pop OS desktop.&lt;/p&gt;&lt;p&gt;In any case, I hope this article was useful and helped you setup your own Wireguard VPN server, access resources on your home network and provide network-wide ad-block for all your devices. By using a VPS you can generally cover a variety of devices for around $10 a month. This beats a lot of the providers out there and it does so while offering you full control over blocking lists and resources on the network.&lt;/p&gt;&lt;p&gt;If this guide was useful in any way, please make sure to support the Wireguard project, the Pi-Hole project, Unbound and all the other open source projects that allow us to gain some modicum of control over out digital lives!&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45552049</guid><pubDate>Sat, 11 Oct 2025 19:41:27 +0000</pubDate></item><item><title>Ask HN: Abandoned/dead projects you think died before their time and why?</title><link>https://news.ycombinator.com/item?id=45553132</link><description>&lt;doc fingerprint="36a899551ba72dd9"&gt;
  &lt;main&gt;
    &lt;p&gt;It's the closest thing to a Unix successor we ever got, taking the "everything is a file" philosophy to another level and allowing to easily share those files over the network to build distributed systems. Accessing any remote resources is easy and robust on Plan9, meanwhile on other systems we need to install specialized software with bad interoperability for each individual use case.&lt;/p&gt;
    &lt;p&gt;Plan9 also had some innovative UI features, such as mouse chording to edit text, nested window managers, the Plumber to run user-configurable commands on known text patterns system-wide, etc.&lt;/p&gt;
    &lt;p&gt;Its distributed nature should have meant it's perfect for today's world with mobile, desktop, cloud, and IoT devices all connected to each other. Instead, we're stuck with operating systems that were never designed for that.&lt;/p&gt;
    &lt;p&gt;There are still active forks of Plan9 such as 9front, but the original from Bell Labs is dead. The reasons it died are likely:&lt;/p&gt;
    &lt;p&gt;- Legal challenges (Plan9 license, pointless lawsuits, etc.) meant it wssn't adopted by major players in the industry.&lt;/p&gt;
    &lt;p&gt;- Plan9 was a distributed OS during a time when having a local computer became popular and affordable, while using a terminal to access a centrally managed computer fell out of fashion (though the latter sort of came back in a worse fashion with cloud computing).&lt;/p&gt;
    &lt;p&gt;- Bad marketing and posing itself as merely a research OS meant they couldn't capitalize on the .com boom.&lt;/p&gt;
    &lt;p&gt;- AT&amp;amp;T lost its near endless source of telephone revenue. Bell Labs was sold multiple times over the coming years, a lot of the Unix/Plan9 guys went to other companies like Google.&lt;/p&gt;
    &lt;p&gt;The reason Plan 9 died a swift death was that, unlike Unix – which hardware manufacturers could license for a song and adapt to their own hardware (and be guaranteed compatibility with lots of Unix software) – Bell Labs tried to sell Plan 9, as commercial software, for $350 a box.&lt;/p&gt;
    &lt;p&gt;Version 1 was never licensed to anyone. Version 2 was only licensed to universities for an undiscolsed price. Version 3 was sold as a book, I think this is the version you are referring to. However note that this version contained a license that only allowed non commercial uses of the source code. It also came with no support, no community and no planned updates (the project was shelved half a year later in favor of inferno)&lt;/p&gt;
    &lt;p&gt;More than the price tag the problem is that plan 9 wasn't really released until 2004.&lt;/p&gt;
    &lt;p&gt;Probably that not everything can be cleanly abstracted as a file.&lt;/p&gt;
    &lt;p&gt;One might want to, e. G., have fine control over a how a network connection is handled. You can abstract that as a file but it becomes increasingly complicated and can make API design painful.&lt;/p&gt;
    &lt;p&gt;They have to an extent. The /proc file system on Linux is directly inspired by plan 9 IIRC. Other things like network sockets never got that far and are more related to their BSD kin.&lt;/p&gt;
    &lt;p&gt;- Photon, the graphical interface for QNX. Oriented more towards real time (widgets included gauges) but good enough to support two different web browsers. No delays. This was a real time operating system.&lt;/p&gt;
    &lt;p&gt;- MacOS 8. Not the Linux thing, but Copeland. This was a modernized version of the original MacOS, continuing the tradition of no command line. Not having a command line forces everyone to get their act together about how to install and configure things. Probably would have eased the tradition to mobile. A version was actually shipped to developers, but it had to be covered up to justify the bailout of Next by Apple to get Steve Jobs.&lt;/p&gt;
    &lt;p&gt;- Transaction processing operating systems. The first one was IBM's Customer Information Control System. A transaction processor is a kind of OS where everything is like a CGI program - load program, do something, exit program. Unix and Linux are, underneath, terminal oriented time sharing systems.&lt;/p&gt;
    &lt;p&gt;- IBM MicroChannel. Early minicomputer and microcomputer designers thought "bus", where peripherals can talk to memory and peripherals look like memory to the CPU. Mainframes, though, had "channels", simple processors which connected peripherals to the CPU. Channels could run simple channel programs, and managed device access to memory. IBM tried to introduce that with the PS2, but they made it proprietary and that failed in the marketplace. Today, everything has something like channels, but they're not a unified interface concept that simplifies the OS.&lt;/p&gt;
    &lt;p&gt;- CPUs that really hypervise properly. That is, virtual execution environments look just like real ones. IBM did that in VM, and it worked well because channels are a good abstraction for both a real machine and a VM. Storing into device registers to make things happen is not. x86 has added several layers below the "real machine" layer, and they're all hacks.&lt;/p&gt;
    &lt;p&gt;- The Motorola 680x0 series. Should have been the foundation of the microcomputer era, but it took way too long to get the MMU out the door. The original 68000 came out in 1978, but then Motorola fell behind.&lt;/p&gt;
    &lt;p&gt;- Modula. Modula 2 and 3 were reasonably good languages. Oberon was a flop. DEC was into Modula, but Modula went down with DEC.&lt;/p&gt;
    &lt;p&gt;- XHTML. Have you ever read the parsing rules for HTML 5, where the semantics for bad HTML were formalized? Browsers should just punt at the first error, display an error message, and render the rest of the page in Times Roman. Would it kill people to have to close their tags properly?&lt;/p&gt;
    &lt;p&gt;- Word Lens. Look at the world through your phone, and text is translated, standalone, on the device. No Internet connection required. Killed by Google in favor of hosted Google Translate.&lt;/p&gt;
    &lt;p&gt;I was all gung ho on XHTML back in the day until I realized that a single unclosed tag in an ad or another portion of our app that I had no control over would cause the entire page to fail. The user would see nothing except a giant ugly error. And your solution of rendering the rest of the page in Times New Roman isn’t an option. Do you try to maintain any of the HTML semantics or just render plain text? If it’s plain text, that’s useless. If you’re rendering anything with any semantics, then you need to know how to parse it. You’re back where you started.&lt;/p&gt;
    &lt;p&gt;Granted, I could ensure that my code was valid XHTML, but I’m a hypermeticulous autistic weirdo, and most other people aren’t. As much as XHTML “made sense”, it was completely unworkable in reality, because most people are slobs. Sometimes, worse really is better.&lt;/p&gt;
    &lt;p&gt;if the world was all XHTML, then you wouldn't put an ad on your site that wasn't valid XHTML, the same way you wouldn't import a python library that's not valid python.&lt;/p&gt;
    &lt;p&gt;&amp;gt;, then you wouldn't put an ad on your site that wasn't valid XHTML,&lt;/p&gt;
    &lt;p&gt;You're overlooking how incentives and motivations work. The gp (and their employer) wants to integrate the advertisement snippet -- even with broken XHTML -- because they receive money for it.&lt;/p&gt;
    &lt;p&gt;The semantic data ("advertiser's message") is more important than the format ("purity of perfect XHTML").&lt;/p&gt;
    &lt;p&gt;If there was a hypothetical browser that refused to load that Monster.com webpage full of errors because it's for the users' own good and the "good of the ecosystem"... the websurfers would perceive that web browser as user-hostile and would choose another browser that would be forgiving of those errors and just load the page. Job hunters care more about the raw data of the actual job listings so they can get a paycheck rather than invalid &amp;lt;style&amp;gt; tags nested inside &amp;lt;div&amp;gt; tags.&lt;/p&gt;
    &lt;p&gt;Those situations above are a different category (semantic_content-overrides-fileformatsyntax) than a developer trying to import a Python library with invalid syntax (fileformatsyntax-Is-The-Semantic_Content).&lt;/p&gt;
    &lt;p&gt;If xhtml really took off, there would just be server side linting/html tidy. Its not that hard a problem to solve. Lots of websites already do this for user generated html, because even if an unclosed div doesnt take down the whole thing its still ugly.&lt;/p&gt;
    &lt;p&gt;The real problem is the benefits of xhtml are largely imaginary so there isn't really a motivation to do that work.&lt;/p&gt;
    &lt;p&gt;Yes, you would be able to put an ad on your site that wasn't XHTML, because XHTML is just text parsed in the browser at runtime. And yes, that would fail, silently, or with a cryptic error&lt;/p&gt;
    &lt;p&gt;&amp;gt; MacOS 8. Not the Linux thing, but Copeland. This was a modernized version of the original MacOS, continuing the tradition of no command line. Not having a command line forces everyone to get their act together about how to install and configure things. Probably would have eased the tradition to mobile. A version was actually shipped to developers, but it had to be covered up to justify the bailout of Next by Apple to get Steve Jobs.&lt;/p&gt;
    &lt;p&gt;You have things backwards. The Copland project was horribly mismanaged. Anybody at Apple who came up with a new technology got it included in Copland, with no regard to feature creep or stability. There's a leaked build floating around from shortly before the project was cancelled. It's extremely unstable and even using basic desktop functionality causes hangs and crashes. In mid-late 1996, it became clear that Copland would never ship, and Apple decided the best course of action was to license an outside OS. They considered options such as Solaris, Windows NT, and BeOS, but of course ended up buying NeXT. Copland wasn't killed to justify buying NeXT, Apple bought NeXT because Copland was unshippable.&lt;/p&gt;
    &lt;p&gt;&amp;gt;- XHTML. [...] Would it kill people to have to close their tags properly?&lt;/p&gt;
    &lt;p&gt;XHTML appeals to the intuition that there should be a Strict Right Way To Do Things ... but you can't use that unforgiving framework for web documents that are widely shared.&lt;/p&gt;
    &lt;p&gt;The "real world" has 2 types of file formats:&lt;/p&gt;
    &lt;p&gt;(1) file types where consumers cannot contact/control/punish the authors (open-loop) : HTML, pdf, zip, csv, etc. The common theme is that the data itself is more important that the file format. That's why Adobe Reader will read malformed pdf files written by buggy PDF libraries. And both 7-Zip and Winrar can read malformed zip files with broken headers (because some old buggy Java libraries wrote bad zip files). MS Excel can import malformed csv files. E.g. the Citi bank export to csv wrote a malformed file and it was desirable that MS Excel imported it anyway because the raw data of dollar amounts was more important than the incorrect commas in the csv file -- and -- I have no way of contacting the programmer at Citi to tell them to fix their buggy code that created the bad csv file.&lt;/p&gt;
    &lt;p&gt;(2) file types where the consumer can control the author (closed-loop): programming language source code like .c, .java, etc or business interchange documents like EDI. There's no need to have a "lenient forgiving" gcc/clang compiler to parse ".c" source code because the "consumer-and-author" will be the same person. I.e. the developer sees the compiler stop at a syntax error so they edit and fix it and try to re-compile. For business interchange formats like EDI, a company like Walmart can tell the vendor to fix their broken EDI files.&lt;/p&gt;
    &lt;p&gt;XHTML wants to be in group (2) but web surfers can't control all the authors of .html so that's why lenient parsing of HTML "wins". XHTML would work better in a "closed-loop" environment such as a company writing internal documentation for its employees. E.g. an employee handbook can be written in strict XHTML because both the consumers and authors work at the same company. E.g. can't see the vacation policy because the XHTML syntax is wrong?!? Get on the Slack channel and tell the programmer or content author to fix it.&lt;/p&gt;
    &lt;p&gt;The problem is that group (1) results in a nightmarish race-to-the-bottom. File creators have zero incentive to create spec-compliant files, because there's no penalty for creating corrupted files. In practice this means a large proportion of documents are going to end up corrupt. Does it open in Chrome? Great, ship it! The file format is no longer the specification, but it has now become a wild guess at whatever weird garbage the incumbent is still willing to accept. This makes it virtually impossible to write a new parser, because the file format suddenly has no specification.&lt;/p&gt;
    &lt;p&gt;On the other hand, imagine a world where Chrome would slowly start to phase out its quirks modes. Something like a yellow address bar and a "Chrome cannot guarantee the safety of your data on this website, as the website is malformed" warning message. Turn it into a red bar and a "click to continue" after 10 years, remove it altogether after 20 years. Suddenly it's no longer that one weird customer who is complaining, but everyone - including your manager. Your mistakes are painfully obvious during development, so you have a pretty good incentive to properly follow the spec. You make a mistake on a prominent page and the CTO sees it? Well, guess you'll be adding an XHTML validator to your CI pipeline next week!&lt;/p&gt;
    &lt;p&gt;It is very tempting to write a lenient parser when you are just one small fish in a big ecosystem, but over time it will inevitably lead to the degradation of that very ecosystem. You need some kind of standards body to publish a validating reference parser. And like it or not, Chrome is big enough that it can act as one for HTML.&lt;/p&gt;
    &lt;p&gt;You’re right, but even standards bodies aren’t enough. At the end of the day, it’s always about what the dominant market leader will accept. The standard just gives your bitching about the corrupted files some abstract moral authority, but that’s about it.&lt;/p&gt;
    &lt;p&gt;That would break decades of the web with no incentive for Google to do so. Plus, any change of that scale that they make is going to draw antitrust consideration from _somebody_.&lt;/p&gt;
    &lt;p&gt;I’d argue a good comparison here is HTTPS. Everyone decided it would be good for sites to move over to serving via HTTPS so browsers incentivised people to move by gating newer features to HTTPS only. They could have easily done the same with XHTML had they wanted.&lt;/p&gt;
    &lt;p&gt;The opportunities to fix this were pretty abundant. For instance, it would take exactly five words from Google to magically make a vast proportion of web pages valid XHTML:&lt;/p&gt;
    &lt;p&gt;This is an argument for a repair function that transforms a broken document into a well-formed one without loss but keeps the spec small, simple and consistent. It's not an argument for baking malformations into a complex messy spec.&lt;/p&gt;
    &lt;p&gt;&amp;gt; - XHTML. Have you ever read the parsing rules for HTML 5, where the semantics for bad HTML were formalized? Browsers should just punt at the first error, display an error message, and render the rest of the page in Times Roman. Would it kill people to have to close their tags properly?&lt;/p&gt;
    &lt;p&gt;We stop at the first sign of trouble for almost every other format, we do not need lax parsing for HTML. This has caused a multitude of security vulnerabilities and only makes it more difficult for pretty much everybody.&lt;/p&gt;
    &lt;p&gt;The attitude towards HTML5 parsing seemed to grow out of this weird contrarianism that everybody who wanted to do better than whatever Internet Explorer did had their head in the clouds and that the role of a standard was just to write down all the bugs.&lt;/p&gt;
    &lt;p&gt;Just to remind you that &amp;lt;bold&amp;gt; &amp;lt;italic&amp;gt; text &amp;lt;/bold&amp;gt; &amp;lt;/italic&amp;gt; [0] that has been working for ages in every browser ever, is NOT a valid XHTML, and should be rejected by GP's proposal.&lt;/p&gt;
    &lt;p&gt;I, for one, is kinda happy that XHTML is dead.&lt;/p&gt;
    &lt;p&gt;[0]: By &amp;lt;bold&amp;gt; I mean &amp;lt;b&amp;gt; and by &amp;lt;italic&amp;gt; I mean &amp;lt;i&amp;gt;, and the reason it's not valid HTML is that the order of closing is not reverse of the order of opening as it should properly be.&lt;/p&gt;
    &lt;p&gt;That caused plenty of incompatibilities in the past. At one point, Internet Explorer would parse that and end up with something that wasn’t even a tree.&lt;/p&gt;
    &lt;p&gt;HTML is not a set of instructions that you follow. It’s a terrible format if you treat it that way.&lt;/p&gt;
    &lt;p&gt;XHTML allows you to use XML and &amp;lt;bold&amp;gt; &amp;lt;italic&amp;gt; are just XML nodes with no schema. The correct form has been and will always be &amp;lt;b&amp;gt; and &amp;lt;i&amp;gt;. Since the beginning.&lt;/p&gt;
    &lt;p&gt;Ooooo… now we’re talking. Sloppy HTML that closes a tag out of order or just declared out of order? Or rendering bugs when bold is before italic? It’s why XHTML should have been standard. Just dump, error out, make the developer fix it.&lt;/p&gt;
    &lt;p&gt;&amp;gt; Would it kill people to have to close their tags properly&lt;/p&gt;
    &lt;p&gt;It would kill the approachability of the language.&lt;/p&gt;
    &lt;p&gt;One of the joys of learning HTML when it tended to be hand-written was that if you made a mistake, you'd still see something just with distorted output.&lt;/p&gt;
    &lt;p&gt;That was a lot more approachable for a lot of people who were put off "real" programming languages because they were overwhelmed by terrible error messages any time they missed a bracket or misspelled something.&lt;/p&gt;
    &lt;p&gt;If you've learned to program in the last decade or two, you might not even realise just how bad compiler errors tended to be in most languages.&lt;/p&gt;
    &lt;p&gt;The kind of thing where you could miss a bracket on line 47 but end up with a compiler error complaining about something 20 lines away.&lt;/p&gt;
    &lt;p&gt;Rust ( in particular ) got everyone to bring up their game with respect to meaningful compiler errors.&lt;/p&gt;
    &lt;p&gt;But in the days of XHTML? Error messages were arcane, you had to dive in to see what the problem actually was.&lt;/p&gt;
    &lt;p&gt;If you forget a closing quote on an attribute in html, all content until next quote is ignored and not rendered - even if it is the rest of the page. I dont think this is more helpful than an error message. It was just simpler to implement.&lt;/p&gt;
    &lt;p&gt;For reference, observe what happens if you try opening this malformed document in a browser: save it with a .xhtml extension, or serve it with MIME type application/xhtml+xml.&lt;/p&gt;
    &lt;p&gt;Firefox displays naught but the error:&lt;/p&gt;
    &lt;p&gt;XML Parsing Error: mismatched tag. Expected: &amp;lt;/b&amp;gt;. Location: file:///tmp/x.xhtml Line Number 22, Column 3: &amp;lt;/p&amp;gt; --^&lt;/p&gt;
    &lt;p&gt;Chromium displays this banner on top of the document up to the error:&lt;/p&gt;
    &lt;p&gt;This page contains the following errors: error on line 22 at column 5: Opening and ending tag mismatch: b line 19 and p Below is a rendering of the page up to the first error.&lt;/p&gt;
    &lt;p&gt;Thanks for showing these. We can see Firefox matches the same style of accurate but unhelpful error message.&lt;/p&gt;
    &lt;p&gt;Chromium is much more helpful in the error message, directing the user to both line 19 and 22. It also made the user-friendly choice to render up to the error.&lt;/p&gt;
    &lt;p&gt;In the context of XHTML, we should also keep in mind that Chrome post-dates XHTML by almost a decade.&lt;/p&gt;
    &lt;p&gt;If, on the other hand, you have some sorts of XSLT errors, Firefox gives you a reasonably helpful error message in the dev tools, whereas Chromium gives you a blank document and nothing else… unless you ran it in a terminal. I’m still a little surprised that I managed to discover that it was emitting XSLT errors to stdout or stderr (don’t remember which).&lt;/p&gt;
    &lt;p&gt;Really, neither has particularly great handling of errors in anything XML. None of it is better than minimally maintained, a lot of it has simply been unmaintained for a decade or more.&lt;/p&gt;
    &lt;p&gt;&amp;gt; XHTML. Have you ever read the parsing rules for HTML 5, where the semantics for bad HTML were formalized?&lt;/p&gt;
    &lt;p&gt;I actually have, and its not that bad.&lt;/p&gt;
    &lt;p&gt;If anything, the worst part is foreign content (svg, mathml) which have different rules more similar to xml but also not the same as xml.&lt;/p&gt;
    &lt;p&gt;Just as an aside, browsers still support xhtml, just serve with application/xhtml+xml mime type, and it all works including aggressive error checking. This is very much a situation where consumers are voting with their feet not browser vendors forcing a choice.&lt;/p&gt;
    &lt;p&gt;- I think without the move to NeXT, even if Jobs had come back to Apple, they would never have been able to get to the iPhone. iOS was - and still is - a unix-like OS, using unix-like philosophy, and I think that philosophy allowed them to build something game-changing compared to the SOTA in mobile OS technology at the time. So much so, Android follows suit. It doesn't have a command line, and installation is fine, so I'm not sure your line of reasoning holds strongly. One thing I think you might be hinting at though that is a missed trick: macOS today could learn a little from the way iOS and iPadOS is forced to do things and centralise configuration in a single place.&lt;/p&gt;
    &lt;p&gt;- I think transaction processing operating systems have been reinvented today as "serverless". The load/execute/quit cycle you describe is how you build in AWS Lambdas, GCP Cloud Run Functions or Azure Functions.&lt;/p&gt;
    &lt;p&gt;- Most of your other ideas (with an exception, see below), died either because of people trying to grab money rather than build cool tech, and arguably the free market decided to vote with its feet - I do wonder when we might next get a major change in hardware architectures again though, it does feel like we've now got "x86" and "ARM" and that's that for the next generation.&lt;/p&gt;
    &lt;p&gt;- XHTML died because it was too hard for people to get stuff done. The forgiving nature of the HTML specs is a feature, not a bug. We shouldn't expect people to be experts at reading specs to publish on the web, nor should it need special software that gatekeeps the web. It needs to be scrappy, and messy and evolutionary, because it is a technology that serves people - we don't want people to serve the technology.&lt;/p&gt;
    &lt;p&gt;&amp;gt; XHTML died because it was too hard for people to get stuff done.&lt;/p&gt;
    &lt;p&gt;This is not true. The reason it died was because Internet Explorer 6 didn’t support it, and that hung around for about a decade and a half. There was no way for XHTML to succeed given that situation.&lt;/p&gt;
    &lt;p&gt;The syntax errors that cause XHTML to stop parsing also cause JSX to stop parsing. If this kind of thing really were a problem, it would have killed React.&lt;/p&gt;
    &lt;p&gt;People can deal with strict syntax. They can manage it with JSX, they can manage it with JSON, they can manage it with JavaScript, they can manage it with every back-end language like Python, PHP, Ruby, etc. The idea that people see XHTML being parsed strictly and give up has never had any truth to it.&lt;/p&gt;
    &lt;p&gt;&amp;gt; The syntax errors that cause XHTML to stop parsing also cause JSX to stop parsing. If this kind of thing really were a problem, it would have killed React.&lt;/p&gt;
    &lt;p&gt;JSX is processed during the build step, XHTML is processed at runtime, by the browser.&lt;/p&gt;
    &lt;p&gt;They would have gotten another modern OS instead of Next as the base for MacOSX (then iOS).&lt;/p&gt;
    &lt;p&gt;Another possibility they were exploring was buying BeOS, which would have been pretty interesting because it was an OS built from scratch in the 90's without any of the cruft from the 70's.&lt;/p&gt;
    &lt;p&gt;Also, the only thing specific to Next that survived in MacOSX and iOS was ObjectiveC and the whole NextStep APIs, which honestly I don't think it a great thing. It was pretty cool in the 90's but when the iPhone was released it was already kinda obsolete. For the kernel, Linux or FreeBSD would have worked just the same.&lt;/p&gt;
    &lt;p&gt;On XHTML, I think there was room for both HTML and a proper XHTML that barks on errors. If you're a human typing HTML or using a language where you build your HTML by concatenation like early PHP, sure it makes sense to allow loosey goosey HTML but if you're using any sort of simple DOM builder which should preclude you from the possibility of outputting invalid HTML, strict XHTML makes a lot more sense.&lt;/p&gt;
    &lt;p&gt;Honestly I'm disappointed the promised XHTML5 never materialized along side HTML5. I guess it just lost steam.&lt;/p&gt;
    &lt;p&gt;But a HTML5 parser will obviously parse "strict" HTML5 just fine too, what value is there to special-case the "this was generated by a DOM builder" path client-side?&lt;/p&gt;
    &lt;p&gt;&amp;gt; Honestly I'm disappointed the promised XHTML5 never materialized along side HTML5. I guess it just lost steam.&lt;/p&gt;
    &lt;p&gt;The HTML Standard supports two syntaxes, HTML and XML. All browsers support XML syntax just fine—always have, and probably always will. Serve your file as application/xhtml+xml, and go ham.&lt;/p&gt;
    &lt;p&gt;&amp;gt; Would it kill people to have to close their tags properly?&lt;/p&gt;
    &lt;p&gt;Probably not, but what would be the benefit of having more pages fail to render? If xhtml had been coupled with some cool features which only worked in xhtml mode, it might have become successful, but on its own it does not provide much value.&lt;/p&gt;
    &lt;p&gt;&amp;gt; but what would be the benefit of having more pages fail to render?&lt;/p&gt;
    &lt;p&gt;I think those benefits are quite similar to having more programs failing to run (due to static and strong typing, other static analysis, and/or elimination of undefined behavior, for instance), or more data failing to be read (due to integrity checks and simply strict parsing): as a user, you get documents closer to valid ones (at least in the rough format), if anything at all, and additionally that discourages developers from shipping a mess. Then parsers (not just those in viewers, but anything that does processing) have a better chance to read and interpret those documents consistently, so even more things work predictably.&lt;/p&gt;
    &lt;p&gt;Sure, authoring tools should help authors avoid mistakes and produce valid content. But the browser is a tool for the consumer of content, and there is no benefit for the user if it fails to to render some existing pages.&lt;/p&gt;
    &lt;p&gt;It is like Windows jumping through hoops to support backwards compatibility even with buggy software. The interest of the customer is that the software runs.&lt;/p&gt;
    &lt;p&gt;&amp;gt; there is no benefit for the user if it fails to to render some existing pages&lt;/p&gt;
    &lt;p&gt;What if the browser renders it incorrectly? If a corrupt tag combination leads to browser X parsing "&amp;lt;script&amp;gt;" as inline text but browser Y parsing it as a script tag, that could lead to serious security issues!&lt;/p&gt;
    &lt;p&gt;Blindly guessing at the original author's intent whenever you encounter buggy content is a recipe for disaster. Sometimes it is to the user's benefit to just refuse to render it.&lt;/p&gt;
    &lt;p&gt;if developer accidentally left opening comment at the start of the html.&lt;/p&gt;
    &lt;p&gt;Rhetorical question: Should the browser display page even if it is commented out?&lt;/p&gt;
    &lt;p&gt;There is some bar for what is expected to work.&lt;/p&gt;
    &lt;p&gt;If all browsers would consistently error out on unclosed tags, then it would definitely force developers to close tags, it would force it become common knowledge, second nature.&lt;/p&gt;
    &lt;p&gt;HTML5 was the answer for the consistency part: where before browsers did different things to recover from "invalid" HTML, HTML5 standardizes it because it doesn't care about valid/invalid as much, it just describes behavior anyways.&lt;/p&gt;
    &lt;p&gt;XHTML is XML. XML-based markup for content can be typeset into PDF, suitable for print media. I invite you to check out the PDFs listed in the intro to my feature matrix comparison page, all being sourced from XHTML:&lt;/p&gt;
    &lt;p&gt;&amp;gt; IBM MicroChannel. Early minicomputer and microcomputer designers thought "bus", where peripherals can talk to memory and peripherals look like memory to the CPU. Mainframes, though, had "channels", simple processors which connected peripherals to the CPU.&lt;/p&gt;
    &lt;p&gt;TIL: what microchannel meant by micro and channel.&lt;/p&gt;
    &lt;p&gt;Also it had OS independent device-class drivers.&lt;/p&gt;
    &lt;p&gt;And you could stuff a new CPU on a card and pop it right in. Went from a 286+2MB to a 486dx2+32MB.&lt;/p&gt;
    &lt;p&gt;&amp;gt; - XHTML. Have you ever read the parsing rules for HTML 5, where the semantics for bad HTML were formalized? Browsers should just punt at the first error, display an error message, and render the rest of the page in Times Roman. Would it kill people to have to close their tags properly?&lt;/p&gt;
    &lt;p&gt;IMO there's a place for XHTML as a generated output format, but I think HTML itself should stay easy to author and lightweight as a markup format. Specifically when it comes to tag omission, if I'm writing text I don't want to see a bunch of `&amp;lt;/li&amp;gt;` or `&amp;lt;/p&amp;gt;` everywhere. It's visual noise, and I just want a lightweight markup.&lt;/p&gt;
    &lt;p&gt;Word lens team was bought by google, its far better in google translate then the local app ever was. You could repeat the old app with a local LLM now pretty easily but it still won't be as close in quality as using google translate&lt;/p&gt;
    &lt;p&gt;I love this mismatched list of grievances and I find myself agreeing with most of them. XHTML and proper CPU hypervisors in particular.&lt;/p&gt;
    &lt;p&gt;People being too lazy to close the &amp;lt;br /&amp;gt; tag was apparently a gateway drug into absolute mayhem. Modern HTML is a cesspool. I would hate to have to write a parser that's tolerant enough to deal with all the garbage people throw at it. Is that part of the reason why we have so few browsers?&lt;/p&gt;
    &lt;p&gt;&amp;gt; People being too lazy to close the &amp;lt;br /&amp;gt; tag was apparently a gateway drug into absolute mayhem.&lt;/p&gt;
    &lt;p&gt;Your chronology is waaaaaaaaaaaay off.&lt;/p&gt;
    &lt;p&gt;&amp;lt;BR&amp;gt; came years before XML was invented. It was a tag that didn’t permit children, so writing it &amp;lt;BR&amp;gt;&amp;lt;/BR&amp;gt; would have been crazy, and inventing a new syntax like &amp;lt;BR// or &amp;lt;BR/&amp;gt; would have been crazy too. Spelling it &amp;lt;BR&amp;gt; was the obvious and reasonable choice.&lt;/p&gt;
    &lt;p&gt;The &amp;lt;br /&amp;gt; or &amp;lt;br/&amp;gt; spelling was added to HTML after XHTML had already basically lost, as a compatibility measure for porting back to HTML, since those enthusiastic about XHTML had taken to writing it and it was nice having a compatible spelling that did the same in both. (In XHTML you could also write &amp;lt;br&amp;gt;&amp;lt;/br&amp;gt;, but that was incorrect in HTML; and if you wrote &amp;lt;br /&amp;gt; in HTML it was equivalent to &amp;lt;br /=""&amp;gt;, giving you one attribute with name "/" and value "". There were a few growing pains there, such as how &amp;lt;input checked&amp;gt; used to mean &amp;lt;input checked="checked"&amp;gt;—it was actually the attribute name that was being omitted, not the value!—except… oh why am I even writing this, messy messy history stuff, engines doing their own thing blah blah blah, these days it’s &amp;lt;input checked=""&amp;gt;.&lt;/p&gt;
    &lt;p&gt;Really, the whole &amp;lt;… /&amp;gt; thing is more an artefact of an arguably-misguided idea after a failed reform. The absolute mayhem came first, not last.&lt;/p&gt;
    &lt;p&gt;&amp;gt; I would hate to have to write a parser that's tolerant enough to deal with all the garbage people throw at it.&lt;/p&gt;
    &lt;p&gt;The HTML parser is magnificent, by far the best spec for something reasonably-sized that I know of. It’s exhaustively defined in terms of state machines. It’s huge, far larger than one would like it to be because of all this compatibility stuff, but genuinely easy to implement if you have the patience. Seriously, go read it some time, it’s really quite approachable.&lt;/p&gt;
    &lt;p&gt;The reason XHTML failed is because the spec required it to be sent with a new MIME type (application/xml+xhtml I believe) which no webserver did out of the box. Everything defaulted to text/html, which all browsers would interpret as HTML, and given the mismatching doctype, would interpret as tag soup (quirks mode/lenient).&lt;/p&gt;
    &lt;p&gt;Meanwhile, local files with the doctype would be treated as XHTML, so people assumed the doctype was all you needed. So everyone who tried to use XHTML didn't realize that it would go back to being read as HTML when they upload it to their webserver/return it from PHP/etc. Then, when something went wrong/worked differently than expected, the author would blame XHTML.&lt;/p&gt;
    &lt;p&gt;Edit: I see that I'm getting downvoted here; if any of this is factually incorrect I would like to be educated please.&lt;/p&gt;
    &lt;p&gt;&amp;gt; The reason XHTML failed is because the spec required it to be sent with a new MIME type (application/xml+xhtml I believe) which no webserver did out of the box. Everything defaulted to text/html, which all browsers would interpret as HTML, and given the mismatching doctype, would interpret as tag soup (quirks mode/lenient).&lt;/p&gt;
    &lt;p&gt;None of that is correct.&lt;/p&gt;
    &lt;p&gt;It was perfectly spec. compliant to label XHTML as text/html. The spec. that covers this is RFC 2854 and it states:&lt;/p&gt;
    &lt;p&gt;&amp;gt; The text/html media type is now defined by W3C Recommendations; the latest published version is [HTML401]. In addition, [XHTML1] defines a profile of use of XHTML which is compatible with HTML 4.01 and which may also be labeled as text/html.&lt;/p&gt;
    &lt;p&gt;There’s no spec. that says you need to parse XHTML served as text/html as HTML not XHTML. As the spec. says, text/html covers both HTML and XHTML. That’s something that browsers did but had no obligation to.&lt;/p&gt;
    &lt;p&gt;The mismatched doctype didn’t trigger quirks mode. Browsers don’t care about that. The prologue could, but XHTML 1.0 Appendix C told you not to use that anyway.&lt;/p&gt;
    &lt;p&gt;Even if it did trigger quirks mode, that makes no difference in terms of tag soup. Tag soup is when you mis-nest tags, for instance &amp;lt;strong&amp;gt;&amp;lt;em&amp;gt;&amp;lt;/strong&amp;gt;&amp;lt;/em&amp;gt;. Quirks mode was predominantly about how it applied CSS layout. There are three different concepts being mixed up here: being parsed as HTML, parsing tag soup, and doctype switching.&lt;/p&gt;
    &lt;p&gt;The problem with serving application/xhtml+xml wasn’t anything to do with web servers. The problem was that Internet Explorer 6 didn’t support it. After Microsoft won the browser wars, they discontinued development and there was a five year gap between Internet Explorer 6 and 7. Combined with long upgrade cycles and operating system requirements, this meant that Internet Explorer 6 had to be supported for almost 15 years globally.&lt;/p&gt;
    &lt;p&gt;Obviously, if you can’t serve XHTML in a way browsers will parse as XML for a decade and a half, this inevitably kills XHTML.&lt;/p&gt;
    &lt;p&gt;Yes, I covered that; everyone assumed that you only needed to specify the doctype, but in practice browsers only accepted it for local files or HTTP responses with Content-Type: application/xml+xhtml. I've edited the comment to make that more explicit.&lt;/p&gt;
    &lt;p&gt;Lytro light field cameras. The tech was impressive and the company was able to put two products on to the shelves, though unfortunately they hadn't quite reached the image quality needed for professional photographers.&lt;/p&gt;
    &lt;p&gt;But now with the new Meta Ray-Bans featuring a light field display and with new media like gaussian splats we're on the verge of being able to make full usage of all the data those cameras were able capture, beyond the demos of "what if you could fix your focus after shooting" of back then.&lt;/p&gt;
    &lt;p&gt;Beyond high tech, there's a big market for novelty kinda-bad cameras like Polaroids or Instax. The first Lytro has the perfect form factor for that and was already bulky enough that slapping a printer on it wouldn't have hurt.&lt;/p&gt;
    &lt;p&gt;&amp;gt; unfortunately they hadn't quite reached the image quality needed for professional photographers.&lt;/p&gt;
    &lt;p&gt;I always wondered about that - since it works by interleaving pixels at different focal depths, there's always going to be a resolution tradeoff that a single-plane focus camera wouldn't.&lt;/p&gt;
    &lt;p&gt;It's such a cool idea though, and no more difficult to manufacturer than a sensor + micro lens array.&lt;/p&gt;
    &lt;p&gt;They don't capture a light field like Lytro did, they capture a regular image with a very deep depth of field, extract a depth map (usually with machine learning, but some phones augment it with stereoscopy or even LIDAR on high end iPhones) and then selectively blur based on depth.&lt;/p&gt;
    &lt;p&gt;Adobe Flash / Shockwave. After all these decades, I've yet to see a tool that makes it as easy to make games or multimedia as Flash did. One of many reminders recently (many others in politics) that humanity doesn't just inevitably or linearly move forward in any domain, or even 2 steps forward 1 step back. Some things are just lost to time - maybe rediscovered in a century, maybe never.&lt;/p&gt;
    &lt;p&gt;Flash players had zoom built in. And I believe there were textareas that allowed people to copy and paste text if they wanted, though it wasn't very common&lt;/p&gt;
    &lt;p&gt;Flash was the last thing that got people excited for the Web generally&lt;/p&gt;
    &lt;p&gt;Enabling novice normies to make games was excellent, and I believe the whole game industry benefited from this resulting injection of fresh ideas. A lot of indy developers with fresh takes on what games could be got started this way. Zachtronics is one example of many that comes to mind right now.&lt;/p&gt;
    &lt;p&gt;On the other hand, for every flash game made there were about ten thousands flash-based ads, and nearly as many websites that used flash poorly for things like basic navigation (remember flash based website dropdown menus?). And for a few years it seemed like every single restaurant with a website was using flash for the entire thing, the results were borderline unusable in the best cases. And let's not forget that as long as flash was dominant, it was choking out the demand to get proper video support into browsers. Flash based video players performed like dog shit and made life on Linux a real chore.&lt;/p&gt;
    &lt;p&gt;Godot is pretty awesome. Easy to learn, can do 2D or 3D, and can export to HTML5/webasm that works across all major OSes and browsers including mobile.&lt;/p&gt;
    &lt;p&gt;It’s far from perfect but I’ve been enjoying playing with it even for things that aren’t games and it has come a long way just in the last year or two. I feel like it’s close to (or is currently) having its Blender moment.&lt;/p&gt;
    &lt;p&gt;Even if Adobe had gotten their act together and fixed all security holes, Apple would have still killed it. It was always a threat as a popular design tool. And decades later, with the HTML canvas hype faded, there's still no replacement to what Adobe Flash could do - any designer could create stellar, interactive design that can be embedded into any website...without a monthly subscription.&lt;/p&gt;
    &lt;p&gt;I don't think thats the case. For the longest while flash was faster than js at doing anything vaguely graphic based. The issue for apple was that the CPU in the iphone wasn't fast enough to do flash and anything else. Moreover Adobe didn't get on with jobs when they were talking about custom versions.&lt;/p&gt;
    &lt;p&gt;You have to remember that "apps" were never meant to be a thing on the iphone, it was all about "desktop" like web performance.&lt;/p&gt;
    &lt;p&gt;Performance was way better than what we have now with modern web stacks, we just have more powerful computers.&lt;/p&gt;
    &lt;p&gt;I agree on security and bugs, but bugs can be fixed. It just shows neglect by Adobe, which was, I think, the real problem. I think that if Adobe seriously wanted to, it could have been a web standard.&lt;/p&gt;
    &lt;p&gt;Lots of people say performance was good, but that seems to be through the nostalgic lens of a handful of cool games.&lt;/p&gt;
    &lt;p&gt;Those did sometimes run really great, but most implementations were indeed very slow.&lt;/p&gt;
    &lt;p&gt;I remember vividly because it was part of my job back then to help with web performance and when we measured page speed and user interface responsiveness flash was almost always the worst.&lt;/p&gt;
    &lt;p&gt;Right. But that doesn't mean the performance of Flash was bad for what it was doing. Or that it was worse than the performance of doing the same thing in modern HTML+CSS now.&lt;/p&gt;
    &lt;p&gt;The default, and by far the most common, output from Flash had significantly slower click-to-response and for network latency and for rendering than HTML+CSS is today.&lt;/p&gt;
    &lt;p&gt;You remembering a few optimised instances does not change the reality that Flash was bad.&lt;/p&gt;
    &lt;p&gt;Macromedia Fireworks was an outstanding piece of software.&lt;/p&gt;
    &lt;p&gt;The 20 most common things you’d do with the tool were there for you in obvious toolbars. It had a lot of advanced features for image editing. It had a scripting language, so you could do bulk editing operations. It supported just about every file extension you could think of.&lt;/p&gt;
    &lt;p&gt;Most useful feature of all was that it’d load instantly. You’d click the icon on the desktop, and there’d be the Fireworks UI before you could finish blinking. Compared to 2025 Adobe apps, where you click the desktop icon and make a coffee while it starts, it’s phenomenal performance.&lt;/p&gt;
    &lt;p&gt;Flash performance is still better than current web stack's. Probably will always be - you could write non trivial games that would work on 128MB memory machine. Currently single browser tab with simple page can take more than that.&lt;/p&gt;
    &lt;p&gt;Yes. I never used flash personally, but I loved those little games people created with them. There was the whole scene of non developers creating little games of all kinds and it just ceased to exist.&lt;/p&gt;
    &lt;p&gt;Ruffle is amazing. I launched a 20+ year old game yesterday with zero compatibility issues. Even better than the original Flash because of superior security isolation mechanisms.&lt;/p&gt;
    &lt;p&gt;Are there any ways that I can make games or something&lt;/p&gt;
    &lt;p&gt;Like I want to make websites about me similar to those in neocities right, those flashy nice (good?) artistic UI&lt;/p&gt;
    &lt;p&gt;I suck at css. I don't know but I never really got a feedback attention loop and heck even AI can make it better than me&lt;/p&gt;
    &lt;p&gt;But I want to show the world what I myself can make as well and not just what I prompt or get back.&lt;/p&gt;
    &lt;p&gt;I want a good feedback loop, can flash be useful for this purpose? Like maybe I want a website like uh something early browser times. I am kinda interested in building something like netscape navigator esque thing even though I wasn't born in that era or maybe windows xp style.&lt;/p&gt;
    &lt;p&gt;I have mixed opinions about AI tbh. I genuinely just want to learn things right now, it might take me more time, I have been beating myself over using AI and not feeling equal to if writing things by hand. So I want to prove to myself that I can write things/learn things by hand as well. Like I tried using it to study but the lure to make things right away and then trapping you later is definitely there, it feels easy in the start imo and that's the lure and I kinda want to stay away with that lure to develop my skills, maybe not right now, then later.&lt;/p&gt;
    &lt;p&gt;Kids now create games in Roblox. More constrained, more commercial, more exploitative- but there is still a huge scene of non-developers creating games if you care to look.&lt;/p&gt;
    &lt;p&gt;Personal pet peeve, but as someone who still makes gifs, Image Ready. Adobe kind of absorbed Image Ready into Photoshop and it's just never lived up to how easy it was to make simple gifs in Image Ready&lt;/p&gt;
    &lt;p&gt;Edit: you asked why. I first saw it at SELF where Chris DiBona showed it to me and a close friend. It was awesome. Real time translation, integration of various types of messaging, tons of cool capabilities, and it was fully open source. What made it out of Google was a stripped down version of what I was shown, the market rejected it, and it was a sad day. Now, I am left with JIRA, Slack, and email. It sucks.&lt;/p&gt;
    &lt;p&gt;Google wave was built on an awesome technology layer, and they they totally blew in on the user interface.... deciding to treat it as a set of separate items instead of a single document everyone everywhere all at once could edit.... killed it.&lt;/p&gt;
    &lt;p&gt;It make it seem needlessly complicated, and effectively erased all the positives.&lt;/p&gt;
    &lt;p&gt;I was blown away by the demo but then after I thought about it, it seemed like a nightmare to me. All the problems of slack of having to manually check channels for updates except X 100 (yea, I get that slack wasn't available then. My point is I saw that it seemed impossible to keep up with nested constantly updated hierarchical threads. Keeping up with channels on slack is bad enough so imagine if Wave had succeeded. It'd be even worse.&lt;/p&gt;
    &lt;p&gt;Wave was great for conversation with one or two other people on a specific project, which I'm sure most people here used it for. I can't imagine it scaling well beyond that.&lt;/p&gt;
    &lt;p&gt;I managed trips with friends and it was a great form factor for ad-hoc discussions with docs and links included. I thought it was the future and in my very early programming days wrote probably the most insecure plugin ever to manage your servers.&lt;/p&gt;
    &lt;p&gt;Google Wave had awesome tech but if you look at the demo in hindsight you can tell it’s just not a very good product. They tried making an all-in-one kind of product which just doesn’t work.&lt;/p&gt;
    &lt;p&gt;In a sense Wave still exists but was split into multiple products, so I wouldn’t say it’s “dead”. The tech that powered it is still used today in many of Google’s popular products. It turns out that having separate interfaces for separate purposes is just more user friendly than an all-in-one.&lt;/p&gt;
    &lt;p&gt;Even the watered-down version of wave was something I used at my host startup, it was effectively our project management tool. And it was amazing at that.&lt;/p&gt;
    &lt;p&gt;I don't know how it would fare compared to the options available today, but back then, it shutting down was a tremendous loss.&lt;/p&gt;
    &lt;p&gt;I haven’t found one showing what Chris showed. Most seem to focus on just communications with little demonstration of productivity or other features. This is sad to me because its most glorious asset was being open source with a rich set of plugins/extensions allowing tons of functionality.&lt;/p&gt;
    &lt;p&gt;It's indeed not a good one. Discord refined instant messaging and bolts other things on top like forums but isn't fundamentally different. Google Wave was (and still is) a completely different paradigm. Everything was natively collaborative: it mixed instant messaging with document edition (like Google Docs or pads) and any widget you could think of (polls, calendars, playing music, drawing, ...) could be added by users through sandboxed Javascript. The current closest I can think of is DeltaChat's webxdc.&lt;/p&gt;
    &lt;p&gt;Google sucked/s at executive function because they completely lack appreciation for proper R&amp;amp;D and long-term investment and also kill things people use and love.&lt;/p&gt;
    &lt;p&gt;Yep. And rather than ask people, focus group, or look at the evidence, they just guess or do whatever they want. Not much leadership or community engagement appears to be involved.&lt;/p&gt;
    &lt;p&gt;Well, that's fair. Overpaid managers and principle engineers spun "secret projects" and products like Glass well to be an elitist experience for special people. But I won't forgive not letting Wave bake and mature.&lt;/p&gt;
    &lt;p&gt;Q: Do they have non-human shareholders I don't know about, or do they have shareholders who lack qualities present in most living human beings?&lt;/p&gt;
    &lt;p&gt;The Ricochet network. A packet mesh network providing ISDN speeds in the dialup era, wirelessly.&lt;/p&gt;
    &lt;p&gt;They burned through $5B of 1999 dollars, building out a network in 23 cities, and had effectively zero customers. Finally shut down in 2001.&lt;/p&gt;
    &lt;p&gt;All their marketing was focused on "mobile professionals", whoever those were, while ignoring home users who were clamoring for faster internet where other ISPs dragged their feet.&lt;/p&gt;
    &lt;p&gt;Today, 5G femtocells have replicated some of the concept (radically small cell radius to increase geographic frequency reuse), but without the redundancy -- a femtocell that loses its uplink is dead in the water, not serving as a relay node. A Ricochet E-radio that lost its uplink (but still had power) would simply adjust its routing table and continue operating.&lt;/p&gt;
    &lt;p&gt;I had a Ricochet modem in '98-99 living in San Francisco. Just 10 years later the iPhone was launched, on 3G networks that had integer multiples better performance. How would I have been better off had Ricochet survived? This seems like a place where technological progress went --- extremely --- in the right direction.&lt;/p&gt;
    &lt;p&gt;I loved my Ricochet modems so damn much. Sitting in a coffeeshop in Palo Alto with an Apple Powerbook and a second generation Ricochet modem rocking web browsing and ssh sessions at 56k when wifi was unknown to the general public. I still have a couple in a box somewhere and I am tempted to see if I can get them into star mode.&lt;/p&gt;
    &lt;p&gt;Google Reader: I will forever be salty about how Google killed something that likely required very little maintenance in the long run. It could have stayed exactly the same for a decade and I wouldn't have cared because I use an RSS reader exactly the same way I do that I did back in 2015.&lt;/p&gt;
    &lt;p&gt;Yes. That was the single worst business decision in Google history, as somebody correctly noted. It burned an enormous amount of goodwill for no gain whatsoever.&lt;/p&gt;
    &lt;p&gt;Killing Google Reader affected a relatively small number of users, but these users disporportionately happened to be founders, CTOs, VPs of engineering, social media luminaries, and people who eventually became founders, CTOs, etc. They had been painfully taught to not trust Google, and, since that time, they didn't. And still don't.&lt;/p&gt;
    &lt;p&gt;Just think of the data mining they could have had there.&lt;/p&gt;
    &lt;p&gt;They had a core set of ultra-connected users who touched key aspects of the entire tech industry. The knowledge graph you could have built out of what those people read and shared…&lt;/p&gt;
    &lt;p&gt;They could have just kept the entire service running with, what, 2 software engineers? Such a waste.&lt;/p&gt;
    &lt;p&gt;This would require the decision-maker to think and act at the scale and in interests of the entire company. Not at the scale of a promo packet for next perf: "saved several millions in operation costs by shutting down a low-impact, unprofitable service."&lt;/p&gt;
    &lt;p&gt;Yes, Google killing Reader was probably the first time they killed a popular product and what started the idea that any Google product could be killed at any time.&lt;/p&gt;
    &lt;p&gt;There is some truth in this. I fit into a few of these buckets and I don’t think I could ever recommend their enterprise stuff after having my favourite consumer products pulled.&lt;/p&gt;
    &lt;p&gt;&amp;gt; Google Play Music: I had uploaded thousands of MP3 files there. They killed it. I won't waste my time uploading again.&lt;/p&gt;
    &lt;p&gt;You can argue whether it's as good as GPM or not, but it's false to imply that your uploaded music disappeared when Google moved to YouTube Music. I made the transition, and all of my music moved without a new upload.&lt;/p&gt;
    &lt;p&gt;Picasa was awesome, they had face recognition years before almost everything else, in a nice offline package.&lt;/p&gt;
    &lt;p&gt;Unfortunately the last public version has a bug that randomly swaps face tags, so you end up training on the wrong persons faces just enough to throw it all off, and the recognition becomes effectively worthless on thousands of family photos. 8(&lt;/p&gt;
    &lt;p&gt;Digikam is a weak sauce replacement that barely gets the job done.&lt;/p&gt;
    &lt;p&gt;Hmm, good to know. But given Google's history, I assumed that it would stop working.&lt;/p&gt;
    &lt;p&gt;I also need to sell my Google Chromecast with Google TV 4K. Brand new, still in its shrink wrap. Bought it last year, to replace a flaky Roku. It was a flaky HDMI cable instead. I trust Roku more than Google for hardware support.&lt;/p&gt;
    &lt;p&gt;In absolutely shocking news, it did stop working and then Google went out of their way to fix it.&lt;/p&gt;
    &lt;p&gt;I genuinely thought all the chromecast audios I owned were useless bricks and was looking around for replacements and then they just started working again from an OTA update. Astounding. I assume someone got fired for taking time away from making search worse to do this.&lt;/p&gt;
    &lt;p&gt;I'm still amused that they killed Google Notebook and then a few years later created Google Keep, an application with basically the same exact feature set.&lt;/p&gt;
    &lt;p&gt;You can say that for a fair few of the services mentioned by GP.&lt;/p&gt;
    &lt;p&gt;Google killed a lot of things to consolidate them into more "integrated" (from their perspective) product offerings. Picasa -&amp;gt; Photos, Hangounts -&amp;gt; Meet, Music -&amp;gt; YT Premium.&lt;/p&gt;
    &lt;p&gt;No idea what NFC Wallet was, other than the Wallet app on my phone that still exists and works?&lt;/p&gt;
    &lt;p&gt;The only one I'm not sure about is Chromecast - a while back my ones had an "update" to start using their newer AI Assistant system for managing it. Still works.&lt;/p&gt;
    &lt;p&gt;I still use PICASA it works fine. However, when google severed the gdrive-photo linking it meant my photos didn’t automatically download from google to my PC. This is what killed google for me.&lt;/p&gt;
    &lt;p&gt;That was probably me, when I stopped using Google Search some years ago. :-) Got tired of the ads, the blog spam, and AI-generated content crap floating to the top of their results page.&lt;/p&gt;
    &lt;p&gt;Kagi has been a great replacement for me. Less blogspam I've found, plus it doesn't give me AI results unless I explicitly tell it I want AI results by adding a "?" to the end of my query.&lt;/p&gt;
    &lt;p&gt;The https://udm14.com/ flavor of Google is quite usable, though, esp with notable operators like inurl:this-or-that. But, all in all, yeah, gimme back vanilla Google search from 2008-2010 or so. Back then it was definitely a tool (I worked in investigative journalism at the time), whereas currently "searching" stands for sitting fingers crossed and hoping for the better. But, oh well. &amp;lt;/rant&amp;gt;&lt;/p&gt;
    &lt;p&gt;That's more what I meant. Sure, lots of people still type stuff into the URL bar that takes them to www.google.com/search. But whatever you want to call that results page now, it's no longer Google Search in anything but name.&lt;/p&gt;
    &lt;p&gt;same can be said if you compare www.google.com search from 2012 and 2022, times are changing… I am not defending google search here, I haven’t used it except by accident in long time now but to say google search is “dying” like you often hear (especially here on HN) is a serious detachment from reality&lt;/p&gt;
    &lt;p&gt;I use this free and extremely bare bones app made by a friend: https://apps.apple.com/us/app/max-where/id1579123291. It tracks your location constantly, has a basic viewer, and lets you export to CSV. That’s about it but it’s all I need.&lt;/p&gt;
    &lt;p&gt;Check out Dawarich, it has an official iOS app and you can use a number of 3rd party mobile apps to track your data and then upload it to server: either ran on your own hardware (FOSS self-hosted) or to the Dawarich Cloud one: https://dawarich.app&lt;/p&gt;
    &lt;p&gt;Which particular thing called Hangouts? There were at least two, frankly I’d say more like four.&lt;/p&gt;
    &lt;p&gt;Google and Microsoft are both terrible about reusing names for different things in confusing ways.&lt;/p&gt;
    &lt;p&gt;&amp;gt; Can't keep track of all the Google chat apps.&lt;/p&gt;
    &lt;p&gt;And Hangouts was part of that problem. Remember Google Talk/Chat? That was where things began, and in my family we never wanted Hangouts, Talk/Chat was better.&lt;/p&gt;
    &lt;p&gt;Allo, Chat, Duo, Hangouts, Meet, Messenger, Talk, Voice… I’ve probably forgotten at least two more names, knowing Google. Most of these products have substantial overlap with most of the rest.&lt;/p&gt;
    &lt;p&gt;I’m still using - free g suite - play music - finance - nfc wallet is just google wallet isn’t it? - chromecast, video and audio-only I guess play music is now YouTube music, and doesn't have uploads, so that can be considered dead, but the others seem alive to me.&lt;/p&gt;
    &lt;p&gt;I used Picasa and loved it, until I realized I want all my photos available from all my devices at all times and so gave in to Google Photos (for access, not backup)&lt;/p&gt;
    &lt;p&gt;I use SyncThing for that purpose. It syncs across my phone, my laptops, and my Synologies. But I don't sync all my photos.&lt;/p&gt;
    &lt;p&gt;I don't like the thought of providing Google thousands of personal photos for their AI training. Which will eventually leak to gov't agencies, fraudsters, and criminals.&lt;/p&gt;
    &lt;p&gt;I used Google Talk than Hangouts, but once they switched to Meet, I gave up on them. By then my family was all using Hangouts, and we never settled on a new service, because one of my siblings didn't want to support any chat services that don't freely give user information to the government, and the rest of us didn't want to use a chat platform that does freely give user information to the government.&lt;/p&gt;
    &lt;p&gt;Am I the only one salty about Google Podcasts? For me that was the straw that broke the camel’s back… I dropped Android, switched to iOS, and slowly phasing out the Google products in my life.&lt;/p&gt;
    &lt;p&gt;From what I can tell (since I am just finding out about this today), they stopped manufacturing the old Chromecast hardware, and at some point, will stop supporting the old devices. The old devices may stop working in the future, for example, because they sunset the servers. Like their thermostats. Who knows?&lt;/p&gt;
    &lt;p&gt;Optane persistent memory had a fascinating value proposition: stop converting data structures for database storage and just persist the data directly. No more booting or application launch or data load: just pick up where you left off. Died because it was too expensive, but probably long after it should have.&lt;/p&gt;
    &lt;p&gt;VM's persist memory snapshots (as do Apple's containers, for macOS at least), so there's still room for something like that workflow.&lt;/p&gt;
    &lt;p&gt;The world had already caught up. By the time it was released, flash memory was already nearing it's speed and latency, to the point that the difference want with the cost.&lt;/p&gt;
    &lt;p&gt;&amp;gt;flash memory was already nearing it's speed and latency&lt;/p&gt;
    &lt;p&gt;Kinda, but for small writes it's still nowhere near.&lt;/p&gt;
    &lt;p&gt;Samsung 990 Pro - IOPS 4KQD1 113 MBytes/Sec&lt;/p&gt;
    &lt;p&gt;P4800X optane - IOPS 4KQD1 206 MBytes/Sec&lt;/p&gt;
    &lt;p&gt;And that's a device 5 years newer and on a faster pcie generation.&lt;/p&gt;
    &lt;p&gt;It disappeared because the market that values above attribute is too small and its hard to market because at first glance they look about the same on a lot of metrics as you say&lt;/p&gt;
    &lt;p&gt;Systems are stuck in old ways in how they model storage, so they weren't ready for something that is neither really RAM nor disk. Optane did inspire quite a few research projects for a while though. A few applications emerged in the server space, in particular.&lt;/p&gt;
    &lt;p&gt;How does that work? It loads kernel from drive to ram?&lt;/p&gt;
    &lt;p&gt;Isn't windows fast boot something like that (only slower, depending on ssd)? It semi-hibernates, stores kernel part of memory on disk for faster startup.&lt;/p&gt;
    &lt;p&gt;This one would have behaved more like suspend to RAM. In suspend to RAM, the RAM is kept powered, while everything else is shut down. The recovery would be near instant, since all the execution contexts are preserved on the RAM.&lt;/p&gt;
    &lt;p&gt;Optane was nearly as fast as RAM, but also persistent like a storage device. So you do a suspend to RAM, without the requirement to keep it powered like a RAM.&lt;/p&gt;
    &lt;p&gt;Not only because of price. The 'ecosystem' infrastructure wasn't there, or at least not spread wide enough. The 'mindshare'/thinking of ways how to do, neither. This is more aligned with (live) 'image-based' working environments like early Lisp and Smalltalk systems. Look at where they are now...&lt;/p&gt;
    &lt;p&gt;A few more thoughts about that, since I happen to have some of the last systems who actually had systems level support for that in their firmware, and early low-capacity optanes designed for that sort of use. It's fascinating to play with these, but they are low capacity, and bound to obsolete operating systems.&lt;/p&gt;
    &lt;p&gt;Given enough RAM, you can emulate that with working suspend and resume to/and from RAM.&lt;/p&gt;
    &lt;p&gt;Another avenue are the ever faster and larger SSDs, in practice, with some models it makes almost no difference anymore, since random access times are so fast, and transfer speeds insane. Maybe total and/or daily TBW remains a concern.&lt;/p&gt;
    &lt;p&gt;Gitless. I'm a fan of software that allows you to get your feet wet with simple concepts and progressively add complex ones when you feel you're ready. Gitless was my introduction to git.&lt;/p&gt;
    &lt;p&gt;Yahoo pipes. It was so great at creating rss feeds and custom workflows. There are replacements now like Zapier and n8n but loved that. Also google reader which is mentioned multiple times already.&lt;/p&gt;
    &lt;p&gt;Yahoo Pipes was what internet should have been. We're so many decades into computing and that kind of inter-tool linking has only barely been matched by unix pipes.&lt;/p&gt;
    &lt;p&gt;Many companies are working very hard to make that impossible unfortunately. For example you can't get posts from public Facebook groups automatically, although that would be a really good source candidate. They used to allow it, but... not anymore.&lt;/p&gt;
    &lt;p&gt;I loved pipes. I had rss feeds from all the sites where I was sharing content collected up and formatted via pipes into a single rss feed that was pulled into a php blog.&lt;/p&gt;
    &lt;p&gt;Then all those sites I used to post on stopped supporting rss one by one and finally pipes was killed off.&lt;/p&gt;
    &lt;p&gt;For a while I used a python library called riko that did the same thing as pipes without the visual editor. I have to thank it for getting me off php and into python.&lt;/p&gt;
    &lt;p&gt;If anyone with time, money and resources wants to revive the ideas of Yahoo! Pipes then I would suggest using Node-RED[^1] as a good starting point.&lt;/p&gt;
    &lt;p&gt;It has the advantage of being open source, has well defined and stable APIs and a solid backend. Plus 10+ years of constant development with many learnings around how to implement flow based programming visually.&lt;/p&gt;
    &lt;p&gt;I used the Node-RED frontend to create Browser-Red[^2] which is a Node-RED that solely executes in the browser, no server required. It does not support all Node-RED functionality but gives a good feel for using Node-RED and flow based programming.&lt;/p&gt;
    &lt;p&gt;The second project with which I am using Node-RED frontend is Erlang-Red[^3] which is Node-RED with an Erlang backend. Erlang is better suited to flow based programming than NodeJS, hence this attempt to demonstrate that!&lt;/p&gt;
    &lt;p&gt;Node-RED makes slightly different assumptions than Yahoo! Pipes - input ports being the biggest: all nodes in Node-RED have either zero or one input wires, nodes in Yahoo! Pipes had multiple input wires.&lt;/p&gt;
    &lt;p&gt;A good knowledge of jQuery is required but that makes it simpler to get into the frontend code - would be my argument ;) I am happy to answer questions related to Node-RED, email in bio.&lt;/p&gt;
    &lt;p&gt;I can recommend Apache Camel (https://camel.apache.org) for similar data integration pipelines and even agentic workflows. There are even visual editors for Camel today, which IMHO make it extremely user friendly to build any kind of pipeline quickly.&lt;/p&gt;
    &lt;p&gt;I missed Yahoo Pipes a lot so I built something similar recently for myself :) I know there are a few alternatives out there, but had to scratch my own itch.&lt;/p&gt;
    &lt;p&gt;Midori, Microsoft's capability-based security OS[1]. Rumor has it that it was getting to the point where it was able to run Windows code, so it was killed through internal politics, but who knows! It was the Fuchsia of its time...&lt;/p&gt;
    &lt;p&gt;I've heard someone at Microsoft describe it as a moonshot but also a retention project; IIRC it had a hundred plus engineers on it at one time, including a lot of very senior people.&lt;/p&gt;
    &lt;p&gt;Apparently a bunch of research from Midori made it into .NET so it wasn't all lost, but still...&lt;/p&gt;
    &lt;p&gt;The technical foundation seems interesting, but knowing Microsoft this would have just become yet another bloated mess with it's own new set of problems. And by now it would have equally become filled with spyware and AI "features" users don't want.&lt;/p&gt;
    &lt;p&gt;Pascal/Delphi - especially in the educational context.&lt;/p&gt;
    &lt;p&gt;Crazy fast compiler so doesn't frustrate trial &amp;amp; erroring students, decent type system without the wildness of say rust and all the basic programming building blocks you want students to grasp are present without language specific funkiness.&lt;/p&gt;
    &lt;p&gt;Delphi isn't dead - ver 13 was recently released - https://www.embarcadero.com/products/delphi. It's even cross platform, uses Skia as its graphics engine, its all very nice.&lt;/p&gt;
    &lt;p&gt;Iirc Delphi didn’t have threads, sockets, or OS integration (signals, file watching …). So it wasn’t suited to systems programming ie servers and services. It nailed gui applications, and that was a lot. Maybe freepascal has threads and sockets but imo it was too late.&lt;/p&gt;
    &lt;p&gt;Delphi 2, the first 32bit version of Delphi, had all of this. Some, like threads, even had wrappers (TThread), but Delphi came with Win32 bindings out of the box so all Win32 functions were available too - and it came bundled with documentation for the APIs. In addition, calling out to a DLL was trivial so even if a function wasn't available, you could just define it. Pretty much anything you could do with a C compiler was possible with Delphi 2 too.&lt;/p&gt;
    &lt;p&gt;Eh, sounds like that wouldn't be a problem for education purposes as the parent suggests? You need to be doing some really specific to leverage threads/file watching. And people probably use C to teach threads anyway.&lt;/p&gt;
    &lt;p&gt;Of course, being a good teaching language probably doesn't make the language popular or even survive. Python is so widely used not necessarily because it's simple to learn but because of its ecosystem.&lt;/p&gt;
    &lt;p&gt;Vine. It was already pretty big back in 2013 but Twitter had no idea what to do with it. TikTok actually launched just a few months before Vine was shut down and erased from the internet.&lt;/p&gt;
    &lt;p&gt;Whoever took the decision to kill Vine was an absolute moron, even without hindsight. It was square videos, how hard could it have been to shove an ads banner above it and call it a day? Incredible&lt;/p&gt;
    &lt;p&gt;That is so fascinating. They completely ignored their most most valuable users and thus the users left and the site collapsed. Fascinating, the hubris of the leadership at twitter to think they knew better than their users&lt;/p&gt;
    &lt;p&gt;Quartz Composer - Apple's "patch-based" visual programming environment. Drag out a bunch of nodes, wire them together, build a neat little GUI.&lt;/p&gt;
    &lt;p&gt;10+ years ago I'd regularly build all sorts of little utilities with it. It was surprisingly easy to use it to tap into things that are otherwise a lot more work. For instance I used it to monitor the data coming from a USB device. Like 3 nodes and 3 patches to make all of that work. Working little GUI app in seconds.&lt;/p&gt;
    &lt;p&gt;Apple hasn't touched it since 2016, I kind of hope it makes a comeback given Blender and more so Unreal Engine giving people a taste of the node based visual programming life.&lt;/p&gt;
    &lt;p&gt;You can still download it from Apple, and it still technically works but a lot of the most powerful nodes are broken in the newer OS's. I'd love to see the whole thing revitalized.&lt;/p&gt;
    &lt;p&gt;I loved quartz composer. It made it really easy to build all sorts of motion graphics. I’d see it used a lot at gigs to create audio-driven visuals. There was even a pretty cool VJ app built on it.&lt;/p&gt;
    &lt;p&gt;I’ve tried things like Touch Designer and Max MSP but they’re too heavy to just pick up and play with. QC was the right balance between simplicity and power.&lt;/p&gt;
    &lt;p&gt;Sandstorm: it seemed quite nice with a lot of possibilities when it launched in 2014, but it didn’t really take off and then it moved to sandstorm.org.&lt;/p&gt;
    &lt;p&gt;The actual problem with Sandstorm wasn't the era in which it was released. It will probably have the same problems even if released today. The problem was its application isolation mechanism - especially the data isolation (I think they were called grains). The mechanism is technically brilliant. But it's a big departure from how apps are developed today. It means that you have to do non-trivial modifications to web applications before they can run on the platform. The platform is better for applications designed to run on it in the start. It should have been marketed as a platform for building web applications, rather than as one for just deploying them.&lt;/p&gt;
    &lt;p&gt;Agreed. The best apps turned out to be the ones written for the platform. And many of those took people an afternoon to write, since the platform handled so much for you. Porting "normal" apps into Sandstorm felt like it defeated the purpose.&lt;/p&gt;
    &lt;p&gt;If I did it again I wouldn't focus on portability of existing apps. Especially today given you could probably vibe code most things (and trust the sandbox to protect you from AI slop security bugs).&lt;/p&gt;
    &lt;p&gt;Sandstorm was a great idea, but in my opinion it was targeted wrong. It should have been a platform and marketplace for B2B SaaS, not B2C SaaS. Specifically, all the third-party services which typical web apps use could have been Sandstorm apps, like analytics, logging, email, customer service etc.&lt;/p&gt;
    &lt;p&gt;Heroku? I know it's still around, though IDK who uses it, but I miss those days when it was thriving. One language, one deployment platform, one database, a couple plugins to choose from, everything simple and straightforward, no decision fatigue.&lt;/p&gt;
    &lt;p&gt;I often wonder, if AI had come 15 years earlier, would it have been a ton better because there weren't a billion different ways to do things? Would we have ever bothered to come up with all the different tech, if AI was just chugging through features efficiently, with consistent training data etc.?&lt;/p&gt;
    &lt;p&gt;As soon as they put a persistent Salesforce brand banner across the top which did nothing but waste space and put that ugly logo in our face every day, my team started our transition off Heroku pretty much right away.&lt;/p&gt;
    &lt;p&gt;&amp;gt; One language, one deployment platform, one database, a couple plugins to choose from, everything simple and straightforward, no decision fatigue.&lt;/p&gt;
    &lt;p&gt;I feel like this also describes something like Vercel. Having never personally used Heroku, is Vercel all that different except Ruby vs JS as the chosen language?&lt;/p&gt;
    &lt;p&gt;I talked to some Heroku reps at a local tech conference a year or so ago; it was clear that they were instructed to not have any personal opinions of the shredding of the free tier, but they did admit in a roundabout way that it lost them a lot of customers - some they were glad to get rid of as they were gaming the goodwill and costing Heroku lots of money, but weren't sure if it was a good long term idea or not.&lt;/p&gt;
    &lt;p&gt;My company still uses Heroku in production actually. Every time I see the Salesforce logo show up I wince, but we haven't had any issues at all. It continues to make deployment very easy.&lt;/p&gt;
    &lt;p&gt;Didn't they offer free compute? IIRC all free compute on the Internet went away with the advent of cryptocurrencies as it became practical to abuse the compute and translate it directly into money.&lt;/p&gt;
    &lt;p&gt;I use the core product for my SaaS apps. Great platform, does what it needs to do. Haven’t felt the need to switch. Sometimes tempted to move to a single VPS with Coolify or Dokku, but not interested in taking on the server admin burden.&lt;/p&gt;
    &lt;p&gt;I think their main failure points were the following:&lt;/p&gt;
    &lt;p&gt;- not lowering prices as time went off. They probably kept a super-huger margin profit, but they’re largely irrelevant today&lt;/p&gt;
    &lt;p&gt;- not building their own datacenters and staying in aws. That would have allowed them to lower prices and gain even more market share. Everyone that has been in amazon/aws likely has seen the internal market rate for ec2 instances and know there’s a HUGE profit margin deriving by building datacenters. Add the recent incredible improvements to compute density (you can easily get 256c/512t and literally terabytes of memory in a 2u box) and you get basically an infinite money glitch.&lt;/p&gt;
    &lt;p&gt;The internet before advertising, artificial intelligence, social media and bots. When folks created startups in their bedrooms or garages. The days when google slogan was “don’t be evil”.&lt;/p&gt;
    &lt;p&gt;I really miss the like 8 year ago push where a lot of major projects were moving to IRC. It's too bad Freenode took the opportunity to jump the shark and killed the momentum.&lt;/p&gt;
    &lt;p&gt;I mean, they're intentionally buried in the name of capital. If you need more than a Google search to find them, of course no one will go to them.&lt;/p&gt;
    &lt;p&gt;I don't like the siloing our information to Discord being a comparison to old internet. We had indexable information in forums that is "lost", not in the literal sense, but because you wouldn't be able to find it without obsessive digging to find it again. Conversations in Discord communities are very surface level and cyclical because it's far less straight forward to keep track of and link to answers from last week let alone two years ago. It is profoundly sad, to be honest.&lt;/p&gt;
    &lt;p&gt;I guess my abandoned/dead project might be Usenet. Sure, there were very dark places, and a lot of it was just a way to distribute porn, but that pretty much describes the Web. Usenet was like Reddit not controlled by a single company; like the Fediverse with infinite channels; like all of the world's threaded web fora displayed in exactly the way you want. We had that in the 1990s, and we're slowly groping toward getting it back.&lt;/p&gt;
    &lt;p&gt;I was a hold out on smartphones for a while and I used to print out k5 articles to read while afk... Just such an amazing collection of people sharing ideas and communal moderation, editing and up voting.&lt;/p&gt;
    &lt;p&gt;I learned about so many wierd and wonderful things from that site.&lt;/p&gt;
    &lt;p&gt;Microsoft Songsmith is another one that deserved a second life. It let you hum or sing a melody and would auto-generate full backing tracks, guitar, bass, drums, chords, in any style you chose.&lt;/p&gt;
    &lt;p&gt;It looked a bit goofy in the promo videos, but under the hood it was doing real-time chord detection and accompaniment generation. Basically a prototype of what AI music tools like Suno, Udio, or Mubert are doing today, fifteen years too early.&lt;/p&gt;
    &lt;p&gt;If Microsoft had kept iterating on it with modern ML models, it could’ve become the "GarageBand for ideas that start as a hum."&lt;/p&gt;
    &lt;p&gt;Full vector dpi aware UI, with grid, complex animation, and all other stuff that html5/css didn’t have in 2018 but silverlight had even in 2010 (probable even earlier).&lt;/p&gt;
    &lt;p&gt;MVVM pattern, two-way bindings. Expression Blend (basically figma) that allowed designers create UI that was XAML, had sample data, and could be used be devs as is with maybe some cleanup.&lt;/p&gt;
    &lt;p&gt;Excellent tooling, static analysis, debugging, what have you.&lt;/p&gt;
    &lt;p&gt;Rendered and worked completely the same in any browser (safari, ie, chrome, opera, firefox) on mac and windows&lt;/p&gt;
    &lt;p&gt;If that thing still worked, boy would we be in a better place regarding web apps.&lt;/p&gt;
    &lt;p&gt;Unfortunately, iPhone killed adobe flash and Silverlight as an aftermath. Too slow processor, too much energy consumption.&lt;/p&gt;
    &lt;p&gt;I am happy this one died. It was just another attempt by Microsoft to sidestep open web standards in favor of a proprietary platform. The other notorious example is Flash, and both should be considered malware.&lt;/p&gt;
    &lt;p&gt;Open web standards are great but consider where we could have been if competition drove them a different way? We're still stuck with JavaScript today (wasm still needs it). Layout/styling is caught up now but where would we be if that came sooner?&lt;/p&gt;
    &lt;p&gt;&amp;gt; Open web standards are great but consider where we could have been if competition drove them a different way? We're still stuck with JavaScript today (wasm still needs it). Layout/styling is caught up now but where would we be if that came sooner?&lt;/p&gt;
    &lt;p&gt;Why do you think JavaScript is a problem? And a big enough problem to risk destroying open web standards.&lt;/p&gt;
    &lt;p&gt;TypeScript exists for the same reason things like mypy exists, and no one in their right mind claims that python's openness should be threatened just because static typing is convenient.&lt;/p&gt;
    &lt;p&gt;Though in principle they serve similar purposes there are some big differences though. Python with types is still just python. Typescript is a different language from JS (guess it a superset?) and it being controlled by a large company could be considered problematic.&lt;/p&gt;
    &lt;p&gt;I suppose JS could go in the same direction and adopt the typing syntax from TS as a non-runtime thing. Then the typescript compiler would become something like mypy, an entirely optional part of the ecosystem.&lt;/p&gt;
    &lt;p&gt;&amp;gt; A remote code execution vulnerability exists when Microsoft Silverlight decodes strings using a malicious decoder that can return negative offsets that cause Silverlight to replace unsafe object headers with contents provided by an attacker. In a web-browsing scenario, an attacker who successfully exploited this vulnerability could obtain the same permissions as the currently logged-on user. If a user is logged on with administrative user rights, an attacker could take complete control of the affected system. An attacker could then install programs; view, change, or delete data; or create new accounts with full user rights. Users whose accounts are configured to have fewer user rights on the system could be less impacted than users who operate with administrative user rights.&lt;/p&gt;
    &lt;p&gt;I loved silverlight. Before I got a “serious” job, I was a summer intern at a small civil engineering consultancy that had gradually moved into developing custom software that it sold mostly to local town/city/county governments in Arizona (mostly custom mapping applications; for example, imagine Google Maps but you can see an overlay of all the street signs your city owns and click on one to insert a note into some database that a worker needs to go repair it… stuff like that).&lt;/p&gt;
    &lt;p&gt;Lots of their stuff was delivered as Silverlight apps. It turns out that getting office workers to install a blessed plugin from Microsoft and navigate to a web page is much easier than distributing binaries that you have to install and keep up to date. And developing for it was pure pleasure; you got to use C# and Visual Studio, and a GUI interface builder, rather than the Byzantine HTML/JS/CSS ecosystem.&lt;/p&gt;
    &lt;p&gt;I get why it never took off, but in this niche of small-time custom software it was really way nicer than anything else that existed at the time. Web distribution combined with classic desktop GUI development.&lt;/p&gt;
    &lt;p&gt;&amp;gt; It turns out that getting office workers to install a blessed plugin from Microsoft and navigate to a web page is much easier than distributing binaries that you have to install and keep up to date. And developing for it was pure pleasure; you got to use C# and Visual Studio, and a GUI interface builder&lt;/p&gt;
    &lt;p&gt;IIRC around that time, you could also distribute full-blown desktop applications (C# WinForms) in a special way via the browser, by which they were easily installable and self-updating. The tech was called ClickOnce https://learn.microsoft.com/en-us/visualstudio/deployment/cl.... I think the flow was possibly IE-only, but that was not a big issue in a business context at that time.&lt;/p&gt;
    &lt;p&gt;Back in the day Microsoft sent someone to our university to demo all of their new and upcoming products. I remember Vista (then named Longhorn) and Silverlight being among them. I also remember people being particularly impressed by the demo they gave of the latter, but everything swiftly falling apart when someone queried whether it worked in other browsers. This was at a time when IE was being increasingly challenged by browsers embracing open standards. So there was an element of quiet amusement/frustration in seeing them continue to not get it.&lt;/p&gt;
    &lt;p&gt;The prismatic news reader. It solved recommendations before the rest, but died because news died, and presumably made little money. Their attributed recommendations model is worth emulation. I don't remember if they supported both positive- and negative feedback, but Google news recommendation today do support attributed negative feedback.&lt;/p&gt;
    &lt;p&gt;I liked del.icio.us, it was online bookmark sharing, but with actual people I knew, and it had genuinely useful category tagging. I guess it was basically replaced with https://old.reddit.com and maybe twitter.&lt;/p&gt;
    &lt;p&gt;Isn’t Pinboard (Who bought delicious) very similar? I also see bookmarks of my friend there, recently switched to Raindrop though as it’s much more maintained.&lt;/p&gt;
    &lt;p&gt;You may be happy to hear that the new Fedora installer is using Firefox under the hood. Ephemeral profile dir on startup, plus custom userChrome.css to hide most of Firefox UI, and I couldn't tell a difference between it and Electron.&lt;/p&gt;
    &lt;p&gt;Tauri apps take advantage of the web view already available on every user’s system. A Tauri app only contains the code and assets specific for that app and doesn’t need to bundle a browser engine with every app.&lt;/p&gt;
    &lt;p&gt;Rendering will still use Edge/Chromium on a generic Windows machine.&lt;/p&gt;
    &lt;p&gt;Looking at firefox memory usage, i’m afraid the issue there is not memory safety but rather the average javascript developer being completely and blissfully unaware of and careless about memory memory usage of the software they write&lt;/p&gt;
    &lt;p&gt;ReactOS, the effort to create a free and open source Windows NT reimplementation.&lt;/p&gt;
    &lt;p&gt;It has been in existence in some form or another for nearly 30 years, but did not gain the traction it needed and as of writing it's still not in a usable state on real hardware. It's not abandoned, but progress on it is moving so slow that I doubt we'll ever see it be released in a state that's useful for real users.&lt;/p&gt;
    &lt;p&gt;It's too bad, because a drop in Windows replacement would be nice for all the people losing Windows 10 support right now.&lt;/p&gt;
    &lt;p&gt;On the other hand, I think people underestimate the difficulty involved in the project and compare it unfavorably to Linux, BSD, etc. Unix and its source code was pretty well publicly documented and understood for decades before those projects started, nothing like that ever really existed for Windows.&lt;/p&gt;
    &lt;p&gt;They had no chance. Look how long it tooks for Wine to get where they are. Their project is Wine + a kernel + device drivers compatibility, and a moving target.&lt;/p&gt;
    &lt;p&gt;&amp;gt; ReactOS, the effort to create a free and open source Windows NT reimplementation.&lt;/p&gt;
    &lt;p&gt;Some projects creep along slowly until something triggers an interest and suddenly they leap ahead.&lt;/p&gt;
    &lt;p&gt;MAME's Tandy 2000 implementation was unusable, until someone found a copy of Windows 1.0 for the Tandy 2000, then the emulation caught up until Windows ran.&lt;/p&gt;
    &lt;p&gt;Maybe ReactOS will get a big influx of activity after Windows 10 support goes offline in a couple days, or even shortly after when you can't turn AI spying off, not even three times a year.&lt;/p&gt;
    &lt;p&gt;Not so long ago there was a leak of windows’ source code, up to xp and 2003 server… the leak was so complete there are videos on YouTube about people building and booting (!!!) windows from there.&lt;/p&gt;
    &lt;p&gt;And yet, no big leap in ReactOS (at least for now).&lt;/p&gt;
    &lt;p&gt;They need to train an LLM with the windows source code and ask it to write an windows clone.&lt;/p&gt;
    &lt;p&gt;Apparently copyright law only applies for humans, generative AI gets away with stealing because there is too much monetary interest involved in looking the other way.&lt;/p&gt;
    &lt;p&gt;Leaks like this actually slow down ReactOS development.&lt;/p&gt;
    &lt;p&gt;The project is supposed to be a clean-room reverse engineering effort. If you even see Windows code, you are compromised, and should not work on ReactOS.&lt;/p&gt;
    &lt;p&gt;Wine, Proton and virtualization all got good enough that there's no need for a half-baked binary-compatible Windows reimplementation, and I think that took a lot of the oxygen out of what could have been energy towards ReactOS. It's a cool concept but not really a thing anybody requires.&lt;/p&gt;
    &lt;p&gt;I've heard people say this, and believed it myself for a long time, but recently I set up a windows XP VM and was shocked by how bad the quality of life was.&lt;/p&gt;
    &lt;p&gt;I think nostalgia is influencing this opinion quite a bit, and we don't realize the mountain of tiny usability improvements that have been made since XP&lt;/p&gt;
    &lt;p&gt;&amp;gt; I think people underestimate the difficulty involved in the project&lt;/p&gt;
    &lt;p&gt;I don't think people do, it sounds like a nearly impossible struggle, and at the end you get a Windows clone. I can't imagine hating yourself enough to work on it for an extended period of time for no money and putting yourself and your hard work in legal risk. It's a miracle we have Wine and serious luck that we have Proton.&lt;/p&gt;
    &lt;p&gt;People losing Windows 10 support need to move on. There's Linux if you want to be free, and Apple if you still prefer to be guided. You might lose some of your video games. You can still move to Windows 11 if you think that people should serve their operating systems rather than vice versa.&lt;/p&gt;
    &lt;p&gt;First Class had broader userbase, such as schools and organizations in the groupware/collaborative segment (but also Mac user groups and so on).&lt;/p&gt;
    &lt;p&gt;First Class was a comercial product (the server). It had filesharing (UL/DL), it had it's own desktop, mail, chat, IM, voice mail and more. Started out on Mac, but later became cross platform. Still you can find presentations and setup guides on old forgotten University/Schools websites.&lt;/p&gt;
    &lt;p&gt;Hotline on the other hand, was very easy to setup and also pretty lightweight. It had a server tracker. In the beginning it was Mac only. Lot's of warez servers, but also different (non-warez) communities. It had filesharing (ul/dl from the file area), chat and a newsboard. The decline came after it's developers released the Windows versions. Most servers became clickbait pron/warez with malware etc. People started to move away to web and it Hotline basically died out.&lt;/p&gt;
    &lt;p&gt;Now, there was some open source/clone projects that kept the spirit alive. But after a few years, web forums, torrents and other p2p-apps took over. But there is some servers running still in 2025 and open source server/client software still developed.&lt;/p&gt;
    &lt;p&gt;Compared to First Class. Hotline was the Wild West. It only took 15 minutes to set up your own server and announce it on a server tracker (or keep it private).&lt;/p&gt;
    &lt;p&gt;When i use Discord and other apps/services, it's not hard to think of FC/HL. But then, they were solutions of it's time.&lt;/p&gt;
    &lt;p&gt;Geocities ; It was a "put your html here" Free web hosting back when people barely knew what html was. Today you have to be a rocket scientist to find a way to host a free static "simple" page online.&lt;/p&gt;
    &lt;p&gt;Valid option - I used it myself for a very brief toe-dip into blogging earlier this year - but maybe worth noting that Google seems to flat-out refuse to crawl anything you put there. Won't pick it up by itself, won't read a sitemap you explicitly tell it about. It'll grudgingly index specific page URLs you tell it about, but that's kind of absurd. I don't know if it's because it's on a subdomain, or a Microsoft property, or because I was 100% ad- and tracker-free or what.&lt;/p&gt;
    &lt;p&gt;I tried DDG (Bing-backed, I believe) and it happily found everything with no manual intervention at all. That was the point where I ditched Google Search after 30 years.&lt;/p&gt;
    &lt;p&gt;tumblr is nothing like a webpage. LLMs were just invented 5 minutes ago and are losing money hand over fist until people are dependent, then will be very expensive to use; and you still have to figure out how to host, where to host, and how much it's going to cost you. So, I have no idea what you're getting at.&lt;/p&gt;
    &lt;p&gt;You could have said Wordpress.com or something. It's not quite a website, but it's close. It's also probably going to be Typepad (i.e. defunct) in a few years and Blogger is probably going to be there quicker than that.&lt;/p&gt;
    &lt;p&gt;The Lockheed D-21 drone. Supersonic ramjet without the complexity of scramjet or the cost of turbojet, hamstrung by the need for a manned launch platform (making operations safety-critical… with predictable results) and recovery to get data off it. Twenty or forty years later it would have been paired by a small number of high-cost launcher UAVs and had its cost driven down to disposable, with data recovery over radio comms… but twenty to forty years later there’s nothing like it, and the maturation of satellites means there almost certainly never will be.&lt;/p&gt;
    &lt;p&gt;I was gonna say Meego. They killed it just as it was getting to a usable state. One of the last chances we had to get a proper third option in the mobile market.&lt;/p&gt;
    &lt;p&gt;I loved my N900, and my N800 before that, and I would have loved to have seen successors. Ultimately, I ended up switching to Android because I was tired of things only available as apps. Since then, web technologies have gotten better, and it's become much more feasible to use almost exclusively websites.&lt;/p&gt;
    &lt;p&gt;They should have partnered not only with Intel, but with Palm, RIM or whatever other then-giant to rival Android. Those two went their own ways with WebOS and buying QNX, so maybe they could have agreed to form a consortium for an open and interoperable mobile OS&lt;/p&gt;
    &lt;p&gt;I really liked Google Circles, a feature of Google+ social media. It allowed you to target content to specific groups of users. You could have a "family" circle or a "work" circle and not have to worry about cross posting something accidentally. It was a small thing but it made it really easy to manage your posts.&lt;/p&gt;
    &lt;p&gt;Boot2Gecko or whatever the browser as Operating system was called. This was a project that should have focused on providing whatever its current users needed expanding and evolving to do whatever those users wanted it to do better.&lt;/p&gt;
    &lt;p&gt;Instead it went chasing markets, abandoning existing users as it did so, in favour of potential larger pools of users elsewhere. In the end it failed to find a niche going forward while leaving a trail of abandoned niches behind it.&lt;/p&gt;
    &lt;p&gt;I adored my Firefox Phones. Writing apps was so easy I built myself dozens of little one-offs. Imagine if it had survived to today, its trivial html/css/js apps could be vibe coded on-device and be the ultimate personalized phone.&lt;/p&gt;
    &lt;p&gt;Luckily it wasn't long after Mozilla abandoned it that PWAs were introduced and I could port the apps I cared about.&lt;/p&gt;
    &lt;p&gt;For a few short months circa 2016 or 2017, KaiOS was the number one mobile OS in India. This was probably because of all the ultra-cheap KaiOS-powered Reliance Jio phones flooding the Indian market at the time.&lt;/p&gt;
    &lt;p&gt;I noticed the trend when I was working on a major web property for the Aditya Birla conglomerate. My whole team was pleasantly surprised, and we made sure to test everything in Firefox for that project. But everyone switched to Android + Chrome over the next few years, which was a shame.&lt;/p&gt;
    &lt;p&gt;I built a chatbot startup in 2015. It integrated with Whatsapp (which was possible at the time with some hacks), and had:&lt;/p&gt;
    &lt;p&gt;- Multimodality: Text/audio/images input and output. Integrated OCR.&lt;/p&gt;
    &lt;p&gt;- Connection with an asterisk server, it could send and receive voice phone calls! I used it to call for pizzas to a local place via whatsapp. This was prior to Google's famous demo calling a hairdresser to book a haircut.&lt;/p&gt;
    &lt;p&gt;- It understood humor and message sentiment, told jokes and sometimes even chimed in with a "haha" if somebody said something funny in a group chat or sent an appropriate gif reaction&lt;/p&gt;
    &lt;p&gt;- Memory (facts database)&lt;/p&gt;
    &lt;p&gt;- Useful features such as scheduling, polling, translations, image search, etc.&lt;/p&gt;
    &lt;p&gt;Regarding the tech, I used external models (Watson was pretty good at the time), plus classical NLP processing and symbolic reasoning that I learned in college.&lt;/p&gt;
    &lt;p&gt;Nobody understood the point of it (where's the GUI? how do I know what to ask it? customers asked) and I didn't make a single dime out of the project. I closed it a couple years later. Sometimes I wonder what could've been of it.&lt;/p&gt;
    &lt;p&gt;MS Sidewinder Force Feedback Pro (1997) and Sidewinder Force Feedback 2 (USB). You can buy similar today, but nowhere near the pricepoint. Also the out of the box support by Windows has vanished, and therefore the incentive of game developers to include force feedback.&lt;/p&gt;
    &lt;p&gt;I still have my MS Force Feedback 2, and it still works great!&lt;/p&gt;
    &lt;p&gt;I heard that some patent troll got a hold of the patent for force feedback joysticks, and all manufacturers just gave up on them because of the troll. The patent expired recently IIRC, so hopefully people will start making them again soon.&lt;/p&gt;
    &lt;p&gt;It was a series of experiments with new approaches to programming. Kind of reminded me of the research that gave us Smalltalk. It would have been interesting to see where they went with it, but they wound down the project.&lt;/p&gt;
    &lt;p&gt;I worked on this project so I can give some insight. The main reason we didn't keep working on it was it was VC funded and we didn't have a model for making money in the short term. At the end we were pursuing research related to natural language programming and reinforcement learning in that area (I recently blogged about it here: https://mech-lang.org/post/2025-01-09-programming-chatgpt), and were considering folding our small team into OpenAI or Microsoft or something. But we wanted to work as a team and no one wanted to take us as a team, so we called it.&lt;/p&gt;
    &lt;p&gt;It didn't get far enough to be "used" in a production sense but there was enough interest and people were playing around with it, but not real traction to speak of. Frankly, language projects are difficult because these days they have to be bootstrapped to a certain size before there's any appreciable use, and VCs are not patient enough for that kind of timetable.&lt;/p&gt;
    &lt;p&gt;RethinkDB. Technically it still exists (under The Linux Foundation), but (IMO) the original company's widening scope (the Horizon BaaS) that eventually led to its demise killed its momentum.&lt;/p&gt;
    &lt;p&gt;Macromedia Flash. Its scope and security profile was too big. It gave way to HTML’s canvas. But man, the tooling is still no where near as good. Movieclips, my beloved. I loved it all.&lt;/p&gt;
    &lt;p&gt;It's incredible to me that they killed the whole tool instead of making a JS/Canvas port. Even without "full flash websites", there's still need for vectorial animations on the web.&lt;/p&gt;
    &lt;p&gt;The iPhone killed Flash, probably because it would've been a way to create apps for it, more probably because it would've been laggy in the 2007 hardware, and people would've considered the iPhone "a piece of junk".&lt;/p&gt;
    &lt;p&gt;Interesting how Flash became the almost universal way to play videos in the browser, in the latter half of the 2000's (damn I'm old...).&lt;/p&gt;
    &lt;p&gt;As a Linux user, I hated Flash with a passion. It mostly didn't work despite several Linux implementations. About the time they sorted all the bugs out, it went away. Good riddance.&lt;/p&gt;
    &lt;p&gt;Non Daw. Its breaking up each function of the DAW into its own application gave a better experience in each of those functions, especially when you only needed that aspect, you were not working around everything else that the DAW offers. The integration between the various parts was not all that it could be but I think the idea has some real potential.&lt;/p&gt;
    &lt;p&gt;Thought about Non immediately, but I figured it must have (had) about 2 other users amongst HNers, though. :) Nice to see it mentioned.&lt;/p&gt;
    &lt;p&gt;I used it quite a bit to produce radio shows for my country's public broadcasting. Because Non's line-oriented session format was so easy to parse with classic Unix tools, I wrote a bunch of scripts for it with Awk etc. (E.g. calculating the total length of clips highlighted with brown color in the DAW -- which was stuff meant for editing out; or creating a poor man's "ripple editing" feature by moving loosely-placed clips precisely side by side; or, eventually, converting the sessions to Samplitude EDL format, and, from there, to Pro Tools via AATranslator [1] (because our studio was using PT), etc. Really fun times!)&lt;/p&gt;
    &lt;p&gt;I've never heard of this software before. Any idea why it's discontinued? There are a bunch of weird messages that point to sort of a hostile take over of the project by forking, but it doesn't say anything about why or how it was discontinued.&lt;/p&gt;
    &lt;p&gt;Skype ; Because my R.I.P. grandma was using it to talk to her relatives overseas just like she would use a phone, but it didn't cost an arm and a leg (unlike phone calls).&lt;/p&gt;
    &lt;p&gt;One of the best P2P software at the time. It was so simple and effective and allowed people to call real phones with Skype credit.&lt;/p&gt;
    &lt;p&gt;A genius product ripped my Microsoft. Have you used Microsoft Teams recently? Bad UI, hard to configure external hardware and good level of incompatibility, missing the good old "Echo / Sound Test Service". At a point I even installed Skype of my old Android but was sucking up too much battery.&lt;/p&gt;
    &lt;p&gt;I tried it twice and the onboarding experience was insurmountable. Never managed to achieve a critical mass of followers or whatever they call it, so things were permanently read-only for me. I'd reply but nobody saw it.&lt;/p&gt;
    &lt;p&gt;It was a fascinating protocol underneath, but the social follow structure seemed to select strongly for folks who already had a following or something.&lt;/p&gt;
    &lt;p&gt;Drama has killed the technological progress in open source, if you ask me.&lt;/p&gt;
    &lt;p&gt;Having seen what goes on in the foss world and what goes on in the large faang-size corporate world, no wonder the corporate world is light-years ahead.&lt;/p&gt;
    &lt;p&gt;You don't need hierarchy, but you need some sort of process. "Consensus-based" just means that the loudest and most enduring shouters get their way, and when their way fails spectacularly, they leave in a huff (taking their work with them, badmouthing the project, and likely starting a fork that will pull more people out of the project and confuse potential users who just bail on trying either.)&lt;/p&gt;
    &lt;p&gt;Those people need to be pushed out early and often. That's what voting is for. You need a supermajority to force an end to discussion, and a majority to make a decision. If you hold up the discussion too long with too slim a minority, the majority can fork your faction out of the group. If the end of debate has been forced, and you can't work with the majority, you should leave yourself.&lt;/p&gt;
    &lt;p&gt;None of this letting the bullies get their way until everything is a disaster, then splitting up anyway stuff.&lt;/p&gt;
    &lt;p&gt;Hah. Naive take. I especially love this “Those people need to be pushed out early and often. That's what voting is for. You need a supermajority to force an end to discussion, and a majority to make a decision”. We know what needs to be done, but it’s not being done. There’s no consensus. Consensus take time and effort and has a lot of friction. I am part of a coop and I have seen first hand how this goes. And it’s fine, consensus based systems have other advantages, but they move slower that hierarchies.&lt;/p&gt;
    &lt;p&gt;The core of the issue is that drama is a way to impose your views of the world.&lt;/p&gt;
    &lt;p&gt;In foss software you quite literally don’t have to agree. You can fork the software and walk your own path. You can even pull changes from the original codebase, most licenses allow that.&lt;/p&gt;
    &lt;p&gt;Consensus is only necessary if you care about imposing your views of the world onto others.&lt;/p&gt;
    &lt;p&gt;It might be too soon to call it abandoned, but I was very intrigued by the Austral [1] language. The spec [2] is worth reading, it has an unusual clarity of thought and originality, and I was hoping that it would find some traction. Unfortunately it seems that the author is no longer actively working on it.&lt;/p&gt;
    &lt;p&gt;I played with Austral about a year ago and really wanted to use it for my projects, but as a hobbyist and mostly inept programmer it lacked the community and ecosystem I require. I found it almost intuitive and the spec does an amazing job of explaining the language. Would love to see it get a foothold.&lt;/p&gt;
    &lt;p&gt;The author got hired by Modular, the AI startup founded by the creators of LLVM and Swift, and is now working on the new language Mojo. He’s been bringing a bunch of ideas from Vale to Mojo&lt;/p&gt;
    &lt;p&gt;Oh nice! I just had an excuse to try mojo via max inference, it was pretty impressive. Basically on par with vllm for some small benchmarks, bit of variance in ttft and tpot. Very cool!&lt;/p&gt;
    &lt;p&gt;In 2011, before TypeScript, Next.js or even React, they had seamless server-client code, in a strongly typed functional language with support for features like JSX-like inline HTML, async/await, string interpolation, built-in MongoDB ORM, CSS-in-JS, and many syntax features that were added to ECMAScript since then.&lt;/p&gt;
    &lt;p&gt;I find it wild how this project was 90%+ correct on how we will build web apps 14 years later.&lt;/p&gt;
    &lt;p&gt;CLPM, the Common Lisp Package Manager. The Quicklisp client doesn't do HTTPS, ql-https doesn't do Ultralisp, and OCICL (which I'm currently using) doesn't do system-wide packages. CLPM is a great project, but it's gone neglected long enough that it's bitrotted and needs some thorough patching to be made usable. Fortunately Common Lisp is still as stable as it has been for 31 years, so it's just the code which interacts with 3rd-party libraries that needs updating.&lt;/p&gt;
    &lt;p&gt;Yeah I felt that Quicklisp doesn't have the same features as package managers in other languages, and https is one of them. Also it's run by a single person which doesn't have too much time to constantly update the libraries.&lt;/p&gt;
    &lt;p&gt;In comparison I found Clojars^[0] for Clojure better and community driven like NPM. But obv Clojure has more business adoption than CL.&lt;/p&gt;
    &lt;p&gt;BT had this grand vision for basically providing rich multi-media through the phone line, but in ~1998. Think a mix of on-demand cable and "teleconferencing" with TV based internet (ceefax/red button on steriods)&lt;/p&gt;
    &lt;p&gt;It would have been revolutionary and kick started the UK's jump into online rich media.&lt;/p&gt;
    &lt;p&gt;However it wouldnt have got past the regulators as both sky and NTL(now virgin) would have protested loudly.&lt;/p&gt;
    &lt;p&gt;Lazarus is nice but both its apis and the ui feel like they're still stuck in the early 00's. It's not enough to look like VB6 / Delphi these days; you've got to keep up with what kinds of conventions we expect now.&lt;/p&gt;
    &lt;p&gt;It's been a number of years but my understanding was they kind of killed all the momentum it had by removing support for custom operators which broke everyone's code?&lt;/p&gt;
    &lt;p&gt;Yeah, Opa was wildly ahead of its time, I actually just wrote a top level comment about it. Basically Next.js+TypeScript+modern ECMAScript features, but in 2011.&lt;/p&gt;
    &lt;p&gt;A simple UI programming pattern, with a circular, unidirectional data flow. It is very rigid by design, to be side-effect free, functional, unidirectional:&lt;/p&gt;
    &lt;p&gt;Flickr - that was the future of photo storage, sharing, discovery.&lt;/p&gt;
    &lt;p&gt;What was the bookmarks social tool called from 00’s? I loved it and it fell off the earth. You could save your bookmarks, “publish” them to the community, share etc..&lt;/p&gt;
    &lt;p&gt;What ever happened to those build your own homepage apps like startpage (I think)? I always thought those would take off&lt;/p&gt;
    &lt;p&gt;Visix Vibe. It was a "WYSIWYG"-type visual programming environment for .. Java.&lt;/p&gt;
    &lt;p&gt;It had its own cross platform UI and other frameworks too, so you could "write once in Java, and ship on all the things" .. well theoretically.&lt;/p&gt;
    &lt;p&gt;It got abandoned too soon. But it was quite fun to build apps with it for a while, almost Delphi- like. I always wonder if it went open source, if things would have been different vis a vis Flash, etc.&lt;/p&gt;
    &lt;p&gt;Was recently reading about Project Ara, the modular smartphone project by Google/Motorola [1]. Would have liked to see a few more iterations of the idea. Something more customizable than what we have today without having to take the phone apart.&lt;/p&gt;
    &lt;p&gt;ICQ ; It was the first instant messenger, the technology could have adopted voice (and not get disrupted by Skype) and mobile (and not get disrupted by whatsapp) and group chat (and not get disrupted by slack/discord). But they didn't even try and put up a fight.&lt;/p&gt;
    &lt;p&gt;They got bought by AOL in 98, long before most/all of this innovation happened?&lt;/p&gt;
    &lt;p&gt;Edit: in fact I'd say they were irrelevant before pretty much all of those innovations. By the time AIM or MSN Messenger really became popular, ICQ didn't matter anymore.&lt;/p&gt;
    &lt;p&gt;Gnome Conduit software. Used to synchronize a lot of my local-first data (calendar, photos, music) to different online services. Nice to see in one place where everything goes and what is the sync status.&lt;/p&gt;
    &lt;p&gt;Dreamweaver or some other real WYSISYG web page editor that could maybe deal with very basic JavaScript.&lt;/p&gt;
    &lt;p&gt;I just wanna make a mostly static site with links in and out of my domain. Maybe a light bit of interactivity for things like search that autocompletes.&lt;/p&gt;
    &lt;p&gt;iirc Atom was the original Electron project. Eventually VS Code came along and took all the good ideas - the modularity through extensions, and Electron / web based cross platform, but made it really fast and added IDE like language support through LSP. Atom may be dead now, but the idea lives on in VS Code and the new project by the original developers of Atom: Zed&lt;/p&gt;
    &lt;p&gt;It's a real shame its raster functionality wasn't integrated into Illustrator. Adobe really butchered the whole Macromedia portfolio, didn't they?&lt;/p&gt;
    &lt;p&gt;(For those unfamiliar, Illustrator is a pure vector graphics editor; once you rasterize its shapes, they become uneditable fixed bitmaps. Fireworks was a vector graphics editor that rendered at a constant DPI, so it basically let you edit raster bitmaps like they were vectors. It was invaluable for pixel-perfect graphic design. Nothing since lets you do that, though with high-DPI screens and resolution-independent UIs being the norm these days, this functionality is less relevant than it used to be.)&lt;/p&gt;
    &lt;p&gt;At my last job m our designer was a Fireworks holdout. It was very pleasant. As someone who has to implement UIs, I greatly preferred it to Figma, though with today's flat boring designs there's a lot less slicing.&lt;/p&gt;
    &lt;p&gt;VPRI, I was really hoping it would profoundly revolutionise desktop application development and maybe even lead to a new desktop model, and instead they wound up the project without having achieved the kind of impact I was dreaming of.&lt;/p&gt;
    &lt;p&gt;Nokia Maps. There was a brief period in the early 2010s where Nokia had the best mapping product on the planet, and it was given away for free on Lumia phones at a time when TomTom and Garmin were still charging $60+ for navigation apps.&lt;/p&gt;
    &lt;p&gt;Anyone remember Openmoko, the first commercialised open source smart phone. Was heaps buggy though, not really polished, etc. It’s only redeeming feature was the open source software and hardware (specs?).&lt;/p&gt;
    &lt;p&gt;There was the https://en.wikipedia.org/wiki/PinePhone and it's successor PinePhonePro. Bugginess and general impracticalities brought up to more recent standards. Inflation-adjusted, of course!&lt;/p&gt;
    &lt;p&gt;There was a virtual platform through which to learn Chinese called ‘Zon’. Someone obviously put years of work into it but no one ever joined and it turned into this great looking ghost town.&lt;/p&gt;
    &lt;p&gt;Just on principle, I'd have liked to see it on the market for more than 49 days! It pains me as an engineer to think of the effort to bring a hardware device to market for such a minuscule run.&lt;/p&gt;
    &lt;p&gt;RAM Disks. Basically extremely fast storage using RAM sticks slotted into a specially made board that fit in a PCIe slot. Not sure what happened to the project exactly but the website disappeared sometime in 2023.&lt;/p&gt;
    &lt;p&gt;The idea that you could read and write data at RAM speeds was really exciting to me. At work it's very common to see microscope image sets anywhere from 20 to 200 GB and file transfer rates can be a big bottleneck.&lt;/p&gt;
    &lt;p&gt;Products to attach RAM to expansion slots have long existed and continue to be developed. It's a matter of adding more memory once all of the DIMMs are full.&lt;/p&gt;
    &lt;p&gt;What to do with it, once it's there, is a concern of software, but specialized hardware is needed to get it there.&lt;/p&gt;
    &lt;p&gt;I always thought Microsoft Popfly had huge potential and was way ahead of its time. It made building web mashups feel like playing with Lego blocks, drag, drop, connect APIs, and instantly see the result.&lt;/p&gt;
    &lt;p&gt;If something like that existed today, powered by modern APIs and AI, it could become the ultimate no-code creativity playground.&lt;/p&gt;
    &lt;p&gt;I've argued this for years on this site...but AOL.&lt;/p&gt;
    &lt;p&gt;At its best, having IM, email, browser, games, keywords, chats, etc. was a beautiful idea IMO. That they were an ISP seemed secondary or even unrelated to the idea. But they chose to charge for access even in the age of broadband, and adopt gym level subscription tactics to boot, and people decided they'd rather not pay it which is to be expected. I often wonder if they'd have survived as a software company otherwise.&lt;/p&gt;
    &lt;p&gt;They were basically a better thought out Facebook before Facebook, in my opinion.&lt;/p&gt;
    &lt;p&gt;Connect your phone to a display, mouse, keyboard and get a full desktop experience.&lt;/p&gt;
    &lt;p&gt;At the time smartphones were not powerful enough, cables were fiddly (adapters, HDMI, USB A instead of a single USB c cable) and virtualization and containers not quite there.&lt;/p&gt;
    &lt;p&gt;Today, going via pkvm seems like promising approach. Seamless sharing of data, apps etc. will take some work, though.&lt;/p&gt;
    &lt;p&gt;&amp;gt;This presentation introduces Via, a virtual file system designed to address the challenges of large game downloads and storage. Unlike cloud gaming, which suffers from poor image quality, input latency, and high hosting costs, Via allows games to run locally while only downloading game data on demand. The setup process is demonstrated with Halo Infinite, showing a simple installation that involves signing into Steam and allocating storage space for Via's cache.&lt;/p&gt;
    &lt;p&gt;&amp;gt;Via creates a virtual Steam library, presenting all owned games as installed, even though their data is not fully downloaded. When a game is launched, Via's virtual file system intercepts requests and downloads only the necessary game content as it's needed. This on-demand downloading is integrated with the game's existing streaming capabilities, leveraging features like level-of-detail and asset streaming. Performance metrics are displayed, showing download rates, server ping, and disk commit rates, illustrating how Via fetches data in real-time.&lt;/p&gt;
    &lt;p&gt;&amp;gt;The system prioritizes caching frequently accessed data. After an initial download, subsequent play sessions benefit from the on-disk cache, significantly reducing or eliminating the need for network downloads. This means the actual size of a game becomes less relevant, as only a portion of it needs to be stored locally. While server locations are currently limited, the goal is to establish a global network to ensure low ping. The presentation concludes by highlighting Via's frictionless user experience, aiming for a setup so seamless that users are unaware of its presence. Via is currently in early access and free to use, with hopes of future distribution partnerships.&lt;/p&gt;
    &lt;p&gt;I'm amazed the video still has under 4,000 views. Sadly, Flaherty got hired by XAI and gave up promoting the project.&lt;/p&gt;
    &lt;p&gt;Wait until you hear that almost all Unity games don't really have asset streaming because the engine loads things eagerly by default.&lt;/p&gt;
    &lt;p&gt;I don't see how this could take off. Internet speeds are getting quicker, disk space is getting cheaper, and this will slow down load times. And what's worse is the more you need this tech the worse experience you have.&lt;/p&gt;
    &lt;p&gt;Google Wave. It was horrible from a performance point of view, but was really interesting to use. Some of its features have made their way into the Google docs etc ecosystem and Office 365. But not all&lt;/p&gt;
    &lt;p&gt;People talk so much about how you need to write code that fits well within the rest of the codebase, but what tools do we have to explore codebases and see what is connected to what? Clicking through files feels kind of stupid because if you have to work with changes that involve 40 files, good luck keeping any of that in your working memory. In my experience, the JetBrains dependency graphs also aren't good enough.&lt;/p&gt;
    &lt;p&gt;Sourcetrail was a code visualization tool that allowed you to visualize those dependencies and click around the codebase that way, see what methods are connected to what and so on, thanks to a lovely UI. I don't think it was enough alone, but I absolutely think we need something like this: https://www.dbvis.com/features/database-management/#explore-... but for your code, especially for codebases with hundreds of thousands or like above a million SLoC.&lt;/p&gt;
    &lt;p&gt;I yearn to some day view entire codebases as graphs with similarly approachable visualization, where all the dependencies are highlighted when I click an element. This could also go so, so much further - you could have a debugger breakpoint set and see the variables at each place, alongside being able to visually see how code is called throughout the codebase, or hell, maybe even visualize every possible route that could be taken.&lt;/p&gt;
    &lt;p&gt;The IBM school's computer. Developed by IBM Hursley in 1967, it was years ahead in its design, display out to a television and storage on normal audio tape. Would have kick started an educational revolution if it had been launched beyond the 10 prototype machines.&lt;/p&gt;
    &lt;p&gt;10/GUI did some deep thinking about the limitations and potential of the (then-fairly new) multi touch input method. I wished something more had come out of it, instead it stayed a niche concept art video that is mostly forgotten now.&lt;/p&gt;
    &lt;p&gt;I’m not arguing the solutions it outlined are good, but I think some more discussion around how we interact with touch screens would be needed. Instead, we are still typing on a layout that was invented for mechanical typewriters - in 2025, on our touch screens.&lt;/p&gt;
    &lt;p&gt;netflix falcor. the graphql hype killed a much better alternative for many usecases. there were only a few missing pieces and improvements such as a proxy based adapter layer for popular frontend frameworks. Im now the lonely last user hoping to find a way to reboot development&lt;/p&gt;
    &lt;p&gt;The TUNES [1] operating system and programming language project. The reason for its failure are described perfectly on the archival website:&lt;/p&gt;
    &lt;p&gt;&amp;gt; TUNES started in 1992-95 as an operating system project, but was never clearly defined, and it succumbed to design-by-committee syndrome and gradually failed. Compared to typical OS projects it had very ambitious goals, which you may find interesting.&lt;/p&gt;
    &lt;p&gt;Pivotal Tracker ; Users loved it, it had an excellent model for tracking work and limiting work in progress on software projects. There is no real good alternative and the usual suspects for tracking project work are horrible in comparison.&lt;/p&gt;
    &lt;p&gt;I don’t know about that. My employer was all in on Pivotal and we used it for several years. Then one day a dev stumbled across Linear, we all tried it, and switched the whole company within a month or so.&lt;/p&gt;
    &lt;p&gt;In the late 90s there was a website called fuckedcompany which was a place where people could spill the beans about startups (mainly in silicon valley). It was anonymous and a pretty good view into the real state of tech. Now there is twitter/x but it's not as focused on this niche.&lt;/p&gt;
    &lt;p&gt;The closest sites I've found are Web3 is Going Just Great and Pivot to AI, which are newsfeeds of various car crashes in their respective hype arenas, although without any insider scoops/gossip.&lt;/p&gt;
    &lt;p&gt;I came to say Opa too. I liked the language but the meteor-like framework it was bundled with, while nice for prototyping, was a pain to work around when it didn't do what you needed.&lt;/p&gt;
    &lt;p&gt;That said, frameworks were all the buzz back in the day, so the language alone probably wouldn't have gone anywhere without it.&lt;/p&gt;
    &lt;p&gt;CueCat it was an affordable barcode scanner that anyone could have connected to their computer, and it scanned barcodes. It took almost two decades before we could finally do it again with our mobile phones.&lt;/p&gt;
    &lt;p&gt;Google Wave ; It had a bunch of agents participating in editing the text together with you, making spelling fixes, finding additional information to enrich your content, and so much more.&lt;/p&gt;
    &lt;p&gt;wua.la … the original version. You share part of your storage to get the same amount back as resilient cloud storage from others. Was bought and killed by LaCie (now Seagate). They later provided paid-for cloud storage under the same name but it didn’t take off.&lt;/p&gt;
    &lt;p&gt;XenClient. I would really love to have some minimal OS HyperVisor running, and then you slap multiple OSes on top of that w/ easy full GUI switching via some hotkeys like Ctrl+Shift+F1. Additionaly, special drivers to virtualize Gfx and Sfx devices so every VM have full desktop capabilities and low latency.&lt;/p&gt;
    &lt;p&gt;Unfortunately, it died because its very niche and also they couldnt keep up with development of drivers for desktops.. This is even worse today...&lt;/p&gt;
    &lt;p&gt;Mozilla heka. As far as data collection and processing goes, we are still stuck with Logstash after all of these years. Heka promised a much more efficient solution, being implemented with Go and Lua plugins.&lt;/p&gt;
    &lt;p&gt;I'm booting and running Haiku on my Thinkpad. It's a from-scratch workalike of BeOS, and able to run Be software. Though, frankly, Be software is totally 1990s, so a lot of Linux software written for Qt has been ported to Haiku.&lt;/p&gt;
    &lt;p&gt;In the end I wound up with basically the same application software as on my Debian desktop, except running on Haiku instead of Linux. Haiku is noticeably snappier and more responsive than Linux+X+Qt+KDE, though.&lt;/p&gt;
    &lt;p&gt;In late September or early October 1996, Fry's Electronics places a full page promo ad on the back of the business section of the San Jose Mercury News for OS/2 4.0 "WRAP [sic]" in 256 pt font in multiple places. Oops!&lt;/p&gt;
    &lt;p&gt;Nah, that time has passed and there's not much to miss from the base OS. What would be interesting is for IBM to publish the source to the Workplace Shell and the underlying SOM code so it might get a new life running on one of the free *nixes.&lt;/p&gt;
    &lt;p&gt;https://www.kite.com for python i first learned about it when i was working in an university group and had the task to transform a windowing algorithm already working on matlab to python. it felt like a modern linter and lsp with additional support through machine learning. i don't quite know why it got comparative small recognition, but perhaps enough to remain an avantgarde pioneering both python and machine learning support for further generations and wider applications.&lt;/p&gt;
    &lt;p&gt;i first learned about it when i was working in an university group and had the task to transform a windowing algorithm already working on matlab to python. it felt like a modern linter and lsp with additional support through machine learning. i don't quite know why it got comparative small recognition, but perhaps enough to remain an avantgarde pioneering both python and machine learning support for further generations and wider applications.&lt;/p&gt;
    &lt;p&gt;I could think of many examples, but I'll talk about the top four that I have in mind, that I'd like to see re-evaluated for today's times.&lt;/p&gt;
    &lt;p&gt;1. When Windows Vista was being developed, there were plans to replace the file system with a database, allowing users to organize and search for files using database queries. This was known as WinFS (https://en.wikipedia.org/wiki/WinFS). I was looking forward to this in the mid-2000s. Unfortunately Vista was famously delayed, and in an attempt to get Vista released, Microsoft pared back features, and one of these features was WinFS. Instead of WinFS, we ended up getting improved file search capabilities. It's unfortunate that there's been no proposals for database file systems for desktop operating systems since.&lt;/p&gt;
    &lt;p&gt;2. OpenDoc (https://en.wikipedia.org/wiki/OpenDoc) was an Apple technology from the mid-1990s that promoted component-based software. Instead of large, monolithic applications such as Microsoft Excel and Adobe Photoshop, functionality would be offered in the form of components, and users and developers can combine these components to form larger solutions. For example, as an alternative to Adobe Photoshop, there would be a component for the drawing canvas, and there would be separate components for each editing feature. Components can be bought and sold on an open marketplace. It reminds me of Unix pipes, but for GUIs. There's a nice promotional video at https://www.youtube.com/watch?v=oFJdjk2rq4E.&lt;/p&gt;
    &lt;p&gt;OpenDoc was a radically different paradigm for software development and distribution, and I think this was could have been an interesting contender against the dominance that Microsoft and Adobe enjoys in their markets. OpenDoc actually did ship, and there were some products made using OpenDoc, most notably Apple's Cyberdog browser (https://en.wikipedia.org/wiki/Cyberdog).&lt;/p&gt;
    &lt;p&gt;Unfortunately, Apple was in dire straits in the mid-1990s. Windows 95 was a formidable challenger to Mac OS, and cheaper x86 PCs were viable alternatives to Macintosh hardware. Apple was an acquisition target; IBM and Apple almost merged, and there was also an attempt to merge Apple with Sun. Additionally, the Macintosh platform depended on the availability of software products like Microsoft Office and Adobe Photoshop, the very types of products that OpenDoc directly challenged. When Apple purchased NeXT in December 1996, Steve Jobs returned to Apple, and all work on OpenDoc ended not too long afterward, leading to this now-famous exchange during WWDC 1997 between Steve Jobs and an upset developer (https://www.youtube.com/watch?v=oeqPrUmVz-o).&lt;/p&gt;
    &lt;p&gt;I don't believe that OpenDoc fits in with Apple's business strategy, even today, and while Microsoft offers component-based technologies that are similar to OpenDoc (OLE, COM, DCOM, ActiveX, .NET), the Windows ecosystem is still dominated by monolithic applications.&lt;/p&gt;
    &lt;p&gt;I think it would have been cool had the FOSS community pursued component-based software. It would have been really cool to apt-get components from remote repositories and link them together, either using GUI tools, command-line tools, or programmatically to build custom solutions. Instead, we ended up with large, monolithic applications like LibreOffice, Firefox, GIMP, Inkscape, Scribus, etc.&lt;/p&gt;
    &lt;p&gt;3. I am particularly intrigued by Symbolics Genera (https://en.wikipedia.org/wiki/Genera_(operating_system)), an operating system designed for Symbolics Lisp machines (https://en.wikipedia.org/wiki/Symbolics). In Genera, everything is a Lisp object. The interface is an interesting hybrid of early GUIs and the command line. To me, Genera could have been a very interesting substrate for building component-based software; in fact, it would have been far easier building OpenDoc on top of Common Lisp than on top of C or C++. Sadly, Symbolics' fortunes soured after the AI winter of the late 1980s/early 1990s, and while Genera was ported to other platforms such as the DEC Alpha and later the x86-64 via the creation of a Lisp machine emulator, it's extremely difficult for people to obtain a legal copy, and it was never made open source. The closest things to Genera we have are Xerox Interlisp, a competing operating system that was recently made open source, and open-source descendants of Smalltalk-80: Squeak, Pharo, and Cuis-Smalltalk.&lt;/p&gt;
    &lt;p&gt;4. Apple's "interregnum" years between 1985 and 1996 were filled with many intriguing projects that were either never commercialized, were cancelled before release, or did not make a splash in the marketplace. One of the most interesting projects during the era was Bauhaus, a Lisp operating system developed for the Newton platform. Mikel Evins, a regular poster here, describes it here (https://mikelevins.github.io/posts/2021-07-12-reimagining-ba...). It would have been really cool to have a mass-market Lisp operating system, especially if it had the same support for ubiquitous dynamic objects like Symbolic Genera.&lt;/p&gt;
    &lt;p&gt;Re: obtaining a legal copy of Genera, as of 2023 Symbolics still existed as a corporate entity and they continued to sell x86-64 laptops with "Portable Genera 2.0". I bought one from them then, and occasionally see them listing some on Ebay. (This isn't intended as an advertisement or endorsement, just a statement. I think it's quite unfortunate that Symbolics's software hasn't been made freely available, since it's now really only of historical interest.)&lt;/p&gt;
    &lt;p&gt;OpenDoc was mostly given to Taligent (the Apple and IBM joint venture) to develop. It was full-on OO: about 35 files for a minimal application, which meant that Erich Gamma had to build a whole new type of IDE which was unusable. He likely learned his lesson: it's pretty hard to define interfaces between unknown components without forcing each one to know about all the others.&lt;/p&gt;
    &lt;p&gt;MIME types for mail addressed much of the demand for pluggable data types.&lt;/p&gt;
    &lt;p&gt;I'm intrigued by Symbolics Genera too. It would have been interesting seeing further development of Lisp OS, especially when they would have had internet connection. Rewriting part of your OS and see the changes in real time? Maybe web apps could have been just software written in Lisp, downloaded on the machine and directly being executed in a safe environment on top of the Genera image. Big stuff.&lt;/p&gt;
    &lt;p&gt;Windows Phone's UI is still with us, from Windows 8 onwards. Everything on 8, 10, and 11 is optimized for a touch interface on a small screen, which is ridiculous on a modern desktop with a 32" or so monitor and a trackball or mouse.&lt;/p&gt;
    &lt;p&gt;False. The Metro design was abandoned long ago. No live tiles, no typography-first minimal UIs in windows 10/11. I pin an email app to taskbar/start, I don't see the unread count.&lt;/p&gt;
    &lt;p&gt;From Windows 10, there is a switch between desktop and touch mode.&lt;/p&gt;
    &lt;p&gt;They stopped supporting small tablets some years ago though, and made it worse with every Windows update. I can only surmise that it was to make people stop using them. Slow GUI, low contrast, killed apps.&lt;/p&gt;
    &lt;p&gt;Fro me, DESQview. Microsoft tried to buy it in order to use its tech in their windows system. I wonder how things would be today if they were able to purchase it. But DESQview said "no".&lt;/p&gt;
    &lt;p&gt;Instead it went into a slow death spiral due to Windows 95.&lt;/p&gt;
    &lt;p&gt;Love seeing this one. My uncle was co-founder of Quarterdeck, and I grew up in a world of DESQview and QEMM. It was a big influence on me as a child.&lt;/p&gt;
    &lt;p&gt;Got a good family story about that whole acquisition attempt, but I don't want to speak publicly on behalf of my uncle. I know we've talked at length about the what-ifs of that moment.&lt;/p&gt;
    &lt;p&gt;I do have a scattering of some neat Quarterdeck memorabilia I can share, though:&lt;/p&gt;
    &lt;p&gt;DESQview/X sucked the wind out of DESQview's sails. It was, on paper, a massive upgrade. I had been running DESQview for years, with a dial-up BBS in the background.&lt;/p&gt;
    &lt;p&gt;But you couldn't actually buy /X. After trying to buy a copy, my publisher even contacted DESQ's marketing people to get a copy for me, and they wouldn't turn one over. Supposedly there were some copies actually sold, but too few, too late, and then /X was dropped. There was at least one more release of plain DESQview after that, but by then Windows was eating its lunch.&lt;/p&gt;
    &lt;p&gt;OSI's session layer did very little more than TCP/UDP port numbers; in the OSI model you would open a connection to a machine, then use that connection to open a session to a particular application.&lt;/p&gt;
    &lt;p&gt;X.400 was a nice idea, but the ideal of having a single global directory predates security. I can understand why it never happened&lt;/p&gt;
    &lt;p&gt;On X.509, the spec spends two chapters on attribute certificates, which I've never seen used in the wild. It's a shame; identity certificates do a terrible job at authentication&lt;/p&gt;
    &lt;p&gt;Fortress language. It suffered from being too Haskell-like in terms of too many, non-orthogonal features. Rust and Go applied lessons from it perhaps indirectly.&lt;/p&gt;
    &lt;p&gt;their operator precedence system was one of my favourite pieces of language design. the tl;dr was that you could group operators into precedence sets, and an expression involving operators that all came from the same set would have that set's precedence rules applied, but if you had an expression involving mixed sets you needed to add the parentheses. crucially, they also supported operator overloading, and the same operator could be used in a different set as long as everything could be parsed unambiguously. (caveat, I never used the language, I just read about the operator design in the docs and it was very eye opening in the sense that every other language's operator precedence system suddenly felt crude and haphazard)&lt;/p&gt;
    &lt;p&gt;Also, I did not experience them personally, but I love watching computing history videos on YouTube, and a lot of the computers and operating systems from the 1980s and early 1990s got buried too soon, mostly because of their owners being short-sighted idiots in not realizing the full potential of what computers and video games could become, and having wildly successful hits on their hands with legions of faithful fans but not knowing how to build on that success or what the fans actually wanted to see in updated hardware.&lt;/p&gt;
    &lt;p&gt;Humane AI Pin. I think they launched 2 years too early and were too greedy with device pricing and subscription. Also if they focused as accessory for Android/iPhone they could reduce power usage and cost as well.&lt;/p&gt;
    &lt;p&gt;Their execution was of course bad but I think today current LLM models are better and faster and there is much more OSS models to reduce costs. Hardware though looked nice and pico projector interesting concept even though not the best executed.&lt;/p&gt;
    &lt;p&gt;Wine predates ReactOS. It was basically a FOSS duplicate of Sun's WABI.&lt;/p&gt;
    &lt;p&gt;I wrote a bunch of software in Borland Delphi, which ran in Windows, Wine, and ReactOS with no problems. Well, except for ReactOS' lack of printing support.&lt;/p&gt;
    &lt;p&gt;As long as you stay within the ECMA or published Windows APIs, everything runs fine in Wine and ReactOS. But Microsoft products are full of undocumented functions, as well as checks to see if they're running on real Windows. That goes back to the Windows 3.1 days, when 3.1 developers regularly used OS/2 instead of DOS, and Microsoft started adding patches to fail under OS/2 and DR-DOS. So all that has to be accounted for by Wine and ReactOS. A lot of third-party software uses undocumented functions as well, especially stuff written back during the days when computer magazines were a thing, and regularly published that kind of information. A lot of programmers found the lure of undocumented calls to be irresistible, and they wound up in all kinds of commercial applications where they really shouldn't have been.&lt;/p&gt;
    &lt;p&gt;In my experience anything that will load under Wine will run with no problems. ReactOS has some stability problems, but then the developers specifically call it "alpha" software. Despite that, I've put customers on ReactOS systems after verifying all their software ran on it. It gets them off the Microsoft upgrade treadmill. Sometimes there are compatibility problems and I fall back to Wine on Linux. Occasionally nothing will do but real Windows.&lt;/p&gt;
    &lt;p&gt;Hard disagree. The Humane AI Pin ad was a classic silicon valley ad that screamed B2VC and demonstrated nothing actually useful that couldn't be done with an all-in-one phone app (or even the ChatGPT app) and bluetooth earbuds that you already have.&lt;/p&gt;
    &lt;p&gt;Which reduces its innovation level to nothing more than a chest-mounted camera.&lt;/p&gt;
    &lt;p&gt;You want real B2C products that people would actually buy? Look at the Superbowl ads instead. Then watch the Humane ad again. It's laughable.&lt;/p&gt;
    &lt;p&gt;Plone CMS. When it appeared in 2001, there was nothing comparable. I'm not sure there still is. It was very flexible, allowed to build complex websites from components. Many ideas were pretty novel, at least I've never seen them in any web framework/CMS before. It still exists but nowhere as popular as it was in 2000-2010s.&lt;/p&gt;
    &lt;p&gt;Knowing when to say "no" to a project is an important skill.&lt;/p&gt;
    &lt;p&gt;One always must define a one sentence goal or purpose, before teams think about how to build something.&lt;/p&gt;
    &lt;p&gt;Cell processors, because most coders can't do parallelism well&lt;/p&gt;
    &lt;p&gt;Altera consumer FPGA, as they chose behavioral rather than declarative best practices... then the Intel merger... metastability in complex systems is hard, and most engineers can't do parallelism well...&lt;/p&gt;
    &lt;p&gt;World Wide Web, because social-media and Marketers&lt;/p&gt;
    &lt;p&gt;Dozens of personal projects, because sometimes things stop being fun. =3&lt;/p&gt;
    &lt;p&gt;LSR, the "Linux Screen Reader", an ambitiousy designed Python implementation of a GUI screen reader developed by IBM starting around 2006 or so. The project was ended 2008 when IBM ended all its Accessibility involvement in FLOSS.&lt;/p&gt;
    &lt;p&gt;Ceylon, JVM language, developed by Red Hat, now abandoned at Eclipse. Lost the race with Kotlin but proposed more than just syntax sugar over Java. Anonymous union types, comprehensions, proper module system...&lt;/p&gt;
    &lt;p&gt;nah, glass was impressive for a such a big org like google, but smartphones are popular because people use them like portable televisions. glanceable info and walking directions are more like an apple watch sized market, without the fashion element. meta is about to find out.&lt;/p&gt;
    &lt;p&gt;I don’t think people are downvoting for the mention of Google Glass, but due to the rest of the comment making a value judgement many are sure to disagree with.&lt;/p&gt;
    &lt;p&gt;google glass sucks though and glasses will never be a thing. google and meta and … can spend $8T and come up with the most insane tech etc but no one will be wearing f’ing glasses :)&lt;/p&gt;
    &lt;p&gt;Founder perspective: “avoid patents by staying 20 years behind” is the tragedy. I published a 2-page CC0 initiative that splits protection into two layers: • GLOBAL layer — fast, low-friction recognition for non-strategic inventions • LOCAL-STRATEGIC layer — conventional national control for sensitive tech Goal: cut admin drag/time-to-market while keeping sovereignty intact.&lt;/p&gt;
    &lt;p&gt;Apple’s scanning system for CSAM. The vast majority of the debate was dominated by how people imagined it worked, which was very different to how it actually worked.&lt;/p&gt;
    &lt;p&gt;It was an extremely interesting effort where you could tell a huge amount of thought and effort went into making it as privacy-preserving as possible. I’m not convinced it’s a great idea, but it was a substantial improvement over what is in widespread use today and I wanted there to be a reasonable debate on it instead of knee-jerk outrage. But congrats, I guess. All the cloud hosting systems scan what they want anyway, and the one that was actually designed with privacy in mind got screamed out of existence by people who didn’t care to learn the first thing about it.&lt;/p&gt;
    &lt;p&gt;Good riddance to a system that would have provided precedent for client-side scanning for arbitrary other things, as well as likely false positives.&lt;/p&gt;
    &lt;p&gt;&amp;gt; I wanted there to be a reasonable debate on it&lt;/p&gt;
    &lt;p&gt;I'm reminded of a recent hit-piece about Chat Control, in which one of the proponent politicians was quoted as complaining about not having a debate. They didn't actually want a debate, they wanted to not get backlash. They would never have changed their minds, so there's no grounds for a debate.&lt;/p&gt;
    &lt;p&gt;We need to just keep making it clear the answer is "no", and hopefully strengthen that to "no, and perhaps the massive smoking crater that used to be your political career will serve as a warning to the next person who tries".&lt;/p&gt;
    &lt;p&gt;This. No matter how cool the engineering might have been, from the perspective of what surveillance policies it would have (and very possibly did) inspire/set precedent for… Apple was very much creating the Torment Nexus from “Don’t Create the Torment Nexus.”&lt;/p&gt;
    &lt;p&gt;&amp;gt; from the perspective of what surveillance policies it would have (and very possibly did) inspire/set precedent for…&lt;/p&gt;
    &lt;p&gt;I can’t think of a single thing that’s come along since that is even remotely similar. What are you thinking of?&lt;/p&gt;
    &lt;p&gt;I think it’s actually a horrible system to implement if you want to spy on people. That’s the point of it! If you wanted to spy on people, there are already loads of systems that exist which don’t intentionally make it difficult to do so. Why would you not use one of those models instead? Why would you take inspiration from this one in particular?&lt;/p&gt;
    &lt;p&gt;The problem isn’t the system as implemented; the problem is the very assertion “it is possible to preserve the privacy your constituents want, while running code at scale that can detect Bad Things in every message.”&lt;/p&gt;
    &lt;p&gt;Once that idea appears, it allows every lobbyist and insider to say “mandate this, we’ll do something like what Apple did but for other types of Bad People” and all of a sudden you have regulations that force messaging systems to make this possible in the name of Freedom.&lt;/p&gt;
    &lt;p&gt;Remember: if a model can detect CSAM at scale, it can also detect anyone who possesses any politically sensitive image. There are many in politics for whom that level of control is the actual goal.&lt;/p&gt;
    &lt;p&gt;&amp;gt; the problem is the very assertion “it is possible to preserve the privacy your constituents want, while running code at scale that can detect Bad Things in every message.”&lt;/p&gt;
    &lt;p&gt;Apple never made that assertion, and the system they designed is incapable of doing that.&lt;/p&gt;
    &lt;p&gt;&amp;gt; if a model can detect CSAM at scale, it can also detect anyone who possesses any politically sensitive image.&lt;/p&gt;
    &lt;p&gt;Apple’s system cannot do that. If you change parts of it, sure. But the system they proposed cannot.&lt;/p&gt;
    &lt;p&gt;To reiterate what I said earlier:&lt;/p&gt;
    &lt;p&gt;&amp;gt; The vast majority of the debate was dominated by how people imagined it worked, which was very different to how it actually worked.&lt;/p&gt;
    &lt;p&gt;So far, you are saying that you don’t have a problem with the system Apple designed, and you do have a problem with some other design that Apple didn’t propose, that is significantly different in multiple ways.&lt;/p&gt;
    &lt;p&gt;Also, what do you mean by “model”? When I used the word “model” it was in the context of using another system as a model. You seem to be using it in the AI sense. You know that’s not how it worked, right?&lt;/p&gt;
    &lt;p&gt;&amp;gt; Chat Control, and other proposals that advocate backdooring individual client systems.&lt;/p&gt;
    &lt;p&gt;Chat Control is older than Apple’s CSAM scanning and is very different from it.&lt;/p&gt;
    &lt;p&gt;&amp;gt; Clients should serve the user.&lt;/p&gt;
    &lt;p&gt;Apple’s system only scanned things that were uploaded to iCloud.&lt;/p&gt;
    &lt;p&gt;You missed the most important part of my comment:&lt;/p&gt;
    &lt;p&gt;&amp;gt; I think it’s actually a horrible system to implement if you want to spy on people. That’s the point of it! If you wanted to spy on people, there are already loads of systems that exist which don’t intentionally make it difficult to do so. Why would you not use one of those models instead? Why would you take inspiration from this one in particular?&lt;/p&gt;
    &lt;p&gt;I don’t think you can accurately describe it as client-side scanning and false positives were not likely. Depending upon how you view it, false positives were either extremely unlikely, or 100% guaranteed for practically everybody. And if you think the latter part is a problem, please read up on it!&lt;/p&gt;
    &lt;p&gt;&amp;gt; I'm reminded of a recent hit-piece about Chat Control, in which one of the proponent politicians was quoted as complaining about not having a debate. They didn't actually want a debate, they wanted to not get backlash. They would never have changed their minds, so there's no grounds for a debate.&lt;/p&gt;
    &lt;p&gt;Right, well I wanted a debate. And Apple changed their minds. So how is it reminding you of that? Neither of those things apply here.&lt;/p&gt;
    &lt;p&gt;No, but I have a hard time imagining a bug that would meaningfully compromise this kind of system. Can you give an example?&lt;/p&gt;
    &lt;p&gt;&amp;gt; How about making Apple vulnerable to demands from every government where they do business?&lt;/p&gt;
    &lt;p&gt;They already are. So are Google, Meta, Microsoft, and all the other giants we all use. And all those other companies are already scanning your stuff. Meta made two million reports in 2024Q4 alone.&lt;/p&gt;
    &lt;p&gt;Imagine harder. Apple has had several high profile security bugs in the last few years, and their OS is decried here as a buggy mess every release. QA teams went out of fashion.&lt;/p&gt;
    &lt;p&gt;The onus is on you to prove perfection before ruining lives on hardware they paid for.&lt;/p&gt;
    &lt;p&gt;100x worse on the vulnerability front, as the tech could be bent to any whim. Importantly, none of what you described is client-side scanning. Even I consider abiding rules on others’ property fair.&lt;/p&gt;
    &lt;p&gt;There is no place for spyware of any kind on my phone. Saying that it is to "protect the children" and "to catch terrorists" does not make it any more acceptable.&lt;/p&gt;
    &lt;p&gt;I believe my retro Nokia phones s60/s90 does not have any spyware. I believe earlier Nokia models like s40 or monochrome does not even have an ability to spy on me (but RMS considers triangulation as spyware). I don't believe any products from the duopoly without even root access are free from all kinds of vendor's rootkits.&lt;/p&gt;
    &lt;p&gt;Apple designed a system. People guessed at what it did. Their guesses were way off the mark. This poisoned all rational discussion on the topic. If you imagine a system that works differently to Apple’s system, you can complain about that imaginary system all you want, but it won’t be meaningful, it’s just noise.&lt;/p&gt;
    &lt;p&gt;You understand it just fine, you're just trying to pass you fantasy pod immutable safe future as rational while painting the obvious objections based on the real world as meaningless noise.&lt;/p&gt;
    &lt;p&gt;Your point did not come across. It still isn’t. I don’t know what you mean by “pass you fantasy pod immutable safe future as rational”. You aren’t making sense to me. I absolutely do not “understand it just fine”.&lt;/p&gt;
    &lt;p&gt;If they are running safe mandatory scans on your phones for this, you seem shocked and angry that anyone would imply that this would lead to safe mandatory scans on your phones for that and the other, and open the door for unsafe mandatory scans for whatever.&lt;/p&gt;
    &lt;p&gt;If you can't acknowledge this, it puts you in a position where you can't be convincing to people who need you to deflect obvious, well-known criticisms before beginning a discussion. It gives you crazy person or salesman vibes. These are arguments that someone with a serious interest in the technology would be aware of already and should be included as a prerequisite to being taken seriously. Doing this shows that you value other people's time and effort.&lt;/p&gt;
    &lt;p&gt;&amp;gt; you seem shocked and angry that anyone would imply that this would lead to safe mandatory scans on your phones for that and the other&lt;/p&gt;
    &lt;p&gt;Where have I given you that impression? The thing that annoys me is the sensible discussion being drowned out by ignorance.&lt;/p&gt;
    &lt;p&gt;&amp;gt; If you can't acknowledge this, it puts you in a position where you can't be convincing to people who need you to deflect obvious, well-known criticisms before beginning a discussion.&lt;/p&gt;
    &lt;p&gt;I cannot parse this, it’s word salad. People who need me to deflect criticisms? What? I genuinely do not understand what you are trying to say here. Maybe just break the sentences up into smaller ones? It feels like you’re trying to say too many things in too few sentences. What people? Why do they need me to deflect criticisms?&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45553132</guid><pubDate>Sat, 11 Oct 2025 22:16:18 +0000</pubDate></item><item><title>CamoLeak: Critical GitHub Copilot Vulnerability Leaks Private Source Code</title><link>https://www.legitsecurity.com/blog/camoleak-critical-github-copilot-vulnerability-leaks-private-source-code</link><description>&lt;doc fingerprint="9d071f1f98be66c7"&gt;
  &lt;main&gt;
    &lt;p&gt;Get details on our discovery of a critical vulnerability in GitHub Copilot Chat.&lt;/p&gt;
    &lt;head rend="h2"&gt;TL;DR:&lt;/head&gt;
    &lt;p&gt;In June 2025, I found a critical vulnerability in GitHub Copilot Chat (CVSS 9.6) that allowed silent exfiltration of secrets and source code from private repos, and gave me full control over Copilot’s responses, including suggesting malicious code or links.&lt;/p&gt;
    &lt;p&gt;The attack combined a novel CSP bypass using GitHub’s own infrastructure with remote prompt injection. I reported it via HackerOne, and GitHub fixed it by disabling image rendering in Copilot Chat completely.&lt;/p&gt;
    &lt;head rend="h2"&gt;Background&lt;/head&gt;
    &lt;p&gt;GitHub Copilot Chat is an AI assistant built into GitHub that helps developers by answering questions, explaining code, and suggesting implementations directly in their workflow.&lt;/p&gt;
    &lt;p&gt;Copilot Chat is context-aware: it can use information from the repository (such as code, commits, or pull requests) to provide tailored answers.&lt;/p&gt;
    &lt;p&gt;As always, more context = more attack surface.&lt;/p&gt;
    &lt;head rend="h2"&gt;Finding the prompt injection&lt;/head&gt;
    &lt;p&gt;As mentioned earlier, GitHub Copilot is context-aware - so I set out to make it notice me. To do this, I embedded a prompt directed at Copilot inside a pull request description.&lt;/p&gt;
    &lt;p&gt;But what’s the point if everyone can see it? Luckily, GitHub came to the rescue with a proper solution: invisible comments are an official feature! 🎉&lt;/p&gt;
    &lt;p&gt;You can find more details in their documentation: Hiding content with comments. By simply putting the content you want to hide inside:&lt;/p&gt;
    &lt;p&gt;I tried the same prompt but this time as a hidden comment inside the PR description, and it worked!&lt;/p&gt;
    &lt;p&gt;Interestingly, posting a hidden comment triggers the usual PR notification to the repo owner, but the content of the hidden comment isn’t revealed anywhere.&lt;/p&gt;
    &lt;p&gt;I attempted logging in with a different user and visited the pull request page. The prompt was injected into my context as well! &lt;/p&gt;
    &lt;p&gt;I then replaced the original “HOORAY” prompt with far more complex instructions, including code suggestions and Markdown rendering, and to my surprise, they worked flawlessly!&lt;/p&gt;
    &lt;p&gt;For instance, notice how effortlessly Copilot suggests this malicious Copilotevil package.&lt;/p&gt;
    &lt;p&gt;* Notice that the user who asked Copilot Chat to explain the PR is different from the user who posted the invisible prompt, demonstrating that the prompt can affect any user who visits the page.&lt;/p&gt;
    &lt;p&gt;Copilot operates with the same permissions as the user making the request, but it obviously needs access to the user’s private repositories to respond accurately. We can exploit this by including instructions in our injected prompt to access a victim user’s private repository, encode its contents in base16, and append it to a URL. Then, when the user clicks the URL, the data is exfiltrated back to us.&lt;/p&gt;
    &lt;p&gt;* Notice that the repository https://github.com/LegitSecurity/issues-service is a private repo inside a private GitHub organization!&lt;/p&gt;
    &lt;head rend="h2"&gt;Recap: What We Can Do&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Influence the responses generated by another user’s Copilot&lt;/item&gt;
      &lt;item&gt;Inject custom Markdown, including URLs, code, and images&lt;/item&gt;
      &lt;item&gt;Exploit the fact that Copilot runs with the same permissions as the victim user&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Bypassing Content Security Policy (CSP)&lt;/head&gt;
    &lt;p&gt;This is where things get tricky. If you’ve followed along so far, you’re probably thinking — just inject an HTML &amp;lt;img&amp;gt; tag into the victim’s chat, encode their private data as a parameter, and once the browser tries to render it, the data will be leaked.&lt;/p&gt;
    &lt;p&gt;Not so fast. GitHub enforces a very restrictive Content Security Policy (CSP), which blocks fetching images and other content types from domains that aren’t explicitly owned by GitHub. So, our “simple” &amp;lt;img&amp;gt; trick won’t work out of the box.&lt;/p&gt;
    &lt;p&gt;You’re probably asking yourself - wait, how does my fancy README manage to show images from third-party sites?&lt;/p&gt;
    &lt;p&gt;When you commit a README or any Markdown file containing external images, GitHub automatically processes the file, during this process:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;GitHub parses the Markdown and identifies any image URLs pointing to domains outside of GitHub.&lt;/item&gt;
      &lt;item&gt;URL rewriting via Camo: Each external URL is rewritten to a Camo proxy URL. This URL includes a HMAC-based cryptographic signature and points to https://camo.githubusercontent.com/….&lt;/item&gt;
      &lt;item&gt;Signed request verification: When a browser requests the image, the Camo proxy verifies the signature to ensure it was generated by GitHub. Only valid, signed URLs are allowed.&lt;/item&gt;
      &lt;item&gt;Content fetching: If the signature is valid, Camo fetches the external image from its original location and serves it through GitHub’s servers.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This process ensures that:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Attackers cannot craft arbitrary URLs to exfiltrate dynamic data.&lt;/item&gt;
      &lt;item&gt;All external images go through a controlled proxy, maintaining security and integrity.&lt;/item&gt;
      &lt;item&gt;The end user sees the image seamlessly in the README, but the underlying URL never exposes the original domain directly.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;More information about Camo can be found here.&lt;/p&gt;
    &lt;p&gt;Let’s look at an example: Committing a README file to GitHub that contains this URL:&lt;/p&gt;
    &lt;p&gt;Will be automatically changed inside the README into:&lt;/p&gt;
    &lt;p&gt;Rather than doing it manually through the website, you can use GitHub’s REST API to submit raw Markdown and receive it back with all external image URLs automatically converted to Camo proxy URLs.&lt;/p&gt;
    &lt;p&gt;Alright, so we can’t generate Camo URLs on the fly — without code execution, every &amp;lt;img&amp;gt; tag we inject into the victim’s chat must include a valid Camo URL signature that was pre-generated. Otherwise, GitHub’s reverse proxy won’t fetch the content.&lt;/p&gt;
    &lt;head rend="h2"&gt;The discovery&lt;/head&gt;
    &lt;p&gt;I spent a long time thinking about this problem before this crazy idea struck me.&lt;lb/&gt;If I create a dictionary of all letters and symbols in the alphabet, pre-generate their corresponding Camo URLs, embed this dictionary into the injected prompt, and then ask Copilot to play a “small game” by rendering the content I want to leak as “ASCII art” composed entirely of images, will Copilot inject valid Camo images that the browser will render by their order? Yes, it will.&lt;/p&gt;
    &lt;p&gt;I quickly got to work. First, I set up a web server that responds to every request with a 1x1 transparent pixel. This way, when GitHub’s Camo reverse proxy fetches the images from my server, they remain invisible in the victim’s chat.&lt;/p&gt;
    &lt;p&gt;Next, by using GitHub’s API, I created a valid Camo URL dictionary of all the letters and symbols that may be used to leak source code / issues content:&lt;/p&gt;
    &lt;p&gt;Turns into:&lt;/p&gt;
    &lt;p&gt;And finally, I created the prompt:&lt;/p&gt;
    &lt;p&gt;* I added "random" parameter at the end of each Camo URL and requested Copilot to generate each time a new random number and append it to the URL, this way caching is not a problem.&lt;/p&gt;
    &lt;p&gt;Our target: the description of a zero-day vulnerability inside an issue of a private project.&lt;/p&gt;
    &lt;p&gt;The result: Stealing zero days from private repositories.&lt;/p&gt;
    &lt;p&gt;PoC showcasing the full attack (Only if you have 4 minutes):&lt;/p&gt;
    &lt;p&gt;I also managed to get Copilot to search the victim’s entire codebase for the keyword "AWS_KEY" and exfiltrate the result.&lt;/p&gt;
    &lt;head rend="h2"&gt;GitHub’s Response&lt;/head&gt;
    &lt;p&gt;GitHub reports that the vulnerability was fixed as of August 14.&lt;/p&gt;
    &lt;head rend="h2"&gt;To learn more&lt;/head&gt;
    &lt;p&gt;Get details on a previous vulnerability we unearthed in GitLab Duo.&lt;/p&gt;
    &lt;p&gt;Get our thoughts on AppSec in the age of AI.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45553422</guid><pubDate>Sat, 11 Oct 2025 22:58:30 +0000</pubDate></item><item><title>Meta Superintelligence's surprising first paper</title><link>https://paddedinputs.substack.com/p/meta-superintelligences-surprising</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45553577</guid><pubDate>Sat, 11 Oct 2025 23:16:05 +0000</pubDate></item><item><title>Paper2video: Automatic video generation from scientific papers</title><link>https://arxiv.org/abs/2510.05096</link><description>&lt;doc fingerprint="341cba0b81f1c4ee"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Computer Vision and Pattern Recognition&lt;/head&gt;&lt;p&gt; [Submitted on 6 Oct 2025 (v1), last revised 9 Oct 2025 (this version, v2)]&lt;/p&gt;&lt;head rend="h1"&gt;Title:Paper2Video: Automatic Video Generation from Scientific Papers&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:Academic presentation videos have become an essential medium for research communication, yet producing them remains highly labor-intensive, often requiring hours of slide design, recording, and editing for a short 2 to 10 minutes video. Unlike natural video, presentation video generation involves distinctive challenges: inputs from research papers, dense multi-modal information (text, figures, tables), and the need to coordinate multiple aligned channels such as slides, subtitles, speech, and human talker. To address these challenges, we introduce Paper2Video, the first benchmark of 101 research papers paired with author-created presentation videos, slides, and speaker metadata. We further design four tailored evaluation metrics--Meta Similarity, PresentArena, PresentQuiz, and IP Memory--to measure how videos convey the paper's information to the audience. Building on this foundation, we propose PaperTalker, the first multi-agent framework for academic presentation video generation. It integrates slide generation with effective layout refinement by a novel effective tree search visual choice, cursor grounding, subtitling, speech synthesis, and talking-head rendering, while parallelizing slide-wise generation for efficiency. Experiments on Paper2Video demonstrate that the presentation videos produced by our approach are more faithful and informative than existing baselines, establishing a practical step toward automated and ready-to-use academic video generation. Our dataset, agent, and code are available at this https URL.&lt;/quote&gt;&lt;head rend="h2"&gt;Submission history&lt;/head&gt;From: Zeyu Zhu Mr. [view email]&lt;p&gt;[v1] Mon, 6 Oct 2025 17:58:02 UTC (6,815 KB)&lt;/p&gt;&lt;p&gt;[v2] Thu, 9 Oct 2025 17:29:00 UTC (6,817 KB)&lt;/p&gt;&lt;p&gt; Current browse context: &lt;/p&gt;&lt;p&gt;cs.CV&lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45553701</guid><pubDate>Sat, 11 Oct 2025 23:32:25 +0000</pubDate></item><item><title>Vancouver Stock Exchange: Scam capital of the world (1989) [pdf]</title><link>https://scamcouver.wordpress.com/wp-content/uploads/2012/04/scam-capital.pdf</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45553783</guid><pubDate>Sat, 11 Oct 2025 23:43:42 +0000</pubDate></item><item><title>Show HN: Rift – A tiling window manager for macOS</title><link>https://github.com/acsandmann/rift</link><description>&lt;doc fingerprint="b650c0e0fcb5811a"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Multiple layout styles &lt;list rend="ul"&gt;&lt;item&gt;Tiling (i3/sway-like)&lt;/item&gt;&lt;item&gt;Binary Space Partitioning (bspwm-like)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Menubar icon that shows all of the workspaces and the layouts within&lt;/item&gt;
      &lt;item&gt;MacOS-style mission control that allows you to visually navigate between workspaces&lt;/item&gt;
      &lt;item&gt;Focus follows the mouse with auto raise&lt;/item&gt;
      &lt;item&gt;Drag windows over one another to swap positions&lt;/item&gt;
      &lt;item&gt;Performant animations (as seen in the demo)&lt;/item&gt;
      &lt;item&gt;Switch to next/previous workspace with trackpad gestures (just like native macOS)&lt;/item&gt;
      &lt;item&gt;Hot reloadable configuration&lt;/item&gt;
      &lt;item&gt;Interop with third-party programs (ie Sketchybar) &lt;list rend="ul"&gt;&lt;item&gt;Requests can be made to rift via the cli or the mach port exposed (lua client here)&lt;/item&gt;&lt;item&gt;Signals can be sent on startup, workspace switches, and when the windows within a workspace change. These signals can be sent via a command(cli) or through a mach connection&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Does not require disabling SIP&lt;/item&gt;
      &lt;item&gt;Works with “Displays have separate Spaces” enabled (unlike all other major WMs)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Get up and running via the wiki: &lt;/p&gt;
    &lt;p&gt;Rift is in active development but is still generally stable. There is no official release yet; expect ongoing changes.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Issues and PRs are very welcome.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Aerospace worked well for me, but I missed animations and the ability to use fullscreen on one display while working on the other. I also prefer leveraging private/undocumented APIs as they tend to be more reliable (due to the OS being built on them and all the public APIs) and performant. for more on why rift exists and what rift strives to do, see the manifesto&lt;/p&gt;
    &lt;p&gt;Rift began as a fork (and is licensed as such) of glide-wm but has since diverged significantly. It uses private APIs reverse engineered by yabai and other projects. It is not affiliated with glide-wm or yabai.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45553995</guid><pubDate>Sun, 12 Oct 2025 00:22:15 +0000</pubDate></item><item><title>Coral Protocol: Open infrastructure connecting the internet of agents</title><link>https://arxiv.org/abs/2505.00749</link><description>&lt;doc fingerprint="49bea48bc7dd808e"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Multiagent Systems&lt;/head&gt;&lt;p&gt; [Submitted on 30 Apr 2025 (v1), last revised 17 Jul 2025 (this version, v2)]&lt;/p&gt;&lt;head rend="h1"&gt;Title:Coral Protocol: Open Infrastructure Connecting The Internet of Agents&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:Coral Protocol is an open and decentralized collaboration infrastructure that enables communication, coordination, trust and payments for The Internet of Agents. It addresses the growing need for interoperability in a world where organizations are deploying multiple specialized AI agents that must work together across domains and vendors. As a foundational platform for multi-agent AI ecosystems, Coral establishes a common language and coordination framework allowing any agent to participate in complex workflows with others. Its design emphasizes broad compatibility, security, and vendor neutrality, ensuring that agent interactions are efficient and trustworthy. In particular, Coral introduces standardized messaging formats for agent communication, a modular coordination mechanism for orchestrating multi-agent tasks, and secure team formation capabilities for dynamically assembling trusted groups of agents. Together, these innovations position Coral Protocol as a cornerstone of the emerging "Internet of Agents," unlocking new levels of automation, collective intelligence, and business value through open agent collaboration.&lt;/quote&gt;&lt;head rend="h2"&gt;Submission history&lt;/head&gt;From: Önder Gürcan [view email]&lt;p&gt;[v1] Wed, 30 Apr 2025 22:17:13 UTC (765 KB)&lt;/p&gt;&lt;p&gt;[v2] Thu, 17 Jul 2025 08:34:37 UTC (785 KB)&lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45555012</guid><pubDate>Sun, 12 Oct 2025 03:41:29 +0000</pubDate></item><item><title>Pipelining in psql (PostgreSQL 18)</title><link>https://postgresql.verite.pro/blog/2025/10/01/psql-pipeline.html</link><description>&lt;doc fingerprint="f799b5d2b8b01ede"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Pipelining in psql (PostgreSQL 18)&lt;/head&gt;
    &lt;head rend="h2"&gt;What is pipelining in Postgres?&lt;/head&gt;
    &lt;p&gt;Pipelining is a client-side feature supported by the network protocol that basically consists of not waiting for the results of previously sent queries before sending the next. This increases the throughput in two ways:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;The client, network and server can work in parallel. For instance, the network may transmit the results of the (N-1)th query while the server executes the Nth query and the client sends the (N+1)th query, all this at the same time.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The network is better utilized because successive queries can be grouped in the same network packets, resulting in less packets overall.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Pipelining is possible since version 7.4 (released in 2003), which introduced the extended query protocol. But itâs only since 2021, with PostgreSQL 14, that it can be used through libpq, the client-side C library. Since then, some libpq-based drivers like psycopg3 have started to support it.&lt;/p&gt;
    &lt;p&gt;With PostgreSQL 18, released last week, &lt;code&gt;psql&lt;/code&gt;, the command line client
comes equipped with commands to use pipelining in SQL scripts, making it
even more accessible.
While this addition is not part of the highlighted features of that release,
it can provide huge gains in query throughput, as weâre going to see in a simple
test.&lt;/p&gt;
    &lt;head rend="h2"&gt;psql commands&lt;/head&gt;
    &lt;p&gt;The pipeline is started with &lt;code&gt;\startpipeline&lt;/code&gt;, and in the most simple case, followed
by the SQL queries and ended with &lt;code&gt;\endpipeline&lt;/code&gt;.
If intermediate results are needed, we can use &lt;code&gt;\syncpipeline&lt;/code&gt; to force a 
synchronisation point and &lt;code&gt;\getresults&lt;/code&gt; to fetch all results up to that point.
Also, starting a pipeline creates an implicit transaction. If a query fails,
all the changes since the start (or before the last synchronization point) will
be rolled back.&lt;/p&gt;
    &lt;p&gt;If you know about the &lt;code&gt;\;&lt;/code&gt; syntax to group several queries in the same request, there are similarities between this technique and pipelining: theyâre both used to reduce server round-trips and have the same semantics with regard to transactions. In a way, pipelining is the evolution in the extended query protocol of what multi-statement queries (&lt;code&gt;\;&lt;/code&gt; in psql) are in the simple query protocol.&lt;/p&gt;
    &lt;head rend="h2"&gt;Performance test&lt;/head&gt;
    &lt;p&gt;Letâs do a simple test where data from devices are imported with &lt;code&gt;INSERT ... ON CONFLICT&lt;/code&gt; queries. Same-device same-date does update the row,
otherwise it inserts a new row.
Note that if we wanted to unconditionally append all rows,
&lt;code&gt;COPY&lt;/code&gt; would be preferable and pipelining not necessary, which is why
the more sophisticated insert-or-update is chosen for that test.&lt;/p&gt;
    &lt;p&gt;The following bash code imports the (random) data, with or without the pipelining depending on a parameter.&lt;/p&gt;
    &lt;code&gt;function import_data
{
  local count=$1  # how many rows?
  local pipeline=$2 # 1 or 0
  local now_ts=$(date +%s)

  (
    echo 'PREPARE s AS insert into events(device, recorded_at, measure)
values($1, to_timestamp($2), $3) on conflict(device,recorded_at) do update set measure=excluded.measure;'
    echo "BEGIN;"
    [[ $pipeline = 1 ]] &amp;amp;&amp;amp; echo "\\startpipeline"
    for i in $(seq 1 $count)
    do
      device=$RANDOM
      secs=$(($now_ts + $RANDOM*50))
      measure=${RANDOM}"."${RANDOM}
      echo "execute s($device, '$secs', $measure);"
    done
    [[ $pipeline = 1 ]] &amp;amp;&amp;amp; echo "\\endpipeline"
    echo "COMMIT;"
  ) | $psql -q -v ON_ERROR_STOP=1
}
&lt;/code&gt;
    &lt;p&gt;Letâs try this with batches of 100, 1000, 5000, 10000, 50000, 100000 rows, with and without pipelining, and compare how fast these batches are processed.&lt;/p&gt;
    &lt;p&gt;Also, since the network speed matters a lot here, letâs try with three typical kinds of network connections:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;localhost (ping time ~ 0.04ms): client and server are on the same host.&lt;/item&gt;
      &lt;item&gt;LAN (ping time ~ 1ms): client and server are separated only by an Ethernet 1GB/s switch.&lt;/item&gt;
      &lt;item&gt;WAN (ping time ~ 4ms): the server is reached through a public Internet connection.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Finally, each case is run 5 times and we keep only the median time of the runs.&lt;/p&gt;
    &lt;p&gt;On the same host, the pipeline acceleration ranges from 1.5x for the smallest batch size, up to 5x.&lt;/p&gt;
    &lt;p&gt;On a local network connection, the smallest batch size is accelerated by 2.6x, and it goes up to 42x with the bigger sizes.&lt;/p&gt;
    &lt;p&gt;On the slowest network, itâs even more impressive. The acceleration is between 5.4x and 71x !&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;These accelerations show how under-utilized the network is when we send batches of small queries without pipelining: the network packets are like 50 seater buses that ride with only one passenger.&lt;/p&gt;
    &lt;p&gt;In our example, all we have to do to optimize on that front is to add a pair of &lt;code&gt;\startpipeline&lt;/code&gt; and &lt;code&gt;\endpipeline&lt;/code&gt;.
Thatâs because our queries do not depend on the results of previous queries
of the same batch, except in the sense that if one fails, the entire batch fails.&lt;/p&gt;
    &lt;p&gt;Without pipelining, we could still optimize our test by adding many rows to the &lt;code&gt;VALUES&lt;/code&gt; clauses for each query instead of one row per
query. But itâs not easy to find the sweet spot for how many data rows
there needs to be per query, and large queries with thousands of parameters
are not the nicest to handle on the server side.
Also, if the client-side logic is more complicated, for instance
conditionally targeting several tables, running simple statements in
a pipeline while using row-by-row logic might be much easier.&lt;/p&gt;
    &lt;p&gt;The pipelining meta-commands were added in psql version 18, but they do not require PostgreSQL 18 on the server side. For those interested in this feature who canât upgrade their server soon, you can still upgrade to the latest version of psql: itâs backward-compatible as much as possible.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45555308</guid><pubDate>Sun, 12 Oct 2025 04:46:57 +0000</pubDate></item><item><title>Show HN: I made an esoteric programming language that's read like a spellbook</title><link>https://github.com/sirbread/spellscript</link><description>&lt;doc fingerprint="943a9b8b627ac612"&gt;
  &lt;main&gt;
    &lt;p&gt;an esoteric programming language where code reads like magical incantations from an ancient spellbook. every program is a "spell" written in a "grimoire," so theorhetically, you can write all your code english essay style due to its no newline/indentation requirement.&lt;/p&gt;
    &lt;p&gt;write code that looks like this:&lt;/p&gt;
    &lt;code&gt;begin the grimoire.
summon the power with essence of 7.
conjure ritual named amplify with value to return value multiplied by value.
summon the result with essence of through ritual amplify with power.
inscribe whispers of "the power is amplified: " bound with result.
close the grimoire.
&lt;/code&gt;
    &lt;p&gt;output: &lt;code&gt;the power is amplified: 49&lt;/code&gt;&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;variables&lt;/item&gt;
      &lt;item&gt;dynamic typing&lt;/item&gt;
      &lt;item&gt;arrays&lt;/item&gt;
      &lt;item&gt;functions&lt;/item&gt;
      &lt;item&gt;conditionals/loops&lt;/item&gt;
      &lt;item&gt;string manipulation&lt;/item&gt;
      &lt;item&gt;type conversion&lt;/item&gt;
      &lt;item&gt;user input&lt;/item&gt;
      &lt;item&gt;output&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;clone thy repo&lt;/item&gt;
      &lt;item&gt;make sure you have python 3.6 +&lt;/item&gt;
      &lt;item&gt;create a file called &lt;code&gt;&amp;lt;filename&amp;gt;.spell&lt;/code&gt;:&lt;/item&gt;
      &lt;item&gt;then run &lt;code&gt;python spellscript.py your-spell.spell&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;concept&lt;/cell&gt;
        &lt;cell role="head"&gt;spellscript&lt;/cell&gt;
        &lt;cell role="head"&gt;traditional&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;declare variable&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;summon the x with essence of 10&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;x = 10&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;modify variable&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;enchant x with 20&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;x = 20&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;inscribe x&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;print(x)&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;input&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;inquire whispers of "prompt" into x&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;x = input("prompt")&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;string&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;whispers of "text"&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;"text"&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;array&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;collection holding 1 and 2 and 3&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;[1, 2, 3]&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;if statement&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;if the signs show x equals 5 then&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;if x == 5:&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;loop&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;repeat the incantation 5 times to begin:&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;for i in range(5):&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;function&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;conjure ritual named add with a and b to&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;def add(a, b):&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;return&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;return result&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;return result&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;operation&lt;/cell&gt;
        &lt;cell role="head"&gt;spellscript&lt;/cell&gt;
        &lt;cell role="head"&gt;traditional&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;addition&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;a greater by b&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;a + b&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;subtraction&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;a lesser by b&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;a - b&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;multiplication&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;a multiplied by b&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;a * b&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;division&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;a divided by b&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;a / b&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;equals&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;a equals b&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;a == b&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;greater than&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;a greater than b&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;a &amp;gt; b&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;less than&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;a less than b&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;a &amp;lt; b&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;and&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;a and b&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;a and b&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;or&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;a or b&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;a or b&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;not&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;not a&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;not a&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;resources/documentation.md - feature documentation&lt;/item&gt;
      &lt;item&gt;resources/examples - example programs&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;no nested arrays&lt;/item&gt;
      &lt;item&gt;no string indexing (use character arrays)&lt;/item&gt;
      &lt;item&gt;no modulo operator&lt;/item&gt;
      &lt;item&gt;no break/continue in loops&lt;/item&gt;
      &lt;item&gt;no comments&lt;/item&gt;
      &lt;item&gt;no recursion (use iteration)&lt;/item&gt;
      &lt;item&gt;functions must have at least one parameter&lt;/item&gt;
      &lt;item&gt;no null concept&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;idea from the muffin programming language&lt;/item&gt;
      &lt;item&gt;ai was used for debugging some inperpreter issues, which included rituals and conditionals.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45555523</guid><pubDate>Sun, 12 Oct 2025 05:31:00 +0000</pubDate></item><item><title>Faster LLM inference</title><link>https://www.together.ai/blog/adaptive-learning-speculator-system-atlas</link><description>&lt;doc fingerprint="e3dd3bba699a0e85"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;AdapTive-LeArning Speculator System (ATLAS): A New Paradigm in LLM Inference via Runtime-Learning Accelerators&lt;/head&gt;
    &lt;p&gt;ATLAS delivers up to 4x faster LLM inference, powered by Together Turboâs latest research.&lt;/p&gt;
    &lt;p&gt;At Together AI, the AIÂ Native Cloud, weâre obsessed with performance. Making large language models faster, cheaper, and more efficient is not a one-trick problem â it requires optimizing along multiple axes. That is the philosophy behind Together Turbo, our suite of inference innovations that draw from research in algorithms, architectures, and modeling recipes. Weâre excited to introduce the AdapTive-LeArning Speculator System (ATLAS), the first speculator of its kind that gives automatic performance improvements without any manual tuning.&lt;/p&gt;
    &lt;p&gt;ATLAS offers a new way of doing speculative decoding â one that dynamically improves at runtime â and it fits seamlessly alongside our other Turbo techniques like the proprietary Together Turbo Speculator or Custom Speculators. But why create an adaptive-learning speculator system?&lt;/p&gt;
    &lt;p&gt;Standard speculators are trained for general workloads. Custom speculators are trained on your specific data, but only for a specific snapshot in time. However, as the workload evolves (codebase grows, traffic patterns shift, request distributions change), even highly customized speculators can fall behind. In contrast, ATLAS evolves automatically with usage, learning from both historical patterns and live traffic to continuously align with the target modelâs behaviors in real time. This means the more you use our inference service, the better ATLAS will perform!Â&lt;/p&gt;
    &lt;p&gt;Built on top of Together Turbo Speculator, ATLAS reaches up to 500 TPS on DeepSeek-V3.1 and up to 460 TPS on Kimi-K2 in a fully adapted scenario â 2.65x faster than standard decoding, outperforming even specialized hardware like Groq (Figure 1).&lt;/p&gt;
    &lt;head rend="h2"&gt;1. Speculative Decoding&lt;/head&gt;
    &lt;p&gt;Speculative decoding is one of the most powerful levers for accelerating inference.2 Instead of having the target model generate every token step by step, a faster speculator (also known as the draft model) proposes multiple tokens ahead, and the target model verifies them in parallel in a single forward pass. The verification process ensures that the quality of the output matches the distribution of non-speculative decoding, while achieving speedups by accepting many tokens at a time.&lt;/p&gt;
    &lt;p&gt;The overall speed is influenced by the acceptance rate $Î±$ (i.e., how often the target model agrees with the drafted tokens from the speculator) and the relative latency $c$ of the draft versus the target. Typically, larger speculators with more parameters yield higher acceptance rates due to their higher capacity but are slower to generate draft tokens. Progress therefore comes from both sides: aligning draft and target models to increase $Î±$ (training objectives, data, and algorithms) and designing draft models/kernels that keep $c$ low while maintaining $Î±$ (sparsity, quantization, lightweight &amp;amp; kernel-efficient architectures). The sweet spot is where a high $Î±$ meets a low $c$, minimizing end-to-end latency.&lt;/p&gt;
    &lt;p&gt;â&lt;/p&gt;
    &lt;p&gt;At Together AI, the Turbo team has developed high-performance speculators that have achieved the worldâs fastest decoding speeds on NVIDIA Blackwell by drawing on advances across architecture, sparsity, algorithms, post-training recipes, and data [1-9]. Weâve built a speculator design and selection framework that determines the optimal speculator architecture (width/depth, lookahead, sparsity/quantization, KV reuse) and a scalable training system that brings up speculators for the largest and most challenging open-source targets quickly and reproducibly (e.g., DeepSeek-V3.1 and Kimi-K2). For instance, while Kimi ships without a ready-to-use speculator, we can train and deploy one rapidly and take Kimi from ~150 TPS out of the box to 270+ TPS on the same hardware and batch settings, while preserving target-model quality (see Figure 1, yellow bars). This pipeline powers Turbo Speculators that deliver state-of-the-art decoding latency, and it sets the stage for what comes next: an Adaptive-Learning Speculator System that adjusts token drafting to the workload in real time.&lt;/p&gt;
    &lt;head rend="h2"&gt;2. Introducing Turboâs Adaptive-Learning Speculator System&lt;/head&gt;
    &lt;p&gt;At Together AI, we power a broad range of inference workloads. But todayâs speculative decoding methods are constrained to using a static speculator, trained on a fixed dataset. Once deployed, the speculator cannot adapt, leading to degrading performance if the input distribution evolves. This problem is particularly pronounced in serverless, multi-tenant environments, where input diversity is sky-high. New users continuously arrive, and bring with them unique workloads that the fixed speculator may not have seen during training. Furthermore, these speculators typically use a fixed lookahead, predicting the same number of tokens regardless of the speculatorâs confidence. Put simply, a static speculator cannot keep up.&lt;/p&gt;
    &lt;p&gt;To address these limitations, we designed the Adaptive-Learning Speculative System with two cooperating speculators, as shown in Figure 3:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A heavyweight static speculator trained on a broad corpus that provides strong, general speculation.&lt;/item&gt;
      &lt;item&gt;A lightweight adaptive speculator that allows for rapid, low-overhead updates from real-time traffic, specializing on-the-fly to emerging domains.&lt;/item&gt;
      &lt;item&gt;A confidence-aware controller that chooses which speculator to trust at each step and what speculation lookahead to use, using longer speculations when the speculator has high confidence.Â&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Efficiency Guardrail via Static Speculator. The static Turbo Speculator serves as an always-on speed floor: it is trained on a broad corpus and remains stable across workloads, so TPS does not collapse when traffic shifts or the adaptive path is cold. In ATLAS, we use it to jump-start speed and provide a fail-safe fallbackâif confidence drops or drift is detected, the controller shortens lookahead or routes back to the static path to preserve latency while the adaptive speculator relearns.&lt;/p&gt;
    &lt;p&gt;Customized Speculator vs. Adaptive-Learning. We know from our previous studies that a customized speculator trained on samples from real traffic that mirror expected usage delivers an additional speed boost. The Adaptive-Learning Speculator enables us to be even more customized in real time. For instance, during a vibe-coding session, the adaptive system can specialize a lightweight speculator for the relevant code files being edited and not seen during training, further increasing the acceptance rate and decoding speed. This kind of on-the-fly specialization is hard to achieve with static speculators.&lt;/p&gt;
    &lt;p&gt;Accelerating RL Training. Reinforcement learning (RL) alternates between two phases: (1) a rollout phase, where the current policy generates trajectories and receives rewards, and (2) an update phase, where we use the rewards to update the policy. In practice, rollouts are often the bottleneck, accounting for roughly 70% of total wall-clock time3. In general, because the policy distribution shifts throughout training, static speculators quickly fall out of alignment with the target policy, resulting in sub-optimal throughput.4 ATLAS addresses this by adapting online to the evolving policy and the specific RL domain, maintaining alignment and reducing the overall rollout time. The domain-specific, iterative nature of RL further enables rapid adaptation, yielding sustained and growing speedups. As shown in Figure 4, applying ATLAS to the RL-MATH pipeline produces increasing speedups as training progresses.&lt;/p&gt;
    &lt;p&gt;Built as part of the Turbo optimization suite. The Adaptive-Learning Speculator System is a core component of the broader Turbo optimization suite, where each layer of optimization compounds the benefits of the others. As illustrated in Figure 5, performance progressively improves through near-lossless quantization (calibrated to preserve quality), the Turbo Speculator, and finally the Adaptive-Learning Speculator System. Additional optimizations in the suite include TurboBoost-TTFT (not shown) for reducing time-to-first-token latency, further contributing to end-to-end acceleration.&lt;/p&gt;
    &lt;p&gt;Extreme Peak Efficiency. When the input distribution is narrow and outputs closely echo previously seen tokens, the adaptive system specializes quickly. In this scenario, the controller becomes confident in utilizing more tokens from the lightweight speculator and lengthens lookahead tokens. This yields consistently higher TPS than static or one-off custom speculators can maintain. As shown in Figures 1 and 5, once fully adapted to Arena-Hard traffic, DeepSeek achieves up to 500 tokens per second for batch size 1 on 4 B200 GPUs, delivering a 400% speedup over the FP8 baseline (improvement from 105 TPS to 501 TPS).&lt;/p&gt;
    &lt;p&gt;{{custom-cta-1}}&lt;/p&gt;
    &lt;head rend="h2"&gt;Build the Future of Efficient AI&lt;/head&gt;
    &lt;p&gt;In parallel to making models smarter and more capable, advancements in inference efficiency are just as transformative â because intelligence only matters when you can deliver it swiftly, cost-effectively, and at scale. At Together AI, our Turbo team turns cutting-edge research (algorithms, quantization, sparsity, distillation, architectures, model pruning, and post-training recipes) into production systems that cut costs and unlock entirely new product experiences. If you love turning elegant ideas into billions of faster tokens, obsess over optimizing efficiency-quality frontier, and want your research to land in real usersâ hands quickly, come build with us. Weâre hiring exceptional research scientists and engineers who can push the frontier of efficient AI. Apply to Together and help define how intelligence is deployed.&lt;/p&gt;
    &lt;head rend="h2"&gt;Footnotes&lt;/head&gt;
    &lt;p&gt;1. Each benchmark burst contained 32 arena-hard prompts (â 3,000 token completions on average). Since the observed TPS depends on the prompts while speculative decoding is used, we report tokens per second (TPS) as the mean across requests. In the Adaptive-Learning Speculator System scenario, we show the peak speed where the system is fully adapted to Arena Hard traffic. We use TP=4 for DeepSeek-V3.1 and TP=8 for Kimi-K2-0905 on NVIDIA B200. This is different from our previous blog where we used TP=8, EP=2 for DeepSeek.&lt;lb/&gt;2. Fast Inference from Transformers via Speculative Decoding.&lt;lb/&gt;3. DeepCoder: A Fully Open-Source 14B Coder at O3-mini Level DeepSWE: Training a Fully Open-sourced, State-of-the-Art Coding Agent by Scaling RL&lt;lb/&gt;4. Speculative decoding is typically not beneficial in pure throughput- or compute-bound settings, where GPUs are fully saturated. However, in reinforcement learning (RL) training, the situation can be different. Agent RL training often operates with small batch sizes and CPU-driven environment steps, where each agent waits for model outputs to do the next action. In this regime, endpoint latency (tokens per second per request) becomes the bottleneck. This makes speculative decoding highly applicable for RL training, if there is a suitable and high performant speculator. Faster decoding pipeline can improve CPU utilization and overall sample throughput.Â &lt;/p&gt;
    &lt;head rend="h2"&gt;References&lt;/head&gt;
    &lt;p&gt;[1] Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads&lt;/p&gt;
    &lt;p&gt;[2] SpecExec: Massively Parallel Speculative Decoding for Interactive LLM Inference on Consumer Devices&lt;/p&gt;
    &lt;p&gt;[3] Ladder-Residual: Parallelism-Aware Architecture for Accelerating Large Model Inference with Communication OverlappingÂ&lt;/p&gt;
    &lt;p&gt;[4] TEAL: Training-Free Activation Sparsity in Large Language Model&lt;/p&gt;
    &lt;p&gt;[5] The Mamba in the Llama: Distilling and Accelerating Hybrid Models&lt;/p&gt;
    &lt;p&gt;[6] SEQUOIA: Scalable and Robust Speculative Decoding&lt;/p&gt;
    &lt;p&gt;[7] Mixture-of-Agents Alignment: Harnessing the Collective Intelligence of Open-Source LLMs to Improve Post-Training&lt;/p&gt;
    &lt;p&gt;[8] Boosting DeepSeek-R1âs Speed with Customized Speculative Decoding&lt;/p&gt;
    &lt;p&gt;[9] DeepSWE: Training a Fully Open-sourced, State-of-the-Art Coding Agent by Scaling RL&lt;/p&gt;
    &lt;p&gt;â&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;LOREM IPSUM&lt;/p&gt;
        &lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;LOREM IPSUM&lt;/p&gt;
        &lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Value Prop #1&lt;/p&gt;
        &lt;p&gt;Body copy goes here lorem ipsum dolor sit amet&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Bullet point goes here lorem ipsum Â&lt;/item&gt;
          &lt;item&gt;Bullet point goes here lorem ipsum Â&lt;/item&gt;
          &lt;item&gt;Bullet point goes here lorem ipsum Â&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Value Prop #1&lt;/p&gt;
        &lt;p&gt;Body copy goes here lorem ipsum dolor sit amet&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Bullet point goes here lorem ipsum Â&lt;/item&gt;
          &lt;item&gt;Bullet point goes here lorem ipsum Â&lt;/item&gt;
          &lt;item&gt;Bullet point goes here lorem ipsum Â&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Value Prop #1&lt;/p&gt;
        &lt;p&gt;Body copy goes here lorem ipsum dolor sit amet&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Bullet point goes here lorem ipsum Â&lt;/item&gt;
          &lt;item&gt;Bullet point goes here lorem ipsum Â&lt;/item&gt;
          &lt;item&gt;Bullet point goes here lorem ipsum Â&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;List Item Â #1&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt.&lt;/item&gt;
          &lt;item&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt.&lt;/item&gt;
          &lt;item&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;List Item Â #1&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt.&lt;/item&gt;
          &lt;item&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt.&lt;/item&gt;
          &lt;item&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;List Item Â #1&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt.&lt;/item&gt;
          &lt;item&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt.&lt;/item&gt;
          &lt;item&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;List Item Â #2&lt;/p&gt;
        &lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;List Item #2&lt;/p&gt;
        &lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;List Item #3&lt;/p&gt;
        &lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;article&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45556474</guid><pubDate>Sun, 12 Oct 2025 08:37:01 +0000</pubDate></item><item><title>Why it took 4 years to get a lock files specification</title><link>https://snarky.ca/why-it-took-4-years-to-get-a-lock-files-specification/</link><description>&lt;doc fingerprint="1700b97754a785d0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Why it took 4 years to get a lock files specification&lt;/head&gt;
    &lt;p&gt;(This is the blog post version of my keynote from EuroPython 2025 in Prague, Czechia.)&lt;/p&gt;
    &lt;p&gt;We now have a lock file format specification. That might not sound like a big deal, but for me it took 4 years of active work to get us that specification. Part education, part therapy, this post is meant to help explain what make creating a lock file difficult and why it took so long to reach this point.&lt;/p&gt;
    &lt;head rend="h1"&gt;What goes into a lock file&lt;/head&gt;
    &lt;p&gt;A lock file is meant to record all the dependencies your code needs to work along with how to install those dependencies.&lt;/p&gt;
    &lt;p&gt;That involves The "how" is source trees, source distributions (aka sdists), and wheels. With all of these forms, the trick is recording the right details in order to know how to install code in any of those three forms. Luckily we already had the &lt;code&gt;direct_url.json&lt;/code&gt; specification that just needed translation into TOML for source trees. As for sdists and wheels, it's effectively recording what an index server provides you when you look at a project's release.&lt;/p&gt;
    &lt;p&gt;The much trickier part is figuring what to install when. For instance, let's consider where your top-level, direct dependencies come from. In &lt;code&gt;pyproject.toml&lt;/code&gt; there's &lt;code&gt;project.dependencies&lt;/code&gt; for dependencies you always need for your code to run, &lt;code&gt;project.optional-dependencies&lt;/code&gt; (aka extras), for when you want to offer your users the option to install additional dependencies, and then there's &lt;code&gt;dependency-groups&lt;/code&gt; for dependencies that are not meant for end-users (e.g. listing your test dependencies).&lt;/p&gt;
    &lt;p&gt;But letting users control what is (not) installed isn't the end of things. There's also the specifiers you can add to any of your listed dependencies. They allow you to not only restrict what versions of things you want (i.e. setting a lower-bound and not setting an upper-bound if you can help it), but also when the dependency actually applies (e.g. is it specific to Windows?).&lt;/p&gt;
    &lt;p&gt;Put that all together and you end up with a graph of dependencies who edges dictate whether a dependency applies on some platform. If you manage to write it all out then you have multi-use lock files which are portable across platforms and whatever options the installing users selects, compared to single-use lock files that have a specific applicability due to only supporting a single platform and set of input dependencies.&lt;/p&gt;
    &lt;p&gt;Oh, and even getting the complete list of dependencies in either case is an NP-complete problem.&lt;/p&gt;
    &lt;p&gt;And it make makes things "interesting", I also wanted the file format to be written by software but readable by people, secure by default, fast to install, and allow the locker which write the lock file to be different from the installer that performs the install (and either be written in a language other than Python).&lt;/p&gt;
    &lt;p&gt;In the end, it all worked out (luckily); you can read the spec for all the nitty-gritty details about &lt;code&gt;pylock.toml&lt;/code&gt; or watch the keynote where I go through the spec. But it sure did take a while to get to this point.&lt;/p&gt;
    &lt;head rend="h1"&gt;Why it took (over) 4 years&lt;/head&gt;
    &lt;p&gt;I'm not sure if this qualifies as the longest single project I have ever taken on for Python (rewriting the import system might still hold that record for me), but it definitely felt the most intense over a prolonged period of time.&lt;/p&gt;
    &lt;p&gt;The oldest record I have that I was thinking about this problem is a tweet from Feb 2019:&lt;/p&gt;
    &lt;head rend="h2"&gt;2019&lt;/head&gt;
    &lt;p&gt;That year there were 106 posts on discuss.python.org about a &lt;code&gt;requirements.txt&lt;/code&gt; v2 proposal. It didn't come to any specific conclusion that I can recall, but it at least got the conversation started.&lt;/p&gt;
    &lt;head rend="h2"&gt;2020&lt;/head&gt;
    &lt;p&gt;The next year, the conversation continued and generated 43 posts. I was personally busy with PEP 621 and the &lt;code&gt;[project]&lt;/code&gt; table in &lt;code&gt;pyproject.toml&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;2021&lt;/head&gt;
    &lt;p&gt;In January of 2021 Tzu-Ping Chung, Pradyun Gedam, and myself began researching how other language ecosystems did lock files. It culminated in us writing PEP 665 and posting it in July. That led to 359 posts that year.&lt;/p&gt;
    &lt;p&gt;The goal of PEP 665 was a very secure lock file which partially achieved that goal by only supporting wheels. With no source trees or sdists to contend with, it meant installation didn't involve executing a build back-end which can be slow, be indeterminate, and a security risk simply due to running more code. We wrote the PEP with the idea that any source trees or sdists would be built into wheels out-of-band so you could then lock against those wheels.&lt;/p&gt;
    &lt;head rend="h2"&gt;2022&lt;/head&gt;
    &lt;p&gt;In the end, PEP 665 was rejected in January of 2022, generating 106 posts on the subject both before and after the rejection. It turns out enough people had workflows dependent on sdists that they balked at having the added step of building wheels out-of-band. There was also some desire to also lock the build back-end dependencies.&lt;/p&gt;
    &lt;head rend="h2"&gt;2023&lt;/head&gt;
    &lt;p&gt;After the failure of PEP 665, I decided to try to tackle the problem again entirely on my own. I didn't want to drag other poor souls into this again and I thought that being opinionated might make things a bit easier (compromising to please everyone can lead to bad outcomes when a spec if large and complicated like I knew this would be).&lt;/p&gt;
    &lt;p&gt;I also knew I was going to need a proof-of-concept. That meant I needed code that could get metadata from an index server, resolve all the dependencies some set of projects needed (at least from a wheel), and at least know what I would install on any given platform. Unfortunately a lot of that didn't exist as some library on PyPI, so I had to write a bunch of it myself. Luckily I had already started the journey before with my mousebender project, but that only covered the metadata from an index server. I still needed to be able to read &lt;code&gt;MEtADATA&lt;/code&gt; files from a wheel and do the resolution. The former Donald Stufft had taken a stab at and which I picked up and completed, leading to &lt;code&gt;packaging.metadata&lt;/code&gt;. I then used resolvelib to create a resolver.&lt;/p&gt;
    &lt;p&gt;As such there were only 54 posts about lock files that were general discussion. The key outcome there was trying to lock for build back-ends confused people too much, and so I dropped that feature request from my thinking.&lt;/p&gt;
    &lt;head rend="h2"&gt;2024&lt;/head&gt;
    &lt;p&gt;Come 2024, I was getting enough pieces together to actually have a proof-of-concept. And then uv came out in February. That complicated things a bit as it did/planned to do things I had planned to help entice people to care about lock files. I also knew I couldn't keep up with the folks at Astral as I didn't get to work on this full-time as a job (although I did get a lot more time starting in September of 2024).&lt;/p&gt;
    &lt;p&gt;I also became a parent in April which initially gave me a chunk of time (babies for the first couple of months sleep a lot, so if gives you a bit of time). And so in July I posted the first draft of PEP 751. It was based on &lt;code&gt;pdm.lock&lt;/code&gt; (which itself is based on &lt;code&gt;poetry.lock&lt;/code&gt;). It covered sdists and wheels and was multi-use, all by recording the projects to install as a set which made installation fast.&lt;/p&gt;
    &lt;p&gt;But uv's popularity was growing and they had extra needs that PDM and Poetry– the other major participants in the PEP discussions --didn't. And do I wrote another draft where I pivoted from a set of projects to a graph of projects. But otherwise the original feature set was all there.&lt;/p&gt;
    &lt;p&gt;And then Hynek came by with what seemed like an innocuous request about making the version of a listed project optional instead of required (which was done because the version is required in &lt;code&gt;PKG-INFO&lt;/code&gt; in sdists and &lt;code&gt;METADATA&lt;/code&gt; in wheels).&lt;/p&gt;
    &lt;p&gt;Unfortunately the back-and-forth on that was enough to cause the Astral folks to want to scale the whole project back all the way to the &lt;code&gt;requirements.txt&lt;/code&gt; v2 solution.&lt;/p&gt;
    &lt;p&gt;While I understood their reasoning and motivation, I would be lying if I said it wasn't disappointing. I felt we were extremely close up to that point in reaching an agreement on the PEP, and then having to walk back so much work and features did not exactly make me happy.&lt;/p&gt;
    &lt;p&gt;This was covered by 974 posts on discuss.python.org.&lt;/p&gt;
    &lt;head rend="h2"&gt;2025&lt;/head&gt;
    &lt;p&gt;But to get consensus among uv, Poetry, and PDM, I did a third draft of PEP 751. This went back to the set of projects to install, but was single-use only. I also became extremely stringent with timelines on when people could provide feedback as well as what would be required to add/remove anything. At this point I was fighting burn-out on this subject and my own wife had grown tired of the subject and seeing me feel dejected every time there was a setback. And so I set a deadline of the end of March to get things done, even if I had to drop features to make it happen.&lt;/p&gt;
    &lt;p&gt;And in February I thought we had reached and agreement on this third draft. But then Frost Ming, the maintainer of PDM, asked why did we drop multi-use lock files when they thought the opposition wasn't that strong?&lt;/p&gt;
    &lt;p&gt;And so, with another 150 posts and some very strict deadlines for feedback, we managed to bring back multi-use lock files and get PEP 751 accepted-- with no changes! -- on March 31.&lt;/p&gt;
    &lt;head rend="h2"&gt;2 PEPs and 6 years later ...&lt;/head&gt;
    &lt;p&gt;If you add in some ancillary discussions, the total number of posts on the subject of lock files since 2019 comes to over 1.8K. But as I write this post, less than 7 months since PEP 751 was accepted, PDM has already been updated to allow users to opt into using &lt;code&gt;pylock.toml&lt;/code&gt; over &lt;code&gt;pdm.lock&lt;/code&gt; (which shows that the lock file format works and meets the needs of at least one of the three key projects I tried to make happy). Uv and pip also have some form of support.&lt;/p&gt;
    &lt;p&gt;I will say, though, that I think I'm done with major packaging projects (work has also had me move on from working on packaging since April, so any time at this point would be my free time, which is scant when you have a toddler). Between &lt;code&gt;pyproject.toml&lt;/code&gt; and &lt;code&gt;pylock.toml&lt;/code&gt;, I'm ready to move on to the next area of Python where I think I could be the most useful.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45556741</guid><pubDate>Sun, 12 Oct 2025 09:21:29 +0000</pubDate></item><item><title>Nostr and ATProto (2024)</title><link>https://shreyanjain.net/2024/07/05/nostr-and-atproto.html</link><description>&lt;doc fingerprint="231dd9512d25a3d4"&gt;
  &lt;main&gt;
    &lt;p&gt;This post could’ve been titled “Nostr vs ATProto”, but that really isn’t what I wanted to do here. While I will be comparing and contrasting them a lot, and that’s kind of even the point of writing this, I didn’t want to really pit the two against each other at all, and especially not with the title. I also want to try avoiding commenting on the differences between the communities that have formed on the protocols and their apps, although I definitely will be looking at the philosophical differences between the two a lot - also kind of the point of writing this. This also isn’t a super deep technical post, though it assumes familiarity with technical concepts. I also might come back to edit parts of it and add more later.&lt;/p&gt;
    &lt;p&gt;You can read and leave comments on this post here on Bluesky, or here on Nostr, or even here on Mastodon.&lt;/p&gt;
    &lt;p&gt;So I wrote a paragraph mostly about what this post isn’t about, with a little bit about what I will talk about in it, but I haven’t really explained what this post is, or why I’m writing it. Honestly, I’m not completely sure of the first one yet either; I’m figuring that out as I write it. The paragraph at the top are really serving as guidelines for myself as I write this.&lt;/p&gt;
    &lt;p&gt;However, I can explain how this post came to be. It started with a showerthought (I was literally in the shower) about how similar ATProto and Nostr really are. This thought came to me after ruminating on ATProto Relays and Nostr Relays, and thinking about how my favorite feature of Nostr Relays (spoiler: it’s filtering) could be added to ATProto Relays, and why you would want to do that. More broadly, this made me think that the two protocols are similar enough that they are likely to slowly converge over time as they learn from each other.&lt;/p&gt;
    &lt;p&gt;A direct result of those thoughts (after getting out of the shower, of course) was to search the internet for a good comparison of Nostr and ATProto. A direct result of my failure to find any was this Bluesky skoot (There’s a lot of good replies and thoughts in that thread as well—you probably want to read it before continuing with this post). A direct result of my skooting that was this reply. Before, I’d been tentatively considering writing a purely technical comparison after not finding any, but that reply really set the stage for deciding what I wanted to do in this post.&lt;/p&gt;
    &lt;p&gt;So, to start, let’s look at…&lt;/p&gt;
    &lt;head rend="h2"&gt;How we got here&lt;/head&gt;
    &lt;head rend="h3"&gt;A Caged Bird&lt;/head&gt;
    &lt;head rend="h4"&gt;or, Twitter&lt;/head&gt;
    &lt;p&gt;Twitter here could, in theory, be replaced here by just “Centralized Social Media”, but really it was Twitter that got us here. Both ATProto and Nostr exist because of Twitter - the AT Protocol very directly so, Nostr as a response to “censorship” (real or perceived) on Twitter. ATProto is the result of Bluesky’s original mission - to build a decentralized protocol Twitter could adopt. Post-Elon, who knows if that will ever happen, but, well, that is how it started.&lt;/p&gt;
    &lt;p&gt;Twitter sprang into existence in 2007, as a small, SMS-based service that allowed people to post short status updates - tweets, as they became known. Who knows if it was the first of its kind? Well, it certainly became the most popular. It really was the service that was able to popularize the concept of microblogging. It developed a multitude of subcultures, each with their own unique characteristics, often intersecting with each other in fascinating, unpredictable places and ways. And while Twitter certainly never became as popular as some of its big tech companions, it may have had the greatest cultural impact - it was one of the only places in existence where an average person (you!!) could, say, ratio a presidential candidate or give interesting new details on a story to some famous journalist (I don’t know, I just made those up). Some have said it was the first “global town square”.&lt;/p&gt;
    &lt;p&gt;Over the years of Twitter’s existence, lots of things happened to Twitter. Moderation issues including Donald Trump, authoritarian governments around the world, all sorts of mini community wars and harassment, etc. Twitter, as beautiful as it was, well… kind of sucked, and people drew many different (not mutually exclusive and often overlapping!!) conclusions about why. Some, like Christopher Bouzy of Spoutible, concluded that the platform’s moderation simply wasn’t enough for what the platform had become, and people needed a smaller, more closed space with stricter moderation policies. Others concluded that a global-scale social network is simply an inherently bad idea and people should stick to smaller, more tight-knit communities. But one of the most popular conclusions was that something as important as Twitter - whether you considered it a “global town square” or a place to make connections with your community or Whatever Else - simply could not and should not be controlled by a single corporation. Indeed, this was the conclusion that Twitter themselves came to! This is the conclusion that both ATProto and Nostr are founded upon - the idea of a move from closed, centralized, corporate-owned social platforms to a world of open, decentralized social protocols.&lt;/p&gt;
    &lt;p&gt;But ATProto and Nostr don’t exist in a vacuum. They weren’t the only ones to come to this conclusion. They weren’t even the first. And that brings us to…&lt;/p&gt;
    &lt;head rend="h3"&gt;The Mastodon in the Room&lt;/head&gt;
    &lt;head rend="h4"&gt;or, ActivityPub and the Fediverse&lt;/head&gt;
    &lt;p&gt;⚠️ I am not an expert on ActivityPub. Take everything in this section with a grain of salt. If I get something wrong, please correct me. ⚠️&lt;/p&gt;
    &lt;p&gt;ActivityPub is kind of a big deal in the decentralized social protocols world. It’s not the first, either - it would be extremely hard to really find a first. But it is, at least for now, the largest, and realistically is about to become a lot larger, at least if Meta Threads federates with it.&lt;/p&gt;
    &lt;p&gt;It’s also got an entirely different philosophy to either Nostr or ATProto - while both of the latter are based on a more individualistic approach to decentralization, ActivityPub opted for a more collectivist approach, one that favors tight-knit communities over a global network (that hasn’t stopped people from trying to build global networks with it, though.)&lt;/p&gt;
    &lt;p&gt;(Side-note: I should also mention that whether the Fediverse should focus on smaller communities or mass-interconnection has been a debate even within the Fediverse since right about the beginning, which a lot of the differing viewpoints around this topic explained brilliantly by Evan Podromou. Since Small Fedi seems to be the dominant philosophy shaping the current Fediverse, I’ve mostly focused on Small Fedi when talking about ActivityPub here.)&lt;/p&gt;
    &lt;p&gt;There are many different server implementations of the ActivityPub Spec, each adding their own unique flair to the ecosystem. The most popular of these implementations is Mastodon. ActivityPub is also, like I said above, kind of a big deal in the decentralized social protocols world. Almost everyone working on decentralized protocols after ActivityPub has been forced to acknowledge its existence, draw comparisons to it, and often been bridged to it. In fact, when Jack Dorsey fired off his famous tweet thread announcing Bluesky, he was definitely aware of ActivityPub, given that in a reply to a reply to that thread, he stated “ActivityPub is great.”&lt;/p&gt;
    &lt;p&gt;Because ActivityPub uses a federation model centered around small community servers, it has a lot of the benefits of centralized social media. For example, it makes it relatively easy to support private content, since it’s a push-based protocol - only those whose inboxes you push content to can view it (there’s also an “Everyone” option that makes your content fetchable, I think). This is also why the Fediverse has things like Follow Requests, server-to-server DMs (though your instance admin can view them - ActivityPub kind of assumes you trust them), and real blocks that mostly work.&lt;/p&gt;
    &lt;p&gt;However, many of the more collectivist choices made in ActivityPub were concluded to not be conductive to a “decentralized Twitter”, and both ATProto and Nostr exist in large part because of this. In fact, both ATProto and Nostr strayed from ActivityPub for the same reasons - identity is extremely tied to your initial server. There are good reasons for this, given that ActivityPub is largely used by smaller communities who federate with each other, but it does have an important consequence:&lt;/p&gt;
    &lt;p&gt;Your data is not really portable. You can move accounts to another server, and if your old server is well-behaved it can add a redirect to your new account, which will help automatically transfer your old social connections over to your new account, but this doesn’t include any of your data except your follows and followers, and falls apart if your old server goes offline, is adversarial to you or your current server, or in basically any situation where you can’t get that redirect.&lt;/p&gt;
    &lt;p&gt;There are many other philosophical differences between the ActivityPub camp and the Nostr and ATProto camp, but this one is the most important one, at least in my opinion - both ATProto and Nostr have sections explaining “Why not just go with ActivityPub?” that state this as their primary reason. Both ATProto and Nostr have real account portability by design.&lt;/p&gt;
    &lt;p&gt;Both of these protocols don’t have much in common with ActivityPub, so I won’t talk about ActivityPub too much here. But there is one older protocol that both of them extensively draw inspiration from…&lt;/p&gt;
    &lt;head rend="h3"&gt;Secure Scuttlebutt&lt;/head&gt;
    &lt;p&gt;This is where things start to get pretty interesting. In 2014, a New Zealand programmer named Dominic Tarr was living on a sailboat. As you might assume, such a life includes little internet, and when it comes, in sporadic bursts. Centralized social media, like Twitter, wants you to be connected at all times, scrolling your feed and looking at ads. Tarr didn’t want that. The result? He designed a protocol designed for offline-first, intentional, slow communication, free from Big Tech. Its name? Secure Scuttlebutt.&lt;/p&gt;
    &lt;p&gt;Scuttlebutt uses an append-only log of cryptographically signed messages. Your identity is an Ed25519 keypair and is pretty much tied to a single device. One consequence of this is that, as the Scuttlebutt developer docs themselves acknowledge, “If a user loses their secret key or has it stolen, they will need to generate a new identity, and tell people to use their new one instead.”&lt;/p&gt;
    &lt;p&gt;Because it’s an append-only log, every message must contain a reference to the previous message - a bit like a blockchain. That also means that deletes are straight-up impossible. This is also not necessarily a bad thing, just a trade-off.&lt;/p&gt;
    &lt;p&gt;Scuttlebutt started as a purely peer-to-peer protocol, using a gossip model - in fact, that’s where its name comes from; in sailor-slang, scuttlebutt means “water-cooler gossip”. The first popular Scuttlebutt client was an app called Patchwork, authored by Paul Frazee (keep this guy in mind, he’s gonna be important later), and initially the protocol and client often evolved together, adapting to each other’s needs.&lt;/p&gt;
    &lt;p&gt;By default, when you add to your append-only log, that addition only exists on your device; but the next time you connect to a peer running a Scuttlebutt client, your two clients will sync with each others’ logs, and then verify them against each others’ public keys. And to verify the newest part of a Scuttlebutt log, you need the whole log - this ensures that if someone gets part of your content, they get all of it.&lt;/p&gt;
    &lt;p&gt;But you don’t just sync each others’ content - your clients sync all the logs they have locally. That’s why it’s called the gossip model - once you put out a post, as long as you’re connected to a few peers every once in a while, your post will spread as fast as gossip to the friends of your friends. It usually takes time for that information to spread to everywhere, which keeps the pace of Scuttlebutt life somewhat slow and relaxed, with the most active communities being, again, small and tight-knit. Scuttlebutt is definitely not a global social network. The gossip model was driven by the social graph, allowing users to sync with others based on who they follow and who their connections follow. This mechanism relied on cloud bot users, known as “pubs,” acting as connectors and community hubs.&lt;/p&gt;
    &lt;p&gt;Scuttlebutt syncing took time due to the necessity of syncing all activity. Pubs played a crucial role in facilitating connectivity within the network, ensuring that users could discover others either by sharing a pub or by following users who were connected to them.&lt;/p&gt;
    &lt;p&gt;Scuttlebutt’s evolution was influenced by the desire for decentralized communication, distinct from the centralized nature of platforms like Twitter. It offered an alternative for those seeking intentional, offline-first communication free from the constraints of Big Tech. While initially designed for smaller, tight-knit communities, the ideas and learnings from Scuttlebutt inspired later attempts to build decentralized networks suitable for global networking.&lt;/p&gt;
    &lt;p&gt;So, now the stage is mostly set. Twitter was the first “global town square”, a social network connecting people and ideas worldwide - but not without a myriad of problems, which many concluded were due to its centralized nature. ActivityPub and Scuttlebutt (and others) experimented with decentralizing the social world, mostly with a focus on smaller communities, though as they evolved people tried to make them more suitable for global networking. Neither of them would prove viable for global social networks, but the learnings from them would help develop the next generation of social protocols.&lt;/p&gt;
    &lt;head rend="h3"&gt;Freeing the Bird&lt;/head&gt;
    &lt;head rend="h4"&gt;or, where ATProto and Nostr came from&lt;/head&gt;
    &lt;p&gt;All of this is important background for understanding the motivation behind these two protocols. Twitter started it all by showing us what microblogging at scale - a “global town square” - looks like. It showed us how many problems there are with it, and to some, that the only way to fix them is to remove corporate control. ActivityPub and Scuttlebutt showed us two very different ways of doing so, each with their own major benefits and major drawbacks. But there’s still a long way to go from these experiments, which were largely paving the way in the late 2010s, to where we are now, almost halfway into the third decade of the 21st century. To fill in these gaps, we can start towards the end of the second decade of the 21st century.&lt;/p&gt;
    &lt;p&gt;It wasn’t just people outside Twitter who were aware of the multitude of issues with Twitter - of course Twitter noticed them too. Twitter had started as a much more open company than it was at this point in December of 2019 - over the years, they’d taken, for a variety of reasons, a more centralized path, facing investor pressure for returns, and other such things. Twitter knew that, in the words of founder then-CEO Jack Dorsey, “centralized enforcement of global policy to address abuse and misleading information is unlikely to scale over the long-term without placing far too much burden on people.” Jack and the rest of Twitter drew the same conclusion as ActivityPub and Scuttlebutt had before - corporate control of social media was simply bad for everyone. Twitter was a company full of people who realized the service was just in a shitty position no matter how you looked at it, and who were doing everything in their power to keep things healthy despite it all - and they saw a way out: to build on, or build, an open protocol for a global social network. And for all the reasons we talked about before, about ActivityPub and Scuttlebutt, neither of those protocols were up to the task.&lt;/p&gt;
    &lt;p&gt;So the Bluesky initiative began. The early history of the project is much better documented elsewhere, but one of the most interesting things to come out of it at this early stage was an ecosystem review of existing decentralized protocols. It was authored by a Zcash developer named Jay Graber, who would go on to become CEO of Bluesky. It included contributions from several notable people in the decentralization space, including Christine Lemmer-Webber, co-author of the ActivityPub spec, Paul Frazee of Patchwork (and at the time now working on Beaker Browser and Dat), Whyrusleeping from IPFS, and Rabble of early Twitter (at the time working on planetary.social, a Scuttlebutt client). It lays out the state of numerous decentralized protocols, including ActivityPub and Scuttlebutt, and explains how user discovery, moderation, etc works in each of them.&lt;/p&gt;
    &lt;p&gt;At the end of all this ecosystem review, Bluesky concluded that none of these existing protocols was really suitable for their goal - a decentralized protocol Twitter, a global social network, could run on. So they decided to create their own - ATProto - and incorporated into a Public Benefit LLC to help achieve this goal. And when their initial team was hired, it included none other than Paul Frazee of Patchwork, in addition to Aaron Goldman, a former security engineer at Twitter, and Daniel Holmgren, an engineer with experience building on IPFS.&lt;/p&gt;
    &lt;p&gt;Now, while all of this was happening, a Bitcoin enthusiast under the pseudonym Fiatjaf was working on his own little thing. His idea was a non-peer-to-peer reimagining of Scuttlebutt and what it would take to make a similar protocol usable on a global scale. And on November 7th, 2020, the first basic working code for his idea of “Relays” quietly slipped onto the scene. Nostr’s initial description even cites Scuttlebutt as an inspiration - the main design differences between the two (at a high level) are that Nostr moves from a p2p network, with pubs as an afterthought, to a purely client-relay model, and that Nostr events are all separate units that do not form a chain.&lt;/p&gt;
    &lt;p&gt;His motivation for creating this protocol was, somewhat similarly to Bluesky, problems with Twitter. Bluesky was motivated by the idea that content moderation at scale is impossible to do well, and centralizing it in the hands of a single company was a bad idea. Nostr, meanwhile, views moderation itself as an enemy - as censorship that the protocol should be resistant to. While in reality, even Nostr has ultimately ended up exploring different forms of communal moderation, the primary motivation behind Nostr’s design choices is an idea of extremely high censorship resistance. This implies that the design, rather than optimizing for consistency, should optimize for availability - if someone wants to see your content, they should be guaranteed to be able to get it from somewhere. The protocol design is pretty conducive to this.&lt;/p&gt;
    &lt;p&gt;Both of these efforts were toiling away in the darkness, waiting for their moment in order to replace centralized social media with a decentralized future. Then in late 2022, something remarkable happened. Centralized social media fell prey to one of its prime weaknesses, right where everyone could see, thanks to one very famous billionaire. Elon Musk payed 44 billion dollars for Twitter, released the so-called “Twitter Files”, and Jack Dorsey, who had earlier kicked off the Bluesky initiative with 13 million dollars, put out a little manifesto in response, titled a native internet protocol for social media. Within a few hours, someone responded pointing him to the Nostr protocol, and he grew very interested, soon giving fiatjaf 14 Bitcoin to help fund Nostr development. A few months later, Bluesky launched their reference app for the AT Protocol. About a year later, Jack Dorsey left the Bluesky board, having chosen to focus on Nostr instead, as it aligned with his “free-speech-Bitcoin-vibes” ethos better. This was despite the fact that ATProto basically does everything he wants in a decentralized social protocol, but he prefers the more Bitcoin-y community of Nostr.&lt;/p&gt;
    &lt;p&gt;Okay, so that’s how we got here. Now we’ve arrived, back in the present. Let’s look at…&lt;/p&gt;
    &lt;head rend="h2"&gt;Where we are&lt;/head&gt;
    &lt;p&gt;Both Nostr and ATProto follow a similar pattern: adapting peer-to-peer data models to work in a client-server model (that isn’t quite federation). The peer-to-peer world had to deal with a unique problem: because there were no servers, there was no canonical source for data where you could go to verify its integrity. Thanks to the wonders of modern cryptography, efforts like Scuttlebutt, IPFS, and Dat all were able to use self-certifying data structures that could be verified independently of any third-party authority. A good example of this is a Merkle Tree, which is a data structure that ATProto also uses (be sure to watch that video, it’s very good and explains well why peer-to-peer networks need this).&lt;/p&gt;
    &lt;p&gt;As it turned out, these data structures and their benefits would help solve many of the problems the federated world faces. Specifically, the federated world, while no longer reliant on a single central server, often ends up simply shifting this reliance to smaller centralized servers that are the only canonical source for user data. When done correctly, applying peer-to-peer data models to the server would reduce this reliance and make data more independent of servers, while also allowing the big-world networking that only servers can achieve.&lt;/p&gt;
    &lt;p&gt;This sounds like a perfect solution, but it’s worth mentioning that it does have some important tradeoffs compared to a pure federation approach like ActivityPub’s. For example, while deletes are still possible on both protocols (though rather difficult on Nostr, which you might be able to piece together why), if someone has your data saved from before your deletion, it is much easier to prove that you said it and hold it up as yours than it is on a protocol that doesn’t have you cryptographically sign everything. And since both protocols heavily optimize for public content, things like Direct Messaging become much more difficult - in fact, on Nostr, DMs are public like everything else (their content is encrypted so no one else can read them). In general, trying to keep data private becomes extremely difficult; these protocols have delivery models which both center around the same self-certifying data being replicated in many places so anyone who wants it can get at it. With this, things like blocking other users become basically impossible, since there’s no canonical source to restrict content from.&lt;/p&gt;
    &lt;p&gt;Now let’s look at a few different protocol building blocks and how each protocol handles them.&lt;/p&gt;
    &lt;head rend="h3"&gt;Identity&lt;/head&gt;
    &lt;p&gt;Identity in networks is a difficult problem. Ideally, you want identifiers to be human-meaningful - for example, a Twitter handle. If I see the Twitter handle @jack, I can be fairly sure that that’s Jack Dorsey. You also want them to be secure - only @jack should be able to create a post that says it’s from @jack, and I shouldn’t easily be able to take over the account @jack without gaining access to some kind of key. And you probably also want them to be decentralized, so that @jack isn’t beholden to anyone else to hold his identity, and can move around.&lt;/p&gt;
    &lt;p&gt;Unfortunately, it’s not easy to have all three of these nice properties - Secure, Human-Meaningful, and Decentralized - at once. Almost every system which tries to have all three has to end up compromising on one of them. This trilemna is known as Zooko’s Triangle. As examples:&lt;/p&gt;
    &lt;p&gt;Twitter usernames are secure - I can’t just put out a tweet that looks like it’s from @jack - and human-meaningful - a guy with the handle @jack is probably named Jack. But they’re obviously not decentralized - they are all reliant on Twitter’s servers, and it’s Twitter who decides that @jack points to Jack Dorsey’s account. If they, say, wanted to rebrand to X, and someone was using the @x handle, Twitter could easily take it from them and make their own handle @X.&lt;/p&gt;
    &lt;p&gt;Scuttlebutt, meanwhile, has identity that’s decentralized - it’s just your private key, essentially a random number - and your public key, the part other people can see. It’s also secure - I need to actually have your private key to pretend to be you. But a public key, which is also just a number (derived from your private key), is not very human meaningful.&lt;/p&gt;
    &lt;p&gt;If you’re familiar with ActivityPub, you might argue that ActivityPub usernames are all three. This isn’t really true - ActivityPub usernames behave like Twitter usernames, except instead of just one big central Twitter server deciding what username points to what, this is handled in smaller centralized servers which federate with each other.&lt;/p&gt;
    &lt;p&gt;Nostr and ATProto also experience this problem, and they both share a few views around identity, listed out here so each one corresponds to a side of Zooko’s Triangle:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Your identity should not be permanently tied to a single server - Decentralization&lt;/item&gt;
      &lt;item&gt;Your data should be cryptographically verifiable as coming from your identity - Security&lt;/item&gt;
      &lt;item&gt;There are two “layers” of identity - a permanent computer-oriented one and a changeable human-friendly one - Human-Meaningful.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Even with these similarities, how that really plays out in both protocols looks extremely different. The idea that your data is cryptographically verifiable as yours implies a keypair somewhere. In Nostr, that’s exactly it - your identity is just a secp256k1 keypair. Nothing more, nothing less.&lt;/p&gt;
    &lt;p&gt;That sounds very much like the permanent computer-oriented layer of identity. So the human-friendly identity is handled by a Nostr event of the profile type - this contains stuff like your bio, display name, and avatar. There’s also NIP-05, which allows using the .well-known/nostr.json path on a domain to get email-style usernames, like &lt;code&gt;jack@cash.app&lt;/code&gt; - and this includes a special case, &lt;code&gt;_@domain&lt;/code&gt;, that gets treated by clients as just &lt;code&gt;@domain&lt;/code&gt;. When you &lt;code&gt;@mention&lt;/code&gt; someone in a Nostr note, it’s just &lt;code&gt;@&amp;lt;their public key&amp;gt;&lt;/code&gt;, which clients then simply display as their display names. Notably, having either a display name or even a real NIP-05 username is completely optional under Nostr, and your public key really is your identity.&lt;/p&gt;
    &lt;p&gt;This looks like mostly a success, at least in terms of taking those views and treating them as criteria. Nostr actually takes the first point - identity should not be permanently tied to a single server - and goes slightly further: in Nostr’s model, where your identity really is just your keypair, no servers are involved in identity at all. Why would you want that? A major benefit of this approach is that if any of the servers involved in the system goes down or is no longer friendly with you, your identity doesn’t even need to be “recovered” - it’s just there, the same as before. This works well with the Nostr Relay model, which we’ll discuss in the next section.&lt;/p&gt;
    &lt;p&gt;The drawbacks of this approach are the same as Scuttlebutt. Thanks to the relay model, your identity is no longer tied to a single client on a single device - you can easily move around, between relays, between clients, between devices. This, by itself, for most people, is a good thing, but it comes with an entirely different kind of problem:&lt;/p&gt;
    &lt;p&gt;Managing a cryptographic keypair is simply not very user-friendly. You simply can’t expect most people to write it down and keep it in a safe place or even take the time to understand what it means. People expect username-password systems, and sure, newer technology like passkeys is actually more secure and potentially easier - but that comes with actual benefits over username-password for most people! Managing a keypair is not only unfriendly, it’s incredibly risky. Since the entirety of your identity is your keypair, and to sign in to Nostr clients is to give them your private key - well, you can probably see where this is going. And again, since your identity is just your keypair, just like with Scuttlebutt, if an attacker gets a hold of your private key, that identity is gone. No longer yours. There’s no-one you can go to for help, no-one who can recover that account, no password reset link.&lt;/p&gt;
    &lt;p&gt;That sounds very negative, but it is worth noting that at least for web Nostr clients, there is a (relatively) good solution to the sign-in problem - NIP-07. In the NIP-07 world, you don’t give every client your private key - you give it once to a browser extension, and then every time a web client wants to do something on your behalf, instead of directly using your private key to sign messages etc, it delegates that to your trusted extension. This is a lot better than giving your private key out to every client that has some cool new feature you want to try. Of course, this doesn’t help with recoverability - if you lose your private key, whether to your memory or to an attacker, it’s still gone. There are attempts to solve this, too, which I’ll talk about in “Where we’re going” because it has interesting future implications.&lt;/p&gt;
    &lt;p&gt;ATProto looks at things a little differently. Because of the aforementioned difficulties involved with users managing their own private keys, Bluesky chose to have your signing keypair live on a server - your Personal Data Server, or PDS. Your PDS is responsible for serving your Data Repository to other services on the network, and serves as more-or-less the canonical source for your content. However, your Repository is fully self-certifiable (that means someone can check whether or not you created the content in a copy of your Repo without needing a third party to verify), and so is not permanently tied to your PDS. This is because your PDS is not the canonical source for your identity - but your identity is also not something as small as a keypair here, and does not live entirely client side.&lt;/p&gt;
    &lt;p&gt;Instead, ATProto uses their own homegrown DID (Decentralized IDentifiers, W3C spec with the aim of helping, well, decentralize identity) method called did:plc, for PLaCeholder. Why is it named “placeholder”? Well, because as of now, it’s centralized. That’s right, the supposedly “Decentralized” Identifier is centralized - and Bluesky actively doesn’t want it to be that way. did:plc was initially intended to be a placeholder until a decentralized method was able to meet their requirements - “a strongly consistent, highly available, recoverable, and cryptographically secure method with fast and cheap propagation of updates”. did:plc has all of these at one major cost - it’s centralized. However, the data in a did:plc is self-certifying (you don’t need to trust/rely on plc.directory to verify the information), so it’s conceivable for it to become more decentralized in the future. (You can also use a did:web, which removes this centralization but forces you to manage everything yourself and relies permanently on your control of a web host on a domain, thus removing most of PLC’s benefits. This is pretty niche, so I won’t talk about it in detail here.)&lt;/p&gt;
    &lt;p&gt;A did:plc: contains two public keys - your rotation key and your signing key. This signing key is the aforementioned key that the PDS uses to sign your data. The rotation key is important because it manages your did:plc: and thus is needed to sign updates to your DID document, such as when migrating PDSes. The canonical source for your current PDS, valid signing key, handle, and rotation keys (which can also be rotated) are all your DID document. In this way, a DID serves as a “Theseus Identity”, an idea Aaron Goldman laid out well in this YouTube video.&lt;/p&gt;
    &lt;p&gt;The canonical source of your identity is your DID doc, and all the information in it, i.e. your handle and current PDS must be a two-way connection - your handle is a domain with a dns txt record or ./well-known/atproto-did that must point to your DID, providing two way verification, and whatever PDS your DID document points to must actually have your account on it. Meanwhile, the PDS handles data, and implements a standard, user-friendly login system, and signs your updates with your key on the server side.&lt;/p&gt;
    &lt;p&gt;Here, there was a trade-off between principles of security, recoverability, and user-friendliness, and a principle of max-decentralization - low-friction identity, with no centralizing points of control at all, extreme takeover resistance. Notice that&lt;/p&gt;
    &lt;p&gt;Where ATProto chooses user-friendliness, Nostr chooses max-decentralization. This is a trend that repeats in many other parts of each protocol’s design, as we’ll see.&lt;/p&gt;
    &lt;head rend="h3"&gt;Data&lt;/head&gt;
    &lt;p&gt;In the traditional federated world of protocols like ActivityPub, there had never been much of an emphasis on data, and the formats and structures it’s stored in. The federated world thought much more about how servers should communicate messages rather than how they should store data - this difference is laid out well by Bryan Newbold, who incidentally now works on protocol design at Bluesky. This emphasis on communication standards rather than data standards is a big part of why there’s no standard “fediverse repo” that you can transfer between servers, and other such problems in the federated world.&lt;/p&gt;
    &lt;p&gt;The peer-to-peer world, as we looked at earlier, couldn’t afford to define pure transport protocols - they had to design standardized data structures that were self-certifying and self-contained. An example of such a data structure is a blockchain, and indeed, the peer-to-peer community and the blockchain community learned much more from each other than either of them and federation did from each other.&lt;/p&gt;
    &lt;p&gt;This was the status quo until ATProto and Nostr came along and broke the mold by bringing these self-certifying data structures into the client-server world. They both use asymmetric cryptography to make this data self-certifying, but the similarities basically end there.&lt;/p&gt;
    &lt;p&gt;In the Nostr model, servers are dumb. They have basically one job - transmit data. There’s only one kind of server in Nostr - a Relay, and a Relay does only three things:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Receive data to store&lt;/item&gt;
      &lt;item&gt;Return that data when asked for it&lt;/item&gt;
      &lt;item&gt;Provide a continuous stream of the data being placed on that Relay&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Notably, Relays store data. Data is placed on Relays. All this data is created on the client-side. Relays don’t manage identity or any of that. Your keys live with your client, and it’s your client who signs your &lt;code&gt;events&lt;/code&gt; (a piece of data in Nostr terminology.) When you fetch data from a Relay, it comes back with signatures and all - which, guess what, your client verifies. Your client almost operates under the assumption that Relays will try to do weird stuff, people will submit fake events, etc - and so Nostr removed the requirement of trust, by making clients verify everything themselves. A trade-off!&lt;/p&gt;
    &lt;p&gt;Nostr, by optimizing for censorship resistance, needs to remove as much rigidity from its design as possible. Data needs to be cheap to create and transmit and store. So Nostr events all exist as individual units following a fixed JSON format with a strict signing convention. Unlike Scuttlebutt, these events don’t need to form a chain - they are purely self-contained. Like your identity, there’s no canonical source for them either - by design, you’re supposed to be able to get them from pretty much any relay that has them. When you create the event, your client signs it and then just publishes it to as many relays as possible, from where it will circulate into other Relays, consuming clients will republish them, etc. Because they are signed against your public and are fully self-contained, it’s trivial to verify them too, removing the necessity of trust in the Relay you get the event from.&lt;/p&gt;
    &lt;p&gt;ATProto data is also very portable, but it is slightly more rigid than Nostr data is. Instead of using these one-off events which are fully self-certifying, ATProto stores your data as records in what it calls a repo. These records live under a collection like &lt;code&gt;app.bsky.feed.post&lt;/code&gt; and are given an &lt;code&gt;rkey&lt;/code&gt; (record key). Together, this forms a URI for any given record that looks like &lt;code&gt;at://did/collection/rkey&lt;/code&gt;. Importantly, records are mutable, unlike nostr events, and the contents an at:// uri points to may change. However, all the commits to your repo, which contain changes like record creation, editing, and deletion, are content-addressed using a CID, and these are immutable, and are all signed using your repo’s signing key (the one from your DID doc, remember?) Your commits can also optionally form a chain if you want, but when they don’t, deletes are easier. (If all of that flew over your head, don’t worry. All you need to know is that ATProto allows deletes and edits, while Nostr can’t.) Because your data all lives in this repo, unlike Nostr, ATProto actually has a canonical source for your data.&lt;/p&gt;
    &lt;p&gt;There’s also a single place where your repo lives, instead of being scattered as a bunch of events across Relays like in Nostr. Your repo lives in your Personal Data Server - as the name implies, a PDS is designed to store your personal data. While Nostr Relays are dumb pipes, PDSes are more like a user agent, which really performs almost all actions on the user’s behalf. It’s responsible for signing and storing commits to your repo and wrapping them in a nice API that’s easy for clients to use.&lt;/p&gt;
    &lt;p&gt;Actually, we should probably take a minute just to talk about deletes and edits. When I said Nostr can’t allow deletes and edits, that wasn’t completely true: Nostr does have a way to request deletes from Relays, which most but not all Relays support, but the real trouble is figuring out what a delete even means (and edits are straight-up impossible since Nostr event IDs are fully content-addressed). Nostr’s model is fundamentally based on an idea of events flowing from the creator into Relays, which then flow into other people’s clients, which cache them and republish them to other Relays, and so on. An event doesn’t have a location to be deleted from - it could be (and in Nostr’s model, should be!) anywhere and everywhere.&lt;/p&gt;
    &lt;p&gt;In ATProto, your repo actually has a place where it lived - your PDS, as specified in your DID doc. And at:// uris are mutable, so a commit can actually change the content it points to. Deletes remove content from your repo - although anybody who has a copy of your content pre-delete will still have it and can very easily cryptographically prove that it’s your content.&lt;/p&gt;
    &lt;head rend="h3"&gt;Trust&lt;/head&gt;
    &lt;p&gt;Nostr and ATProto have relatively similar approaches to trust, though with some important differences. Nostr trusts nobody, and is built accordingly, with clients verifying everything themselves. ATProto assumes you trust somebody, but lets you choose whom you trust, and provides the mechanisms needed to verify that trust is placed correctly (although this could be improved).&lt;/p&gt;
    &lt;p&gt;Nostr, as mentioned earlier, was designed to basically eliminate the necessity of trust in the first place. Because everything is verified client-side, and essentially functions as a bunch of self-authenticated units of data traveling between relays and clients, there really is no one to trust. Relays can choose not to carry content, but other relays might have them instead. However, the fact that all data moves as individual units means that it would be harder to spot if only certain events are available.&lt;/p&gt;
    &lt;p&gt;Since every user is assumed to be pointing their client at more than one relay, it doesn’t really matter if one relay chooses not to carry someone’s content; there’s a high likelihood another one is. If many relays agree to hide something from the network, then it won’t show up, but that’s pretty unlikely to happen. As for trusting the authenticity of the content delivered by the relay, because it’s cryptographically verifiable as coming from the attached pubkey, any shenanigans will be spotted quickly. And verifying a pubkey’s identity is done by attaching it to a trusted NIP-05, i.e. @jack@cash.app or @jb55.com.&lt;/p&gt;
    &lt;p&gt;ATProto isn’t that different, all things considered, but there’s multiple other hops between the source of data and the client you view it in. Each ATProto PDS puts out a cryptographically verifiable stream of commits being pushed to repos on the PDS, carrying every bit of data to the subscribers, called the firehose. Because there are a lot of PDSes, an optimization also called a Relay was introduced, which basically aggregates PDS firehoses into its own giant firehose. In a way, this Relay could be considered its own centralization point where bad untrustworthy things could happen, but once more than one Relay exists this should be less of a problem. At the Relay and PDS, everything is cryptographically verifiable, and as a bonus because of ATProto’s repo structure, you can tell if you’re not getting the whole picture.&lt;/p&gt;
    &lt;p&gt;After the Relay, things get a bit murkier, because as an optimization ATProto applications use something called the AppView. The AppView reads in the firehose from the Relay constantly and pieces it together into fully hydrated and speedy APIs which make clients’ lives much easier. The thing about the AppView is that it’s basically centralized, and though it’s not super difficult to spot inconsistencies between what the AppView gives you and the true state of the network, the AppView doesn’t even provide the cryptographic signatures that were passed into it, making its trustworthiness a bit murky at some unknown time in the future, at which point other contenders will hopefully exist to replace it, based on analysis of which one is more trustworthy by comparing the data each AppView gives you with what actually exists on the Relay and PDSes.&lt;/p&gt;
    &lt;head rend="h3"&gt;Privacy&lt;/head&gt;
    &lt;p&gt;Everything is completely public on both protocols and in fact being actively broadcasted to loads of consumers, not just sitting around waiting to be stepped on and found. Nothing you do is really hideable from anyone.&lt;/p&gt;
    &lt;p&gt;However, at least on ATProto, there have been attempts to add some semblance of privacy to the network. For example, there are AppView-enforced blocks, but they can be bypassed very easily. There is also a setting which asks the client to not show your posts to logged-out users, but this is superficial at best, since only some clients really follow it anyways, and the “official” popular client does so it does kind of work. But overall these measures both run a risk of making people feel like their posts and other activity are hidden and safe, lulling them into acting with less precaution than they should, especially since there is a lack of user awareness around the all-public nature of data on the network.&lt;/p&gt;
    &lt;p&gt;No such attempts have been made on Nostr. This is on the one hand unfortunate, but on the other hand possibly better since it is more honest about the true nature of how public everything is on the network.&lt;/p&gt;
    &lt;head rend="h3"&gt;Development&lt;/head&gt;
    &lt;p&gt;Due to the Bluesky devs’ past experiences with developing on peer-to-peer and federated protocols, many of them felt burnt by a Scuttlebutt-and-Nostr-style approach to development, where specifications were loose and implementations varied wildly. Because of these past experiences, Bluesky chose to go with a slightly more slow, intentional, and centralized development model. The protocol is mostly developed within Bluesky the company, though often adapts to the needs and feedback from the wider ATProto developer community, and community members often contribute to both the protocol and the clients. The rollout of core features like federation and stackable moderation has also been much more slow on ATProto than similar features in Nostr implementations, because in general Bluesky prefers to take their time and “get it right” and standardized before letting things out into the wild. Also, despite the existence of third-party clients, the “official” Bluesky app and service is still the most popular one by a huge margin, due to its being the default (and basically only) inroad into the protocol and ecosystem. There are other up-and-coming AT Protocol projects that aren’t just Twitter clones, like WhiteWind for blogging, but overall the ecosystem remains sparse compared to Nostr.&lt;/p&gt;
    &lt;p&gt;Nostr, meanwhile, takes the same approach as these previous projects - the protocol itself just exists, very small, letting anyone expand on it. When an extension wants to become standardized, it’s reviewed by a small team including fiatjaf and a few others, and becomes part of the NIPs repository (Nostr Implementation Possibilities). This is basically classic BDFL open-source. However, clients and relays are free to try their own wild things without being “official” NIPs, and any NIP proposal must be adopted by a few clients and relays before it can be considered for “official” status. So it’s a much wilder, freer ecosystem so far.&lt;/p&gt;
    &lt;head rend="h3"&gt;Applications&lt;/head&gt;
    &lt;p&gt;One of the places where ATProto and Nostr differ greatly is their model for building applications.&lt;/p&gt;
    &lt;p&gt;ATProto takes the AppView approach. An AppView is basically a service that reads in the firehose of all the public data on the network, and indexes it into hydrated “views” as an API which clients then use. AppViews are pretty resource-intensive to run and functionally centralized in nature. If you want to make a new ATProto app, you first design your schemas for content in a DSL called Lexicon. Then you make a client that can start publishing your record type, and retrieving and displaying it. For the retrieval and displaying, you create an AppView which monitors the firehose for your record types and indexes them into hydrated views, which your client can then fetch from and display nicely and neatly. This is, for example, how the Bluesky app can show a list of users who liked a post; because instead of the client having to crawl the entire network itself and figure out which likes are for the post you just viewed and then get the DID and fetch each of that user’s profiles and whether or not you’re following them by checking your own repo, and whether or not they’re following you by looking all over their follow lists, the client just makes one HTTP request and makes the result human-readable. Nice and fast. Of course, the relief that comes to the client means a lot of responsibility is thrusted onto the AppView, which becomes very resource-intensive to run.&lt;/p&gt;
    &lt;p&gt;The first steps to the Nostr model look similar at first, but rapidly diverge. With Nostr, you also start with defining event kinds, and then creating a client which can publish them, and then adding fetching and displaying. The key difference is in how events are fetched. With ATProto, you write an AppView to do the heavy lifting; with Nostr, the heavy lifting is shared between the Relay and the Client. When defining your event kinds, you make sure to also define how to use the “tags” field for that event kind, which is an array of key-value pairs with single letter keys which are indexed by the relays the events are sent to. Basically, if you want to do any kind of linking between events, or inserting any kind of indexable data, that’s where you want to do it.&lt;/p&gt;
    &lt;p&gt;Then for the fetching of the data, we use Nostr’s filtering system. With Nostr, there are two kinds of communication between the client and the relays; publishing events, which pushes the signed client-created event into the relay’s data store, and subscription. Subscription is the interesting part we’re looking at here.&lt;/p&gt;
    &lt;p&gt;Nostr clients can request a subscription to a stream of events from the relays they’re connected to, and this stream subscription can have filters attached. A filter is fully specified using the following attributes, all optional:&lt;/p&gt;
    &lt;code&gt;{
  "ids": &amp;lt;a list of event ids&amp;gt;,
  "authors": &amp;lt;a list of lowercase pubkeys, the pubkey of an event must be one of these&amp;gt;,
  "kinds": &amp;lt;a list of a kind numbers&amp;gt;,
  "#&amp;lt;single-letter (a-zA-Z)&amp;gt;": &amp;lt;a list of tag values, for #e — a list of event ids, for #p — a list of pubkeys, etc.&amp;gt;,
  "since": &amp;lt;an integer unix timestamp in seconds, events must be newer than this to pass&amp;gt;,
  "until": &amp;lt;an integer unix timestamp in seconds, events must be older than this to pass&amp;gt;,
  "limit": &amp;lt;maximum number of events relays SHOULD return in the initial query&amp;gt;
}
&lt;/code&gt;
    &lt;p&gt;By adding multiple filters, you can get all the events matching any of the filters. By adding multiple attributes to a single filter, you add multiple conditions that all have to be fulfilled for events to make it through that filter. Filters are expressly the mechanism for fetching content, since subscriptions are supposed to start by backfilling everything that meets the criteria, and then pushing any new events that meet the filters’ requirements to the client.&lt;/p&gt;
    &lt;p&gt;By studying the filter specification, it’s clear that basically every behavior of ATProto AppViews can be recreated through filters on the client-side, knowing how tags allow extensibility as well. There’s an obvious cost though: clients must be very complex and do a lot of work themselves, and for big events duplicating a lot of effort that could be handled by something akin to an AppView. The benefit of this is that it is very generic and means that any relay can generally be used for any functionality since everything you need is baked into the core protocol, and the speed of development is basically only constrained by the client, and not an AppView. And by not spending any resources on building a giant indexer yourself, you basically shift the cost onto the Relays instead. It’s another example of the more “bazaar” philosophy of Nostr compared to a more “cathedral” approach from ATProto.&lt;/p&gt;
    &lt;p&gt;So, all in all, this gives a pretty good picture of where the two protocols are now. But exciting things are on the horizon for both. We’re heading into uncharted territory…&lt;/p&gt;
    &lt;head rend="h2"&gt;Where we’re going&lt;/head&gt;
    &lt;p&gt;When Jack Dorsey wrote a native internet protocol for social media, he wrote that “As far as the free and open social media protocol goes, there are many competing projects: @bluesky is one with the AT Protocol, nostr another, Mastodon yet another, Matrix yet another…and there will be many more. One will have a chance at becoming a standard like HTTP or SMTP.”&lt;/p&gt;
    &lt;p&gt;That’s one way of thinking about it, as a competition for the final spot of “the standard for social”. But as you’ve probably noticed from reading this post up to here, I don’t really agree with this viewpoint. ATProto, Nostr, ActivityPub, Scuttlebutt, Matrix, IPFS, Dat, Holepunch, and others all share similar goals, yet have vastly different perspectives about how to accomplish them. Maybe these different perspectives will all lose! Maybe, as Jack says, one of them will win, becoming a standard that everyone adopts. Or maybe they will all learn from each other and slowly begin to converge. And it’s not hard to make the case that that last possibility will happen for at least two of these protocols - of course, Nostr and ATProto. In fact, that’s already happening.&lt;/p&gt;
    &lt;head rend="h3"&gt;Convergence&lt;/head&gt;
    &lt;p&gt;Because a lot of core ideas in the protocols were already very similar, they can quite easily borrow ideas from each other in order to improve themselves. By making nearly opposite compromises, they now face roughly opposite problems as well - but often, the other protocol already has a solution waiting for them. So first let’s look at some of the ways Nostr is becoming more like ATProto.&lt;/p&gt;
    &lt;p&gt;First, the idea of keys in a server, instead of purely client-side. As mentioned earlier, one of the dangers of Nostr keys is that by giving them to lots of random clients you try, they might accidentally end up in the hands of bad actors. One of the solutions to this was NIP-07 browser extensions; another one is the idea of an NSecBunker, for Nostr Secret Key Bunker. The idea is that this is a server, similar to a PDS, which holds your Nostr private key, and when your client wants to sign an event, it makes a request to your NSecBunker to sign that event using your private key, which stays safe in your Bunker. These requests usually are authenticated using measures like OAuth. It allows Nostr to bring back at least one part of the user experience people are familiar with.&lt;/p&gt;
    &lt;p&gt;Another idea that Nostr is ending up trying is something similar to AppViews. This is particularly divisive within the community, with many feeling that only the relay-based filtering mechanisms should be used to build clients. But because this is often inefficient, clients like Primal have begun doing their own pre-indexing of many users and posts in order to improve their UX. Unfortunately, Primal’s is proprietary, and only Primal can interact with it, due to the lack of any built-in support for AppView-style services in the Nostr protocol, vs. ATProto’s numerous mechanisms to provide explicit support for this use case.&lt;/p&gt;
    &lt;p&gt;Meanwhile, some Nostr ideas are naturally going to the ATProto world as well. The idea of keys directly owned by the users has long been floated, and at this point developers can get control of their did:plc and its rotationKeys (fun fact: I set one of my plc rotationKeys to my Nostr pubkey). Unfortunately no nice UI exists for this yet. And as for signing keys, with commits that could be pushed to a PDS instead of made there, that would rely on a PDS supporting this use case. No PDS implementation currently supports this, but there is one in development which hopes to at some point ;)&lt;/p&gt;
    &lt;p&gt;Another idea which I hope to see adopted in the ATProto world is something similar to Nostr’s filters model. While the AppView model is nice for production apps, something like Nostr filters could help a lot early in development to just play with an idea and try it out. And it could help those with concerns about the trustworthiness of AppViews quickly verify it against certain queries. You can do a shocking amount with backlinks alone.&lt;/p&gt;
    &lt;p&gt;Of course, the slow convergence of both protocols isn’t the only way the divide between them is being bridged…&lt;/p&gt;
    &lt;head rend="h3"&gt;Bridging&lt;/head&gt;
    &lt;p&gt;Recently, Bridgy Fed started bridging the Fediverse and the ATmospherewith each other. For a while, services like Mostrhave been bridging the Fediverse and Nostr with each other. Now, if you visit the Mostr homepage and scroll down, you can probably see where this is going…&lt;/p&gt;
    &lt;p&gt;Soon after Bridgy Fed started bridging the Fediverse and the ATmosphere, Nostr users experimented with this to bridge between Nostr and Bluesky. Very much an indirect hack, but also a glimpse at the future.&lt;/p&gt;
    &lt;p&gt;One of the most important promises of decentralized social media was that no matter what service you signed up on and post on, you would be able to see content from and interact with anyone, no matter which service they used either. Now, all this would work, if every service signed on to the same decentralized social protocol. However, instead, we have many, and none of them show much of a sign of becoming the singular standard for social media. Instead of Jack’s vision of one winner, bridges offer a vision of a world where every protocol can win, and it truly won’t matter which protocol your service uses, either.&lt;/p&gt;
    &lt;p&gt;While the bridging I talked about above was very indirect, Bridgy Fed itself may soon have native Nostr support. Soon all three major decentralized protocols may be able to talk to each other, and easily too.&lt;/p&gt;
    &lt;p&gt;So. Let’s recap what we’ve been through in this post so far. In the beginning, there was Twitter. Twitter’s problems caused them to look to decentralization as a way to make social media more fair. This caused many new decentralized protocols to emerge, taking inspiration from older ones. Of these new protocols, two of them, Nostr and ATProto, evolved in similar directions, yet unaware of each other made many opposite compromises. And now they are evolving back towards each other, converging in potentially very interesting ways, with bridging offering to make social media not just platform- but protocol-agnostic.&lt;/p&gt;
    &lt;p&gt;The future is looking good for decentralized social media.&lt;/p&gt;
    &lt;p&gt;You can join the conversation on Bluesky here.&lt;/p&gt;
    &lt;head rend="h3"&gt;Comments from Bluesky:&lt;/head&gt;
    &lt;p&gt;Or on Nostr:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45556763</guid><pubDate>Sun, 12 Oct 2025 09:24:43 +0000</pubDate></item><item><title>Macro Gaussian Splats</title><link>https://danybittel.ch/macro.html</link><description>&lt;doc fingerprint="7bcdcf46dcb0d210"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Macro Splats 2025&lt;/head&gt;
    &lt;p&gt; A Gaussian splat is essentially a bunch of blurry ellipsoids. Each one has a view-dependent color, using a process similar to training an AI model, you can optimize until it converges to the photos you feed in. The result is a sort of 3D photograph that can be viewed freely from any angle. &lt;lb/&gt; Captivated by this possibility, I wanted to create splats of macro subjects. The hairy, fuzzy textures and complex structures of insects make them a perfect fit for this technique. &lt;lb/&gt; In theory, creating a splat is as simple as filming the object from all sides. Unfortunately, the extremely shallow depth of field in macro photography completely throws this process off. If you feed unsharp photos into it, the resulting model will contain unsharp areas as well. &lt;lb/&gt; Thankfully, there’s a common technique in macro photography called focus stacking, where multiple images taken from the same angle but with slightly different focal points are combined into one fully sharp photo. A single stack usually contains anywhere from 50 to 500 images. Since I needed to photograph the subject from many angles, I optimized the process to use as few photos per stack as possible and settled on 16. I shot at a small aperture of f/18 to maximize depth of field. The diffraction introduced by this setup can be minimized later in post. &lt;lb/&gt; To capture the specimen from all angles, covering a bit more than half a hemisphere, I mounted the insect on a rotary disk and tilted the camera up and down on a boom arm. A script rotated the disk by fixed increments, and each focus stack was captured using a WeMacro automated focus rail. The vertical angle was adjusted manually (only eight times), so it wasn’t a big issue. In total, I captured 111 perspectives. A full session of 1776 photos took about four hours. The main bottleneck is my Nikon D810, which isn’t built for such continuous shooting, it slows down to one frame every one or two seconds once the buffer fills up. I used a Tamron 90mm lens with a 20mm extension and shot in DX (cropped sensor) mode. Shorter lenses would change the perspective too much between focus areas, making image alignment impossible. &lt;lb/&gt; After batch focus-stacking all the photos, I ended up with 111 fully sharp images. The camera positions could then be reconstructed in COLMAP. I performed some color correction and background masking before feeding the data into training with Postshot. Out comes the splat, requiring only minimal retouching to remove the mounting. &lt;/p&gt;
    &lt;head rend="h1"&gt;See it in 3D&lt;/head&gt;
    &lt;p&gt; You can view all the insects on my superspl.at page. &lt;lb/&gt; I’m also releasing the cluster fly model for free under a CC BY license: Download here. You’re free to use this model for both commercial and non-commercial purposes, as long as you provide credit. &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45556952</guid><pubDate>Sun, 12 Oct 2025 10:08:02 +0000</pubDate></item><item><title>Quantification of fibrinaloid clots in plasma from pediatric Long COVID patients</title><link>https://www.researchsquare.com/article/rs-7483367/v1</link><description>&lt;doc fingerprint="2c9b11db8577b373"&gt;
  &lt;main&gt;
    &lt;p&gt;Long COVID (LC) impacts one in five children after an acute SARS-CoV-2 infection. Typical LC symptoms include fatigue, brain fog, pain, and shortness of breath, which can significantly impact individuals and society. Moreover, LC may impair school performance and have long-term health and development consequences. However, the diagnosis of LC is often imprecise and cumbersome, delaying appropriate care. To address the LC diagnosis challenges, we focused on fibrinaloid clots (microclots), recently proposed as contributing to LC’s underlying mechanisms. We overcame the limitations of current microclot assessment methods, which are qualitative, by developing a microfluidic device to quantify the number of microclots in patient blood samples. We found significantly higher microclot levels in samples from pediatric LC patients than from healthy children. We evaluated the diagnostic power of the device in a cohort of 45 LC patients and 14 healthy pediatric donors. We estimated a 94% accuracy for the microclot count using the devices, significantly higher than the traditional counting of microclots on slides (66% accuracy). Intriguingly, we found the highest microclot counts in samples from patients with persistent SARS-CoV-2 spike protein in the blood. Further studies will evaluate the utility of the assay as a screening test for Long COVID in larger populations and for assessing treatment responses.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45557267</guid><pubDate>Sun, 12 Oct 2025 11:08:58 +0000</pubDate></item><item><title>Extreme weather caused more than $100B in damage by June</title><link>https://www.livescience.com/planet-earth/climate-change/extreme-weather-caused-more-than-usd100-billion-in-damage-by-june-smashing-us-records</link><description>&lt;doc fingerprint="7d8dac744fd0a599"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Extreme weather caused more than $100 billion in damage by June — smashing US records&lt;/head&gt;
    &lt;p&gt;After damaging wildfires in LA, tornadoes and series of floods, the first six months of this year smashed multiple extreme weather records in the U.S., data show — and experts say this trend is likely to continue.&lt;/p&gt;
    &lt;p&gt;The first half of 2025 logged the most damaging extreme weather on record in terms of cost, even after accounting for inflation, data shows.&lt;/p&gt;
    &lt;p&gt;This is partly because of a handful of extraordinary events, such as the Los Angeles wildfires. But the number of natural disasters that struck this winter was also "exceptional," Paul Ullrich, a professor of regional climate modeling at the University of California, Davis, told Live Science.&lt;/p&gt;
    &lt;p&gt;All told, the first half of the year likely incurred $93 billion to $126 billion in damages, half-year estimates from insurance companies Munich Re, AON and Gallagher Re suggest. This total smashes the previous most costly first six months on record, of $57 billion (inflation-adjusted), set in 2023, according to National Oceanic and Atmospheric Administration (NOAA) data.&lt;/p&gt;
    &lt;p&gt;This is part of a larger trend. Natural weather disasters have become more frequent and destructive over the past several decades as a result of climate change and land use changes, and that trend is likely to continue.&lt;/p&gt;
    &lt;p&gt;"As long as we continue to warm the planet, we can expect extreme weather to grow more severe and more frequent," Kristina Dahl, a climate scientist and vice president for science at Climate Central, told Live Science in an email. "Combine that with our propensity to build communities in areas prone to climate-related hazards — think of the heavily built-up shoreline along the hurricane-prone coast of Florida or of small California communities tucked up against flammable forests — and we can expect the economic cost of those events to continue to rise as well."&lt;/p&gt;
    &lt;p&gt;So far, these natural disasters haven't led to a dramatic increase in deaths, due in part to advances in weather forecasting and early warning systems that help get people out of harm's way, experts say. But as climate change intensifies, that could change, as some disasters are projected to increase in parts of the U.S., such as floods and regional heat waves, will be more difficult for people to avoid.&lt;/p&gt;
    &lt;head rend="h2"&gt;Record-breaking winter&lt;/head&gt;
    &lt;p&gt;One event this winter caused much of 2025's natural disaster damage — the Los Angeles wildfires. In January, wildfires burned through mostly "high-value single-family homes" in Pacific Palisades and Eaton Canyon, Zhiyun Li, a climate economist at UCLA, told Live Science in an email. Together, they are the most costly wildfires in U.S. history — and the costliest in world history.&lt;/p&gt;
    &lt;p&gt;Get the world’s most fascinating discoveries delivered straight to your inbox.&lt;/p&gt;
    &lt;p&gt;Insurance companies Gallagher Re, Munich Re and AON estimated total damages between $53 billion and $65 billion, taking into account direct losses such as property damage. Other estimates, which factor in property damage, rebuilding costs and capital losses such as impacts on gross domestic product (GDP), and in some cases health care and relocation costs and other factors, range from $76 billion to $275 billion.&lt;/p&gt;
    &lt;p&gt;In comparison, California's entire 2018 wildfire season — the costliest season on record — incurred $30 billion (inflation-adjusted) in damages, according to estimates from NOAA.&lt;/p&gt;
    &lt;p&gt;The LA fires were extraordinarily destructive because they intensified quickly, as a result of strong Santa Ana winds, extremely dry conditions and overgrown vegetation from previous wet years, which dried rapidly.&lt;/p&gt;
    &lt;p&gt;"We could call it a one-two or a one-two-three punch, leading up to the catastrophic events in January," Daniel Swain, a climate scientist at UCLA, told Live Science.&lt;/p&gt;
    &lt;p&gt;And unlike most previous wildfires, these blazes hit expensive, densely populated urban areas.&lt;/p&gt;
    &lt;p&gt;In addition to the California wildfires, a high number of other extreme weather events have occurred across the U.S. — in particular tornadoes. Two tornado events, in March and May, each caused at least $8 billion in insured losses each, according to AON. Floods and winter weather have also caused a lot of damage this year.&lt;/p&gt;
    &lt;p&gt;These have all helped make the first half of the year extremely costly.&lt;/p&gt;
    &lt;head rend="h2"&gt;Costly year in the forecast&lt;/head&gt;
    &lt;p&gt;If hurricane season is bad, 2025 could turn out to be the most expensive in history for weather disaster damage.&lt;/p&gt;
    &lt;p&gt;The most damaging and deadly weather events in the U.S. have been hurricanes, NOAA data show. So far, the Atlantic hurricane season has been quiet— but the season runs through November, and forecasters have predicted an above-average season.&lt;/p&gt;
    &lt;p&gt;Since 1980, tropical cyclones have caused more than $1.5 trillion of damage in the U.S. (inflation-adjusted) — more than all other types of extreme events combined — and have caused 7,211 deaths, more than any other extreme weather type, NOAA data show.&lt;/p&gt;
    &lt;p&gt;One reason hurricanes can be so destructive is that they often make landfall in heavily developed areas. "One of the most desirable places for building new homes and places to live is the ocean against the shore," said Stephen Strader, an associate professor of geography and the environment at Villanova University. "It is beautiful 99.9% of the time, until the hurricane comes knocking and you end up with a lot of people in very highly exposed areas," he told Live Science.&lt;/p&gt;
    &lt;p&gt;Government cuts mean NOAA has stopped publishing data on billion-dollar weather disasters. But overall, if the second half of this year is in line with the five-year average for July to December, the year's total damage cost could be in the ballpark of $220 billion. This would mean $110 billion January through June and $110 billion for July through December.&lt;/p&gt;
    &lt;p&gt;To calculate this number, Live Science took the estimates of the first half of 2025 from Gallagher Re, AON and Munich Re — which NOAA previously said is comparable with its own estimates — and combined the average of those with NOAA's estimates of average damage costs from July through December between 2019 and 2024.&lt;/p&gt;
    &lt;head rend="h2"&gt;Climate change impact&lt;/head&gt;
    &lt;p&gt;The destruction caused by natural disasters has been rising rapidly in recent decades.&lt;/p&gt;
    &lt;p&gt;In 1980, there were just three events that caused more than $1 billion in inflation-adjusted damage and the 1980s in total saw 33 such disasters, according to NOAA data. By comparison, 2024 alone saw 27 inflation-adjusted billion-dollar events, and the 2020s so far have experienced 115 disasters that cost at least $1 billion, not including this year.&lt;/p&gt;
    &lt;p&gt;"It just seems like every year we have an increasing trend. And it's ticking up and up and up," Strader said.&lt;/p&gt;
    &lt;p&gt;Part of this is due to human-caused climate change, which is making extreme weather events increasingly likely and intense.&lt;/p&gt;
    &lt;p&gt;"We're just going to see more intense storms," Ullrich said. "They're going to intensify more rapidly as they make landfall, and you're going to see larger storm surge associated with them, which is going to affect coastal infrastructure."&lt;/p&gt;
    &lt;p&gt;"Any sort of extreme storm, particularly ones producing heavy rainfall, [is] worsened by climate change — that is, they produce more precipitation because of warmer atmospheric conditions," Ullrich said.&lt;/p&gt;
    &lt;p&gt;This effect was seen during the Texas flash floods in July, when a large storm dropped about a foot (30 centimeters) of water in just a few hours over the Texas Hill Country, in turn causing the Guadalupe River to rise by more than 26 feet (8 meters) and burst its banks.&lt;/p&gt;
    &lt;p&gt;Flood losses are projected to increase by as much as 147% in Louisiana and 74% in Florida in 2050 relative to 2020's losses, according to Climate Central. Much of this will be driven by climate change, Climate Central found.&lt;/p&gt;
    &lt;p&gt;But it's not just storms that are supercharged by climate change. "We know that climate change is increasing the frequency and severity of fire weather — the hot, dry, windy conditions that are so conducive to the spreading of fire," Dahl said.&lt;/p&gt;
    &lt;p&gt;The rapid transition between wet and dry conditions that fueled the LA blazes also may be worsened by climate change, research suggests.&lt;/p&gt;
    &lt;p&gt;Overall, "there's abundant and overwhelming evidence that climate change is increasing the severity and likelihood of extreme fire conditions in the American West," Swain told Live Science.&lt;/p&gt;
    &lt;head rend="h2"&gt;Expanding bull's-eye&lt;/head&gt;
    &lt;p&gt;Land-use changes also contribute to the increased devastation of natural disasters.&lt;/p&gt;
    &lt;p&gt;"We sort of have this two-headed monster going on in the world, which is our climate is changing; we're having [worse] extreme weather," Strader said. At the same time, more people live in the U.S. and on the planet, which increases our exposure, he added.&lt;/p&gt;
    &lt;p&gt;One factor that makes natural disasters more destructive is the expansion of urban areas. In 2017, 3.3% of the entire land area in the U.S. was urban — up from 2.2% in 1982 and just 0.8% in 1949. Population density has also increased, and the urban population has ballooned from 167 million in 1980 to 249 million in 2010.&lt;/p&gt;
    &lt;p&gt;This means there are more buildings and infrastructure to be destroyed, that weather events are more likely to hit urban areas, and that more people are affected when these areas are hit. Experts have dubbed this the "expanding bull's-eye effect."&lt;/p&gt;
    &lt;p&gt;Strader noted that the expanding bull's-eye effect creates a particular risk for tornadoes.&lt;/p&gt;
    &lt;p&gt;"We're not really seeing any change in the number of tornadoes," and those tornadoes aren't getting stronger, Strader said. Instead, tornadoes are now more damaging because we have built more homes and infrastructure in tornado-prone areas, he noted.&lt;/p&gt;
    &lt;p&gt;In addition, we are modifying the land in ways that make some disasters more likely. Replacing open space with roads, for instance, can worsen flooding.&lt;/p&gt;
    &lt;p&gt;"If we've modified the environment to have more pavement versus open areas where water can percolate, then we may have increased that urban flood hazard, even if the rain hasn't changed and the size of the city hasn't changed," Swain said.&lt;/p&gt;
    &lt;p&gt;Our actions have also worsened the risks of wildfires. In the past, small, low-intensity fires regularly cleared out brush and helped regenerate forests in the American West. But after a century of fire suppression, the wilderness is overgrown. "There's a deficit of natural, beneficial fires," Swain said. This leads to a large buildup of fuel for wildfires, so when one does occur, it burns hotter, grows larger and is more devastating, Swain said.&lt;/p&gt;
    &lt;head rend="h2"&gt;Deaths versus damages&lt;/head&gt;
    &lt;p&gt;Even though natural disasters are becoming more frequent and destructive, more people aren't dying as a result. That's largely because warning systems and forecasting have improved, which, in turn, helps people get to safety in time, experts noted.&lt;/p&gt;
    &lt;p&gt;However, as climate change worsens, forecasting may not prevent a rise in fatalities.&lt;/p&gt;
    &lt;p&gt;"The two biggest [disaster types] I worry about are heat and flooding," Strader said. "When we have very high heat waves, it affects a huge portion of the country, and heat is something you can't escape from, especially if you're vulnerable" or if you don't have resources, he added, noting that minority groups are particularly affected.&lt;/p&gt;
    &lt;p&gt;Cities can become like "concrete ovens" — and while such extreme weather events may not always cause massive infrastructure damage, they can be as deadly as a massive hurricane or wildfire. For instance, a heat wave in the Midwest in September 1995 killed 872 people and caused $2 billion (inflation-adjusted) in damage, according to NOAA. But 2017's Hurricane Harvey — the second-most-destructive extreme event on record, which hit Texas and Louisiana — caused $160 billion (inflation-adjusted) of damage and just 89 deaths.&lt;/p&gt;
    &lt;p&gt;And government policies could lead to more deaths in the short term, experts said.&lt;/p&gt;
    &lt;p&gt;The government is looking to cut funding and staff headcount at the Federal Emergency Management Agency (FEMA), the Forest Service and NOAA. These cuts will likely lead to significant impacts, both in the near future and longer term, experts told Live Science.&lt;/p&gt;
    &lt;p&gt;"It is actually highly plausible that the cuts that are being done right now … will probably increase disaster losses. It's very much a self-inflicted wound," Swain said. "The savings by cutting these programs are almost certainly to be far less than the costs incurred through greater disaster losses to come as a result of these objectively very short-sighted decisions."&lt;/p&gt;
    &lt;p&gt;James is Live Science’s production editor and is based near London in the U.K. Before joining Live Science, he worked on a number of magazines, including How It Works, History of War and Digital Photographer. He also previously worked in Madrid, Spain, helping to create history and science textbooks and learning resources for schools. He has a bachelor’s degree in English and History from Coventry University.&lt;/p&gt;
    &lt;p&gt;You must confirm your public display name before commenting&lt;/p&gt;
    &lt;p&gt;Please logout and then login again, you will then be prompted to enter your display name.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45558160</guid><pubDate>Sun, 12 Oct 2025 13:38:25 +0000</pubDate></item><item><title>Jeep pushed software update that bricked all 2024 Wrangler 4xe models</title><link>https://twitter.com/StephenGutowski/status/1977055831720862101</link><description>&lt;doc fingerprint="d635f48b34542867"&gt;
  &lt;main&gt;
    &lt;p&gt;We’ve detected that JavaScript is disabled in this browser. Please enable JavaScript or switch to a supported browser to continue using x.com. You can see a list of supported browsers in our Help Center.&lt;/p&gt;
    &lt;p&gt;Help Center&lt;/p&gt;
    &lt;p&gt;Terms of Service Privacy Policy Cookie Policy Imprint Ads info © 2025 X Corp.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45558318</guid><pubDate>Sun, 12 Oct 2025 14:07:13 +0000</pubDate></item><item><title>Germany's Schleswig-Holstein Completes Migration to Open Source Email</title><link>https://news.itsfoss.com/schleswig-holstein-email-system-migration/</link><description>&lt;doc fingerprint="6b5b391b3d96b4a7"&gt;
  &lt;main&gt;
    &lt;p&gt;European nations have generally been more progressive in adopting open source solutions for government operations. Sure, regressive proposals like the EU Chat Control bill make headlines, but there's genuine progress happening too.&lt;/p&gt;
    &lt;p&gt;The German state of Schleswig-Holstein is back in the news for its open source efforts. This time, it's their email system that's undergone a complete transformation.&lt;/p&gt;
    &lt;head rend="h2"&gt;Microsoft Booted Out, Again&lt;/head&gt;
    &lt;p&gt;Schleswig-Holstein has successfully migrated (in Deutsch) its entire state administration email system from Microsoft Exchange and Outlook to open source alternatives, Open-Xchange and Thunderbird. The German state completed the transition on October 2, 2025, after a six-month process.&lt;/p&gt;
    &lt;p&gt;The migration affected around 30,000 employees across various government departments. This includes the State Chancellery, ministries, judiciary, state police, and other state authorities. Over 40,000 mailboxes containing more than 100 million emails and calendar entries were moved to the new system.&lt;/p&gt;
    &lt;p&gt;The state has adopted Open-Xchange as its email server solution and Thunderbird as the email client.&lt;/p&gt;
    &lt;p&gt;Of course, the transition wasn't without challenges. Digitization Minister Dirk Schrödter previously acknowledged problems during migration to open source software, including downtime and delays in email traffic. Despite these hurdles, this particular move has now been completed successfully.&lt;/p&gt;
    &lt;p&gt;Plus, this switch fits into Schleswig-Holstein's broader open source strategy that has been in development for several years. The state began rolling out LibreOffice as its standard office software last year, gradually replacing Microsoft Office across all state computers.&lt;/p&gt;
    &lt;p&gt;Dirk also emphasized that:&lt;/p&gt;
    &lt;quote&gt;We are real pioneers. We can't fall back on the experience of others –, there is hardly a comparable project of this magnitude anywhere in the world.&lt;lb/&gt;In future, we will be able to use our experience from data analysis to monitoring in the data centre to help others and support them when they embark on the path that we are currently the first to take.&lt;/quote&gt;
    &lt;head rend="h3"&gt;My Thoughts&lt;/head&gt;
    &lt;p&gt;Well, I like what I see here. Not many governments around the world care about open source software, unless it is about optics, of course. But when concrete steps are being taken to make good on past claims, who am I to complain?&lt;/p&gt;
    &lt;p&gt;Via: heise online&lt;/p&gt;
    &lt;p&gt;Suggested Read 📖&lt;/p&gt;
    &lt;p&gt;- Even the biggest players in the Linux world don't care about desktop Linux users. We do.&lt;/p&gt;
    &lt;p&gt;- We don't put informational content behind paywall. Your support keeps it open for everyone. Think of it like 'pay it forward'.&lt;/p&gt;
    &lt;p&gt;- Don't like ads? With the Plus membership, you get an ad-free reading experience.&lt;/p&gt;
    &lt;p&gt;- When millions of AI-generated content is being published daily, you read and learn from real human Linux users.&lt;/p&gt;
    &lt;p&gt;- It costs just $2 a month, less than the cost of your favorite burger.&lt;/p&gt;
    &lt;p&gt;Become a Plus Member today and join over 300 people in supporting our work.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45558635</guid><pubDate>Sun, 12 Oct 2025 14:53:45 +0000</pubDate></item><item><title>No I don't want to turn on Windows Backup with One Drive</title><link>https://idiallo.com/byte-size/say-no-to-onedrive-backup</link><description>&lt;doc fingerprint="fe842b4d37a7f8e7"&gt;
  &lt;main&gt;
    &lt;p&gt;What are my options here?&lt;/p&gt;
    &lt;p&gt;Inside the "Remind me again in" label are the following options:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;1 Week&lt;/item&gt;
      &lt;item&gt;30 days&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And when I select one option, what does "No thanks" do? Does it dismiss the box entirely and I never have to see it? Or does it validate the reminder schedule? What if I just don't want OneDrive? Microsoft has embedded it so deep into Windows that there are no easy ways of getting rid of it. I would understand if they had asked me when I set up my machine the first time, but why every single time?&lt;/p&gt;
    &lt;p&gt;This is the illusion of choice. It is "Get it now" or "Get it later". What I'm looking for is the "Get it never option!"&lt;/p&gt;
    &lt;p&gt;Dark patterns at their finest.&lt;/p&gt;
    &lt;p&gt;After I restarted, I got this!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45559023</guid><pubDate>Sun, 12 Oct 2025 15:42:00 +0000</pubDate></item></channel></rss>